{
    "links": [
        "https://helmholtz.software/software/2x2-3x3-and-nxn-space-filling-curves",
        "https://helmholtz.software/software/ai4hpc",
        "https://helmholtz.software/software/aiida-kkr",
        "https://helmholtz.software/software/aiida-spirit",
        "https://helmholtz.software/software/album",
        "https://helmholtz.software/software/alice2modelica",
        "https://helmholtz.software/software/allpix-squared",
        "https://helmholtz.software/software/alpaca",
        "https://helmholtz.software/software/alpaka",
        "https://helmholtz.software/software/amiris",
        "https://helmholtz.software/software/anndata",
        "https://helmholtz.software/software/ansible-collection-toolkit",
        "https://helmholtz.software/software/anvio",
        "https://helmholtz.software/software/arbor",
        "https://helmholtz.software/software/ardoco",
        "https://helmholtz.software/software/arosics",
        "https://helmholtz.software/software/atomec",
        "https://helmholtz.software/software/autogaita",
        "https://helmholtz.software/software/autopq",
        "https://helmholtz.software/software/autopv",
        "https://helmholtz.software/software/autowp",
        "https://helmholtz.software/software/aviator",
        "https://helmholtz.software/software/awi-gpt",
        "https://helmholtz.software/software/awi-pangaea-ai-hub",
        "https://helmholtz.software/software/base-repo",
        "https://helmholtz.software/software/basic",
        "https://helmholtz.software/software/basicpy",
        "https://helmholtz.software/software/beluga",
        "https://helmholtz.software/software/bending-stiffness",
        "https://helmholtz.software/software/beos",
        "https://helmholtz.software/software/bifurcations-discrete-maps",
        "https://helmholtz.software/software/bioautoml",
        "https://helmholtz.software/software/blenderproc",
        "https://helmholtz.software/software/boxbeam",
        "https://helmholtz.software/software/brainprint",
        "https://helmholtz.software/software/cadet",
        "https://helmholtz.software/software/calibr8",
        "https://helmholtz.software/software/cat4kit",
        "https://helmholtz.software/software/catena",
        "https://helmholtz.software/software/celldetection",
        "https://helmholtz.software/software/cellrank",
        "https://helmholtz.software/software/chase",
        "https://helmholtz.software/software/cheetah",
        "https://helmholtz.software/software/chemotion-eln",
        "https://helmholtz.software/software/climate-index-collection",
        "https://helmholtz.software/software/cimpredict",
        "https://helmholtz.software/software/circlize",
        "https://helmholtz.software/software/citation-file-format",
        "https://helmholtz.software/software/citychem",
        "https://helmholtz.software/software/climsight",
        "https://helmholtz.software/software/cnpypp",
        "https://helmholtz.software/software/cola",
        "https://helmholtz.software/software/collector-app",
        "https://helmholtz.software/software/comando",
        "https://helmholtz.software/software/comola",
        "https://helmholtz.software/software/complexheatmap",
        "https://helmholtz.software/software/cipm",
        "https://helmholtz.software/software/copy-paste-imputation",
        "https://helmholtz.software/software/corc",
        "https://helmholtz.software/software/corsika",
        "https://helmholtz.software/software/cosmoscout-vr",
        "https://helmholtz.software/software/cp2k",
        "https://helmholtz.software/software/cryogrid",
        "https://helmholtz.software/software/crystfel",
        "https://helmholtz.software/software/cuas-mpi",
        "https://helmholtz.software/software/cubegui",
        "https://helmholtz.software/software/cubelib",
        "https://helmholtz.software/software/cubew",
        "https://helmholtz.software/software/cudamemtest",
        "https://helmholtz.software/software/cupla",
        "https://helmholtz.software/software/damnit",
        "https://helmholtz.software/software/dasf-messaging-python",
        "https://helmholtz.software/software/datadesc",
        "https://helmholtz.software/software/datalad",
        "https://helmholtz.software/software/datalad-container-extension",
        "https://helmholtz.software/software/datalad-next-extension",
        "https://helmholtz.software/software/datasail",
        "https://helmholtz.software/software/dcache",
        "https://helmholtz.software/software/deeploki",
        "https://helmholtz.software/software/deploy2zenodo",
        "https://helmholtz.software/software/deus",
        "https://helmholtz.software/software/digital-earth-viewer",
        "https://helmholtz.software/software/dirschema",
        "https://helmholtz.software/software/displam",
        "https://helmholtz.software/software/dl4pude",
        "https://helmholtz.software/software/earth-system-model-evaluation-tool-esmvaltool",
        "https://helmholtz.software/software/easywave",
        "https://helmholtz.software/software/egsim",
        "https://helmholtz.software/software/ehrapy",
        "https://helmholtz.software/software/electrode",
        "https://helmholtz.software/software/elephant",
        "https://helmholtz.software/software/emipy",
        "https://helmholtz.software/software/enpt",
        "https://helmholtz.software/software/enrichedheatmap",
        "https://helmholtz.software/software/esm-tools",
        "https://helmholtz.software/software/esmvalcore",
        "https://helmholtz.software/software/ethosfine-framework-for-integrated-energy-system-assessment",
        "https://helmholtz.software/software/hisim",
        "https://helmholtz.software/software/ethospenalps",
        "https://helmholtz.software/software/ethosreflow",
        "https://helmholtz.software/software/ethoszoomin",
        "https://helmholtz.software/software/elias",
        "https://helmholtz.software/software/beta-faircore4eosc",
        "https://helmholtz.software/software/fairmq",
        "https://helmholtz.software/software/fame",
        "https://helmholtz.software/software/fastscape-toolbox",
        "https://helmholtz.software/software/fastsurfer",
        "https://helmholtz.software/software/fatsegnet",
        "https://helmholtz.software/software/fesom",
        "https://helmholtz.software/software/fishinspector",
        "https://helmholtz.software/software/fiware-deployment-kit",
        "https://helmholtz.software/software/fleur",
        "https://helmholtz.software/software/formatfuzzer",
        "https://helmholtz.software/software/fracspy",
        "https://helmholtz.software/software/fsqc",
        "https://helmholtz.software/software/gaia",
        "https://helmholtz.software/software/gasnetsim",
        "https://helmholtz.software/software/geomultisens",
        "https://helmholtz.software/software/gfzrnx",
        "https://helmholtz.software/software/ginkgo",
        "https://helmholtz.software/software/gipptools",
        "https://helmholtz.software/software/glaes",
        "https://helmholtz.software/software/global-benchmark-database-gbd",
        "https://helmholtz.software/software/gmgpolar",
        "https://helmholtz.software/software/goac",
        "https://helmholtz.software/software/golem",
        "https://helmholtz.software/software/gravis",
        "https://helmholtz.software/software/gr-framework",
        "https://helmholtz.software/software/gstools",
        "https://helmholtz.software/software/habitat-sampler",
        "https://helmholtz.software/software/hcocena",
        "https://helmholtz.software/software/heat",
        "https://helmholtz.software/software/heatnetsim",
        "https://helmholtz.software/software/heliport",
        "https://helmholtz.software/software/hifis-rsd",
        "https://helmholtz.software/software/higgs-dataset-training",
        "https://helmholtz.software/software/hilbertcurve",
        "https://helmholtz.software/software/hipsta",
        "https://helmholtz.software/software/holowizard",
        "https://helmholtz.software/software/hybridmt",
        "https://helmholtz.software/software/icgem",
        "https://helmholtz.software/software/ideal-equilibrium-oxygen-membrane-reactor",
        "https://helmholtz.software/software/igmas",
        "https://helmholtz.software/software/interactivecomplexheatmap",
        "https://helmholtz.software/software/interactivevis",
        "https://helmholtz.software/software/ioproc",
        "https://helmholtz.software/software/iqtools",
        "https://helmholtz.software/software/isaac",
        "https://helmholtz.software/software/jards",
        "https://helmholtz.software/software/jcuber",
        "https://helmholtz.software/software/jemris",
        "https://helmholtz.software/software/jplag",
        "https://helmholtz.software/software/jtrack",
        "https://helmholtz.software/software/jube",
        "https://helmholtz.software/software/jukkr",
        "https://helmholtz.software/software/julearn",
        "https://helmholtz.software/software/jumpdiff",
        "https://helmholtz.software/software/jumper",
        "https://helmholtz.software/software/jupedsim",
        "https://helmholtz.software/software/jupyterhub-outpost",
        "https://helmholtz.software/software/jupyterhub-outpostspawner",
        "https://helmholtz.software/software/jurassic",
        "https://helmholtz.software/software/juri",
        "https://helmholtz.software/software/kaapana",
        "https://helmholtz.software/software/kadi4mat",
        "https://helmholtz.software/software/kagen-communication-free-massively-distributed-graph-generators",
        "https://helmholtz.software/software/kahypar",
        "https://helmholtz.software/software/kaminpar",
        "https://helmholtz.software/software/kamping-karlsruhe-mpi-next-generation",
        "https://helmholtz.software/software/karri",
        "https://helmholtz.software/software/kd-tree-python",
        "https://helmholtz.software/software/key",
        "https://helmholtz.software/software/keymaera-x",
        "https://helmholtz.software/software/kinfit",
        "https://helmholtz.software/software/kramersmoyal",
        "https://helmholtz.software/software/lapy",
        "https://helmholtz.software/software/libertem",
        "https://helmholtz.software/software/lightning-uq-box",
        "https://helmholtz.software/software/linmog",
        "https://helmholtz.software/software/llama",
        "https://helmholtz.software/software/llview",
        "https://helmholtz.software/software/lynx",
        "https://helmholtz.software/software/maftools",
        "https://helmholtz.software/software/mainzelliste",
        "https://helmholtz.software/software/mallob",
        "https://helmholtz.software/software/mallocmc",
        "https://helmholtz.software/software/mapman",
        "https://helmholtz.software/software/massbank",
        "https://helmholtz.software/software/materials-learning-algorithms",
        "https://helmholtz.software/software/matrad",
        "https://helmholtz.software/software/mcodac",
        "https://helmholtz.software/software/me-compute",
        "https://helmholtz.software/software/mitk",
        "https://helmholtz.software/software/membrain-v2",
        "https://helmholtz.software/software/mercy",
        "https://helmholtz.software/software/merkle-dag-matlab",
        "https://helmholtz.software/software/meshit",
        "https://helmholtz.software/software/messy",
        "https://helmholtz.software/software/metabolator",
        "https://helmholtz.software/software/methylkit",
        "https://helmholtz.software/software/metrics-reloaded",
        "https://helmholtz.software/software/mfdfa",
        "https://helmholtz.software/software/mhm",
        "https://helmholtz.software/software/mibianto",
        "https://helmholtz.software/software/micromechanics-indentationgui",
        "https://helmholtz.software/software/millepede-ii",
        "https://helmholtz.software/software/minterpy",
        "https://helmholtz.software/software/mirp",
        "https://helmholtz.software/software/mlair",
        "https://helmholtz.software/software/mmpxrt",
        "https://helmholtz.software/software/mdis",
        "https://helmholtz.software/software/mode-behave",
        "https://helmholtz.software/software/moovie",
        "https://helmholtz.software/software/mptrac",
        "https://helmholtz.software/software/mss",
        "https://helmholtz.software/software/mtress",
        "https://helmholtz.software/software/multiphase-code-repository-by-hzdr",
        "https://helmholtz.software/software/nest",
        "https://helmholtz.software/software/nest-ml",
        "https://helmholtz.software/software/nuclear-nexus",
        "https://helmholtz.software/software/nnu-net",
        "https://helmholtz.software/software/nodejs-tcp-server-client",
        "https://helmholtz.software/software/nodejs-tls-server-client",
        "https://helmholtz.software/software/novosparc",
        "https://helmholtz.software/software/o3as",
        "https://helmholtz.software/software/odm2sms",
        "https://helmholtz.software/software/odv",
        "https://helmholtz.software/software/oemof-solph",
        "https://helmholtz.software/software/opencarp",
        "https://helmholtz.software/software/openfuelcell2",
        "https://helmholtz.software/software/opengeosys",
        "https://helmholtz.software/software/openpmd-api",
        "https://helmholtz.software/software/osadcp-toolbox",
        "https://helmholtz.software/software/otter",
        "https://helmholtz.software/software/palladio",
        "https://helmholtz.software/software/training-catalogue-for-photon-neutron",
        "https://helmholtz.software/software/pasta-bit-vector",
        "https://helmholtz.software/software/pdaf",
        "https://helmholtz.software/software/peakperformance",
        "https://helmholtz.software/software/pecon",
        "https://helmholtz.software/software/pedpy",
        "https://helmholtz.software/software/pepc",
        "https://helmholtz.software/software/perihub",
        "https://helmholtz.software/software/perilab",
        "https://helmholtz.software/software/perun",
        "https://helmholtz.software/software/petrack",
        "https://helmholtz.software/software/php-codemeta-crosswalk",
        "https://helmholtz.software/software/picongpu",
        "https://helmholtz.software/software/pigx",
        "https://helmholtz.software/software/pkgndep",
        "https://helmholtz.software/software/pointneighborsjl",
        "https://helmholtz.software/software/postwrf",
        "https://helmholtz.software/software/potsdam-open-source-radio-interferometry-tool",
        "https://helmholtz.software/software/profasi",
        "https://helmholtz.software/software/propulate",
        "https://helmholtz.software/software/pia",
        "https://helmholtz.software/software/ptylab",
        "https://helmholtz.software/software/pyapi-rts",
        "https://helmholtz.software/software/pycomlink",
        "https://helmholtz.software/software/pygms",
        "https://helmholtz.software/software/pyquickmaps",
        "https://helmholtz.software/software/pysdc",
        "https://helmholtz.software/software/python-icat",
        "https://helmholtz.software/software/pyxmake",
        "https://helmholtz.software/software/quast",
        "https://helmholtz.software/software/radiative-forcing-of-hypersonic-aircraft-trajectories",
        "https://helmholtz.software/software/radplanbio",
        "https://helmholtz.software/software/rafcon",
        "https://helmholtz.software/software/random-simplex",
        "https://helmholtz.software/software/rankings-reloaded",
        "https://helmholtz.software/software/rayx",
        "https://helmholtz.software/software/rce",
        "https://helmholtz.software/software/reflectorch",
        "https://helmholtz.software/software/remix",
        "https://helmholtz.software/software/restore",
        "https://helmholtz.software/software/rgreat",
        "https://helmholtz.software/software/ribodetector",
        "https://helmholtz.software/software/rtlola-frontend",
        "https://helmholtz.software/software/rtlola-interpreter",
        "https://helmholtz.software/software/s2downloader",
        "https://helmholtz.software/software/sampledb",
        "https://helmholtz.software/software/saqc",
        "https://helmholtz.software/software/scalasca",
        "https://helmholtz.software/software/scanpy",
        "https://helmholtz.software/software/scits",
        "https://helmholtz.software/software/score-p",
        "https://helmholtz.software/software/sdaas",
        "https://helmholtz.software/software/seisbench",
        "https://helmholtz.software/software/seiscomp",
        "https://helmholtz.software/software/sensor-management-system",
        "https://helmholtz.software/software/serghei",
        "https://helmholtz.software/software/sfctools",
        "https://helmholtz.software/software/shepard",
        "https://helmholtz.software/software/shockhash",
        "https://helmholtz.software/software/sichash",
        "https://helmholtz.software/software/signal-processor",
        "https://helmholtz.software/software/simcats",
        "https://helmholtz.software/software/simona",
        "https://helmholtz.software/software/simpa",
        "https://helmholtz.software/software/simplifyenrichment",
        "https://helmholtz.software/software/smash",
        "https://helmholtz.software/software/smg2s",
        "https://helmholtz.software/software/somesy",
        "https://helmholtz.software/software/spatialdata-framework",
        "https://helmholtz.software/software/spatialio",
        "https://helmholtz.software/software/spechomo",
        "https://helmholtz.software/software/spex",
        "https://helmholtz.software/software/spiralize",
        "https://helmholtz.software/software/spirit",
        "https://helmholtz.software/software/stable-baselines3",
        "https://helmholtz.software/software/stmlab",
        "https://helmholtz.software/software/stream2segment",
        "https://helmholtz.software/software/sumo",
        "https://helmholtz.software/software/supervillain",
        "https://helmholtz.software/software/swh-client",
        "https://helmholtz.software/software/t8code",
        "https://helmholtz.software/software/tamarin-prover",
        "https://helmholtz.software/software/tbt-segmentation",
        "https://helmholtz.software/software/tereno-doi",
        "https://helmholtz.software/software/tetrax",
        "https://helmholtz.software/software/tigl",
        "https://helmholtz.software/software/tigramite",
        "https://helmholtz.software/software/timeio",
        "https://helmholtz.software/software/timeseries-management",
        "https://helmholtz.software/software/tixi",
        "https://helmholtz.software/software/tomato-tools",
        "https://helmholtz.software/software/tomobear",
        "https://helmholtz.software/software/treams",
        "https://helmholtz.software/software/tridec-cloud",
        "https://helmholtz.software/software/trimmomatic",
        "https://helmholtz.software/software/trixiparticlesjl",
        "https://helmholtz.software/software/tsmp",
        "https://helmholtz.software/software/ukis-csmask",
        "https://helmholtz.software/software/ultimodel",
        "https://helmholtz.software/software/ultramassexplorer-ume",
        "https://helmholtz.software/software/unicore",
        "https://helmholtz.software/software/uqtestfuns",
        "https://helmholtz.software/software/urbem-urban-emission-downscaling-for-air-quality-modeling",
        "https://helmholtz.software/software/urmoac",
        "https://helmholtz.software/software/utile-oxy",
        "https://helmholtz.software/software/varfish",
        "https://helmholtz.software/software/vasca",
        "https://helmholtz.software/software/velocityconversion",
        "https://helmholtz.software/software/vencopy",
        "https://helmholtz.software/software/vinos",
        "https://helmholtz.software/software/vitess",
        "https://helmholtz.software/software/vitruvius",
        "https://helmholtz.software/software/voltron",
        "https://helmholtz.software/software/weskit",
        "https://helmholtz.software/software/wombat",
        "https://helmholtz.software/software/wps-command-line-tool-repository",
        "https://helmholtz.software/software/wrainfo",
        "https://helmholtz.software/software/xcascade",
        "https://helmholtz.software/software/xdibias",
        "https://helmholtz.software/software/xraypac"
    ],
    "final_links": [
        {
            "software_organization": "https://helmholtz.software/software/2x2-3x3-and-nxn-space-filling-curves",
            "repo_link": "https://github.com/jokergoo/sfcurve",
            "content": {
                "codemeta": "",
                "readme": "# sfcurve: 2x2, 3x3 and nxn Space-Filling Curves\n\n[![CRAN](https://www.r-pkg.org/badges/version/sfcurve)](https://cran.r-project.org/web/packages/sfcurve/index.html)\n[![CRAN](https://cranlogs.r-pkg.org/badges/grand-total/sfcurve)](https://cran.r-project.org/web/packages/sfcurve/index.html)\n\n\n![](https://github.com/user-attachments/assets/7e0e14e7-1300-421f-8ffe-113b80caee97)\n\n\nThis package provides a way to encode all possible forms of 2x2 and 3x3\nspace-filling curves. For example, the following eight forms correspond to the\n2x2 curve on level 3 and with `R(0)` (bottom-in right-out base pattern with rotation\nof 0 degree) as the seed.\n\n<img src=\"https://github.com/user-attachments/assets/82b56013-8e9e-45f6-b77a-0875769c6369\" width=700 />\n\nIt also supports nxn curves expanded from any valid level-1 unit.\n\n## Install\n\n```r\ninstall.packages(\"sfcurve\")\n```\n\nor the devel version:\n\n```r\ndevtools::install_github(\"jokergoo/sfcurve\")\n```\n\n## Usage\n\nHilbert curve (2x2):\n\n```\n> sfc_2x2(\"I\", \"111\")\nAn sfc_2x2 object.\n  Increase mode: 2 x 2\n  Level: 3\n  Expansion rule: 2x2\n\nA sequence of 64 base patterns.\n  R(0)L(270)L(0)R(90)     I(0)R(0)R(270)L(180)\n  L(270)R(0)R(270)I(180)  R(180)L(90)L(180)I(270)\n  .... other 4 lines ....\n  I(90)L(90)L(180)R(270)  I(180)R(180)R(90)L(0)\n  L(90)R(180)R(90)I(0)    R(0)L(270)L(0)R(90)\n\nSeed: A sequence of 1 base pattern.\n  I(0)\n```\n\nPeano curve (3x3):\n\n```\n> sfc_3x3_peano(\"I\", \"111\")\nAn sfc_3x3_peano object.\n  Increase mode: 3 x 3\n  Level: 3\n  Expansion rule: 3x3 Peano\n\nA sequence of 729 base patterns.\n  I(0)J(0)R(0)R(270)  I(180)L(180)L(270)J(0)\n  I(0)J(0)I(0)L(0)    L(90)J(180)R(180)R(90)\n  .... other 88 lines ....\n  I(0)J(0)R(0)R(270)  I(180)L(180)L(270)J(0)\n  I(0)\n\nSeed: A sequence of 1 base pattern.\n  I(0)\n```\n\nMeander curve (3x3):\n\n```\n> sfc_3x3_meander(\"I\", \"111\")\nAn sfc_3x3_meander object.\n  Increase mode: 3 x 3\n  Level: 3\n  Expansion rule: 3x3 Meander\n\nA sequence of 729 base patterns.\n  R(0)I(270)L(270)I(0)  L(0)L(90)R(180)R(90)\n  I(0)R(0)I(270)L(270)  I(0)L(0)L(90)R(180)\n  .... other 88 lines ....\n  R(0)I(270)L(270)I(0)  L(0)L(90)R(180)R(90)\n  I(0)\n\nSeed: A sequence of 1 base pattern.\n  I(0)\n```\n\nIt also allows using a sequence as the seed:\n\n\n```\np = sfc_seed(\"LLLILILIILIILIIILIIILIIII\")\np2 = sfc_2x2(p, \"1111\")\nplot(p2)\n```\n\n<img src=\"https://github.com/user-attachments/assets/f1144f7f-282f-4988-aafd-9f712dd3ed2d\" width=500 />\n\nFor more comprehensive introduction of the theory and the package, please refer to the vignettes.\n\n## License\n\nMIT @ Zuguang Gu\n\n",
                "dependencies": "Package: sfcurve\nType: Package\nTitle: 2x2, 3x3 and Nxn Space-Filling Curves\nVersion: 1.0.1\nDate: 2024-09-10\nAuthors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"),\n                  comment = c('ORCID'=\"0000-0002-7395-8709\"))\nDepends: R (>= 4.0.0)\nImports: grid, Rcpp, methods, colorRamp2\nSuggests: rmarkdown, knitr, rgl, testthat, ComplexHeatmap, igraph, digest\nVignetteBuilder: knitr\nDescription: Implementation of all possible forms of 2x2 and 3x3 space-filling curves, \n    i.e., the generalized forms of the Hilbert curve <https://en.wikipedia.org/wiki/Hilbert_curve>, \n    the Peano curve <https://en.wikipedia.org/wiki/Peano_curve> and the Peano curve in the \n    meander type (Figure 5 in <https://eudml.org/doc/141086>). It can generates nxn curves expanded from\n    any specific level-1 units. It also implements the H-curve and the three-dimensional Hilbert curve.\nURL: https://github.com/jokergoo/sfcurve, https://jokergoo.github.io/sfcurve/\nLicense: MIT + file LICENSE\nLinkingTo: Rcpp\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.3.1\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ai4hpc",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/CoE-RAISE/FZJ/ai4hpc/ai4hpc",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/aiida-kkr",
            "repo_link": "https://github.com/JuDFTteam/aiida-kkr",
            "content": {
                "codemeta": "",
                "readme": "[![aiida-core](https://img.shields.io/badge/AiiDA->=v2.0.0,<3.0.0-1f425f.svg?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAACMAAAAhCAYAAABTERJSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAFhgAABYYBG6Yz4AAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAAUbSURBVFiFzZhrbFRVEMd%2Fc%2B5uu6UUbIFC%2FUAUVEQCLbQJBIiBDyiImJiIhmohYNCkqJAQxASLF8tDgYRHBLXRhIcKNtFEhVDgAxBJqgmVh4JEKg3EIn2QYqBlt917xg%2BFss%2ByaDHOtzsz5z%2B%2FuZl7ztmF%2F5HJvxVQN6cPYX8%2FPLnOmsvNAvqfwuib%2FbNIk9cQeQnLcKRL5xLIV%2Fic9eJeunjPYbRs4FjQSpTB3aS1IpRKeeOOewajy%2FKKEO8Q0DuVdKy8IqsbPulxGHUfCBBu%2BwUYGuFuBTK7wQnht6PEbf4tlRomVRjCbXNjQEB0AyrFQOL5ENIJm7dTLZE6DPJCnEtFZVXDLny%2B4Sjv0PmmYu1ZdUek9RiMgoDmJ8V0L7XJqsZ3UW8YsBOwEeHeeFce7jEYXBy0m9m4BbXqSj2%2Bxnkg26MCVrN6DEZcwggtd8pTFx%2Fh3B9B50YLaFOPwXQKUt0tBLegtSomfBlfY13PwijbEnhztGzgJsK5h9W9qeWwBqjvyhB2iBs1Qz0AU974DciRGO8CVN8AJhAeMAdA3KbrKEtvxhsI%2B9emWiJlGBEU680Cfk%2BSsVqXZvcFYGXjF8ABVJ%2BTNfVXehyms1zzn1gmIOxLEB6E31%2FWBe5rnCarmo7elf7dJEeaLh80GasliI5F6Q9cAz1GY1OJVNDxTzQTw7iY%2FHEZRQY7xqJ9RU2LFe%2FYqakdP911ha0XhjjiTVAkDwgatWfCGeYocx8M3glG8g8EXhSrLrHnEFJ5Ymow%2FkhIYv6ttYUW1iFmEqqxdVoUs9FmsDYSqmtmJh3Cl1%2BVtl2s7owDUdocR5bceiyoSivGTT5vzpbzL1uoBpmcAAQgW7ArnKD9ng9rc%2BNgrobSNwpSkkhcRN%2BvmXLjIsDovYHHEfmsYFygPAnIDEQrQPzJYCOaLHLUfIt7Oq0LJn9fxkSgNCb1qEIQ5UKgT%2Fs6gJmVOOroJhQBXVqw118QtWLdyUxEP45sUpSzqP7RDdFYMyB9UReMiF1MzPwoUqHt8hjGFFeP5wZAbZ%2F0%2BcAtAAcji6LeSq%2FMYiAvSsdw3GtrfVSVFUBbIhwRWYR7yOcr%2FBi%2FB1MSJZ16JlgH1AGM3EO2QnmMyrSbTSiACgFBv4yCUapZkt9qwWVL7aeOyHvArJjm8%2Fz9BhdI4XcZgz2%2FvRALosjsk1ODOyMcJn9%2FYI6IrkS5vxMGdUwou2YKfyVqJpn5t9aNs3gbQMbdbkxnGdsr4bTHm2AxWo9yNZK4PXR3uzhAh%2BM0AZejnCrGdy0UvJxl0oMKgWSLR%2B1LH2aE9ViejiFs%2BXn6bTjng3MlIhJ1I1TkuLdg6OcAbD7Xx%2Bc3y9TrWAiSHqVkbZ2v9ilCo6s4AjwZCzFyD9mOL305nV9aonvsQeT2L0gVk4OwOJqXXVRW7naaxswDKVdlYLyMXAnntteYmws2xcVVZzq%2BtHPAooQggmJkc6TLSusOiL4RKgwzzYU1iFQgiUBA1H7E8yPau%2BZl9P7AblVNebtHqTgxLfRqrNvZWjsHZFuqMqKcDWdlFjF7UGvX8Jn24DyEAykJwNcdg0OvJ4p5pQ9tV6SMlP4A0PNh8aYze1ArROyUNTNouy8tNF3Rt0CSXb6bRFl4%2FIfQzNMjaE9WwpYOWQnOdEF%2BTdJNO0iFh7%2BI0kfORzQZb6P2kymS9oTxzBiM9rUqLWr1WE5G6ODhycQd%2FUnNVeMbcH68hYkGycNoUNWc8fxaxfwhDbHpfwM5oeTY7rUX8QAAAABJRU5ErkJggg%3D%3D)](https://www.aiida.net/)\n[![Documentation Status](https://readthedocs.org/projects/aiida-kkr/badge/?version=latest)](https://aiida-kkr.readthedocs.io/en/latest/?badge=latest)\n[![Build status](https://github.com/JuDFTteam/aiida-kkr/actions/workflows/ci.yml/badge.svg)](https://github.com/JuDFTteam/aiida-kkr/actions)\n[![codecov](https://codecov.io/gh/JuDFTteam/aiida-kkr/branch/develop/graph/badge.svg)](https://codecov.io/gh/JuDFTteam/aiida-kkr)\n[![MIT license](http://img.shields.io/badge/license-MIT-brightgreen.svg)](http://opensource.org/licenses/MIT)\n[![GitHub version](https://badge.fury.io/gh/JuDFTteam%2Faiida-kkr.svg)](https://badge.fury.io/gh/JuDFTteam%2Faiida-kkr)\n[![PyPI version](https://badge.fury.io/py/aiida-kkr.svg)](https://badge.fury.io/py/aiida-kkr)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3628250.svg)](https://doi.org/10.5281/zenodo.3628250)\n\n\n\n# aiida-kkr\n\n[AiiDA](https://aiida.net) plugin for the [Jülich KKR codes](https://jukkr.fz-juelich.de) plus workflows and utility.\n\n## Features\n\n* KKR calculations for bulk and interfaces\n* treatment of alloys using VCA or CPA\n* self-consistency, DOS and bandstructure calculations\n* extraction of magnetic exchange coupling parameters (*J_ij*, *D_ij*)\n* impurity embedding solving the Dyson equation\n\n\n## How to cite\n\nIf you use this plugin please cite:\n> [Rüßmann, P., Bertoldo, F. & Blügel, S. The AiiDA-KKR plugin and its application to high-throughput impurity embedding into a topological insulator. *npj Comput Mater* **7**, 13 (2021). https://doi.org/10.1038/s41524-020-00482-5](https://doi.org/10.1038/s41524-020-00482-5)\n\nThe ArXiv preprint can be found here:\n> [Philipp Rüßmann, Fabian Bertoldo and Stefan Blügel, *The AiiDA-KKR plugin and its application to high-throughput impurity embedding into a topological insulator*, arXiv:2003.08315 [cond-mat.mtrl-sci] (2020)](https://arxiv.org/abs/2003.08315)\n\n\n# Installation\n\n```shell\n$ pip install aiida-kkr  # install latest version of aiida-kkr (published on pypi.org)\n$ reentry scan -r aiida  # update entry points, needed in order to find kkr.* entrypoints in aiida\n\n# setupt aiida if this was not done already:\n$ verdi quicksetup  # better to set up a new profile\n$ verdi calculation plugins  # should now show kkr.* entrypoints\n```\n\nTo install the developer version download the repository and install the downloaded version (see `setup.json` for a list of optional packages that are installed with the extras given in `[]`)\n\n```shell\n$ git clone https://github.com/JuDFTteam/aiida-kkr.git\n$ pip install -e aiida-kkr[testing,devtools,docs]\n$ reentry scan -r aiida\n```\n\n## Remarks about dependencies and extras\n\n- The `aiida-kkr` plugin uses the `ase` and `pymatgen` packages for structure conversions.\n- For `aiida-core>=1.5,<1.6` make sure to use the requirements specified in `requirements_aiida-core_1.5.txt` (use `pip install -r requirements_aiida-core_1.5.txt aiida-kkr` for the installation to overwrite the aiida-core dependency).\n- Other extras that can be optionally installed with `aiida-kkr` are\n  * `pre-commit` which installes the pre-commit hooks that allow style (`yapf`) and static code checking (`pylint`)\n  * `testing` which installs `pytest` and all extension used in the tests\n  * `docs` which installs `Sphinx` to build the documentation\n  * `devtools` which installs tools that might be helpful during development\n\n\n# Usage and Documentation\n\n* see http://aiida-kkr.readthedocs.io for user's and developer's guides and API reference\n* check out http://judft.de and https://jukkr.fz-juelich.de for information of the KKR codes used by the plugin\n\n# Contributing\n\nThank you for your interest in contributing to aiida-kkr.\nCheck out our [contributing guide](CONTRIBUTING.md) for some information.\n\n# Releasing new versions\n\nTo create a new release follow these steps:\n- finish your development and merge it into the `develop` branch\n- update documentation\n- update / fix tests\n- bump version numbers (in files `aiida_kkr/__init__.py`, `pyproject.toml`, `.bumpversion.cfg`)\n- merge changes from `develop` back into `master` and create a tag for the new version number (this triggers publication to pypi)\n\n",
                "dependencies": "[build-system]\nrequires = [ \"setuptools>=61.2\",]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"aiida-kkr\"\nversion = \"2.3.1\"\ndescription = \"AiiDA plugin for the JuKKR codes\"\nclassifiers = [\n    \"License :: OSI Approved :: MIT License\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.7\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Development Status :: 4 - Beta\",\n    \"Environment :: Plugins\",\n    \"Intended Audience :: Science/Research\",\n    \"Topic :: Scientific/Engineering :: Physics\",\n    \"Natural Language :: English\",\n    \"Framework :: AiiDA\",\n]\ndependencies = [\n    \"aiida-core >= 2.0.0,<3.0.0\",\n    \"masci-tools >= 0.4.8.dev5,<1.0.0\",\n    \"seekpath >= 1.9.2\",\n    \"ase\",\n    \"pymatgen\",\n]\nlicense = {file = \"LICENSE.txt\"}\nkeywords = [\n    \"material science\",\n    \"aiida\",\n    \"dft\",\n    \"all-electron\",\n    \"kkr\",\n]\n\n[[project.authors]]\nname = \"Philipp Ruessmann\"\nemail = \"p.ruessmann@fz-juelich.de\"\n\n[[project.authors]]\nname = \"Jens Broeder\"\nemail = \"j.broeder@fz-juelich.de\"\n\n[[project.authors]]\nname = \"Fabian Bertoldo\"\nemail = \"f.bertoldo@fz-juelich.de\"\n\n[project.readme]\nfile = \"README.md\"\ncontent-type = \"text/markdown\"\n\n[project.urls]\nHomepage = \"https://github.com/JuDFTteam/aiida-kkr\"\nDownload = \"https://github.com/JuDFTteam/aiida-kkr\"\nDocumentation = \"https://aiida-kkr.readthedocs.io\"\n\n[project.optional-dependencies]\npre-commit = [\n    \"pre-commit >= 4.0.1\",\n    \"yapf >= 0.43.0\",\n    \"pylint == 1.9.4; python_version < '3.0'\",\n    \"pylint >= 3.3.1; python_version >= '3.0'\",\n]\ntesting = [\n    \"pgtest >= 1.3.0\",\n    \"pytest-xdist\",\n    \"pytest-cov >= 2.5.0\",\n    \"pytest-mpl >= 0.10\",\n    \"pytest-timeout >= 1.3.3\",\n    \"pytest-regressions >= 1.0\",\n    \"MarkupSafe < 3.1.0\",\n    \"aiida-test-cache\"\n]\ndocs = [\n    \"Sphinx >= 1.8.2\",\n    \"sphinx_rtd_theme >= 0.4.2\",\n]\ndevtools = [\n    \"bump2version >= 0.5.10\",\n]\nwidgets = [\n    \"ase_notebook\",\n]\n\n[project.scripts]\naiida-kkr = \"aiida_kkr.cmdline:cmd_root\"\n\n[tool.setuptools]\ninclude-package-data = false\n\n[project.entry-points.\"aiida.calculations\"]\n\"kkr.voro\" = \"aiida_kkr.calculations.voro:VoronoiCalculation\"\n\"kkr.kkr\" = \"aiida_kkr.calculations.kkr:KkrCalculation\"\n\"kkr.kkrimp\" = \"aiida_kkr.calculations.kkrimp:KkrimpCalculation\"\n\"kkr.kkrnano\" = \"aiida_kkr.calculations.kkrnano:KKRnanoCalculation\"\n\"kkr.kkrimporter\" = \"aiida_kkr.calculations.kkrimporter:KkrImporterCalculation\"\n\n[project.entry-points.\"aiida.parsers\"]\n\"kkr.voroparser\" = \"aiida_kkr.parsers.voro:VoronoiParser\"\n\"kkr.kkrparser\" = \"aiida_kkr.parsers.kkr:KkrParser\"\n\"kkr.kkrimpparser\" = \"aiida_kkr.parsers.kkrimp:KkrimpParser\"\n\"kkr.kkrnanoparser\" = \"aiida_kkr.parsers.kkrnano:KKRnanoParser\"\n\"kkr.kkrimporterparser\" = \"aiida_kkr.parsers.kkrimporter:KkrImporterParser\"\n\n[project.entry-points.\"aiida.data\"]\n\"kkr.strucwithpot\" = \"aiida_kkr.data.strucwithpot:StrucWithPotData\"\n\n[project.entry-points.\"aiida.workflows\"]\n\"kkr.scf\" = \"aiida_kkr.workflows.kkr_scf:kkr_scf_wc\"\n\"kkr.dos\" = \"aiida_kkr.workflows.dos:kkr_dos_wc\"\n\"kkr.bs\" = \"aiida_kkr.workflows.bs:kkr_bs_wc\"\n\"kkr.eos\" = \"aiida_kkr.workflows.eos:kkr_eos_wc\"\n\"kkr.startpot\" = \"aiida_kkr.workflows.voro_start:kkr_startpot_wc\"\n\"kkr.gf_writeout\" = \"aiida_kkr.workflows.gf_writeout:kkr_flex_wc\"\n\"kkr.imp\" = \"aiida_kkr.workflows.kkr_imp:kkr_imp_wc\"\n\"kkr.imp_sub\" = \"aiida_kkr.workflows.kkr_imp_sub:kkr_imp_sub_wc\"\n\"kkr.imp_dos\" = \"aiida_kkr.workflows.kkr_imp_dos:kkr_imp_dos_wc\"\n\"kkr.imp_BdG\" = \"aiida_kkr.workflows.imp_BdG:kkrimp_BdG_wc\"\n\"kkr.decimation\" = \"aiida_kkr.workflows._decimation:kkr_decimation_wc\"\n\"kkr.jij\" = \"aiida_kkr.workflows.jijs:kkr_jij_wc\"\n\"kkr.combine_imp\" = \"aiida_kkr.workflows._combine_imps:combine_imps_wc\"\n\"kkr.STM\" = \"aiida_kkr.workflows.kkr_STM:kkr_STM_wc\"\n\n[tool.setuptools.packages.find]\nnamespaces = false\ninclude = [\"aiida_kkr*\"]\nexclude = [\"docs*\", \"tests*\"]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/aiida-spirit",
            "repo_link": "https://github.com/JuDFTteam/aiida-spirit",
            "content": {
                "codemeta": "",
                "readme": "[![Build Status](https://github.com/JuDFTteam/aiida-spirit/workflows/ci/badge.svg?branch=master)](https://github.com/JuDFTteam/aiida-spirit/actions)\n[![Coverage Status](https://codecov.io/gh/JuDFTteam/aiida-spirit/branch/main/graph/badge.svg?token=F7ISM4558S)](https://codecov.io/gh/JuDFTteam/aiida-spirit)\n[![Docs status](https://readthedocs.org/projects/aiida-spirit/badge)](http://aiida-spirit.readthedocs.io/)\n[![PyPI version](https://badge.fury.io/py/aiida-spirit.svg)](https://badge.fury.io/py/aiida-spirit)\n[![DOI](https://zenodo.org/badge/364820045.svg)](https://zenodo.org/badge/latestdoi/364820045)\n\n# aiida-spirit\n\nAiiDA plugin for the [spirit code](http://spirit-code.github.io/)\n\n\n## Installation\n\n```shell\npip install aiida-spirit # install aiida-spirit from pypi\nverdi quicksetup  # better to set up a new profile\nverdi plugin list aiida.calculations  # should now show your calclulation plugins\n```\n\n\n## Usage\n\nHere goes a complete example of how to submit a test calculation using this plugin.\n\nA quick demo of how to submit a calculation (the spirit python API needs to be installed for this to work: `pip install spirit`):\n```shell\nverdi daemon start     # make sure the daemon is running\ncd examples\n./example_LLG.py       # run test calculation\nverdi process list -a  # check record of calculation\n```\n\n## Development\n\n```shell\ngit clone https://github.com/JuDFTteam/aiida-spirit .\ncd aiida-spirit\npip install -e .[pre-commit,testing]  # install extra dependencies\npre-commit install  # install pre-commit hooks\npytest -v  # discover and run all tests\n```\n\nNote that `pytest -v` will create a test database and profile which requires to find the `pg_ctl` command.\nIf `pg_ctl` is not found you need to nake sure that postgres is installed and then add the localtion of\n`pg_ctl` to the `PATH`:\n```\n# add postgres path for pg_ctl to PATH\n# this is an example for Postgres 9.6 installed on a mac\nPATH=\"/Applications/Postgres.app/Contents/Versions/9.6/bin/:$PATH\"\nexport PATH\n```\n\n## Citation\n\nIf you use AiiDA-Spirit please cite the method paper\n - P. Rüßmann, J. Ribas Sobreviela, M. Sallermann, M. Hoffmann, F. Rhiem, and S. Blügel, *The AiiDA-Spirit Plugin for Automated Spin-Dynamics Simulations and Multi-Scale Modeling Based on First-Principles Calculations*, Front. Mater. **9**, 825043 (2022). [doi: 10.3389/fmats.2022.825043](https://doi.org/10.3389/fmats.2022.825043),\n\n and the latest code release\n - P. Rüßmann, J. Ribas Sobreviela, M. Sallermann, M. Hoffmann, F. Rhiem, and S. Blügel. JuDFTteam/aiida-spirit. Zenodo. [doi: 10.5281/zenodo.8070770](https://doi.org/10.5281/zenodo.8070770).\n\n## License\n\nThe AiiDA-Spirit code is under the [MIT license](LICENSE).\n\n## Contact\n\np.ruessmann@fz-juelich.de\n\n",
                "dependencies": "[tool.pylint.format]\nmax-line-length = 125\n\n[tool.pylint.messages_control]\ndisable = [\n    \"too-many-ancestors\",\n    \"invalid-name\",\n]\n\n[tool.pytest.ini_options]\npython_files = \"test_*.py example_*.py\"\nfilterwarnings = [\n    \"ignore::DeprecationWarning:aiida:\",\n    \"ignore::DeprecationWarning:plumpy:\",\n    \"ignore::DeprecationWarning:django:\",\n    \"ignore::DeprecationWarning:yaml:\",\n]\n\n# -*- coding: utf-8 -*-\n\"\"\"Setup script for aiida-spirit\"\"\"\nimport json\nfrom setuptools import setup, find_packages\n\nif __name__ == '__main__':\n    # Provide static information in setup.json\n    # such that it can be discovered automatically\n    with open('setup.json', 'r') as info:\n        kwargs = json.load(info)\n    setup(\n        packages=find_packages(include=['aiida_spirit', 'aiida_spirit.*']),\n        # this doesn't work when placed in setup.json (something to do with str type)\n        package_data={\n            '': ['*'],\n        },\n        long_description=open('README.md').read(),\n        long_description_content_type='text/markdown',\n        **kwargs)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/album",
            "repo_link": "https://gitlab.com/album-app/album",
            "content": {
                "codemeta": "",
                "readme": "# album\n\nIntroduction: [https://album.solutions/](https://album.solutions/)\n\nDocumentation: [https://docs.album.solutions/](https://docs.album.solutions/)\n\n## Citation\n\nAlbrecht, J.P.\\*, Schmidt, D.\\*, and Harrington, K., 2021. Album: a\nframework for scientific data processing with software solutions of\nheterogeneous tools. arXiv preprint arXiv:2110.00601.  \nhttps://arxiv.org/abs/2110.00601\n\n## Developers\n\n- Kyle Harrington, Max Delbrueck Center for Molecular Medicine in the\nHelmholtz Association\n- Jan Philipp Albrecht, Max Delbrueck Center for Molecular Medicine in\n  the Helmholtz Association\n- Deborah Schmidt, Max Delbrueck Center for Molecular Medicine in\n  the Helmholtz Association\n\n",
                "dependencies": "[build-system]\nrequires = [\n    \"setuptools>=42\",\n    \"wheel\"\n]\nbuild-backend = \"setuptools.build_meta\"\nfrom setuptools import setup\n\nsetup()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/alice2modelica",
            "repo_link": "https://jugit.fz-juelich.de/iek-10/public/tools/alice2modelica",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/allpix-squared",
            "repo_link": "https://gitlab.cern.ch/allpix-squared/allpix-squared",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/alpaca",
            "repo_link": "https://github.com/INM-6/alpaca",
            "content": {
                "codemeta": "",
                "readme": "# alpaca\n\n## Automated Lightweight Provenance Capture\n\n[![tests](https://github.com/INM-6/alpaca/actions/workflows/CI.yml/badge.svg)](https://github.com/INM-6/alpaca/actions/workflows/CI.yml)\n[![Documentation Status](https://readthedocs.org/projects/alpaca-prov/badge/?version=latest)](https://alpaca-prov.readthedocs.io/en/latest/?badge=latest)\n\nAlpaca is a Python package for the capture of provenance information during the\nexecution of Python scripts that process data.\n\nAlpaca provides a simple API for recording the details of the functions being\nexecuted, the data flow, and a description of parameters used.\nThis is accomplished with minimal code instrumentation and user intervention.\n\nProvenance information is structured and serialized according to a model\nbased on the [W3C PROV format](https://www.w3.org/TR/prov-overview).\n\n\n## Table of contents\n  - [Prerequisites](#prerequisites)\n  - [Installation](#installation)\n  - [Documentation](#documentation)\n  - [How to run](#how-to-run)\n  - [Collaborators](#collaborators)\n  - [How to contribute](#how-to-contribute)\n  - [Get support](#get-support)\n  - [Acknowledgments](#acknowledgments)\n  - [License](#license)\n  - [Copyright](#copyright)\n\n\n## Prerequisites\n\n### Requirements\n\nAlpaca requires Python 3.8 or higher and the packages specified in \n[requirements.txt](requirements/requirements.txt).\n\n\n## Installation\n\nUse the package manager [pip](https://pip.pypa.io/en/stable/) to install Alpaca.\n\nPackage on [pypi](https://pypi.org/)\n```bash\npip install alpaca-prov\n```\n\nMore detailed instructions on how to setup conda environments and additional\ninstall options can be checked in the [Installation](doc/install.rst) page.\n\n\n## Documentation\n\nSee [https://alpaca-prov.readthedocs.io/en/latest/](https://alpaca-prov.readthedocs.io/en/latest/).\n\n\n## How to run\n\nExamples showing how to use Alpaca can be found in the [examples](examples/)\nfolder. Detailed instructions on how to set up and run are \n[here](doc/examples.rst).\n\n\n## Colaborators\n\nAll the contributors to the development of Alpaca can be found in the \n[Authors and contributors](doc/authors.rst) page.\n\n\n## How to contribute\n\nIf you want to suggest new features, changes, or make a contribution, please\nfirst open an issue on the [project page on GitHub](https://github.com/INM-6/alpaca/issues)\nto discuss your idea.\n\nPull requests are welcome. Any contribution should also\nbe covered by appropriate unit tests in the [tests](alpaca/test) folder.\n\n\n## Get support\n\nIf you experience any issue or wish to report a bug, please open an issue on\nthe [project page on GitHub](https://github.com/INM-6/alpaca/issues).\n\n\n## Acknowledgments\n\nSee [acknowledgments](doc/acknowledgments.rst).\n\n\n## License\n \nBSD 3-Clause License, see [LICENSE.txt](LICENSE.txt) for details.\n\n\n## Copyright\n\n:copyright: 2022-2023, Forschungszentrum Jülich GmbH, INM-6, IAS-6. All rights reserved.\n",
                "dependencies": "import os\n\nfrom setuptools import setup\n\nwith open(os.path.join(os.path.dirname(__file__),\n                       \"alpaca\", \"VERSION\")) as version_file:\n    version = version_file.read().strip()\n\nwith open(\"README.md\") as f:\n    long_description = f.read()\n\nwith open('requirements/requirements.txt') as fp:\n    install_requires = fp.read()\n\n\nsetup(\n    name=\"alpaca-prov\",\n    version=version,\n    packages=['alpaca', 'alpaca.utils', 'alpaca.serialization',\n              'alpaca.ontology', 'alpaca.code_analysis'],\n    include_package_data=True,\n    install_requires=install_requires,\n    author=\"Alpaca authors and contributors\",\n    author_email=\"\",\n    description=\"Alpaca is a Python package for the capture of provenance \"\n                \"information during the execution of Python scripts that \"\n                \"process data.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    license=\"BSD\",\n    url='https://github.com/INM-6/alpaca',\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: BSD License',\n        'Natural Language :: English',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python :: 3',\n        'Topic :: Scientific/Engineering']\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/alpaka",
            "repo_link": "https://github.com/alpaka-group/alpaka",
            "content": {
                "codemeta": "",
                "readme": "**alpaka** - Abstraction Library for Parallel Kernel Acceleration\n=================================================================\n\n\n[![Continuous Integration](https://github.com/alpaka-group/alpaka/workflows/Continuous%20Integration/badge.svg)](https://github.com/alpaka-group/alpaka/actions?query=workflow%3A%22Continuous+Integration%22)\n[![Documentation Status](https://readthedocs.org/projects/alpaka/badge/?version=latest)](https://alpaka.readthedocs.io)\n[![Doxygen](https://img.shields.io/badge/API-Doxygen-blue.svg)](https://alpaka-group.github.io/alpaka)\n[![Language](https://img.shields.io/badge/language-C%2B%2B17-orange.svg)](https://isocpp.org/)\n[![Platforms](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20mac-lightgrey.svg)](https://github.com/alpaka-group/alpaka)\n[![License](https://img.shields.io/badge/license-MPL--2.0-blue.svg)](https://www.mozilla.org/en-US/MPL/2.0/)\n\n![alpaka](docs/logo/alpaka_401x135.png)\n\nThe **alpaka** library is a header-only C++20 abstraction library for accelerator development.\n\nIts aim is to provide performance portability across accelerators through the abstraction (not hiding!) of the underlying levels of parallelism.\n\nIt is platform independent and supports the concurrent and cooperative use of multiple devices such as the hosts CPU (x86, ARM, RISC-V and Power 8+) and  GPU accelerators from different vendors (NVIDIA, AMD and Intel).\nA multitude of accelerator back-end variants using CUDA, HIP, SYCL, OpenMP 2.0+, std::thread and also serial execution is provided and can be selected depending on the device.\nOnly one implementation of the user kernel is required by representing them as function objects with a special interface.\nThere is no need to write special CUDA, HIP, SYCL, OpenMP or custom threading code.\nAccelerator back-ends can be mixed and synchronized via compute device queue.\nThe decision which accelerator back-end executes which kernel can be made at runtime.\n\nThe abstraction used is very similar to the CUDA grid-blocks-threads domain decomposition strategy.\nAlgorithms that should be parallelized have to be divided into a multi-dimensional grid consisting of small uniform work items.\nThese functions are called kernels and are executed in parallel threads.\nThe threads in the grid are organized in blocks.\nAll threads in a block are executed in parallel and can interact via fast shared memory and low level synchronization methods.\nBlocks are executed independently and can not interact in any way.\nThe block execution order is unspecified and depends on the accelerator in use.\nBy using this abstraction the execution can be optimally adapted to the available hardware.\n\n\nSoftware License\n----------------\n\n**alpaka** is licensed under **MPL-2.0**.\n\n\nDocumentation\n-------------\n\nThe alpaka documentation can be found in the [online manual](https://alpaka.readthedocs.io).\nThe documentation files in [`.rst` (reStructuredText)](https://www.sphinx-doc.org/en/stable/rest.html) format are located in the `docs` subfolder of this repository.\nThe [source code documentation](https://alpaka-group.github.io/alpaka/) is generated with [doxygen](http://www.doxygen.org).\n\n\nAccelerator Back-ends\n---------------------\n\n| Accelerator Back-end   | Lib/API                                                 | Devices                    | Execution strategy grid-blocks     | Execution strategy block-threads     |\n|------------------------|---------------------------------------------------------|----------------------------|------------------------------------|--------------------------------------|\n| Serial                 | n/a                                                     | Host CPU (single core)     | sequential                         | sequential (only 1 thread per block) |\n| OpenMP 2.0+ blocks     | OpenMP 2.0+                                             | Host CPU (multi core)      | parallel (preemptive multitasking) | sequential (only 1 thread per block) |\n| OpenMP 2.0+ threads    | OpenMP 2.0+                                             | Host CPU (multi core)      | sequential                         | parallel (preemptive multitasking)   |\n| std::thread            | std::thread                                             | Host CPU (multi core)      | sequential                         | parallel (preemptive multitasking)   |\n| TBB                    | TBB 2.2+                                                | Host CPU (multi core)      | parallel (preemptive multitasking) | sequential (only 1 thread per block) |\n| CUDA                   | CUDA 12.0+                                              | NVIDIA GPUs                | parallel (undefined)               | parallel (lock-step within warps)    |\n| HIP(clang)             | [HIP 6.0+](https://github.com/ROCm-Developer-Tools/HIP) | AMD GPUs                   | parallel (undefined)               | parallel (lock-step within warps)    |\n| SYCL(oneAPI)           | oneAPI 2024.2+                                          | CPUs, Intel GPUs and FPGAs | parallel (undefined)               | parallel (lock-step within warps)    |\n\n\nSupported Compilers\n-------------------\n\nThis library uses C++20 (or newer when available).\n\n| Accelerator Back-end | gcc 10.4 / 11.1 (Linux)        | gcc 12.3 (Linux)                      | gcc 13.1 (Linux)                      | clang 10/11 (Linux)            | clang 12 (Linux)               | clang 13 (Linux)               | clang 14 (Linux)               | clang 15 (Linux)               | clang 16 (Linux)               | clang 17 (Linux)                      | clang 18 (Linux)                      | clang 19 (Linux)   | icpx 2025.0 (Linux)     | Xcode 15.4 / 16.1 (macOS) | Visual Studio 2022 (Windows) |\n|----------------------|--------------------------------|---------------------------------------|---------------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|---------------------------------------|---------------------------------------|--------------------|-------------------------|---------------------------|------------------------------|\n| Serial               | :white_check_mark:             | :white_check_mark:                    | :white_check_mark:                    | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:                    | :white_check_mark:                    | :white_check_mark: | :white_check_mark:      | :white_check_mark:        | :white_check_mark:           |\n| OpenMP 2.0+ blocks   | :white_check_mark:             | :white_check_mark:                    | :white_check_mark:                    | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:                    | :white_check_mark:                    | :white_check_mark: | :white_check_mark: [^1] | :white_check_mark:        | :white_check_mark:           |\n| OpenMP 2.0+ threads  | :white_check_mark:             | :white_check_mark:                    | :white_check_mark:                    | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:                    | :white_check_mark:                    | :white_check_mark: | :white_check_mark: [^1] | :white_check_mark:        | :white_check_mark:           |\n| std::thread          | :white_check_mark:             | :white_check_mark:                    | :white_check_mark:                    | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:                    | :white_check_mark:                    | :white_check_mark: | :white_check_mark:      | :white_check_mark:        | :white_check_mark:           |\n| TBB                  | :white_check_mark:             | :white_check_mark:                    | :white_check_mark:                    | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:             | :white_check_mark:                    | :white_check_mark:                    | :white_check_mark: | :white_check_mark:      | :white_check_mark:        | :white_check_mark:           |\n| CUDA (nvcc)          | :white_check_mark: (CUDA 12.0) | :white_check_mark: (CUDA 12.0 - 12.5) | :white_check_mark: (CUDA 12.4 - 12.5) | :white_check_mark: (CUDA 12.0) | :white_check_mark: (CUDA 12.0) | :white_check_mark: (CUDA 12.0) | :white_check_mark: (CUDA 12.0) | :white_check_mark: (CUDA 12.2) | :white_check_mark: (CUDA 12.3) | :white_check_mark: (CUDA 12.4 - 12.5) | :white_check_mark: (CUDA 12.4 - 12.5) | :x:                | :x:                     | -                         | :x:                          |\n| CUDA (clang)         | -                              | -                                     | -                                     | :x:                            | :x:                            | :x:                            | :x:                            | :x:                            | :x:                            | :x:                                   | :x:                                   | :x:                | :x:                     | -                         | -                            |\n| HIP (clang)          | -                              | -                                     | -                                     | :x:                            | :x:                            | :x:                            | :x:                            | :x:                            | :x:                            | :white_check_mark: (HIP 6.0 - 6.1)    | :white_check_mark: (HIP 6.2)          | :x:                | :x:                     | -                         | -                            |\n| SYCL                 | :x:                            | :x:                                   | :x:                                   | :x:                            | :x:                            | :x:                            | :x:                            | :x:                            | :x:                            | :x:                                   | :x:                                   | :x:                | :white_check_mark: [^2] | -                         | :x:                          |\n\nOther compilers or combinations marked with :x: in the table above may work but are not tested in CI and are therefore not explicitly supported.\n\n[^1]: Due to an [LLVM bug](https://github.com/llvm/llvm-project/issues/58491) in debug mode only release builds are supported.\n[^2]: Currently, the unit tests are compiled but not executed.\n\nDependencies\n------------\n\n[Boost](https://boost.org/) 1.74.0+ is the only mandatory external dependency.\nThe **alpaka** library itself just requires header-only libraries.\nHowever some of the accelerator back-end implementations require different boost libraries to be built.\n\nWhen an accelerator back-end using *CUDA* is enabled, version *11.2* (with nvcc as CUDA compiler) or version *11.2* (with clang as CUDA compiler) of the *CUDA SDK* is the minimum requirement.\n*NOTE*: When using clang as a native *CUDA* compiler, the *CUDA accelerator back-end* can not be enabled together with any *OpenMP accelerator back-end* because this combination is currently unsupported.\n*NOTE*: Separable compilation is disabled by default and can be enabled via the CMake flag `CMAKE_CUDA_SEPARABLE_COMPILATION`.\n\nWhen an accelerator back-end using *OpenMP* is enabled, the compiler and the platform have to support the corresponding minimum *OpenMP* version.\n\nWhen an accelerator back-end using *TBB* is enabled, the compiler and the platform have to support the corresponding minimum *TBB* version.\n\n\nUsage\n-----\n\nThe library is header only so nothing has to be built.\nCMake 3.22+ is required to provide the correct defines and include paths.\nJust call `alpaka_add_executable` instead of `add_executable` and the difficulties of the CUDA nvcc compiler in handling `.cu` and `.cpp` files are automatically taken care of.\nSource files do not need any special file ending.\nExamples of how to utilize alpaka within CMake can be found in the `example` folder.\n\nThe whole alpaka library can be included with: `#include <alpaka/alpaka.hpp>`\nCode that is not intended to be utilized by the user is hidden in the `detail` namespace.\n\nFurthermore, for a CUDA-like experience when adopting alpaka we provide the library [*cupla*](https://github.com/alpaka-group/cupla).\nIt enables a simple and straightforward way of porting existing CUDA applications to alpaka and thus to a variety of accelerators.\n\n### Single header\n\nThe CI creates a single-header version of alpaka on each commit,\nwhich you can find on the [single-header branch](https://github.com/alpaka-group/alpaka/tree/single-header).\n\nThis is especially useful, if you would like to play with alpaka on [Compiler explorer](https://godbolt.org/z/hzPnhnna9).\nJust include alpaka like\n```c++\n#include <https://raw.githubusercontent.com/alpaka-group/alpaka/single-header/include/alpaka/alpaka.hpp>\n```\nand enable the desired backend on the compiler's command line using the corresponding macro, e.g. via `-DALPAKA_ACC_CPU_B_SEQ_T_SEQ_ENABLED`.\n\nIntroduction\n------------\n\nFor a quick introduction, feel free to playback the recording of our presentation at\n[GTC 2016](https://www.nvidia.com/gtc/):\n\n - E. Zenker, R. Widera, G. Juckeland et al.,\n   *Porting the Plasma Simulation PIConGPU to Heterogeneous Architectures with Alpaka*,\n   [video link (39 min)](http://on-demand.gputechconf.com/gtc/2016/video/S6298.html),\n   [slides (PDF)](https://on-demand.gputechconf.com/gtc/2016/presentation/s6298-erik-zenker-porting-the-plasma.pdf),\n   [DOI:10.5281/zenodo.6336086](https://doi.org/10.5281/zenodo.6336086)\n\n\nCiting alpaka\n-------------\n\nCurrently all authors of **alpaka** are scientists or connected with\nresearch. For us to justify the importance and impact of our work, please\nconsider citing us accordingly in your derived work and publications:\n\n```latex\n% Peer-Reviewed Publication %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n%\n% Peer reviewed and accepted publication in\n%   \"2nd International Workshop on Performance Portable\n%    Programming Models for Accelerators (P^3MA)\"\n% colocated with the\n%   \"2017 ISC High Performance Conference\"\n%   in Frankfurt, Germany\n@inproceedings{MathesP3MA2017,\n  author    = {{Matthes}, A. and {Widera}, R. and {Zenker}, E. and {Worpitz}, B. and\n               {Huebl}, A. and {Bussmann}, M.},\n  title     = {Tuning and optimization for a variety of many-core architectures without changing a single line of implementation code\n               using the Alpaka library},\n  archivePrefix = \"arXiv\",\n  eprint    = {1706.10086},\n  keywords  = {Computer Science - Distributed, Parallel, and Cluster Computing},\n  day       = {30},\n  month     = {Jun},\n  year      = {2017},\n  url       = {https://arxiv.org/abs/1706.10086},\n}\n\n% Peer-Reviewed Publication %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n%\n% Peer reviewed and accepted publication in\n%   \"The Sixth International Workshop on\n%    Accelerators and Hybrid Exascale Systems (AsHES)\"\n% at the\n%   \"30th IEEE International Parallel and Distributed\n%    Processing Symposium\" in Chicago, IL, USA\n@inproceedings{ZenkerAsHES2016,\n  author    = {Erik Zenker and Benjamin Worpitz and Ren{\\'{e}} Widera\n               and Axel Huebl and Guido Juckeland and\n               Andreas Kn{\\\"{u}}pfer and Wolfgang E. Nagel and Michael Bussmann},\n  title     = {Alpaka - An Abstraction Library for Parallel Kernel Acceleration},\n  archivePrefix = \"arXiv\",\n  eprint    = {1602.08477},\n  keywords  = {Computer science;CUDA;Mathematical Software;nVidia;OpenMP;Package;\n               performance portability;Portability;Tesla K20;Tesla K80},\n  day       = {23},\n  month     = {May},\n  year      = {2016},\n  publisher = {IEEE Computer Society},\n  url       = {http://arxiv.org/abs/1602.08477},\n}\n\n\n% Original Work: Benjamin Worpitz' Master Thesis %%%%%%%%%%\n%\n@MasterThesis{Worpitz2015,\n  author = {Benjamin Worpitz},\n  title  = {Investigating performance portability of a highly scalable\n            particle-in-cell simulation code on various multi-core\n            architectures},\n  school = {{Technische Universit{\\\"{a}}t Dresden}},\n  month  = {Sep},\n  year   = {2015},\n  type   = {Master Thesis},\n  doi    = {10.5281/zenodo.49768},\n  url    = {http://dx.doi.org/10.5281/zenodo.49768}\n}\n```\n\nContributing\n------------\n\nRules for contributions can be found in [CONTRIBUTING.md](CONTRIBUTING.md).\nAny pull request will be reviewed by a [maintainer](https://github.com/orgs/alpaka-group/teams/alpaka-maintainers).\n\nThanks to all [active and former contributors](.zenodo.json).\n\n",
                "dependencies": "#\n# Copyright 2023 Benjamin Worpitz, Jan Stephan, Bernhard Manfred Gruber\n# SPDX-License-Identifier: MPL-2.0\n#\n\n################################################################################\n# Required CMake version\n\ncmake_minimum_required(VERSION 3.25)\n\ncmake_policy(SET CMP0091 NEW)\n\ninclude(CMakePrintHelpers)\n\n#-------------------------------------------------------------------------------\n# Find alpaka version.\nfile(STRINGS \"${CMAKE_CURRENT_LIST_DIR}/include/alpaka/version.hpp\" alpaka_VERSION_MAJOR_HPP REGEX \"#define ALPAKA_VERSION_MAJOR \")\nfile(STRINGS \"${CMAKE_CURRENT_LIST_DIR}/include/alpaka/version.hpp\" alpaka_VERSION_MINOR_HPP REGEX \"#define ALPAKA_VERSION_MINOR \")\nfile(STRINGS \"${CMAKE_CURRENT_LIST_DIR}/include/alpaka/version.hpp\" alpaka_VERSION_PATCH_HPP REGEX \"#define ALPAKA_VERSION_PATCH \")\n\nstring(REGEX MATCH \"([0-9]+)\" alpaka_VERSION_MAJOR  ${alpaka_VERSION_MAJOR_HPP})\nstring(REGEX MATCH \"([0-9]+)\" alpaka_VERSION_MINOR  ${alpaka_VERSION_MINOR_HPP})\nstring(REGEX MATCH \"([0-9]+)\" alpaka_VERSION_PATCH  ${alpaka_VERSION_PATCH_HPP})\n\nset(PACKAGE_VERSION \"${alpaka_VERSION_MAJOR}.${alpaka_VERSION_MINOR}.${alpaka_VERSION_PATCH}\")\n\nproject(alpaka VERSION      ${alpaka_VERSION_MAJOR}.${alpaka_VERSION_MINOR}.${alpaka_VERSION_PATCH}\n               DESCRIPTION  \"The alpaka library is a header-only C++20 abstraction library for accelerator development.\"\n               HOMEPAGE_URL \"https://github.com/alpaka-group/alpaka\"\n               LANGUAGES    CXX)\n\nset_property(GLOBAL PROPERTY USE_FOLDERS ON)\n\n################################################################################\n# Options and Variants\n\noption(alpaka_BUILD_EXAMPLES \"Build the examples\" OFF)\noption(alpaka_BUILD_BENCHMARKS \"Build the benchmarks\" OFF)\n\n# Enable the test infrastructure only if alpaka is the top-level project\nif(CMAKE_PROJECT_NAME STREQUAL PROJECT_NAME)\n    option(alpaka_ENABLE_WERROR \"Treat all warnings as errors.\" OFF)\n    option(BUILD_TESTING \"Build the testing tree.\" OFF)\n    include(CTest)\nendif()\n\noption(alpaka_INSTALL_TEST_HEADER \"Install headers of the namespace alpaka::test. Attention, headers are not designed for production code, see documentation.\" OFF)\n\ninclude(CMakeDependentOption)\n\ncmake_dependent_option(alpaka_CHECK_HEADERS \"Check all alpaka headers as part of the tests whether they can be compiled standalone.\" OFF BUILD_TESTING OFF)\ncmake_dependent_option(alpaka_USE_INTERNAL_CATCH2 \"Use internally shipped Catch2\" ON \"BUILD_TESTING OR alpaka_BUILD_BENCHMARKS\" OFF)\n\n################################################################################\n# Internal variables.\n\n# Set found to true initially and set it to false if a required dependency is missing.\nset(_alpaka_FOUND TRUE)\n\n# This file's directory.\nset(_alpaka_ROOT_DIR ${CMAKE_CURRENT_LIST_DIR})\n# Normalize the path (e.g. remove ../)\nget_filename_component(_alpaka_ROOT_DIR ${_alpaka_ROOT_DIR} ABSOLUTE)\n\n# Compiler feature tests.\nset(_alpaka_FEATURE_TESTS_DIR \"${_alpaka_ROOT_DIR}/cmake/tests\")\n\n# Add common functions.\nset(_alpaka_COMMON_FILE \"${_alpaka_ROOT_DIR}/cmake/common.cmake\")\ninclude(${_alpaka_COMMON_FILE})\n\n# Add alpaka_ADD_EXECUTABLE function.\nset(_alpaka_ADD_EXECUTABLE_FILE \"${_alpaka_ROOT_DIR}/cmake/addExecutable.cmake\")\ninclude(${_alpaka_ADD_EXECUTABLE_FILE})\n\n# Add alpaka_ADD_LIBRARY function.\nset(_alpaka_ADD_LIBRARY_FILE \"${_alpaka_ROOT_DIR}/cmake/addLibrary.cmake\")\ninclude(${_alpaka_ADD_LIBRARY_FILE})\n\n# Set include directories\nset(_alpaka_INCLUDE_DIRECTORY \"${_alpaka_ROOT_DIR}/include\")\nset(_alpaka_SUFFIXED_INCLUDE_DIR \"${_alpaka_INCLUDE_DIRECTORY}/alpaka\")\n\n# the sequential accelerator is required for the tests and examples\nif(alpaka_BUILD_EXAMPLES OR alpaka_BUILD_BENCHMARKS OR BUILD_TESTING)\n  if (NOT (alpaka_ACC_GPU_CUDA_ONLY_MODE OR alpaka_ACC_GPU_HIP_ONLY_MODE))\n    if (NOT DEFINED alpaka_ACC_CPU_B_SEQ_T_SEQ_ENABLE)\n      option(alpaka_ACC_CPU_B_SEQ_T_SEQ_ENABLE \"enable alpaka serial accelerator\" ON)\n    elseif(NOT alpaka_ACC_CPU_B_SEQ_T_SEQ_ENABLE)\n      set(alpaka_ACC_CPU_B_SEQ_T_SEQ_ENABLE ON CACHE BOOL \"enable alpaka serial accelerator\" FORCE)\n    endif()\n  else() #CUDA or HIP-only mode\n    message(WARNING \"CUDA/HIP-only mode enabled: not enabling alpaka serial accelerator (required for examples and tests)\")\n  endif()\nendif()\ninclude(${_alpaka_ROOT_DIR}/cmake/alpakaCommon.cmake)\n\n# Add all the source and include files in all recursive subdirectories and group them accordingly.\nappend_recursive_files_add_to_src_group(\"${_alpaka_SUFFIXED_INCLUDE_DIR}\" \"${_alpaka_SUFFIXED_INCLUDE_DIR}\" \"hpp\" _alpaka_FILES_HEADER)\nappend_recursive_files_add_to_src_group(\"${_alpaka_SUFFIXED_INCLUDE_DIR}\" \"${_alpaka_SUFFIXED_INCLUDE_DIR}\" \"h\" _alpaka_FILES_HEADER)\n\n# remove headers of the folder alpaka/test, if alpaka_INSTALL_TEST_HEADER is disabled\nif(NOT alpaka_INSTALL_TEST_HEADER)\n  list(FILTER _alpaka_FILES_HEADER EXCLUDE REGEX \"(.*)/alpaka/test/(.*)\")\nendif()\n\nappend_recursive_files_add_to_src_group(\"${_alpaka_ROOT_DIR}/script\" \"${_alpaka_ROOT_DIR}\" \"sh\" _alpaka_FILES_SCRIPT)\nset_source_files_properties(${_alpaka_FILES_SCRIPT} PROPERTIES HEADER_FILE_ONLY TRUE)\n\nappend_recursive_files_add_to_src_group(\"${_alpaka_ROOT_DIR}/cmake\" \"${_alpaka_ROOT_DIR}\" \"cmake\" _alpaka_FILES_CMAKE)\nlist(APPEND _alpaka_FILES_CMAKE \"${_alpaka_ROOT_DIR}/cmake/alpakaConfig.cmake.in\" \"${_alpaka_ROOT_DIR}/CMakeLists.txt\")\nset_source_files_properties(${_alpaka_FILES_CMAKE} PROPERTIES HEADER_FILE_ONLY TRUE)\n\nappend_recursive_files_add_to_src_group(\"${_alpaka_ROOT_DIR}/docs/markdown\" \"${_alpaka_ROOT_DIR}\" \"md\" _alpaka_FILES_DOC)\nset_source_files_properties(${_alpaka_FILES_DOC} PROPERTIES HEADER_FILE_ONLY TRUE)\n\nappend_recursive_files_add_to_src_group(\"${_alpaka_ROOT_DIR}/.github\" \"${_alpaka_ROOT_DIR}\" \"yml\" _alpaka_FILES_OTHER)\nlist(APPEND _alpaka_FILES_OTHER \"${_alpaka_ROOT_DIR}/.clang-format\" \"${_alpaka_ROOT_DIR}/.gitignore\" \"${_alpaka_ROOT_DIR}/.zenodo.json\" \"${_alpaka_ROOT_DIR}/LICENSE\" \"${_alpaka_ROOT_DIR}/README.md\")\nset_source_files_properties(${_alpaka_FILES_OTHER} PROPERTIES HEADER_FILE_ONLY TRUE)\n\nif(TARGET alpaka)\n    # HACK: Workaround for the limitation that files added to INTERFACE targets (target_sources) can not be marked as PUBLIC or PRIVATE but only as INTERFACE.\n    # Therefore those files will be added to projects \"linking\" to the INTERFACE library, but are not added to the project itself within an IDE.\n    add_custom_target(\"alpakaIde\"\n                      SOURCES ${_alpaka_FILES_HEADER} ${_alpaka_FILES_SCRIPT} ${_alpaka_FILES_CMAKE} ${_alpaka_FILES_DOC} ${_alpaka_FILES_OTHER})\nendif()\n\n################################################################################\n# Export NVCC/HIPCC flags to parent scope if alpaka is used as a CMake\n# subdirectory.\n#\n# These flags are set in cmake/alpakaCommon.cmake but are visible in this scope\n# since alpakaCommon.cmake is included.\n\nif(NOT CMAKE_PROJECT_NAME STREQUAL PROJECT_NAME)\n    if(alpaka_ACC_GPU_HIP_ENABLE)\n        # export HIPCC flags to parent scope in case alpaka is another project's subdirectory\n        set(HIP_HIPCC_FLAGS ${HIP_HIPCC_FLAGS} PARENT_SCOPE)\n        set(HIP_NVCC_FLAGS ${HIP_NVCC_FLAGS} PARENT_SCOPE)\n        set(HIP_VERBOSE_BUILD ${HIP_VERBOSE_BUILD} PARENT_SCOPE)\n    endif()\nendif()\n\n################################################################################\n# Add subdirectories\n\nadd_subdirectory(thirdParty)\n\nif(alpaka_BUILD_EXAMPLES)\n    add_subdirectory(\"example/\")\nendif()\n\nif(alpaka_BUILD_BENCHMARKS)\n    add_subdirectory(\"benchmarks/\")\nendif()\n\n# Only build the tests if alpaka is the top-level project and BUILD_TESTING is ON\nif(CMAKE_PROJECT_NAME STREQUAL PROJECT_NAME AND BUILD_TESTING)\n    add_subdirectory(\"test/\")\nendif()\n\n################################################################################\n# Installation.\n\n# Do not install if alpaka is used as a CMake subdirectory\nif(CMAKE_PROJECT_NAME STREQUAL PROJECT_NAME)\n    include(CMakePackageConfigHelpers)\n    include(GNUInstallDirs)\n\n    set(_alpaka_INSTALL_CMAKEDIR \"${CMAKE_INSTALL_LIBDIR}/cmake/alpaka\")\n\n    install(TARGETS alpaka\n            ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n            LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n            RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR})\n\n    write_basic_package_version_file(\n        \"alpakaConfigVersion.cmake\"\n        VERSION ${PROJECT_VERSION}\n        COMPATIBILITY SameMajorVersion)\n\n    configure_package_config_file(\n        \"${_alpaka_ROOT_DIR}/cmake/alpakaConfig.cmake.in\"\n        \"${PROJECT_BINARY_DIR}/alpakaConfig.cmake\"\n        INSTALL_DESTINATION \"${_alpaka_INSTALL_CMAKEDIR}\")\n\n    install(FILES \"${PROJECT_BINARY_DIR}/alpakaConfig.cmake\"\n                  \"${PROJECT_BINARY_DIR}/alpakaConfigVersion.cmake\"\n            DESTINATION \"${_alpaka_INSTALL_CMAKEDIR}\")\n\n    if(alpaka_INSTALL_TEST_HEADER)\n        install(DIRECTORY \"${_alpaka_SUFFIXED_INCLUDE_DIR}\"\n            DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}\")\n    else()\n    install(DIRECTORY \"${_alpaka_SUFFIXED_INCLUDE_DIR}\"\n            DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}\"\n                PATTERN \"test\" EXCLUDE)\n    endif()\n\n    install(FILES \"${_alpaka_ROOT_DIR}/cmake/addExecutable.cmake\"\n                  \"${_alpaka_ROOT_DIR}/cmake/addLibrary.cmake\"\n                  \"${_alpaka_ROOT_DIR}/cmake/alpakaCommon.cmake\"\n                  \"${_alpaka_ROOT_DIR}/cmake/common.cmake\"\n            DESTINATION \"${_alpaka_INSTALL_CMAKEDIR}\")\n    install(DIRECTORY \"${_alpaka_ROOT_DIR}/cmake/tests\"\n            DESTINATION \"${_alpaka_INSTALL_CMAKEDIR}\")\nendif()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/amiris",
            "repo_link": "https://gitlab.com/dlr-ve/esy/amiris/amiris",
            "content": {
                "codemeta": "",
                "readme": "<!-- SPDX-FileCopyrightText: 2023 German Aerospace Center <amiris@dlr.de>\n\nSPDX-License-Identifier: Apache-2.0 -->\n# AMIRIS\nAMIRIS is the **A**gent-based **M**arket model for the **I**nvestigation of **R**enewable and **I**ntegrated energy **S**ystems.\n\n## Statement of need\nAMIRIS is an agent-based simulation of electricity markets and their actors.\nIt enables researchers to analyse and evaluate energy policy instruments and their impact on the actors involved in the simulation context.\nDifferent prototypical agents on the electricity market interact with each other, each employing complex decision strategies. \nAMIRIS allows calculating the impact of policy instruments on economic performance of power plant operators and marketers.\nIt is based on [FAME](https://gitlab.com/fame-framework), the open Framework for distributed Agent-based Modelling of Energy systems.\nAMIRIS follows an explorative approach.\nThus, it does not optimise the energy system like other tools, e.g. [REMix](https://gitlab.com/dlr-ve/esy/remix/framework), but explores emerging effects created by energy system actors and their interactions under a given set of assumptions.\n\n## Further information\nPlease have a look at the [AMIRIS-Wiki](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/wikis/home) for further information.\nDo not hesitate to ask questions about AMIRIS at the [openMod-Forum](https://forum.openmod.org/tag/amiris).\n\n## Recommended Skills\nAMIRIS is a *JAVA* application configured via *Python* scripts.\nTo configure and run AMIRIS applications, no programming skills are strictly necessary, but experience with energy system modelling and Python is helpful.\nDevelopers, who want to modify the functionality or enhance the capabilities of AMIRIS, however, should have at least basic understanding of Java.\nIn addition, a basic understanding of [(FAME)](https://gitlab.com/fame-framework) is required in order to design new agents and their interactions.\n\n## Get Started\n### System Requirements\nAMIRIS is based on [FAME](https://gitlab.com/fame-framework), the open Framework for distributed Agent-based Modelling of Energy systems.\nTo run AMIRIS, Python 3.9 or higher and Java Development Kit (JDK) 11 or higher are required.\nIn case you want to modify the AMIRIS code, additional tools might be required.\nSee our [Wiki](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/wikis/GetStarted/Getting-started) for additional instructions.\n\n#### Java\nAMIRIS requires JDK version 11 or higher and has been tested to work with versions 11, 17, and 21.\nYou can test if you have a JDK by using the command `java --version` (or `java -version` on some systems).\nThis should show your Java version if Java was found.\nIf you get a command not found error, or if Java version is less than 11 please download and install a recent JDK from e.g. [here](https://adoptium.net/).\n\n#### Python\nYou will need a Python-enabled shell with Python 3.9 or higher and pip.\nYou can test if you have Python available by using the command `python --version`.\nThis should show your Python version if the Python command was found.\nNote that if you use a Python environment manager you can have several Python versions on your system side by side.\nIf you do not have Python installed on your system, you may use e.g. [conda](https://docs.conda.io/en/latest/miniconda.html) or [mamba](https://github.com/conda-forge/miniforge#mambaforge) or [Poetry](https://python-poetry.org/).\n\n### Set up Python Environment\nIn case you do not have any experience with creating a Python environment, we recommend to use [anaconda](https://www.anaconda.com/).\nInstall anaconda, start the anaconda prompt or powershell and enter:\n\n1. `conda create -n amirisEnv python=3.9`\n2. `conda activate amirisEnv`\n\nIn case you are using mamba, simple replace \"conda\" in the first command with \"mamba\" (but not in the second).\n\n### Install AMIRIS-Py\nWe recommend to use [AMIRIS-Py](https://gitlab.com/dlr-ve/esy/amiris/amiris-py/-/blob/main/README.md).\nAMIRIS-Py provides \"one-command\" installation and execution scripts, but you may also run AMIRIS using FAME scripts (see [here](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/wikis/GetStarted/Getting-started)).\nIn your AMIRIS Python environment (called \"amirisEnv\" above), run\n\n```\npip install amirispy\n```\n\n### Download AMIRIS\n1. Create a new folder on your disk called, e.g., \"AMIRIS\": `mkdir <AMIRIS>`\n2. Open your Python-enabled shell and navigate to this newly created folder: `cd <AMIRIS>` \n3. If not done yet, activate your Python environment with amiris-py: `conda activate <amirisEnv>`\n4. To download the latest AMIRIS build use: `amiris install`. This downloads the latest [AMIRIS model](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/jobs/artifacts/main/download?job=deploy:jdk11) and the latest version of [AMIRIS examples](https://gitlab.com/dlr-ve/esy/amiris/examples) into the current folder.\n\nYour \"AMIRIS\" folder should now look like this:\n\n```\nAMIRIS\n├─── examples\n│    ├─── Austria2019/\n│    ├─── Germany2019/\n│    ├─── ...\n│    ├─── Simple/\n│    └─── README.md\n├─── amiris-core_X.y.z-with-dependencies.jar\n└─── fameSetup.yaml\n```\n\nYou are now ready to execute AMIRIS.\n\n## Run AMIRIS with AMIRIS-Py\nUse amiris-py again to run AMIRIS:\n\n```\namiris run -j ./amiris-core_3.1.0-jar-with-dependencies.jar -s ./examples/Simple/scenario.yaml -o simple\n```\n\nThis runs the packaged AMIRIS Java archive (Jar) file specified after the `-j` option and simulates the scenario specified after the `-s` option.\nThe AMIRIS outputs are stored in a folder as designated after the `-o` option. \nCheck out the files in the AMIRIS folder - use the version code of the jar file you downloaded.\n\n## Results \nOpen the created output folder called e.g. \"simple\".\nEach type of agent has its own output file in CSV format.\nOpen the files in your favourite CSV editor.\nThe files take the following general structure:\n\n| AgentId | TimeStep | Col1 | Col2 | Col3 | ... |\n|---------|----------|------|------|------|-----|\n\nwhere:\n* `AgentId` refers to the unique ID of that agent - as specified in the input scenario.yaml \n* `TimeStep` refers to the time step at which the output was created; the number refers to the passed seconds since January 1st 2000, 00:00h (ignoring leap years). To convert to a human-readable time stamp best use the python function `fameio.source.time.FameTime.convert_fame_time_step_to_datetime`\n* `Col1` refers to the agent-type specific first output column\n* `Col2` refers to the agent-type specific second output column\n* `...` there can be arbitrarily many output columns - depending on the type of the agent \n\nHere, `AgentId` and `TimeStep` form a 2-column multiindex.\nThus, each agent can only write one value per column and simulation time.\nFor example, open the \"EnergyExchange.csv\". \nThe Agent with ID 1 is the only one of type EnergyExchange - so this column is kind of uninteresting in this file.\nThe fourth column is named \"ElectricityPriceInEURperMWH\" and contains the market-clearing day-ahead electricity prices.\n\nAlthough in this file, all columns are filled in every time step, this is not the case for all types of agents.\nSome agents write their column entries at slightly different time steps.\nThis is caused by the simulation, which saves output data at the time step the action is performed.\n\nSome types of agents need to write out more than one output per time step.\nE.g., the conventional plant operators writes out the dispatched power for every power plant of each agent and each time step.\nSuch output will be assigned an extra CSV file named \"AgentType_MultiindexColumn\".\nThese files can feature an N-dimensional-multiindex with a single \"value\" column like so:\n\n| AgentId | TimeStep | 3rd index | 4th index | ... | Value |\n|---------|----------|-----------|-----------|-----|-------|\n\nIn this example of \"ConventionalPlantOperator_DispatchedPowerInMWHperPlant.csv\", `AgentId`, `TimeStep` and `ID` of the power plant form a 3D-multiindex.\nEach index is assigned a single value for \"DispatchedPowerInMWHperPlant\".\n\n## Next Steps\nCongratulations, you have now successfully run AMIRIS. \nYou want to see which inputs led to those results? See the input scenario at \"./examples/Simple/scenario.yaml\".\nOr do you want to create your own simulation configuration or how to modify AMIRIS?\nCheck out the [AMIRIS-Wiki](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/wikis/GetStarted/Getting-started).\nPlease also refer to the [FAME-Wiki](https://gitlab.com/fame-framework/wiki/-/wikis) when applying more advanced adaptations to your scenario, such as changing the [simulation duration](https://gitlab.com/fame-framework/wiki/-/wikis/GetStarted/core/Contracts). \n\n\n## Available Support\nIf you experience any trouble with AMIRIS, you may contact the developers at the [openMod-Forum](https://forum.openmod.org/tag/amiris) or via [amiris@dlr.de](mailto:amiris@dlr.de).\nPlease report bugs and make feature requests by filing issues following the provided templates (see also [CONTRIBUTING](CONTRIBUTING.md)).\nFor substantial enhancements, we recommend that you contact us via [amiris@dlr.de](mailto:amiris@dlr.de) for working together on the code in joint projects or towards common publications in order to further develop AMIRIS.\n\n### AMIRIS Open Forum\nYou are also welcome to join the [AMIRIS Open Forum](https://meet.jit.si/AMIRISOpenForum) to discuss issues and ideas, every Friday morning from 10:00 to 10:30 CET - no registration required.\n\n## Citing AMIRIS\nIf you use AMIRIS in your scientific work please cite:\n\nChristoph Schimeczek, Kristina Nienhaus, Ulrich Frey, Evelyn Sperber, Seyedfarzad Sarfarazi, Felix Nitsch, Johannes Kochems & A. Achraf El Ghazi (2023).\nAMIRIS: Agent-based Market model for the Investigation of Renewable and Integrated energy Systems.\nJournal of Open Source Software. [doi: 10.21105/joss.05041](https://doi.org/10.21105/joss.05041)\n\n## Acknowledgements\nDevelopment of AMIRIS was funded by the German Aerospace Center, the German Federal Ministry for Economic Affairs and Climate Action, the German Federal Ministry of Education and Research, and the German Federal for the Environment, Nature Conservation and Nuclear Safety. \nIt received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 864276.\nWe express our gratitude to all [contributors](CONTRIBUTING.md#list-of-contributors).\n\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/anndata",
            "repo_link": "https://github.com/scverse/anndata",
            "content": {
                "codemeta": "",
                "readme": "[![Build Status](https://dev.azure.com/scverse/anndata/_apis/build/status/scverse.anndata?branchName=main)](https://dev.azure.com/scverse/anndata/_build)\n[![Conda](https://img.shields.io/conda/vn/conda-forge/anndata.svg)](https://anaconda.org/conda-forge/anndata)\n[![Coverage](https://codecov.io/gh/scverse/anndata/branch/main/graph/badge.svg?token=IN1mJN1Wi8)](https://codecov.io/gh/scverse/anndata)\n[![Docs](https://readthedocs.com/projects/icb-anndata/badge/?version=latest)](https://anndata.readthedocs.io)\n[![PyPI](https://img.shields.io/pypi/v/anndata.svg)](https://pypi.org/project/anndata)\n[![Downloads](https://static.pepy.tech/badge/anndata/month)](https://pepy.tech/project/anndata)\n[![Downloads](https://static.pepy.tech/badge/anndata)](https://pepy.tech/project/anndata)\n[![Stars](https://img.shields.io/github/stars/scverse/anndata?style=flat&logo=github&color=yellow)](https://github.com/scverse/anndata/stargazers)\n[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](http://numfocus.org)\n\n<img\n  src=\"https://raw.githubusercontent.com/scverse/anndata/main/docs/_static/img/anndata_schema.svg\"\n  class=\"dark-light\" align=\"right\" width=\"350\" alt=\"image\"\n/>\n\n# anndata - Annotated data\n\nanndata is a Python package for handling annotated data matrices in memory and on disk, positioned between pandas and xarray. anndata offers a broad range of computationally efficient features including, among others, sparse data support, lazy operations, and a PyTorch interface.\n\n- Discuss development on [GitHub](https://github.com/scverse/anndata).\n- Read the [documentation](https://anndata.readthedocs.io).\n- Ask questions on the [scverse Discourse](https://discourse.scverse.org).\n- Install via `pip install anndata` or `conda install anndata -c conda-forge`.\n- See [Scanpy's documentation](https://scanpy.readthedocs.io/) for usage related to single cell data. anndata was initially built for Scanpy.\n\n[//]: # (numfocus-fiscal-sponsor-attribution)\n\nanndata is part of the scverse project ([website](https://scverse.org), [governance](https://scverse.org/about/roles)) and is fiscally sponsored by [NumFOCUS](https://numfocus.org/).\nPlease consider making a tax-deductible [donation](https://numfocus.org/donate-to-scverse) to help the project pay for developer time, professional services, travel, workshops, and a variety of other needs.\n\n\n<a href=\"https://numfocus.org/project/scverse\">\n  <img\n    src=\"https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png\"\n    width=\"200\"\n  >\n</a>\n\n## Public API\n\nOur public API is documented in the [API section][] of these docs.\nWe cannot guarantee the stability of our internal APIs, whether it's the location of a function, its arguments, or something else.\nIn other words, we do not officially support (or encourage users to do) something like `from anndata._core import AnnData` as `_core` is both not documented and contains a [leading underscore][].\nHowever, we are aware that [many users do use these internal APIs][] and thus encourage them to [open an issue][] or migrate to the public API.\nThat is, if something is missing from our public API as documented, for example a feature you wish to be exported publicly, please open an issue.\n\n[api section]: https://anndata.readthedocs.io/en/stable/api.html\n[leading underscore]: https://peps.python.org/pep-0008/#public-and-internal-interfaces\n[many users do use these internal APIs]: https://github.com/search?q=%22anndata._io%22&type=code\n[open an issue]: https://github.com/scverse/anndata/issues/new/choose\n\n\n## Citation\n\nIf you use `anndata` in your work, please cite the `anndata` publication as follows:\n\n> **anndata: Annotated data**\n>\n> Isaac Virshup, Sergei Rybakov, Fabian J. Theis, Philipp Angerer, F. Alexander Wolf\n>\n> _JOSS_ 2024 Sep 16. doi: [10.21105/joss.04371](https://doi.org/10.21105/joss.04371).\n\nYou can cite the scverse publication as follows:\n\n> **The scverse project provides a computational ecosystem for single-cell omics data analysis**\n>\n> Isaac Virshup, Danila Bredikhin, Lukas Heumos, Giovanni Palla, Gregor Sturm, Adam Gayoso, Ilia Kats, Mikaela Koutrouli, Scverse Community, Bonnie Berger, Dana Pe’er, Aviv Regev, Sarah A. Teichmann, Francesca Finotello, F. Alexander Wolf, Nir Yosef, Oliver Stegle & Fabian J. Theis\n>\n> _Nat Biotechnol._ 2023 Apr 10. doi: [10.1038/s41587-023-01733-8](https://doi.org/10.1038/s41587-023-01733-8).\n\n",
                "dependencies": "[build-system]\nbuild-backend = \"hatchling.build\"\nrequires = [\"hatchling\", \"hatch-vcs\"]\n\n[project]\nname = \"anndata\"\ndescription = \"Annotated data.\"\nrequires-python = \">=3.10\"\nlicense = \"BSD-3-Clause\"\nauthors = [\n    { name = \"Philipp Angerer\" },\n    { name = \"Alex Wolf\" },\n    { name = \"Isaac Virshup\" },\n    { name = \"Sergei Rybakov\" },\n]\nmaintainers = [\n    { name = \"Isaac Virshup\", email = \"ivirshup@gmail.com\" },\n    { name = \"Philipp Angerer\", email = \"philipp.angerer@helmholtz-munich.de\" },\n    { name = \"Alex Wolf\", email = \"f.alex.wolf@gmx.de\" },\n]\nreadme = \"README.md\"\nclassifiers = [\n    \"Environment :: Console\",\n    \"Framework :: Jupyter\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n    \"Natural Language :: English\",\n    \"Operating System :: MacOS :: MacOS X\",\n    \"Operating System :: Microsoft :: Windows\",\n    \"Operating System :: POSIX :: Linux\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering :: Bio-Informatics\",\n    \"Topic :: Scientific/Engineering :: Visualization\",\n]\ndependencies = [\n    # pandas <1.4 has pandas/issues/35446\n    # pandas 2.1.0rc0 has pandas/issues/54622\n    \"pandas >=1.4, !=2.1.0rc0, !=2.1.2\",\n    \"numpy>=1.23\",\n    # https://github.com/scverse/anndata/issues/1434\n    \"scipy >1.8\",\n    \"h5py>=3.6\",\n    \"exceptiongroup; python_version<'3.11'\",\n    \"natsort\",\n    \"packaging>=20.0\",\n    # array-api-compat 1.5 has https://github.com/scverse/anndata/issues/1410\n    \"array_api_compat>1.4,!=1.5\",\n]\ndynamic = [\"version\"]\n\n[project.urls]\nDocumentation = \"https://anndata.readthedocs.io/\"\nSource = \"https://github.com/scverse/anndata\"\nHome-page = \"https://github.com/scverse/anndata\"\n\n\n[project.optional-dependencies]\ndev = [\n    # dev version generation\n    \"setuptools-scm\",\n    \"anndata[dev-doc,dev-test]\",\n]\ndoc = [\n    \"sphinx>=7.4.6\",\n    \"sphinx-book-theme>=1.1.0\",\n    \"sphinx-autodoc-typehints>=2.2.0\",\n    \"sphinx-issues\",\n    \"sphinx-copybutton\",\n    \"sphinx-toolbox>=3.8.0\",\n    \"sphinxext.opengraph\",\n    \"nbsphinx\",\n    \"scanpydoc[theme,typehints] >=0.14.1\",\n    \"zarr\",\n    \"awkward>=2.0.7\",\n    \"IPython\",                          # For syntax highlighting in notebooks\n    \"myst_parser\",\n    \"sphinx_design>=0.5.0\",\n    \"readthedocs-sphinx-search\",\n    # for unreleased changes\n    \"anndata[dev-doc]\",\n]\ndev-doc = [\"towncrier>=24.8.0\"] # release notes tool\ntest = [\n    \"loompy>=3.0.5\",\n    \"pytest>=8.2,<8.3.4\",\n    \"pytest-cov>=2.10\",\n    \"zarr<3.0.0a0\",\n    \"matplotlib\",\n    \"scikit-learn\",\n    \"openpyxl\",\n    \"joblib\",\n    \"boltons\",\n    \"scanpy\",\n    \"httpx\", # For data downloading\n    \"dask[distributed]\",\n    \"awkward>=2.3\",\n    \"pyarrow\",\n    \"pytest_memray\",\n    \"pytest-mock\",\n    \"anndata[dask]\",\n]\ndev-test = [\"pytest-xdist\"] # local test speedups\ngpu = [\"cupy\"]\ncu12 = [\"cupy-cuda12x\"]\ncu11 = [\"cupy-cuda11x\"]\n# https://github.com/dask/dask/issues/11290\ndask = [\"dask[array]>=2022.09.2,!=2024.8.*,!=2024.9.*\"]\n\n[tool.hatch.version]\nsource = \"vcs\"\n[tool.hatch.build.hooks.vcs]\nversion-file = \"src/anndata/_version.py\"\nraw-options.version_scheme = \"release-branch-semver\"\n[tool.hatch.build.targets.wheel]\npackages = [\"src/anndata\", \"src/testing\"]\n\n[tool.coverage.run]\ndata_file = \"test-data/coverage\"\nsource_pkgs = [\"anndata\"]\nomit = [\"src/anndata/_version.py\", \"**/test_*.py\"]\n[tool.coverage.xml]\noutput = \"test-data/coverage.xml\"\n[tool.coverage.paths]\nsource = [\"./src\", \"**/site-packages\"]\n\n[tool.coverage.report]\nexclude_also = [\n    \"if TYPE_CHECKING:\",\n]\n\n[tool.pytest.ini_options]\naddopts = [\n    \"--import-mode=importlib\",\n    \"--strict-markers\",\n    \"--doctest-modules\",\n    \"--pyargs\",\n    \"-ptesting.anndata._pytest\",\n]\nfilterwarnings = [\n    # all `ignore::anndata.*` entries are in `conftest.py`\n    # See: https://github.com/pytest-dev/pytest-cov/issues/437\n]\n# When `--strict-warnings` is used, all warnings are treated as errors, except those:\nfilterwarnings_when_strict = [\n    \"default::anndata._warnings.ImplicitModificationWarning\",\n    \"default:Transforming to str index:UserWarning\",\n    \"default:(Observation|Variable) names are not unique. To make them unique:UserWarning\",\n    \"default::scipy.sparse.SparseEfficiencyWarning\",\n    \"default::dask.array.core.PerformanceWarning\",\n]\npython_files = \"test_*.py\"\ntestpaths = [\n    \"anndata\", # docstrings (module name due to --pyargs)\n    \"./tests\", # unit tests\n    \"./ci/scripts\", # CI script tests\n    \"./docs/concatenation.rst\", # further doctests\n]\n# For some reason this effects how logging is shown when tests are run\nxfail_strict = true\nmarkers = [\"gpu: mark test to run on GPU\"]\n\n[tool.ruff]\nsrc = [\"src\"]\n\n[tool.ruff.format]\ndocstring-code-format = true\n\n[tool.ruff.lint]\nselect = [\n    \"E\",   # Error detected by Pycodestyle\n    \"F\",   # Errors detected by Pyflakes\n    \"W\",   # Warning detected by Pycodestyle\n    \"PLW\", # Pylint\n    \"UP\",  # pyupgrade\n    \"I\",   # isort\n    \"TCH\", # manage type checking blocks\n    \"TID\", # Banned imports\n    \"ICN\", # Follow import conventions\n    \"PTH\", # Pathlib instead of os.path\n    \"PT\",  # Pytest conventions\n    \"PYI\", # Typing\n]\nignore = [\n    # line too long -> we accept long comment lines; formatter gets rid of long code lines\n    \"E501\",\n    # Do not assign a lambda expression, use a def -> AnnData allows lambda expression assignments,\n    \"E731\",\n    # allow I, O, l as variable names -> I is the identity matrix, i, j, k, l is reasonable indexing notation\n    \"E741\",\n    # We use relative imports from parent modules\n    \"TID252\",\n    # Shadowing loop variables isn’t a big deal\n    \"PLW2901\",\n]\n[tool.ruff.lint.per-file-ignores]\n# E721 comparing types, but we specifically are checking that we aren't getting subtypes (views)\n\"tests/test_readwrite.py\" = [\"E721\"]\n[tool.ruff.lint.isort]\nknown-first-party = [\"anndata\"]\nrequired-imports = [\"from __future__ import annotations\"]\n[tool.ruff.lint.flake8-tidy-imports.banned-api]\n\"subprocess.call\".msg = \"Use `subprocess.run([…])` instead\"\n\"subprocess.check_call\".msg = \"Use `subprocess.run([…], check=True)` instead\"\n\"subprocess.check_output\".msg = \"Use `subprocess.run([…], check=True, capture_output=True)` instead\"\n[tool.ruff.lint.flake8-type-checking]\nexempt-modules = []\nstrict = true\n\n[tool.codespell]\nskip = \".git,*.pdf,*.svg\"\nignore-words-list = \"theis,coo,homogenous\"\n\n[tool.towncrier]\npackage = \"anndata\"\ndirectory = \"docs/release-notes\"\nfilename = \"docs/release-notes/{version}.md\"\nsingle_file = false\npackage_dir = \"src\"\nissue_format = \"{{pr}}`{issue}`\"\ntitle_format = \"(v{version})=\\n### {version} {{small}}`{project_date}`\"\nfragment.bugfix.name = \"Bug fixes\"\nfragment.doc.name = \"Documentation\"\nfragment.feature.name = \"Features\"\nfragment.misc.name = \"Miscellaneous improvements\"\nfragment.performance.name = \"Performance\"\nfragment.breaking.name = \"Breaking changes\"\nfragment.dev.name = \"Development Process\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ansible-collection-toolkit",
            "repo_link": "https://github.com/hifis-net/ansible-collection-toolkit",
            "content": {
                "codemeta": "",
                "readme": "<!--\nSPDX-FileCopyrightText: Helmholtz Centre for Environmental Research (UFZ)\nSPDX-FileCopyrightText: Helmholtz-Zentrum Dresden-Rossendorf (HZDR)\n\nSPDX-License-Identifier: Apache-2.0\n-->\n\n# Ansible Collection - hifis.toolkit\n\n[![Latest release](https://img.shields.io/github/v/release/hifis-net/ansible-collection-toolkit)](https://github.com/hifis-net/ansible-collection-toolkit/releases)\n[![hifis.gitlab_runner](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/gitlab_runner.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/gitlab_runner.yml)\n[![hifis.gitlab](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/gitlab.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/gitlab.yml)\n[![hifis.haproxy](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/haproxy.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/haproxy.yml)\n[![hifis.keepalived](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/keepalived.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/keepalived.yml)\n[![hifis.netplan](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/netplan.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/netplan.yml)\n[![hifis.redis](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/redis.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/redis.yml)\n[![hifis.ssh_keys](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/ssh_keys.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/ssh_keys.yml)\n[![hifis.unattended_upgrades](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/unattended_upgrades.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/unattended_upgrades.yml)\n[![hifis.zammad](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/zammad.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/zammad.yml)\n[![DOI](https://zenodo.org/badge/495697576.svg)](https://zenodo.org/doi/10.5281/zenodo.11147483)\n\nThis collection provides production-ready Ansible roles used for providing services used in research and by research\nsoftware engineers, but not exclusively. The following use cases are supported:\n\n* **DevOps platform:**\n    * [GitLab](roles/gitlab)\n    * deploy [GitLab-Runner](roles/gitlab_runner) with a focus, but not limited, on Openstack autoscaling\n    * [Redis](roles/redis)\n* **Help desk:**\n    * [Zammad](roles/zammad)\n* **High Availability (HA) / Load Balancing:**\n    * [HAProxy](roles/haproxy)\n    * [Keepalived](roles/keepalived)\n* **OS-related:**\n    * [unattended-upgrades](roles/unattended_upgrades)\n    * [netplan](roles/netplan)\n    * distribute authorized [SSH keys](roles/ssh_keys) to users\n\n## Looking for the unattended_upgrades role?\n\nYou can now find it under [roles/unattended_upgrades](roles/unattended_upgrades).\n\nWe moved our existing Ansible roles into a single collection to deduplicate code and have a common test suite for all roles.\nWe decided to reuse the unattended_upgrades repository as a collection repo as it is our most popular role.\n\n## Minimum required Ansible-version\n\n* Ansible >= 2.16\n\n## Installation\n\nInstall the collection via ansible-galaxy:\n\n```shell\nansible-galaxy collection install hifis.toolkit\n```\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## License\n\nApache-2.0\n\n## Author\n\nThis collection is maintained by [HIFIS Software Services](https://hifis.net/).\n\n",
                "dependencies": "# SPDX-FileCopyrightText: Helmholtz Centre for Environmental Research (UFZ)\n# SPDX-FileCopyrightText: Helmholtz-Zentrum Dresden-Rossendorf (HZDR)\n#\n# SPDX-License-Identifier: Apache-2.0\n\n[[source]]\nname = \"pypi\"\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\n\n[dev-packages]\nyamllint = \"~=1.35.1\"\nansible-lint = \"~=24.10.0\"\nmolecule = \"~=24.12.0\"\nmolecule-plugins = {extras = [\"podman\"], version = \"~=23.5.3\"}\nreuse = \"~=5.0.2\"\nnetaddr = \"~=1.3.0\"\n\n[packages]\nansible = \"~=11.1.0\"\n\n[requires]\npython_version = \"3.12\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/anvio",
            "repo_link": "https://github.com/merenlab/anvio",
            "content": {
                "codemeta": "",
                "readme": "<p align=\"center\"><img src=\"https://github.com/merenlab/anvio/raw/master/anvio/data/interactive/images/logo-fancy.png\" height=\"256\" /></p>\n\n[![Daily Component Tests and Migrations](https://github.com/merenlab/anvio/actions/workflows/daily-component-tests-and-migrations.yaml/badge.svg)](https://github.com/merenlab/anvio/actions/workflows/daily-component-tests-and-migrations.yaml)\n\n### Releases\n\nGithub [releases page](https://github.com/merenlab/anvio/releases) lists all the stable releases of anvi'o.\n\n### Installation and tutorials\n\nThe [anvi'o project page](https://anvio.org) gives access to installation manuals, user tutorials, and other sweets.\n\n### Help on anvi'o programs and artifacts\n\n[The anvi'o help pages](https://anvio.org/help) describe individual anvi'o programs as well as artifacts they consume or produce.\n\n### Coding style considerations\n\nPlease see [relevant discussions](https://github.com/merenlab/anvio/issues?q=label%3A%22coding+style%22+).\n\n### Community chat\n\nClick [this link](https://discord.gg/C6He6mSNY4) to join the anvi'o Discord channel.\n\n### Others on anvi'o\n\nRead our [user testimonials](http://merenlab.org/2017/07/12/testimonials/).\n\n",
                "dependencies": "numpy<=1.24\nscipy\nbottle\npysam\nete3\nscikit-learn==1.2.2\ndjango\nrequests\nmistune\nsix\nmatplotlib==3.5.1\nstatsmodels\ncolored\nillumina-utils\ntabulate\nrich-argparse\nnumba\npaste\npyani\npsutil\npandas==1.4.4\nsnakemake\nmultiprocess\nplotext\nnetworkx\npulp==2.7.0\nbiopython\nreportlab\npymupdf\n\nimport os\nimport sys\nimport glob\n\ninit_py_path = os.path.normpath(os.path.dirname(os.path.abspath(__file__))) + '/anvio/__init__.py'\nversion_string = [l.strip() for l in open(init_py_path).readlines() if l.strip().startswith('anvio_version')][0]\nanvio_version = version_string.split('=')[1].strip().strip(\"'\").strip('\"')\n\nrequirements = [req.strip() for req in open('requirements.txt', 'r').readlines() if not req.startswith('#')]\n\ntry:\n    if sys.version_info.major != 3:\n        sys.stderr.write(\"Your active Python major version ('%d') is not compatible with what anvi'o expects :/ We recently switched to Python 3.\\n\" % sys.version_info.major)\n        sys.exit(-1)\nexcept Exception:\n    sys.stderr.write(\"(anvi'o failed to learn about your Python version, but it will pretend as if nothing happened)\\n\\n\")\n\nfrom setuptools import setup, find_packages\n\nos.chdir(os.path.normpath(os.path.join(os.path.abspath(__file__), os.pardir)))\n\nwith open(os.path.join(os.path.dirname(__file__), 'README.md')) as readme:\n    README = readme.read()\n\n\nsetup(\n    name = \"anvio\",\n    version = anvio_version,\n\n    scripts = [script for script in glob.glob('bin/*') + glob.glob('sandbox/*') if not script.endswith('-OBSOLETE')],\n    include_package_data = True,\n\n    packages = find_packages(),\n\n    install_requires = requirements,\n    author = \"anvi'o Authors\",\n    author_email = \"a.murat.eren@gmail.com\",\n    description = \"An interactive analysis and visualization platform for 'omics data. See https://merenlab.org/projects/anvio for more information\",\n    license = \"GPLv3+\",\n    keywords = \"metagenomics metatranscriptomics microbiology shotgun genomics MBL pipeline sequencing bam visualization SNP SNV\",\n    url = \"https://merenlab.org/projects/anvio/\",\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Environment :: Console',\n        'Environment :: Web Environment',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)',\n        'Natural Language :: English',\n        'Operating System :: MacOS',\n        'Operating System :: POSIX',\n        'Programming Language :: Python :: 3 :: Only',\n        'Programming Language :: JavaScript',\n        'Programming Language :: C',\n        'Topic :: Scientific/Engineering',\n    ],\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/arbor",
            "repo_link": "https://github.com/arbor-sim/arbor/",
            "content": {
                "codemeta": "",
                "readme": "[![ci](https://github.com/arbor-sim/arbor/actions/workflows/test-matrix.yml/badge.svg)](https://github.com/arbor-sim/arbor/actions/workflows/test-matrix.yml)\n[![spack](https://github.com/arbor-sim/arbor/actions/workflows/test-spack.yml/badge.svg)](https://github.com/arbor-sim/arbor/actions/workflows/test-spack.yml)\n[![pip](https://github.com/arbor-sim/arbor/actions/workflows/test-pip.yml/badge.svg)](https://github.com/arbor-sim/arbor/actions/workflows/test-pip.yml)\n[![pythonwheels](https://github.com/arbor-sim/arbor/actions/workflows/build-pip-wheels.yml/badge.svg)](https://github.com/arbor-sim/arbor/actions/workflows/build-pip-wheels.yml)\n[![gitpod](https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/arbor-sim/arbor)\n[![docs](https://readthedocs.org/projects/arbor/badge/?version=latest)](https://docs.arbor-sim.org/en/latest/)\n[![gitter](https://badges.gitter.im/arbor-sim/community.svg)](https://gitter.im/arbor-sim/community)\n[![CodeQL](https://github.com/arbor-sim/arbor/actions/workflows/codeql.yml/badge.svg?branch=master)](https://github.com/arbor-sim/arbor/actions/workflows/codeql.yml)\n\n# Arbor Library\n\n[Arbor](https://arbor-sim.org) is a library for implementing performance portable network simulations of multi-compartment neuron models.\n\nAn installation guide and library documentation are available online at [docs.arbor-sim.org](http://docs.arbor-sim.org).\n\n[Submit a ticket](https://github.com/arbor-sim/arbor/issues) or [join Gitter](https://gitter.im/arbor-sim/community) or [Matrix](https://matrix.to/#/#arbor-sim_community:gitter.im) if you have any questions or need help.\n\n### Citing Arbor\n\nThe Arbor introductory paper and entry on Zenodo can be cited, see [CITATION.bib](CITATION.bib). Please refer to [our documentation](https://docs.arbor-sim.org/en/latest/index.html#citing-arbor) for more information.\n\n",
                "dependencies": "cmake_minimum_required(VERSION 3.19)\ninclude(CMakeDependentOption)\ninclude(CheckIPOSupported)\n\n# Usually we don't want to hear those\nset(CMAKE_SUPPRESS_DEVELOPER_WARNINGS ON CACHE INTERNAL \"\" FORCE)\n\n# Make CUDA support throw errors if architectures remain unclear\ncmake_policy(SET CMP0104 NEW)\n# Ensure CMake is aware of the policies for modern RPATH behavior\ncmake_policy(SET CMP0072 NEW)\n\n# Set release as the default build type (CMake default is debug.)\n\nif (NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE release CACHE STRING \"Choose the type of build.\" FORCE)\n    # Set the possible values of build type for cmake-gui\n    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"debug\" \"release\")\nendif()\n\nset(CPM_USE_LOCAL_PACKAGES ON)\ninclude(cmake/CPM.cmake)\n\nfile(READ VERSION FULL_VERSION_STRING)\nstring(STRIP \"${FULL_VERSION_STRING}\" FULL_VERSION_STRING)\nstring(REGEX MATCH \"^[0-9]+(\\\\.[0-9]+)?(\\\\.[0-9]+)?(\\\\.[0-9]+)?\" numeric_version \"${FULL_VERSION_STRING}\")\n\nproject(arbor VERSION ${numeric_version})\nenable_language(CXX)\n\ninclude(GNUInstallDirs)\ninclude(CheckCXXCompilerFlag)\n\n# Effectively adds '-fpic' flag to CXX_FLAGS. Needed for dynamic catalogues.\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\n\n# Have LTO where possible, ie add -flto\ncheck_ipo_supported(RESULT HAVE_LTO OUTPUT ERR_LTO)\nif(NOT DEFINED CMAKE_INTERPROCEDURAL_OPTIMIZATION)\n  if(HAVE_LTO)\n    message (VERBOSE \"LTO support found, enabling\")\n    set(CMAKE_INTERPROCEDURAL_OPTIMIZATION TRUE)\n  else()\n    message(STATUS \"No LTO: ${ERR_LTO}\")\n  endif()\nendif()\n\n# Use pybind11-stubgen to make type stubs.\ncmake_dependent_option(ARB_BUILD_PYTHON_STUBS \"Use pybind11-stubgen to build type stubs.\" ON \"ARB_WITH_PYTHON\" OFF)\n\n# Turn on this option to force the compilers to produce color output when output is\n# redirected from the terminal (e.g. when using ninja or a pager).\n\noption(ARBDEV_COLOR \"Always produce ANSI-colored output (GNU/Clang only).\" OFF)\nmark_as_advanced(FORCE ARBDEV_COLOR)\n\n#----------------------------------------------------------\n# Configure-time build options for Arbor:\n#----------------------------------------------------------\n\n# Specify target architecture.\ncheck_cxx_compiler_flag(\"-march=native\" CXX_HAS_NATIVE)\nif(CXX_HAS_NATIVE)\n    set(ARB_DEFAULT_ARCH \"native\")\nelse()\n    set(ARB_DEFAULT_ARCH \"none\")\nendif()\nset(ARB_ARCH ${ARB_DEFAULT_ARCH} CACHE STRING \"Target architecture for arbor libraries\")\n\n# Perform explicit vectorization?\n\noption(ARB_VECTORIZE \"use explicit SIMD code in generated mechanisms\" OFF)\n\n# Support for Thread pinning\n\noption(ARB_USE_HWLOC \"request support for thread pinning via HWLOC\" OFF)\nmark_as_advanced(ARB_USE_HWLOC)\n\n# Build tests and benchmarks, docs\n\noption(BUILD_TESTING \"build tests and benchmarks\" ON)\noption(BUILD_DOCUMENTATION \"build documentation\" ON)\n\n# Use externally built modcc?\n\nset(ARB_MODCC \"\" CACHE STRING \"path to external modcc NMODL compiler\")\nmark_as_advanced(FORCE ARB_MODCC)\n\n# Use libunwind to generate stack traces on errors?\n\noption(ARB_BACKTRACE \"Enable stacktraces on assertion and exceptions (requires Boost).\" OFF)\nmark_as_advanced(FORCE ARB_BACKTRACE)\n\n# Specify GPU build type\n\nset(ARB_GPU \"none\" CACHE STRING \"GPU backend and compiler configuration\")\nset_property(CACHE PROPERTY STRINGS \"none\" \"cuda\" \"cuda-clang\" \"hip\")\nif(NOT ARB_GPU STREQUAL \"none\")\n    set(ARB_USE_GPU_DEP ON)\nendif()\ncmake_dependent_option(ARB_USE_GPU_RNG\n    \"Use GPU generated random numbers (only cuda, not bitwise equal to CPU version)\" OFF\n    \"ARB_USE_GPU_DEP\" OFF)\n\n# Optional additional CXX Flags used for all code that will run on the target\n# CPU architecture. Recorded in installed target, for downstream dependencies\n# to use.\n# Useful, for example, when a user wants to compile with target-specific\n# optimization flag.spr\nset(ARB_CXX_FLAGS_TARGET \"\" CACHE STRING \"Optional additional flags for compilation\")\nmark_as_advanced(FORCE ARB_CXX_FLAGS_TARGET)\n\n#----------------------------------------------------------\n# Debug support\n#----------------------------------------------------------\n\n# Print builtin catalogue configuration while building\noption(ARB_CAT_VERBOSE \"Print catalogue build information\" OFF)\nmark_as_advanced(ARB_CAT_VERBOSE)\n\n#----------------------------------------------------------\n# Configure-time features for Arbor:\n#----------------------------------------------------------\n\noption(ARB_WITH_MPI \"build with MPI support\" OFF)\n\noption(ARB_WITH_PROFILING \"enable Tracy profiling\" OFF)\ncmake_dependent_option(ARB_WITH_STACK_PROFILING \"enable stack collection in profiling\" OFF \"ARB_WITH_PROFILING\" OFF)\ncmake_dependent_option(ARB_WITH_MEMORY_PROFILING \"enable memory in profiling\" OFF \"ARB_WITH_PROFILING\" OFF)\nmark_as_advanced(FORCE ARB_WITH_STACK_PROFILING ARB_WITH_MEMORY_PROFILING)\n\noption(ARB_WITH_ASSERTIONS \"enable arb_assert() assertions in code\" OFF)\n\n#----------------------------------------------------------\n# Python front end for Arbor:\n#----------------------------------------------------------\n\noption(ARB_WITH_PYTHON \"enable Python front end\" OFF)\n\n#----------------------------------------------------------\n# Global CMake configuration\n#----------------------------------------------------------\n\n# Include own CMake modules in search path, load common modules.\n\nset(CMAKE_MODULE_PATH \"${PROJECT_SOURCE_DIR}/cmake\")\ninclude(GitSubmodule) # required for check_git_submodule\ninclude(ErrorTarget)  # reguired for add_error_target\n\n# Add CUDA as a language if GPU support requested. (This has to be set early so\n# as to enable CUDA tests in generator expressions.)\nif(ARB_GPU STREQUAL \"cuda\")\n    include(FindCUDAToolkit)\n    set(ARB_WITH_NVCC TRUE)\n    # CMake 3.18 and later set the default CUDA architecture for\n    # each target according to CMAKE_CUDA_ARCHITECTURES. \n\n    # This fixes nvcc picking up a wrong host compiler for linking, causing\n    # issues with outdated libraries, eg libstdc++ and std::filesystem. Must\n    # happen before all calls to enable_language(CUDA)\n    set(CMAKE_CUDA_HOST_COMPILER ${CMAKE_CXX_COMPILER})\n    enable_language(CUDA)\n    find_package(CUDAToolkit)\n    if(${CUDAToolkit_VERSION_MAJOR} GREATER_EQUAL 12)\n        if(NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n            # Pascal, Volta, Ampere, Hopper\n            set(CMAKE_CUDA_ARCHITECTURES 60 70 80 90)\n        endif()\n    else()\n        message(FATAL_ERROR \"Need at least CUDA 12, got ${CUDAToolkit_VERSION_MAJOR}\")\n    endif()\n\n    # We _still_ need this otherwise CUDA symbols will not be exported\n    # from libarbor.a leading to linker errors when link external clients.\n    # Unit tests are NOT external enough. Re-review this somewhere in the\n    # future.\n    find_package(CUDA ${CUDAToolkit_VERSION_MAJOR} REQUIRED)\nelseif(ARB_GPU STREQUAL \"cuda-clang\")\n    include(FindCUDAToolkit)\n    if(NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n        set(CMAKE_CUDA_ARCHITECTURES 60 70 80 90)\n    endif()\n    set(ARB_WITH_CUDA_CLANG TRUE)\n    enable_language(CUDA)\nelseif(ARB_GPU STREQUAL \"hip\")\n    set(ARB_WITH_HIP_CLANG TRUE)\n    # Specify AMD architecture using a (user provided) list.\n    # Note: CMake native HIP architectures are introduced with version 3.21.\n    set(ARB_HIP_ARCHITECTURES gfx906 gfx900 CACHE STRING \"AMD offload architectures (semicolon separated)\")\nendif()\n\nif(ARB_WITH_NVCC OR ARB_WITH_CUDA_CLANG OR ARB_WITH_HIP_CLANG)\n    set(ARB_WITH_GPU TRUE)\nendif()\n\n# Build paths.\n\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\n\n# Generate a .json file with full compilation command for each file.\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS \"YES\")\n\n# Compiler options common to library, examples, tests, etc.\n\ninclude(\"CompilerOptions\")\ncheck_supported_cxx()\nadd_compile_options(\"$<$<COMPILE_LANGUAGE:CXX>:${CXXOPT_WALL}>\")\nset(CMAKE_CXX_STANDARD 20)\nset(CMAKE_CUDA_STANDARD 20)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\n\nmark_as_advanced(FORCE CMAKE_OSX_ARCHITECTURES CMAKE_OSX_DEPLOYMENT_TARGET CMAKE_OSX_SYSROOT)\n\n#----------------------------------------------------------\n# Set up flags and dependencies:\n#----------------------------------------------------------\n\n# Note: any target dependency of arbor needs to be explicitly added\n# to the 'export set', even the private ones, and this must be done\n# in the same CMakeLists.txt in which the target is defined.\n\n# Data and internal scripts go here\nset(ARB_INSTALL_DATADIR ${CMAKE_INSTALL_DATAROOTDIR}/arbor)\n\n# Interface library `arbor-config-defs` collects configure-time defines\n# for arbor, arborenv, arborio, of the form ARB_HAVE_XXX. These\n# defines should _not_ be used in any installed public headers.\n\nadd_library(arbor-config-defs INTERFACE)\ninstall(TARGETS arbor-config-defs EXPORT arbor-targets)\n\n# Interface library `arbor-private-deps` collects dependencies, options etc.\n# for the arbor library.\nadd_library(arbor-private-deps INTERFACE)\ntarget_link_libraries(arbor-private-deps INTERFACE arbor-config-defs ext-random123 ${CMAKE_DL_LIBS})\ninstall(TARGETS arbor-private-deps EXPORT arbor-targets)\n\n# Interface library `arborenv-private-deps` collects dependencies, options etc.\n# for the arborenv library.\n\nadd_library(arborenv-private-deps INTERFACE)\ntarget_link_libraries(arborenv-private-deps INTERFACE arbor-config-defs)\ninstall(TARGETS arborenv-private-deps EXPORT arbor-targets)\n\n# Interface library `arborio-private-deps` collects dependencies, options etc.\n# for the arborio library.\n\nadd_library(arborio-private-deps INTERFACE)\ntarget_link_libraries(arborio-private-deps INTERFACE arbor-config-defs)\ninstall(TARGETS arborio-private-deps EXPORT arbor-targets)\n\n# Interface library `arbor-public-deps` collects requirements for the\n# users of the arbor library (e.g. mpi) that will become part\n# of arbor's PUBLIC interface.\n\nadd_library(arbor-public-deps INTERFACE)\ninstall(TARGETS arbor-public-deps EXPORT arbor-targets)\n\n# Interface library `arborio-public-deps` collects requirements for the\n# users of the arborio library (e.g. xml libs) that will become part\n# of arborio's PUBLIC interface.\n\nadd_library(arborio-public-deps INTERFACE)\ninstall(TARGETS arborio-public-deps EXPORT arborio-targets)\n\n# Add scripts and supporting CMake for setting up external catalogues\n\ninstall(PROGRAMS scripts/arbor-build-catalogue DESTINATION ${CMAKE_INSTALL_BINDIR})\ninstall(FILES mechanisms/BuildModules.cmake DESTINATION ${ARB_INSTALL_DATADIR})\n\n# Add all dependencies.\n\n# First make ourselves less chatty\nset(_saved_CMAKE_MESSAGE_LOG_LEVEL ${CMAKE_MESSAGE_LOG_LEVEL})\nset(CMAKE_MESSAGE_LOG_LEVEL STATUS)\n\n# in the event we can find hwloc, just add it\nfind_package(hwloc QUIET)\nadd_library(ext-hwloc INTERFACE)\nif(hwloc_FOUND)\n    # We'd like to use the package syntax, here, yet if we do, we'd need to\n    # provide the find script to the system.\n    target_link_directories(ext-hwloc INTERFACE ${hwloc_LIBRARY_DIRS})\n    target_link_libraries(ext-hwloc INTERFACE ${hwloc_LIBRARY})\n    target_include_directories(ext-hwloc INTERFACE ${hwloc_INCLUDE_DIR})\n    target_compile_definitions(ext-hwloc INTERFACE ARB_HAVE_HWLOC)\n    target_link_libraries(arbor-private-deps INTERFACE ext-hwloc)\nelse()\n    if(ARB_USE_HWLOC)\n        message(SEND_ERROR \"Requested support for hwloc, but CMake couldn't find it.\")\n    endif()\nendif()\ninstall(TARGETS ext-hwloc EXPORT arbor-targets)\n\nCPMAddPackage(NAME json\n              GITHUB_REPOSITORY nlohmann/json\n              VERSION 3.11.2\n              OPTIONS \"CMAKE_SUPPRESS_DEVELOPER_WARNINGS ON\")\ninstall(TARGETS nlohmann_json EXPORT arbor-targets)\n\nadd_library(ext-random123 INTERFACE)\nCPMAddPackage(NAME random123\n              DOWNLOAD_ONLY YES\n              GITHUB_REPOSITORY DEShawResearch/random123\n              VERSION 1.14.0)\nif(random123_ADDED)\n    target_include_directories(ext-random123 INTERFACE $<BUILD_INTERFACE:${random123_SOURCE_DIR}/include>)\nelse()\n    find_package(Random123 REQUIRED)\n    target_include_directories(ext-random123 INTERFACE ${RANDOM123_INCLUDE_DIR})\nendif()\ninstall(TARGETS ext-random123 EXPORT arbor-targets)\n\nif (ARB_WITH_PYTHON)\n    CPMAddPackage(NAME pybind11\n                  GITHUB_REPOSITORY pybind/pybind11\n                  VERSION 2.10.1\n                  OPTIONS \"PYBIND11_CPP_STANDARD -std=c++20\")\n    # required for find_python_module\n    include(FindPythonModule)\nendif()\n\nCPMAddPackage(NAME pugixml\n              GITHUB_REPOSITORY zeux/pugixml\n              VERSION 1.13\n              DOWNLOAD_ONLY YES)\nadd_library(ext-pugixml INTERFACE)\nif(pugixml_ADDED)\n    target_compile_definitions(ext-pugixml INTERFACE PUGIXML_HEADER_ONLY)\n    target_include_directories(ext-pugixml INTERFACE $<BUILD_INTERFACE:${pugixml_SOURCE_DIR}/src>)\nelse()\n    find_package(pugixml REQUIRED)\n    target_link_libraries(ext-pugixml INTERFACE pugixml::pugixml)\nendif()\ninstall(TARGETS ext-pugixml EXPORT arbor-targets)\n\nCPMAddPackage(NAME fmt\n              GITHUB_REPOSITORY fmtlib/fmt\n              VERSION 10.0.0\n              GIT_TAG 10.0.0)\n\nadd_library(ext-gtest INTERFACE)\nadd_library(ext-bench INTERFACE)\nif (BUILD_TESTING)\n    CPMAddPackage(NAME benchmark\n                  GITHUB_REPOSITORY google/benchmark\n                  VERSION 1.8.3\n                  OPTIONS \"BENCHMARK_ENABLE_TESTING OFF\" \"CMAKE_BUILD_TYPE release\" \"BUILD_SHARED_LIBS OFF\")\n    CPMAddPackage(NAME googletest\n                  GITHUB_REPOSITORY google/googletest\n                  GIT_TAG release-1.12.1\n                  VERSION 1.12.1\n                  OPTIONS \"INSTALL_GTEST OFF\" \"BUILD_GMOCK OFF\")\n    if(benchmark_ADDED)\n        target_link_libraries(ext-bench INTERFACE benchmark)\n    else()\n        find_package(benchmark REQUIRED)\n        target_link_libraries(ext-bench INTERFACE benchmark::benchmark)\n    endif()\n    if(googletest_ADDED)\n        target_link_libraries(ext-gtest INTERFACE )\n    else()\n        find_package(googletest REQUIRED)\n        target_link_libraries(ext-gtest INTERFACE gtest gtest_main)\n    endif()\nendif()\n\nCPMAddPackage(NAME units\n              GITHUB_REPOSITORY llnl/units\n              VERSION 0.9.1\n              OPTIONS \"CMAKE_PROJECT_NAME UNITS\"\n                      \"UNITS_INSTALL ON\"\n                      \"UNITS_BUILD_STATIC_LIBRARY ON\"\n                      \"UNITS_ENABLE_TESTS OFF\"\n                      \"UNITS_BUILD_CONVERTER_APP OFF\"\n                      \"UNITS_BUILD_WEBSERVER OFF\")\nadd_library(ext-units INTERFACE)\nif(units_ADDED)\n    target_link_libraries(ext-units INTERFACE units::units)\nelse()\n    find_package(units REQUIRED)\n    target_link_libraries(ext-units INTERFACE units::units)\nendif()\ntarget_link_libraries(arbor-public-deps INTERFACE ext-units)\ninstall(TARGETS ext-units EXPORT arbor-targets)\n\nCPMAddPackage(NAME tinyopt\n              GITHUB_REPOSITORY halfflat/tinyopt\n              GIT_TAG 7e6d707d49c6cb4be27ebd253856be65293288df\n              DOWNLOAD_ONLY YES)\n\nadd_library(ext-tinyopt INTERFACE)\nif(tinyopt_ADDED)\n  target_include_directories(ext-tinyopt INTERFACE $<BUILD_INTERFACE:${tinyopt_SOURCE_DIR}/include>)\nelse()\n    message(FATAL_ERROR \"Could not obtain tinyopt.\")\nendif()\n\n# hide all internal vars\nmark_as_advanced(FORCE benchmark_DIR BENCHMARK_BUILD_32_BITS BENCHMARK_DOWNLOAD_DEPENDENCIES BENCHMARK_ENABLE_ASSEMBLY_TESTS BENCHMARK_ENABLE_DOXYGEN BENCHMARK_ENABLE_EXCEPTIONS BENCHMARK_ENABLE_GTEST_TESTS BENCHMARK_ENABLE_INSTALL BENCHMARK_ENABLE_LIBPFM BENCHMARK_ENABLE_LTO BENCHMARK_ENABLE_WERROR BENCHMARK_FORCE_WERROR BENCHMARK_INSTALL_DOCS BENCHMARK_USE_BUNDLED_GTEST BENCHMARK_USE_LIBCXX)\nmark_as_advanced(FORCE googletest_DIR BUILD_GMOCK)\nmark_as_advanced(FORCE json_DIR JSON_CI JSON_BuildTests JSON_Diagnostics JSON_DisableEnumSerialization JSON_GlobalUDLs JSON_ImplicitConversions JSON_Install JSON_LegacyDiscardedValueComparison JSON_MultipleHeaders JSON_SystemInclude)\nmark_as_advanced(FORCE RANDOM123_INCLUDE_DIR)\nmark_as_advanced(FORCE pybind11_DIR PYBIND11_PYTHONLIBS_OVERWRITE PYBIND11_PYTHON_VERSION PYBIND11_FINDPYTHON PYBIND11_INSTALL PYBIND11_INTERNALS_VERSION PYBIND11_NOPYTHON PYBIND11_SIMPLE_GIL_MANAGEMENT PYBIND11_TEST)\nmark_as_advanced(FORCE pugixml_DIR)\nmark_as_advanced(FORCE fmt_DIR)\nmark_as_advanced(FORCE units_DIR UNITS_BUILD_OBJECT_LIBRARY UNITS_BUILD_SHARED_LIBRARY UNITS_HEADER_ONLY UNITS_NAMESPACE UNITS_BUILD_FUZZ_TARGETS UNITS_ENABLE_TESTS)\nmark_as_advanced(FORCE tinyopt_DIR)\nmark_as_advanced(FORCE CXXFEATURECHECK_DEBUG)\nmark_as_advanced(FORCE CPM_DONT_CREATE_PACKAGE_LOCK CPM_DONT_UPDATE_MODULE_PATH CPM_DOWNLOAD_ALL CPM_INCLUDE_ALL_IN_PACKAGE_LOCK CPM_LOCAL_PACKAGES_ONLY CPM_SOURCE_CACHE CPM_USE_NAMED_CACHE_DIRECTORIES)\nmark_as_advanced(FORCE FETCHCONTENT_BASE_DIR FETCHCONTENT_FULLY_DISCONNECTED FETCHCONTENT_QUIET FETCHCONTENT_SOURCE_DIR_BENCHMARK FETCHCONTENT_SOURCE_DIR_GOOGLETEST FETCHCONTENT_SOURCE_DIR_JSON FETCHCONTENT_SOURCE_DIR_PYBIND11 FETCHCONTENT_SOURCE_DIR_RANDOM123 FETCHCONTENT_SOURCE_DIR_TINYOPT FETCHCONTENT_SOURCE_DIR_UNITS FETCHCONTENT_UPDATES_DISCONNECTED FETCHCONTENT_UPDATES_DISCONNECTED_BENCHMARK FETCHCONTENT_UPDATES_DISCONNECTED_GOOGLETEST FETCHCONTENT_UPDATES_DISCONNECTED_JSON FETCHCONTENT_UPDATES_DISCONNECTED_PYBIND11 FETCHCONTENT_UPDATES_DISCONNECTED_RANDOM123 FETCHCONTENT_UPDATES_DISCONNECTED_TINYOPT FETCHCONTENT_UPDATES_DISCONNECTED_UNITS)\n\n\n# Restore chattyness\nset(CMAKE_MESSAGE_LOG_LEVEL ${_saved_CMAKE_MESSAGE_LOG_LEVEL})\n\n# Keep track of packages we need to add to the generated CMake config\n# file for arbor.\n\nset(arbor_export_dependencies)\n\n# Keep track of which 'components' of arbor are included (this is\n# currently just 'MPI' support and 'neuroml' for NeuroML support in\n# libarborio.)\n\nset(arbor_supported_components)\n\n# Target microarchitecture for building arbor libraries, tests and examples\n#---------------------------------------------------------------------------\n\n# Set the full set of target flags in ARB_CXX_FLAGS_TARGET_FULL, which\n# will include target-specific -march flags if ARB_ARCH is not \"none\".\nif(ARB_ARCH STREQUAL \"none\")\n    set(ARB_CXX_FLAGS_TARGET_FULL ${ARB_CXX_FLAGS_TARGET})\n    set(ARB_CXX_FLAGS_TARGET_FULL_CPU ${ARB_CXX_FLAGS_TARGET})\nelse()\n    set_arch_target(ARB_CXXOPT_ARCH_CPU ARB_CXXOPT_ARCH ${ARB_ARCH})\n    set(ARB_CXX_FLAGS_TARGET_FULL ${ARB_CXX_FLAGS_TARGET} ${ARB_CXXOPT_ARCH})\n    set(ARB_CXX_FLAGS_TARGET_FULL_CPU ${ARB_CXX_FLAGS_TARGET} ${ARB_CXXOPT_ARCH_CPU})\nendif()\n\n# Add SVE compiler flags if detected/desired\nset(ARB_SVE_WIDTH \"auto\" CACHE STRING \"Default SVE vector length in bits. Default: auto (detection during configure time).\")\nmark_as_advanced(ARB_SVE_WIDTH)\n\nif (ARB_VECTORIZE)\n    if (ARB_SVE_WIDTH STREQUAL \"auto\")\n        get_sve_length(ARB_HAS_SVE ARB_SVE_BITS)\n        if (ARB_HAS_SVE)\n            message(STATUS \"SVE detected with vector size = ${ARB_SVE_BITS} bits\")\n            set(ARB_CXX_SVE_FLAGS \" -msve-vector-bits=${ARB_SVE_BITS}\")\n        else()\n            message(STATUS \"NO SVE detected\")\n            set(ARB_CXX_SVE_FLAGS \"\")\n        endif()\n    else()\n        set(ARB_SVE_BITS ${ARB_SVE_WIDTH})\n        set(ARB_CXX_SVE_FLAGS \" -msve-vector-bits=${ARB_SVE_BITS}\")\n    endif()\n    list(APPEND ARB_CXX_FLAGS_TARGET_FULL\n        \"$<$<BUILD_INTERFACE:$<COMPILE_LANGUAGE:CXX>>:${ARB_CXX_SVE_FLAGS}>\")\nendif()\n\n# Compile with `-fvisibility=hidden` to ensure that the symbols of the generated\n# arbor static libraries are hidden from the dynamic symbol tables of any shared\n# libraries that link against them.\nlist(APPEND ARB_CXX_FLAGS_TARGET_FULL\n            \"$<$<BUILD_INTERFACE:$<COMPILE_LANGUAGE:CXX>>:-fvisibility=hidden>\"\n            \"$<$<BUILD_INTERFACE:$<COMPILE_LANGUAGE:CUDA>>:-Xcompiler=-fvisibility=hidden>\")\nseparate_arguments(ARB_CXX_FLAGS_TARGET_FULL)\n\ntarget_compile_options(arbor-private-deps INTERFACE ${ARB_CXX_FLAGS_TARGET_FULL})\ntarget_compile_options(arborenv-private-deps INTERFACE ${ARB_CXX_FLAGS_TARGET_FULL})\ntarget_compile_options(arborio-private-deps INTERFACE ${ARB_CXX_FLAGS_TARGET_FULL})\n\n# Profiling and test features\n#-----------------------------\n\nif(ARB_WITH_PROFILING)\n    target_compile_definitions(arbor-config-defs INTERFACE ARB_HAVE_PROFILING)\nendif()\nif(ARB_WITH_ASSERTIONS)\n    target_compile_definitions(arbor-config-defs INTERFACE ARB_HAVE_ASSERTIONS)\nendif()\n\n# Python bindings\n#----------------------------------------------------------\n\n# The minimum version of Python supported by Arbor.\nset(arb_py_version 3.9.0)\n\nif(DEFINED PYTHON_EXECUTABLE)\n    set(Python3_EXECUTABLE ${PYTHON_EXECUTABLE})\nendif()\n\nif(ARB_WITH_PYTHON)\n    if(DEFINED ENV{CIBUILDWHEEL} AND (UNIX AND NOT APPLE))\n        find_package(Python3 ${arb_py_version} COMPONENTS Interpreter Development.Module REQUIRED)\n    else()\n        find_package(Python3 ${arb_py_version} COMPONENTS Interpreter Development REQUIRED)\n    endif()\nelse()\n    # If not building the Python module, the interpreter is still required\n    # to build some targets, e.g. when building the documentation.\n    find_package(Python3 ${arb_py_version} COMPONENTS Interpreter)\nendif()\n\nif(${Python3_FOUND})\n    set(PYTHON_EXECUTABLE \"${Python3_EXECUTABLE}\")\n    message(VERBOSE \"PYTHON_EXECUTABLE: ${PYTHON_EXECUTABLE}\")\nendif()\n\n# Threading model\n#-----------------\n\nfind_package(Threads REQUIRED)\ntarget_link_libraries(arbor-private-deps INTERFACE Threads::Threads)\n\nlist(APPEND arbor_export_dependencies \"Threads\")\n\n# MPI support\n#-------------------\n\nif(ARB_WITH_MPI)\n    find_package(MPI REQUIRED CXX)\n    target_compile_definitions(arbor-config-defs INTERFACE ARB_HAVE_MPI)\n\n    # CMake 3.9 does not allow us to add definitions to an import target. so\n    # wrap MPI::MPI_CXX in an interface library 'mpi-wrap' instead.\n    add_library(mpi-wrap INTERFACE)\n    target_link_libraries(mpi-wrap INTERFACE MPI::MPI_CXX)\n    target_compile_definitions(mpi-wrap INTERFACE MPICH_SKIP_MPICXX=1 OMPI_SKIP_MPICXX=1)\n\n    target_link_libraries(arbor-public-deps INTERFACE mpi-wrap)\n    install(TARGETS mpi-wrap EXPORT arbor-targets)\n\n    list(APPEND arbor_export_dependencies \"MPI\\;COMPONENTS\\;CXX\")\n    list(APPEND arbor_supported_components \"MPI\")\nendif()\n\n# CUDA support\n#--------------\n\nif(ARB_WITH_GPU)\n    if(ARB_WITH_NVCC OR ARB_WITH_CUDA_CLANG)\n        target_include_directories(arborenv-private-deps INTERFACE ${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES})\n        add_compile_options(\n                \"$<$<COMPILE_LANGUAGE:CUDA>:-Xcudafe=--diag_suppress=integer_sign_change>\"\n                \"$<$<COMPILE_LANGUAGE:CUDA>:-Xcudafe=--diag_suppress=unsigned_compare_with_zero>\")\n    endif()\n\n    if(ARB_WITH_NVCC)\n        target_compile_definitions(arbor-private-deps INTERFACE ARB_CUDA)\n        target_compile_definitions(arborenv-private-deps INTERFACE ARB_CUDA)\n    elseif(ARB_WITH_CUDA_CLANG)\n        # Transform cuda archtitecture list into clang cuda flags\n        list(TRANSFORM CMAKE_CUDA_ARCHITECTURES PREPEND \"--cuda-gpu-arch=sm_\" OUTPUT_VARIABLE TMP)\n        string(REPLACE \";\" \" \" CUDA_ARCH_STR \"${TMP}\")\n\n        set(clang_options_ -DARB_CUDA -xcuda ${CUDA_ARCH_STR} --cuda-path=${CUDA_TOOLKIT_ROOT_DIR})\n        target_compile_options(arbor-private-deps INTERFACE $<$<COMPILE_LANGUAGE:CXX>:${clang_options_}>)\n        target_compile_options(arborenv-private-deps INTERFACE $<$<COMPILE_LANGUAGE:CXX>:${clang_options_}>)\n    elseif(ARB_WITH_HIP_CLANG)\n        # Transform hip archtitecture list into clang hip flags\n        list(TRANSFORM ARB_HIP_ARCHITECTURES PREPEND \"--offload-arch=\" OUTPUT_VARIABLE TMP)\n        string(REPLACE \";\" \" \" HIP_ARCH_STR \"${TMP}\")\n\n        set(clang_options_ -DARB_HIP -xhip ${HIP_ARCH_STR})\n        target_compile_options(arbor-private-deps INTERFACE $<$<COMPILE_LANGUAGE:CXX>:${clang_options_}>)\n        target_compile_options(arborenv-private-deps INTERFACE $<$<COMPILE_LANGUAGE:CXX>:${clang_options_}>)\n    endif()\nendif()\n\n# Use boost::stacktrace if requested for pretty printing stack traces\n#--------------------------------------------------------------------\n\nif (ARB_BACKTRACE)\n    find_package(Boost REQUIRED\n                 COMPONENTS stacktrace_basic\n                            stacktrace_addr2line)\n    target_link_libraries(arbor-private-deps INTERFACE Boost::stacktrace_basic Boost::stacktrace_addr2line ${CMAKE_DL_LIBS})\n    target_compile_definitions(arbor-private-deps INTERFACE WITH_BACKTRACE)\nendif()\n\n# Build modcc flags\n#------------------------------------------------\n\nif(ARB_MODCC)\n    find_program(modcc NAMES ${ARB_MODCC} NO_CMAKE_PATH NO_CMAKE_ENVIRONMENT_PATH NO_CMAKE_SYSTEM_PATH REQUIRED)\n    if(NOT modcc)\n        message(FATAL_ERROR \"Unable to find modcc executable.\")\n    endif()\n    set(ARB_WITH_EXTERNAL_MODCC TRUE)\nelse()\n    set(modcc $<TARGET_FILE:modcc>)\n    set(ARB_WITH_EXTERNAL_MODCC FALSE)\nendif()\nset(ARB_MODCC_FLAGS)\nif(ARB_VECTORIZE)\n    list(APPEND ARB_MODCC_FLAGS \"--simd\")\nendif()\n\n# Random number creation\n# -----------------------------------------------\n\nif(ARB_USE_GPU_RNG AND (ARB_WITH_NVCC OR ARB_WITH_CUDA_CLANG))\n    set(ARB_USE_GPU_RNG_IMPL TRUE)\nelse()\n    set(ARB_USE_GPU_RNG_IMPL FALSE)\nendif()\n\n#----------------------------------------------------------\n# Set up install paths, permissions.\n#----------------------------------------------------------\n\n# Set up install paths according to GNU conventions.\n#\n# GNUInstallDirs picks (e.g.) `lib64` for the library install path on some\n# systems where this is definitely not correct (e.g. Arch Linux). If there\n# are cases where `lib` is inappropriate, we will have to incorporate special\n# case behaviour here.\n\nif(NOT CMAKE_INSTALL_LIBDIR)\n    set(CMAKE_INSTALL_LIBDIR lib)\nendif()\ninclude(GNUInstallDirs)\n\n# Implicitly created directories require permissions to be set explicitly\n# via this CMake variable.\n#\n# Note that this has no effect until CMake version 3.11.\n\nset(CMAKE_INSTALL_DEFAULT_DIRECTORY_PERMISSIONS\n    OWNER_READ\n    OWNER_WRITE\n    OWNER_EXECUTE\n    GROUP_READ\n    GROUP_EXECUTE\n    WORLD_READ\n    WORLD_EXECUTE)\n\n# CMake versions 3.11 and 3.12 ignore this variable for directories\n# implicitly created by install(DIRECTORY ...), which for us corresponds\n# to our doc and include directories. Work-around by trying to install\n# a non-existant file to these locations.\n\nforeach(directory \"${CMAKE_INSTALL_DOCDIR}\" \"${CMAKE_INSTALL_INCLUDEDIR}\")\n    install(FILES _no_such_file_ OPTIONAL DESTINATION \"${directory}\")\nendforeach()\n\n#----------------------------------------------------------\n# Configure targets in sub-directories.\n#----------------------------------------------------------\n\n# arbor-public-headers:\nadd_subdirectory(arbor/include)\n\n# arbor-sup:\nadd_subdirectory(sup)\n\n# modcc, libmodcc:\nadd_subdirectory(modcc)\n\n# arbor, arbor-private-headers:\nadd_subdirectory(arbor)\n\n# arborenv, arborenv-public-headers:\nadd_subdirectory(arborenv)\n\n# arborio, arborio-public-headers:\nadd_subdirectory(arborio)\n\n# unit, unit-mpi, unit-local, unit-modcc\nif (BUILD_TESTING)\n    add_subdirectory(test)\nendif()\n\n# self contained examples:\nadd_subdirectory(example)\n\n# html:\nif (BUILD_DOCUMENTATION)\n    add_subdirectory(doc)\nendif()\n\n# python interface:\nif(ARB_WITH_PYTHON)\n    add_subdirectory(python)\nendif()\n\n#----------------------------------------------------------\n# Generate CMake config/version files for install.\n#----------------------------------------------------------\n\n# Note: each dependency for the arbor library target, private or otherwise,\n# needs to add itself to the arbor-exports EXPORT target in the subdirectory\n# in which they are defined, or none of this will work.\n\nset(cmake_config_dir \"${CMAKE_INSTALL_LIBDIR}/cmake/arbor\")\ninstall(EXPORT arbor-targets NAMESPACE arbor:: DESTINATION \"${cmake_config_dir}\")\n\ninclude(CMakePackageConfigHelpers)\nwrite_basic_package_version_file(\n    \"${CMAKE_CURRENT_BINARY_DIR}/arbor-config-version.cmake\"\n    COMPATIBILITY SameMajorVersion)\n\n# Template file will use contents of arbor_export_dependencies to include the\n# required `find_dependency` statements, and arbor_supported_components will\n# be used to check feature support.\n#\n# To avoid CMake users of the installed arbor library conditionally requiring\n# that they add CUDA to their project language, explicitly munge the import\n# language and library dependencies on the installed target if ARB_WITH_GPU\n# is set, via the variables arbor_override_import_lang and arbor_add_import_libs.\n# arbor_build_config records our build type in a way compatible with the\n# generated export cmake files.\n\nset(arbor_build_config NOCONFIG)\nif(CMAKE_BUILD_TYPE)\n    string(TOUPPER \"${CMAKE_BUILD_TYPE}\" arbor_build_config)\nendif()\n\nset(arbor_override_import_lang)\nset(arbor_add_import_libs)\nset(arborenv_add_import_libs)\nset(arborio_add_import_libs)\n\nif(ARB_WITH_GPU)\n    set(arbor_override_import_lang CXX)\n    set(arbor_add_import_libs ${CUDA_LIBRARIES})\n    set(arborenv_add_import_libs ${CUDA_LIBRARIES})\nendif()\n\n# (We remove old generated one so that the generation happens every time we run cmake.)\nfile(REMOVE \"${CMAKE_CURRENT_BINARY_DIR}/arbor-config.cmake\")\nconfigure_file(\n    \"${CMAKE_CURRENT_SOURCE_DIR}/cmake/arbor-config.cmake.in\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/arbor-config.cmake\"\n    @ONLY)\n\ninstall(\n    FILES\n        \"${CMAKE_CURRENT_BINARY_DIR}/arbor-config.cmake\"\n        \"${CMAKE_CURRENT_BINARY_DIR}/arbor-config-version.cmake\"\n    DESTINATION \"${cmake_config_dir}\")\n\nadd_subdirectory(lmorpho)\n\n[project]\nname = \"arbor\"\ndynamic = [\"version\"]\nreadme = {file = \"README.md\", content-type = \"text/markdown\"}\nlicense = {file = \"LICENSE\"}\ndescription = \"High performance simulation of networks of multicompartment neurons.\"\nrequires-python = \">=3.9\"\nkeywords = [\"simulator\", \"neuroscience\", \"morphological detail\", \"HPC\", \"GPU\", \"C++\"]\nauthors = [\n    {name = \"Arbor Dev Team\", email = \"contact@arbor-sim.org\"}\n]\nmaintainers = [\n    {name = \"Arbor Dev Team\", email = \"contact@arbor-sim.org\"}\n]\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: BSD License\",\n    \"Programming Language :: Python\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Programming Language :: C++\"\n]\ndependencies = [\n    \"numpy\"\n]\n\n[project.scripts]\nmodcc = \"arbor:modcc\"\narbor-build-catalogue = \"arbor:build_catalogue\"\n\n[tool.scikit-build]\ncmake.args = [\n        \"-DARB_WITH_PYTHON=ON\",\n]\nsdist.include = [\"ext/*/.git\"]\nwheel.install-dir = \"arbor\"\nwheel.packages = []\n\n[tool.scikit-build.metadata.version]\nprovider = \"scikit_build_core.metadata.regex\"\ninput = \"VERSION\"\nregex = \"(?P<value>\\\\d+\\\\.\\\\d+\\\\.\\\\d+(-.+)?)\"\n\n[tool.ruff]\nexclude = [\n    \".direnv\",\n    \".eggs\",\n    \".git\",\n    \".ipynb_checkpoints\",\n    \"_deps\",\n    \".ruff_cache\",\n    \".venv\",\n    \".vscode\",\n    \"build\",\n    \"dist\",\n    \"ext\",\n    \"doc/scripts/inputs.py\",\n    \"doc/scripts/make_images.py\",\n    \".*\",\n    \"spack/package.py\"]\n\nline-length = 88\nindent-width = 4\ntarget-version = \"py312\"\n\n[tool.ruff.lint]\nignore = [ # for black\n           \"E203\", \"E231\",\n           # zealous line lengths\n           \"E501\",\n           # ambiguous varnames I ./. l etc\n           \"E741\",\n           # Ruff doesn't like this rule\n           \"ISC001\",\n           # old school zip\n           \"B905\"]\nselect = [\"C\", \"E\", \"F\", \"W\", \"B\"]\n# Allow fix for all enabled rules (when `--fix`) is provided.\nfixable = [\"ALL\"]\nunfixable = []\nmccabe.max-complexity = 15\n# Allow unused variables when underscore-prefixed.\ndummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"\n\n[tool.ruff.lint.per-file-ignores]\n\"python/example/brunel/analysis.py\" = [\"F405\", \"F403\"]\n\"python/example/brunel/arbor_brunel.py\" = [\"F405\", \"F403\"]\n\"python/example/brunel/nest_brunel.py\" = [\"F405\", \"F403\"]\n\n[tool.ruff.format]\nquote-style = \"double\"\nindent-style = \"space\"\nskip-magic-trailing-comma = false\nline-ending = \"auto\"\n# Disable auto-formatting of code examples in docstrings. Markdown,\n# reStructuredText code/literal blocks and doctests are all supported.\ndocstring-code-format = false\ndocstring-code-line-length = \"dynamic\"\n\n[project.urls]\nhomepage = \"https://arbor-sim.org\"\ndocumentation = \"https://docs.arbor-sim.org\"\nrepository = \"https://github.com/arbor-sim/arbor\"\nchangelog = \"https://github.com/arbor-sim/arbor/releases\"\n\n[build-system]\nrequires = [\n    \"scikit-build-core\",\n    \"numpy\",\n    \"pybind11-stubgen\",\n]\nbuild-backend = \"scikit_build_core.build\"\n\n[tool.cibuildwheel]\nbuild-frontend = \"build\"\nbuild = [\"*linux*\",\"*macosx*\"]\nskip = [\"cp36*\", \"cp37*\", \"cp38*\", \"*musllinux*\"]\ntest-command = \"python -m unittest discover -v -s {project}/python\"\ndependency-versions = \"latest\"\n\n[tool.cibuildwheel.macos]\n#archs = [\"universal2\"]\narchs = [\"arm64\", \"x86_64\"]\nenvironment = { MACOSX_DEPLOYMENT_TARGET = \"12.0.1\" }\n\n[tool.cibuildwheel.linux]\narchs = [\"x86_64\"]\n\n[[tool.cibuildwheel.overrides]]\nselect = \"*-musllinux*\"\n#TBD\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ardoco",
            "repo_link": "https://github.com/ArDoCo/Core",
            "content": {
                "codemeta": "",
                "readme": "# ArDoCo Core\n\n[![Maven Verify](https://github.com/ArDoCo/Core/actions/workflows/verify.yml/badge.svg)](https://github.com/ArDoCo/Core/actions/workflows/verify.yml)\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/io.github.ardoco.core/parent/badge.svg)](https://maven-badges.herokuapp.com/maven-central/io.github.ardoco.core/parent)\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=ArDoCo_Core&metric=alert_status)](https://sonarcloud.io/dashboard?id=ArDoCo_Core)\n[![Latest Release](https://img.shields.io/github/release/ArDoCo/Core.svg)](https://github.com/ArDoCo/Core/releases/latest)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7274034.svg)](https://doi.org/10.5281/zenodo.7274034)\n\nThe goal of the ArDoCo project is to connect architecture documentation and models with Traceability Link Recovery (TLR) while identifying missing or deviating elements (inconsistencies).\nAn element can be any representable item of the model, like a component or a relation.\nTo do so, we first create trace links and then make use of them and other information to identify inconsistencies.\n\nArDoCo is actively developed by researchers of the _[Modelling for Continuous Software Engineering (MCSE) group](https://mcse.kastel.kit.edu)_ of _[KASTEL - Institute of Information Security and Dependability](https://kastel.kit.edu)_ at the [KIT](https://www.kit.edu).\n\nThis **Core** repository contains the framework and core definitions for the other approaches.\nAs such, there is the definition of our pipeline and the data handling as well as the definitions for the various pipeline steps, inputs, outputs, etc.\n\nFor more information about the setup, the project structure, or the architecture, please have a look at the [Wiki](https://github.com/ArDoCo/Core/wiki).\n\n## Maven\n\n```xml\n\n<dependencies>\n\t<dependency>\n\t\t<groupId>io.github.ardoco.core</groupId>\n\t\t<artifactId>framework</artifactId> <!-- or any other subproject -->\n\t\t<version>VERSION</version>\n\t</dependency>\n</dependencies>\n```\n\nFor snapshot releases, make sure to add the following repository\n\n```xml\n\n<repositories>\n\t<repository>\n\t\t<releases>\n\t\t\t<enabled>false</enabled>\n\t\t</releases>\n\t\t<snapshots>\n\t\t\t<enabled>true</enabled>\n\t\t</snapshots>\n\t\t<id>mavenSnapshot</id>\n\t\t<url>https://s01.oss.sonatype.org/content/repositories/snapshots</url>\n\t</repository>\n</repositories>\n```\n\n## Relevant repositories\nThe following is an excerpt of repositories that use this framework and implement the different approaches and pipelines of ArDoCo:\n* [ArDoCo/TLR](https://github.com/ArDoCo/TLR): implementing different traceability link recovery approaches\n* [ArDoCo/InconsistencyDetection](https://github.com/ArDoCo/InconsistencyDetection): implementing inconsistency detection approaches\n* [ArDoCo/LiSSA](https://github.com/ArDoCo/LiSSA): implementing processing of sketches and diagrams for, e.g., TLR\n",
                "dependencies": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n\n  <groupId>io.github.ardoco.core</groupId>\n  <artifactId>parent</artifactId>\n  <version>${revision}</version>\n  <packaging>pom</packaging>\n\n  <name>ArDoCo - The Consistency Analyzer: Core Framework</name>\n  <description>The goal of this project is to connect architecture documentation and models while identifying missing\n        or deviating elements (inconsistencies). An element can be any representable item of the model, like a component\n        or a relation. To do so, we first create trace links and then make use of them and other information to identify\n        inconsistencies. ArDoCo is actively developed by researchers of the Modelling for Continuous Software\n        Engineering (MCSE) group of KASTEL - Institute of Information Security and Dependability at the KIT.</description>\n  <url>https://github.com/ArDoCo/Core</url>\n  <licenses>\n    <license>\n      <name>MIT License</name>\n      <url>https://www.opensource.org/licenses/mit-license.php</url>\n    </license>\n  </licenses>\n  <developers>\n    <developer>\n      <id>Hossiphi</id>\n      <name>Sophie Corallo</name>\n      <email>sophie.corallo@kit.edu</email>\n      <url>https://mcse.kastel.kit.edu/staff_sophie_corallo.php</url>\n      <organization>KASTEL</organization>\n      <organizationUrl>https://mcse.kastel.kit.edu/</organizationUrl>\n      <timezone>GMT+1</timezone>\n    </developer>\n    <developer>\n      <id>dfuchss</id>\n      <name>Dominik Fuchss</name>\n      <email>dominik.fuchss@kit.edu</email>\n      <url>https://mcse.kastel.kit.edu/staff_dominik_fuchss.php</url>\n      <organization>KASTEL</organization>\n      <organizationUrl>https://mcse.kastel.kit.edu/</organizationUrl>\n      <timezone>GMT+1</timezone>\n    </developer>\n    <developer>\n      <id>Gram21</id>\n      <name>Jan Keim</name>\n      <email>jan.keim@kit.edu</email>\n      <url>https://mcse.kastel.kit.edu/staff_Keim_Jan.php</url>\n      <organization>KASTEL</organization>\n      <organizationUrl>https://mcse.kastel.kit.edu/</organizationUrl>\n      <timezone>GMT+1</timezone>\n    </developer>\n  </developers>\n\n  <scm>\n    <connection>scm:git:git://github.com/ArDoCo/Core.git</connection>\n    <developerConnection>scm:git:ssh://github.com:ArDoCo/Core.git</developerConnection>\n    <tag>HEAD</tag>\n    <url>http://github.com/ArDoCo/Core/tree/main</url>\n  </scm>\n  <issueManagement>\n    <system>GitHub Issues</system>\n    <url>https://github.com/ArDoCo/Core/issues</url>\n  </issueManagement>\n  <distributionManagement>\n    <snapshotRepository>\n      <id>ossrh</id>\n      <url>https://s01.oss.sonatype.org/content/repositories/snapshots</url>\n    </snapshotRepository>\n  </distributionManagement>\n\n  <properties>\n    <revision>2.0.0-SNAPSHOT</revision>\n    <ardoco.version>${revision}</ardoco.version>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n    <project.source.encoding>UTF-8</project.source.encoding>\n    <java.version>21</java.version>\n    <maven.compiler.source>${java.version}</maven.compiler.source>\n    <maven.compiler.target>${java.version}</maven.compiler.target>\n    <maven.compiler.release>${java.version}</maven.compiler.release>\n\n    <!-- Plugin Versions -->\n    <slf4j.version>2.0.14</slf4j.version>\n    <spotless.version>2.43.0</spotless.version>\n    <junit.version>5.11.0</junit.version>\n    <eclipse-collections.version>12.0.0.M3</eclipse-collections.version>\n    <jackson.version>2.17.2</jackson.version>\n    <javaparser.version>3.25.8</javaparser.version>\n    <error-prone.version>2.31.0</error-prone.version>\n    <mockito.version>5.2.0</mockito.version>\n    <maven-jar-plugin.version>3.3.0</maven-jar-plugin.version>\n    <jgrapht.version>1.5.2</jgrapht.version>\n\n    <sonar.projectKey>ArDoCo_Core</sonar.projectKey>\n    <sonar.moduleKey>${project.groupId}:${project.artifactId}</sonar.moduleKey>\n    <sonar.organization>ardoco</sonar.organization>\n    <sonar.host.url>https://sonarcloud.io</sonar.host.url>\n    <sonar.coverage.jacoco.xmlReportPaths>${project.basedir}/../${aggregate.report.dir},\n            ${project.basedir}/../../${aggregate.report.dir}</sonar.coverage.jacoco.xmlReportPaths>\n    <aggregate.report.dir>report/target/site/jacoco-aggregate/jacoco.xml</aggregate.report.dir>\n    <argLine>-Xmx4g -Xss256m</argLine>\n    <stanford.corenlp.version>4.5.6</stanford.corenlp.version>\n    <doclint.options>all,-missing</doclint.options>\n  </properties>\n\n  <dependencyManagement>\n    <dependencies>\n      <dependency>\n        <groupId>com.fasterxml.jackson.core</groupId>\n        <artifactId>jackson-annotations</artifactId>\n        <version>${jackson.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.fasterxml.jackson.core</groupId>\n        <artifactId>jackson-core</artifactId>\n        <version>${jackson.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.fasterxml.jackson.core</groupId>\n        <artifactId>jackson-databind</artifactId>\n        <version>${jackson.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.fasterxml.jackson.datatype</groupId>\n        <artifactId>jackson-datatype-jdk8</artifactId>\n        <version>${jackson.version}</version>\n      </dependency>\n\n      <!-- Kotlin -->\n      <dependency>\n        <groupId>com.fasterxml.jackson.module</groupId>\n        <artifactId>jackson-module-kotlin</artifactId>\n        <version>${jackson.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>com.tngtech.archunit</groupId>\n        <artifactId>archunit-junit5</artifactId>\n        <version>1.3.0</version>\n      </dependency>\n      <dependency>\n        <groupId>commons-io</groupId>\n        <artifactId>commons-io</artifactId>\n        <version>2.16.1</version>\n      </dependency>\n\n      <dependency>\n        <groupId>org.apache.commons</groupId>\n        <artifactId>commons-lang3</artifactId>\n        <version>3.14.0</version>\n      </dependency>\n      <dependency>\n        <groupId>org.apache.commons</groupId>\n        <artifactId>commons-text</artifactId>\n        <version>1.12.0</version>\n      </dependency>\n\n      <dependency>\n        <groupId>org.apache.httpcomponents.client5</groupId>\n        <artifactId>httpclient5</artifactId>\n        <version>5.3.1</version>\n      </dependency>\n      <dependency>\n        <groupId>org.assertj</groupId>\n        <artifactId>assertj-core</artifactId>\n        <version>3.26.3</version>\n        <scope>test</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.collections</groupId>\n        <artifactId>eclipse-collections</artifactId>\n        <version>${eclipse-collections.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.collections</groupId>\n        <artifactId>eclipse-collections-api</artifactId>\n        <version>${eclipse-collections.version}</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.jgit</groupId>\n        <artifactId>org.eclipse.jgit</artifactId>\n        <version>6.9.0.202403050737-r</version>\n      </dependency>\n      <dependency>\n        <groupId>org.mockito</groupId>\n        <artifactId>mockito-core</artifactId>\n        <version>${mockito.version}</version>\n        <scope>test</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.mockito</groupId>\n        <artifactId>mockito-inline</artifactId>\n        <version>${mockito.version}</version>\n        <scope>test</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.mockito</groupId>\n        <artifactId>mockito-junit-jupiter</artifactId>\n        <version>${mockito.version}</version>\n        <scope>test</scope>\n      </dependency>\n      <dependency>\n        <groupId>org.reflections</groupId>\n        <artifactId>reflections</artifactId>\n        <version>0.10.2</version>\n      </dependency>\n      <dependency>\n        <groupId>org.slf4j</groupId>\n        <artifactId>log4j-over-slf4j</artifactId>\n        <version>${slf4j.version}</version>\n      </dependency>\n\n      <!-- Testing -->\n      <dependency>\n        <groupId>org.slf4j</groupId>\n        <artifactId>slf4j-simple</artifactId>\n        <version>${slf4j.version}</version>\n      </dependency>\n    </dependencies>\n  </dependencyManagement>\n\n  <dependencies>\n    <dependency>\n      <groupId>com.google.errorprone</groupId>\n      <artifactId>error_prone_core</artifactId>\n      <version>${error-prone.version}</version>\n    </dependency>\n    <dependency>\n      <groupId>org.junit.jupiter</groupId>\n      <artifactId>junit-jupiter-api</artifactId>\n      <version>${junit.version}</version>\n      <scope>test</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.junit.jupiter</groupId>\n      <artifactId>junit-jupiter-engine</artifactId>\n      <version>${junit.version}</version>\n      <scope>test</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.junit.jupiter</groupId>\n      <artifactId>junit-jupiter-params</artifactId>\n      <version>${junit.version}</version>\n      <scope>test</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.junit.vintage</groupId>\n      <artifactId>junit-vintage-engine</artifactId>\n      <version>${junit.version}</version>\n      <scope>test</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.slf4j</groupId>\n      <artifactId>slf4j-api</artifactId>\n      <version>${slf4j.version}</version>\n    </dependency>\n  </dependencies>\n  <repositories>\n    <repository>\n      <id>mavenCentral</id>\n      <url>https://repo1.maven.org/maven2/</url>\n    </repository>\n    <repository>\n      <releases>\n        <enabled>false</enabled>\n      </releases>\n      <snapshots>\n        <enabled>true</enabled>\n      </snapshots>\n      <id>mavenSnapshot</id>\n      <url>https://s01.oss.sonatype.org/content/repositories/snapshots</url>\n    </repository>\n  </repositories>\n\n  <build>\n    <pluginManagement>\n      <plugins>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-assembly-plugin</artifactId>\n          <version>3.7.1</version>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-compiler-plugin</artifactId>\n          <version>3.13.0</version>\n          <configuration>\n            <release>${java.version}</release>\n            <source>${java.version}</source>\n            <target>${java.version}</target>\n            <encoding>UTF-8</encoding>\n            <fork>true</fork>\n            <compilerArgs>\n              <arg>-XDcompilePolicy=simple</arg>\n              <arg>-Xplugin:ErrorProne</arg>\n              <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED</arg>\n              <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED</arg>\n              <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.main=ALL-UNNAMED</arg>\n              <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.model=ALL-UNNAMED</arg>\n              <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED</arg>\n              <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.processing=ALL-UNNAMED</arg>\n              <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED</arg>\n              <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED</arg>\n              <arg>-J--add-opens=jdk.compiler/com.sun.tools.javac.code=ALL-UNNAMED</arg>\n              <arg>-J--add-opens=jdk.compiler/com.sun.tools.javac.comp=ALL-UNNAMED</arg>\n            </compilerArgs>\n            <meminitial>128m</meminitial>\n            <maxmem>512m</maxmem>\n            <annotationProcessorPaths>\n              <path>\n                <groupId>com.google.errorprone</groupId>\n                <artifactId>error_prone_core</artifactId>\n                <version>${error-prone.version}</version>\n              </path>\n            </annotationProcessorPaths>\n          </configuration>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-failsafe-plugin</artifactId>\n          <version>3.2.5</version>\n          <executions>\n            <execution>\n              <goals>\n                <goal>integration-test</goal>\n                <goal>verify</goal>\n              </goals>\n              <phase>integration-test</phase>\n            </execution>\n          </executions>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-gpg-plugin</artifactId>\n          <version>3.2.4</version>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-install-plugin</artifactId>\n          <version>3.1.2</version>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-jar-plugin</artifactId>\n          <version>3.4.2</version>\n        </plugin>\n        <plugin>\n          <groupId>org.apache.maven.plugins</groupId>\n          <artifactId>maven-surefire-plugin</artifactId>\n          <version>3.2.5</version>\n        </plugin>\n        <plugin>\n          <groupId>org.codehaus.mojo</groupId>\n          <artifactId>flatten-maven-plugin</artifactId>\n          <version>1.6.0</version>\n          <configuration>\n            <updatePomFile>true</updatePomFile>\n            <flattenMode>resolveCiFriendliesOnly</flattenMode>\n            <pomElements>\n              <dependencyManagement>expand</dependencyManagement>\n              <dependencies>expand</dependencies>\n            </pomElements>\n          </configuration>\n          <executions>\n            <execution>\n              <id>flatten.clean</id>\n              <goals>\n                <goal>clean</goal>\n              </goals>\n              <phase>clean</phase>\n            </execution>\n            <execution>\n              <id>flatten</id>\n              <goals>\n                <goal>flatten</goal>\n              </goals>\n              <phase>process-resources</phase>\n            </execution>\n          </executions>\n        </plugin>\n        <plugin>\n          <groupId>org.sonatype.plugins</groupId>\n          <artifactId>nexus-staging-maven-plugin</artifactId>\n          <version>1.7.0</version>\n          <extensions>true</extensions>\n          <configuration>\n            <serverId>ossrh</serverId>\n            <nexusUrl>https://s01.oss.sonatype.org/</nexusUrl>\n            <autoReleaseAfterClose>true</autoReleaseAfterClose>\n          </configuration>\n        </plugin>\n      </plugins>\n    </pluginManagement>\n    <plugins>\n      <plugin>\n        <groupId>com.diffplug.spotless</groupId>\n        <artifactId>spotless-maven-plugin</artifactId>\n        <version>${spotless.version}</version>\n        <configuration>\n          <formats>\n            <format>\n              <includes>\n                <include>*.md</include>\n                <include>.gitignore</include>\n              </includes>\n              <trimTrailingWhitespace />\n              <endWithNewline />\n              <indent>\n                <tabs>true</tabs>\n                <spacesPerTab>4</spacesPerTab>\n              </indent>\n            </format>\n          </formats>\n          <!-- define a language-specific format -->\n          <java>\n            <eclipse>\n              <!--suppress UnresolvedMavenProperty -->\n              <file>${maven.multiModuleProjectDirectory}/formatter.xml</file>\n            </eclipse>\n            <removeUnusedImports />\n            <licenseHeader>\n              <!--suppress UnresolvedMavenProperty -->\n              <file>${maven.multiModuleProjectDirectory}/license-header</file>\n            </licenseHeader>\n            <importOrder>\n              <!--suppress UnresolvedMavenProperty -->\n              <file>${maven.multiModuleProjectDirectory}/spotless.importorder</file>\n            </importOrder>\n          </java>\n          <pom>\n            <sortPom>\n              <encoding>UTF-8</encoding>\n              <keepBlankLines>true</keepBlankLines>\n              <indentBlankLines>false</indentBlankLines>\n              <nrOfIndentSpace>2</nrOfIndentSpace>\n              <expandEmptyElements>false</expandEmptyElements>\n              <spaceBeforeCloseEmptyElement>true</spaceBeforeCloseEmptyElement>\n              <sortDependencies>groupId,artifactId</sortDependencies>\n              <sortDependencyExclusions>groupId,artifactId</sortDependencyExclusions>\n              <sortDependencyManagement>groupId,artifactId</sortDependencyManagement>\n              <sortPlugins>groupId,artifactId</sortPlugins>\n              <sortProperties>false</sortProperties>\n              <sortModules>true</sortModules>\n              <sortExecutions>true</sortExecutions>\n              <predefinedSortOrder>recommended_2008_06</predefinedSortOrder>\n            </sortPom>\n          </pom>\n          <ratchetFrom>origin/main</ratchetFrom>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-javadoc-plugin</artifactId>\n        <version>3.7.0</version>\n        <executions>\n          <execution>\n            <id>attach-javadocs</id>\n            <goals>\n              <goal>jar</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-source-plugin</artifactId>\n        <version>3.3.1</version>\n        <executions>\n          <execution>\n            <id>attach-sources</id>\n            <goals>\n              <goal>jar-no-fork</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.codehaus.mojo</groupId>\n        <artifactId>flatten-maven-plugin</artifactId>\n        <version>1.6.0</version>\n        <configuration>\n          <updatePomFile>true</updatePomFile>\n          <flattenMode>resolveCiFriendliesOnly</flattenMode>\n        </configuration>\n        <executions>\n          <execution>\n            <id>flatten.clean</id>\n            <goals>\n              <goal>clean</goal>\n            </goals>\n            <phase>clean</phase>\n          </execution>\n          <execution>\n            <id>flatten</id>\n            <goals>\n              <goal>flatten</goal>\n            </goals>\n            <phase>process-resources</phase>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.codehaus.mojo</groupId>\n        <artifactId>versions-maven-plugin</artifactId>\n        <version>2.16.2</version>\n        <configuration>\n          <ruleSet>\n            <rule>\n              <groupId>*</groupId>\n              <ignoreVersion>\n                <type>regex</type>\n                <version>.+-(alpha|Alpha|beta|Beta|RC).*</version>\n              </ignoreVersion>\n            </rule>\n          </ruleSet>\n        </configuration>\n      </plugin>\n      <plugin>\n        <groupId>org.jacoco</groupId>\n        <artifactId>jacoco-maven-plugin</artifactId>\n        <version>0.8.12</version>\n        <executions>\n          <execution>\n            <id>prepare-agent</id>\n            <goals>\n              <goal>prepare-agent</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n      <plugin>\n        <groupId>org.sonatype.plugins</groupId>\n        <artifactId>nexus-staging-maven-plugin</artifactId>\n        <configuration>\n          <serverId>ossrh</serverId>\n          <nexusUrl>https://s01.oss.sonatype.org/</nexusUrl>\n          <autoReleaseAfterClose>true</autoReleaseAfterClose>\n        </configuration>\n      </plugin>\n    </plugins>\n  </build>\n\n  <profiles>\n    <profile>\n      <id>deployment</id>\n      <activation>\n        <activeByDefault>false</activeByDefault>\n      </activation>\n      <modules>\n        <module>framework</module>\n        <module>pipeline-core</module>\n      </modules>\n      <build>\n        <plugins>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-compiler-plugin</artifactId>\n            <executions>\n              <execution>\n                <id>default-testCompile</id>\n                <goals>\n                  <goal>testCompile</goal>\n                </goals>\n                <phase>test-compile</phase>\n                <configuration>\n                  <skip>true</skip>\n                </configuration>\n              </execution>\n            </executions>\n          </plugin>\n          <plugin>\n            <groupId>org.apache.maven.plugins</groupId>\n            <artifactId>maven-gpg-plugin</artifactId>\n            <executions>\n              <execution>\n                <id>sign-artifacts</id>\n                <goals>\n                  <goal>sign</goal>\n                </goals>\n                <phase>verify</phase>\n                <configuration>\n                  <keyname>2673EE7DF64D33426A93D642E88F0DA2FB06A126</keyname>\n                </configuration>\n              </execution>\n            </executions>\n          </plugin>\n        </plugins>\n      </build>\n    </profile>\n    <profile>\n      <id>complete</id>\n      <activation>\n        <activeByDefault>true</activeByDefault>\n      </activation>\n      <modules>\n        <module>framework</module>\n        <module>pipeline-core</module>\n        <module>report</module>\n      </modules>\n    </profile>\n  </profiles>\n</project>\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/arosics",
            "repo_link": "https://git.gfz-potsdam.de/danschef/arosics",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/atomec",
            "repo_link": "https://github.com/atomec-project/atoMEC",
            "content": {
                "codemeta": "",
                "readme": "![image](https://github.com/atomec-project/atoMEC/blob/develop/docs/source/img/logos/atoMEC_horizontal2.png)\n\n# atoMEC: Average-Atom Code for Matter under Extreme Conditions\n\n[![docs](https://github.com/atomec-project/atoMEC/actions/workflows/gh-pages.yml/badge.svg)](https://github.com/atomec-project/atoMEC/actions/workflows/gh-pages.yml)\n[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/release/python-3100/)\n[![image](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n[![codecov](https://codecov.io/gh/atomec-project/atoMEC/branch/develop/graph/badge.svg?token=V66CJJ3KPI)](https://codecov.io/gh/atomec-project/atoMEC)\n[![CodeFactor](https://www.codefactor.io/repository/github/atomec-project/atomec/badge)](https://www.codefactor.io/repository/github/atomec-project/atomec)\n\natoMEC is a python-based average-atom code for simulations of high energy density phenomena such as in warm dense matter.\nIt is designed as an open-source and modular python package.\n\natoMEC uses Kohn-Sham density functional theory, in combination with an average-atom approximation,\nto solve the electronic structure problem for single-element materials at finite temperature.\n\nMore information on the average-atom methodology and Kohn-Sham density functional theory can be found (for example) in this [paper](https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.4.023055) and references therein.\n\nThis repository is structured as follows:\n```\n├── atoMEC : source code\n├── docs : sphinx documentation\n├── examples : simple examples to get you started with the package\n└── tests : CI tests\n```\n\n\n## Installation\n\nThe latest stable release of `atoMEC` can be installed via `pip`. It is first necessary to install the `libxc` package from a tarball source, because it currently has no official wheels distribution on PyPI. This step takes some time.\n\n```sh\n$ pip install https://gitlab.com/libxc/libxc/-/archive/6.2.2/libxc-6.2.2.tar.gz\n$ pip install atoMEC\n```\n\nNote that atoMEC does not (yet) support Windows installation (please see the section below on supported operating systems).\n\nRead on for instructions on how to install `atoMEC` from source, using the recommended `pipenv` installation route.\n\n### Installation via `pipenv`\n\nFirst, clone the atoMEC repository and ``cd`` into the main directory.\n\n* It is recommended to install atoMEC inside a virtual environment. Below, we detail how to achive this with [pipenv](https://pypi.org/project/pipenv/).\n\n  This route is recommended because `pipenv` automatically creates a virtual environment and manages dependencies. Note that `pyblibxc` is automatically installed in this case, so there is no need to install it separately.\n\n  1. First, install `pipenv` if it is not already installed, for example via `pip install pipenv` (or see [pipenv](https://pypi.org/project/pipenv/) for installation instructions)\n  2. Next, install `atoMEC`'s dependencies with `pipenv install` (use `--dev` option to install the test dependencies in the same environment)\n  3. Use `pipenv shell` to activate the virtual environment\n  4. Install atoMEC with `pip install atoMEC` (for developers: `pip install -e .`)\n  5. Now run scripts from inside the `atoMEC` virtual environment, e.g. `python examples/simple.py`\n\n* Run the tests (see Testing section below) and report any failures (for example by raising an issue).\n\n### Supported operating systems\n\n* **Linux and macOS**: atoMEC has been installed on various linux distributions and macOS, and is expected to work for most distributions and versions\n* **Windows**: atoMEC does **not** support Windows installation. This is due to the dependency on `pylibxc` which currently lacks Windows support. We are looking into ways to make the dependency on `pylibxc` optional, in order to allow installation on Windows. However, this is not currently a priority.\n\n\n### Supported Python versions\n\n* atoMEC has been tested and is expected to work for all Python versions >= 3.8 and <= 3.12\n* atoMEC does not work for Python <= 3.7\n* Until 09.10.2023 (release 1.4.0), all development and CI testing was done with Python 3.8. As of this date, development and CI testing is done with Python 3.12.\n* Python 3.12 is therefore the recommended version for atoMEC >= 1.4.0, since this is used for the current testing and development environment\n\n\n## Running\nYou can familiarize yourself with the usage of this package by running the example scripts in `examples/`.\n\n## Contributing to atoMEC\nWe welcome your contributions, please adhere to the following guidelines when contributing to the code:\n* In general, contributors should develop on branches based off of `develop` and merge requests should be to `develop`\n* Please choose a descriptive branch name\n* Merges from `develop` to `master` will be done after prior consultation of the core development team\n* Merges from `develop` to `master` are only done for code releases. This way we always have a clean `master` that reflects the current release\n* Code should be formatted using [black](https://pypi.org/project/black/) style\n\n## Testing\n* First, install the test requirements (if not already installed in the virtual env with `pipenv install --dev`):\n```sh\n# activate environment first (optional)\n$ pipenv shell\n\n# install atoMEC as editable project in current directory (for developers)\n$ pip install -e .[tests]\n\n# alternatively install package from PyPI with test dependencies\n$ pip install atoMEC[tests]\n```\n\n* To run the tests:\n```sh\n$ pytest --cov=atoMEC --random-order tests/\n```\n\n### Build documentation locally (for developers)\n\nInstall the prerequisites:\n```sh\n$ pip install -r docs/requirements.txt\n```\n\n1. Change into `docs/` folder.\n2. Run `make apidocs`.\n3. Run `make html`. This creates a `_build` folder inside `docs`. You may also want to use `make html SPHINXOPTS=\"-W\"` sometimes. This treats warnings as errors and stops the output at first occurrence of an error (useful for debugging rST syntax).\n4. Open `docs/_build/html/index.html`.\n5. `make clean` if required (e.g. after fixing errors) and building again.\n\n## Developers\n### Scientific Supervision\n- Attila Cangi ([Center for Advanced Systems Understanding](https://www.casus.science/))\n- Eli Kraisler ([Hebrew University of Jerusalem](https://en.huji.ac.il/en))\n\n### Core Developers and Maintainers\n- Tim Callow ([Center for Advanced Systems Understanding](https://www.casus.science/))\n- Daniel Kotik ([Center for Advanced Systems Understanding](https://www.casus.science/))\n\n### Contributions (alphabetical)\n- Nathan Rahat ([Hebrew University of Jerusalem](https://en.huji.ac.il/en))\n- Ekaterina Tsvetoslavova Stankulova ([Center for Advanced Systems Understanding](https://www.casus.science/))\n\n## Citing atoMEC\nIf you use code from this repository in a published work, please cite\n\n1. T. J. Callow, D. Kotik, E. Kraisler, and A. Cangi, \"atoMEC: An open-source average-atom Python code\", _Proceedings of the 21st Python in Science Conference_, edited by Meghann Agarwal, Chris Calloway, Dillon Niederhut, and David Shupe (2022), pp. 31 – 39\n2. The DOI corresponding to the specific version of atoMEC that you used (DOIs are listed at [Zenodo.org](https://doi.org/10.5281/zenodo.5205718))\n\n",
                "dependencies": "[[source]]\nurl = \"https://pypi.python.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\nnumpy = \">=1.20.3\"\nscipy = \">=1.6.3\"\nmendeleev = \">=0.7.0\"\ntabulate = \">=0.8.9\"\njoblib = \">=1.0.1\"\npylibxc = {file = \"https://gitlab.com/libxc/libxc/-/archive/6.2.2/libxc-6.2.2.tar.gz\"}\n\n[requires]\npython_version = \"3.12\"\n\n[dev-packages]\npytest = \">=7.1.3\"\npytest-cov = \">=4.0.0\"\npytest-random-order = \">=1.0.4\"\npytest-lazy-fixture = \">=0.6.3\"\nflake8 = \">=6.1.0\"\npydocstyle = \">=6.3.0\"\nblack = \">=23.11.0\"\n\n[build-system]\nrequires = [\n    \"setuptools>=53\",\n    \"wheel\"\n]\nbuild-backend = \"setuptools.build_meta\"\n\nnumpy>=1.20.3\nscipy>=1.6.3\nmendeleev>=0.7.0\ntabulate>=0.8.9\njoblib>=1.0.1\n\nfrom setuptools import setup, find_packages\n\n\nwith open(\"README.md\") as f:\n    readme = f.read()\n\nwith open(\"LICENSE\") as f:\n    license = f.read()\n\nextras = {\n    \"dev\": [\"bump2version\"],\n    \"docs\": open(\"docs/requirements.txt\").read().splitlines(),\n    \"tests\": open(\"tests/requirements.txt\").read().splitlines(),\n}\n\nsetup(\n    name=\"atoMEC\",\n    version=\"1.4.0\",\n    description=\"KS-DFT average-atom code\",\n    long_description=readme,\n    long_description_content_type=\"text/markdown\",\n    author=\"Tim Callow et al.\",\n    author_email=\"t.callow@hzdr.de\",\n    url=\"https://github.com/atomec-project/atoMEC\",\n    license=license,\n    packages=find_packages(exclude=(\"tests\", \"docs\", \"examples\")),\n    install_requires=open(\"requirements.txt\").read().splitlines(),\n    extras_require=extras,\n    python_requires=\">=3.6\",\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/autogaita",
            "repo_link": "https://github.com/mahan-hosseini/AutoGaitA",
            "content": {
                "codemeta": "",
                "readme": "![AutoGaitA](https://github.com/mahan-hosseini/AutoGaitA/blob/main/autogaita/resources/logo.png?raw=true)\n![Repository Active](https://www.repostatus.org/badges/latest/active.svg)\n[![Test AutoGaitA](https://github.com/mahan-hosseini/AutoGaitA/actions/workflows/autogaita_test_and_black.yml/badge.svg)](https://github.com/mahan-hosseini/AutoGaitA/actions/workflows/autogaita_test_and_black.yml)\n![Python](https://img.shields.io/badge/python-v3.10+-blue.svg)\n[![PyPI - Version](https://img.shields.io/pypi/v/autogaita)](https://pypi.org/project/autogaita/)\n![license: GPL v3](https://img.shields.io/badge/license-GPLv3-blue.svg)\n[![paper: biorxiv](https://img.shields.io/badge/paper-biorxiv-blue)](https://doi.org/10.1101/2024.04.14.589409) \n\n![Black](https://img.shields.io/badge/code%20style-black-000000.svg)\n[![X URL](https://img.shields.io/twitter/url?url=https%3A%2F%2Fx.com%2Fautogaita&style=social&label=updates)](https://x.com/autogaita)\n\n# Automated Gait Analysis in Python 🐸\n\n- AutoGaitA simplifies, accelerates, and standardises gait analyses after body posture tracking in 2D with [DeepLabCut](https://github.com/DeepLabCut/DeepLabCut) and [SLEAP](https://github.com/talmolab/sleap) or marker-based (such as [Simi Motion](http://www.simi.com/en/products/movement-analysis/simi-motion-2d3d.html?type=rss%2F)) as well as marker-less methods for obtaining 3D coordinates. \n- AutoGaitA's first-level tools provide a wide range of automated kinematic analyses for each input video and AutoGaitA Group allows the comparison of up to six groups. \n- AutoGaitA enables comparisons to be made across experimental conditions, species, disease states or genotypes. \n- Despite being developed with gait data, AutoGaitA can be utilised for the analysis of any motor behaviour.\n\n## Getting Started\n\n***Note!** [Our documentation](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?usp=sharing) provides step-by-step walkthroughs of how to install autogaita for **[Windows](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?tab=t.0#heading=h.28j6wu2vamre)** and **[Mac](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?tab=t.0#heading=h.ljmdh7hfayyx)***\n\nIt is strongly recommended that a separate virtual environment for AutoGaitA is created (note that the approach below creates the virtual environment to your current directory):\n\n- Create the virtual environment:\n    - `python -m venv env_gaita`\n\n- After creation, activate the virtual environment via:\n    - *Windows:* `env_gaita\\Scripts\\activate`\n    - *Mac:* `source env_gaita/bin/activate`\n\n- Once activated, install AutoGaitA in the virtual environment via pip: `pip install autogaita`.\n\n- Access the main user interface via: `python -m autogaita`.\n\n- To update to the latest release (see the *Releases* panel on the right for the latest release) activate the virtual environment and: `pip install autogaita -U`. \n\n## Demo Video\n*Check out the video below for a demonstration of AutoGaitA's main workflow!*\n<p><a href=\"https://youtu.be/_HIZVuUzpzk?feature=shared\">\n<img src=\"https://github.com/mahan-hosseini/AutoGaitA/blob/main/autogaita/resources/pic_to_demo_for_repo.png\" width=\"550\">\n\n## Tutorials & Examples\n\n### Walkthrough Tutorial Videos  \n\n**[The AutoGaitA YouTube Channel](https://youtube.com/playlist?list=PLCn5T7K_H8K56NIcEsfDK664OP7cN_Bad&si=mV5p2--nYvbofkPh) provides tutorials for file preparation and instructions on how to use AutoGaitA. This includes in-depth explanations of all details, (main & advanced) configurations, possibilities, and outputs.**\n\n*Please note that tutorial videos might not always reflect the most up-to-date version of our toolbox, especially in the beginning when things are regularly changing. We will make sure to record new videos whenever there are major changes though. Last tutorial-update was with v0.4.0. (August 2024).*\n\n### Example Data\nWe provide an example dataset in the **example data** folder of this repository, with a set of mice walking over differently wide beams and both the beam as well as body coordinates being tracked with DLC. Note that this dataset was used in our tutorial videos introducing *AutoGaitA DLC*, *AutoGaitA Group* and in our video explaining file preparation for *AutoGaitA DLC*.  We further provide a **group** folder there that can be used alongside the *AutoGaitA Group* tutorial to confirm that users generate the same set of results following our instructions.\n\n### Annotation Table Examples and Templates\nAnnotation Table example and template files for *AutoGaitA DLC* and *AutoGaitA Universal 3D* can be found in the [**annotation tables**](https://github.com/mahan-hosseini/AutoGaitA/tree/main/annotation%20tables) folder of this repository.\n\nUsers are advised to read the **General Recommendations** section of that folder, use the template to enter their data's timestamp information and to then compare the resulting table with our example to check formatting. Users working with ImageJ/FIJI are encouraged to check out the [AnnotationTable-Plugin](https://github.com/luca-flemming/AnnotationTable-Plugin) developed by our contributor Luca Flemming.\n\n## Documentation\n\n**[The AutoGaitA Documentation](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?usp=sharing) provides complete guidelines on installation, file preparation, AutoGaitA GUIs, using AutoGaitA via the command line, installing FFmpeg for rotating 3D PCA videos, lists known issues and FAQ.**  \n\n## Two important options\n\n### Custom joints & angles\n**We strongly advise** users to pay attention to the *custom joints and angles* windows of AutoGaitA's first level toolboxes. Please see the relevant links below. These windows allow users to customise which columns of their data should be analysed and how angles should be computed. \n\nBy default, *AutoGaitA DLC* and *AutoGaitA Universal 3D* implement standard values for mouse and human locomotion, respectively. If your analysis deviates from these standards (e.g. by focussing on another behaviour or a different species) **you must change these values!** \n\n**Find out more about *AutoGaitA's custom joints and angles:***\n- [YouTube - AutoGaitA DLC Advanced Configuration](https://youtu.be/MP9g9kXRE_Q?feature=shared) \n- [YouTube - AutoGaitA Universal 3D (prev. called Simi)](https://youtu.be/rTG-Fc9XI9g?feature=shared) \n- [Documentation - AutoGaitA DLC](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?tab=t.0#heading=h.20bg7b7ymt0b)\n- [Documentation - AutoGaitA Universal 3D](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?tab=t.0#heading=h.uz61bpmua7qz)\n\n### Bin number of step cycle normalisation\nAn important step in AutoGaitA is normalising step cycles (or instances of other behaviours) to a uniform length before calculating the video-level average. This uniform length is called *bin number*, must be set by users and defaults to a value of 25.\n\nStep cycles are normalised via averaging temporally adjacent data points if their original length was larger than the bin number and repeating values if they were shorter originally. Examples are provided here: \n- [Documentation/AutoGaitA DLC/Main Configuration/Option #6](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?tab=t.0#heading=h.bboivsfqr2lz).\n\n**We strongly advise** users to think carefully about an appropriate bin number for their datasets. The correct value varies and depends strongly on the studied species, behaviour and the frame rate of cameras.\n\n## Analysing other behaviours - AutoCyclA 🚴\nEven though AutoGaitA's main focus is to automate and standardise gait analyses, our toolbox can be used to automate the analyses of any rhythmic behaviour of interest. For a proof-of-principle demonstration and an introduction of the general workflow of such analyses, see **[AutoCyclA - Automated Cycling Analysis with AutoGaitA.](https://github.com/mahan-hosseini/AutoGaitA/tree/main/autocycla)**\n\n## Updating AutoGaitA\nIt is strongly recommended that AutoGaitA is kept up to date since new features and important bugfixes are provided regularly. \n\nAutoGaitA's cfg files and dictionaries sometimes change as a result, which means that previously generated first-level *Results* folders cannot always be analysed with AutoGaitA Group after an update. In such cases, it is recommended to re-run first-level analyses. \n\nWe document each version's cfg-changes in [AutoGaitA Releases](https://github.com/mahan-hosseini/AutoGaitA/releases), which is particularly relevant for users wrapping custom scripts around AutoGaitA's functions.\n\n## Reference\nIf you use this code or data please [cite our preprint](https://www.biorxiv.org/content/10.1101/2024.04.14.589409v1).\n\n## License\nAutoGaitA is licensed under [GPL v3.0](https://github.com/mahan-hosseini/AutoGaitA/blob/main/LICENSE) and Forschungszentrum Jülich GmbH holds all copyrights. \n\nThe AutoGaitA software is provided without warranty of any kind, express or implied, including, but not limited to, the implied warranty of fitness for a particular purpose.\n\n## Authors\n[Mahan Hosseini](https://github.com/mahan-hosseini)\n\n## Contributors\n[Luca Flemming](https://github.com/luca-flemming) - Undergraduate Student\n\n[Nicholas del Grosso](https://github.com/nickdelgrosso) - RSE Advisor\n\n## Contributing\nIf you would like to contribute to the AutoGaitA toolbox, feel free to open a pull request or contact us at autogaita@fz-juelich.de! \n\nWe are looking forward to your input and ideas 😊\n\n## Archive\nWe have archived the resources of outdated AutoGaitA versions here:\n\n- v0.4.1 - [Documentation](https://docs.google.com/document/d/1Y4wrrsjs0ybLDKPzE2LAatqPDq9jtwjIuk4M0jRZ3wE/edit?usp=sharing)\n- v0.3.1 - [YouTube Tutorials](https://youtube.com/playlist?list=PLCn5T7K_H8K776DLuXKoPsUpI6Yb0NU33&si=7ZAAvcrPxR7WsB8a) & [Documentation](https://docs.google.com/document/d/11mJd7jUHk7joQ0BdZT98CJRrIANdyosMQMJGFtp6yR4/edit?usp=sharing)\n\n",
                "dependencies": "# import\nfrom setuptools import setup, find_packages\nimport platform\n\n# list of across-platform dependencies\ninstall_requires = [\n    \"customtkinter>=5.2\",\n    \"pandas>=2.0\",\n    \"numpy>=1.24\",\n    \"seaborn>=0.13\",\n    \"matplotlib>=3.7\",\n    \"scikit-learn>=1.2\",\n    \"pingouin>=0.5\",\n    \"scipy>=1.11\",\n    \"ffmpeg-python>=0.2\",\n    \"openpyxl>=3.1\",\n    \"pillow>=10.3\",\n    \"h5py>=3.11\",\n]\n\n# add platform-specific dependencies\nif platform.system() == \"Darwin\":\n    install_requires.append(\"pyobjc\")\n\n# call setup function\nsetup(\n    name=\"autogaita\",\n    python_requires=\">=3.10\",\n    version=\"1.0.3\",  # rc == release candidate (before release is finished)\n    author=\"Mahan Hosseini\",\n    description=\"Automatic Gait Analysis in Python. A toolbox to streamline and standardise the analysis of kinematics across species after ML-based body posture tracking. Despite being optimised for gait analyses, AutoGaitA has the potential to be used for any kind of kinematic analysis.\",\n    packages=find_packages(),\n    include_package_data=True,\n    package_data={\"\": [\"*.txt\", \"*.rst\", \"*.png\", \"*.icns\", \"*.ico\", \"*.json\"]},\n    install_requires=install_requires,\n    extras_require={\"dev\": [\"pytest\", \"hypothesis\"]},\n    license=\"GPLv3\",\n    url=\"https://github.com/mahan-hosseini/AutoGaitA/\",\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/autopq",
            "repo_link": "https://github.com/SMEISEN/AutoPQ",
            "content": {
                "codemeta": "",
                "readme": "# AutoPQ: Automated point forecast-based quantile forecasts\n\nAutoPQ addresses three challenges:\n- Many state-of-the-art forecasting methods are still point forecasts and remain unused for probabilistic forecasts\n- According to the no-free-lunch theorem, no forecasting method exists that excels in all forecasting tasks\n- Smart grid applications typically require forecasts with customized probabilistic characteristics\n\n## Methodology\n\nThe underlying idea of AutoPQ is to generate a probabilistic forecast based on an arbitrary point forecast using a conditional Invertible Neural Network (cINN) and to make corresponding design decisions automatically, aiming to increase the probabilistic performance. To account for different computing systems and performance requirements, two variants are available: AutoPQ-default suitable for standard computing systems achieving competitive forecasting performance, and AutoPQ-advanced requiring High-Performance Computing (HPC) systems to further increase forecasting performance for smart grid applications with high decision costs.\n\n![concept_pipeline_github](https://github.com/SMEISEN/AutoPQ/assets/33990691/40344260-77ee-4515-9964-16875b9383d7)\n\n## Installation\n\nTo install this project, perform the following steps.\n1) Clone the project\n2) Open a terminal of the virtual environment where you want to use the project\n3) cd AutoPQ\n4) pip install . or pip install -e . if you want to install the project editable.\n\n## How to use\n\nExemplary evaluations using AutoPQ are given in the examples folder.\n\n### Hyperparameter optimization\n\n- The default configuration optimizes the sampling hyperparameter $\\lambda_\\text{q}$ for generating samples in the latent space of the cINN.\n- The advanced configuration simultaneously optimizes the point forecasting method's hyperparameters $\\boldsymbol{\\lambda_\\text{p}}$ and the sampling hyperparameter $\\lambda_\\text{q}$.\n\n### Evaluation types\n\nThe evaluation trains the models using the training data sub-set, optimizes hyperparameters based on the validation data sub-set, and makes probabilistic forecasts for the test data sub-set.\n\n## Citation\n\nIf you use this method please cite the corresponding papers:\n> Kaleb Phipps, Stefan Meisenbacher, Benedikt Heidrich, Marian Turowski, Ralf Mikut, and Veit Hagenmeyer. 2023. Loss-customised probabilistic energy time series forecasts using automated hyperparameter optimisation. In Proceedings of the 14th ACM International Conference on Future Energy Systems (e-Energy ’23), Association for Computing Machinery, New York, NY, USA, 271–286. [https://doi.org/10.1145/3575813.3595204](https://doi.org/10.1145/3575813.3595204)\n\n> Stefan Meisenbacher et al. 2024. AutoPQ: Automated point forecast-based quantile forecasts. In preparation.\n\n## Funding\n\nThis project is funded by the Helmholtz Association under the Program “Energy System Design” and the Helmholtz Association's Initiative and Networking Fund through Helmholtz AI.\n\n## References\n\nThe cINN is based on:\n> B. Heidrich, M. Turowski, K. Phipps, K. Schmieder, W. Süß, R. Mikut, and V. Hagenmeyer, “Controlling non-stationarity and periodicities in time series generation using conditional invertible neural networks”, Applied Intelligence, vol. 53, no. 8, pp. 8826–8843, 2023.\n\nGenerating probabilistic forecasts by sampling in the cINN's latent space is based on:\n> K. Phipps, B. Heidrich, M. Turowski, M. Wittig, R. Mikut, and V. Hagenmeyer, “Generating probabilistic forecasts from arbitrary point forecasts using a conditional invertible neural network”, Applied Intelligence, 2024.\n\nOptimization of the sampling hyperparameter is performed using [Hyperopt](https://github.com/hyperopt/hyperopt)\n> J. Bergstra, D. Yamins, and D. Cox, “Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures”, in Proceedings of the 30th International Conference on Machine Learning, ser. ICML ’13, Proceedings of Machine Learning Research, PMLR, 2013, pp. 115–123.\n\nOptimization of the point forecasting method's hyperparameters is performed using [Propulate](https://github.com/Helmholtz-AI-Energy/propulate)\n> O. Taubert, M. Weiel, D. Coquelin, A. Farshian, C. Debus, A. Schug, A. Streit, and M. Götz, “Massively parallel genetic optimization through asynchronous propagation of populations”, in High Performance Computing, A. Bhatele, J. Hammond, M. Baboulin, and C. Kruse, Eds., Cham, Switzerland: Springer Nature, 2023, pp. 106–124.\n\nThe Load-BW data is taken from the Open Power System Data (OPSD) portal:\n> F. Wiese et al., “Open Power System Data: Frictionless data for electricity system modelling”, Applied Energy, vol. 236, pp. 401–409, 2019.\n\nThe Load-GCP data set is taken from the UCI Machine Learning Repository:\n> A. Trindade, Electricity load diagrams 2011-2014, UCI Machine Learning Repository, 2015.\n\nThe Mobility data set is taken from the UCI Machine Learning Repository:\n> H. Fanaee-T, Bike sharing dataset, UCI Machine Learning Repository, 2013.\n\nThe Price, PV, and WP data are from the price, solar power, and wind power forecasting tracks of the Global Energy Forecasting Competition (GEFCom) 2014:\n> T. Hong, P. Pinson, S. Fan, H. Zareipour, A. Troccoli, and R. J. Hyndman, “Probabilistic energy forecasting: Global energy forecasting competition 2014 and beyond”, International Journal of Forecasting, vol. 32, no. 3, pp. 896–913, 2016.\n\n",
                "dependencies": "git+https://github.com/SMEISEN/pyWATTS.git@453133590beb64866ec7576fa58bebaf38a84ed5#egg=pywatts\ngit+https://github.com/SMEISEN/propulate.git@main#egg=propulate\ngit+https://github.com/SMEISEN/msvr@master#egg=msvr\ngit+https://github.com/SMEISEN/sktime@main#egg=sktime\npytorch-forecasting==0.10.3\npytorch-lightning==1.9.0\nproperscoring==0.1\nFrEIA==0.2\ngluonts==0.12.2\ntensorflow>=2\nnumpy<1.24\nxgboost==1.7.3\nscikit-learn==1.1.3\nhyperopt==0.2.7\npmdarima==2.0.3\ntbats==1.1.3\npvlib==0.9.3\n--extra-index-url https://download.pytorch.org/whl/cu116\ntorch==1.13.1+cu116\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/autopv",
            "repo_link": "https://github.com/SMEISEN/AutoPV",
            "content": {
                "codemeta": "",
                "readme": "# AutoPV: Automated photovoltaic forecasts with limited information using an ensemble of pre-trained models\nAutoPV addresses three challenges:\n- Missing information about the PV mounting configuration (tilt and azimuth angles, mixed-oriented configurations)\n- Missing or limited training data for PV model design (cold-start problem)\n- Adaption to drifting PV power generation capabilities during operation (e.g. age-related degradation, soiling, maintenance)\n\n## Methodology\n\nThe underlying idea of AutoPV is to describe the arbitrary mounting configuration of a new PV plant as a convex linear combination of outputs from a sufficiently diverse ensemble pool of PV models of the same region. AutoPV incorporates three steps: i) create the ensemble model pool, ii) form the ensemble output by an optimally weighted sum of the scaled model outputs in the pool, and iii) rescale the ensemble output with the new PV plant’s peak power rating.\n\n![pipeline](https://github.com/SMEISEN/AutoPV/assets/33990691/56363d4b-5418-427b-b723-bf14255804ce)\n\n\n## Installation\n\nTo install this project, perform the following steps.\n1) Clone the project\n2) Open a terminal of the virtual environment where you want to use the project\n3) cd AuroPV\n4) pip install . or pip install -e . if you want to install the project editable.\n\n## How to use\n\nExemplary evaluations using AutoPV are given in the examples folder.\n\n### Model pools\n- The default model pool is based on physical-inspired modeling and uses 12 models (tilt: 15°, 45°, 75°, azimuth: 0°, 90°, 180°, 270°). The default model pool is suitable for situations where no data of nearby PV plants are available.\n- The nearby plants model pool uses machine learning-based modeling to create the models using data from nearby PV plants. The nearby plants model pool can represent shading if it is present in the nearby PV plants.\n\n### Evaluation types\n- The offline evaluation optimizes the ensemble weights using the entire training data and does not adapt the weights over time.\n- The online evaluation cyclically adapts the weights based on the testing data (cold-start) and does not require training data.\n\n## Citation\n\nIf you use this method please cite the corresponding paper:\n> Stefan Meisenbacher, Benedikt Heidrich, Tim Martin, Ralf Mikut, and Veit Hagenmeyer. 2023. AutoPV: Automated photovoltaic forecasts with limited information using an ensemble of pre-trained models. In Proceedings of the Fourteenth ACM International Conference on Future Energy Systems (e-Energy ’23). Association for Computing Machinery, New York, NY, USA, 386–414. https://doi.org/10.1145/3575813.3597348\n\n## Funding\nThis project is funded by the Helmholtz Association under the Program “Energy System Design” and the Helmholtz Association’s Initiative and Networking Fund through Helmholtz AI.\n\n## References\n\nThe example data includes weather measurements from DWD:\n> Deutscher Wetterdienst. 2023. Historical 10-minute station observations of solar incoming radiation, longwave downward radiation, pressure, air temperature, and mean wind speed for Germany. https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes\n\n",
                "dependencies": "-e git+https://github.com/SMEISEN/pyWATTS.git@453133590beb64866ec7576fa58bebaf38a84ed5#egg=pywatts\nhyperopt=0.2.7\nray=2.0.1\npvlib=0.9.3\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/autowp",
            "repo_link": "https://github.com/SMEISEN/AutoWP",
            "content": {
                "codemeta": "",
                "readme": "# AutoWP: Automated wind power forecasts with limited computing resources using an ensemble of diverse wind power curves\n\nAutoWP addresses two challenges:\n- Achieving good accuracy in Wind Power (WP) forecasting with low computational effort\n- Handling regular or irregular interventions in the WP generation capabilities\n\n## Methodology\n\nThe underlying idea of AutoWP is to represent a new WP turbine as a convex linear combination of WP curves from a sufficiently diverse ensemble. The method consists of three steps: i) create the ensemble of normalized WP curves, ii) form the normalized ensemble WP curve by the optimally weighted sum of the WP curves in the ensemble, and iii) re-scale the ensemble WP curve with the new WP turbine’s peak power rating.\n\n![autowp_pipeline](https://github.com/SMEISEN/AutoWP/assets/33990691/46b4f23c-4a8f-423e-8e20-a24e2b02ff5d)\n\n## Installation\n\nTo install this project, perform the following steps.\n1) Clone the project\n2) Open a terminal of the virtual environment where you want to use the project\n3) cd AutoWP\n4) pip install . or pip install -e . if you want to install the project editable.\n\n## How to use\n\nExemplary evaluations using AutoWP are given in the examples folder.\n\n### Model pool\n\nThe model pool is based on a selection of 10 WP curves from the [windpowerlib](https://github.com/wind-python/windpowerlib). The selection reduces redundancy and preserves diversity. Since these WP curves provided by turbine Original Equipment Manufacturers (OEMs) have the hub height as reference height, height correction of the wind speed forecast based on the wind profile power law is used. \n\n### Evaluation types\n\nThe offline evaluation optimizes the ensemble weights using the entire training data and does not adapt the weights over time.\n\n## Citation\n\nIf you use this method please cite the corresponding paper:\n> S. Meisenbacher et al, “AutoWP: Automated wind power forecasts with limited computing resources using an ensemble of diverse wind power curves”, 2024, in preparation.\n\n## Funding\n\nThis project is funded by the Helmholtz Association under the Program “Energy System Design” and the Helmholtz Association?s Initiative and Networking Fund through Helmholtz AI.\n\n## References\n\nThe example data is from the wind power forecasting track of the Global Energy Forecasting Competition (GEFCom) 2014:\n> T. Hong, P. Pinson, S. Fan, H. Zareipour, A. Troccoli, and R. J. Hyndman, “Probabilistic energy forecasting: Global energy forecasting competition 2014 and beyond”, International Journal of Forecasting, vol. 32, no. 3, pp. 896–913, 2016.\n\n",
                "dependencies": "git+https://github.com/SMEISEN/pyWATTS.git@453133590beb64866ec7576fa58bebaf38a84ed5#egg=pywatts\nwindpowerlib==0.2.1\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/aviator",
            "repo_link": "https://github.com/CCB-SB/Aviator",
            "content": {
                "codemeta": "",
                "readme": "# Aviator\n\n### [https://ccb-compute2.cs.uni-saarland.de/aviator](https://ccb-compute2.cs.uni-saarland.de/aviator)\n\nAviator is a web-server monitoring the availability of other published web-servers.\nIt allows researchers to monitor their own tools or to asses if a tool they would like to\naccess is temporarily or permanently offline.\n\nAviator is composed of two modules:\n\n### - [Tool List](https://ccb-compute2.cs.uni-saarland.de/aviator/tools): web-servers collected automatically from literature\n### - [Aviator-enabled](https://ccb-compute2.cs.uni-saarland.de/aviator/aviator-enabled): web-servers manually added by their authors\n\nThe web-server URL or an API endpoint provided by the authors are queried twice per day. In addition to providing an availability overview we provide the possibility for authors to be notified if their webserver is offline for an unexpected period of time. \n\nTo add your published web-server to Aviator a simple [API endpoint](https://ccb-compute2.cs.uni-saarland.de/aviator/aviator-enable) and a [registration](https://ccb-compute2.cs.uni-saarland.de/aviator/register) is needed. \n\n## License\n\n[MIT © CCB-SB](/LICENSE)\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/awi-gpt",
            "repo_link": "https://github.com/CliDyn",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/awi-pangaea-ai-hub",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/base-repo",
            "repo_link": "https://github.com/kit-data-manager/base-repo",
            "content": {
                "codemeta": "",
                "readme": "# KIT Data Manager - Base Repository Service\n\n[![Build Status](https://github.com/kit-data-manager/base-repo/actions/workflows/gradle.yml/badge.svg)](https://github.com/kit-data-manager/base-repo/actions/workflows/gradle.yml)\n[![Codecov](https://codecov.io/gh/kit-data-manager/base-repo/branch/master/graph/badge.svg)](https://codecov.io/gh/kit-data-manager/base-repo)\n![License](https://img.shields.io/github/license/kit-data-manager/base-repo.svg)\n[![SQAaaS badge shields.io](https://img.shields.io/badge/Docker-white?logo=docker)](https://github.com/kit-data-manager/base-repo/pkgs/container/base-repo)\n[![SQAaaS badge shields.io](https://img.shields.io/badge/sqaaas%20software-silver-lightgrey)](https://api.eu.badgr.io/public/assertions/onNKx_lhTn68bPKnMAg-eQ \"SQAaaS silver badge achieved\")\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7660036.svg)](https://doi.org/10.5281/zenodo.7660036)\n\nThis project contains the repository service microservice for the KIT DM infrastructure. The service provides\ndata resource management, e.g. register DataCite-oriented metadata and upload/download content to data resources.\n\n## How to build\n\nIn order to build this microservice you'll need:\n\n* Java SE Development Kit 17 or higher\n\nAfter obtaining the sources change to the folder where the sources are located perform the following steps:\n\n```bash\nuser@localhost:/home/user/base-repo$ ./gradlew -Dprofile=minimal build\nRunning gradle version: 7.4.2\nBuilding base-repo version: 1.5.5\nJDK version: 11\nUsing minimal profile for building base-repoo\n<-------------> 0% EXECUTING [0s]\n[...]\nuser@localhost:/home/user/base-repo$\n```\n\nThe Gradle wrapper will now take care of downloading the configured version of Gradle, checking out all required libraries, build these\nlibraries and finally build the base-repo microservice itself. As a result, a fat jar containing the entire service is created at 'build/libs/base-repo.jar'.\n\n## How to start\n\n### Prerequisites\n\n* PostgreSQL 9.1 or higher\n* RabbitMQ 3.7.3 or higher (in case you want to use the messaging feature, which is recommended)\n* Elastic 8.X or higher (in case you want to use the search feature)\n\n### Setup\n\nBefore you are able to start the repository microservice, you have provide a configuration file according to your local setup.\nTherefor, copy the file 'config/application-default.properties' to your project folder, rename it to 'application.properties' and customize it as required. Special attentioned should be payed to the datasource url as well as to the repository base path. Also, the property 'repo.messaging.enabled' should be changed to 'true' in case you want to use the messaging feature of the repository.\n\nAs soon as you finished modifying 'application.properties', you may start the repository microservice by executing the following command inside the project folder,\ne.g. where the service has been built before:\n\n```bash\nuser@localhost:/home/user/base-repo$ ./build/libs/base-repo.jar\n\n  .   ____          _            __ _ _\n /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n  '  |____| .__|_| |_|_| |_\\__, | / / / /\n =========|_|==============|___/=/_/_/_/\n :: Spring Boot ::        (v2.7.5)\n[...]\n1970-01-01 00:00:00.000  INFO 56918 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''\n\n```\n\nIf your 'application.properties' is not located inside the project folder you can provide it using the command line argument --spring.config.location=<PATH_TO_APPLICATION.PROPERTIES>\n\nAs soon as the microservice is started, you can browse to\n\n<http://localhost:8090/swagger-ui.html>\n\nin order to see available RESTful endpoints and their documentation.\n\n### Enhanced Startup\n\nAt certain points, base-repo offers and will offer extension points allowing to add custom features that are not part of the default distribution, e.g. custom message handlers. If you are familiar with software development, it might be no big deal to include an additional dependency to 'build.gradle' of base-repo. However, in some cases this might not be desirable or possible. Therefor, base-repo allows to place additional libraries required at runtime in a separate folder which is then loaded as soon as the microservice starts and made available using the dependency injection feature of Spring Boot.\n\nIn order to tell Spring Boot where to look for additional libraries, you have to define an environment variable JAVA_OPTS looking as follows:\n\n```bash\nexport JAVA_OPTS=\"-cp .:./config:./base-repo.jar -Dloader.path=./base-repo.jar,./lib/,.\"\n```\n\nThe first part '-cp' has to contain three elements divided by ':':\n\n1. The configuration folder where your application.properties is located (this element can be omitted, if application.properties\nis located in the current folder),\n2. the current folder,\n3. and the microservice jar file.\n\nThe second part '-Dloader.path' basically contains the same information as '-cp' but with the difference, that the config folder is not required, whereas the folder\ncontaining all additional libraries has to be provided, in our case it's './lib'.\n\nPlease keep in mind that all arguments shown in the example assume, that you are in the same folder where your microservice jar file is located and that you start the service\nby calling './base-repo.jar'. If your microservice jar is located elsewhere, you should consider to provide absolute paths for all arguments above.\nIn case you want to choose a different folder for placing your additional libraries, you have to rename it in JAVA_OPTS accordingly.\n\nWhat you now have to do before you start the microservice is to place additional jar files (and required dependencies!) in the 'lib' folder. At the next startup, the new functionality should be available.\n\n## More Information\n\n* [Getting Started & Documentation](https://kit-data-manager.github.io/webpage/base-repo/index.html)\n* [API documentation](https://kit-data-manager.github.io/webpage/base-repo/documentation/api-docs.html)\n* [Docker container](https://github.com/kit-data-manager/base-repo/pkgs/container/base-repo%2Fbase-repo)\n* [Information about the DataCite metadata schema](https://schema.datacite.org/)\n\n## License\n\nThe KIT Data Manager is licensed under the Apache License, Version 2.0.\n\n",
                "dependencies": "plugins { \n    id 'org.springframework.boot' version '3.3.5'\n    id 'io.spring.dependency-management' version '1.1.6'\n    id 'io.freefair.lombok' version '8.10.2'\n    id 'io.freefair.maven-publish-java' version '8.10.2'\n    id 'org.owasp.dependencycheck' version '11.1.0'\n    id 'org.asciidoctor.jvm.convert' version '4.0.3'\n    id 'net.researchgate.release' version '3.0.2'\n    id 'com.gorylenko.gradle-git-properties' version '2.4.2'\n    id 'java'\n    id 'jacoco'\n}\n\njar {\n    archiveBaseName = 'base-repo'\n    // version is defined in file 'gradle.properties'\n    archiveVersion = System.getenv('version')\n}\n\nrepositories {\n    mavenLocal() \n    mavenCentral()\n}\n\n\n//configurations {\n //   all*.exclude module : 'spring-boot-starter-logging'\n//}\n\next {\n    set('javersVersion', \"7.6.3\")\n    set('springBootVersion', \"3.2.1\")\n    set('springDocVersion', \"2.6.0\")\n    set('keycloakVersion', \"19.0.0\")\n\n    // directory for generated code snippets during tests\n    snippetsDir = file(\"build/generated-snippets\")\n}\n\nprintln \"Running gradle version: $gradle.gradleVersion\"\nprintln \"Building ${name} version: ${version}\"\nprintln \"JDK version: ${JavaVersion.current()}\"\n\nsourceCompatibility = JavaVersion.VERSION_17\ntargetCompatibility = JavaVersion.VERSION_17\n\nif (System.getProperty('profile') == 'minimal') {\n    println 'Using minimal profile for building ' + project.getName()\n    apply from: 'gradle/profile-minimal.gradle'   \n} else {\n    println 'Using default profile executing all tests for building ' + project.getName()\n    apply from: 'gradle/profile-complete.gradle'\n}\n\ndependencies {\n   // boot starter\n    implementation \"org.springframework.boot:spring-boot-starter-validation\"    \n    implementation \"org.springframework.boot:spring-boot-starter-data-jpa\"\n    implementation \"org.springframework.boot:spring-boot-starter-data-rest\"\n    implementation \"org.springframework.boot:spring-boot-starter-mail\"\n    implementation \"org.springframework.boot:spring-boot-starter-security\"\n    implementation \"org.springframework.boot:spring-boot-starter-actuator\"\n    implementation 'org.springframework.data:spring-data-elasticsearch:5.3.5'\n\n    implementation \"org.springframework:spring-messaging:6.1.14\"\n    \n    // cloud support\n    implementation \"org.springframework.cloud:spring-cloud-starter-config:4.1.3\"\n    implementation \"org.springframework.cloud:spring-cloud-starter-netflix-eureka-client:4.1.3\"\n    implementation \"org.springframework.cloud:spring-cloud-gateway-mvc:4.1.5\"\n    implementation 'de.codecentric:spring-boot-admin-starter-client:3.3.5'\n\n    // springdoc\n    implementation \"org.springdoc:springdoc-openapi-starter-webmvc-ui:${springDocVersion}\"\n    implementation \"org.springdoc:springdoc-openapi-starter-common:${springDocVersion}\"\n    implementation \"org.springdoc:springdoc-openapi-starter-webmvc-api:${springDocVersion}\"\n\n    implementation \"edu.kit.datamanager:repo-core:1.2.3\"\n    implementation \"edu.kit.datamanager:service-base:1.3.2\"\n\n    //implementation \"com.github.victools:jsonschema-generator:4.23.0\"\n\n    //Keycloak\n    // implementation \"org.keycloak:keycloak-spring-boot-starter:${keycloakVersion}\"\n    implementation \"com.nimbusds:nimbus-jose-jwt:9.46\"\n   // implementation \"io.jsonwebtoken:jjwt-api:0.11.5\"\n    //implementation \"io.jsonwebtoken:jjwt-impl:0.11.5\"\n    //implementation \"io.jsonwebtoken:jjwt-jackson:0.11.5\"\n\n    implementation \"org.javers:javers-core:${javersVersion}\"\n    implementation \"com.github.fge:json-patch:1.9\"\n    implementation \"com.bazaarvoice.jolt:jolt-core:0.1.7\"    \n    implementation \"com.bazaarvoice.jolt:json-utils:0.1.8\"    \n\n   // implementation \"javax.xml.bind:jaxb-api:2.3.1\"\n\n    runtimeOnly    \"org.apache.httpcomponents:httpclient:4.5.14\"\n\n     // driver for postgres\n    implementation \"org.postgresql:postgresql:42.7.4\"\n    //driver for h2\n    implementation \"com.h2database:h2:2.3.232\"\n\n    testImplementation \"org.springframework.restdocs:spring-restdocs-mockmvc:3.0.2\"\n    testImplementation \"org.springframework.boot:spring-boot-starter-test\"\n    testImplementation \"org.springframework:spring-test\"\n    testImplementation \"org.springframework.security:spring-security-test\"\n   \n    //Java 11 Support \n    testImplementation \"org.mockito:mockito-inline:5.2.0\"\n    testImplementation \"junit:junit:4.13.2\"\n}\n\nif (project.hasProperty('release')) {\n    println 'Using \\'release\\' profile for building ' + project.getName()\n    apply from: 'gradle/profile-deploy.gradle'\n}\n\ntest {\n    testLogging {\n        outputs.upToDateWhen {false}\n        showStandardStreams = true\n    }\n   environment \"spring.config.location\", \"classpath:/test-config/\"\n}\n\ntasks.withType(Test) {\n    testLogging {\n        events 'started', 'passed'\n    }\n}\n\nspringBoot {    \n    buildInfo()\n}\n\ngitProperties {\n    failOnNoGitDirectory = false\n}\n\nbootJar {\n    println 'Create bootable jar...'\n    archiveFileName = \"${archiveBaseName.get()}.${archiveExtension.get()}\"\n    duplicatesStrategy = DuplicatesStrategy.EXCLUDE\n    manifest {\n        attributes 'Main-Class': 'org.springframework.boot.loader.launch.PropertiesLauncher'\n    }\n    launchScript()\n}\n\njacoco {\n    toolVersion = \"0.8.12\"\n}\n\n// task for printing project name.\ntask printProjectName {\n    doLast {\n        println \"${project.name}\"\n    }\n}\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/basic",
            "repo_link": "https://github.com/marrlab/BaSiC",
            "content": {
                "codemeta": "",
                "readme": "# BaSiC\n\nMatlab code accompanying \n\n**A BaSiC Tool for Background and Shading Correction of Optical Microscopy Images**\n\nby Tingying Peng, Kurt Thorn, Timm Schroeder, Lichao Wang, Fabian J Theis, Carsten Marr\\*, Nassir Navab\\*, Nature Communication 8:14836 (2017). [doi: 10.1038/ncomms14836](http://www.nature.com/articles/ncomms14836).\n\nBaSiC is licensed under \n\n[Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License](https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode)\n\nIt is free for academic use and please contact us for any commercial use.\n\n## Usage\n![BaSiC corrects both spatial uneven illumination of microscopy images and temporal background bleaching for time-lapse movies.](images/usage.png)\n\n\n## Demo\n\nDownload demo data examples from [Dropbox](https://www.dropbox.com/s/plznvzdjglrse3h/Demoexamples.zip?dl=0) and run matlab files under example folder.\n\n## ImageJ/Fiji Plugin\nBaSiC is also available as a ImageJ/Fiji Plugin.\n\n\n### Installation instruction\n\nNote: If you do not have Fiji installed on your computer, you can download it from [Fiji website](http://fiji.sc/).\n\n\n### Install via Fiji Updater\n\n1. Start Fiji and run the updater (\"Help->Update Fiji\")\n2. Select the \"Manage Update Sites\" button at the bottom-left of the updater window\n3. Scroll the list of available update sites to find \"BaSiC\" (Note: If you cannot find \"BaSiC\" in the list, select \"Add Update Sites\", Change the name field from default \"New\" to \"BaSiC\", set the URL field to http://sites.imagej.net/BaSiC/)\n4. Check the box at the left of \"BaSiC\"\n5. Select \"Close\" \n6. Select \"Apply Changes\" \n7. Restart Fiji. BaSiC should appear in the Plugins menu.\n\nFrom now on, running the Fiji updater will also check for BaSiC updates, and install them if they are available.\n\n\n### Install manually\n\nPlease download [BaSiC Plugin](https://github.com/QSCD/BaSiC/blob/master/BaSiCPlugin.zip) from this repository. \n\n1. Copy “BaSiC_.jar” to the “$FIJIROOT/plugins” folder of your Fiji/ImageJ installation.\n2. Copy all dependent jar files in the \"Dependent\" folder to your Fiji/ImageJ \"$FIJIROOT/jars\" directory.\n\n\n### Troubleshooting\n\nIf you get the error message \n\n\"java.lang.NoSuchMethodError: edu.emory.mathcs.utils.ConcurrencyUtils.submit\"\n\nmake sure that in your Fiji/ImageJ \"$FIJIROOT/jars\" directory, there is only one version of each jar from the \"Dependent\" folder. Particularly, delete jtransforms-2.4.jar and replace it with our jtransform.jar.\n\n## Issues\nIf you have any issues concerning BaSiC, please report them in the [Issues](https://github.com/QSCD/BaSiC/issues) section of this GitHub repository and we will try to find a solution.\n\n\n## BaSiCPy\nPython version of BaSiC implementation (https://github.com/peng-lab/BaSiCPy)\n\n\n\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/basicpy",
            "repo_link": "https://github.com/peng-lab/BaSiCPy",
            "content": {
                "codemeta": "",
                "readme": "# BaSiCPy\nA python package for background and shading correction of optical microscopy images\n\n[![PyPI](https://img.shields.io/pypi/v/basicpy.svg)](https://pypi.org/project/basicpy)\n[![Status](https://img.shields.io/pypi/status/basicpy.svg)](https://pypi.org/project/basicpy/)\n[![Python Version](https://img.shields.io/pypi/pyversions/basicpy.svg)](https://python.org)\n[![License](https://img.shields.io/pypi/l/basicpy)](https://github.com/peng-lab/BaSiCPy/blob/main/LICENSE)\n[![Tests](https://github.com/peng-lab/basicpy/workflows/CI/badge.svg)](https://github.com/peng-lab/basicpy/actions?workflow=CI)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Read the Docs](https://img.shields.io/readthedocs/basicpy/latest.svg?label=Read%20the%20Docs)](https://basicpy.readthedocs.io/)\n<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\n[![All Contributors](https://img.shields.io/badge/all_contributors-5-orange.svg?style=flat-square)](#contributors-)\n<!-- ALL-CONTRIBUTORS-BADGE:END -->\n\nBaSiCPy is a python package for background and shading correction of optical microscopy images.\nIt is developed based on the Matlab version of [BaSiC](https://github.com/marrlab/BaSiC) tool with major improvements in the algorithm.\n\nReference:\n- BaSiCPy: A robust and scalable shadow correction tool for optical microscopy images (in prep.)\n- A BaSiC Tool for Background and Shading Correction of Optical Microscopy Images\n  by Tingying Peng, Kurt Thorn, Timm Schroeder, Lichao Wang, Fabian J Theis, Carsten Marr\\*, Nassir Navab\\*, Nature Communication 8:14836 (2017). [doi: 10.1038/ncomms14836](http://www.nature.com/articles/ncomms14836).\n\n\n## Simple examples\n\n|Notebook|Description|Colab Link|\n| :------------------------: |:---------------:| :---------------------------------------------------: |\n| [timelapse_brightfield](https://github.com/peng-lab/BaSiCPy/tree/dev/docs/notebooks/timelapse_brightfield.ipynb)| 100 continuous brightfield frames of a time-lapse movie of differentiating mouse hematopoietic stem cells. | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/peng-lab/BaSiCPy/blob/dev/docs/notebooks/timelapse_brightfield.ipynb) |\n| [timelapse_nanog](https://github.com/peng-lab/BaSiCPy/tree/dev/docs/notebooks/timelapse_nanog.ipynb)| 189 continuous fluorescence frames of a time-lapse movie of differentiating mouse embryonic stem cells, which move much more slower compared to the fast moving hematopoietic stem cells, resulting in a much larger correlation between frames. Note that in this challenging case, the automatic parameters are no longer optimal, so we use the manual parameter setting (larger smooth regularization on both flat-field and dark-field) to improve BaSiC’s performance. | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/peng-lab/BaSiCPy/blob/dev/docs/notebooks/timelapse_nanog.ipynb) |\n| [WSI_brain](https://github.com/peng-lab/BaSiCPy/tree/dev/docs/notebooks/WSI_brain.ipynb)| you can stitch image tiles together to view the effect of shading correction | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/peng-lab/BaSiCPy/blob/dev/docs/notebooks/WSI_brain.ipynb) |\n\nYou can also find examples of running the package at [notebooks folder](https://github.com/peng-lab/BaSiCPy/tree/dev/docs/notebooks). Data used in the examples and a description can be downloaded from [Zenodo](https://doi.org/10.5281/zenodo.6334809).\n\n---\n## Usage\n\nSee [Read the Docs](https://basicpy.readthedocs.io/en/latest/) for the detailed usage.\n\n## Installation\n\n### For Mac (Intel chip), Linux or WSL2 users\n\n Install from PyPI\n\n```console\npip install basicpy\n```\n\nor install the latest development version\n\n```console\ngit clone https://github.com/peng-lab/BaSiCPy.git\ncd BaSiCPy\npip install .\n```\n\n### For Mac users with M1 / M2 chip\n\nBaSiCPy requires [`jax`](https://github.com/google/jax/),\nwhich has potential build issue with M1 chips.\nOne easiest solution is using [Miniforge](https://github.com/conda-forge/miniforge)\nas explained [here](https://github.com/google/jax/issues/5501).\nIn the Miniforge environment, please try the following:\n```bash\nconda install -c conda-forge jax jaxlib\npip install basicpy\n```\n\n### For Windows users\n\nBaSiCPy requires [`jax`](https://github.com/google/jax/) which does not support Windows officially.\nHowever, thanks to [cloudhan/jax-windows-builder](https://github.com/cloudhan/jax-windows-builder), we can install BaSiCPy as follows:\n\n```bash\npip install \"jax[cpu]==0.4.11\" -f https://whls.blob.core.windows.net/unstable/index.html --use-deprecated legacy-resolver\npip install ml-dtypes==0.2.0\npip install basicpy\n```\n\nOne may need to add\n```python\nimport jax\njax.config.update('jax_platform_name', 'cpu')\n```\nat the top of the script to ensure that JAX uses CPU.\n\nFor details and latest updates, see [this issue](https://github.com/google/jax/issues/438).\n\n### Install with dev dependencies\n\n```console\ngit clone https://github.com/peng-lab/BaSiCPy.git\ncd BaSiCPy\npython -m venv venv\nsource venv/bin/activate\npip install -e '.[dev]'\n```\n\n## Development\n\n### bump2version\n\nThis repository uses [bump2version](https://github.com/c4urself/bump2version) to manage dependencies. New releases are pushed to PyPi in the CI pipeline when a new version is committed with a version tag and pushed to the repo.\n\nThe development flow should use the following process:\n1. New features and bug fixes should be pushed to `dev`\n2. When tests have passed a new development version is ready to be release, use `bump2version major|minor|patch`. This will commit and create a new version tag with the `-dev` suffix.\n3. Additional fixes/features can be added to the current development release by using `bump2version build`.\n4. Once the new bugs/features have been tested and a main release is ready, use `bump2version release` to remove the `-dev` suffix.\n\nAfter creating a new tagged version, push to Github and the version will be built and pushed to PyPi.\n\n### All-contributors\n\nThis repository uses [All Contributors](https://allcontributors.org/) to manage the contributor list. Please execute the following to add/update contributors.\n\n```bash\nyarn\nyarn all-contributors add username contribution\nyarn all-contributors generate # to reflect the changes to README.md\n```\n\nFor the possible contribution types, see the [All Contributors documentation](https://allcontributors.org/docs/en/emoji-key).\n\n## Contributors\n\n### Current version\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/Nicholas-Schaub\"><img src=\"https://avatars.githubusercontent.com/u/15925882?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Nicholas-Schaub</b></sub></a><br /><a href=\"#projectManagement-Nicholas-Schaub\" title=\"Project Management\">📆</a> <a href=\"https://github.com/peng-lab/BaSiCPy/pulls?q=is%3Apr+reviewed-by%3ANicholas-Schaub\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#infra-Nicholas-Schaub\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=Nicholas-Schaub\" title=\"Tests\">⚠️</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=Nicholas-Schaub\" title=\"Code\">💻</a> <a href=\"#ideas-Nicholas-Schaub\" title=\"Ideas, Planning, & Feedback\">🤔</a></td>\n    <td align=\"center\"><a href=\"https://github.com/tdmorello\"><img src=\"https://avatars.githubusercontent.com/u/34800427?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Tim Morello</b></sub></a><br /><a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=tdmorello\" title=\"Code\">💻</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=tdmorello\" title=\"Documentation\">📖</a> <a href=\"https://github.com/peng-lab/BaSiCPy/pulls?q=is%3Apr+reviewed-by%3Atdmorello\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=tdmorello\" title=\"Tests\">⚠️</a> <a href=\"#ideas-tdmorello\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"#infra-tdmorello\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a></td>\n    <td align=\"center\"><a href=\"https://github.com/tying84\"><img src=\"https://avatars.githubusercontent.com/u/11461947?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Tingying Peng</b></sub></a><br /><a href=\"#data-tying84\" title=\"Data\">🔣</a> <a href=\"#financial-tying84\" title=\"Financial\">💵</a> <a href=\"#projectManagement-tying84\" title=\"Project Management\">📆</a> <a href=\"#talk-tying84\" title=\"Talks\">📢</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=tying84\" title=\"Code\">💻</a></td>\n    <td align=\"center\"><a href=\"https://github.com/yfukai\"><img src=\"https://avatars.githubusercontent.com/u/5919272?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Yohsuke T. Fukai</b></sub></a><br /><a href=\"#research-yfukai\" title=\"Research\">🔬</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=yfukai\" title=\"Code\">💻</a> <a href=\"#ideas-yfukai\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"https://github.com/peng-lab/BaSiCPy/pulls?q=is%3Apr+reviewed-by%3Ayfukai\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=yfukai\" title=\"Tests\">⚠️</a> <a href=\"#question-yfukai\" title=\"Answering Questions\">💬</a> <a href=\"#infra-yfukai\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a></td>\n    <td align=\"center\"><a href=\"https://github.com/YuLiu-web\"><img src=\"https://avatars.githubusercontent.com/u/70626217?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>YuLiu-web</b></sub></a><br /><a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=YuLiu-web\" title=\"Documentation\">📖</a> <a href=\"#userTesting-YuLiu-web\" title=\"User Testing\">📓</a></td>\n  </tr>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\nFor details on the contribution roles, see the [documentation](https://basicpy.readthedocs.io/en/latest/contributors.html).\n\n\n### Old version (`f3fcf19`), used as the reference implementation to check the approximate algorithm\n- Lorenz Lamm (@LorenzLamm)\n- Mohammad Mirkazemi (@Mirkazemi)\n\n",
                "dependencies": "{\n  \"devDependencies\": {\n    \"all-contributors-cli\": \"^6.20.0\"\n  }\n}\n\n[build-system]\nrequires = [\n    \"setuptools>=42\",\n    \"wheel\"\n]\nbuild-backend = \"setuptools.build_meta\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/beluga",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/bending-stiffness",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Bending-Stiffness",
            "content": {
                "codemeta": "",
                "readme": "![Java](https://img.shields.io/badge/java-%23ED8B00.svg?style=plastic&logo=openjdk&logoColor=white) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/bendingStiffness?style=plastic)\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808969.svg)](https://doi.org/10.5281/zenodo.12808969) [![SWH](https://archive.softwareheritage.org/badge/swh:1:dir:65f4716d51672926f9ae328ea314d969e37534c6/)](https://archive.softwareheritage.org/swh:1:dir:65f4716d51672926f9ae328ea314d969e37534c6;origin=https://github.com/Ramy-Badr-Ahmed/bendingStiffness;visit=swh:1:snp:cf3a5710e567c74b08a7144be79796fb78e9743c;anchor=swh:1:rev:ae6455bbac2db3f8838eb0d69b5ba09e5f50d06e) \n\n### Summary\n\nA java code analysing the Bending Stiffness of Actin Filament Experiment.\n\n- Reads coordinates from `snake.txt` of a measured filament (an example is in the `images` directory)\n- Calculates the mean squared displacement (MSD) and contour length from data\n- Error calculation in persistence length and contour length are saved in `mathematica` directory.\n- (optional) Modify `config.properties` as needed.\n\n### Running the Demo\n\n1. Place your `snake.txt` and `elongation.txt` files in the `data` directory (an example exists).\n2. Compile and run the `Snake` class.\n\n```sh\njavac -d out src/Snake.java\njava -cp out Snake\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/beos",
            "repo_link": "https://gitlab.com/dlr-sy/beos",
            "content": {
                "codemeta": "",
                "readme": "[![PyPi](https://img.shields.io/pypi/v/beos?label=PyPi)](https://pypi.org/project/beos/)\n[![doi](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.13175694-red.svg)](https://zenodo.org/records/13175694)\n[![pipeline status](https://gitlab.com/dlr-sy/boxbeam/badges/master/pipeline.svg)]()\n\n# BEOS\nBEOS is a legacy Fortran-based buckling tool. It is compiled for Python using [f2py](https://numpy.org/doc/stable/f2py).\n> Installation from source requires an active Fortran compiler (ifort, gfortran). \n## Downloading\nUse GIT to get the latest code base. From the command line, use\n```\ngit clone https://gitlab.dlr.de/fa_sw/beos beos\n```\nIf you check out the repository for the first time, you have to initialize all submodule dependencies first. Execute the following from within the repository. \n```\ngit submodule update --init --recursive\n```\nTo update all refererenced submodules to the latest production level, use\n```\ngit submodule foreach --recursive 'git pull origin $(git config -f $toplevel/.gitmodules submodule.$name.branch || echo master)'\n```\n## Installation\nBEOS can be installed from source using [poetry](https://python-poetry.org). If you don't have [poetry](https://python-poetry.org) installed, run\n```\npip install poetry --pre --upgrade\n```\nto install the latest version of [poetry](https://python-poetry.org) within your python environment. Use\n```\npoetry update\n```\nto update all dependencies in the lock file or directly execute\n```\npoetry install\n```\nto install all dependencies from the lock file. Last, you should be able to import BEOS as a python package.\n```python\nimport beos\n```\n## Example\nPlease refer to the linked [repository](https://gitlab.com/dlr-sy/beos) for specific application examples.\n## Contact\n* [Marc Garbade](mailto:marc.garbade@dlr.de)\n## Support\n* [List of Contributors](CONTRIBUTING.md)\n\n",
                "dependencies": "# TOML file to create BEOS\r\n#  \r\n# @note: TOML file            \r\n# Created on 13.12.2022    \r\n# \r\n# @version:  1.0    \r\n# ----------------------------------------------------------------------------------------------\r\n# @requires:\r\n#        - \r\n# \r\n# @change: \r\n#        -    \r\n#    \r\n# @author: garb_ma                                                     [DLR-FA,STM Braunschweig]\r\n# ----------------------------------------------------------------------------------------------\r\n[build-system]\r\nrequires = [\"poetry-core>=1.0\",\"packaging\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n\r\n[tool.poetry]\r\nname = \"beos\"\r\nversion = \"1.3.0\"\r\ndescription = \"Calculation of the buckling behavior of composite shells\"\r\nauthors = [\"Freund, Sebastian <sebastian.freund@dlr.de>\"]\r\nmaintainers = [\"Garbade, Marc <marc.garbade@dlr.de>\"]\r\nlicense = \"MIT\"\r\npackages = [{include=\"**/*\", from=\"bin\"}]\r\nexclude = [\"bin/_*\",\r\n           \"bin/**/.*\"]\r\nrepository = \"https://gitlab.com/dlr-sy/beos\"\r\ndocumentation = \"https://gitlab.com/dlr-sy/beos/-/blob/master/README.md\"\r\nkeywords = [\"analysis\",\"buckling\",\"composite\"]\r\nreadme = \"README.md\"\r\nclassifiers = [\r\n    \"Development Status :: 7 - Inactive\",\r\n    \"Topic :: Scientific/Engineering\",\r\n    \"Programming Language :: Python :: 2\",\r\n    \"Programming Language :: Python :: 3\",\r\n    \"License :: OSI Approved :: MIT License\",\r\n    \"Operating System :: OS Independent\"\r\n]\r\n\r\n[tool.poetry.urls]\r\nChangelog = \"https://gitlab.com/dlr-sy/beos/-/blob/master/CHANGELOG.md\"\r\n\r\n[[tool.poetry.source]]\r\nname = \"dlr-pypi\"\r\nurl = \"https://pypi.python.org/simple\"\r\npriority = \"primary\"\r\n\r\n[[tool.poetry.source]]\r\nname = \"dlr-sy\"\r\nurl = \"https://gitlab.dlr.de/api/v4/groups/541/-/packages/pypi/simple\"\r\npriority = \"supplemental\"\r\n\r\n[tool.poetry.build]\r\nscript = \"config/build.py\"\r\ngenerate-setup-file = false\r\n\r\n[tool.poetry.dependencies]\r\npython = \"~2.7 || ^3.5\"\r\nnumpy = [{version = \"^1.16\",   python = \"~2.7 || ~3.5\"},\r\n         {version = \"^1.18\",   python = \"~3.6\"},\r\n         {version = \"^1.21\",   python = \"~3.7\"},\r\n         {version = \"^1.22\", python = \"~3.8\"},\r\n         {version = \">=1.22,<2\", python = \"^3.9\"}]\r\n\r\n# All optional dependencies\r\nfa-pyutils  = [{version = \"*\", python = \"^3.7\", optional = true}]\r\nPyQt5-Qt5 = [{version = \"5.15.2\", python = \"^3.7\", optional = true}]\r\nPyQt5-sip = [{version = \"~12.12\", python = \"~3.7\", optional = true},\r\n             {version = \">=12.13\", python = \"^3.8\", optional = true}]\r\nvampire = [{version = \">=0.2.5\", python = \"^3.7\", source=\"dlr-sy\", optional = true}]\r\n\r\n# All mandatory development dependencies\r\n[tool.poetry.group.dev.dependencies]\r\ndelis = [{git = \"https://gitlab.dlr.de/fa_sw/stmlab/plugins/delis.git\", develop=true, python=\"^3.7\"}]\r\nmkl = [{version = \"~2022.2\", platform = \"win32\"}]\r\nmkl-devel = [{version = \"~2022.2\", platform = \"win32\"}]\r\ntbb = [{version = \"~2021.7\", platform = \"win32\"}]\r\npyx-core = [{version = \">=1.17\", python = \"~2.7 || ^3.5,<3.7\"},\r\n            {git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake.git\", develop=true, python=\"^3.7\"}]\r\n\r\n[tool.poetry.group.lock.dependencies]\r\nmatplotlib = [{version = \"^2\", python = \"~2.7 || ~3.5\"},\r\n              {version = \"^3\", python = \"~3.6\"},\r\n              {version = \"~3.5\", python = \"~3.7\"},\r\n\t\t\t  {version = \">=3.6\", python = \"^3.8\"}]\r\nsetuptools = [{version = \"^39.0\", python = \"~2.7\"},\r\n              {version = \"^49.0,\", python = \"~3.5\"},\r\n              {version = \"^58.0,\", python = \"~3.6\"},\r\n              {version = \"^64.0\", python = \"~3.7\"},\r\n              {version = \">=64.0\", python = \"^3.8\"}]\r\n\r\n[tool.poetry.extras]\r\ndevel = [\"fa-pyutils\",\"vampire\",\"PyQt5-Qt5\",\"PyQt5-sip\"]\r\n\r\n[tool.pytest.ini_options]\r\naddopts = \"-ra -v -m ''\"\r\nmarkers = [\"long: Marks all long running tests\"]\r\n\r\n[tool.pyxmake.doxygen]\r\nname = \"BEOS\"\r\ntitle = [\"BEOS\", \"BEOS Developer Guide\"]\r\nsource = \"src\"\r\nfiles = [\"beos_data\",\"beos_tools\",\"beos_main\"]\r\noutput = \"doc/beos_core\"\r\n\r\n[tool.pyxmake.coverage]\r\nname = \"beos\"\r\nsource = \"bin\"\r\ninclude = [\"example/beos_delis.py\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/bifurcations-discrete-maps",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Bifurcations-Discrete-Maps",
            "content": {
                "codemeta": "",
                "readme": "![Python](https://img.shields.io/badge/Python-3670A0?style=plastic&logo=python&logoColor=ffdd54) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/Bifurcations?style=plastic)\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13276873.svg)](https://doi.org/10.5281/zenodo.13276873)\n\n# Bifurcation Diagrams for Discrete-Time Maps\n\nThis repository provides Python implementations for generating interactive bifurcation diagrams and analysing chaotic behavior in discrete-time dynamical systems. \n\nThe following maps are currently included:\n\n- Logistic Map\n- Tent Map\n- Sine Map\n\n### Overview\n\nEach map is implemented to analyse its bifurcation diagram and Lyapunov exponent, which are key to understanding the dynamics and chaos within these systems.\n\n### Installation\n\n1) Create and source virtual environment:\n```shell\npython -m venv env\nsource env/bin/activate  # On Windows use `env\\Scripts\\activate`\n```\n2) Install the dependencies:\n```shell\npip install -r requirements.txt\n```\n\n### Example Usage\n\nTo analyse a specific map, run the corresponding script in the Maps directory. You can tweak the map parameters as needed.\n\nInteractive plots will be generated and saved as offline HTML files within each map's directory.\n\n>[!Note]\n> Some plots have been uploaded to the `Maps` directories for reference.\n\n##### Scripts\n```shell\npython Maps/LogisticMap/main.py\n\npython Maps/TentMap/main.py\n\npython Maps/SineMap/main.py\n```\n##### Example (Logistic Map):\n```python\nfrom logistic import LogisticMap\nfrom utils.plotting import saveInteractivePlot\nimport datetime\n\n# Parameters for logistic map\nparams = {\n    'paramMin': 3.57,  # bifurcation parameter\n    'paramMax': 4.0,   \n    'stepSize': 1e-3,\n    'numTransient': 300,\n    'numPlotPoints': 300,\n    'numIterationsLyapunov': 200\n}\n\nlogisticMap = LogisticMap(**params)\n\n# Generate and save bifurcation diagram\nrhoValues, xValues, hoverText = logisticMap.generateBifurcationData()\n\ntimestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n\nplotParams = {\n    'title': 'Logistic Map Bifurcation Diagram',\n    'xAxisTitle': 'Bifurcation Parameter (rho)',\n    'yAxisTitle': 'System States x(n)',\n    'fileName': f'logisticMap_bifurcation_{timestamp}.html',\n    'markerSize': 0.05,\n    'opacity': 0.6,\n}\n\nsaveInteractivePlot(rhoValues, xValues, hoverText, **plotParams)\n\n# Generate and save Lyapunov exponent plot\nrhoValuesLyapunov, lyapunovExponents, lyapunovHoverText = logisticMap.generateLyapunovData()\n\nplotParams = {\n    'title': 'Logistic Map Lyapunov Exponent',\n    'xAxisTitle': 'Bifurcation Parameter (rho)',\n    'yAxisTitle': 'Lyapunov Exponent',\n    'fileName': f'logisticMap_lyapunov_exponent_{timestamp}.html',\n    'mode': 'lines'\n}\n\nsaveInteractivePlot(rhoValuesLyapunov, lyapunovExponents, lyapunovHoverText, **plotParams)\n```\n\n",
                "dependencies": "plotly\nnumpy\nmatplotlib\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/bioautoml",
            "repo_link": "https://github.com/Bonidia/BioAutoML",
            "content": {
                "codemeta": "",
                "readme": "![Python](https://img.shields.io/badge/python-v3.7-blue)\n![Dependencies](https://img.shields.io/badge/dependencies-up%20to%20date-brightgreen.svg)\n![Contributions welcome](https://img.shields.io/badge/contributions-welcome-orange.svg)\n![Status](https://img.shields.io/badge/status-up-brightgreen)\n\n<h1 align=\"center\">\n  <img src=\"https://github.com/Bonidia/BioAutoML/blob/main/img/BioAutoML.png\" alt=\"BioAutoML\" width=\"400\">\n</h1>\n\n<h4 align=\"center\">BioAutoML: Automated Feature Engineering and Metalearning for Classification of Biological Sequences</h4>\n\n<h4 align=\"center\">Democratizing Machine Learning in Life Sciences</h4>\n\n<p align=\"center\">\n  <a href=\"https://github.com/Bonidia/BioAutoML/\">Home</a> •\n  <a href=\"https://bonidia.github.io/BioAutoML/\">Documentation</a> •\n  <a href=\"#installing-dependencies-and-package\">Installing</a> •\n  <a href=\"#how-to-use\">How To Use</a> •\n  <a href=\"#citation\">Citation</a> \n</p>\n\n<h1 align=\"center\"></h1>\n\n\n## Update news!!!\n\n**New Website: [[https://bonidia.github.io/BioAutoML-WP/](https://bonidia.github.io/BioAutoML-WP/)]** \n\n**New Version - Protein:** BioAutoML + iFeature[[Ref](https://github.com/Superzchen/iFeature)]- Access on [[https://bonidia.github.io/BioAutoML/](https://bonidia.github.io/BioAutoML/)]\n\n**New Version - Protein:** Access on [[https://bonidia.github.io/BioAutoML/](https://bonidia.github.io/BioAutoML/)]\n\n**Published Paper:** Access on [[https://doi.org/10.1093/bib/bbac218](https://doi.org/10.1093/bib/bbac218)]\n\n\n## Awards\n\n⭐ Latin America Research Awards (LARA), Google, 2021. Project: BioAutoML: Automated Feature Engineering for Classification of Biological Sequences (24 awarded projects, from a base of 700 submissions). Elected by LARA-Google among the 24 most promising ideas in Latin America - 2021 - [[Link](https://blog.google/intl/pt-br/novidades/iniciativas/conheca-os-vencedores-do-premio-lara-2021-o-programa-de-bolsas-de-pesquisa-do-google/)] [[Link](http://www.saocarlos.usp.br/programa-de-bolsas-do-google-premia-trabalhos-orientados-pelo-cemeai/)].\n\n⭐ Finalist Project (Top 15 of 82), Falling Walls Lab Brazil 2022, DWIH São Paulo, Falling Walls Foundation, DAAD The German Center for Science and Innovation [[Link](https://www.youtube.com/watch?v=H5C_UIgVeQM)].\n\n⭐ Helmholtz Visiting Researcher Grant/Award - Helmholtz Information & Data Science Academy (HIDA), 2023. Project Title: BioAutoML-Fast: End-to-End Multi-Threaded Machine Learning Package for Life Sciences [[Link](https://bonidia.github.io/website/Certificate%20HVRG_Bonidia-1.pdf)].\n\n⭐ FEMS Research & Training Grant/Award - Federation of European Microbiological Societies (FEMS), 2023 (€: 5.000,00) [[Link](https://ibbsonline.org/wp-content/uploads/2023/10/IBBS-NL_Sep-2023-Issue.pdf)].\n\n⭐ BioAutoML - Top 10 Finalist - Santander X Brazil Award - Selected among the top 10 university projects (from over 200 entries) in Brazil in the national innovation competition promoted by Banco Santander.\n\n⭐ BioAutoML received an honorable mention from the Young Bioinformatics Award 2024, being chosen among the best theses in Bioinformatics and Computational Biology in Brazil, 2024.\n\n⭐ BioAutoML received third place in the ARTUR ZIVIANI THESIS AWARD (SBCAS), being chosen among the best theses in computing applied to health in Brazil, 2024.\n\n## Abstract\n\nRecent technological advances allowed an exponential expansion of biological sequence data and the extraction of meaningful information through Machine Learning (ML) algorithms. This knowledge improved the understanding of the mechanisms related to several fatal diseases, e.g., Cancer and COVID-19, helping to develop innovative solutions, such as CRISPR-based gene editing, coronavirus vaccine, and precision medicine. These advances benefit our society and economy, directly impacting people's lives in various areas, such as health care, drug discovery, forensic analysis, and food processing. Nevertheless, ML-based approaches to biological data require representative, quantitative, and informative features. Many ML algorithms can handle only numerical data, so sequences need to be translated into a numerical feature vector. This process, known as feature extraction, is a fundamental step for elaborating high-quality ML-based models in bioinformatics, by allowing the feature engineering stage, with the design and selection of suitable features. Feature engineering, ML algorithm selection, and hyperparameter tuning are often manual and time-consuming processes, requiring extensive domain knowledge. To deal with this problem, we present a new package, BioAutoML. BioAutoML automatically runs an end-to-end ML pipeline, extracting numerical and informative features from biological sequence databases, using the MathFeature package, and automating the feature selection, ML algorithm(s) recommendation and tuning of the selected algorithm(s) hyperparameters, using Automated ML (AutoML). BioAutoML has two components, divided into four modules, (1) automated feature engineering (feature extraction and selection modules) and (2) Metalearning (algorithm recommendation and hyper-parameter tuning modules). We experimentally evaluate BioAutoML in two different scenarios: (i) prediction of the three main classes of ncRNAs and (ii) prediction of the seven categories of ncRNAs in bacteria, including housekeeping and regulatory types. To assess BioAutoML predictive performance, it is experimentally compared with two other AutoML tools (RECIPE and TPOT). According to the experimental results, BioAutoML can accelerate new studies, reducing the cost of feature engineering processing and either keeping or improving predictive performance.\n\n* First study to propose an automated feature engineering and metalearning pipeline for ncRNA sequences in bacteria;\n    \n* BioAutoML can be applied in multi-class and binary problems;\n    \n* BioAutoML can be used in other DNA/RNA sequences scenarios;\n    \n* BioAutoML can accelerate new studies, reducing the feature engineering time-consuming stage and improving the design and performance of ML pipelines in bioinformatics;\n    \n* BioAutoML does not require specialist human assistance.\n\n<h1 align=\"center\">\n  <img src=\"https://github.com/Bonidia/BioAutoML/blob/main/img/bio-v2-1.png\" alt=\"BioAutoML\" width=\"1000\">\n</h1>\n\n<h1 align=\"center\">\n  <img src=\"https://github.com/Bonidia/BioAutoML/blob/main/img/bio-v4-1.png\" alt=\"BioAutoML\" width=\"900\">\n</h1>\n\n## Authors\n\n* Robson Parmezan Bonidia, Anderson Paulo Avila Santos, Breno Lívio Silva de Almeida, Peter F. Stadler, Ulisses Nunes da Rocha, Danilo Sipoli Sanches, and André Carlos Ponce de Leon Ferreira de Carvalho.\n\n* **Correspondence:** bonidia@usp.br, andre@icmc.usp.br, ulisses.rocha@ufz.de\n\n## Publication\n\nRobson P Bonidia, Anderson P Avila Santos, Breno L S de Almeida, Peter F Stadler, Ulisses N da Rocha, Danilo S Sanches, André C P L F de Carvalho, BioAutoML: automated feature engineering and metalearning to predict noncoding RNAs in bacteria, Briefings in Bioinformatics, 2022, bbac218, [[DOI](https://doi.org/10.1093/bib/bbac218)].\n\n\n## Installing dependencies and package\n\n## Conda - Terminal\n\nInstalling BioAutoML using miniconda, e.g.:\n\n```sh\n$ git clone https://github.com/Bonidia/BioAutoML.git BioAutoML\n\n$ cd BioAutoML\n\n$ git submodule init\n\n$ git submodule update\n```\n\n**1 - Install Miniconda:** \n\n```sh\n\nSee documentation: https://docs.conda.io/en/latest/miniconda.html\n\n$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\n$ chmod +x Miniconda3-latest-Linux-x86_64.sh\n\n$ ./Miniconda3-latest-Linux-x86_64.sh\n\n$ export PATH=~/miniconda3/bin:$PATH\n\n```\n\n**2 - Create environment:**\n\n```sh\n\nconda env create -f BioAutoML-env.yml -n bioautoml\n\n```\n\n**3 - Activate environment:**\n\n```sh\n\nconda activate bioautoml\n\n```\n\n**4 - You can deactivate the environment, using:**\n\n```sh\n\nconda deactivate\n\n```\n## How to use\n\nSee our [documentation](https://bonidia.github.io/BioAutoML/).\n\n## Citation\n\nIf you use this code in a scientific publication, we would appreciate citations to the following paper:\n\nRobson P Bonidia, Anderson P Avila Santos, Breno L S de Almeida, Peter F Stadler, Ulisses N da Rocha, Danilo S Sanches, André C P L F de Carvalho, BioAutoML: automated feature engineering and metalearning to predict noncoding RNAs in bacteria, Briefings in Bioinformatics, 2022, bbac218, [[DOI](https://doi.org/10.1093/bib/bbac218)].\n\n```sh\n\n@article{10.1093/bib/bbac218,\n    author = {Bonidia, Robson P and Santos, Anderson P Avila and de Almeida, Breno L S and Stadler, Peter F and da Rocha, Ulisses N and Sanches, Danilo S and de Carvalho, André C P L F},\n    title = \"{BioAutoML: automated feature engineering and metalearning to predict noncoding RNAs in bacteria}\",\n    journal = {Briefings in Bioinformatics},\n    year = {2022},\n    month = {06},\n    issn = {1477-4054},\n    doi = {10.1093/bib/bbac218},\n    url = {https://doi.org/10.1093/bib/bbac218},\n    note = {bbac218},\n}\n\n\n```\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/blenderproc",
            "repo_link": "https://github.com/DLR-RM/BlenderProc",
            "content": {
                "codemeta": "",
                "readme": "# BlenderProc2\n\n[![Documentation](https://img.shields.io/badge/documentation-passing-brightgreen.svg)](https://dlr-rm.github.io/BlenderProc/)\n[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DLR-RM/BlenderProc/blob/main/examples/basics/basic/basic_example.ipynb)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/6104887/137109535-275a2aa3-f5fd-4173-9d16-a9a9b86f66e7.gif\" alt=\"Front readme image\" width=100%>\n</p>\n\nA procedural Blender pipeline for photorealistic rendering.\n\n[Documentation](https://dlr-rm.github.io/BlenderProc) | [Tutorials](#tutorials) | [Examples](#examples) | [ArXiv paper](https://arxiv.org/abs/1911.01911) | [Workshop paper](https://sim2real.github.io/assets/papers/2020/denninger.pdf) | [JOSS article](https://joss.theoj.org/papers/10.21105/joss.04901)\n\n## Features\n\n* Loading: `*.obj`, `*.ply`, `*.blend`, `*.fbx`, BOP, ShapeNet, Haven, 3D-FRONT, etc.\n* Objects: Set or sample object poses, apply physics and collision checking.\n* Materials: Set or sample physically-based materials and textures\n* Lighting: Set or sample lights, automatic lighting of 3D-FRONT scenes.\n* Cameras: Set, sample or load camera poses from file.\n* Rendering: RGB, stereo, depth, normal and segmentation images/sequences.\n* Writing: .hdf5 containers, COCO & BOP annotations.\n\n\n## Installation\n\n### Via pip\n\nThe simplest way to install blenderproc is via pip:\n\n```bash\npip install blenderproc\n```\n\n### Via git\n\nAlternatively, if you need to make changes to blenderproc or you want to make use of the most recent version on the main-branch, clone the repository:\n\n```bash\ngit clone https://github.com/DLR-RM/BlenderProc\n```\n\nTo still make use of the blenderproc command and therefore use blenderproc anywhere on your system, make a local pip installation:\n\n```bash\ncd BlenderProc\npip install -e .\n```\n\n## Usage\n\nBlenderProc has to be run inside the blender python environment, as only there we can access the blender API. \nTherefore, instead of running your script with the usual python interpreter, the command line interface of BlenderProc has to be used.\n\n```bash\nblenderproc run <your_python_script>\n```\n\nIn general, one run of your script first loads or constructs a 3D scene, then sets some camera poses inside this scene and renders different types of images (RGB, distance, semantic segmentation, etc.) for each of those camera poses.\nUsually, you will run your script multiple times, each time producing a new scene and rendering e.g. 5-20 images from it.\nWith a little more experience, it is also possible to change scenes during a single script call, read [here](docs/tutorials/key_frames.md#render-multiple-times) how this is done.\n\n## Quickstart\n\nYou can test your BlenderProc pip installation by running\n\n```bash\nblenderproc quickstart\n```\n\nThis is an alias to `blenderproc run quickstart.py` where `quickstart.py` is:\n\n```python\nimport blenderproc as bproc\nimport numpy as np\n\nbproc.init()\n\n# Create a simple object:\nobj = bproc.object.create_primitive(\"MONKEY\")\n\n# Create a point light next to it\nlight = bproc.types.Light()\nlight.set_location([2, -2, 0])\nlight.set_energy(300)\n\n# Set the camera to be in front of the object\ncam_pose = bproc.math.build_transformation_mat([0, -5, 0], [np.pi / 2, 0, 0])\nbproc.camera.add_camera_pose(cam_pose)\n\n# Render the scene\ndata = bproc.renderer.render()\n\n# Write the rendering into an hdf5 file\nbproc.writer.write_hdf5(\"output/\", data)\n```\n\nBlenderProc creates the specified scene and renders the image into `output/0.hdf5`.\nTo visualize that image, simply call:\n\n```bash\nblenderproc vis hdf5 output/0.hdf5\n```\n\nThats it! You rendered your first image with BlenderProc!\n\n### Debugging in the Blender GUI\n\nTo understand what is actually going on, BlenderProc has the great feature of visualizing everything inside the blender UI.\nTo do so, simply call your script with the `debug` instead of `run` subcommand:\n```bash\nblenderproc debug quickstart.py\n```\n*Make sure that `quickstart.py` actually exists in your working directory.*\n\nNow the Blender UI opens up, the scripting tab is selected and the correct script is loaded.\nTo start the BlenderProc pipeline, one now just has to press `Run BlenderProc` (see red circle in image).\nAs in the normal mode, print statements are still printed to the terminal.\n\n<p align=\"center\">\n<img src=\"images/debug.jpg\" alt=\"Front readme image\" width=500>\n</p>\n\nThe pipeline can be run multiple times, as in the beginning of each run the scene is cleared.\n\n### Breakpoint-Debugging in IDEs\n\nAs blenderproc runs in blenders separate python environment, debugging your blenderproc script cannot be done in the same way as with any other python script.\nTherefore, remote debugging is necessary, which is explained for vscode and PyCharm in the following:\n\n#### Debugging with vscode\n\nFirst, install the `debugpy` package in blenders python environment.\n\n```\nblenderproc pip install debugpy\n```\n\nNow add the following configuration to your vscode [launch.json](https://code.visualstudio.com/docs/python/debugging#_initialize-configurations).\n\n```json\n{                        \n    \"name\": \"Attach\",\n    \"type\": \"python\",\n    \"request\": \"attach\",\n    \"connect\": {\n        \"host\": \"localhost\",\n        \"port\": 5678\n    }\n}\n```\n\nFinally, add the following lines to the top (after the imports) of your blenderproc script which you want to debug.\n\n```python\nimport debugpy\ndebugpy.listen(5678)\ndebugpy.wait_for_client()\n```\n\nNow run your blenderproc script as usual via the CLI and then start the added \"Attach\" configuration in vscode.\nYou are now able to add breakpoints and go through the execution step by step.\n\n#### Debugging with PyCharm Professional\n\nIn Pycharm, go to `Edit configurations...` and create a [new configuration](https://www.jetbrains.com/help/pycharm/remote-debugging-with-product.html#remote-debug-config) based on `Python Debug Server`.\nThe configuration will show you, specifically for your version, which pip package to install and which code to add into the script.\nThe following assumes Pycharm 2021.3:\n\nFirst, install the `pydevd-pycharm` package in blenders python environment.\n\n```\nblenderproc pip install pydevd-pycharm~=212.5457.59\n```\n\nNow, add the following code to the top (after the imports) of your blenderproc script which you want to debug.\n\n```python\nimport pydevd_pycharm\npydevd_pycharm.settrace('localhost', port=12345, stdoutToServer=True, stderrToServer=True)\n```\n\nThen, first run your `Python Debug Server` configuration in PyCharm and then run your blenderproc script as usual via the CLI.\nPyCharm should then go in debug mode, blocking the next code line.\nYou are now able to add breakpoints and go through the execution step by step.\n\n## What to do next?\n\nAs you now ran your first BlenderProc script, your ready to learn the basics:\n\n### Tutorials\n\nRead through the tutorials, to get to know with the basic principles of how BlenderProc is used:\n\n1. [Loading and manipulating objects](docs/tutorials/loader.md)\n2. [Configuring the camera](docs/tutorials/camera.md)\n3. [Rendering the scene](docs/tutorials/renderer.md)\n4. [Writing the results to file](docs/tutorials/writer.md)\n5. [How key frames work](docs/tutorials/key_frames.md)\n6. [Positioning objects via the physics simulator](docs/tutorials/physics.md)\n\n### Examples\n\nWe provide a lot of [examples](examples/README.md) which explain all features in detail and should help you understand how BlenderProc works. Exploring our examples is the best way to learn about what you can do with BlenderProc. We also provide support for some datasets.\n\n* [Basic scene](examples/basics/basic/README.md): Basic example, this is the ideal place to start for beginners\n* [Camera sampling](examples/basics/camera_sampling/README.md): Sampling of different camera positions inside of a shape with constraints for the rotation.\n* [Object manipulation](examples/basics/entity_manipulation/README.md): Changing various parameters of objects.\n* [Material manipulation](examples/basics/material_manipulation/README.md): Material selecting and manipulation.\n* [Physics positioning](examples/basics/physics_positioning/README.md): Enabling simple simulated physical interactions between objects in the scene.\n* [Semantic segmentation](examples/basics/semantic_segmentation/README.md): Generating semantic segmentation labels for a given scene.\n* [BOP Challenge](README_BlenderProc4BOP.md): Generate the pose-annotated data used at the BOP Challenge 2020\n* [COCO annotations](examples/advanced/coco_annotations/README.md): Write COCO annotations to a .json file for selected objects in the scene.\n\nand much more, see our [examples](examples/README.md) for more details.\n\n\n## Contributions\n\nFound a bug? help us by reporting it. Want a new feature in the next BlenderProc release? Create an issue. Made something useful or fixed a bug? Start a PR. Check the [contributions guidelines](CONTRIBUTING.md).\n\n## Change log\n\nSee our [change log](change_log.md). \n\n## Citation \n\nIf you use BlenderProc in a research project, please cite as follows:\n\n```\n@article{Denninger2023, \n    doi = {10.21105/joss.04901},\n    url = {https://doi.org/10.21105/joss.04901},\n    year = {2023},\n    publisher = {The Open Journal}, \n    volume = {8},\n    number = {82},\n    pages = {4901}, \n    author = {Maximilian Denninger and Dominik Winkelbauer and Martin Sundermeyer and Wout Boerdijk and Markus Knauer and Klaus H. Strobl and Matthias Humt and Rudolph Triebel},\n    title = {BlenderProc2: A Procedural Pipeline for Photorealistic Rendering}, \n    journal = {Journal of Open Source Software}\n} \n```\n\n---\n\n<div align=\"center\">\n  <a href=\"https://www.dlr.de/EN/Home/home_node.html\"><img src=\"images/logo.svg\" hspace=\"3%\" vspace=\"60px\"></a>\n</div>\n\n",
                "dependencies": "from setuptools import setup, find_packages\nimport os\n\n# Extract version from blenderproc/version.py\nhere = os.path.abspath(os.path.dirname(__file__))\nversion = {}\nwith open(os.path.join(here, \"blenderproc\", \"version.py\")) as fp:\n    exec(fp.read(), version)\n\nwith open(os.path.join(here, \"README.md\")) as fp:\n    long_description = fp.read()\n\nsetup(name='blenderproc',\n    version=version['__version__'],\n    url='https://github.com/DLR-RM/BlenderProc',\n    author='Maximilian Denninger, Dominik Winkelbauer, Martin Sundermeyer',\n    maintainer='Dominik Winkelbauer',\n    packages=find_packages(exclude=['docs', 'examples', 'external', 'images', 'resources', 'scripts', 'tests']),\n    include_package_data=True,\n    entry_points={\n        'console_scripts': ['blenderproc=blenderproc.command_line:cli'],\n    },      \n    install_requires=[\"setuptools\", \"pyyaml\", \"requests\", \"matplotlib\", \"numpy\", \"Pillow\", \"h5py\", \"progressbar\"],\n    long_description=long_description,\n    long_description_content_type='text/markdown'\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/boxbeam",
            "repo_link": "https://gitlab.com/dlr-sy/boxbeam",
            "content": {
                "codemeta": "",
                "readme": "[![PyPi](https://img.shields.io/pypi/v/boxbeam?label=PyPi)](https://pypi.org/project/boxbeam/)\n[![doi](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.12795533-red.svg)](https://zenodo.org/records/12795533)\n[![pipeline status](https://gitlab.com/dlr-sy/boxbeam/badges/master/pipeline.svg)]()\n\n# BoxBeam\nBoxBeam is a legacy Fortran-based beam calculation tool. It is compiled for Python using [f2py](https://numpy.org/doc/stable/f2py).\n> Installation from source requires an active Fortran compiler (ifort, gfortran). \n## Downloading\nUse GIT to get the latest code base. From the command line, use\n```\ngit clone https://gitlab.dlr.de/fa_sw/boxbeam boxbeam\n```\nIf you check out the repository for the first time, you have to initialize all submodule dependencies first. Execute the following from within the repository. \n```\ngit submodule update --init --recursive\n```\nTo update all refererenced submodules to the latest production level, use\n```\ngit submodule foreach --recursive 'git pull origin $(git config -f $toplevel/.gitmodules submodule.$name.branch || echo master)'\n```\n## Installation\nBoxBeam can be installed from source using [poetry](https://python-poetry.org). If you don't have [poetry](https://python-poetry.org) installed, run\n```\npip install poetry --pre --upgrade\n```\nto install the latest version of [poetry](https://python-poetry.org) within your python environment. Use\n```\npoetry update\n```\nto update all dependencies in the lock file or directly execute\n```\npoetry install\n```\nto install all dependencies from the lock file. Last, you should be able to import BoxBeam as a python package.\n```python\nimport boxbeam\n```\n## Example\nCopy and paste the following text into a new python script to verify the local installation\n```python\nimport os, sys\nimport boxbeam as bbeam\n \nfrom itertools import count, zip_longest\nfrom operator import itemgetter\nfrom collections import OrderedDict\nimport numpy as np\n\ndef toolInitiate(directory):\n    #---INITIATE BOXBEAM VARIABLES\n    bbeam.boxbeam.initialize()\n\n    extendedLogFile = False\n    newPath = directory#+'\\\\TestProfile.out'\n\n    #setting control parameters for BOXBEAM\n    bbeam.steuer.druck = extendedLogFile # extended log file\n    bbeam.steuer.nurqer = False\n    bbeam.steuer.klog = 11\n    bbeam.steuer.kraeft = False # calculate nodal forces (utilization for FE applications)\n\n    #---CHANGE THE NAME OF THE OUTPUT FILE TO THE ACTUAL CROSS SECTION NAME\n    if extendedLogFile:\n        for ffile, bbeamPathVar in zip([\"boxbeam_results.out\", \n                                        \"boxbeam_test.out\"], \n                                        [\"csinfopath\", \n                                         \"vbinfopath\"]):\n\n            fullPathFfile = os.path.abspath(os.path.join(newPath, ffile))\n\n            # 240 is the length of the character variable reserved\n            # within FORTRAN to store the directory name\n            if len(fullPathFfile) > 240:\n                raise Exception(\"Path length of file %s to long!\" % fullPathFfile)\n\n            pathList = [\"\"] * 240\n            pathList[: len(fullPathFfile)] = list(fullPathFfile)\n\n            if bbeamPathVar == \"csinfopath\":\n                self.bbeam.path.csinfopath = np.array(pathList,dtype=\"object\")\n            else:\n                self.bbeam.path.vbinfopath = np.array(pathList,dtype=\"object\")\n\ndef toolCalculate(capCount, webCount, cellCount, yi, zi, yk0, zk0, yklk, zklk, ig1, ig2, iak, ia, \n                    webExtensionalStiffness, webShearStiffness, webRefPlaneDist, webThickness,webDensity):\n    #---ASSIGN GURT DATA\n    bbeam.gurt.ig = capCount\n    \n    #---ASSIGN GURT COORDINATES\n    variableList = np.zeros(bbeam.restr.maxgu-bbeam.gurt.ig).tolist()\n    bbeam.gurt.yi = yi + variableList\n    bbeam.gurt.zi = zi + variableList\n    \n    #---ASSIGN GURT MATERIAL INFORMATION WITHIN BOXBEAM\n    bbeam.gurt.bi = np.zeros(bbeam.restr.maxgu)\n    bbeam.gurt.myi = np.zeros(bbeam.restr.maxgu)\n\n    #---ASSIGN BOXBEAM WAND DATA\n    bbeam.wand.kw = webCount\n    \n    #---ASSIGN WAND COORDINATES\n    variableList = np.zeros(bbeam.restr.maxwa-len(yk0)).tolist()\n    bbeam.wand.yk0 = yk0 + variableList\n    bbeam.wand.yklk = yklk + variableList\n    bbeam.wand.zk0 = zk0 + variableList\n    bbeam.wand.zklk = zklk + variableList\n    \n    #---ASSIGN WAND MATERIAL INFORMATION WITHIN BOXBEAM\n    variableList = np.zeros(bbeam.restr.maxwa-bbeam.wand.kw).tolist()\n    bbeam.wand.it = (5*np.ones(bbeam.wand.kw)).tolist()+variableList\n    bbeam.wand.bk = webExtensionalStiffness+variableList\n    bbeam.wand.gk = webShearStiffness+variableList\n    bbeam.wand.rhok = webDensity+variableList\n    bbeam.wand.e = webRefPlaneDist+variableList\n    \n    #---ASSIGN WAND TOPOLOGY WITHIN BOXBEAM\n    bbeam.wand.th = webThickness+variableList\n    bbeam.wand.ig1 = ig1+variableList\n    bbeam.wand.ig2 = ig2+variableList\n\n    #---ASSIGN BOXBEAM ZELLE DATA\n    bbeam.zelle.az = cellCount\n    variableList = np.zeros(bbeam.restr.maxze-bbeam.zelle.az).tolist()\n    bbeam.zelle.iak = iak+variableList\n    \n    iaArrayTransposed = np.zeros((bbeam.restr.maxze, bbeam.restr.maxgu))\n    for cellNumber in range(int(bbeam.restr.maxze)):\n        if cellNumber < cellCount:\n            iaArrayTransposed[cellNumber, :len(ia[cellNumber])] += ia[cellNumber]\n    bbeam.zelle.ia = iaArrayTransposed.T\n\n    #---ASSIGN BOXBEAM UNIFY LOADS\n    for attrName, load in zip_longest([\"qqx\",\"qqy\",\"qqz\",\"qmx\",\"qmy\",\"qmz\"], reactionForces):\n        setattr(bbeam.spanug, attrName, load)\n\n    #---EXECUTE BOXBEAM FOR CALCULATING THE CROSS SECTION PARAMETERS\n    bbeam.boxbeam.getequivalentxsection()\n\n    crossSectionParamNames = ['YS','ZS','YT','ZT','YMST','ZMST','YMSTAC','ZMSTAC','BX',\n                                'ALPHA','DYYSTE','DZZSTE','DYY','DZZ','DZY','DT','M','IYYS','IZZS','IZYS','ITT']\n\n    crossSectionParameters = {}\n    for param in crossSectionParamNames:\n        crossSectionParameters[param] = float(getattr(bbeam.quer, param.lower()))\n\n    effectiveProps = OrderedDict([\n                        ('EA',crossSectionParameters['BX']    ),\n                        ('EIxx',crossSectionParameters['DYY'] ),\n                        ('EIyy',crossSectionParameters['DZZ'] ),\n                        ('GJ',crossSectionParameters['DT']    ),\n                        ('YS',crossSectionParameters['YS']    ),\n                        ('ZS',crossSectionParameters['ZS']    ),\n                        ])\n\n\nif __name__ == '__main__':\n\n    #Specify folder where output files are to be stored\n    runDir = os.path.join(os.getcwd(),\"boxbeam\")\n    try:\n        os.makedirs(runDir)\n    except WindowsError: \n        pass\n\n    #Tool specific limitations\n    #maxCaps = 22 #variable specifying the maximum number of caps within a BoxBeam cross section - defined in bbeam.pyd\n    #maxWebs = 31 #variable specifying the maximum number of walls within a BoxBeam cross section - defined in bbeam.pyd\n    #maxCells = 10 #variable specifying the maximum number of cells within a BoxBeam cross section - defined in bbeam.pyd\n\n#------------------------------------------------------------------------------------------------------------------------\n# Initiation of boxbeam\n#------------------------------------------------------------------------------------------------------------------------\n\n    toolInitiate(runDir)\n\n#------------------------------------------------------------------------------------------------------------------------\n# Input for profile\n#------------------------------------------------------------------------------------------------------------------------\n\n    calcGeometry = False # If true the material data is set in a fashion, that the geometric properties (e.g. area moments of inertia) can be calculated.\n    \n    thickness1 = 1.25\n    thickness2 = .375\n    if calcGeometry:\n        extensionalStiffness1 = 1.*thickness1\n        extensionalStiffness2 = 1.*thickness2\n        shearStiffness1 = 1.*thickness1\n        shearStiffness2 = 1.*thickness2\n        density1 = 1.*thickness1\n        density2 = 1.*thickness2\n        bbeam.steuer.nurqer = True\n\n        #\"qqx\",\"qqy\",\"qqz\",\"qmx\",\"qmy\",\"qmz\"\n        reactionForces = [0., 0., 0., 0., 0., 0.]     \n        \n    else:\n        extensionalStiffness1 = 7.3335e4*thickness1\n        extensionalStiffness2 = 3.2232e4*thickness2\n        shearStiffness1 = 1.7327e4*thickness1\n        shearStiffness2 = 2.5012e4*thickness2\n        density1 = 0.00158*thickness1\n        density2 = 0.00158*thickness2\n\n        #\"qqx\",\"qqy\",\"qqz\",\"qmx\",\"qmy\",\"qmz\"\n        reactionForces = [0., 0., 500., 0., -62500., 0.]        \n\n    #---RETRIEVING POINT LOCATIONS AND TOPOLOGY\n    yi,zi,yk0,zk0 = [],[],[],[]\n    yklk, zklk, ig1, ig2 = [],[],[],[]\n    iak = []\n    ia = []\n    \n    webExtensionalStiffness = []\n    webShearStiffness, webRefPlaneDist = [], []\n    webThickness, webDensity = [],[]\n    \n    capExtensionalStiffness = []\n    capMass = []\n\n    #definition of simple profile\n    capCount = 8 \n    webCount = 9\n    cellCount = 2\n    \n    yi = [55., 55., -225., 13., 13., 75., -75., 75.]\n    zi = [12., -12., 0., 18., -18., 0., 16., 16.]\n    yk0 = [55., 75., 55., 13., -75., -225., -75., 13., 13., 13.]\n    zk0 = [12., 0., -12., -18., -16., 0., 16., 18., 18., 18.]\n    yklk = [75., 55., 13., -75., -225., -75., 13., 55., 13., 13.] \n    zklk = [0., -12., -18., -16., 0., 16., 18., 12., -18., -18.]\n\n    ig1 = [1, 6, 2, 5, 8, 3, 7, 4, 4]\n    ig2 = [6, 2, 5, 8, 3, 7, 4, 1, 5]\n    iak = [5, 5]\n    ia = [[1, 6, 2, 5, 4], [5, 8, 3, 7, 4]]\n\n    webExtensionalStiffness = [extensionalStiffness1]*(webCount-1)+[extensionalStiffness2]\n    webShearStiffness = [shearStiffness1]*(webCount-1)+[shearStiffness2]\n    webRefPlaneDist = [thickness1/2.]*(webCount-1)+[thickness2/2.]\n    webThickness = [thickness1]*(webCount-1)+[thickness2]\n    webDensity = [density1]*(webCount-1)+[density2]\n\n#------------------------------------------------------------------------------------------------------------------------\n# Assigning variables of boxbeam\n# Executing boxbeam\n# Retrieving results from boxbeam\n#------------------------------------------------------------------------------------------------------------------------\n\n    toolCalculate(\n        capCount, webCount, cellCount, yi, zi, yk0, zk0, yklk, zklk, ig1, ig2, iak, ia, \n        webExtensionalStiffness, webShearStiffness, webRefPlaneDist, webThickness,webDensity\n    )\n```\n## Contact\n* [Marc Garbade](mailto:marc.garbade@dlr.de)\n## Support\n* [List of Contributors](CONTRIBUTING.md)\n\n",
                "dependencies": "# TOML file to create Boxbeam\n#  \n# @note: TOML file            \n# Created on 13.12.2022    \n# \n# @version:  1.0    \n# ----------------------------------------------------------------------------------------------\n# @requires:\n#        - \n# \n# @change: \n#        -    \n#    \n# @author: garb_ma                                                     [DLR-SY,STM Braunschweig]\n# ----------------------------------------------------------------------------------------------\n[build-system]\nrequires = [\"poetry-core>=1.0\",\"packaging\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.poetry]\nname = \"boxbeam\"\nversion = \"1.3.0.post1\"\ndescription = \"Calculation of effective cross-sectional properties of composite beams\"\nauthors = [\"Heinecke, Falk <falk.heinecke@volkswagen.de>\"]\nmaintainers = [\"Garbade, Marc <marc.garbade@dlr.de>\",\n               \"Schuster, Andreas <andreas.schuster@dlr.de>\"]\nlicense = \"MIT\"\npackages = [{include=\"**/*\", from=\"bin\"}]\nexclude = [\"bin/_*\",\n           \"bin/**/.*\"]\nrepository = \"https://gitlab.com/dlr-sy/boxbeam\"\ndocumentation = \"https://gitlab.com/dlr-sy/boxbeam/-/blob/master/README.md\"\nkeywords = [\"analysis\",\"beam\",\"composite\"]\nreadme = \"README.md\"\nclassifiers = [\n    \"Development Status :: 7 - Inactive\",\n    \"Topic :: Scientific/Engineering\",\n    \"Programming Language :: Python :: 2\",\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\"\n]\n\n[tool.poetry.urls]\nChangelog = \"https://gitlab.com/dlr-sy/boxbeam/-/blob/master/CHANGELOG.md\"\n\n[[tool.poetry.source]]\nname = \"dlr-pypi\"\nurl = \"https://pypi.python.org/simple\"\npriority = \"primary\"\n\n[[tool.poetry.source]]\nname = \"dlr-sy\"\nurl = \"https://gitlab.dlr.de/api/v4/groups/541/-/packages/pypi/simple\"\npriority = \"supplemental\"\n\n[tool.poetry.build]\nscript = \"config/build.py\"\ngenerate-setup-file = false\n\n[tool.poetry.dependencies]\npython = \"~2.7 || ^3.5\"\nnumpy = [{version = \"^1.16\", python = \"~2.7 || ~3.5\"},\n         {version = \"^1.18\", python = \"~3.6\"},\n         {version = \"^1.21\", python = \"~3.7\"},\n         {version = \"^1.22\", python = \"~3.8\"},\n         {version = \">=1.22,<2\", python = \"^3.9\"}]\n\n# All optional dependencies\nfa-pyutils  = [{version = \"*\", python = \"^3.7\", optional = true}]\nPyQt5-Qt5 = [{version = \"5.15.2\", python = \"^3.7\", optional = true}]\nPyQt5-sip = [{version = \"~12.12\", python = \"~3.7\", optional = true},\n             {version = \">=12.13\", python = \"^3.8\", optional = true}]\nvampire = [{version = \">=0.2.5\", python = \"^3.7\", source=\"dlr-sy\", optional = true}]\n\n# All mandatory development dependencies\n[tool.poetry.group.dev.dependencies]\ndelis = [{git = \"https://gitlab.dlr.de/fa_sw/stmlab/plugins/delis.git\", develop=true, python=\"^3.7\"}]\nmkl = [{version = \"~2022.2\", platform = \"win32\"}]\nmkl-devel = [{version = \"~2022.2\", platform = \"win32\"}]\ntbb = [{version = \"~2021.7\", platform = \"win32\"}]\npyx-core = [{version = \">=1.18\", python = \"~2.7 || ^3.5,<3.7\"},\n            {git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake.git\", develop=true, python=\"^3.7\"}]\n\t\t\t\n[tool.poetry.group.lock.dependencies]\nmatplotlib = [{version = \"^2\", python = \"~2.7 || ~3.5\"},\n              {version = \"^3\", python = \"~3.6\"},\n              {version = \"~3.5\", python = \"~3.7\"},\n\t\t\t  {version = \">=3.6\", python = \"^3.8\"}]\nsetuptools = [{version = \"^39.0\", python = \"~2.7\"},\n              {version = \"^49.0,\", python = \"~3.5\"},\n              {version = \"^58.0,\", python = \"~3.6\"},\n              {version = \"^64.0\", python = \"~3.7\"},\n              {version = \">=64.0\", python = \"^3.8\"}]\n\n[tool.poetry.extras]\ndevel = [\"fa-pyutils\",\"vampire\",\"PyQt5-Qt5\",\"PyQt5-sip\"]\n\n[tool.pyxmake.doxygen]\nname = \"BoxBeam\"\ntitle = [\"BoxBeam\", \"BoxBeam Developer Guide\"]\nsource = \"src\"\noutput = \"doc/box_core\"\n\n[tool.pyxmake.coverage]\nname = \"boxbeam\"\nsource = \"bin\"\ninclude = [\"example/box_kernel.py\",\n           \"example/box_delis.py\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/brainprint",
            "repo_link": "https://github.com/Deep-MI/brainprint",
            "content": {
                "codemeta": "",
                "readme": "[![PyPI version](https://badge.fury.io/py/brainprint.svg)](https://pypi.org/project/brainprint/)\n# BrainPrint\n\nThis is the `brainprint` python package, a derivative of the original\n[BrainPrint-legacy](https://github.com/Deep-MI/BrainPrint-legacy) scripts,\nwith the primary goal to provide a Python-only version, to integrate the\n[LaPy](https://github.com/Deep-MI/LaPy) package, and to remove dependencies\non third-party software (shapeDNA-* binaries, gmsh, meshfix). As a result,\nsome functionality of the original BrainPrint-legacy scripts is no longer\nmaintained (currently no support of tetrahedral meshes and no support of\ncortical parcellations or label files).\n\n## Installation\n\nUse the following code to install the latest release into your local\nPython package directory:\n\n`python3 -m pip install brainprint`\n\nThis will also install the necessary dependencies, e.g. the [LaPy](https://github.com/Deep-MI/LaPy)\npackage. You may need to add your local Python package directory to your $PATH\nin order to run the scripts.\n\n## Usage\n### Command Line Interface (CLI)\n\nOnce installed, the package provides a `brainprint` executable which can be run from the command line.\n\nThe `brainprint` CLI enables per-subject computation of the individual brainprint descriptors. Its usage and options are summarized below;\ndetailed info is available by calling the script without any arguments from the command line.\n\n```sh\nbrainprint --sdir <directory> --sid <SubjectID>  [--num <num>] [--evec] [--skipcortex] [--norm <surface|volume|geometry|none> ] [--reweight] [--asymmetry] [--outdir <directory>] [--help] [--more-help]\n\nOptions:\n  --help           Show this help message and exit\n  --more-help      Show extensive help message and exit\n\nRequired options:\n  --sid <SubjectID>\n                   Subject ID (FreeSurfer-processed directory inside the\n                   subjects directory)\n  --sdir <directory>\n                   FreeSurfer subjects directory\n\nProcessing directives:\n  --num <num>      Number of eigenvalues/vectors to compute (default: 50)\n  --evec           Switch on eigenvector computation (default: off)\n  --skipcortex     Skip cortical surfaces (default: off)\n  --norm <surface|volume|geometry|none>\n                   Switch on eigenvalue normalization; will be either surface,\n                   volume, or determined by the geometry of the object. Use\n                   \"none\" or leave out entirely to skip normalization.\n  --reweight       Switch on eigenvalue reweighting (default: off)\n  --asymmetry      Perform left-right asymmetry calculation (default: off)\n  --cholmod        Switch on use of (faster) Cholesky decomposition instead\n                   of (slower) LU decomposition (default: off). May require \n                   manual install of scikit-sparse package. \n\nOutput parameters:\n  --outdir=OUTDIR  Output directory (default: <sdir>/<sid>/brainprint)\n  --keep-temp      Whether to keep the temporary files directory or not\n                   by default False\n```\n\n### Python Package\n\n`brainprint` can also be run within a pure Python environment, i.e. installed and imported as a Python package. E.g.:\n\n```python\n>>> from brainprint import Brainprint\n\n>>> subjects_dir = \"/path/to/freesurfer/subjects_dir/\"\n>>> subject_id = \"42\"\n\n>>> bp = Brainprint(subjects_dir=subjects_dir, asymmetry=True, keep_eigenvectors=True)\n>>> results = bp.run(subject_id=subject_id)\n>>> results\n{\"eigenvalues\": PosixPath(\"/path/to/freesurfer/subjects_dir/subject_id/brainprint/subject_id.brainprint.csv\"), \"eigenvectors\": PosixPath(\"/path/to/freesurfer/subjects_dir/subject_id/brainprint/eigenvectors\"), \"distances\": PosixPath(\"/path/to/freesurfer/subjects_dir/subject_id/brainprint/subject_id.brainprint.asymmetry.csv\")}\n```\n\n## Output\n\nThe script will create an output directory that contains a CSV table with\nvalues (in that order) for the area, volume, and first n eigenvalues per each\nFreeSurfer structure. An additional output file will be created if the\nasymmetry calculation is performed and/or for the eigenvectors (CLI `--evecs` flag or `keep_eigenvectors` on class initialization).\n\n## Changes\n\nSince version 0.5.0, some changes break compatibility with earlier versions (0.4.0 and lower) as well as the [original BrainPrint](https://github.com/Deep-MI/BrainPrint-legacy). These changes include:\n\n- for the creation of surfaces from voxel-based segmentations, we have replaced FreeSurfer's marching cube algorithm by scikit-image's marching cube algorithm. Similarly, other FreeSurfer binaries have been replaced by custom Python functions. As a result, a parallel FreeSurfer installation is no longer a requirement for running the brainprint software.\n- we have changed / removed the following composite structures from the brainprint shape descriptor: the left and right *striatum* (composite of caudate, putamen, and nucleus accumbens) and the left and right *ventricles* (composite of lateral, inferior lateral, 3rd ventricle, choroid plexus, and CSF) have been removed; the left and right *cerebellum-white-matter* and *cerebellum-cortex* have been merged into left and right *cerebellum*.\n\nAs a result of these changes, numerical values for the brainprint shape descriptor that are obtained from version 0.5.0 and higher are expected to differ from earlier versions when applied to the same data, but should remain highly correlated with earlier results.\n\nThere are some changes in version 0.4.0 (and lower) in functionality in comparison to the original [BrainPrint](https://github.com/Deep-MI/BrainPrint-legacy)\nscripts:\n\n- currently no support for tetrahedral meshes\n- currently no support for analyses of cortical parcellation or label files\n- no more Python 2.x compatibility\n\n## API Documentation\n\nThe API Documentation can be found at https://deep-mi.org/BrainPrint .\n\n## References\n\nIf you use this software for a publication please cite:\n\n[1] BrainPrint: a discriminative characterization of brain morphology. Wachinger C, Golland P, Kremen W, Fischl B, Reuter M. Neuroimage. 2015;109:232-48. http://dx.doi.org/10.1016/j.neuroimage.2015.01.032 http://www.ncbi.nlm.nih.gov/pubmed/25613439\n\n[2] Laplace-Beltrami spectra as 'Shape-DNA' of surfaces and solids. Reuter M, Wolter F-E, Peinecke N Computer-Aided Design. 2006;38:342-366. http://dx.doi.org/10.1016/j.cad.2005.10.011\n\n",
                "dependencies": "[build-system]\nrequires = ['setuptools >= 61.0.0']\nbuild-backend = 'setuptools.build_meta'\n\n[project]\nname = 'brainprint'\nversion = '0.5.0-dev'\ndescription = 'A package to compute BrainPrint (shape descriptors) from FastSurfer/FreeSurfer MRI segmentations'\nreadme = 'README.md'\nlicense = {file = 'LICENSE'}\nrequires-python = '>=3.9'\nauthors = [\n    {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'},\n    {name = 'Kersten Diers', email = 'kersten.diers@dzne.de'},\n]\nmaintainers = [\n    {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'},\n]\nkeywords = [\n    'neuroscience', \n    'sMRI', \n    'FreeSurfer'\n]\nclassifiers = [\n    'Operating System :: Microsoft :: Windows',\n    'Operating System :: Unix',\n    'Operating System :: MacOS',\n    'Programming Language :: Python :: 3 :: Only',\n    'Programming Language :: Python :: 3.9',\n    'Programming Language :: Python :: 3.10',\n    'Programming Language :: Python :: 3.11',\n    'Programming Language :: Python :: 3.12',\n    'Natural Language :: English',\n    'License :: OSI Approved :: MIT License',\n    'Intended Audience :: Science/Research',\n]\ndependencies = [\n    'numpy>=1.21',\n    'scipy!=1.13.0',\n    'pandas',\n    'lapy >= 1.1.1',\n    'psutil',\n    'nibabel',\n    'scikit-image',\n]\n\n[project.optional-dependencies]\nbuild = [\n    'build',\n    'twine',\n]\ndoc = [\n    'furo!=2023.8.17',\n    'matplotlib',\n    'memory-profiler',\n    'numpydoc',\n    'sphinx!=7.2.*',\n    'sphinxcontrib-bibtex',\n    'sphinx-copybutton',\n    'sphinx-design',\n    'sphinx-gallery',\n    'sphinx-issues',\n    'pypandoc',\n    'nbsphinx',\n    'IPython', # For syntax highlighting in notebooks\n    'ipykernel',\n]\nstyle = [\n    'bibclean',\n    'codespell',\n    'pydocstyle[toml]',\n    'ruff',\n]\ntest = [\n    'pytest',\n    'pytest-cov',\n    'pytest-timeout',\n]\nall = [\n    'brainprint[build]',\n    'brainprint[doc]',\n    'brainprint[style]',\n    'brainprint[test]',\n]\nfull = [\n    'brainprint[all]',\n]\n\n[project.urls]\nhomepage = 'https://github.com/Deep-MI/BrainPrint'\ndocumentation = 'https://github.com/Deep-MI/BrainPrint'\nsource = 'https://github.com/Deep-MI/BrainPrint'\ntracker = 'https://github.com/Deep-MI/BrainPrint/issues'\n\n[project.scripts]\nbrainprint = 'brainprint.cli:main'\nbrainprint-sys_info = 'brainprint.commands.sys_info:run'\n\n[tool.setuptools]\ninclude-package-data = false\n\n[tool.setuptools.packages.find]\ninclude = ['brainprint*']\nexclude = ['brainprint*tests']\n\n[tool.pydocstyle]\nconvention = 'numpy'\nignore-decorators = '(copy_doc|property|.*setter|.*getter|pyqtSlot|Slot)'\nmatch = '^(?!setup|__init__|test_).*\\.py'\nmatch-dir = '^brainprint.*'\nadd_ignore = 'D100,D104,D107'\n\n[tool.ruff]\nline-length = 88\nextend-exclude = [\n    \"doc\",\n    \"setup.py\",\n]\n\n[tool.ruff.lint]\n# https://docs.astral.sh/ruff/linter/#rule-selection\nselect = [\n    \"E\",   # pycodestyle\n    \"F\",   # Pyflakes\n    \"UP\",  # pyupgrade\n    \"B\",   # flake8-bugbear\n    \"I\",   # isort\n    # \"SIM\", # flake8-simplify\n]\n\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\"F401\"]\n\n[tool.pytest.ini_options]\nminversion = '6.0'\naddopts = '--durations 20 --junit-xml=junit-results.xml --verbose'\nfilterwarnings = []\n\n[tool.coverage.run]\nbranch = true\ncover_pylib = false\nomit = [\n    '**/__init__.py',\n    '**/brainprint/_version.py',\n    '**/brainprint/commands/*',\n    '**/tests/**',\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    'pragma: no cover',\n    'if __name__ == .__main__.:',\n]\nprecision = 2\n\nfrom setuptools import setup\n\nsetup()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cadet",
            "repo_link": "https://github.com/cadet/CADET-Core",
            "content": {
                "codemeta": "",
                "readme": "CADET-Core\n==========\n\n.. image:: https://img.shields.io/github/release/cadet/cadet-core.svg\n   :target: https://github.com/cadet/cadet-core/releases\n\n.. image:: https://github.com/cadet/cadet-core/actions/workflows/ci.yml/badge.svg?branch=master\n   :target: https://github.com/cadet/cadet-core/actions/workflows/ci.yml?query=branch%3Amaster\n\n.. image:: https://anaconda.org/conda-forge/cadet/badges/downloads.svg\n   :target: https://anaconda.org/conda-forge/cadet\n\n.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.8179015.svg\n   :target: https://doi.org/10.5281/zenodo.8179015\n\n.. image:: https://img.shields.io/badge/JuRSE_Code_Pick-Oct_2024-blue.svg\n   :target: https://www.fz-juelich.de/en/rse/community-initiatives/jurse-code-of-the-month/october-2024\n\n- **Website (including documentation):** https://cadet.github.io\n- **Forum:** https://forum.cadet-web.de\n- **Source:** https://github.com/cadet/cadet-core\n- **Bug reports:** https://github.com/cadet/cadet-core/issues\n- **Demo:** https://www.cadet-web.de \n- **Newsletter:** https://cadet-web.de/newsletter/\n\nInstallation\n------------\nCADET-Core can be installed via conda from the ``conda-forge`` channel.\n\n``conda install -c conda-forge cadet``\n\nThis requires a working `conda installation <https://github.com/conda-forge/miniforge>`_.\n\n`Additional information <https://cadet.github.io/master/getting_started/installation>`_ and a `tutorial <https://cadet.github.io/master/getting_started/tutorials/breakthrough>`_ are available to guide you through the installation and the first steps of using CADET.\n\nCiting\n------------\nThe development of CADET-Core has been a collaborative effort, with multiple dedicated individuals contributing their expertise to create a powerful and versatile open-source software tool.\nCountless hours of hard work have been invested to provide the scientific community with a valuable resource.\nAs an open-source project, CADET-Core relies on the support and recognition from users and researchers to thrive.\nTherefore, we kindly ask that any publications or projects leveraging the capabilities of CADET-Core acknowledge its creators and their contributions by citing an adequate selection of our publications.\n\n**General:**\n\n- Leweke, S.; von Lieres, E.: `Chromatography Analysis and Design Toolkit (CADET) <https://doi.org/10.1016/j.compchemeng.2018.02.025>`_, Computers and Chemical Engineering **113** (2018), 274–294.\n\n- von Lieres, E.; Andersson, J.: `A fast and accurate solver for the general rate model of column liquid chromatography <https://doi.org/10.1016/j.compchemeng.2010.03.008>`_, Computers and Chemical Engineering **34,8** (2010), 1180–1191.\n\n**Major extensions:**\n\n- Breuer, J. M.; Leweke, S.; Schmölder, J.; Gassner, G.; von Lieres, E.: `Spatial discontinuous Galerkin spectral element method for a family of chromatography models in CADET <https://doi.org/10.1016/j.compchemeng.2023.108340>`_, Computers and Chemical Engineering **177** (2023), 108340.\n\n- Zhang, W.; Przybycien T., Schmölder J. , Leweke S. , von Lieres E.: `Solving crystallization/precipitation population balance models in CADET, part I: Nucleation growth and growth rate dispersion in batch and continuous modes on nonuniform grids <https://doi.org/10.1016/j.compchemeng.2024.108612>`_, Computers and Chemical Engineering **183** (2024), 108612.\n\n- Püttmann, A.; Schnittert, S.; Naumann, U.; von Lieres, E.: `Fast and accurate parameter sensitivities for the general rate model of column liquid chromatography <http://dx.doi.org/10.1016/j.compchemeng.2013.04.021>`_, Computers and Chemical Engineering **56** (2013), 46–57.\n\nAdditionally, to ensure reproducibility of your work, we recommend citing the zenodo doi corresponding to the specific CADET-Core release that you used.\n\nFor a comprehensive list and guidance on citing CADET-Core publications, please refer to the publications section of the `documentation <https://cadet.github.io/master/publications.html>`_.\n\nOngoing Development\n-------------------\n\nWe do our best to provide you with a stable API. However, CADET-Core is actively developed and breaking changes can sometimes be unavoidable. For non-developers, it is recommended to upgrade from release to release instead of always working with the most recent commit.\n\nBugs\n----\n\nPlease report any bugs that you find `here <https://github.com/cadet/cadet-core/issues>`_. Or, even better, fork the repository on `GitHub <https://github.com/cadet/cadet-core>`_ and create a pull request (PR) with the fix. \n\nDonations\n---------\n\n`Donations <https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=FCQ2M89558ZAG>`_ for helping to host, maintain, and further develop the CADET-Core project are highly appreciated.\n\n\nCopyright and License Notice\n----------------------------\n\nCopyright (C) 2008-present: The CADET-Core Authors (see `AUTHORS.md <https://github.com/cadet/cadet-core/blob/master/AUTHORS.md>`_).\n\nThis program is free software: you can redistribute it and/or modify it under the terms of the\nGNU General Public License as published by the Free Software Foundation, either version 3 of\nthe License, or (at your option) any later version (see `LICENSE.txt <https://github.com/cadet/cadet-core/blob/master/LICENSE.txt>`_).\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without\neven the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License along with this program, see below.\nIf not, see <https://www.gnu.org/licenses/>.\n\nExcept as contained in this notice, the name of a copyright holder shall not be used in advertising\nor otherwise to promote the sale, use, or other dealings in this Software without prior written\nauthorization of the copyright holder.\n\n\nAcknowledgments\n---------------\n\nPlease refer to the `list of contributors <https://github.com/cadet/cadet-core/blob/master/CONTRIBUTING.md>`_ who helped building and funding this project.\n\n\n",
                "dependencies": "# =============================================================================\n#  CADET\n#  \n#  Copyright © 2008-present: The CADET-Core Authors\n#            Please see the AUTHORS.md file.\n#  \n#  All rights reserved. This program and the accompanying materials\n#  are made available under the terms of the GNU Public License v3.0 (or, at\n#  your option, any later version) which accompanies this distribution, and\n#  is available at http://www.gnu.org/licenses/gpl.html\n# =============================================================================\n \n# Require a fairly new cmake version\ncmake_minimum_required(VERSION 3.12)\n\n# Prohibit in-source build\nset(CMAKE_DISABLE_SOURCE_CHANGES ON)\nset(CMAKE_DISABLE_IN_SOURCE_BUILD ON)\nif (\"${CMAKE_SOURCE_DIR}\" STREQUAL \"${CMAKE_BINARY_DIR}\")\n\tmessage(FATAL_ERROR \"In-source build prohibited.\")\nendif()\n\n# Set module path in order to use custom CMake modules\nset(CMAKE_MODULE_PATH \"${CMAKE_SOURCE_DIR}/cmake/Modules\")\n\nfind_package(Git)\n\n# Write the current version number to variable\nif (GIT_FOUND)\n\tif (EXISTS \"${CMAKE_SOURCE_DIR}/.git\")\n\t\texecute_process(COMMAND ${GIT_EXECUTABLE} describe --abbrev=0 HEAD\n\t\t                WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\n\t\t                OUTPUT_VARIABLE CADET_VERSION\n\t\t                OUTPUT_STRIP_TRAILING_WHITESPACE)\n\n\t\tif (NOT \"${CADET_VERSION}\" STREQUAL \"\")\n\t\t\tmessage(STATUS \"Get version from git\")\n\n\t\t\t# Remove first character (\"v\")\n\t\t\tstring(LENGTH \"${CADET_VERSION}\" CADET_VERSION_STRLEN)\n\t\t\tmath(EXPR CADET_VERSION_STRLEN \"${CADET_VERSION_STRLEN}-1\")\n\t\t\tstring(SUBSTRING \"${CADET_VERSION}\" 1 ${CADET_VERSION_STRLEN}  CADET_VERSION)\n\t\tendif()\n\tendif()\nendif()\n\n# In case of missing tags, default to versions.txt file\nif (\"${CADET_VERSION}\" STREQUAL \"\")\n\tmessage(STATUS \"Get version from file\")\n\tfile(STRINGS \"${CMAKE_SOURCE_DIR}/version.txt\" CADET_VERSION)\nendif()\n\nmessage(STATUS \"CADET version: ${CADET_VERSION}\")\n\n# Get current commit hash from git\nif (GIT_FOUND)\n\tinclude(GetGitRevisionDescription)\n\tget_git_head_revision(GIT_REFSPEC GIT_SHA1)\nendif()\nif (NOT DEFINED GIT_SHA1)\n\tset(GIT_SHA1 \"NO-COMMIT-HASH\")\n\tset(GIT_REFSPEC \"NO-REFSPEC\")\nendif()\nmessage(STATUS \"Current git HEAD: ${GIT_REFSPEC} SHA1 ${GIT_SHA1}\")\n\n# Name of the current project\nproject(CadetFramework\n\tVERSION ${CADET_VERSION}\n\tDESCRIPTION \"Liquid column chromatography simulator\"\n\tHOMEPAGE_URL \"https://github.com/cadet/cadet-core\"\n\tLANGUAGES CXX C)\n\n# Always use '-fPIC'/'-fPIE' option\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\n\n# Hide symbols by default\nset(CMAKE_C_VISIBILITY_PRESET hidden)\nset(CMAKE_CXX_VISIBILITY_PRESET hidden)\nset(CMAKE_VISIBILITY_INLINES_HIDDEN ON)\n\n# Enable folders for IDEs\nset_property(GLOBAL PROPERTY USE_FOLDERS ON)\n\n# ---------------------------------------------------\n#   Other configuration options\n# ---------------------------------------------------\n\n# Option that allows users to build release or debug version\nif (NOT CMAKE_BUILD_TYPE)\n\tset(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"Choose the type of build, options are: None Debug Release RelWithDebInfo MinSizeRel\" FORCE)\n\tmessage(STATUS \"Build type: ${CMAKE_BUILD_TYPE} (default)\")\nendif()\n\n# Default IPO setting: OFF for Debug, ON for all other build types\nset(DEFAULT_IPO_ENABLED ON)\nif (CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n\tset(DEFAULT_IPO_ENABLED OFF)\nendif()\n\ninclude(FeatureSummary)\n\noption(ENABLE_LOGGING \"Enables logging\" ON)\nadd_feature_info(ENABLE_LOGGING ENABLE_LOGGING \"Enables logging\")\n\noption(ENABLE_BENCHMARK \"Enables benchmark mode (fine-grained timing)\" OFF)\nadd_feature_info(ENABLE_BENCHMARK ENABLE_BENCHMARK \"Enables benchmark mode (fine-grained timing)\")\n\noption(ENABLE_PLATFORM_TIMER \"Use a platform-dependent timer\" OFF)\nadd_feature_info(ENABLE_PLATFORM_TIMER ENABLE_PLATFORM_TIMER \"Use a platform-dependent timer\")\n\noption(ENABLE_THREADING \"Use multi-threading\" OFF)\nadd_feature_info(ENABLE_THREADING ENABLE_THREADING \"Use multi-threading\")\n\noption(ENABLE_DEBUG_THREADING \"Use multi-threading in debug builds\" OFF)\nadd_feature_info(ENABLE_DEBUG_THREADING ENABLE_DEBUG_THREADING \"Use multi-threading in debug builds\")\n\noption(ENABLE_2D_MODELS \"Build 2D models (e.g., 2D general rate model, multichannel transport)\" ON)\nadd_feature_info(ENABLE_2D_MODELS ENABLE_2D_MODELS \"Build 2D models (e.g., 2D general rate model, multichannel transport)\")\n\noption(ENABLE_DG \"Build DG variants of models\" ON)\nadd_feature_info(ENABLE_DG ENABLE_DG \"Build DG variants of models\")\n\noption(ENABLE_SUNDIALS_OPENMP \"Prefer OpenMP vector implementation of SUNDIALS if available (for large problems)\" OFF)\nadd_feature_info(ENABLE_SUNDIALS_OPENMP ENABLE_SUNDIALS_OPENMP \"Prefer OpenMP vector implementation of SUNDIALS if available (for large problems)\")\n\noption(ENABLE_ANALYTIC_JACOBIAN_CHECK \"Enable verification of analytical Jacobian by AD\" OFF)\nadd_feature_info(ENABLE_ANALYTIC_JACOBIAN_CHECK ENABLE_ANALYTIC_JACOBIAN_CHECK \"Enable verification of analytical Jacobian by AD\")\n\nset(ADLIB \"sfad\" CACHE STRING \"Selects the AD library, options are 'sfad', 'setfad'\")\nstring(TOLOWER ${ADLIB} ADLIB)\n\nset(NUM_MAX_AD_DIRS 80 CACHE STRING \"Sets the maximum number of AutoDiff directions\")\n\noption(ENABLE_CADET_CLI \"Build CADET command line interface\" ON)\nadd_feature_info(ENABLE_CADET_CLI ENABLE_CADET_CLI \"Build CADET command line interface\")\n\noption(ENABLE_CADET_TOOLS \"Build CADET tools\" ON)\nadd_feature_info(ENABLE_CADET_TOOLS ENABLE_CADET_TOOLS \"Build CADET tools\")\n\noption(ENABLE_TESTS \"Build CADET tests\" OFF)\nadd_feature_info(ENABLE_TESTS ENABLE_TESTS \"Build CADET tests\")\n\noption(ENABLE_PACKAGED_SUNDIALS \"Use packaged SUNDIALS code\" ON)\nadd_feature_info(ENABLE_PACKAGED_SUNDIALS ENABLE_PACKAGED_SUNDIALS \"Use packaged SUNDIALS code\")\n\noption(ENABLE_IPO \"Enable interprocedural optimization if compiler supports it\" ${DEFAULT_IPO_ENABLED})\nadd_feature_info(ENABLE_IPO ENABLE_IPO \"Enable interprocedural optimization if compiler supports it\")\n\noption(ENABLE_ASAN \"Enable address sanitizer (clang and gcc)\" OFF)\nadd_feature_info(ENABLE_ASAN ENABLE_ASAN \"Enable address sanitizer (clang and gcc)\")\n\noption(ENABLE_UBSAN \"Enable undefined behavior sanitizer (clang and gcc)\" OFF)\nadd_feature_info(ENABLE_UBSAN ENABLE_UBSAN \"Enable undefined behavior sanitizer (clang and gcc)\")\n\noption(ENABLE_STATIC_LINK_DEPS \"Prefer static over dynamic linking of dependencies\" OFF)\nadd_feature_info(ENABLE_STATIC_LINK_DEPS ENABLE_STATIC_LINK_DEPS \"Prefer static over dynamic linking of dependencies\")\n\noption(ENABLE_STATIC_LINK_LAPACK \"Prefer static over dynamic linking of LAPACK and BLAS\" OFF)\nadd_feature_info(ENABLE_STATIC_LINK_LAPACK ENABLE_STATIC_LINK_LAPACK \"Prefer static over dynamic linking of LAPACK and BLAS\")\n\noption(ENABLE_STATIC_LINK_CLI \"Prefer static over dynamic linking for CADET CLI\" OFF)\nadd_feature_info(ENABLE_STATIC_LINK_CLI ENABLE_STATIC_LINK_CLI \"Prefer static over dynamic linking for CADET CLI\")\n\noption(CMAKE_INSTALL_RPATH_USE_LINK_PATH \"Add paths to linker search and installed rpath\" ON)\nadd_feature_info(CMAKE_INSTALL_RPATH_USE_LINK_PATH CMAKE_INSTALL_RPATH_USE_LINK_PATH \"Add paths to linker search and installed rpath\")\n\n# Hande RPATH on OSX when not installing to a system directory, see\n# https://groups.google.com/d/msg/fenics-dev/KSCrob4M_1M/zsJwdN-SCAAJ\n# and https://cmake.org/Wiki/CMake_RPATH_handling#Always_full_RPATH\nif (UNIX)\n\t# The RPATH to be used when installing, but only if it's not a system directory\n\tset(CMAKE_INSTALL_RPATH \"${CMAKE_INSTALL_PREFIX}/lib\")\n\tlist(FIND CMAKE_PLATFORM_IMPLICIT_LINK_DIRECTORIES \"${CMAKE_INSTALL_PREFIX}/lib\" isSystemDir)\n\tif (\"${isSystemDir}\" STREQUAL \"-1\")\n\t\tset(CMAKE_INSTALL_RPATH \"${CMAKE_INSTALL_PREFIX}/lib\")\n\tendif()\nendif()\n\n# ---------------------------------------------------\n#   Check build environment\n# ---------------------------------------------------\ninclude(WriteCompilerDetectionHeader)\n\nset(TBB_TARGET \"\")\nif (ENABLE_THREADING)\n\tset(TBB_PREFER_STATIC_LIBS ${ENABLE_STATIC_LINK_DEPS})\n\tset(TBB_USE_DEBUG_BUILD OFF)\n\tfind_package(TBB COMPONENTS tbb OPTIONAL_COMPONENTS tbb_preview)\n\tset_package_properties(TBB PROPERTIES\n\t\tTYPE RECOMMENDED\n\t\tPURPOSE \"Accelerates computation via multi-threading\"\n\t)\n\n\tset(CADET_PARALLEL_FLAG \"\")\n\tif (TBB_FOUND)\n\t\t# Use tbb_preview instead of tbb if it has been found\n\t\tif (TBB_tbb_preview_FOUND)\n\t\t\tset(TBB_TARGET \"TBB::TBBpreview\")\n\t\telse()\n\t\t\tset(TBB_TARGET \"TBB::TBB\")\n\t\tendif()\n\n\t\tif (CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n\t\t\tif (ENABLE_DEBUG_THREADING)\n\t\t\t\tset(CADET_PARALLEL_FLAG \"CADET_PARALLELIZE\")\n\t\t\tendif()\n\t\telse()\n\t\t\tset(CADET_PARALLEL_FLAG \"CADET_PARALLELIZE\")\n\t\tendif()\n\n\t\tget_target_property(TBB_IFACE_COMP_DEF ${TBB_TARGET} INTERFACE_COMPILE_DEFINITIONS)\n\t\tif (TBB_IFACE_COMP_DEF)\n\t\t\tlist(APPEND TBB_IFACE_COMP_DEF ${CADET_PARALLEL_FLAG})\n\t\telse()\n\t\t\tset(TBB_IFACE_COMP_DEF ${CADET_PARALLEL_FLAG})\n\t\tendif()\n\t\tif (TBB_IFACE_COMP_DEF)\n\t\t\tset_target_properties(${TBB_TARGET} PROPERTIES INTERFACE_COMPILE_DEFINITIONS \"${TBB_IFACE_COMP_DEF}\")\n\t\tendif()\n\t\tunset(TBB_IFACE_COMP_DEF)\n\n\t\tif (TBB_INTERFACE_VERSION GREATER_EQUAL 11004)\n\t\t\t# Use global_control instead of task_scheduler_init\n\t\t\tget_target_property(TBB_IFACE_COMP_DEF ${TBB_TARGET} INTERFACE_COMPILE_DEFINITIONS)\n\t\t\tset_target_properties(${TBB_TARGET} PROPERTIES INTERFACE_COMPILE_DEFINITIONS \"${TBB_IFACE_COMP_DEF}\")\n\t\t\tif (TBB_IFACE_COMP_DEF)\n\t\t\t\tlist(APPEND TBB_IFACE_COMP_DEF \"CADET_TBB_GLOBALCTRL\")\n\t\t\telse()\n\t\t\t\tset(TBB_IFACE_COMP_DEF \"CADET_TBB_GLOBALCTRL\")\n\t\t\tendif()\n\t\t\tif (TBB_IFACE_COMP_DEF)\n\t\t\t\tset_target_properties(${TBB_TARGET} PROPERTIES INTERFACE_COMPILE_DEFINITIONS \"${TBB_IFACE_COMP_DEF}\")\n\t\t\tendif()\n\t\t\tunset(TBB_IFACE_COMP_DEF)\n\t\tendif()\n\tendif()\nendif()\n\nset(BLA_STATIC ${ENABLE_STATIC_LINK_LAPACK})\nfind_package(LAPACK)\nset_package_properties(LAPACK PROPERTIES\n\tTYPE RECOMMENDED\n\tPURPOSE \"Solution of dense linear systems\"\n)\n\nif (NOT ENABLE_PACKAGED_SUNDIALS)\n\t# SUNDIALS_ROOT environment variable can be used to find SUNDIALS package\n\tset(SUNDIALS_PREFER_STATIC_LIBRARIES ${ENABLE_STATIC_LINK_DEPS})\n\tfind_package(SUNDIALS REQUIRED COMPONENTS sundials_idas sundials_nvecserial OPTIONAL_COMPONENTS sundials_nvecopenmp)\n\tset_package_properties(SUNDIALS PROPERTIES\n\t\tTYPE REQUIRED\n\t\tPURPOSE \"Time integration\"\n\t)\n\n\t# Check whether OpenMP is available in SUNDIAL'S NVECTOR module\n\tset(SUNDIALS_NVEC_TARGET \"SUNDIALS::sundials_nvecserial\")\n\tif (SUNDIALS_sundials_nvecopenmp_LIBRARY AND ENABLE_SUNDIALS_OPENMP)\n\t\t# Prefer OpenMP over serial version\n\t\tset(SUNDIALS_NVEC_TARGET \"SUNDIALS::sundials_nvecopenmp\")\n\n\t\tget_target_property(SUNDIALS_IFACE_COMP_DEF SUNDIALS::sundials_nvecopenmp INTERFACE_COMPILE_DEFINITIONS)\n\t\tif (SUNDIALS_IFACE_COMP_DEF)\n\t\t\tlist(APPEND SUNDIALS_IFACE_COMP_DEF \"CADET_SUNDIALS_OPENMP\")\n\t\telse()\n\t\t\tset(SUNDIALS_IFACE_COMP_DEF \"CADET_SUNDIALS_OPENMP\")\n\t\tendif()\n\t\tset_target_properties(SUNDIALS::sundials_nvecopenmp PROPERTIES INTERFACE_COMPILE_DEFINITIONS \"${SUNDIALS_IFACE_COMP_DEF}\")\n\t\tunset(SUNDIALS_IFACE_COMP_DEF)\n\tendif()\n\n\t# Determine SUNDIALS interface version\n\tif (SUNDIALS_FOUND)\n\t\tget_target_property(SUNDIALS_IFACE_COMP_DEF SUNDIALS::sundials_idas INTERFACE_COMPILE_DEFINITIONS)\n\t\tif (SUNDIALS_IFACE_COMP_DEF)\n\t\t\tlist(APPEND SUNDIALS_IFACE_COMP_DEF \"CADET_SUNDIALS_IFACE=${SUNDIALS_VERSION_MAJOR}\")\n\t\telse()\n\t\t\tset(SUNDIALS_IFACE_COMP_DEF \"CADET_SUNDIALS_IFACE=${SUNDIALS_VERSION_MAJOR}\")\n\t\tendif()\n\t\tset_target_properties(SUNDIALS::sundials_idas PROPERTIES INTERFACE_COMPILE_DEFINITIONS \"${SUNDIALS_IFACE_COMP_DEF}\")\n\t\tunset(SUNDIALS_IFACE_COMP_DEF)\n\tendif()\nelse()\n\tset(SUNDIALS_FOUND TRUE)\n\tset(SUNDIALS_VERSION \"3.2.1\")\n\tset(SUNDIALS_NVEC_TARGET \"SUNDIALS::sundials_nvecserial\")\n\tadd_subdirectory(ThirdParty/sundials)\n\n\tadd_library(SUNDIALS::sundials_idas INTERFACE IMPORTED)\n\ttarget_link_libraries(SUNDIALS::sundials_idas INTERFACE sundials_idas_static)\n\ttarget_include_directories(SUNDIALS::sundials_idas INTERFACE ThirdParty/sundials/include ThirdParty/sundials/src \"${CMAKE_BINARY_DIR}/ThirdParty/sundials/include\")\n\tset_target_properties(SUNDIALS::sundials_idas PROPERTIES INTERFACE_COMPILE_DEFINITIONS \"CADET_SUNDIALS_IFACE=3\")\n\n\tadd_library(SUNDIALS::sundials_nvecserial INTERFACE IMPORTED)\n\ttarget_include_directories(SUNDIALS::sundials_nvecserial INTERFACE ThirdParty/sundials/include \"${CMAKE_BINARY_DIR}/ThirdParty/sundials/include\")\n\ttarget_link_libraries(SUNDIALS::sundials_nvecserial INTERFACE sundials_nvecserial_static)\nendif()\n\nif (ENABLE_CADET_TOOLS OR ENABLE_CADET_CLI)\n\tset(HDF5_USE_STATIC_LIBRARIES ${ENABLE_STATIC_LINK_DEPS})\n\tfind_package(HDF5 COMPONENTS C)\n\tset_package_properties(HDF5 PROPERTIES\n\t\tDESCRIPTION \"Hierarchical Data Format 5 (HDF5)\"\n\t\tURL \"https://www.hdfgroup.org/HDF5\"\n\t\tTYPE RECOMMENDED\n\t\tPURPOSE \"File IO\"\n\t)\n\n\tif (HDF5_FOUND)\n\n\t\t# Create custom HDF5 target if CMake's FindHDF5 is too old\n\t\tif (NOT TARGET HDF5::HDF5)\n\t\t\tlist(LENGTH HDF5_C_LIBRARIES HDF5_C_LEN)\n\t\t\tif (HDF5_C_LEN GREATER 1)\n\t\t\t\tlist(GET HDF5_C_LIBRARIES 0 HDF5_MAIN_LIBRARY)\n\t\t\t\tset(HDF5_SUPPORT_LIBRARIES ${HDF5_C_LIBRARIES})\n\t\t\t\tlist(REMOVE_AT HDF5_SUPPORT_LIBRARIES 0)\n\t\t\telse()\n\t\t\t\tset(HDF5_MAIN_LIBRARY ${HDF5_C_LIBRARIES}) \n\t\t\t\tset(HDF5_SUPPORT_LIBRARIES)\n\t\t\tendif()\n\n\t\t\tadd_library(HDF5::HDF5 UNKNOWN IMPORTED)\n\t\t\tset_target_properties(HDF5::HDF5 PROPERTIES\n\t\t\t\tIMPORTED_LOCATION ${HDF5_MAIN_LIBRARY}\n\t\t\t\tINTERFACE_INCLUDE_DIRECTORIES \"${HDF5_C_INCLUDE_DIRS}\"\n#\t\t\t\tINTERFACE_COMPILE_DEFINITIONS ${HDF5_C_DEFINITIONS}\n\t\t\t)\n\n\t\t\tif (HDF5_SUPPORT_LIBRARIES)\n\t\t\t\ttarget_link_libraries(HDF5::HDF5 INTERFACE ${HDF5_SUPPORT_LIBRARIES})\n\t\t\tendif()\n\n\t\t\tunset(HDF5_SUPPORT_LIBRARIES)\n\t\t\tunset(HDF5_MAIN_LIBRARY)\n\t\t\tunset(HDF5_C_LEN)\n\t\tendif()\n\n\t\t# Make sure HDF5_LIBRARY_DIRS is defined\n\t\tif ((NOT DEFINED HDF5_LIBRARY_DIRS) OR (NOT HDF5_LIBRARY_DIRS) OR (\"${HDF5_LIBRARY_DIRS}\" STREQUAL \"\"))\n\t\t\tlist(GET HDF5_LIBRARIES 0 HDF5_LIB_TEMP)\n\t\t\tget_filename_component(HDF5_LIBRARY_DIRS ${HDF5_LIB_TEMP} DIRECTORY)\n\t\t\tunset(HDF5_LIB_TEMP)\n\t\tendif()\n\n\t\t# Check if we need additional libraries for linking (i.e., zlib, szip)\n\t\tinclude(${CMAKE_ROOT}/Modules/CheckCXXSourceCompiles.cmake)\n\t\tinclude(${CMAKE_ROOT}/Modules/CMakePushCheckState.cmake)\n\n\t\tcmake_push_check_state(RESET)\n\n\t\t# Set libs and includes\n\t\tset(CMAKE_REQUIRED_LIBRARIES ${HDF5_LIBRARIES})\n\t\tset(CMAKE_REQUIRED_INCLUDES ${HDF5_INCLUDE_DIRS})\n\n\t\tCHECK_CXX_SOURCE_COMPILES(\"#include <hdf5.h>\\nint main(int argc, char** argv){\\n H5Zfilter_avail(H5Z_FILTER_SZIP);\\nH5Zfilter_avail(H5Z_FILTER_DEFLATE);\\nreturn 0;\\n}\\n\" HDF5_DONT_NEED_ZLIBS)\n\n\t\t# Reset libs and includes\n\t\tcmake_pop_check_state()\n\n\t\t# Find szip and zlib libs if we need them\n\t\tif (NOT HDF5_DONT_NEED_ZLIBS)\n\n\t\t\t# Prefer static libs if enabled\n\t\t\tset(_HDF5_ORIG_CMAKE_FIND_LIBRARY_SUFFIXES ${CMAKE_FIND_LIBRARY_SUFFIXES})\n\t\t\tif(ENABLE_STATIC_LINK_DEPS)\n\t\t\t\tif(WIN32)\n\t\t\t\t\tset(CMAKE_FIND_LIBRARY_SUFFIXES .lib ${CMAKE_FIND_LIBRARY_SUFFIXES})\n\t\t\t\telse()\n\t\t\t\t\tset(CMAKE_FIND_LIBRARY_SUFFIXES .a ${CMAKE_FIND_LIBRARY_SUFFIXES})\n\t\t\t\tendif()\n\t\t\tendif()\n\n\t\t\tfind_library(HDF5_SZLIB NAMES libszip szip PATHS ${HDF5_LIBRARY_DIRS})\n\t\t\tfind_library(HDF5_ZLIB NAMES libzlib zlib PATHS ${HDF5_LIBRARY_DIRS})\n\n\t\t\tif (HDF5_SZLIB)\n\t\t\t\tlist(APPEND HDF5_LIBRARIES ${HDF5_SZLIB})\n\t\t\t\tadd_library(HDF5::SZLIB UNKNOWN IMPORTED)\n\t\t\t\tset_target_properties(HDF5::SZLIB PROPERTIES IMPORTED_LOCATION ${HDF5_SZLIB})\n\t\t\t\ttarget_link_libraries(HDF5::HDF5 INTERFACE HDF5::SZLIB)\n\t\t\tendif()\n\t\t\tif (HDF5_ZLIB)\n\t\t\t\tlist(APPEND HDF5_LIBRARIES ${HDF5_ZLIB})\n\t\t\t\tadd_library(HDF5::ZLIB UNKNOWN IMPORTED)\n\t\t\t\tset_target_properties(HDF5::ZLIB PROPERTIES IMPORTED_LOCATION ${HDF5_ZLIB})\n\t\t\t\ttarget_link_libraries(HDF5::HDF5 INTERFACE HDF5::ZLIB)\n\t\t\tendif()\n\t\t\tunset(HDF5_SZLIB)\n\t\t\tunset(HDF5_ZLIB)\n\n\t\t\tset(CMAKE_FIND_LIBRARY_SUFFIXES ${_HDF5_ORIG_CMAKE_FIND_LIBRARY_SUFFIXES})\n\t\t\tunset(_HDF5_ORIG_CMAKE_FIND_LIBRARY_SUFFIXES)\n\t\tendif()\n\tendif()\nendif()\n\nif (ENABLE_2D_MODELS)\n\tset(SUPERLU_PREFER_STATIC_LIBS ${ENABLE_STATIC_LINK_DEPS})\n\tfind_package(SuperLU)\n\tset_package_properties(SuperLU PROPERTIES\n\t\tTYPE RECOMMENDED\n\t\tPURPOSE \"Sparse matrix solver\"\n\t)\n\n\tset(UMFPACK_PREFER_STATIC_LIBS ${ENABLE_STATIC_LINK_DEPS})\n\tfind_package(UMFPACK)\n\tset_package_properties(UMFPACK PROPERTIES\n\t\tTYPE RECOMMENDED\n\t\tPURPOSE \"Sparse matrix solver\"\n\t)\nendif()\n\nset(EIGEN_TARGET \"\")\nif (ENABLE_DG)\n\tfind_package(Eigen3 3.4 REQUIRED NO_MODULE)\n\n\t# Disable DG if Eigen is not present\n\tif (NOT TARGET Eigen3::Eigen)\n\t\tmessage(STATUS \"Disabling DG support because Eigen3 could not be found\")\n\t\tset(ENABLE_DG OFF)\n\telse()\n\t\tset(EIGEN_TARGET \"Eigen3::Eigen\")\n\tendif()\nendif()\n\nset(IPO_AVAILABLE OFF)\nif (ENABLE_IPO)\n\tinclude(CheckIPOSupported)\n\tcheck_ipo_supported(RESULT IPO_RESULT OUTPUT IPO_OUT LANGUAGES CXX)\n\tif (IPO_RESULT)\n\t\tset(IPO_AVAILABLE ON)\n\t\tset(CMAKE_INTERPROCEDURAL_OPTIMIZATION ON)\n\telse()\n\t\tmessage(WARNING \"IPO is not supported: ${IPO_OUT}\")\n\tendif()\n\tunset(IPO_RESULT)\n\tunset(IPO_OUT)\nendif()\n\n\n# ---------------------------------------------------\n#   Add selected modules to the build system and add the targets to the list of all targets\n# ---------------------------------------------------\n\nadd_library(CADET::CompileOptions INTERFACE IMPORTED)\ntarget_compile_features(CADET::CompileOptions INTERFACE cxx_std_23)\nset(CMAKE_CXX_EXTENSIONS OFF)\n\nif (WIN32)\n\ttarget_compile_definitions(CADET::CompileOptions INTERFACE NOMINMAX)\nendif()\nif (CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n\ttarget_compile_definitions(CADET::CompileOptions INTERFACE CADET_LOGLEVEL_MIN=Trace DEBUG _DEBUG)\nelse()\n\ttarget_compile_definitions(CADET::CompileOptions INTERFACE CADET_LOGLEVEL_MIN=Warning NDEBUG)\nendif()\n\nif (ENABLE_BENCHMARK)\n\ttarget_compile_definitions(CADET::CompileOptions INTERFACE CADET_BENCHMARK_MODE)\nendif()\n\nif (ENABLE_PLATFORM_TIMER)\n\ttarget_compile_definitions(CADET::CompileOptions INTERFACE CADET_USE_PLATFORM_TIMER)\n\tif ((NOT APPLE) AND (NOT WIN32))\n\t\ttarget_link_libraries(CADET::CompileOptions INTERFACE rt)\n\tendif()\nendif()\n\nif (CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n\ttarget_compile_options(CADET::CompileOptions INTERFACE $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:\n\t\t-Wall -pedantic-errors -Wextra -Wno-unused-parameter -Wno-unused-function> #-Wconversion -Wsign-conversion\n\t$<$<CXX_COMPILER_ID:MSVC>:\n\t\t/W4 /wd4100 /bigobj /MP>\n)\nelse()\n\ttarget_compile_options(CADET::CompileOptions INTERFACE $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:\n\t\t-Wall -pedantic-errors -Wextra -Wno-unused-parameter -Wno-unused-function> #-Wconversion -Wsign-conversion\n\t$<$<CXX_COMPILER_ID:MSVC>:\n\t\t/W4 /wd4100 /MP>\n)\nendif()\n\nif (ENABLE_ASAN)\n\ttarget_compile_options(CADET::CompileOptions INTERFACE $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fsanitize=address>)\n\ttarget_link_options(CADET::CompileOptions INTERFACE $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fsanitize=address>)\nendif()\n\nif (ENABLE_UBSAN)\n\ttarget_compile_options(CADET::CompileOptions INTERFACE $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fsanitize=undefined>)\n\ttarget_link_options(CADET::CompileOptions INTERFACE $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fsanitize=undefined>)\nendif()\n\n\nadd_library(CADET::AD INTERFACE IMPORTED)\nif (ADLIB STREQUAL \"sfad\")\n\tmessage(STATUS \"AD library: SFAD\")\n\ttarget_compile_definitions(CADET::AD INTERFACE ACTIVE_SFAD)\n\ttarget_include_directories(CADET::AD INTERFACE \"${CMAKE_SOURCE_DIR}/include/ad\")\nelseif (ADLIB STREQUAL \"setfad\")\n\tmessage(STATUS \"AD library: SETFAD\")\n\ttarget_compile_definitions(CADET::AD INTERFACE ACTIVE_SETFAD)\n\ttarget_include_directories(CADET::AD INTERFACE \"${CMAKE_SOURCE_DIR}/include/ad\")\nelse()\n\tmessage(FATAL_ERROR \"Unkown AD library ${ADLIB} (options are 'sfad', 'setfad')\")\nendif()\n\n\n# Build tools\nadd_subdirectory(src/build-tools)\n\n# CADET library\nadd_subdirectory(src/libcadet)\n\nif (ENABLE_CADET_CLI)\n\tadd_subdirectory(src/cadet-cli)\nendif()\n\nif (ENABLE_CADET_TOOLS)\n\tadd_subdirectory(src/tools)\nendif()\n\nif (ENABLE_TESTS)\n\tadd_subdirectory(test)\n\n\t# Add \"make check\" target\n\tinclude(ProcessorCount)\n\tProcessorCount(NPROC)\n\tadd_custom_target(check COMMAND test/testRunner -d yes --tbbthreads ${NPROC})\nendif()\n\n\n# ---------------------------------------------------\n#   Set properties, definitions, install target etc.\n# ---------------------------------------------------\n\n# Packaging support\ninclude(CPack)\nset(CPACK_PACKAGE_VENDOR \"CADET\")\nset(CPACK_PACKAGE_DESCRIPTION_SUMMARY ${PROJECT_DESCRIPTION})\nset(CPACK_PACKAGE_VERSION_MAJOR ${PROJECT_VERSION_MAJOR})\nset(CPACK_PACKAGE_VERSION_MINOR ${PROJECT_VERSION_MINOR})\nset(CPACK_PACKAGE_VERSION_PATCH ${PROJECT_VERSION_PATCH})\nset(CPACK_STRIP_FILES ON)\n\n# Combine LICENSE.txt and ThirdParty-LICENSES.txt\nfile(WRITE \"${CMAKE_BINARY_DIR}/LICENSE.txt\" \"####################################################################################\\n\")\nfile(APPEND \"${CMAKE_BINARY_DIR}/LICENSE.txt\" \"##                                      CADET                                     ##\\n\")\nfile(APPEND \"${CMAKE_BINARY_DIR}/LICENSE.txt\" \"####################################################################################\\n\")\n\nfile(READ \"${CMAKE_SOURCE_DIR}/LICENSE.txt\" LICENSE_TEXT)\nfile(APPEND \"${CMAKE_BINARY_DIR}/LICENSE.txt\" \"\\n${LICENSE_TEXT}\")\n\nfile(READ \"${CMAKE_SOURCE_DIR}/ThirdParty-LICENSES.txt\" LICENSE_THIRDPARTY)\nfile(APPEND \"${CMAKE_BINARY_DIR}/LICENSE.txt\" \"\\n\\n${LICENSE_THIRDPARTY}\")\n\nset(CPACK_RESOURCE_FILE_LICENSE \"${CMAKE_BINARY_DIR}/LICENSE.txt\")\nset(CPACK_RESOURCE_FILE_README \"${CMAKE_SOURCE_DIR}/README.md\")\n\nset(CPACK_SOURCE_IGNORE_FILES\n\t/.git\n\t/\\\\\\\\.DS_Store\n)\n\nmessage(\"\")\nmessage(\"--------------------------- Feature Summary ---------------------------\")\n         \nfeature_summary(WHAT ALL)\n\n# Summary\nmessage(\"\")\nmessage(\"------------------------------- Summary -------------------------------\")\nmessage(\"C++ compiler name: ${CMAKE_CXX_COMPILER_ID} at ${CMAKE_CXX_COMPILER}\")\nmessage(\"Build type: ${CMAKE_BUILD_TYPE}\")\nmessage(\"Source dir: ${CMAKE_SOURCE_DIR}\")\nmessage(\"Binary dir: ${CMAKE_BINARY_DIR}\")\nmessage(\"Install dir: ${CMAKE_INSTALL_PREFIX}\")\nmessage(\"C Flags: ${CMAKE_C_FLAGS}\")\nmessage(\"C++ Flags: ${CMAKE_CXX_FLAGS}\")\nmessage(\"IPO enabled: ${IPO_AVAILABLE}\")\nmessage(\"------------------------------- Modules -------------------------------\")\nmessage(\"CADET-CLI: ${ENABLE_CADET_CLI}\")\nmessage(\"Tools: ${ENABLE_CADET_TOOLS}\")\nmessage(\"Tests: ${ENABLE_TESTS}\")\nmessage(\"------------------------------- Options -------------------------------\")\nmessage(\"Logging: ${ENABLE_LOGGING}\")\nmessage(\"Benchmark mode: ${ENABLE_BENCHMARK}\")\nmessage(\"Platform-dependent timer: ${ENABLE_PLATFORM_TIMER}\")\nmessage(\"AD library: ${ADLIB}\")\nmessage(\"2D Models: ${ENABLE_2D_MODELS}\")\nmessage(\"Check analytic Jacobian: ${ENABLE_ANALYTIC_JACOBIAN_CHECK}\")\nmessage(\"----------------------------- Dependencies ----------------------------\")\n\nmessage(\"Found BLAS: ${BLAS_FOUND}\")\nif (BLAS_FOUND)\n\tmessage(\"  Linker flags ${BLAS_LINKER_FLAGS}\")\n\tmessage(\"  Libs ${BLAS_LIBRARIES}\")\n\tmessage(\"  Underscore suffix ${BLAS_UNDERSCORE_SUFFIX}\")\nendif()\n\nmessage(\"Found LAPACK: ${LAPACK_FOUND}\")\nif (LAPACK_FOUND)\n\tmessage(\"  Linker flags ${LAPACK_LINKER_FLAGS}\")\n\tmessage(\"  Libs ${LAPACK_LIBRARIES}\")\nendif()\n\nmessage(\"Found TBB: ${TBB_FOUND}\")\nif (TBB_FOUND)\n\tmessage(\"  Version ${TBB_VERSION} (Interface ${TBB_INTERFACE_VERSION})\")\n\tmessage(\"  Include ${TBB_INCLUDE_DIRS}\")\n\tmessage(\"  Definitions ${TBB_DEFINITIONS}\")\n\tmessage(\"  Libs ${TBB_LIBRARIES}\")\nendif()\n\nif (ENABLE_PACKAGED_SUNDIALS)\n\tmessage(\"Found SUNDIALS: ${SUNDIALS_FOUND}\")\n\tmessage(\"  Version ${SUNDIALS_VERSION}\")\n\tmessage(\"  Packaged\")\nelse()\n\tmessage(\"Found SUNDIALS: ${SUNDIALS_FOUND}\")\n\tif (SUNDIALS_FOUND)\n\t\tmessage(\"  Version ${SUNDIALS_VERSION}\")\n\t\tmessage(\"  Includes ${SUNDIALS_INCLUDE_DIRS}\")\n\t\tmessage(\"  Libs ${SUNDIALS_LIBRARIES}\")\n\tendif()\nendif()\n\nif (ENABLE_2D_MODELS)\n\tmessage(\"Found SuperLU: ${SUPERLU_FOUND}\")\n\tif (SUPERLU_FOUND)\n\t\tmessage(\"  Version ${SUPERLU_VERSION}\")\n\t\tmessage(\"  Includes ${SUPERLU_INCLUDE_DIRS}\")\n\t\tmessage(\"  Libs ${SUPERLU_LIBRARIES}\")\n\t\tmessage(\"  Integer type ${SUPERLU_INT_TYPE}\")\n\tendif()\n\tmessage(\"Found UMFPACK: ${UMFPACK_FOUND}\")\n\tif (UMFPACK_FOUND)\n\t\tmessage(\"  Version ${UMFPACK_VERSION}\")\n\t\tmessage(\"  Includes ${UMFPACK_INCLUDE_DIRS}\")\n\t\tmessage(\"  Libs ${UMFPACK_LIBRARIES}\")\n\tendif()\nendif()\n\nif (ENABLE_DG)\n\tmessage(\"Found Eigen3: ${Eigen3_FOUND}\")\n\tif (TARGET Eigen3::Eigen)\n\t\tmessage(\"  Version ${Eigen3_VERSION}\")\n\t\tmessage(\"  Includes ${Eigen3_INCLUDE_DIRS}\")\n\tendif()\nendif()\n\nmessage(\"Found HDF5: ${HDF5_FOUND}\")\nif (HDF5_FOUND)\n\tmessage(\"  Version ${HDF5_VERSION}\")\n\tmessage(\"  Includes ${HDF5_INCLUDE_DIRS}\")\n\tmessage(\"  Libs ${HDF5_LIBRARIES}\")\n\tmessage(\"  Defs ${HDF5_C_DEFINITIONS}\")\nendif()\n\nmessage(\"-----------------------------------------------------------------------\")\nmessage(\"\")\n\n{\n  \"dependencies\": [\n    \"hdf5\",\n    \"suitesparse\",\n    \"superlu\",\n    \"eigen3\"\n  ]\n}\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/calibr8",
            "repo_link": "https://github.com/JuBiotech/calibr8",
            "content": {
                "codemeta": "",
                "readme": "[![PyPI version](https://img.shields.io/pypi/v/calibr8)](https://pypi.org/project/calibr8)\n[![pipeline](https://github.com/jubiotech/calibr8/workflows/pipeline/badge.svg)](https://github.com/jubiotech/calibr8/actions)\n[![coverage](https://codecov.io/gh/jubiotech/calibr8/branch/master/graph/badge.svg)](https://codecov.io/gh/jubiotech/calibr8)\n[![documentation](https://readthedocs.org/projects/calibr8/badge/?version=latest)](https://calibr8.readthedocs.io/en/latest/?badge=latest)\n[![DOI](https://zenodo.org/badge/306862348.svg)](https://zenodo.org/badge/latestdoi/306862348)\n\n\n# `calibr8`\nThis package provides templates and functions for performing likelihood-based calibration modeling.\nTo see implementation examples & excercises, you can go to [notebooks/](notebooks).\n\n# Installation\n`calibr8` is released on [PyPI](https://pypi.org/project/calibr8/):\n\n```\npip install calibr8\n```\n# Documentation\nRead the package documentation [here](https://calibr8.readthedocs.io/en/latest/?badge=latest).\n\n# Usage and Citing\n`calibr8` is licensed under the [GNU Affero General Public License v3.0](https://github.com/JuBiotech/calibr8/blob/master/LICENSE).\n\nWhen using `calibr8` in your work, please cite the [Helleckes & Osthege et al. (2022) paper](https://doi.org/10.1371/journal.pcbi.1009223) __and__ the [corresponding software version](https://doi.org/10.5281/zenodo.4127012).\n\nNote that the paper is a shared first co-authorship, which can be indicated by <sup>1</sup> in the bibliography.\n\n```bibtex\n@article{calibr8Paper,\n  doi       = {10.1371/journal.pcbi.1009223},\n  author    = {Helleckes$^1$, Laura Marie and\n  \t       Osthege$^1$, Michael and\n\t       Wiechert, Wolfgang and\n\t       von Lieres, Eric and\n\t       Oldiges, Marco},\n  journal   = {PLOS Computational Biology},\n  publisher = {Public Library of Science},\n  title     = {Bayesian and calibration, process modeling and uncertainty quantification in biotechnology},\n  year      = {2022},\n  month     = {03},\n  volume    = {18},\n  url       = {https://doi.org/10.1371/journal.pcbi.1009223},\n  pages     = {1-46},\n  number    = {3}\n}\n\n@software{calibr8version,\n  author    = {Michael Osthege and\n               Laura Helleckes},\n  title     = {JuBiotech/calibr8: v6.5.2},\n  month     = mar,\n  year      = 2022,\n  publisher = {Zenodo},\n  version   = {v6.5.2},\n  doi       = {10.5281/zenodo.4127012},\n  url       = {https://doi.org/10.5281/zenodo.4127012}\n}\n```\n\nHead over to Zenodo to [generate a BibTeX citation](https://doi.org/10.5281/zenodo.4127012) for the latest release.\n\n",
                "dependencies": "# inspired by https://hynek.me/articles/sharing-your-labor-of-love-pypi-quick-and-dirty/\n\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.black]\nline-length = 110\n\n[tool.isort]\nprofile = \"black\"\n\n[tool.mypy]\nignore_missing_imports = true\nexclude = [\n    'test_.*?\\.py$',\n]\n\nmatplotlib\nnumpy\nscipy>1.6.0\ntyping_extensions\n\n# Copyright 2021 Forschungszentrum Jülich GmbH\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as published\n# by the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\nimport os\nimport pathlib\nimport re\n\nimport setuptools\n\n__packagename__ = \"calibr8\"\nROOT = pathlib.Path(__file__).parent\n\n\ndef package_files(directory):\n    assert os.path.exists(directory)\n    fp_typed = pathlib.Path(ROOT, __packagename__, \"py.typed\")\n    fp_typed.touch()\n    paths = [str(fp_typed.absolute())]\n    for (path, directories, filenames) in os.walk(directory):\n        for filename in filenames:\n            paths.append(os.path.join(\"..\", path, filename))\n    return paths\n\n\ndef get_version():\n    VERSIONFILE = pathlib.Path(pathlib.Path(__file__).parent, __packagename__, \"core.py\")\n    initfile_lines = open(VERSIONFILE, \"rt\").readlines()\n    VSRE = r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\"\n    for line in initfile_lines:\n        mo = re.search(VSRE, line, re.M)\n        if mo:\n            return mo.group(1)\n    raise RuntimeError(\"Unable to find version string in %s.\" % (VERSIONFILE,))\n\n\n__version__ = get_version()\n\n\nsetuptools.setup(\n    name=__packagename__,\n    packages=setuptools.find_packages(),\n    version=__version__,\n    description=\"Toolbox for non-linear calibration modeling.\",\n    long_description=open(pathlib.Path(ROOT, \"README.md\")).read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/JuBiotech/calibr8\",\n    author=\"Laura Marie Helleckes, Michael Osthege\",\n    author_email=\"l.helleckes@fz-juelich.de, m.osthege@fz-juelich.de\",\n    license=\"GNU Affero General Public License v3\",\n    classifiers=[\n        \"Programming Language :: Python\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"License :: OSI Approved :: GNU Affero General Public License v3\",\n        \"Intended Audience :: Science/Research\",\n        \"Topic :: Scientific/Engineering\",\n        \"Topic :: Scientific/Engineering :: Mathematics\",\n    ],\n    install_requires=[open(pathlib.Path(ROOT, \"requirements.txt\")).readlines()],\n    package_data={\n        \"calibr8\": package_files(str(pathlib.Path(pathlib.Path(__file__).parent, \"calibr8\").absolute()))\n    },\n    include_package_data=True,\n    python_requires=\">=3.10\",\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cat4kit",
            "repo_link": "https://codebase.helmholtz.cloud/cat4kit/cat4kit-docker",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/catena",
            "repo_link": "https://gitlab.dlr.de/catena/",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/celldetection",
            "repo_link": "https://github.com/FZJ-INM1-BDA/celldetection",
            "content": {
                "codemeta": "",
                "readme": "# Cell Detection\n\n[![Downloads](https://static.pepy.tech/badge/celldetection?l)](https://pepy.tech/project/celldetection)\n[![Test](https://github.com/FZJ-INM1-BDA/celldetection/workflows/Test/badge.svg)](https://github.com/FZJ-INM1-BDA/celldetection/actions?query=workflow%3ATest)\n[![PyPI](https://img.shields.io/pypi/v/celldetection?l)](https://pypi.org/project/celldetection/)\n[![Documentation Status](https://readthedocs.org/projects/celldetection/badge/?version=latest)](https://celldetection.readthedocs.io/en/latest/?badge=latest)\n[![DOI](https://zenodo.org/badge/349111085.svg)](https://zenodo.org/badge/latestdoi/349111085)\n\n## ⭐ Showcase\n\n###### NeurIPS 22 Cell Segmentation Competition\n\n![neurips22](https://raw.githubusercontent.com/FZJ-INM1-BDA/celldetection/main/assets/neurips-cellseg-demo.png \"NeurIPS 22 Cell Segmentation Competition - Find more information here: https://neurips.cc/Conferences/2022/CompetitionTrack\")\n*https://openreview.net/forum?id=YtgRjBw-7GJ*\n\n###### Nuclei of U2OS cells in a chemical screen\n\n![bbbc039](https://raw.githubusercontent.com/FZJ-INM1-BDA/celldetection/main/assets/bbbc039-cpn-u22-demo.png \"BBBC039 demo with CpnU22 - Find the dataset here: https://bbbc.broadinstitute.org/BBBC039\")\n*https://bbbc.broadinstitute.org/BBBC039 (CC0)*\n\n###### P. vivax (malaria) infected human blood\n\n![bbbc041](https://raw.githubusercontent.com/FZJ-INM1-BDA/celldetection/main/assets/bbbc041-cpn-u22-demo.png \"BBBC041 demo with CpnU22 - Find the dataset here: https://bbbc.broadinstitute.org/BBBC041\")\n*https://bbbc.broadinstitute.org/BBBC041 (CC BY-NC-SA 3.0)*\n\n## 🛠 Install\n\nMake sure you have [PyTorch](https://pytorch.org/get-started/locally/) installed.\n\n### PyPI\n\n```\npip install -U celldetection\n```\n\n### GitHub\n\n```\npip install git+https://github.com/FZJ-INM1-BDA/celldetection.git\n```\n\n## 💾 Trained models\n\n```python\nmodel = cd.fetch_model(model_name, check_hash=True)\n```\n\n| model name                                  | training data                                                                                                        |                                           link                                            |\n|---------------------------------------------|----------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------:| \n| `ginoro_CpnResNeXt101UNet-fbe875f1a3e5ce2c` | BBBC039, BBBC038, Omnipose, Cellpose, Sartorius - Cell Instance Segmentation, Livecell, NeurIPS 22 CellSeg Challenge | [🔗](https://celldetection.org/torch/models/ginoro_CpnResNeXt101UNet-fbe875f1a3e5ce2c.pt) |\n\n<details>\n  <summary style=\"font-weight: bold; color: #888888\">Run a demo with a pretrained model</summary>\n\n```python\nimport torch, cv2, celldetection as cd\nfrom skimage.data import coins\nfrom matplotlib import pyplot as plt\n\n# Load pretrained model\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = cd.fetch_model('ginoro_CpnResNeXt101UNet-fbe875f1a3e5ce2c', check_hash=True).to(device)\nmodel.eval()\n\n# Load input\nimg = coins()\nimg = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\nprint(img.dtype, img.shape, (img.min(), img.max()))\n\n# Run model\nwith torch.no_grad():\n    x = cd.to_tensor(img, transpose=True, device=device, dtype=torch.float32)\n    x = x / 255  # ensure 0..1 range\n    x = x[None]  # add batch dimension: Tensor[3, h, w] -> Tensor[1, 3, h, w]\n    y = model(x)\n\n# Show results for each batch item\ncontours = y['contours']\nfor n in range(len(x)):\n    cd.imshow_row(x[n], x[n], figsize=(16, 9), titles=('input', 'contours'))\n    cd.plot_contours(contours[n])\n    plt.show()\n```\n\n</details>\n\n## 🔬 Architectures\n\n```python\nimport celldetection as cd\n```\n\n<details>\n  <summary style=\"font-weight: bold; color: #888888\">Contour Proposal Networks</summary>\n\n- [`cd.models.CPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CPN)\n- [`cd.models.CpnU22`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnU22)\n- [`cd.models.CPNCore`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CPNCore)\n- [`cd.models.CpnResUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResUNet)\n- [`cd.models.CpnSlimU22`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnSlimU22)\n- [`cd.models.CpnWideU22`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnWideU22)\n- [`cd.models.CpnResNet18FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet18FPN)\n- [`cd.models.CpnResNet34FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet34FPN)\n- [`cd.models.CpnResNet50FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet50FPN)\n- [`cd.models.CpnResNeXt50FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNeXt50FPN)\n- [`cd.models.CpnResNet101FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet101FPN)\n- [`cd.models.CpnResNet152FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet152FPN)\n- [`cd.models.CpnResNet18UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet18UNet)\n- [`cd.models.CpnResNet34UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet34UNet)\n- [`cd.models.CpnResNet50UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet50UNet)\n- [`cd.models.CpnResNeXt101FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNeXt101FPN)\n- [`cd.models.CpnResNeXt152FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNeXt152FPN)\n- [`cd.models.CpnResNeXt50UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNeXt50UNet)\n- [`cd.models.CpnResNet101UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet101UNet)\n- [`cd.models.CpnResNet152UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet152UNet)\n- [`cd.models.CpnResNeXt101UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNeXt101UNet)\n- [`cd.models.CpnResNeXt152UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNeXt152UNet)\n- [`cd.models.CpnWideResNet50FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnWideResNet50FPN)\n- [`cd.models.CpnWideResNet101FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnWideResNet101FPN)\n- [`cd.models.CpnMobileNetV3LargeFPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnMobileNetV3LargeFPN)\n- [`cd.models.CpnMobileNetV3SmallFPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnMobileNetV3SmallFPN)\n\n</details>\n\n<details>\n  <summary style=\"font-weight: bold; color: #888888\">PyTorch Image Models (timm)</summary>\n\nAlso have a look at [Timm Documentation](https://huggingface.co/docs/timm/index).\n\n```python\nimport timm\n\ntimm.list_models(filter='*')  # explore available models\n```\n\n- [`cd.models.CpnTimmMaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnTimmMaNet)\n- [`cd.models.CpnTimmUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnTimmUNet)\n- [`cd.models.TimmEncoder`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.timmodels.TimmEncoder)\n- [`cd.models.TimmFPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.TimmFPN)\n- [`cd.models.TimmMaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.manet.TimmMaNet)\n- [`cd.models.TimmUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.TimmUNet)\n\n</details>\n\n<details>\n  <summary style=\"font-weight: bold; color: #888888\">Segmentation Models PyTorch (smp)</summary>\n\n```python\nimport segmentation_models_pytorch as smp\n\nsmp.encoders.get_encoder_names()  # explore available models\n```\n\n```python\nencoder = cd.models.SmpEncoder(encoder_name='mit_b5', pretrained='imagenet')\n```\n\nFind a list of [Smp Encoders](https://smp.readthedocs.io/en/latest/encoders.html) in the `smp` documentation.\n\n- [`cd.models.CpnSmpMaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnSmpMaNet)\n- [`cd.models.CpnSmpUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnSmpUNet)\n- [`cd.models.SmpEncoder`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.smp.SmpEncoder)\n- [`cd.models.SmpFPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.SmpFPN)\n- [`cd.models.SmpMaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.manet.SmpMaNet)\n- [`cd.models.SmpUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.SmpUNet)\n\n</details>\n\n<details>\n    <summary style=\"font-weight: bold; color: #888888\">U-Nets</summary>\n\n```python\n# U-Nets are available in 2D and 3D\nimport celldetection as cd\n\nmodel = cd.models.ResNeXt50UNet(in_channels=3, out_channels=1, nd=3)\n```\n\n- [`cd.models.U22`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.U22)\n- [`cd.models.U17`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.U17)\n- [`cd.models.U12`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.U12)\n- [`cd.models.UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.UNet)\n- [`cd.models.WideU22`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.WideU22)\n- [`cd.models.SlimU22`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.SlimU22)\n- [`cd.models.ResUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResUNet)\n- [`cd.models.UNetEncoder`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.UNetEncoder)\n- [`cd.models.ResNet50UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNet50UNet)\n- [`cd.models.ResNet18UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNet18UNet)\n- [`cd.models.ResNet34UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNet34UNet)\n- [`cd.models.ResNet152UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNet152UNet)\n- [`cd.models.ResNet101UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNet101UNet)\n- [`cd.models.ResNeXt50UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNeXt50UNet)\n- [`cd.models.ResNeXt152UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNeXt152UNet)\n- [`cd.models.ResNeXt101UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNeXt101UNet)\n- [`cd.models.WideResNet50UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.WideResNet50UNet)\n- [`cd.models.WideResNet101UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.WideResNet101UNet)\n- [`cd.models.MobileNetV3SmallUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.MobileNetV3SmallUNet)\n- [`cd.models.MobileNetV3LargeUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.MobileNetV3LargeUNet)\n\n</details>\n\n<details>\n    <summary style=\"font-weight: bold; color: #888888\">MA-Nets</summary>\n\n```python\n# Many MA-Nets are available in 2D and 3D\nimport celldetection as cd\n\nencoder = cd.models.ConvNeXtSmall(in_channels=3, nd=3)\nmodel = cd.models.MaNet(encoder, out_channels=1, nd=3)\n```\n\n- [`cd.models.MaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.manet.MaNet)\n- [`cd.models.SmpMaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.manet.SmpMaNet)\n- [`cd.models.TimmMaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.manet.TimmMaNet)\n\n</details>\n\n<details>\n    <summary style=\"font-weight: bold; color: #888888\">Feature Pyramid Networks</summary>\n\n- [`cd.models.FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.FPN)\n- [`cd.models.ResNet18FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNet18FPN)\n- [`cd.models.ResNet34FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNet34FPN)\n- [`cd.models.ResNet50FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNet50FPN)\n- [`cd.models.ResNeXt50FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNeXt50FPN)\n- [`cd.models.ResNet101FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNet101FPN)\n- [`cd.models.ResNet152FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNet152FPN)\n- [`cd.models.ResNeXt101FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNeXt101FPN)\n- [`cd.models.ResNeXt152FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNeXt152FPN)\n- [`cd.models.WideResNet50FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.WideResNet50FPN)\n- [`cd.models.WideResNet101FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.WideResNet101FPN)\n- [`cd.models.MobileNetV3LargeFPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.MobileNetV3LargeFPN)\n- [`cd.models.MobileNetV3SmallFPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.MobileNetV3SmallFPN)\n\n</details>\n\n<details>\n    <summary style=\"font-weight: bold; color: #888888\">ConvNeXt Networks</summary>\n\n```python\n# ConvNeXt Networks are available in 2D and 3D\nimport celldetection as cd\n\nmodel = cd.models.ConvNeXtSmall(in_channels=3, nd=3)\n```\n\n- [`cd.models.ConvNeXt`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.convnext.MaNet)\n- [`cd.models.ConvNeXtTiny`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.convnext.ConvNeXtTiny)\n- [`cd.models.ConvNeXtSmall`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.convnext.ConvNeXtSmall)\n- [`cd.models.ConvNeXtBase`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.convnext.ConvNeXtBase)\n- [`cd.models.ConvNeXtLarge`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.convnext.ConvNeXtLarge)\n\n</details>\n\n<details>\n    <summary style=\"font-weight: bold; color: #888888\">Residual Networks</summary>\n\n```python\n# Residual Networks are available in 2D and 3D\nimport celldetection as cd\n\nmodel = cd.models.ResNet50(in_channels=3, nd=3)\n```\n\n- [`cd.models.ResNet18`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNet18)\n- [`cd.models.ResNet34`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNet34)\n- [`cd.models.ResNet50`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNet50)\n- [`cd.models.ResNet101`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNet101)\n- [`cd.models.ResNet152`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNet152)\n- [`cd.models.WideResNet50_2`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.WideResNet50_2)\n- [`cd.models.ResNeXt50_32x4d`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNeXt50_32x4d)\n- [`cd.models.WideResNet101_2`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.WideResNet101_2)\n- [`cd.models.ResNeXt101_32x8d`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNeXt101_32x8d)\n- [`cd.models.ResNeXt152_32x8d`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNeXt152_32x8d)\n\n</details>\n\n<details>\n    <summary style=\"font-weight: bold; color: #888888\">Mobile Networks</summary>\n\n- [`cd.models.MobileNetV3Large`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.mobilenetv3.MobileNetV3Large)\n- [`cd.models.MobileNetV3Small`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.mobilenetv3.MobileNetV3Small)\n\n</details>\n\n## 🐳 Docker\n\nFind us on Docker Hub: https://hub.docker.com/r/ericup/celldetection\n\nYou can pull the latest version of `celldetection` via:\n```\ndocker pull ericup/celldetection:latest\n```\n\n<details>\n    <summary style=\"font-weight: bold; color: #888888\">CPN inference via Docker with GPU</summary>\n\n```\ndocker run --rm \\\n  -v $PWD/docker/outputs:/outputs/ \\\n  -v $PWD/docker/inputs/:/inputs/ \\\n  -v $PWD/docker/models/:/models/ \\\n  --gpus=\"device=0\" \\\n  celldetection:latest /bin/bash -c \\\n  \"python cpn_inference.py --tile_size=1024 --stride=768 --precision=32-true\"\n```\n</details>\n<details>\n    <summary style=\"font-weight: bold; color: #888888\">CPN inference via Docker with CPU</summary>\n\n```\ndocker run --rm \\\n  -v $PWD/docker/outputs:/outputs/ \\\n  -v $PWD/docker/inputs/:/inputs/ \\\n  -v $PWD/docker/models/:/models/ \\\n  celldetection:latest /bin/bash -c \\\n  \"python cpn_inference.py --tile_size=1024 --stride=768 --precision=32-true --accelerator=cpu\"\n```\n</details>\n\n\n\n### Apptainer\n\nYou can also pull our Docker images for the use with [Apptainer](https://apptainer.org/) (formerly [Singularity](https://github.com/apptainer/singularity)) with this command:\n\n```\napptainer pull --dir . --disable-cache docker://ericup/celldetection:latest\n```\n\n\n## 🤗 Hugging Face Spaces\n\nFind us on Hugging Face and upload your own images for segmentation: https://huggingface.co/spaces/ericup/celldetection\n\nThere's also an API (Python & JavaScript), allowing you to utilize community GPUs (currently Nvidia A100) remotely!\n\n<details>\n    <summary style=\"font-weight: bold; color: #888888\">Hugging Face API</summary>\n\n### Python\n\n```python\nfrom gradio_client import Client\n\n# Define inputs (local filename or URL)\ninputs = 'https://raw.githubusercontent.com/scikit-image/scikit-image/main/skimage/data/coins.png'\n\n# Set up client\nclient = Client(\"ericup/celldetection\")\n\n# Predict\noverlay_filename, img_filename, h5_filename, csv_filename = client.predict(\n    inputs,  # str: Local filepath or URL of your input image\n    \n    # Model name\n    'ginoro_CpnResNeXt101UNet-fbe875f1a3e5ce2c',\n    \n    # Custom Score Threshold (numeric value between 0 and 1)\n    False, .9,  # bool: Whether to use custom setting; float: Custom setting\n    \n    # Custom NMS Threshold\n    False, .3142,  # bool: Whether to use custom setting; float: Custom setting\n    \n    # Custom Number of Sample Points\n    False, 128,  # bool: Whether to use custom setting; int: Custom setting\n    \n    # Overlapping objects\n    True,  # bool: Whether to allow overlapping objects\n    \n    # API name (keep as is)\n    api_name=\"/predict\"\n)\n\n\n# Example usage: Code below only shows how to use the results\nfrom matplotlib import pyplot as plt\nimport celldetection as cd\nimport pandas as pd\n\n# Read results from local temporary files\nimg = imread(img_filename)\noverlay = imread(overlay_filename)  # random colors per instance; transparent overlap\nproperties = pd.read_csv(csv_filename)\ncontours, scores, label_image = cd.from_h5(h5_filename, 'contours', 'scores', 'labels')\n\n# Optionally display overlay\ncd.imshow_row(img, img, figsize=(16, 9))\ncd.imshow(overlay)\nplt.show()\n\n# Optionally display contours with text\ncd.imshow_row(img, img, figsize=(16, 9))\ncd.plot_contours(contours, texts=['score: %d%%\\narea: %d' % s for s in zip((scores * 100).round(), properties.area)])\nplt.show()\n```\n\n### Javascript\n\n```javascript\nimport { client } from \"@gradio/client\";\n\nconst response_0 = await fetch(\"https://raw.githubusercontent.com/scikit-image/scikit-image/main/skimage/data/coins.png\");\nconst exampleImage = await response_0.blob();\n\t\t\t\t\t\t\nconst app = await client(\"ericup/celldetection\");\nconst result = await app.predict(\"/predict\", [\n    exampleImage,  // blob: Your input image\n    \n    // Model name (hosted model or URL)\n    \"ginoro_CpnResNeXt101UNet-fbe875f1a3e5ce2c\",\n    \n    // Custom Score Threshold (numeric value between 0 and 1)\n    false, .9,  // bool: Whether to use custom setting; float: Custom setting\n    \n    // Custom NMS Threshold\n    false, .3142,  // bool: Whether to use custom setting; float: Custom setting\n    \n    // Custom Number of Sample Points\n    false, 128,  // bool: Whether to use custom setting; int: Custom setting\n    \n    // Overlapping objects\n    true,  // bool: Whether to allow overlapping objects\n    \n    // API name (keep as is)\n    api_name=\"/predict\"\n]);\n```\n\n</details>\n\n## 🧑‍💻 Napari Plugin\n\nFind our Napari Plugin here: https://github.com/FZJ-INM1-BDA/celldetection-napari </br>\nFind out more about Napari here: https://napari.org\n![bbbc039](https://raw.githubusercontent.com/FZJ-INM1-BDA/celldetection-napari/main/assets/coins-demo.png \"Napari Plugin\")\nYou can install it via pip:\n```\npip install git+https://github.com/FZJ-INM1-BDA/celldetection-napari.git\n```\n\n## 🏆 Awards\n\n- [NeurIPS 2022 Cell Segmentation Challenge](https://neurips22-cellseg.grand-challenge.org/): Winner Finalist Award\n\n## 📝 Citing\n\nIf you find this work useful, please consider giving a **star** ⭐️ and **citation**:\n\n```\n@article{UPSCHULTE2022102371,\n    title = {Contour proposal networks for biomedical instance segmentation},\n    journal = {Medical Image Analysis},\n    volume = {77},\n    pages = {102371},\n    year = {2022},\n    issn = {1361-8415},\n    doi = {https://doi.org/10.1016/j.media.2022.102371},\n    url = {https://www.sciencedirect.com/science/article/pii/S136184152200024X},\n    author = {Eric Upschulte and Stefan Harmeling and Katrin Amunts and Timo Dickscheid},\n    keywords = {Cell detection, Cell segmentation, Object detection, CPN},\n}\n```\n\n## 🔗 Links\n\n- [Article (sciencedirect)](https://www.sciencedirect.com/science/article/pii/S136184152200024X \"Contour Proposal Networks for Biomedical Instance Segmentation\")\n- [PDF (sciencedirect)](https://www.sciencedirect.com/sdfe/reader/pii/S136184152200024X/pdf \"Contour Proposal Networks for Biomedical Instance Segmentation\")\n- [PyPI](https://pypi.org/project/celldetection/ \"CellDetection\")\n- [Documentation](https://docs.celldetection.org \"Documentation\")\n\n## 🧑‍🔬 Thanks!\n\n[![Stargazers repo roster for @FZJ-INM1-BDA/celldetection](http://reporoster.com/stars/FZJ-INM1-BDA/celldetection)](https://github.com/FZJ-INM1-BDA/celldetection/stargazers)\n[![Forkers repo roster for @FZJ-INM1-BDA/celldetection](http://reporoster.com/forks/FZJ-INM1-BDA/celldetection)](https://github.com/FZJ-INM1-BDA/celldetection/network/members)\n",
                "dependencies": "numpy\nmatplotlib\nscipy\nscikit-image\nopencv-python\ntorch>=2.0.0\ntorchvision\ntensorboard\nseaborn\ntqdm\nh5py\npillow\nnvidia-ml-py\nalbumentations>=1.3.0\ntimm\nsegmentation-models-pytorch\npandas\npytorch_lightning\npyyaml\nfrom setuptools import setup\nfrom os.path import join, dirname, abspath\n\n\ndef read_utf8(*args):\n    with open(join(*args), encoding=\"utf-8\") as f:\n        return f.read()\n\n\ndirectory, m = dirname(abspath(__file__)), {}\nexec(read_utf8(directory, 'celldetection', '__meta__.py'), m)\nrequirements = read_utf8(directory, 'requirements.txt').strip().split(\"\\n\")\nlong_description = read_utf8(directory, 'README.md')\n\nsetup(\n    author=m['__author__'],\n    author_email=m['__email__'],\n    name=m['__title__'],\n    version=m['__version__'],\n    description=m['__summary__'],\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    url=m['__url__'],\n    packages=['celldetection', 'celldetection.data', 'celldetection.callbacks', 'celldetection.optim',\n              'celldetection.data.datasets', 'celldetection.models', 'celldetection.mpi', 'celldetection.ops',\n              'celldetection.util', 'celldetection.visualization', 'celldetection_scripts'],\n    package_data={'': ['LICENSE', 'requirements.txt', 'README.md']},\n    include_package_data=True,\n    install_requires=requirements,\n    license=m['__license__'],\n    keywords=['cell', 'detection', 'object', 'segmentation', 'pytorch', 'cpn', 'contour', 'proposal', 'network', 'deep',\n              'learning', 'unet', 'fzj', 'julich', 'juelich', 'ai'],\n    classifiers=[\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'Topic :: Scientific/Engineering :: Medical Science Apps.',\n        'License :: OSI Approved :: Apache Software License',\n        'Operating System :: OS Independent'\n    ],\n    entry_points={\n        'console_scripts': [\n            'cd-inference-cpn=celldetection_scripts.cpn_inference:main',\n            # 'cd-train=celldetection_scripts.train:main'\n         ]\n    }\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cellrank",
            "repo_link": "https://github.com/theislab/cellrank",
            "content": {
                "codemeta": "",
                "readme": "|PyPI| |Downloads| |CI| |Docs| |Codecov| |Discourse|\n\nCellRank 2: Unified fate mapping in multiview single-cell data\n==============================================================\n.. image:: docs/_static/img/light_mode_overview.png#gh-light-mode-only\n    :width: 600px\n    :align: center\n    :class: only-light\n\n.. image:: docs/_static/img/dark_mode_overview.png#gh-dark-mode-only\n    :width: 600px\n    :align: center\n\n**CellRank** is a modular framework to study cellular dynamics based on Markov state modeling of\nmulti-view single-cell data. See our `documentation`_, and the `CellRank 1`_ and `CellRank 2 manuscript`_ to learn more.\n\n.. important::\n    Please refer to :doc:`our citation guide <https://github.com/theislab/cellrank/blob/main/docs/about/cite.rst>` to cite our software correctly.\n\nCellRank scales to large cell numbers, is fully compatible with the `scverse`_ ecosystem, and easy to use.\nIn the backend, it is powered by `pyGPCCA`_ (`Reuter et al. (2018)`_). Feel\nfree to open an `issue`_ if you encounter a bug, need our help or just want to make a comment/suggestion.\n\nCellRank's key applications\n---------------------------\n- Estimate differentiation direction based on a varied number of biological priors, including RNA velocity\n  (`La Manno et al. (2018)`_, `Bergen et al. (2020)`_), any pseudotime or developmental potential,\n  experimental time points, metabolic labels, and more.\n- Compute initial, terminal and intermediate macrostates.\n- Infer fate probabilities and driver genes.\n- Visualize and cluster gene expression trends.\n- ... and much more, check out our `documentation`_.\n\n.. |PyPI| image:: https://img.shields.io/pypi/v/cellrank.svg\n    :target: https://pypi.org/project/cellrank\n    :alt: PyPI\n\n.. |Downloads| image:: https://static.pepy.tech/badge/cellrank\n    :target: https://pepy.tech/project/cellrank\n    :alt: Downloads\n\n.. |Discourse| image:: https://img.shields.io/discourse/posts?color=yellow&logo=discourse&server=https%3A%2F%2Fdiscourse.scverse.org\n    :target: https://discourse.scverse.org/c/ecosystem/cellrank/\n    :alt: Discourse\n\n.. |CI| image:: https://img.shields.io/github/actions/workflow/status/theislab/cellrank/test.yml?branch=main\n    :target: https://github.com/theislab/cellrank/actions\n    :alt: CI\n\n.. |Docs|  image:: https://img.shields.io/readthedocs/cellrank\n    :target: https://cellrank.readthedocs.io/\n    :alt: Documentation\n\n.. |Codecov| image:: https://codecov.io/gh/theislab/cellrank/branch/main/graph/badge.svg\n    :target: https://codecov.io/gh/theislab/cellrank\n    :alt: Coverage\n\n\n.. _La Manno et al. (2018): https://doi.org/10.1038/s41586-018-0414-6\n.. _Bergen et al. (2020): https://doi.org/10.1038/s41587-020-0591-3\n.. _Reuter et al. (2018): https://doi.org/10.1021/acs.jctc.8b00079\n\n.. _scverse: https://scverse.org/\n.. _pyGPCCA: https://github.com/msmdev/pyGPCCA\n\n.. _CellRank 1: https://www.nature.com/articles/s41592-021-01346-6\n.. _CellRank 2 manuscript: https://doi.org/10.1038/s41592-024-02303-9\n.. _documentation: https://cellrank.org\n\n.. _issue: https://github.com/theislab/cellrank/issues/new/choose\n\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools>=61\", \"setuptools-scm[toml]>=6.2\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"cellrank\"\ndynamic = [\"version\"]\ndescription = \"CellRank: dynamics from multi-view single-cell data\"\nreadme = \"README.rst\"\nrequires-python = \">=3.9\"\nlicense = {file = \"LICENSE\"}\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n    \"Natural Language :: English\",\n    \"Operating System :: POSIX :: Linux\",\n    \"Operating System :: MacOS :: MacOS X\",\n    \"Operating System :: Microsoft :: Windows\",\n    \"Typing :: Typed\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering :: Bio-Informatics\",\n    \"Topic :: Scientific/Engineering :: Mathematics\",\n    \"Topic :: Scientific/Engineering :: Visualization\",\n]\nkeywords = [\n    \"single-cell\",\n    \"bio-informatics\",\n    \"RNA velocity\",\n    \"Markov chain\",\n    \"GPCCA\",\n]\nauthors = [\n    {name = \"Marius Lange\"},\n    {name = \"Michal Klein\"},\n    {name = \"Philipp Weiler\"},\n]\nmaintainers = [\n    {name = \"Michal Klein\", email = \"info@cellrank.org\"}\n]\n\n\ndependencies = [\n    \"anndata>=0.9\",\n    \"docrep>=0.3.0\",\n    \"joblib>=0.13.1\",\n    \"matplotlib>=3.5.0\",\n    \"networkx>=2.2\",\n    \"numba>=0.51.0,!=0.57.0\",\n    \"numpy>=1.22.0\",\n    \"pandas>=1.5.0\",\n    \"pygam>=0.8.0\",\n    \"pygpcca>=1.0.4\",\n    \"scanpy>=1.7.2\",\n    \"scikit-learn>=0.24.0\",\n    \"scipy>=1.12.0\",\n    \"scvelo>=0.2.5\",\n    \"seaborn>=0.10.0\",\n    \"wrapt>=1.12.1\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pre-commit>=3.0.0\",\n    \"tox>=4\",\n]\ntest = [\n    \"pytest>=8\",\n    \"pytest-mock>=3.5.0\",\n    \"pytest-cov>=4\",\n    \"pytest-xdist\",\n    \"coverage[toml]>=7\",\n    \"zarr\",\n    \"igraph\",\n    \"leidenalg\",\n    \"Pillow\",\n    \"jax\",\n]\ndocs = [\n    \"sphinx>=5.1.1\",\n    \"furo>=2022.09.29\",\n    \"myst-nb>=0.17.1\",\n    \"sphinx-tippy>=0.4.1\",\n    \"sphinx-autodoc-typehints>=1.10.3\",\n    \"sphinx_copybutton>=0.5.0\",\n    \"sphinx_design>=0.3.0\",\n    \"sphinxcontrib-bibtex>=2.3.0\",\n    \"sphinxcontrib-spelling>=7.6.2\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/theislab/cellrank\"\nDownload = \"https://cellrank.readthedocs.io/en/latest/installation.html\"\n\"Bug Tracker\" = \"https://github.com/theislab/cellrank/issues\"\nDocumentation = \"https://cellrank.readthedocs.io\"\n\"Source Code\" = \"https://github.com/theislab/cellrank\"\n\n[tool.setuptools]\npackage-dir = {\"\" = \"src\"}\ninclude-package-data = true\n\n[tool.setuptools_scm]\n\n[tool.ruff]\ntarget-version = \"py39\"\nline-length = 120\n\n[tool.ruff.lint]\nexclude = [\n    \".eggs\",\n    \".git\",\n    \".ruff_cache\",\n    \".tox\",\n    \"__pypackages__\",\n    \"_build\",\n    \"build\",\n    \"dist\",\n]\nignore = [\n    \"PT011\",  # TODO/\n    # Do not implicitly `return None` in function able to return non-`None` value\n    \"RET502\",\n    # Missing explicit `return` at the end of function able to return non-`None` value\n    \"RET503\",\n    # Do not assign a lambda expression, use a def -> lambda expression assignments are convenient\n    \"E731\",\n    # allow I, O, l as variable names -> I is the identity matrix, i, j, k, l is reasonable indexing notation\n    \"E741\",\n    # Missing docstring in public package\n    \"D104\",\n    # Missing docstring in public module\n    \"D100\",\n    # Missing docstring in __init__\n    \"D107\",\n    # Missing docstring in magic method\n    \"D105\",\n]\nselect = [\n    \"D\", # flake8-docstrings\n    \"E\", # pycodestyle\n    \"F\", # pyflakes\n    \"W\", # pycodestyle\n    \"Q\", # flake8-quotes\n    \"SIM\", # flake8-simplify\n    \"NPY\",  # NumPy-specific rules\n    \"PT\",  # flake8-pytest-style\n    \"TID\",  # flake8-tidy-imports\n    \"B\", # flake8-bugbear\n    \"UP\", # pyupgrade\n    \"C4\", # flake8-comprehensions\n    \"BLE\", # flake8-blind-except\n    \"T20\",  # flake8-print\n    \"RET\", # flake8-raise\n]\nunfixable = [\"B\", \"C4\", \"BLE\", \"T20\", \"RET\"]\n[tool.ruff.lint.per-file-ignores]\n\"tests/*\" = [\"D\"]\n\"*/__init__.py\" = [\"F401\"]\n\"docs/*\" = [\"D\"]\n[tool.ruff.lint.pydocstyle]\nconvention = \"numpy\"\n[tool.ruff.lint.flake8-tidy-imports]\nban-relative-imports = \"all\"\n[tool.ruff.lint.flake8-quotes]\ninline-quotes = \"double\"\n\n[tool.black]\nline-length = 120\ntarget-version = ['py39']\ninclude = '\\.pyi?$'\n\n[tool.isort]\nprofile = \"black\"\ninclude_trailing_comma = true\nsections = [\"FUTURE\", \"STDLIB\", \"THIRDPARTY\", \"GENERIC\", \"NUMERIC\", \"PLOTTING\", \"BIO\", \"FIRSTPARTY\", \"LOCALFOLDER\"]\n# also contains what we import in notebooks\nknown_generic = [\"wrapt\", \"joblib\"]\nknown_numeric = [\"numpy\", \"numba\", \"scipy\", \"jax\", \"pandas\", \"sklearn\", \"networkx\", \"statsmodels\"]\nknown_bio = [\"anndata\", \"scanpy\"]\nknown_plotting = [\"IPython\", \"matplotlib\", \"mpl_toolkits\", \"seaborn\"]\n\n[tool.pytest.ini_options]\ntestpaths = \"tests\"\nxfail_strict = true\n\n[tool.coverage.run]\nbranch = true\nparallel = true\nsource = [\"src/\"]\nomit = [\n    \"*/__init__.py\",\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    '\\#.*pragma:\\s*no.?cover',\n    \"^if __name__ == .__main__.:$\",\n    '^\\s*raise AssertionError\\b',\n    '^\\s*raise NotImplementedError\\b',\n    '^\\s*return NotImplemented\\b',\n]\nprecision = 2\nshow_missing = true\nskip_empty = true\nsort = \"Miss\"\n\n[tool.rstcheck]\nignore_directives = [\n    \"toctree\",\n    \"currentmodule\",\n    \"autosummary\",\n    \"module\",\n    \"automodule\",\n    \"autoclass\",\n    \"bibliography\",\n    \"grid\",\n]\nignore_roles = [\n    \"mod\",\n    \"class\",\n    \"attr\",\n    \"func\",\n    \"meth\",\n    \"doc\",\n    \"cite\",\n]\n\n[tool.doc8]\nmax_line_length = 120\nignore-path = \"docs/release/**.rst\"\n\n[tool.tox]\nlegacy_tox_ini = \"\"\"\n[tox]\n# TODO(michalk8): upgrade to `tox>=4.0` once `tox-conda` supports it\nrequires = tox-conda\nisolated_build = true\nenvlist = lint-code,py{3.9,3.10,3.11,3.12,3.13}-{slepc,noslepc}\nskip_missing_interpreters = true\n\n[testenv]\nconda_deps =\n    py: r-mgcv\n    py: rpy2\n    slepc: mpi4py\n    slepc: petsc4py\n    slepc: slepc4py\nconda_channels=\n    conda-forge\nextras = test\npassenv = PYTEST_* CI\ncommands =\n    python -m pytest {tty:--color=yes} {posargs: \\\n        --cov={envsitepackagesdir}{/}cellrank --cov-config={toxinidir}{/}pyproject.toml \\\n        --no-cov-on-fail --cov-report=xml --cov-report=term-missing:skip-covered}\n\n[testenv:lint-code]\ndescription = Lint the code.\ndeps = pre-commit>=3.0.0\nskip_install = true\ncommands =\n    pre-commit run --all-files --show-diff-on-failure\n\n[testenv:lint-docs]\ndescription = Lint the documentation.\ndeps =\nextras = docs\nignore_errors = true\nallowlist_externals = make\npassenv = PYENCHANT_LIBRARY_PATH\nsetenv = SPHINXOPTS = -W -q --keep-going\nchangedir = {toxinidir}{/}docs\ncommands =\n    # make linkcheck {posargs}\n    # make spelling {posargs}\n\n[testenv:clean-docs]\ndescription = Remove the documentation.\ndeps =\nskip_install = true\nchangedir = {toxinidir}{/}docs\nallowlist_externals = make\ncommands =\n    make clean\n\n[testenv:build-docs]\ndescription = Build the documentation.\ndeps =\nextras = docs\nallowlist_externals = make\nchangedir = {toxinidir}{/}docs\ncommands =\n    make html {posargs}\ncommands_post =\n    python -c 'import pathlib; print(\"Documentation is under:\", pathlib.Path(\"{toxinidir}\") / \"docs\" / \"_build\" / \"html\" / \"index.html\")'\n\n[testenv:build-package]\ndescription = Build the package.\ndeps =\n    build\n    twine\nallowlist_externals = rm\ncommands =\n    rm -rf {toxinidir}{/}dist\n    python -m build --sdist --wheel --outdir {toxinidir}{/}dist{/} {posargs:}\n    python -m twine check {toxinidir}{/}dist{/}*\ncommands_post =\n    python -c 'import pathlib; print(f\"Package is under:\", pathlib.Path(\"{toxinidir}\") / \"dist\")'\n\n[testenv:format-references]\ndescription = Format references.bib.\ndeps =\nskip_install = true\nallowlist_externals = biber\ncommands = biber --tool --output_file={toxinidir}{/}docs{/}references.bib --nolog \\\n    --output_align --output_indent=2 --output_fieldcase=lower \\\n    --output_legacy_dates --output-field-replace=journaltitle:journal,thesis:phdthesis,institution:school \\\n    {toxinidir}{/}docs{/}references.bib\n\"\"\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/chase",
            "repo_link": "https://github.com/ChASE-library/ChASE",
            "content": {
                "codemeta": "",
                "readme": "[![License](https://img.shields.io/github/license/ChASE-library/ChASE)](https://github.com/ChASE-library/ChASE/blob/master/LICENSE) [![DOI](https://zenodo.org/badge/349075288.svg)](https://zenodo.org/badge/latestdoi/349075288) [![Latest Version](https://img.shields.io/github/v/release/ChASE-library/ChASE)](https://github.com/ChASE-library/ChASE/releases/latest) [![DOI](https://img.shields.io/badge/DOI-10.1145%2F3313828%20-orange)](https://doi.org/10.1145/3313828) [![DOI](https://img.shields.io/badge/DOI-10.1002%2Fcpe.3394%20-orange)](https://doi.org/10.1002/cpe.3394) [![coverage](https://gitlab.jsc.fz-juelich.de/chase/chase-library/ChASE/badges/master/coverage.svg?job=coverage)](https://gitlab.jsc.fz-juelich.de/chase/chase-library/ChASE/badges/master/coverage.svg)\n<img src=\"docs/images/ChASE_Logo_RGB.png\" alt=\"Matrix Generation Pattern\" style=\"zoom:40%;\" />\n# ChASE: a Chebyshev Accelerated Subspace Eigensolver for Dense Eigenproblems\n\nThe **Ch**ebyshev **A**ccelerated **S**ubspace **E**igensolver (ChASE) is a modern and scalable library based on subspace iteration with polynomial acceleration to solve dense Hermitian (Symmetric) algebraic eigenvalue problems, especially solving dense Hermitian eigenproblems arragend in a sequence. Novel to ChASE is the computation of the spectral estimates that enter in the filter and an optimization of the polynomial degree that further reduces the necessary floating-point operations. \n\nChASE is written in C++ using the modern software engineering concepts that favor a simple integration in application codes and a straightforward portability over heterogeneous platforms. When solving sequences of Hermitian eigenproblems for a portion of their extremal spectrum, ChASE greatly benefits from the sequence’s spectral properties and outperforms direct solvers in many scenarios. The library ships with two distinct parallelization schemes, supports execution over distributed GPUs, and is easily extensible to other parallel computing architectures.\n\n## Use Case and Features\n\n- **Real and Complex:** ChASE is templated for real and complex numbers. So it can be used to solve *real symmetric* eigenproblems as well as *complex Hermitian* ones.\n- **Eigespectrum:** ChASE algorithm is designed to solve for the *extremal portion* of the eigenspectrum of matrix `A`. The library is particularly efficient when no more than `20%` of the extremal portion of the eigenspectrum is sought after. For larger fractions the subspace iteration algorithm may struggle to be competitive. Converge could become an issue for fractions close to or larger than `50%`.\n- **Type of Problem:** ChASE can currently handle only standard eigenvalue problems. \n- **Sequences:** ChASE is particularly efficient when dealing with *sequences of eigenvalue problems*, where the eigenvectors solving for one problem can be use as input to accelerate the solution of the next one.\n- **Vectors input:** Since it is based on subspace iteration, ChASE can receive as input a matrix of vector equal to the number of desired eigenvalues. ChASE can experience substantial speed-ups when this input matrix contains some information about the sought after eigenvectors.\n- **Degree optimization:** For a fixed accuracy level, ChASE can optimize the degree of the Chebyshev polynomial filter so as to minimize the number of FLOPs necessary to reach convergence.\n- **Precision:** ChASE is also templated to work in *Single Precision* (SP) or *Double Precision* (DP).\n\n## Versions of the library\n\nT\nCurrently, the library comes in one main versions: \n\n1. **ChASE-MPI**\n\n   ChASE-MPI is the default version of the library and can be installed with the minimum amount of dependencies (BLAS, LAPACK, and MPI).  It supports different configurations depending on the available hardware resources.\n\n   - **Shared memory build:** This is the simplest configuration and should be exclusively selected when ChASE is used on only one computing node or on a single CPU. \n   - **MPI+Threads build:** On multi-core homogeneous CPU clusters, ChASE is best used in its pure MPI build. In this configuration, ChASE is typically used with one MPI rank per NUMA domain and as many threads as number of available cores per NUMA domain.\n   - **GPU build:** ChASE-MPI can be configured to take advantage of GPUs on heterogeneous computing clusters. Currently we support the use of one GPU per MPI rank. Multiple-GPU per computing node can be used when MPI rank\nnumber per node equals to the GPU number per node.   \n   \n   ChASE-MPI support two types of data distribution of matrix `A` across 2D MPI grid:\n\n   - **Block Distribution**:  each MPI rank of 2D grid is assigned a block of dense matrix **A**.\n\n   - **Block-Cyclic Distribution**: an distribution scheme for implementation of dense matrix computations on distributed-memory machines, to improve the load balance of matrix computation if the amount of work differs for different entries of a matrix. For more details, please refer to [Netlib](https://www.netlib.org/scalapack/slug/node75.html) .\n\n## Quick Start\n\n### Installing Dependencies\n\n```bash\n#Linux Operating System\nsudo apt-get install cmake #install CMake\nsudo apt-get install build-essential #install GNU Compiler\nsudo apt-get install libopenblas-dev #install BLAS and LAPACK\nsudo apt-get install libopenmpi-dev #install MPI\n\n#Apple Mac Operating System \nsudo port install cmake #install CMake\nsudo port install gcc10 #install GNU Compiler\nsudo port select --set gcc mp-gcc10 #Set installed GCC as C compiler\nsudo port install OpenBLAS +native #install BLAS and LAPACK\nsudo port install openmpi #install MPI\nsudo port select --set mpi openmpi-mp-fortran #Set installed MPI as MPI compiler\n```\n\n### Cloning ChASE source code\n\n```bash\ngit clone https://github.com/ChASE-library/ChASE #cloning the ChASE repository\ngit checkout v1.0.0 #it is recommended to check out the latest stable tag.\n```\n\n### Building and Installing the ChASE library\n\n```bash\ncd ChASE/\nmkdir build\ncd build/\ncmake .. -DCMAKE_INSTALL_PREFIX=${ChASEROOT}\nmake install\n```\n\nMore details about the installation on both local machine and clusters, please refer to [User Documentation](https://chase-library.github.io/ChASE/quick-start.html).\n\n## Documentation\n\nThe documentation of ChASE is available [online](https://chase-library.github.io/ChASE/index.html).\n\nCompiling the documentation in local requires  enable `-DBUILD_WITH_DOCS=ON` flag when compiling ChASE library:\n\n```bash\ncmake .. -DBUILD_WITH_DOCS=ON\n```\n\n## Examples\n\nMultiple examples are provided, which helps user get familiar with ChASE. \n\n**Build ChASE with Examples** requires enable `-DBUILD_WITH_EXAMPLES=ON` flag when compiling ChASE library:\n\n```bash\ncmake .. -DBUILD_WITH_EXAMPLES=ON\n```\n\n**5 examples are available** in folder [examples](https://github.com/ChASE-library/ChASE/tree/master/examples):\n\n0. The example [0_hello_world](https://github.com/ChASE-library/ChASE/tree/master/examples/0_hello_world) constructs a simple Clement matrix and find a given number of its eigenpairs.\n\n1. The example [1_sequence_eigenproblems](https://github.com/ChASE-library/ChASE/tree/master/examples/1_sequence_eigenproblems) illustrates how ChASE can be used to solve a sequence of eigenproblems.\n2. The example [2_input_output](https://github.com/ChASE-library/ChASE/tree/master/examples/2_input_output) provides the configuration of parameters of ChASE from command line (supported by Boost); the parallel I/O which loads the local matrices into the computing nodes in parallel.\n3. The example [3_installation](https://github.com/ChASE-library/ChASE/tree/master/examples/3_installation) shows the way to link ChASE to other applications.\n4. The example [4_interface](https://github.com/ChASE-library/ChASE/tree/master/examples/4_interface) shows examples to use the C and Fortran interfaces of ChASE.\n\n## Developers\n\n### Main developers\n\n- Edoardo Di Napoli – Algorithm design and development\n- Xinzhe Wu – Algorithm development, advanced parallel (MPI and GPU) implementation and optimization, developer documentation\n\n### Current contributors\n\n- Davor Davidović – Advanced parallel GPU implementation and optimization\n- Nenad Mijić – ARM-based implementation and optimization, CholeskyQR, unitests, parallel IO\n\n\n### Past contributors\n\n- Xiao Zhang – Integration of ChASE into Jena BSE code\n- Miriam Hinzen, Daniel Wortmann – Integration of ChASE into FLEUR code\n- Sebastian Achilles – Library benchmarking on parallel platforms, documentation\n- Jan Winkelmann – DoS algorithm development and advanced `C++` implementation\n- Paul Springer – Advanced GPU implementation\n- Marija Kranjcevic – OpenMP `C++` implementation\n- Josip Zubrinic – Early GPU algorithm development and implementation\n- Jens Rene Suckert – Lanczos algorithm and GPU implementation\n- Mario Berljafa – Early `C` and `MPI` implementation using the Elemental library\n\n\n## Contribution\n\nThis repository mirrors the principal Gitlab repository. If you want to contribute as developer to this project please contact e.di.napoli@fz-juelich.de.\n\n## How to Cite the Code\n\nThe main reference of ChASE is [1] while [2] provides some early results on scalability and usage on sequences of eigenproblems generated by Materials Science applications.\n\n- [1] J. Winkelmann, P. Springer, and E. Di Napoli. *ChASE: a Chebyshev Accelerated Subspace iteration Eigensolver for sequences of Hermitian eigenvalue problems.* ACM Transaction on Mathematical Software, **45** Num.2, Art.21, (2019). [DOI:10.1145/3313828](https://doi.org/10.1145/3313828) , [[arXiv:1805.10121](https://arxiv.org/abs/1805.10121/) ]\n- [2] M. Berljafa, D. Wortmann, and E. Di Napoli. *An Optimized and Scalable Eigensolver for Sequences of Eigenvalue Problems.* Concurrency & Computation: Practice and Experience **27** (2015), pp. 905-922. [DOI:10.1002/cpe.3394](https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.3394) , [[arXiv:1404.4161](https://arxiv.org/abs/1404.4161) ].\n- [3] X. Wu, D. Davidović, S. Achilles,E. Di Napoli. ChASE: a distributed hybrid CPU-GPU eigensolver for large-scale hermitian eigenvalue problems. Proceedings of the Platform for Advanced Scientific Computing Conference (PASC22). [DOI:10.1145/3539781.3539792](https://dl.acm.org/doi/10.1145/3539781.3539792) , [[arXiv:2205.02491](https://arxiv.org/pdf/2205.02491/) ].\n\n## Copyright and License\n\n[3-Clause BSD License (BSD License 2.0)](https://github.com/ChASE-library/ChASE/blob/master/LICENSE)\n\n\n",
                "dependencies": "# -*- Mode: cmake -*-\ncmake_minimum_required( VERSION 3.8 )\n\nproject( ChASE LANGUAGES C CXX VERSION 1.3.0 )\nset(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} \"${CMAKE_CURRENT_SOURCE_DIR}/cmake\")\n# ## algorithm ##\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\n\nadd_library(chase_algorithm INTERFACE)\n\ninclude(GNUInstallDirs)\n\ntarget_include_directories( chase_algorithm INTERFACE\n  \"$<BUILD_INTERFACE:${CMAKE_CURRENT_LIST_DIR}>\"\n  $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>  # <prefix>/include/mylib\n)\n\ntarget_compile_features(chase_algorithm INTERFACE cxx_auto_type)\n\noption( CHASE_OUTPUT \"ChASE will provide output at each iteration\")\nif( CHASE_OUTPUT )\n  target_compile_definitions( chase_algorithm  INTERFACE \"-DCHASE_OUTPUT\" )\nendif()\n\noption(ENABLE_NSIGHT \"Enable profiling with Nvidia Nsight Systems\" OFF)\n\ninstall( TARGETS chase_algorithm\n  EXPORT chase-headers\n  LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n  INCLUDES DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n  ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n  )\n\n\ninstall(DIRECTORY algorithm DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n  FILES_MATCHING\n    PATTERN \"*.hpp\"\n    PATTERN \"*.inc\"\n)\n\ninstall(EXPORT chase-headers\n  NAMESPACE ChASE::\n  FILE chase-header.cmake\n  EXPORT_LINK_INTERFACE_LIBRARIES\n  DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\n  )\n\n## ChASE-MPI ##\nadd_subdirectory( \"ChASE-MPI/\")\n\n## Tests\nadd_executable( \"chase_driver\" tests/noinput.cpp )\nif(TARGET chase_cuda )\n  enable_language(CUDA)\n  target_link_libraries(chase_driver chase_mpi chase_cuda)\n  target_compile_definitions(chase_driver PRIVATE USE_GPU=1)\nelse()\n  target_link_libraries(chase_driver chase_mpi)\nendif()\n\nadd_subdirectory(\"interface\")\n\n# Examples\noption(BUILD_WITH_EXAMPLES \"Build the examples\" OFF)\nif(BUILD_WITH_EXAMPLES)\n        message(STATUS \"Building the examples of ChASE\")\n        add_subdirectory(\"./examples\")\nendif()\n\n\noption(ENABLE_TESTS \"Enable unit tests.\" OFF)\nif(ENABLE_TESTS)\n    MESSAGE(\"Test enabled. Finding GoogleTests.\")\n    include(${CMAKE_SOURCE_DIR}/cmake/external/Gtest/FetchGtest.cmake)\n    include(CTest)\n\n    set(MPI_RUN mpirun CACHE STRING \"MPI runner (mpirun/srun)\")\n    set(MPI_RUN_ARGS  CACHE STRING \"\")\n    set(MPI_TEST ON CACHE BOOL \"Run test with mpi\")\n    add_subdirectory(tests)\nendif()\n\n# Documentation\noption(BUILD_WITH_DOCS \"Build the examples\" OFF)\nif(BUILD_WITH_DOCS)\n    message(STATUS \"Building Documentation of ChASE\")\n    add_subdirectory(\"./docs\")\nendif()\n\n# Install ChASE as a CMake package\ninclude(CMakePackageConfigHelpers)\n\nif(TARGET chase_cuda )\nconfigure_package_config_file(\n    \"cmake/Config_CUDA.cmake.in\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/chase-config.cmake\"\n    INSTALL_DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\n)\n\nelse()\nconfigure_package_config_file(\n    \"cmake/Config.cmake.in\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/chase-config.cmake\"\n    INSTALL_DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\n)\n\nendif()\n\ninstall( FILES\n  \"${CMAKE_CURRENT_BINARY_DIR}/chase-config.cmake\"\n  DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\n  )\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cheetah",
            "repo_link": "https://github.com/desy-ml/cheetah.git",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/chemotion-eln",
            "repo_link": "https://github.com/ComPlat/chemotion_ELN",
            "content": {
                "codemeta": "",
                "readme": "# Chemotion [![Badge DOI]][DOI]\n\nAn **Electronic Lab Notebook** for chemists!\n\n---\n\n**⸢ [Installation] ⸥ ⸢ [Documentation] ⸥ ⸢ [Changelog] ⸥**\n\n---\n\n## Tests\n\n![Badge CI]\n\n---\n\n## Acknowledgments\n\nThis project has been funded by the **[DFG]**.\n\n[![DFG Logo]][DFG]\n\n\nFunded by the [Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)](https://www.dfg.de/) under the [National Research Data Infrastructure – NFDI4Chem](https://nfdi4chem.de/) – Projektnummer **441958208** since 2020.\n\n\n---\n\n## License\n\n**Copyright © `2015` - `2023` [Nicole Jung]** <br>\nof the **[Karlsruhe Institute of Technology]**.\n\n> This program is free software:\n>\n> You can redistribute it and / or  modify it under the terms <br>\n> of the GNU Affero General Public License as published by <br>\n> the Free Software Foundation, either version 3 of the <br>\n> License, or (at your option) any later version.\n>\n> This program is distributed in the hope that it will be useful, but <br>\n> WITHOUT ANY WARRANTY; without even the implied warranty <br>\n> of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n>\n> See the GNU Affero General Public License for more details.\n>\n> You should have received a copy of the GNU Affero<br>\n> General Public License along with this program.\n>\n> If not, see <https://www.gnu.org/licenses/>.\n\n\n\n<!----------------------------------------------------------------------------->\n\n[Installation]: https://www.chemotion.net/docs/eln/install_configure\n[Documentation]: https://www.chemotion.net/docs/eln\n[Changelog]: CHANGELOG.md\n\n[DFG]: https://www.dfg.de/en/\n[DFG Logo]: https://chemotion.net/img/logos/DFG_logo.png\n\n[Nicole Jung]: mailto:nicole.jung@kit.edu\n[Karlsruhe Institute of Technology]: https://www.kit.edu/english/\n\n[DOI]: https://doi.org/10.5281/zenodo.1054134\n\n[Badge CI]: https://github.com/ComPlat/chemotion_ELN/actions/workflows/ci.yml/badge.svg?branch=main\n[Badge DOI]: https://zenodo.org/badge/DOI/10.5281/zenodo.1054134.svg\n\n",
                "dependencies": "# frozen_string_literal: true\n\nsource 'https://rubygems.org'\n\ngem 'aasm'\ngem 'activejob-status'\ngem 'activerecord-nulldb-adapter'\ngem 'ancestry'\ngem 'api-pagination'\ngem 'caxlsx'\n\ngem 'backup'\ngem 'barby'\ngem 'bcrypt_pbkdf'\ngem 'bibtex-ruby'\ngem 'bootsnap'\ngem 'bootstrap', '~> 5.3'\ngem 'charlock_holmes'\ngem 'closure_tree'\ngem 'countries'\n\ngem 'delayed_cron_job'\ngem 'delayed_job_active_record'\ngem 'devise'\ngem 'dotenv-rails', require: 'dotenv/rails-now'\n\ngem 'ed25519'\n\ngem 'faker', require: false\ngem 'faraday'\ngem 'faraday-follow_redirects'\ngem 'faraday-multipart'\ngem 'font-awesome-rails'\ngem 'fugit'\ngem 'fun_sftp', git: 'https://github.com/fl9/fun_sftp.git', branch: 'allow-port-option'\ngem 'fx'\n\ngem 'grape'\ngem 'grape-entity'\ngem 'grape-kaminari'\ngem 'grape-swagger'\ngem 'grape-swagger-entity'\ngem 'grape-swagger-rails'\n\ngem 'graphql', '< 2.2'\n\ngem 'haml-rails'\ngem 'hashie-forbidden_attributes'\ngem 'httparty'\n\ngem 'image_processing', '~> 1.8'\ngem 'inchi-gem', '1.06.1', git: 'https://github.com/ComPlat/inchi-gem.git', branch: 'main'\n\ngem 'jquery-rails' # must be in, otherwise the views lack jquery, even though the gem is supplied by ketcher-rails\ngem 'jwt'\n\ngem 'kaminari'\ngem 'kaminari-grape'\n# gem 'ketcherails', git: 'https://github.com/complat/ketcher-rails.git', branch: 'upgrade-to-rails-6'\ngem 'ketcherails', git: 'https://github.com/complat/ketcher-rails.git', ref: 'd4ae864a0e2d9e853eac8e4fc4ce7e3ab8174f80'\n\ngem 'labimotion', '1.4.1'\n\ngem 'mimemagic', '0.3.10'\ngem 'mime-types'\n\n# locked to enforce latest version of net-scp. without lock net-ssh would be updated first which locks\n# out newer net-scp versions\ngem 'net-scp', '3.0.0'\ngem 'net-sftp'\ngem 'net-ssh'\ngem 'nokogiri'\n\ngem 'omniauth', '~> 1.9.1'\ngem 'omniauth-github', '~> 1.4.0'\ngem 'omniauth-oauth2', '~> 1.7', '>= 1.7.2'\ngem 'omniauth_openid_connect'\ngem 'omniauth-orcid', git: 'https://github.com/datacite/omniauth-orcid'\ngem 'omniauth-shibboleth'\n\ngem 'chemical_elements'\ngem 'openbabel', '2.4.90.3', git: 'https://github.com/ptrxyz/openbabel-gem.git', branch: 'ptrxyz-ctime-fix'\ngem 'pandoc-ruby'\ngem 'paranoia', '2.6.0'\ngem 'pg'\ngem 'pg_search'\ngem 'prawn'\ngem 'prawn-svg'\ngem 'puma', '< 6.0.0'\ngem 'pundit'\n\ngem 'rack'\ngem 'rack-cors', require: 'rack/cors'\ngem 'rails', '~> 6.1.7.7'\ngem 'rdkit_chem', git: 'https://github.com/ptrxyz/rdkit_chem.git', branch: 'pk01'\ngem 'rinchi-gem', '1.0.2', git: 'https://github.com/ComPlat/rinchi-gem.git', branch: 'main'\ngem 'rmagick'\ngem 'roo'\ngem 'rqrcode' # required for Barby to work but not listed as its dependency -_-\ngem 'rtf'\ngem 'ruby-geometry', require: 'geometry'\ngem 'ruby-mailchecker'\ngem 'ruby-ole'\n\ngem 'sablon', git: 'https://github.com/ComPlat/sablon'\ngem 'sassc-rails'\ngem 'scenic'\ngem 'schmooze'\ngem 'semacode', git: 'https://github.com/toretore/semacode.git', branch: 'master' # required for Barby but not listed...\n\ngem 'sentry-delayed_job'\ngem 'sentry-rails'\ngem 'sentry-ruby'\ngem 'shrine', '~> 3.0'\ngem 'sys-filesystem'\n\ngem 'thor'\ngem 'thumbnailer', git: 'https://github.com/merlin-p/thumbnailer.git'\ngem 'turbo-sprockets-rails4'\ngem 'tzinfo-data'\n\ngem 'webpacker', git: 'https://github.com/rails/webpacker', branch: 'master'\ngem 'whenever', require: false\n\ngem 'yaml_db'\n\ngem 'icalendar'\n\ngroup :development do\n  gem 'better_errors' # allows to debug exception on backend from browser\n\n  gem 'fast_stack'    # For Ruby MRI 2.0\n  gem 'flamegraph'\n\n  gem 'memory_profiler'\n\n  #  gem 'rack-mini-profiler', git: 'https://github.com/MiniProfiler/rack-mini-profiler'\n  gem 'stackprof' # For Ruby MRI 2.1+\n\n  gem 'web-console'\nend\n\ngroup :vscode do\n  gem 'debase'\n  gem 'ruby-debug-ide'\n  gem 'solargraph'\nend\n\ngroup :development, :test do\n  gem 'annotate'\n  gem 'awesome_print'\n\n  gem 'binding_of_caller'\n  gem 'bullet'\n  gem 'byebug'\n\n  gem 'chronic'\n\n  gem 'listen'\n\n  gem 'meta_request'\n\n  gem 'pry-byebug'\n  gem 'pry-rails'\n\n  gem 'rubocop', require: false\n  gem 'rubocop-performance', require: false\n  gem 'rubocop-rails', require: false\n  gem 'rubocop-rspec', require: false\n\n  gem 'rspec'\n  gem 'rspec-rails'\n  gem 'ruby_jard'\n\n  gem 'spring'\nend\n\ngroup :test do\n  gem 'capybara'\n  gem 'cypress-on-rails'\n\n  gem 'database_cleaner'\n  gem 'database_cleaner-active_record'\n\n  gem 'factory_bot_rails'\n\n  gem 'launchy'\n\n  gem 'rspec-repeat'\n\n  gem 'shoulda-matchers'\n\n  gem 'simplecov', require: false\n  gem 'simplecov-lcov', require: false\n\n  gem 'webdrivers'\n  gem 'webmock'\nend\n\n{\n  \"name\": \"chemotion\",\n  \"repository\": {},\n  \"dependencies\": {\n    \"@babel/core\": \"^7.11.5\",\n    \"@babel/plugin-proposal-class-properties\": \"^7.11.0\",\n    \"@babel/plugin-proposal-object-rest-spread\": \"^7.11.0\",\n    \"@babel/plugin-transform-modules-commonjs\": \"^7.13.8\",\n    \"@babel/plugin-transform-runtime\": \"^7.24.7\",\n    \"@babel/polyfill\": \"^7.10.4\",\n    \"@babel/preset-env\": \"^7.11.0\",\n    \"@babel/preset-react\": \"7.10.4\",\n    \"@citation-js/plugin-isbn\": \"0.3.0\",\n    \"@complat/chem-spectra-client\": \"1.3.2\",\n    \"@complat/chemotion-converter-client\": \"~0.12.0\",\n    \"@complat/react-spectra-editor\": \"1.3.2\",\n    \"@novnc/novnc\": \"^1.2.0\",\n    \"@rails/ujs\": \"^6.1.3-1\",\n    \"@rails/webpacker\": \"https://github.com/rails/webpacker.git\",\n    \"@sentry/react\": \"^7.73.0\",\n    \"@sentry/tracing\": \"^7.16.0\",\n    \"@sentry/webpack-plugin\": \"^2.22.4\",\n    \"acorn\": \"^5.7.0\",\n    \"ag-grid-community\": \"^32.0.1\",\n    \"ag-grid-react\": \"^32.0.1\",\n    \"alt\": \"0.18.6\",\n    \"alt-utils\": \"2.0.0\",\n    \"antd\": \"^5.17.2\",\n    \"aviator\": \"v0.6.1\",\n    \"base-64\": \"^0.1.0\",\n    \"chem-generic-ui\": \"^1.4.9\",\n    \"citation-js\": \"0.6.8\",\n    \"classnames\": \"^2.2.5\",\n    \"commonmark\": \"^0.28.1\",\n    \"create-react-class\": \"^15.6.3\",\n    \"d3\": \"^3.5.15\",\n    \"d3-selection\": \"^3.0.0\",\n    \"deep-equal\": \"1.0.1\",\n    \"es6-promise-debounce\": \"^1.0.1\",\n    \"factory-bot\": \"^6.3.1\",\n    \"html2pdf.js\": \"^0.10.1\",\n    \"humps\": \"^2.0.1\",\n    \"immutable\": \"^4.0.0-rc.12\",\n    \"jcampconverter\": \"^2.11.0\",\n    \"js-yaml\": \"^3.13.1\",\n    \"jsdom\": \"^22.1.0\",\n    \"lodash\": \"^4.17.20\",\n    \"mime-types\": \"^2.1.35\",\n    \"mobx\": \"^6.6.0\",\n    \"mobx-react\": \"^7.5.0\",\n    \"mobx-state-tree\": \"^5.1.5\",\n    \"moment\": \"^2.29.4\",\n    \"moment-precise-range-plugin\": \"^1.2.4\",\n    \"npm\": \"^8.11.0\",\n    \"numeral\": \"^1.5.3\",\n    \"path\": \"^0.12.7\",\n    \"pdf-lib\": \"^1.17.1\",\n    \"prop-types\": \"15.6.2\",\n    \"quagga\": \"^0.11.5\",\n    \"querystring-es3\": \"^0.2.1\",\n    \"quill\": \"^1.3.7\",\n    \"quill-delta\": \"3.4.3\",\n    \"quill-delta-to-html\": \"0.8.2\",\n    \"quill-delta-to-plaintext\": \"^1.0.0\",\n    \"raw-loader\": \"^4.0.2\",\n    \"react\": \"^17.0.2\",\n    \"react-barcode\": \"^1.1.0\",\n    \"react-big-calendar\": \"^1.5.1\",\n    \"react-bootstrap\": \"~2.10.2\",\n    \"react-color\": \"^2.17.3\",\n    \"react-contextmenu\": \"^2.14.0\",\n    \"react-cookie\": \"^0.4.8\",\n    \"react-datepicker\": \"~7.3.0\",\n    \"react-datetime-picker\": \"^4.1.1\",\n    \"react-dnd\": \"^14.0.3\",\n    \"react-dnd-html5-backend\": \"^14.0.3\",\n    \"react-dom\": \"^17.0.2\",\n    \"react-draggable\": \"^4.4.3\",\n    \"react-dropzone\": \"^3.6.0\",\n    \"react-html-id\": \"^0.1.5\",\n    \"react-inlinesvg\": \"0.8.4\",\n    \"react-input-autosize\": \"1.1.0\",\n    \"react-json-editor-ajrm\": \"^2.5.10\",\n    \"react-markdown\": \"^6.0.2\",\n    \"react-molviewer\": \"^1.1.3\",\n    \"react-notification-system\": \"^0.2.7\",\n    \"react-papaparse\": \"^3.17.2\",\n    \"react-pdf\": \"^7.7.3\",\n    \"react-qr-reader\": \"^2.1.0\",\n    \"react-select\": \"^5.8.1\",\n    \"react-svg-file-zoom-pan\": \"0.1.5\",\n    \"react-svg-file-zoom-pan-latest\": \"npm:@complat/react-svg-file-zoom-pan@1.1.3\",\n    \"react-ui-tree\": \"3.1.0\",\n    \"react-vis\": \"1.12.1\",\n    \"reactflow\": \"^11.7.2\",\n    \"redux\": \"^4.1.0\",\n    \"sha256\": \"^0.2.0\",\n    \"spark-md5\": \"^3.0.1\",\n    \"svgedit\": \"^7.3.0\",\n    \"uglify-js\": \"~3.10.3\",\n    \"uglifyify\": \"^5.0.2\",\n    \"util\": \"^0.12.4\",\n    \"uuid\": \"^3.3.2\",\n    \"webpack-cli\": \"^5.1.4\",\n    \"whatwg-fetch\": \"^3.6.2\",\n    \"yarn\": \"^1.22.19\"\n  },\n  \"devDependencies\": {\n    \"@babel/eslint-parser\": \"^7.15.0\",\n    \"@babel/plugin-proposal-class-properties\": \"^7.11.0\",\n    \"@babel/register\": \"^7.11.5\",\n    \"@pmmmwh/react-refresh-webpack-plugin\": \"^0.5.5\",\n    \"@wojtekmaj/enzyme-adapter-react-17\": \"^0.6.3\",\n    \"babel-loader\": \"^8.2.2\",\n    \"cypress\": \"^12.3.0\",\n    \"enzyme\": \"^3.7.0\",\n    \"eslint\": \"^8.20.0\",\n    \"eslint-config-airbnb\": \"^19.0.4\",\n    \"eslint-filtered-fix\": \"^0.3.0\",\n    \"eslint-plugin-import\": \"^2.26.0\",\n    \"eslint-plugin-jsx-a11y\": \"^6.6.1\",\n    \"eslint-plugin-no-relative-import-paths\": \"^1.4.0\",\n    \"eslint-plugin-react\": \"^7.30.1\",\n    \"expect\": \"^24.7.1\",\n    \"mocha\": \"^10.2.0\",\n    \"nyc\": \"^15.1.0\",\n    \"process\": \"^0.11.10\",\n    \"react-refresh\": \"^0.12.0\",\n    \"sinon\": \"^15.2.0\",\n    \"webpack-dev-server\": \"^4.8.1\"\n  },\n  \"scripts\": {\n    \"postinstall\": \"./package_postinstall.sh\",\n    \"test\": \"NODE_PATH=./spec/javascripts:./app/packs yarn mocha --exit --require '@babel/register' './spec/javascripts/helper/setup.js' './spec/javascripts/**/*.spec.js'\",\n    \"coverage\": \"yarn nyc npm test;yarn nyc report --reporter=html\"\n  },\n  \"nyc\": {\n    \"exclude\": [\n      \"**/*.spec.js\",\n      \"spec/javascripts/fixture/*\",\n      \"spec/javascripts/helper/*\"\n    ]\n  },\n  \"license\": \"MIT\",\n  \"engines\": {\n    \"node\": \"^18.19.1\"\n  },\n  \"browserslist\": [\n    \"defaults\"\n  ],\n  \"babel\": {\n    \"presets\": [\n      \"@rails/webpacker/package/babel/preset.js\",\n      \"@babel/preset-react\"\n    ]\n  }\n}\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/climate-index-collection",
            "repo_link": "https://github.com/MarcoLandtHayen/climate_index_collection",
            "content": {
                "codemeta": "",
                "readme": "# Climate Index Collection\n\n[![Build Status](https://github.com/MarcoLandtHayen/climate_index_collection/workflows/Tests/badge.svg)](https://github.com/MarcoLandtHayen/climate_index_collection/actions)\n[![codecov](https://codecov.io/gh/MarcoLandtHayen/climate_index_collection/branch/main/graph/badge.svg)](https://codecov.io/gh/MarcoLandtHayen/climate_index_collection)\n[![License:MIT](https://img.shields.io/badge/License-MIT-lightgray.svg?style=flt-square)](https://opensource.org/licenses/MIT)\n[![Docker Image Version (latest by date)](https://img.shields.io/docker/v/mlandthayen/climate_index_collection?label=DockerHub)](https://hub.docker.com/r/mlandthayen/climate_index_collection/tags)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7779883.svg)](https://doi.org/10.5281/zenodo.7779883)\n\n\nCollection of climate indices derived from climate model outputs.\n\n\n## Quickstart: Using the climate indices\n\nThe resulting climate-index time series are published to Zenodo under the DOI [10.5281/zenodo.7779883](https://doi.org/10.5281/zenodo.7779883) and you can obtain the index timeseries by manually downloading the file `climate_indices.csv` from this dataset.\n\nYou can also use [pooch](https://www.fatiando.org/pooch/latest/) to obtain the published index time series programmatically:\n```python\nimport pooch\n\nclimate_indices_file = pooch.retrieve(\n    url=\"doi:10.5281/zenodo.7779883/climate_indices.csv\",\n    known_hash=None,\n)\n```\nWith `climate_indices_file` containing the path to the CSV file either resulting from the code above or from manually setting it to the location of the manually downloaded data, we recommend using [Pandas](https://pandas.pydata.org/docs/) for reading the data:\n```python\nimport pandas as pd\n\nclimate_indices = pd.read_csv(climate_indices_file)\n```\nThis results in a dataframe with the following structure:\n```python\nprint(climate_indices)\n```\n```\n       model  year  month   index                         long_name     value\n0       FOCI     1      1  SAM_ZM  southern_annular_mode_zonal_mean -0.295492\n1       FOCI     1      2  SAM_ZM  southern_annular_mode_zonal_mean  0.530890\n2       FOCI     1      3  SAM_ZM  southern_annular_mode_zonal_mean  1.684005\n3       FOCI     1      4  SAM_ZM  southern_annular_mode_zonal_mean  1.409169\n4       FOCI     1      5  SAM_ZM  southern_annular_mode_zonal_mean  0.984511\n...      ...   ...    ...     ...                               ...       ...\n695647  CESM   999      8      NP                     north_pacific -0.210202\n695648  CESM   999      9      NP                     north_pacific  0.206541\n695649  CESM   999     10      NP                     north_pacific -0.331067\n695650  CESM   999     11      NP                     north_pacific  0.487844\n695651  CESM   999     12      NP                     north_pacific -0.782657\n\n[695652 rows x 6 columns]\n```\nTo apply statistics or to plot all indices, you can apply standard modifications provided by Pandas. Calculating, e.g., the standard deviation of all indices amounts to\n```python\nprint(climate_indices.groupby([\"model\", \"index\"])[[\"value\"]].std())\n```\n```\nmodel  index\nCESM   AMO            0.109084\n       ENSO_12        0.603481\n       ENSO_3         0.881938\n       ENSO_34        0.933544\n       ENSO_4         0.909434\n       NAO_PC         1.000042\n       NAO_ST         1.554596\n       NP             0.569459\n       PDO_PC         1.000042\n...    ...            ...\nFOCI   AMO            0.128715\n       ENSO_12        0.342368\n       ENSO_3         0.600242\n       ENSO_34        0.759405\n       ENSO_4         0.923854\n       NAO_PC         1.000042\n       NAO_ST         1.459676\n       NP             0.644014\n       PDO_PC         1.000042\n...    ...            ...\nName: value, dtype: float64\n```\n\n## Quickstart: Reproducing the dataset\n\nThe Python package in this repository can be installed using [`pip`](https://pip.pypa.io/en/stable/getting-started/#install-a-package-from-github):\n```shell\n$ python -m pip install git+https://github.com/MarcoLandtHayen/climate_index_collection.git@v2023.03.29.1\n```\nThe data from which the indices have been calculated are published under the DOI [10.5281/zenodo.7060385](https://doi.org/10.5281/zenodo.7060385). After downloading the data to, e.g., `./cicmod_data/`, you can run the command line version of this package by\n```shell\n$ climate_index_collection_run --input-path ./cicmod_data/ --output-path .\n```\nwhich will create a file `climate_indices.csv`.\n\nPlease see either `$ climate_index_collection_run --help` on the command line, or the tutorial notebook in [notebooks/Tutorial.ipynb](notebooks/Tutorial.ipynb) for more details.\n\n\n## Development\n\nFor now, we're developing in the Pangeo notebook container. More details: https://github.com/pangeo-data/pangeo-docker-images\n\nTo start a JupyterLab within this container, run\n```shell\n$ docker pull pangeo/pangeo-notebook:2022.07.27\n$ docker run -p 8888:8888 --rm -it -v $PWD:/work -w /work pangeo/pangeo-notebook:2022.07.27 jupyter lab --ip=0.0.0.0\n```\nand open the URL starting on `http://127.0.0.1...`.\n\nThen, open a Terminal within JupyterLab and run\n```shell\n$ python -m pip install -e .\n```\nto have a local editable installation of the package.\n\n## Container Image\n\nThere's a container image: https://hub.docker.com/r/mlandthayen/climate_index_collection\n\n### Use with Docker\n\nYou can use it wherever Docker is installed by running:\n```shell\n$ docker pull mlandthayen/climate_index_collection:<tag>\n$ docker run --rm -v $PWD:/work -w /work mlandthayen/climate_index_collection:<tag> climate_index_collection_run --help\n```\nHere, `<tag>` can either be `latest` or a more specific tag.\n\n### Use with Singularity\n\nYou can use it wherever Singularity is installed by essentially running:\n```shell\n$ singularity pull --disable-cache --dir \"${PWD}\" docker://mlandthayen/climate_index_collection:<tag>\n$ singularity run climate_index_collection_<tag>.sif climate_index_collection_run --help\n```\nHere, `<tag>` can either be `latest` or a more specific tag.\n\n_Note_ that for NESH, it's currently necessary to\n- specify the version of singularity to use, and\n- to make sure to bind mount various parts of the file system explicitly.\n\nSo the full call on NESH would look like:\n```shell\n$ module load singularity/3.5.2\n$ singularity pull --disable-cache --dir \"${PWD}\" docker://mlandthayen/climate_index_collection:<tag>\n$ singularity run -B /sfs -B /gxfs_work1 -B ${PWD}:/work --pwd /work climate_index_collection_<tag>.sif climate_index_collection_run --help\n```\n\n## Release Procedure\n\nA release will contain the specific version of the package (taken care of automatically) and the CSV file created with the full data.\n\n1. _**Draft a release:**_ Go to https://github.com/MarcoLandtHayen/climate_index_collection/releases/new and draft a new release (don't publish yet).\n\n2. _**Prepeare data:**_ For the commit in `main` for which the release is planned, pull the container on NESH (see above) and run:\n```\n$ singularity run -B /sfs -B /gxfs_work1 -B ${PWD}:/work --pwd /work climate_index_collection_<tag>.sif climate_index_collection_run --input-path <path_to_full_data>\n```\n\n3. _**Attach data:**_ Attach the CSV file to the drafted release.\n\n4. _**Publish:**_ By clicking on the `Publish release` button.\n\n--------\n\n<p><small>Project based on the <a target=\"_blank\" href=\"https://github.com/jbusecke/cookiecutter-science-project\">cookiecutter science project template</a>.</small></p>\n\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools>=42\", \"wheel\", \"setuptools_scm[toml]>=3.4\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools_scm]\n\n[tool.interrogate]\nignore-init-method = true\nignore-init-module = false\nignore-magic = false\nignore-semiprivate = true\nignore-private = true\nignore-property-decorators = true\nignore-module = false\nfail-under = 95\nexclude = [\"setup.py\", \"docs\", \"tests\"]\nverbose = 1\nquiet = false\ncolor = true\n\n[tool.isort]\nknown_third_party = [\"cftime\", \"click\", \"numpy\", \"pandas\", \"pkg_resources\", \"pytest\", \"scipy\", \"setuptools\", \"shapely\", \"xarray\", \"xhistogram\"]\n\n[tool.pytest.ini_options]\nminversion = \"6.0\"\naddopts = \"-v\"\n# only test the root level, otherwise it picks up the tests of the project template\ntestpaths = [\n    \"tests\",\n]\n\nfrom setuptools import setup\n\n\nsetup(\n    use_scm_version={\n        \"write_to\": \"climate_index_collection/_version.py\",\n        \"write_to_template\": '__version__ = \"{version}\"',\n        \"tag_regex\": r\"^(?P<prefix>v)?(?P<version>[^\\+]+)(?P<suffix>.*)?$\",\n        \"local_scheme\": \"node-and-date\",\n    },\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cimpredict",
            "repo_link": "https://github.com/tonitacker/CIMPredict",
            "content": {
                "codemeta": "",
                "readme": "# CIMPredict \n\n\n## Description:\nCIMPredict is used to generate Fracture Risk Values in % for A timeframe of two years upon consultation.\nThe CIM-blood value and the patients age are used to calculate a Fracture Risk based on a model trained by study data\nthat include CIM-Values and information about fractures that occured within two years.\n\n## Features: \nCIMPredict offers a numerical risk in percent of a patient suffering an osteoporotic fracture within the next two years\n\n## Installation:\n1. Download the ZIP-File found in the releases tab and extract it to your preferred folder\n2. Start CIMPredict.exe\n\nDo not move the EXE-File out of the directory in comes in. You can create a shortcut to the exe if you wish. \nThe functionality of the software relies on the data in the directory.\n\nThe software checks for new versions of the dependencies, thus it may take a few minutes upon first start for the main window to appear.\nDo not interrupt this process\n\n## Usage:\n\nEnter the CIM-Value using a decimal point, not a comma into the dedicated field.\nSimilarly, provide a valid age (between 51 and 90 years).\nThe \"Berechnen\"-button is made accessible once all values are provided. \nClick it to retrieve the fracture risk value.\n\n## License\n\nThis project is licensed under the GNU General Public License v3.0 - see the [LICENSE](./.LICENSE) file for details.\n\n### Contact\n\nFor further information or inquiries, please contact:\n\n- **Name:** Anton Krackhardt\n- **E-Mail:** antonkrackhardt444@gmail.com\n\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/circlize",
            "repo_link": "https://github.com/jokergoo/circlize",
            "content": {
                "codemeta": "",
                "readme": "\r\n\r\n# circlize: circular visualization in R <a href=\"https://jokergoo.github.io/circlize_book/book/\"><img src=\"https://jokergoo.github.io/circlize_book/book/images/circlize_cover.jpg\" width=240 align=\"right\" ></a>\r\n\r\n\r\n[![R-CMD-check](https://github.com/jokergoo/circlize/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/circlize/actions)\r\n[![CRAN](https://www.r-pkg.org/badges/version/circlize)](https://cran.r-project.org/web/packages/circlize/index.html)\r\n[![CRAN](https://cranlogs.r-pkg.org/badges/grand-total/circlize)](https://cran.r-project.org/web/packages/circlize/index.html)\r\n[![Codecov test coverage](https://codecov.io/gh/jokergoo/circlize/branch/master/graph/badge.svg)](https://codecov.io/gh/jokergoo/circlize?branch=master)\r\n\r\nCircular layout is an efficient way for the visualization of huge\r\n    amounts of information. Here the circlize package provides an implementation\r\n    of circular layout generation in R as well as an enhancement of available\r\n    software. The flexibility of this package is based on the usage of low-level\r\n    graphics functions such that self-defined high-level graphics can be easily\r\n    implemented by users for specific purposes. Together with the seamless\r\n    connection between the powerful computational and visual environment in R,\r\n    circlize gives users more convenience and freedom to design figures for\r\n    better understanding complex patterns behind multi-dimensional data.\r\n\r\n## Citation\r\n\r\nZuguang Gu, et al., circlize Implements and enhances circular visualization in R. Bioinformatics (Oxford, England) 2014. [PubMed](https://www.ncbi.nlm.nih.gov/pubmed/24930139)\r\n\r\n## Documentation\r\n\r\nThe full documentations are available at https://jokergoo.github.io/circlize_book/book/ and the online website is at https://jokergoo.github.io/circlize/.\r\n\r\n## Blog posts\r\n\r\nThere are the following blog posts focusing on specific topics.\r\n\r\n- [Make circular heatmaps](https://jokergoo.github.io/2020/05/21/make-circular-heatmaps/)\r\n- [Multiple-group Chord diagram](https://jokergoo.github.io/2020/06/08/multiple-group-chord-diagram/)\r\n- [Changes in circlize 0.4.10](https://jokergoo.github.io/2020/06/14/changes-in-circlize-0.4.10/)\r\n- [Reverse x-axes in the circular plot](https://jokergoo.github.io/2020/08/17/reverse-x-axes-in-the-circular-plot/)\r\n\r\n## Examples\r\n\r\nSee https://jokergoo.github.io/circlize_examples/.\r\n\r\n<img width=\"700\" alt=\"circlize_example\" src=\"https://jokergoo.github.io/circlize_book/book/images/ciclize_examples.jpg\">\r\n\r\n## Install\r\n\r\nThe package can be installed from CRAN:\r\n\r\n```r\r\ninstall.packages(\"circlize\")\r\n```\r\n\r\nor directly from GitHub:\r\n\r\n```r\r\ndevtools::install_github(\"jokergoo/circlize\")\r\n```\r\n\r\n## Basic design\r\n\r\nSince most of the figures are composed of points, lines and polygons,\r\nwe just need to implement functions for drawing points, lines and polygons,\r\nthen the plots will not be restricted in any specific types.\r\n\r\nCurrent there are following low-level graphic functions:\r\n\r\n- `circos.points()`\r\n- `circos.lines()`\r\n- `circos.segments()`\r\n- `circos.rect()`\r\n- `circos.polygon()`\r\n- `circos.text()`\r\n- `circos.axis()`\r\n- `circos.raster()`\r\n- `circos.arrow()`\r\n- `circos.raster()`\r\n- `circos.barplot()`\r\n- `circos.boxplot()`\r\n- `circos.link()`, This maybe the unique feature for circos layout to represent relationships between elements.\r\n\r\nFor drawing points, lines and text through the whole track (among several sectors), the following\r\nfunctions are available:\r\n\r\n- `circos.trackPoints()`\r\n- `circos.trackLines()`\r\n- `circos.trackText()`\r\n\r\nDraw circular heatmaps\r\n\r\n- `circos.heatmap()`\r\n\r\nFunctions to arrange the circular layout:\r\n\r\n- `circos.track()`\r\n- `circos.update()`\r\n- `circos.nested()`\r\n- `circos.par()`\r\n- `circos.info()`\r\n- `circos.clear()`\r\n\r\nTheoretically, you are able to draw most kinds of circular plots by the above functions.\r\n\r\nFor specific use in Genomics, we also implement functions which add graphics in genome scale.\r\n\r\nFunctions to initialize circular plot with genomic coordinates:\r\n\r\n- `circos.initializeWithIdeogram()`\r\n- `circos.genomicInitialize()`\r\n\r\nFunctions to arrange genomic circular layout:\r\n\r\n- `circos.genomicTrack()`\r\n\r\nFunctions to add basic graphics in genomic scale:\r\n\r\n- `circos.genomicPoints()`\r\n- `circos.genomicLines()`\r\n- `circos.genomicText()`\r\n- `circos.genomicRect()`\r\n- `circos.genomicLink()`\r\n\r\nFunctions with specific purpose:\r\n\r\n- `circos.genomicIdeogram()`\r\n- `circos.genomicHeatmap()`\r\n- `circos.genomicLabels()`\r\n- `circos.genomicDensity()`\r\n- `circos.genomicRainfall()`\r\n\r\nFinally, function that draws Chord diagram:\r\n\r\n- `chordDiagram()`\r\n\r\n\r\n## License\r\n\r\nMIT @ Zuguang Gu\r\n\n",
                "dependencies": "Package: circlize\nType: Package\nTitle: Circular Visualization\nVersion: 0.4.16\nDate: 2022-11-24\nAuthors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"),\n                  comment = c('ORCID'=\"0000-0002-7395-8709\"))\nDepends: R (>= 3.0.0), graphics\nImports: GlobalOptions (>= 0.1.2), shape, grDevices, utils, stats,\n    colorspace, methods, grid\nSuggests: \n    knitr,\n    dendextend (>= 1.0.1),\n    ComplexHeatmap (>= 2.0.0),\n    gridBase,\n    png,\n    markdown,\n    bezier,\n    covr,\n    rmarkdown\nVignetteBuilder: knitr\nDescription: Circular layout is an efficient way for the visualization of huge \n    amounts of information. Here this package provides an implementation \n    of circular layout generation in R as well as an enhancement of available \n    software. The flexibility of the package is based on the usage of low-level \n    graphics functions such that self-defined high-level graphics can be easily \n    implemented by users for specific purposes. Together with the seamless \n    connection between the powerful computational and visual environment in R, \n    it gives users more convenience and freedom to design figures for \n    better understanding complex patterns behind multiple dimensional data. \n    The package is described in Gu et al. 2014 <doi:10.1093/bioinformatics/btu393>.\nURL: https://github.com/jokergoo/circlize, https://jokergoo.github.io/circlize_book/book/\nLicense: MIT + file LICENSE\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/citation-file-format",
            "repo_link": "https://github.com/citation-file-format/citation-file-format",
            "content": {
                "codemeta": "",
                "readme": "# Citation File Format\n\n[![Build Status](https://github.com/citation-file-format/citation-file-format/workflows/testing/badge.svg?branch=main)](https://github.com/citation-file-format/citation-file-format/actions/workflows/testing.yml?query=branch%3Amain)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1003149.svg)](https://doi.org/10.5281/zenodo.1003149)\n[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)\n[![Project homepage](https://img.shields.io/badge/Project%20homepage-citation--file--format.github.io-ff0080)](https://citation-file-format.github.io)\n\nThe Citation File Format lets you provide citation metadata for software or datasets \nin plaintext files that are easy to read by both humans and machines.\n\n## Structure\n\nYou can specify citation metadata for your software (or dataset) in a file named `CITATION.cff`. \nThis is what a typical `CITATION.cff` file may look like for research software:\n\n```yaml\ncff-version: 1.2.0\nmessage: If you use this software, please cite it using these metadata.\ntitle: My Research Software\nabstract: This is my awesome research software. It does many things.\nauthors:\n  - family-names: Druskat\n    given-names: Stephan\n    orcid: \"https://orcid.org/1234-5678-9101-1121\"\n  - name: \"The Research Software project\"\nversion: 0.11.2\ndate-released: \"2021-07-18\"\nidentifiers:\n  - description: This is the collection of archived snapshots of all versions of My Research Software\n    type: doi\n    value: \"10.5281/zenodo.123456\"\n  - description: This is the archived snapshot of version 0.11.2 of My Research Software\n    type: doi\n    value: \"10.5281/zenodo.123457\"\nlicense: Apache-2.0\nrepository-code: \"https://github.com/citation-file-format/my-research-software\"\n```\n\nIn addition, the Citation File Format allows you to\n\n- provide references to works that your software or dataset builds on ([see here for more info](schema-guide.md#referencing-other-work));\n- ask people to cite a different, related work instead of the software or dataset itself ([see here for more info](schema-guide.md#credit-redirection)).\n\n## Format specifications :books:\n\n**You can find the complete format specifications in the [Guide to the Citation File Format schema](schema-guide.md).**\n\n## Why should I add a `CITATION.cff` file to my repository? :bulb:\n\nWhen you do this, great things may happen:\n\n1. Users of your software can easily cite it using the metadata from `CITATION.cff`!\n2. If your repository is hosted on GitHub, they will [show the citation information in the sidebar](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-citation-files), which makes it easy for visitors to cite your software or dataset correctly.\n3. When you publish your software on Zenodo via the [GitHub-Zenodo integration](https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content), they will use the metadata from your `CITATION.cff` file.\n4. People can import the correct reference to your software into the [Zotero](https://www.zotero.org) reference manager via a [browser plugin](https://www.zotero.org/download/).\n\n## Creation :heavy_plus_sign:\n\nTo create a `CITATION.cff` file, you can \n\n- use the [**cffinit** website](https://citation-file-format.github.io/cff-initializer-javascript/#/),\n- copy and paste the [example snippet](#structure), and adapt it to your needs, or\n- create a new file called `CITATION.cff` using the *Add file* button on GitHub, and use the template they provide.\n\n## Validation :heavy_check_mark:\n\nYou can validate your `CITATION.cff` file on the command line with the [`cffconvert` Python package](https://pypi.org/project/cffconvert/):\n\n```shell\n# Install cffconvert with pip in user space\npython3 -m pip install --user cffconvert\n\n# Validate your CFF file\ncffconvert --validate\n```\n\nIf you get a Traceback with error messages, look for the relevant validation error and fix it.\nIf the output is very long, it may help if you search it for lines starting with `jsonschema.exceptions.ValidationError`.\n\nIf you prefer to use Docker, you can use the [`cffconvert` Docker image](https://hub.docker.com/r/citationcff/cffconvert):\n\n```bash\ncd <directory-containing-your-CITATION.cff>\ndocker run --rm -v ${PWD}:/app citationcff/cffconvert --validate\n```\n\n<!-- Later, this should link to tutorials -->\n\n## Tools to work with `CITATION.cff` files :wrench:\n\nThere is tooling available to work with `CITATION.cff` files to do different things:\ncreate new files, edit existing files, validate existing files, convert files from the Citation File Format into another format.\nThe following table gives an overview of the tools that we know about. If there is a tool missing from this table, please [open a new issue](https://github.com/citation-file-format/citation-file-format/issues/new/choose) and let us know.\n\n|                | Creation                                                                        | Editing/Updating                                                    | Validation                                                                      | Conversion                                                                                                                                                                                                                                                                                                                                                                                        |\n| -------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Command line   |                                                                                 |                                                                     | • [cffconvert](#validation-heavy_check_mark)                                    | • [cffconvert](https://pypi.org/project/cffconvert/)<br> • [bibtex-to-cff](https://github.com/monperrus/bibtexbrowser/)<br>• [cff-from-621](https://pypi.org/project/cff-from-621/)<br>• [openCARP-CI](https://git.opencarp.org/openCARP/openCARP-CI/-/tree/master/#create_cff)                                                                                                                   |\n| GitHub Actions |                                                                                 |                                                                     | [cff-validator](https://github.com/marketplace/actions/cff-validator)           | • [cffconvert](https://github.com/marketplace/actions/cffconvert)<br>• [codemeta2cff](https://github.com/caltechlibrary/codemeta2cff)                                                                                                                                                                                                                                                             |\n| GitHub Bot     |                                                                                 |                                                                     | [#238](https://github.com/citation-file-format/citation-file-format/issues/238) |                                                                                                                                                                                                                                                                                                                                                                                                   |\n| Docker         |                                                                                 |                                                                     | [cffconvert Docker image](#validation-heavy_check_mark)                         | [cffconvert Docker image](https://hub.docker.com/r/citationcff/cffconvert)                                                                                                                                                                                                                                                                                                                        |\n| Go             |                                                                                 |                                                                     |                                                                                 | • [datatools/codemeta2cff](https://github.com/caltechlibrary/datatools/)                                                                                                                                                                                                                                                                                                                          |\n| Haskell        |                                                                                 | • [cffreference](https://github.com/kevinmatthes/cffreference)      |                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                   |\n| Java           | • [CFF Maven plugin](https://github.com/hexatomic/cff-maven-plugin)             | • [CFF Maven plugin](https://github.com/hexatomic/cff-maven-plugin) |                                                                                 | • [CFF Maven plugin](https://github.com/hexatomic/cff-maven-plugin)                                                                                                                                                                                                                                                                                                                               |\n| JavaScript     |                                                                                 |                                                                     |                                                                                 | • [Citation.js](https://citation.js.org/) [plugin](https://www.npmjs.com/package/@citation-js/plugin-software-formats)                                                                                                                                                                                                                                                                            |\n| Julia          |                                                                                 |                                                                     | • [Bibliography.jl](https://github.com/Humans-of-Julia/Bibliography.jl)         | • [Bibliography.jl](https://github.com/Humans-of-Julia/Bibliography.jl)                                                                                                                                                                                                                                                                                                                           |\n| PHP            |                                                                                 |                                                                     |                                                                                 | • [bibtex-to-cff](https://github.com/monperrus/bibtexbrowser/)                                                                                                                                                                                                                                                                                                                                    |\n| Python         |                                                                                 | • [cff2toml](https://github.com/willynilly/cff2toml)<br> • [doi2cff](https://github.com/citation-file-format/doi2cff) | • [cffconvert](#validation-heavy_check_mark)                                    | • [cff-from-621](https://pypi.org/project/cff-from-621/)<br>• [cff2toml](https://github.com/willynilly/cff2toml)<br>• [cffconvert](https://github.com/citation-file-format/cff-converter-python)<br>• [doi2cff](https://github.com/citation-file-format/doi2cff)<br>• [openCARP-CI](https://git.opencarp.org/openCARP/openCARP-CI/-/tree/master/#create_cff)<br>• [py_bibtex_to_cff_converter](https://github.com/vdplasthijs/py_bibtex_to_cff_converter) |\n| R              |                                                                                 |                                                                     | • [cffr](https://CRAN.R-project.org/package=cffr)                                                                                | • [citation](https://cran.r-project.org/web/packages/citation/)<br>• [r2cff](https://github.com/ocbe-uio/RCFF)<br>• [handlr](https://github.com/ropensci/handlr)<br>• [cffr](https://CRAN.R-project.org/package=cffr)                                                                                                                                                                             |\n| Ruby           | • [ruby-cff](https://github.com/citation-file-format/ruby-cff)                  | • [ruby-cff](https://github.com/citation-file-format/ruby-cff)      | • [ruby-cff](https://github.com/citation-file-format/ruby-cff)                  | • [ruby-cff](https://github.com/citation-file-format/ruby-cff)                                                                                                                                                                                                                                                                                                                                    |\n| Rust           | • [Aeruginous](https://github.com/kevinmatthes/aeruginous-rs)                   | • [Aeruginous](https://github.com/kevinmatthes/aeruginous-rs)       |                                                                                 | • [citeworks](https://github.com/passcod/citeworks)                                                                                                                                                                                                                                                                                                                                               |\n| TypeScript     |                                                                                 |                                                                     |                                                                                 | [#28](https://github.com/citation-file-format/citation-file-format/issues/28#issuecomment-892105342)                                                                                                                                                                                                                                                                                              |\n| Website        | • [cffinit](https://citation-file-format.github.io/cff-initializer-javascript/) |                                                                     |                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                   |\n\n## Maintainers :nerd_face:\n\nThe Citation File Format schema is maintained by\n\n- Stephan Druskat ([@sdruskat](https://github.com/sdruskat/))\n- Jurriaan H. Spaaks ([@jspaaks](https://github.com/jspaaks/))\n\n## Contributing :handshake:\n\nThe Citation File Format is a collaborative project and we welcome suggestions and contributions. We hope one of the invitations below works for you, but if not, please let us know!\n\n:running: **I'm busy, I only have 1 minute**\n- Tell a friend about the Citation File Format, or tweet about it!\n- Give the project a star :star:!\n\n:hourglass_flowing_sand: **I've got 10 minutes - tell me what I should do**\n- Create a `CITATION.cff` file for your repository.\n- Suggest ideas for how you would like to use the Citation File Format, or for an improvement to the format or its tooling.\n- If you know how to validate `CITATION.cff` files, help someone with a validation problem and look at the [issues labeled ![GitHub labels](https://img.shields.io/github/labels/citation-file-format/citation-file-format/validation)](https://github.com/citation-file-format/citation-file-format/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+label%3Avalidation)\n\n:computer: **I've got a few hours to work on this**\n- Help create tooling for the community by looking at the [issues labeled ![GitHub labels](https://img.shields.io/github/labels/citation-file-format/citation-file-format/tooling)](https://github.com/citation-file-format/citation-file-format/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+label%3Atooling)\n\n:tada: **I want to help grow the community**\n- Write a blog post or news item for your own community.\n- Organise a hack event or workshop to help others use or improve the Citation File Format.\n\nPlease read the more detailed [contributing guidelines](CONTRIBUTING.md) and [open a GitHub issue](https://github.com/citation-file-format/citation-file-format/issues) to suggest a new idea or let us know about bugs. Please put up pull requests for changes to the format and schema against the `develop` branch!\n\n## License :balance_scale:\n\nCopyright © 2016 - 2023. The Citation File Format Contributors\n\nThis work is licensed under a [Creative Commons Attribution 4.0 International (CC-BY-4.0)](https://creativecommons.org/licenses/by/4.0/legalcode) license.\n\n## Acknowledgments :pray:\n\n**We'd like to thank everyone who has contributed to the Citation File Format!**  \nThey are listed in the [`CITATION.cff`](CITATION.cff) file for this repository. Please open an issue if you find that you are missing from the file.\n\nWe gratefully acknowledge support from:\n\n- The [Institute for Software Technology](https://www.dlr.de/sc/en/desktopdefault.aspx/) of the [German Aerospace Center (DLR)](https://www.dlr.de/en/)\n- The [Netherlands eScience Center](https://www.esciencecenter.nl)\n- The [Software Sustainability Institute](https://software.ac.uk/)\n\n",
                "dependencies": "cffconvert\npytest\njsonschema\nruamel.yaml\nbump2version\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/citychem",
            "repo_link": "https://github.com/matthkarl/citychem",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/climsight",
            "repo_link": "https://github.com/CliDyn/climsight",
            "content": {
                "codemeta": "",
                "readme": "# Climate Foresight\n\nPrototype of a system that answers questions about climate change impacts on planned human activities.\n![screencast](https://github.com/koldunovn/climsight/assets/3407313/bf7cd327-c8a9-4a09-bfb5-778269fcd15c)\n\n\n## Running with docker\n\n### simplest: running prebuild container\n\nYou should have [Docker](https://docs.docker.com/engine/install/) installed. Then execute:\n\n```bash\ndocker pull koldunovn/climsight:stable\ndocker run -p 8501:8501 -e OPENAI_API_KEY=$OPENAI_API_KEY climsight\n```\n\nThen open `http://localhost:8501/` in your browser.\n\n### Build and run container with the latest code\n\nYou should have the following packages installed:\n\n- git\n- wget\n- docker\n\nAs long as you have them, do:\n\n```bash\ngit clone https://github.com/koldunovn/climsight.git\ncd climsight\n./download_data.sh\ndocker build -t climsight .\ndocker run -p 8501:8501 climsight\n```\nThen open `http://localhost:8501/` in your browser. If you don't want to add OpenAI key every time, you can expose it through:\n\n```bash\ndocker run -p 8501:8501 -e OPENAI_API_KEY=$OPENAI_API_KEY climsight\n```\nwhere `$OPENAI_API_KEY` not necessarily should be environment variable, you can insert the key directly.\n\nIf you do not have an OpenAI key but want to test Climsight without sending requests to OpenAI, you can run Climsight with the `skipLLMCall` argument:\n```bash\ndocker run -p 8501:8501 -e STREAMLIT_ARGS=\"skipLLMCall\" climsight\n```\n\n## Installation\n\nThe easiest way is to install it through conda or mamba. We recommend mamba, as it's faster. \n\n[Install mamba](https://mamba.readthedocs.io/en/latest/mamba-installation.html#mamba-install) if you don't have it.\n\n```bash\ngit clone https://github.com/koldunovn/climsight.git\ncd climsight\n```\n\nCreate environment and install necessary packages:\n\n```bash\n\nmamba env create -f environment.yml\n```\n\nActivate the environment:\n\n```bash\nconda activate climsight\n```\n## Climsight package installation \n```bash\npip install climsight\n```\n\nFor installation, use either pip alone for all packages and dependencies in a pure Python setup, or use mamba for dependencies followed by pip for Climsight in a Conda environment. Mixing package sources can lead to conflicts and is generally not recommended.\n\n## Before you run\n\nYou have to download example climate data, NaturalEarth coastlines, and the RAG database. To do it simply run:\n\n```bash\npython download_data.py\n```\nIn case you are a developer and want to experiment with the text file based RAG, set the ```--source_files``` flag to ```True``` to also download the origial source files.\n\n```bash\npython download_data.py --source_files=True\n```\n\nYou would also need an [OpenAI API key](https://platform.openai.com/docs/api-reference) to run the prototype. You can provide it as environment variable:\n\n```bash\nexport OPENAI_API_KEY=\"???????\"\n```\n<ins>config settings</ins>\nThere is a possibility to also provide it in the running app. The cost of each request (status September 2023) is about 6 cents with `gpt-4` and about 0.3 cents with `gpt-3.5-turbo` (you can change it in the config file).\nMoreover, if you want to use your own climate data, please adjust the data_settings, variable_mappings, and dimension_mappings according to the structure of your NetCDF files.\n\nAnd you need to export the path of the configuration file. If you don't want to exchange anything and just test the prepared version, simply run\n```bash\nexport CONFIG_PATH=\"./config.yml\"\n```\nOtherwise you might want to adjust this path to direct to an individual config file, but keep in mind that it must be a path relative to the one from where you are running climsight. Also, you will have to adjust the paths in the config file you are using to point back to the climsight data folder. \n\n\nThere is a possibility to also provide it in the running app. The cost of each request (status September 2023) is about 6 cents with `gpt-4` and about 0.3 cents with `gpt-3.5-turbo` (you can change it in the beggining of `climsight.py` script).\n\n\n### Running \n\nChange to the `climsight` folder:\n\n```bash\ncd climsight\nstreamlit run src/climsight/climsight.py\n```\n\nIf you install climsight via pip, make sure to run it in the directory where the data folder has been downloaded:\n```bash\nclimsight\n```\n\nThe browser window should pop up, with the app running. Ask the questions and don't forget to press \"Generate\".\n\n<img width=\"800\" alt=\"Screenshot 2023-09-26 at 15 26 51\" src=\"https://github.com/koldunovn/climsight/assets/3407313/569a4c38-a601-4014-b10d-bd34c59b91bb\">\n\nIf you do not have an OpenAI key but want to test Climsight without sending requests to OpenAI, you can run Climsight with the `skipLLMCall` argument:\n```bash\nstreamlit run src/climsight/climsight.py skipLLMCall\n```\n\n\n## Citation\n\nIf you use or refer to ClimSight in your work, please cite the following publication:\n\nKoldunov, N., Jung, T. Local climate services for all, courtesy of large language models. _Commun Earth Environ_ **5**, 13 (2024). https://doi.org/10.1038/s43247-023-01199-1 \n\n",
                "dependencies": "[project]\nname = \"climsight\"\nversion = \"0.2.0\"\nreadme = \"README.md\"\ndependencies = [\n    \"streamlit\",\n    \"xarray\",\n    \"geopy\",\n    \"geopandas\",\n    \"pyproj\",\n    \"requests\",\n    \"requests-mock\",\n    \"pandas\",\n    \"folium\",\n    \"langchain\",\n    \"streamlit-folium\",\n    \"netcdf4\",\n    \"dask\",\n    \"pip\",\n    \"osmnx\",\n    \"matplotlib\",\n    \"openai\",\n    \"langchain-community\",\n    \"langchain-openai\",\n    \"langchain-chroma\",\n    \"langchain-core\",\n    \"pydantic\",\n    \"langgraph\",\n    \"bs4\",\n    \"wikipedia\",\n    \"scipy\",\n    \"pyproj\"\n]\n\n\n[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\n#[tool.setuptools.packages]\n#find = {}  # Scan the project directory with the default parameters\n\n\n[project.scripts]\nclimsight = \"climsight.launch:launch_streamlit\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cnpypp",
            "repo_link": "https://gitlab.iap.kit.edu/mreininghaus/cnpypp/",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cola",
            "repo_link": "https://github.com/jokergoo/cola",
            "content": {
                "codemeta": "",
                "readme": "# cola: A General Framework for Consensus Partitioning <img src=\"https://user-images.githubusercontent.com/449218/54158555-03e3af80-444b-11e9-9773-070823101263.png\" width=250 align=\"right\" style=\"border:4px solid black;\" />\n\n\n[![R-CMD-check](https://github.com/jokergoo/cola/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/cola/actions)\n[ ![bioc](https://bioconductor.org/shields/downloads/devel/cola.svg) ](http://bioconductor.org/packages/stats/bioc/cola)\n[ ![bioc](http://bioconductor.org//shields/lastcommit/devel/bioc/cola.svg) ](http://bioconductor.org/checkResults/devel/bioc-LATEST/cola/)\n\n\n\n## Citation\n\nZuguang Gu, et al., cola: an R/Bioconductor package for consensus partitioning through a general framework, Nucleic Acids Research, 2021. https://doi.org/10.1093/nar/gkaa1146\n\nZuguang Gu, et al., Improve consensus partitioning via a hierarchical procedure. Briefings in bioinformatics 2022. https://doi.org/10.1093/bib/bbac048 \n\n\n\n## Install\n\n*cola* is available on [Bioconductor](http://bioconductor.org/packages/devel/bioc/html/cola.html), you can install it by:\n\n```r\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"cola\")\n```\n\nThe latest version can be installed directly from GitHub:\n\n```r\nlibrary(devtools)\ninstall_github(\"jokergoo/cola\")\n```\n\n## Methods\n\nThe **cola** supports two types of consensus partitioning.\n\n### Standard consensus partitioning\n\n#### Features\n\n1. It modularizes the consensus clustering processes that various methods can\n   be easily integrated in different steps of the analysis.\n2. It provides rich visualizations for intepreting the results.\n3. It allows running multiple methods at the same time and provides\n   functionalities to compare results in a straightforward way.\n4. It provides a new method to extract features which are more efficient to\n   separate subgroups.\n5. It generates detailed HTML reports for the complete analysis.\n\n\n\n#### Workflow\n\n<img width=\"700\" src=\"https://user-images.githubusercontent.com/449218/52628723-86af3400-2eb8-11e9-968d-b7f47a408818.png\" />\n\nThe steps of consensus partitioning is:\n\n1. Clean the input matrix. The processing are: adjusting outliers, imputing missing\n   values and removing rows with very small variance. This step is optional.\n2. Extract subset of rows with highest scores. Here \"scores\" are calculated by\n   a certain method. For gene expression analysis or methylation data\n   analysis, $n$ rows with highest variance are used in most cases, where\n   the \"method\", or let's call it **\"the top-value method\"** is the variance (by\n   `var()` or `sd()`). Note the choice of \"the top-value method\" can be\n   general. It can be e.g. MAD (median absolute deviation) or any user-defined\n   method.\n3. Scale the rows in the sub-matrix (e.g. gene expression) or not (e.g. methylation data).\n   This step is optional.\n4. Randomly sample a subset of rows from the sub-matrix with probability $p$ and\n   perform partition on the columns of the matrix by a certain partition\n   method, with trying different numbers of subgroups.\n5. Repeat step 4 several times and collect all the partitions.\n6. Perform consensus partitioning analysis and determine the best number of\n   subgroups which gives the most stable subgrouping.\n7. Apply statistical tests to find rows that show significant difference\n   between the predicted subgroups. E.g. to extract subgroup specific genes.\n8. If rows in the matrix can be associated to genes, downstream analysis such\n   as function enrichment analysis can be performed.\n\n#### Usage\n\nThree lines of code to perfrom *cola* analysis:\n\n```r\nmat = adjust_matrix(mat) # optional\nrl = run_all_consensus_partition_methods(\n    mat, \n    top_value_method = c(\"SD\", \"MAD\", ...),\n    partition_method = c(\"hclust\", \"kmeans\", ...),\n    cores = ...)\ncola_report(rl, output_dir = ...)\n```\n\n#### Plots\n\nFollowing plots compare consensus heatmaps with k = 4 under all combinations of methods.\n\n<img src=\"https://user-images.githubusercontent.com/449218/52631118-3a66f280-2ebe-11e9-8dea-0172d9beab91.png\" />\n\n\n### Hierarchical consensus partitioning\n\n\n#### Features\n\n1. It can detect subgroups which show major differences and also moderate differences.\n2. It can detect subgroups with large sizes as well as with tiny sizes.\n3. It generates detailed HTML reports for the complete analysis.\n\n\n#### Hierarchical Consensus Partitioning\n\n\n<img src=\"https://user-images.githubusercontent.com/449218/126491482-31a9496f-cc4d-4c4f-80b7-7b752d8d8d06.png\" width=\"400\" />\n\n\n\n#### Usage\n\nThree lines of code to perfrom hierarchical consensus partitioning analysis:\n\n```r\nmat = adjust_matrix(mat) # optional\nrh = hierarchical_partition(mat, mc.cores = ...)\ncola_report(rh, output_dir = ...)\n```\n\n#### Plots\n\nFollowing figure shows the hierarchy of the subgroups.\n\n<img src=\"https://user-images.githubusercontent.com/449218/100014572-d7b2c280-2dd6-11eb-9265-a84d324122f2.png\" width=\"300\" />\n\nFollowing figure shows the signature genes.\n\n<img src=\"https://user-images.githubusercontent.com/449218/100014657-f913ae80-2dd6-11eb-9bf7-53f733e9f8f0.png\" width=\"600\" />\n\n\n\n## License\n\nMIT @ Zuguang Gu\n\n\n",
                "dependencies": "Package: cola\nType: Package\nTitle: A Framework for Consensus Partitioning\nVersion: 2.9.1\nDate: 2024-02-26\nAuthors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"),\n                  comment = c('ORCID'=\"0000-0002-7395-8709\"))\nDepends: R (>= 4.0.0)\nImports: grDevices, \n         graphics, \n         grid, \n         stats, \n         utils,\n         ComplexHeatmap (>= 2.5.4), \n         matrixStats, \n         GetoptLong, \n         circlize (>= 0.4.7), \n         GlobalOptions (>= 0.1.0),\n         clue, \n         parallel, \n         RColorBrewer, \n         cluster, \n         skmeans, \n         png, \n         mclust, \n         crayon, \n         methods, \n         xml2,\n         microbenchmark, \n         httr,\n         knitr (>= 1.4.0),\n         markdown (>= 1.6),\n         digest,\n         impute,\n         brew,\n         Rcpp (>= 0.11.0),\n         BiocGenerics,\n         eulerr,\n         foreach,\n         doParallel,\n         doRNG,\n         irlba\nSuggests: genefilter,\n          mvtnorm,\n          testthat (>= 0.3),\n          samr,\n          pamr,\n          kohonen,\n          NMF,\n          WGCNA,\n          Rtsne,\n          umap,\n          clusterProfiler,\n          ReactomePA,\n          DOSE,\n          AnnotationDbi,\n          gplots,\n          hu6800.db,\n          BiocManager,\n          data.tree,\n          dendextend,\n          Polychrome,\n          rmarkdown,\n          simplifyEnrichment,\n          cowplot,\n          flexclust,\n          randomForest,\n          e1071\nDescription: Subgroup classification is a basic task in\n  genomic data analysis, especially for gene expression and DNA methylation data\n  analysis. It can also be used to test the agreement to known clinical\n  annotations, or to test whether there exist significant batch effects. The\n  cola package provides a general framework for subgroup classification by\n  consensus partitioning. It has the following features: 1. It modularizes the\n  consensus partitioning processes that various methods can be easily\n  integrated. 2. It provides rich visualizations for interpreting the results.\n  3. It allows running multiple methods at the same time and provides\n  functionalities to straightforward compare results. 4. It provides a new\n  method to extract features which are more efficient to separate subgroups. 5.\n  It automatically generates detailed reports for the complete analysis. 6. It allows\n  applying consensus partitioning in a hierarchical manner.\nURL: https://github.com/jokergoo/cola, \n     https://jokergoo.github.io/cola_collection/\nVignetteBuilder: knitr\nbiocViews: Clustering, GeneExpression, Classification, Software\nLicense: MIT + file LICENSE\nLinkingTo: Rcpp\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/collector-app",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/comando",
            "repo_link": "https://jugit.fz-juelich.de/iek-10/public/optimization/comando",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/comola",
            "repo_link": "https://github.com/Helmholtz-UFZ/CoMOLA",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/complexheatmap",
            "repo_link": "https://github.com/jokergoo/ComplexHeatmap",
            "content": {
                "codemeta": "",
                "readme": "# Make Complex Heatmaps <a href=\"https://jokergoo.github.io/ComplexHeatmap-reference/book/\"><img src=\"https://jokergoo.github.io/ComplexHeatmap-reference/book/complexheatmap-cover.jpg\" width=240 align=\"right\" style=\"border:2px solid black;\" ></a>\n\n[![R-CMD-check](https://github.com/jokergoo/ComplexHeatmap/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/ComplexHeatmap/actions)\n[![codecov](https://img.shields.io/codecov/c/github/jokergoo/ComplexHeatmap.svg)](https://codecov.io/github/jokergoo/ComplexHeatmap) \n[![bioc](http://www.bioconductor.org/shields/downloads/devel/ComplexHeatmap.svg)](https://bioconductor.org/packages/stats/bioc/ComplexHeatmap/) \n[![bioc](http://www.bioconductor.org/shields/years-in-bioc/ComplexHeatmap.svg)](http://bioconductor.org/packages/devel/bioc/html/ComplexHeatmap.html)\n\n<img src=\"http://jokergoo.github.io/complexheatmap_logo.svg\" width=\"550\">\n\n\nComplex heatmaps are efficient to visualize associations between different\nsources of data sets and reveal potential patterns. Here the\n**ComplexHeatmap** package provides a highly flexible way to arrange multiple\nheatmaps and supports various annotation graphics.\n\nThe [**InteractiveComplexHeatmap**](https://github.com/jokergoo/InteractiveComplexHeatmap) package can directly export static complex heatmaps into an interactive Shiny app. Have a try!\n\n## Citation\n\nZuguang Gu, et al., [Complex heatmaps reveal patterns and correlations in multidimensional genomic data](http://bioinformatics.oxfordjournals.org/content/early/2016/05/20/bioinformatics.btw313.abstract), Bioinformatics, 2016.\n\nZuguang Gu. [Complex Heatmap Visualization](https://doi.org/10.1002/imt2.43), iMeta, 2022. \n\n\n## Install\n\n`ComplexHeatmap` is available on [Bioconductor](http://www.bioconductor.org/packages/devel/bioc/html/ComplexHeatmap.html), you can install it by:\n\n```r\nif (!requireNamespace(\"BiocManager\", quietly=TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"ComplexHeatmap\")\n```\n\nIf you want the latest version, install it directly from GitHub:\n\n```r\nlibrary(devtools)\ninstall_github(\"jokergoo/ComplexHeatmap\")\n```\n\n## Usage\n\nMake a single heatmap:\n\n```r\nHeatmap(mat, ...)\n```\n\nA single Heatmap with column annotations:\n\n```r\nha = HeatmapAnnotation(df = anno1, anno_fun = anno2, ...)\nHeatmap(mat, ..., top_annotation = ha)\n```\n\nMake a list of heatmaps:\n\n```r\nHeatmap(mat1, ...) + Heatmap(mat2, ...)\n```\n\nMake a list of heatmaps and row annotations:\n\n```r\nha = HeatmapAnnotation(df = anno1, anno_fun = anno2, ..., which = \"row\")\nHeatmap(mat1, ...) + Heatmap(mat2, ...) + ha\n```\n\n## Documentation\n\nThe full documentations are available at https://jokergoo.github.io/ComplexHeatmap-reference/book/ and the website is at https://jokergoo.github.io/ComplexHeatmap.\n\n## Blog posts\n\nThere are following blog posts focusing on specific topics:\n\n- [Make 3D heatmap](https://jokergoo.github.io/2021/03/24/3d-heatmap/)\n- [Translate from pheatmap to ComplexHeatmap](https://jokergoo.github.io/2020/05/06/translate-from-pheatmap-to-complexheatmap/)\n- [Set cell width/height in the heatmap](https://jokergoo.github.io/2020/05/11/set-cell-width/height-in-the-heatmap/)\n- [Interactive ComplexHeatmap](https://jokergoo.github.io/2020/05/15/interactive-complexheatmap/)\n- [Word cloud as heatmap annotation](https://jokergoo.github.io/2020/05/31/word-cloud-as-heatmap-annotation/)\n- [Which heatmap function is faster?](https://jokergoo.github.io/2020/06/19/which-heatmap-function-is-faster/)\n- [Rasterization in ComplexHeatmap](https://jokergoo.github.io/2020/06/30/rasterization-in-complexheatmap/)\n- [Block annotation over several slices](https://jokergoo.github.io/2020/07/06/block-annotation-over-several-slices/)\n- [Integrate ComplexHeatmap with cowplot package](https://jokergoo.github.io/2020/07/14/integrate-complexheatmap-with-cowplot-package/)\n\n\n## Examples\n\n### Visualize Methylation Profile with Complex Annotations\n\n![complexheatmap_example4](https://user-images.githubusercontent.com/449218/47718635-2ec22980-dc49-11e8-9f01-37becb19e0d5.png)\n\n### Correlations between methylation, expression and other genomic features\n\n![complexheatmap_example3](https://user-images.githubusercontent.com/449218/47718636-2ec22980-dc49-11e8-8db0-1659c27dcf40.png)\n\n### Visualize Cell Heterogeneity from Single Cell RNASeq\n\n![complexheatmap_example2](https://user-images.githubusercontent.com/449218/47718637-2ec22980-dc49-11e8-925e-955c16cfa982.png)\n\n### Making Enhanced OncoPrint\n\n![complexheatmap_example1](https://user-images.githubusercontent.com/449218/47718638-2ec22980-dc49-11e8-845e-21e51d3b8e73.png)\n\n### UpSet plot\n\n<img src=\"https://user-images.githubusercontent.com/449218/102615477-48c76a80-4136-11eb-98d9-3c528844fbe8.png\" width=500 />\n\n### 3D heatmap\n\n![image](https://user-images.githubusercontent.com/449218/112284448-8c77c600-8c89-11eb-8d38-c5538900df20.png)\n\n\n\n## License\n\nMIT @ Zuguang Gu\n\n\n",
                "dependencies": "Package: ComplexHeatmap\nType: Package\nTitle: Make Complex Heatmaps\nVersion: 2.21.1\nDate: 2024-09-24\nAuthors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"),\n                  comment = c('ORCID'=\"0000-0002-7395-8709\"))\nDepends: R (>= 3.5.0), methods, grid, graphics, stats, grDevices\nImports: circlize (>= 0.4.14), GetoptLong, colorspace, clue,\n    RColorBrewer, GlobalOptions (>= 0.1.0), png,\n    digest, IRanges, matrixStats, foreach, doParallel, codetools\nSuggests: testthat (>= 1.0.0), knitr, markdown, dendsort, \n    jpeg, tiff, fastcluster, EnrichedHeatmap,\n    dendextend (>= 1.0.1), grImport, grImport2, glue,\n    GenomicRanges, gridtext, pheatmap (>= 1.0.12),\n    gridGraphics, gplots, rmarkdown, Cairo, magick\nVignetteBuilder: knitr\nDescription: Complex heatmaps are efficient to visualize associations \n    between different sources of data sets and reveal potential patterns. \n    Here the ComplexHeatmap package provides a highly flexible way to arrange \n    multiple heatmaps and supports various annotation graphics.\nbiocViews: Software, Visualization, Sequencing\nURL: https://github.com/jokergoo/ComplexHeatmap, https://jokergoo.github.io/ComplexHeatmap-reference/book/\nLicense: MIT + file LICENSE\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cipm",
            "repo_link": "https://github.com/CIPM-tools/CIPM",
            "content": {
                "codemeta": "",
                "readme": "# Commit-Based Continuous Integration of architectural Performance Models\n\nThis repository provides the prototypical implementation for the change extraction, change propagation, incremental model update, and adaptive instrumentation of the [CIPM approach](https://sdq.kastel.kit.edu/wiki/CIPM).\n\n# Setup\n\nThis project requires Java 11 for all actions. In particular, if a script is executed in the following, it usually uses Maven to build projects. As a result, the `JAVA_HOME` environment variable must be set, pointing to a JDK 11 (the top-level JDK directory, not the `bin` folder so that Maven can find the Java executable in `%JAVA_HOME%\\bin\\java.exe`). Additionally, any script must be executed from the top-level directory of this repository (and not within the `scripts` directory).\n\nWhen executing one of the scripts, it is possible that the error `Internal error: java.lang.IllegalArgumentException: bundleLocation not found: [home]/.m2/[...]` occurs. In such a case, it can help to delete the file `[home]/.m2/repository/.meta/p2-artifacts.properties` and restart the script.\n\nCurrently, only Windows is supported.\n\n## Build\n\nTo build the complete project for the first time, the script `scripts/build.bat` needs to be executed. Subsequent builds can be executed with the script `scripts/rebuild.bat`.\n\nIf a complete clean build is required, the script `scripts/clean.bat` allows to clean the complete repository with all build artifacts and submodules. Afterward, the script `scripts/build.bat` can be executed again.\n\n## Execute TEAMMATES\n\nWith the script `scripts/build-and-test.bat`, the project is built. In addition, the TEAMMATES case is executed. For more information, see [the test plugin description](commit-based-cipm/tests/cipm.consistency.vsum.test).\n\n## Simple Setup with Eclipse\n\nWith the following steps, the project can be setup within Eclipse to view the source code. It is possible to also edit the code, but not to test it. As a reminder, this project requires Java 11. As Eclipse version, the Eclipse Modeling Tools 2022-09 are currently supported.\n\n1. In Eclipse, install the plugins from the CIPM update site.\n\n1. Import any CIPM plugin.\n\n## Full Setup with Eclipse\n\nThe following steps are necessary to setup the project for development within Eclipse. As a reminder, this project requires Java 11. As Eclipse version, the project currently supports the Eclipse Modeling Tools 2022-09. It requires the installation of Xtext (from the Marketplace), Lombok (from [its update site](https://projectlombok.org/p2)), and Checkstyle (optional, from [update site](https://checkstyle.org/eclipse-cs-update-site)).\n\n1. Execute the `scripts/build.bat` script if it was not executed before.\n\n1. After executing the script, all bundles from `commit-based-cipm/bundles/fi` and `commit-based-cipm/releng-dev` can be imported into the Eclipse instance. The `releng-dev` directory contains the bundle `cipm.consistency.targetplatform` with the `cipm.consistency.targetplatform.target` file. Within Eclipse, open this file and click on `Set as active target platform`. Wait until the target platform is set, loaded, and the plugins are successfully compiled. It is possible that the target platform needs to be reloaded.\n\n1. The project requires a second running Eclipse instance. After all plugins in the first instance has been setup, start the provided `SecondInstance` run configuration. It should start the second instance. In the second instance, import all bundles from `commit-based-cipm/bundles/si` and `commit-based-cipm/tests`. In `cipm.consistency.vsum.test`, the test cases should be executable.\n\n## About the Internal Structure of the Setup\n\nThe current build process provides a replicable build. Therefore, dependencies are provided via Eclipse P2 Update Sites with fixed versions or via Git submodules. In particular, the submodules include JaMoPP, SoMoX, and a specialized old Vitruv version. As the submodules only contain source code, they need to be compiled after cloning the repository or if they are cleaned. The build process contains necessary steps to build the submodules.\n\nThe artifacts from the submodules are also packaged into local Eclipse P2 Update Sites. Unfortunately, Maven Tycho, which is internally used to build the artifacts, does not support local Eclipse P2 Update Sites via file paths and requires HTTP or HTTPS paths instead. Thus, a simple update site server, which serves the local Eclipse P2 Update Sites, is started and stopped during the build process.\n\nThe Reactions language from Vitruv detects the meta-models from Vitruv domains. To find the Vitruv domains, the corresponding lookup mechanisms in Eclipse and in the build process require the domains and domain providers to be built and to be on the classpath. As a consequence, a separation between the bundles is performed. The first half of the bundles (located in `commit-based-cipm/bundles/fi`, also imported into the first Eclipse instance) contain two domains (one for the instrumentation model and one adjusted domain for Java) so that they are built in a first build step and for the second Eclipse instance. In the second build step and in the second Eclipse instance, the built domains can be found by the Reactions language.\n\nNotes on generating code for Reactions in the build process: \n\n1. The plugin containing Reactions requires a `.maven_enable_dsls-generation` file in the plugin directory (the `[plugin name]` directory, not `[plugin name]/src`). \n\n2. Furthermore, all classes or methods imported within a Reactiosn file cannot be located in the same plugin as the Reactions. They need to be in separated plugins.\n\n# Executing the Pipeline with TeaStore or TEAMMATES\n\nThe manual to execute the pipeline with TeaStore or TEAMMATES can be found [here](commit-based-cipm/tests/cipm.consistency.vsum.test). Reference PCM repository models for these executions can be found [here](data).\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/copy-paste-imputation",
            "repo_link": "https://github.com/KIT-IAI/CopyPasteImputation",
            "content": {
                "codemeta": "",
                "readme": "# Copy-Paste Imputation (CPI) for Energy Time Series\n\nThis repository contains the Python implementation of the Copy-Paste Imputation (CPI) method presented in the following paper:\n>M. Weber, M. Turowski, H. K. Çakmak, R. Mikut, U. Kühnapfel and V. Hagenmeyer, 2021, \"Data-Driven Copy-Paste Imputation for Energy Time Series,\" in IEEE Transactions on Smart Grid, 12, 6, pp. 5409–5419, doi: [10.1109/TSG.2021.3101831](https://doi.org/10.1109/TSG.2021.3101831).\n\n## Installation\n\nTo install this project, perform the following steps:\n1. Clone the project\n2. Open a terminal of the virtual environment where you want to use the project\n3. `cd` into the cloned directory\n4. `pip install .` or `pip install -e .` to install the project editable.\n    * Use `pip install -e .[dev]` to install with development dependencies\n\n## Use\n\n    from cpiets.cpi import CopyPasteImputation\n    import pandas as pd\n\n    cpi = CopyPasteImputation()\n    data = pd.read_csv('data.csv')\n    cpi.fit(data)\n    result = cpi.impute()\n\n### Input Data Requirements\n\n**Example data:**\n\n| time                | energy |\n| ------------------- | ------:|\n| 2012-01-02 00:15:00 |  11.60 |\n| 2012-01-02 00:30:00 |  24.87 |\n| 2012-01-02 00:45:00 |  37.31 |\n\n\nThe names of the columns are arbitrary.\n\n**Assumptions:**\n* There are no missing values (nan) at the start or end of the time series.\n* A day starts with the first value after 0:00 (0:15 in the example above) and ends with 0:00.\n* The time series starts at the beginning of a day and ends at the end of a day.\n\n**Supported time formats:**\n* %Y-%m-%d %H:%M:%S (2020-01-17 13:37:42)\n* %d-%b-%Y %H:%M:%S (17-Jan-2020 13:37:42)\n\n\n## Example\n\nIn this repository, we included example data derived from the [ElectricityLoadDiagrams20112014](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014) data set.\n\nTo run the CPI method with simple test data, you can run the example\n\n    python example/simple_imputation.py\n\nand play around with the parameters.\n\n\n## Funding\n\nThis project is supported by the Helmholtz Association under the Joint Initiative \"Energy System 2050 - A Contribution of the Research Field Energy\".\n\n\n## License\n\nThis code is licensed under the [LGPL-3.0 License](COPYING).\n\n",
                "dependencies": "import setuptools\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=\"cpi-ets\",\n    version=\"0.0.2\",\n    author=\"Moritz Weber\",\n    author_email=\"moritz.weber@kit.edu\",\n    description=\"Copy-Paste Imputation for Energy Time Series\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"...tbd...\",\n    packages=['cpiets'],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n    ],\n    python_requires='>=3.6',\n    install_requires=['numpy', 'pandas', 'prophet'],\n    extras_require={\n        'dev': [\n            'pylint',\n        ]\n    },\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/corc",
            "repo_link": "https://github.com/KIT-TVA/CorC",
            "content": {
                "codemeta": "",
                "readme": "# CorC\n\nCbC is an approach to create correct programs incrementally. Guided by pre-/postcondition specifications, a program is developed using refinement rules, guaranteeing the implementation is correct. With [CorC](https://github.com/KIT-TVA/CorC/wiki), we implemented a graphical and textual IDE to create programs following the CbC approach. Starting with an abstract specification, CorC supports CbC developers in refining a program by a sequence of refinement steps and in verifying the correctness of these refinement steps using a deductive verifier.\n\n# C-CorC\nCorC for information flow ([C-CorC](https://github.com/KIT-TVA/CorC/wiki/CorC-for-Information-Flow)) is an extension of CorC. \n\n# Getting Started\n## Java Version\nInstall [**JDK 16**](https://www.oracle.com/java/technologies/javase/jdk16-archive-downloads.html). CorC may not work out of the box with newer versions.\n## Eclipse Modelling Tools\n- Install [Eclipse Modelling Tools (EMT)](https://www.eclipse.org/downloads/packages/release/2023-09/r/eclipse-modeling-tools) (Version 2023-09). \n- Get the latest release of [Z3](https://github.com/Z3Prover/z3/releases) and add the `*/z3-[cur-version]-[x64/x32]-win/bin` folder to the environment variable [PATH](https://www.wikihow.com/Change-the-PATH-Environment-Variable-on-Windows)\n\n## EMT Plugins\n- **Graphiti** Install Graphiti using the update site https://download.eclipse.org/graphiti/updates/release/0.18.0\n\n- **FeatureIDE** Available in [Eclipse Marketplace](https://marketplace.eclipse.org/content/featureide)\n\n- **Mylyn** Available in [Eclipse Marketplace](https://marketplace.eclipse.org/content/mylyn) (Mylyn 3.23)\n\n- **TestNG** Available in [Eclipse Marketplace](https://marketplace.eclipse.org/content/testng-eclipse)\n\n## CorC Setup\n1. Clone the repo:\n    ```sh\n    git clone https://github.com/KIT-TVA/CorC.git\n    ```\n2. In EMT, select `Open Projects... -> CorC` and check the boxes for the following packages:\n    - `de.tu-bs.cs.isf.cbc.exceptions`\n    - `de.tu-bs.cs.isf.cbc.model`\n    - `de.tu-bs.cs.isf.cbc.mutation`\n    - `de.tu-bs.cs.isf.cbc.tool`\n    - `de.tu-bs.cs.isf.cbc.util`\n    - `de.tu-bs.cs.isf.cbcclass.tool`\n    - `de.tu-bs.cs.isf.wizards`\n    - `de.tu_bs.cs.isf.cbc.parser`\n    - `de.tu_bs.cs.isf.cbc.statistics`\n    - `de.tu_bs.cs.isf.cbc.statistics.ui`\n    - `de.tu_bs.cs.isf.commands.toolbar`\n    - `de.tu_bs.cs.isf.lattice`\n\n3.  Open:\n    - `*.cbc.model -> model -> genmodel.genmodel`\n    - `*.cbc.statistics -> model -> cbcstatistics.genmodel` \n    \n    Right click and `Generate Model/Edit/Editor Code` for all files listed.\n    If EMT still displays errors, clean and rebuild all projects as described in the [common issues](#common-issues) section.\n\n4. Select any package and run project as `Eclipse Application`.\n\n# Examples & Case Study Introduction\nWe provide different examples and case studies to explore CorC!\n## Examples\nCreate CorC-examples via `File -> New -> Other... -> CorC -> CorC Examples`.\n## Case studies\nThe repository you checked out contains various software product line case studies in the folder `CaseStudies`. They can be loaded via `File -> Open project from file system`. \n### BankAccount\nThe BankAccount implements basic functions of a bank account such as withdrawals, limits, money transfers and checking the account balance.\n- **BankAccount** Object-oriented implementation with class structure and CbC-Classes.\n- **BankAccountOO** Object-oriented implementation with class structure and CbC-Classes. Non-SPL implementation.\n### Elevator\nThe Elevator implements basic functions of an elevator such as the movement and entering and leaving of persons.\n- **Elevator** Object-oriented implementation with class structure and CbC-Classes.\n### Email\nThe product line Email implements basic functions of an email system including server- and client-side interactions.\n- **EmailOO** Object-oriented implementation with class structure and CbC-Classes. Non-SPL implementation.\n- **EmailFeatureInteraction** Java-Implementation without implementation with CbC.\n### IntegerList\nThe IntegerList implements a list of integers with add and sort operations.\n- **IntegerList** Object-oriented implementation with class structure and CbC-Classes.\n- **IntegerListOO** Object-oriented implementation with class structure and CbC-Classes. Non-SPL implementation.\n\n# Common Issues\n\n**Problem:** EMT gets stuck while trying to generate a model.\n\n**Solution:** Terminate EMT using any process manager and generate the model again.\n\n---\n\n**Problem:** Multiple resolving errors after generating model files.\n\n**Solution:** Clean and rebuild all projects via `Project -> Clean...`.\n\n---\n\n**Problem:** Cycling depedency issues.\n\n**Solution:** Navigate to: `Project -> Properties -> Java Compiler -> Building -> Configure Workspace Settings -> Build path problems -> Circular dependencies` and set the listbox to `Warning`.\n\n---\n\n**Problem:** Errors in certain files about undefined methods and classes.\n\n**Solution:** Changing the compliance: `Project -> Java Compiler -> JDK Complicance -> Use compliance from execution environment 'JavaSE-16'`.\n\n",
                "dependencies": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\" xmlns=\"http://maven.apache.org/POM/4.0.0\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n  \t<modelVersion>4.0.0</modelVersion>\n\n  \t<!-- The parent pom.xml and its children were build using the website \nhttps://eclipse.dev/Xtext/documentation/350_continuous_integration.html#tycho-build \nand the associated git repository\nhttps://github.com/xtext/maven-xtext-example\ngit@github.com:xtext/maven-xtext-example.git-->\n\n\n  \t<groupId>de.tu-bs.cbc</groupId>\n  \t<artifactId>CorC-feat-autobuild_final</artifactId>\n  \t<version>0.0.1-SNAPSHOT</version>\n  \t<packaging>pom</packaging>\n\n  \t<modules>\n\t    <module>de.tu-bs.cs.isf.cbc.model</module>\n\t\t\t<module>de.tu-bs.cs.isf.cbc.mutation</module>\n\t\t\t<module>de.tu-bs.cs.isf.cbc.exceptions</module>\n\t    <module>de.tu-bs.cs.isf.cbc.tool</module>\n\t    <module>de.tu-bs.cs.isf.cbc.util</module>\n\t    <module>de.tu-bs.cs.isf.cbcclass.tool</module>\n\t    <module>de.tu-bs.cs.isf.wizards</module>\n\t    <module>de.tu_bs.cs.isf.cbc.statistics</module>\n\t    <module>de.tu_bs.cs.isf.cbc.statistics.ui</module>\n\t    <module>de.tu_bs.cs.isf.commands.toolbar</module>\n\t    <module>de.tu_bs.cs.isf.cbc.parser</module>\n\t    <module>de.tu_bs.cs.isf.lattice</module>\n\t</modules>\n\n\t<properties>\n\t\t<maven.compiler.source>17</maven.compiler.source>\n\t\t<maven.compiler.target>17</maven.compiler.target>\n\t \t<tycho-version>3.0.0</tycho-version>\n\t \t<xtext-version>2.31.0</xtext-version>\n\t    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n\t    <eclipse-repo.url>https://download.eclipse.org/releases/latest</eclipse-repo.url>\n\t    <tycho.disableP2Mirrors>true</tycho.disableP2Mirrors>\n\t</properties>\n\n\t<build>\n\t    <pluginManagement>\n\t      \t<plugins>\n\n\t      \t\t<plugin>\n\t\t            <groupId>org.eclipse.tycho</groupId>\n\t\t            <artifactId>tycho-p2-director-plugin</artifactId>\n\t\t            <version>${tycho-version}</version>\n\t          \t</plugin>\n\n\t          \t<plugin>\n\t\t            <groupId>org.eclipse.tycho.extras</groupId>\n\t\t            <artifactId>tycho-eclipserun-plugin</artifactId>\n\t\t            <version>3.0.0</version>\n\t          \t</plugin>\n\n\t          \t<!-- xtend-maven-plugin is in pluginManagement instead of in plugins\n\t\t       so that it doesn't run before the exec-maven-plugin's *.mwe2 gen;\n\t\t       this way we can list it after.-->\n      \t\t\t\n\t\t\t    <plugin>\n\t\t\t      \t<groupId>org.eclipse.xtend</groupId>\n\t\t\t      \t<artifactId>xtend-maven-plugin</artifactId>\n\t\t\t      \t<version>2.29.0</version>\n\t\t\t      \t<executions>\n\t\t\t        \t<execution>\n\t\t\t          \t\t<goals>\n\t\t\t\t\t            <goal>compile</goal>\n\t\t\t\t\t            <goal>xtend-install-debug-info</goal>\n\t\t\t\t\t            <goal>testCompile</goal>\n\t\t\t\t\t            <goal>xtend-test-install-debug-info</goal>\n\t\t\t          \t\t</goals>\n\t\t\t        \t</execution>\n\t\t\t      \t</executions>\n\t\t\t      \t<configuration>\n\t\t\t        \t<outputDirectory>xtend-gen</outputDirectory>\n\t\t\t      \t</configuration>\n\t\t\t    </plugin>\n\n\t\t\t    <plugin>\n\t\t\t\t\t<groupId>org.eclipse.tycho</groupId>\n\t\t\t\t\t<artifactId>tycho-packaging-plugin</artifactId>\n\t\t\t\t\t<version>${tycho-version}</version>\n\t\t\t\t\t<configuration>\n\t\t\t\t\t\t<skipPomGeneration>true</skipPomGeneration>\n\t\t\t\t\t</configuration>\n\t\t\t\t</plugin>\n\t\t\t\t\n\t      \t</plugins>\n\t    </pluginManagement>\n\n\t    <plugins>\n\t    \t<plugin>\n\t\t        <groupId>org.eclipse.tycho</groupId>\n\t\t        <artifactId>tycho-maven-plugin</artifactId>\n\t\t        <version>${tycho-version}</version>\n\t\t        <extensions>true</extensions>\n\t      \t</plugin>\n\n\t        <!--<plugin>\n\t            <artifactId>maven-failsafe-plugin</artifactId>\n\t            <version>2.22.2</version>\n\t        </plugin>-->\n\n\t      \t<plugin>\n\t\t        <groupId>org.eclipse.tycho</groupId>\n\t\t        <artifactId>target-platform-configuration</artifactId>\n\t\t        <version>${tycho-version}</version>\n\t\t        <configuration>\n\t\t        <!-- Optional set the Java version you are using-->\n\t\t        <executionEnvironment>JavaSE-17</executionEnvironment>\n\t\t        \t<target>\n\t\t\t\t\t\t<file>\n\t\t\t\t\t\t\t${project.basedir}/../targetplatform.target\n\t\t\t\t\t\t</file>\n\t\t\t\t\t</target>\n\t\t         \t<environments>\n\t\t              \t<environment>\n\t\t\t                <os>linux</os>\n\t\t\t                <ws>gtk</ws>\n\t\t\t                <arch>x86_64</arch>\n\t\t              \t</environment>\n\t\t              \t<environment>\n\t\t\t                <os>win32</os>\n\t\t\t                <ws>win32</ws>\n\t\t\t                <arch>x86_64</arch>\n\t\t              \t</environment>\n\t\t             \t<environment>\n\t\t\t             \t<os>macosx</os>\n\t\t\t                <ws>cocoa</ws>\n\t\t\t                <arch>x86_64</arch>\n\t\t              \t</environment>\n\t\t          \t</environments>\n\t        \t</configuration>\n\t      \t</plugin>\n\t    </plugins>\n\t</build>\n\n\t<repositories>\n\t\t<repository>\n\t    \t<id>eclipse-release</id>\n\t\t    <url>${eclipse-repo.url}</url>\n\t\t    <layout>p2</layout>\n\t  \t</repository>\n\t  \t<repository>\n\t\t    <id>key</id>\n\t\t    <url>https://formal.kastel.kit.edu/key/download/releases/2.6/eclipse/</url>\n\t\t    <layout>p2</layout>\n\t  \t</repository>\n\t  \t<repository>\n\t\t    <id>graphiti</id>\n\t\t    <url>https://download.eclipse.org/graphiti/updates/0.18.0/</url>\n\t\t    <layout>p2</layout>\n\t  \t</repository>\n\t  \t<repository>\n\t    \t<id>emftext</id>\n\t\t    <url>http://update.emftext.org/release/</url>\n\t\t    <layout>p2</layout>\n\t  \t</repository>\n\t  \t<repository>\n\t\t    <id>featureIDE</id>\n\t\t    <url>http://featureide.cs.ovgu.de/update/v3/</url>\n\t\t    <layout>p2</layout>\n\t  \t</repository>\n\t  \t<repository>\n\t\t    <id>mylyn</id>\n\t\t    <url>https://download.eclipse.org/mylyn/snapshots/weekly/</url>\n\t\t    <layout>p2</layout>\n\t  \t</repository>\n\t  \t<repository>\n\t\t    <id>jamopp</id>\n\t\t    <url>http://update.jamopp.org/release/</url>\n\t\t    <layout>p2</layout>\n\t  \t</repository>\n\t  \t<repository>\n\t\t  \t<id>justj</id>\n\t\t  \t<url>https://download.eclipse.org/justj/jres/17/updates/release/latest/</url>\n\t\t  \t<layout>p2</layout>\n\t  \t</repository>\n\t  \t<repository>\n\t  \t\t<id>testng</id>\n\t  \t\t<url>https://testng.org/testng-eclipse-update-site/</url>\n\t  \t\t<layout>p2</layout>\n  \t\t</repository>\n\t  \t\n\t\t<repository>\n\t\t\t<id>codehaus-snapshots</id>\n\t\t\t<name>disable dead 'Codehaus Snapshots' repository, see https://bugs.eclipse.org/bugs/show_bug.cgi?id=481478</name>\n\t\t\t<url>http://nexus.codehaus.org/snapshots/</url>\n\t\t\t<releases>\n\t\t\t\t<enabled>false</enabled>\n\t\t\t</releases>\n\t\t\t<snapshots>\n\t\t\t\t<enabled>false</enabled>\n\t\t\t</snapshots>\n\t\t</repository>\n\t\t<!-- This must be disabled explicitly, otherwise it is enabled by https://github.com/mojohaus/mojo-parent \n\t\t\twhich is taken from exec-maven-plugin from at least version 1.6.0 -->\n\t\t<repository>\n\t\t\t<id>ossrh-snapshots</id>\n\t\t\t<name>ossrh-snapshots</name>\n\t\t\t<releases>\n\t\t\t\t<enabled>false</enabled>\n\t\t\t</releases>\n\t\t\t<snapshots>\n\t\t\t\t<enabled>false</enabled>\n\t\t\t</snapshots>\n\t\t\t<url>http://oss.sonatype.org/content/repositories/snapshots</url>\n\t\t</repository>\n\t\t<!-- This is enabled by /org/sonatype/oss/oss-parent/7 used as parent by \n\t\t\torg/xtext/antlr-generator/3.2.1 -->\n\t\t<repository>\n\t\t\t<id>sonatype-nexus-snapshots</id>\n\t\t\t<name>Sonatype Nexus Snapshots</name>\n\t\t\t<url>https://oss.sonatype.org/content/repositories/snapshots</url>\n\t\t\t<releases>\n\t\t\t\t<enabled>false</enabled>\n\t\t\t</releases>\n\t\t\t<snapshots>\n\t\t\t\t<enabled>false</enabled>\n\t\t\t</snapshots>\n\t\t</repository>\n\t</repositories>\n\t<pluginRepositories>\n\t\t<pluginRepository>\n\t\t\t<id>codehaus-snapshots</id>\n\t\t\t<name>disable dead 'Codehaus Snapshots' repository, see https://bugs.eclipse.org/bugs/show_bug.cgi?id=481478</name>\n\t\t\t<url>http://nexus.codehaus.org/snapshots/</url>\n\t\t\t<releases>\n\t\t\t\t<enabled>false</enabled>\n\t\t\t</releases>\n\t\t\t<snapshots>\n\t\t\t\t<enabled>false</enabled>\n\t\t\t</snapshots>\n\t\t</pluginRepository>\n\t\t<pluginRepository>\n\t\t\t<id>ossrh-snapshots</id>\n\t\t\t<name>ossrh-snapshots</name>\n\t\t\t<releases>\n\t\t\t\t<enabled>false</enabled>\n\t\t\t</releases>\n\t\t\t<snapshots>\n\t\t\t\t<enabled>false</enabled>\n\t\t\t</snapshots>\n\t\t\t<url>http://oss.sonatype.org/content/repositories/snapshots</url>\n\t\t</pluginRepository>\n\t\t<pluginRepository>\n\t\t\t<id>sonatype-nexus-snapshots</id>\n\t\t\t<name>Sonatype Nexus Snapshots</name>\n\t\t\t<url>https://oss.sonatype.org/content/repositories/snapshots</url>\n\t\t\t<releases>\n\t\t\t\t<enabled>false</enabled>\n\t\t\t</releases>\n\t\t\t<snapshots>\n\t\t\t\t<enabled>false</enabled>\n\t\t\t</snapshots>\n\t\t</pluginRepository>\n\t</pluginRepositories>\n</project>\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/corsika",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/cosmoscout-vr",
            "repo_link": "https://github.com/cosmoscout/cosmoscout-vr",
            "content": {
                "codemeta": "",
                "readme": "<!-- \nSPDX-FileCopyrightText: German Aerospace Center (DLR) <cosmoscout@dlr.de>\nSPDX-License-Identifier: CC-BY-4.0\n -->\n \n<p align=\"center\"> \n  <img src =\"resources/logo/large.svg\" />\n</p>\n\nCosmoScout VR is a modular virtual universe developed at the German Aerospace Center (DLR).\nIt lets you explore, analyze and present huge planetary data sets and large simulation data in real-time.\n\n[![Build Status](https://github.com/cosmoscout/cosmoscout-vr/workflows/Build/badge.svg?branch=main)](https://github.com/cosmoscout/cosmoscout-vr/actions)\n[![REUSE](https://api.reuse.software/badge/github.com/cosmoscout/cosmoscout-vr)](https://api.reuse.software/info/github.com/cosmoscout/cosmoscout-vr)\n[![Coverage Status](https://coveralls.io/repos/github/cosmoscout/cosmoscout-vr/badge.svg?branch=main)](https://coveralls.io/github/cosmoscout/cosmoscout-vr?branch=main)\n[![documentation](https://img.shields.io/badge/Docs-online-34D058.svg)](docs/README.md)\n[![license](https://img.shields.io/badge/License-MIT-purple.svg)](LICENSE.md)\n[![source loc](https://img.shields.io/badge/LoC-15.6k-green.svg)](tools/cloc.sh)\n[![plugin loc](https://img.shields.io/badge/LoC_Plugins-24.5k-green.svg)](tools/cloc.sh)\n[![comments](https://img.shields.io/badge/Comments-8.1k-yellow.svg)](tools/cloc.sh)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3381953.svg)](https://doi.org/10.5281/zenodo.3381953)\n\nThe software can be build on Linux (gcc or clang) and Windows (msvc).\nNearly all dependencies are included as [git submodules](externals), please refer to the [**documentation**](docs) in order to get started.\n\n# Features\n\n<p align=\"center\"> \n  <img src =\"docs/img/banner-mars.jpg\" />\n</p>\n\nBelow is a rough sketch of the possibilities you have with CosmoScout VR.\nWhile this list is far from complete it provides a good overview of the current feature set.\nYou can also read the [**changelog**](docs/changelog.md) to learn what's new in the current version. There is also an [**interesting article in the DLR magazine**](https://dlr.de/dlr/portaldata/1/resources/documents/dlr_magazin_161_EN/DLR-Magazin_161-GB/?page=18), and [**several papers**](docs/citation.md) which provide some insight into the ideas behind CosmoScout VR. \n\n- [X] Solar System Simulation\n  - [X] Positioning of celestial bodies and space crafts based on [SPICE](https://naif.jpl.nasa.gov/naif)\n  - [X] Rendering of highly detailed level-of-detail planets based on WebMapServices (with [csp-lod-bodies](plugins/csp-lod-bodies))\n  - [X] Rendering of configurable atmospheres (Mie- and Rayleigh-scattering) around planets (with [csp-atmospheres](plugins/csp-atmospheres))\n  - [X] Physically based rendering of 3D satellites (with [csp-satellites](plugins/csp-satellites))\n  - [X] Rendering of Tycho, Tycho2 and Hipparcos star catalogues (with [csp-stars](plugins/csp-stars))\n  - [X] Rendering of orbits and trajectories based on SPICE (with [csp-trajectories](plugins/csp-trajectories))\n  - [X] Rendering of shadows\n  - [X] HDR-Rendering\n- [x] Flexible User Interface\n  - [X] Completely written in JavaScript with help of the [Chromium Embedded Framework](https://bitbucket.org/chromiumembedded/cef/src)\n  - [X] Main UI can be drawn in the screen- or world-space\n  - [X] Web pages can be placed on planetary surfaces\n  - [X] Interaction works both in VR and on the Desktop\n  - [x] Clear API between C++ and JavaScript \n- [ ] Cross-Platform\n  - [X] Runs on Linux\n  - [X] Runs on Windows\n  - [ ] Runs on MacOS\n- [ ] System Architecture\n  - [X] Plugin-based - most functionality is loaded at run-time\n  - [ ] Network synchronization of multiple instances\n- [x] Hardware device support - CosmoScout VR basically supports everything which is supported by [ViSTA](https://github.com/cosmoscout/vista) and [VRPN](https://github.com/vrpn/vrpn). The devices below are actively supported (or planned to be supported).\n  - [X] Mouse\n  - [X] Keyboard\n  - [X] HTC-Vive\n  - [X] ART-Tracking systems\n  - [X] 3D-Connexion Space Navigator\n  - [X] Multi-screen systems like tiled displays or CAVE's\n  - [X] Multi-screen systems on distributed rendering clusters\n  - [X] Side-by-side stereo systems\n  - [X] Quad-buffer stereo systems\n  - [X] Anaglyph stereo systems\n  - [x] Game Pads like the X-Box controller\n\n# Getting Started\n\n<p align=\"center\"> \n  <img src =\"docs/img/banner-light-shafts.jpg\" />\n</p>\n\n:warning: _**Warning:** CosmoScout VR is research software which is still under heavy development and changes on a daily basis.\nMany features are badly documented, it will crash without warning and may do other unexpected things.\nWe are working hard on improving the user experience - please [report all issues and suggestions](https://github.com/cosmoscout/cosmoscout-vr/issues) you have!_\n\nFor each release, [binary packages](https://github.com/cosmoscout/cosmoscout-vr/releases) are automatically created via [Github Actions](https://github.com/cosmoscout/cosmoscout-vr/actions).\n\nWhen started for the very first time, some example datasets will be downloaded from the internet.\n**This will take some time!**\nThe progress of this operation is shown on the loading screen.\n\n\nIf the binary releases do not work for you or you want to test the latest features, you have to compile CosmoScout VR yourself.\nThis is actually quite easy as there are several guides in the **[`docs`](docs)** directory to get you started!\n\n# Plugins for CosmoScout VR\n\nCosmoScout VR can be extended via plugins.\nIn fact, without any plugins, CosmoScout VR is just a black and empty universe. Here is a list of plugins which are included in this repository.\nThere are also additional plugins which are listed further below.\n\nCore Plugins | Description | Screenshot\n:----|:-----------------|:----------\n[csp-anchor-labels](plugins/csp-anchor-labels) | Draws a click-able label at each celestial anchor. When activated, the user automatically travels to the selected body. The size and overlapping-behavior of the labels can be adjusted. | ![screenshot](docs/img/csp-anchor-labels.jpg)\n[csp-atmospheres](plugins/csp-atmospheres) | Draws atmospheres around celestial bodies. It supports multiple atmospheric models. | ![screenshot](docs/img/csp-atmospheres.jpg)\n[csp-custom-web-ui](plugins/csp-custom-web-ui) | Allows adding custom HTML-based user interface elements as sidebar-tabs, as floating windows or into free space. | ![screenshot](docs/img/csp-custom-web-ui.jpg)\n[csp-demo-node-editor](plugins/csp-demo-node-editor) | An example on how to use the `csl-node-editor` plugin library for creating data flow graphs within CosmoScout VR. | ![screenshot](docs/img/csp-demo-node-editor.jpg)\n[csp-fly-to-locations](plugins/csp-fly-to-locations) | Adds several quick travel targets to the sidebar. It supports shortcuts to celestial bodies and to specific geographic locations on those bodies. | ![screenshot](docs/img/csp-fly-to-locations.jpg)\n[csp-lod-bodies](plugins/csp-lod-bodies) | Draws level-of-detail planets and moons. This plugin supports the visualization of entire planets in a 1:1 scale. The data is streamed via Web-Map-Services (WMS) over the internet. A dedicated MapServer is required to use this plugin. | ![screenshot](docs/img/csp-lod-bodies.jpg)\n[csp-measurement-tools](plugins/csp-measurement-tools) | Provides several tools for terrain measurements. Like measurement of distances, height profiles, volumes or areas. | ![screenshot](docs/img/csp-measurement-tools.jpg)\n[csp-minimap](plugins/csp-minimap) | Displays a configurable 2D-Minimap in the user interface. | ![screenshot](docs/img/csp-minimap.jpg)\n[csp-recorder](plugins/csp-recorder) | A CosmoScout VR plugin which allows basic capturing of high-quality videos. Requires that `csp-web-api` is enabled. | ![screenshot](docs/img/csp-recorder.jpg)\n[csp-rings](plugins/csp-rings) | Draws simple rings around celestial bodies. The rings can be configured with an inner and an outer radius and a texture. | ![screenshot](docs/img/csp-rings.jpg)\n[csp-satellites](plugins/csp-satellites) | Draws GTLF models at positions based on SPICE data. It uses physically based rendering for surface shading. | ![screenshot](docs/img/csp-satellites.jpg)\n[csp-sharad](plugins/csp-sharad) | Renders radar datasets acquired by the Mars Reconnaissance Orbiter. The SHARAD profiles are rendered inside of Mars, the Martian surface is made translucent in front of the profiles. | ![screenshot](docs/img/csp-sharad.jpg)\n[csp-simple-bodies](plugins/csp-simple-bodies) | Renders simple spherical celestial bodies. The bodies are drawn as an ellipsoid with an equirectangular texture. | ![screenshot](docs/img/csp-simple-bodies.jpg)\n[csp-stars](plugins/csp-stars) | Draws 3D-stars loaded from catalogues. For now Tycho, Tycho2 and the Hipparcos catalogue are supported. | ![screenshot](docs/img/csp-stars.jpg)\n[csp-timings](plugins/csp-timings) | Uses the built-in timer queries of CosmoScout VR to draw on-screen live frame timing statistics. This plugin can also be used to export recorded time series to a CSV file. | ![screenshot](docs/img/csp-timings.jpg)\n[csp-trajectories](plugins/csp-trajectories) | Draws trajectories of celestial bodies and spacecrafts based on SPICE. The color, length, number of samples and the reference frame can be configured. | ![screenshot](docs/img/csp-trajectories.jpg)\n[csp-web-api](plugins/csp-web-api) | Allows to control CosmoScout VR via an HTTP protocol. It also allows capturing screenshots over HTTP. | ![screenshot](docs/img/csp-web-api.jpg)\n[csp-wms-overlays](plugins/csp-wms-overlays) | Overlays time dependent map data from Web-Map-Services (WMS) over bodies rendered by other plugins. | ![screenshot](docs/img/csp-wms-overlays.jpg)\n\nAdditional Plugins | Description | Screenshot\n:----|:-----------------|:----------\n[csp-gaussian-splatting](https://github.com/cosmoscout/csp-gaussian-splatting) | This plugin uses the code provided for the paper \"3D Gaussian Splatting for Real-Time Radiance Field Rendering\" to visualize radiance fields. | ![screenshot](docs/img/csp-gaussian-splatting.jpg)\n[csp-user-study](https://github.com/cosmoscout/csp-user-study) |This plugin was used for the user study of the IEEE Aerospace paper \"CosmoScout VR: A Modular 3D Solar System Based on SPICE\". It can be used to record a series of checkpoints which the user has to fly through. | ![screenshot](docs/img/csp-user-study.jpg)\n\n### Credits\n\nSome badges in this README.md are from [shields.io](https://shields.io). The documentation of CosmoScout VR also uses icons from [simpleicons.org](https://simpleicons.org/).\n\n<p align=\"center\"><img src =\"docs/img/hr.svg\"/></p>\n\n",
                "dependencies": "# ------------------------------------------------------------------------------------------------ #\n#                                This file is part of CosmoScout VR                                #\n# ------------------------------------------------------------------------------------------------ #\n\n# SPDX-FileCopyrightText: German Aerospace Center (DLR) <cosmoscout@dlr.de>\n# SPDX-License-Identifier: MIT\n\ncmake_minimum_required(VERSION 3.13)\nproject(cosmoscout-vr VERSION 1.9.0)\n\n# Use cmake 3.12's <PACKAGE>_ROOT variabled for searching.\ncmake_policy(SET CMP0074 NEW)\n\n# Ensure local modules (for dependencies etc.) are found.\nlist(APPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/cmake)\n\n# CMAKE_BUILD_TYPE must be set (except for Visual Studio).\nif(NOT MSVC)\n  if(NOT CMAKE_BUILD_TYPE OR\n    (NOT ${CMAKE_BUILD_TYPE} STREQUAL \"Release\" AND\n    NOT ${CMAKE_BUILD_TYPE} STREQUAL \"Debug\"))\n\n    set(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"Release or Debug\" FORCE)\n  endif()\nendif()\n\n# Use folders when targeting an IDE\nset_property(GLOBAL PROPERTY USE_FOLDERS ON)\n\n# find dependencies --------------------------------------------------------------------------------\n\ninclude(GenerateExportHeader)\n\n# Boost and OpenGL must be present on your system. All other dependencies are included as submodules\n# in \"externals/\". Those must be built beforehand, breferably using the scripts \"make_externals.*\".\nset(Boost_REALPATH ON)\nset(Boost_USE_MULTITHREADED ON)\nset(Boost_USE_STATIC_LIBS OFF)\nfind_package(Boost REQUIRED COMPONENTS system chrono date_time filesystem)\nfind_package(OpenGL REQUIRED)\n\n# You have to provide the directory where the externals got installed to. The scripts make_*.sh and\n# make_*.bat set this directory via the command line.\nset(COSMOSCOUT_EXTERNALS_DIR COSMOSCOUT_EXTERNALS_DIR-NotFound \n  CACHE STRING \"Directory where the externals got installed to.\")\n\n# Make sure to use forward slashes only.\nfile(TO_CMAKE_PATH ${COSMOSCOUT_EXTERNALS_DIR} COSMOSCOUT_EXTERNALS_DIR)\n\nif (DEFINED ENV{CARES_ROOT_DIR})\n  SET(CARES_ROOT_DIR \"$ENV{CARES_ROOT_DIR}\")\nelse()\n  SET(CARES_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{CURL_ROOT_DIR})\n  SET(CURL_ROOT_DIR \"$ENV{CURL_ROOT_DIR}\")\nelse()\n  SET(CURL_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{CURLPP_ROOT_DIR})\n  SET(CURLPP_ROOT_DIR \"$ENV{CURLPP_ROOT_DIR}\")\nelse()\n  SET(CURLPP_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{SPDLOG_ROOT_DIR})\n  SET(SPDLOG_ROOT_DIR \"$ENV{SPDLOG_ROOT_DIR}\")\nelse()\n  SET(SPDLOG_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{GLM_ROOT_DIR})\n  SET(GLM_ROOT_DIR \"$ENV{GLM_ROOT_DIR}\")\nelse()\n  SET(GLM_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{GLI_ROOT_DIR})\n  SET(GLI_ROOT_DIR \"$ENV{GLI_ROOT_DIR}\")\nelse()\n  SET(GLI_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{DOCTEST_ROOT_DIR})\n  SET(DOCTEST_ROOT_DIR \"$ENV{DOCTEST_ROOT_DIR}\")\nelse()\n  SET(DOCTEST_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{TINYGLTF_ROOT_DIR})\n  SET(TINYGLTF_ROOT_DIR \"$ENV{TINYGLTF_ROOT_DIR}\")\nelse()\n  SET(TINYGLTF_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{CSPICE_ROOT_DIR})\n  SET(CSPICE_ROOT_DIR \"$ENV{CSPICE_ROOT_DIR}\")\nelse()\n  SET(CSPICE_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{CEF_ROOT_DIR})\n  SET(CEF_ROOT_DIR \"$ENV{CEF_ROOT_DIR}\")\nelse()\n  SET(CEF_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{TIFF_ROOT_DIR})\n  SET(TIFF_ROOT_DIR \"$ENV{TIFF_ROOT_DIR}\")\nelse()\n  SET(TIFF_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{GLEW_ROOT_DIR})\n  SET(GLEW_ROOT_DIR \"$ENV{GLEW_ROOT_DIR}\")\nelse()\n  SET(GLEW_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{SDL2_ROOT_DIR})\n  SET(SDL2_ROOT_DIR \"$ENV{SDL2_ROOT_DIR}\")\nelse()\n  SET(SDL2_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{SDL2_TTF_ROOT_DIR})\n  SET(SDL2_TTF_ROOT_DIR \"$ENV{SDL2_TTF_ROOT_DIR}\")\nelse()\n  SET(SDL2_TTF_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{OPENSG_ROOT_DIR})\n  SET(OPENSG_ROOT_DIR \"$ENV{OPENSG_ROOT_DIR}\")\nelse()\n  SET(OPENSG_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{CIVETWEB_ROOT_DIR})\n  SET(CIVETWEB_ROOT_DIR \"$ENV{CIVETWEB_ROOT_DIR}\")\nelse()\n  SET(CIVETWEB_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR})\nendif()\n\nif (DEFINED ENV{VISTA_CMAKE_CONFIG_DIR})\n  SET(VistaCoreLibs_DIR \"$ENV{VISTA_CMAKE_CONFIG_DIR}\")\nelse()\n  SET(VistaCoreLibs_DIR ${COSMOSCOUT_EXTERNALS_DIR}/share/VistaCoreLibs/cmake)\nendif()\n\nfind_package(GLM REQUIRED)\nfind_package(GLI REQUIRED)\nfind_package(DOCTEST REQUIRED)\nfind_package(TINYGLTF REQUIRED)\nfind_package(CSPICE REQUIRED)\nfind_package(CARES REQUIRED)\nfind_package(CURL REQUIRED)\nfind_package(CURLPP REQUIRED)\nfind_package(SPDLOG REQUIRED)\nfind_package(CEF REQUIRED)\nfind_package(TIFF REQUIRED)\nfind_package(GLEW REQUIRED)\nfind_package(SDL2 REQUIRED)\nfind_package(SDL2_ttf REQUIRED)\nfind_package(OPENSG REQUIRED)\nfind_package(CIVETWEB REQUIRED)\nfind_package(VistaCoreLibs REQUIRED \n  COMPONENTS \"VistaBase\" \"VistaKernel\" \"VistaKernelOpenSGExt\" \"VistaOGLExt\"\n)\n\n# X11 is used on Linux to set the application window's name and icon.\nif (UNIX)\n  find_package(X11)\nendif()\n\n# install some files -------------------------------------------------------------------------------\n\n# Copy all third-party libraries to install directory.\ninstall(\n  DIRECTORY ${COSMOSCOUT_EXTERNALS_DIR}/lib/ ${COSMOSCOUT_EXTERNALS_DIR}/bin/ \n  DESTINATION \"lib\"\n  FILES_MATCHING PATTERN \"*.so*\" PATTERN \"*.dll*\"\n)\n\n# Copy boost libraries to install directory.\nforeach(BOOST_LIB \n  ${Boost_CHRONO_LIBRARY_DEBUG}\n  ${Boost_CHRONO_LIBRARY_RELEASE}\n  ${Boost_DATE_TIME_LIBRARY_DEBUG}\n  ${Boost_DATE_TIME_LIBRARY_RELEASE}\n  ${Boost_FILESYSTEM_LIBRARY_RELEASE}\n  ${Boost_FILESYSTEM_LIBRARY_DEBUG}\n  ${Boost_SYSTEM_LIBRARY_RELEASE}\n  ${Boost_SYSTEM_LIBRARY_DEBUG}\n)\n  if(EXISTS \"${BOOST_LIB}\")\n    get_filename_component(LIB_BASE_NAME ${BOOST_LIB} NAME_WE)\n    get_filename_component(LIB_PATH ${BOOST_LIB} PATH)\n    if (WIN32)\n      install(FILES ${LIB_PATH}/${LIB_BASE_NAME}.dll DESTINATION \"lib\")\n    endif()\n    if (UNIX)\n      file(GLOB LIB_FILES ${LIB_PATH}/${LIB_BASE_NAME}.so*)\n      install(FILES ${LIB_FILES} DESTINATION \"lib\")\n    endif()\n  endif()\nendforeach()\n\n# Install documentation directory\ninstall(DIRECTORY\n  ${CMAKE_SOURCE_DIR}/docs\n  DESTINATION \".\"\n)\n\n# Install license files\ninstall(FILES\n  ${CMAKE_SOURCE_DIR}/LICENSE-3RD-PARTY.txt\n  ${CMAKE_SOURCE_DIR}/LICENSE.md\n  DESTINATION \"docs\"\n)\n\ninstall(DIRECTORY\n  ${CMAKE_SOURCE_DIR}/LICENSES\n  DESTINATION \"docs\"\n)\n\n# create version header ----------------------------------------------------------------------------\n\nmessage(STATUS \"Trying to get current git branch and commit...\")\n\n# Get the current git branch name.\nexecute_process(\n  COMMAND git rev-parse --abbrev-ref HEAD\n  WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\n  OUTPUT_VARIABLE GIT_BRANCH\n  OUTPUT_STRIP_TRAILING_WHITESPACE\n)\n\n# Get the current commit hash.\nexecute_process(\n  COMMAND git log -1 --format=%h\n  WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\n  OUTPUT_VARIABLE GIT_COMMIT_HASH\n  OUTPUT_STRIP_TRAILING_WHITESPACE\n)\n\n# Fallback to project version if it failed.\nif (GIT_BRANCH STREQUAL \"\")\n  message(STATUS \"  Failed - falling back to project version: v${PROJECT_VERSION}\")\nelse()\n  message(STATUS \"  Success: v${PROJECT_VERSION} (${GIT_BRANCH} @${GIT_COMMIT_HASH})\")\nendif()\n\nconfigure_file(\n  ${CMAKE_SOURCE_DIR}/src/cs-core/cs-version.hpp.in\n  ${CMAKE_BINARY_DIR}/src/cs-core/cs-version.hpp\n)\n\n# compiler settings --------------------------------------------------------------------------------\n\nadd_definitions(\n  -DBOOST_ALL_DYN_LINK\n  -DGLM_ENABLE_EXPERIMENTAL\n  -DGLM_FORCE_SWIZZLE\n  -DNOMINMAX\n  -DSPDLOG_COMPILED_LIB\n)\n\nif (MSVC)\n  add_definitions(\n    # For whatever reason ViSTA checks for 'WIN32' instead of '_WIN32' in a lot of places, which\n    # results in Linux code being included on Windows.\n    -DWIN32\n  )\n\n  add_compile_options(\n    $<$<COMPILE_LANGUAGE:CXX>:-experimental:external>\n    $<$<COMPILE_LANGUAGE:CXX>:-external:anglebrackets>\n    $<$<COMPILE_LANGUAGE:CXX>:-external:W0>\n    $<$<COMPILE_LANGUAGE:CXX>:-W3>\n    $<$<COMPILE_LANGUAGE:CXX>:-WX>\n    $<$<COMPILE_LANGUAGE:CXX>:-EHsc>\n    $<$<COMPILE_LANGUAGE:CXX>:-wd4251>\n\n    # Warns about multiple inheritance problems, which we can't avoid inheriting from ViSTA classes.\n    $<$<COMPILE_LANGUAGE:CXX>:-wd4250>\n  )\nelse()\n  add_compile_options(\n    $<$<COMPILE_LANGUAGE:CXX>:-Wall>\n    $<$<COMPILE_LANGUAGE:CXX>:-Werror>\n  )\nendif()\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\n# Enable unit tests\noption(COSMOSCOUT_UNIT_TESTS \"Enable compilation of tests\" OFF)\nif (NOT COSMOSCOUT_UNIT_TESTS)\n  add_definitions(-DDOCTEST_CONFIG_DISABLE)\nendif()\n\n# Enable code coverage measurements\noption(COSMOSCOUT_COVERAGE_INFO \"Run code coverage analytics\" OFF)\n\nif(COSMOSCOUT_COVERAGE_INFO AND \"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n  add_definitions(--coverage)\n  add_link_options(--coverage)\nendif()\n\n# subdirectories -----------------------------------------------------------------------------------\n\ninclude_directories(\n  ${CMAKE_BINARY_DIR}/src/cs-core\n  ${CMAKE_BINARY_DIR}/src/cs-utils\n  ${CMAKE_BINARY_DIR}/src/cs-graphics\n  ${CMAKE_BINARY_DIR}/src/cs-gui\n  ${CMAKE_BINARY_DIR}/src/cs-scene\n)\n\nadd_subdirectory(config)\nadd_subdirectory(resources)\nadd_subdirectory(src)\nadd_subdirectory(plugins)\nadd_subdirectory(tools/eclipse-shadow-generator)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cp2k",
            "repo_link": "https://github.com/cp2k/cp2k",
            "content": {
                "codemeta": "",
                "readme": "# CP2K\n\n[![Release Status][release-badge]][release-link] [![Debian Status][debian-badge]][debian-link]\n[![Fedora Status][fedora-badge]][fedora-link] [![Ubuntu Status][ubuntu-badge]][ubuntu-link]\n[![Arch Status][arch-badge]][arch-link] [![Homebrew Status][homebrew-badge]][homebrew-link]\n[![Docker Status][docker-badge]][docker-link] [![Spack Status][spack-badge]][spack-link]\n[![Conda Status][conda-badge]][conda-link]\n\nCP2K is a quantum chemistry and solid state physics software package that can perform atomistic\nsimulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems.\nCP2K provides a general framework for different modeling methods such as DFT using the mixed\nGaussian and plane waves approaches GPW and GAPW. Supported theory levels include DFT, MP2, RPA, GW,\ntight-binding (xTB, DFTB), semi-empirical methods (AM1, PM3, PM6, RM1, MNDO, ...), and classical\nforce fields (AMBER, CHARMM, ...). CP2K can do simulations of molecular dynamics, metadynamics,\nMonte Carlo, Ehrenfest dynamics, vibrational analysis, core level spectroscopy, energy minimization,\nand transition state optimization using NEB or dimer method.\n\nCP2K is written in Fortran 2008 and can be run efficiently in parallel using a combination of\nmulti-threading, MPI, and CUDA.\n\n## Downloading CP2K source code\n\nTo clone the current master (development version):\n\n```shell\ngit clone --recursive https://github.com/cp2k/cp2k.git cp2k\n```\n\nNote the `--recursive` flag that is needed because CP2K uses git submodules.\n\nTo clone a release version v*x.y*:\n\n```shell\ngit clone -b support/vx.y --recursive https://github.com/cp2k/cp2k.git cp2k\n```\n\nFor more information on downloading CP2K, see [Downloading CP2K](https://www.cp2k.org/download). For\nhelp on git, see [Git Tips & Tricks](https://github.com/cp2k/cp2k/wiki/Git-Tips-&-Tricks).\n\n## Install CP2K\n\nThe easiest way to build CP2K with all of its dependencies is as a\n[Docker container](./tools/docker/README.md).\n\nFor building CP2K from scratch see the [installation instructions](./INSTALL.md).\n\n## Links\n\n- [CP2K.org](https://www.cp2k.org) for showcases of scientific work, tutorials, exercises,\n  presentation slides, etc.\n- [The manual](https://manual.cp2k.org/) with descriptions of all the keywords for the CP2K input\n  file\n- [The dashboard](https://dashboard.cp2k.org) to get an overview of the currently tested\n  architectures\n- [The Google group](https://groups.google.com/group/cp2k) to get help if you could not find an\n  answer in one of the previous links\n- [Acknowledgements](https://www.cp2k.org/funding) for list of institutions and grants that help to\n  fund the development of CP2K\n\n## Directory organization\n\n- [`arch`](./arch): Collection of definitions for different architectures and compilers\n- [`benchmarks`](./benchmarks): Inputs for benchmarks\n- [`data`](./data): Simulation parameters e.g. basis sets and pseudopotentials\n- [`exts`](./exts): Access to external libraries via GIT submodules\n- [`src`](./src): The source code\n- [`tests`](./tests): Inputs for tests and regression tests\n- [`tools`](./tools): Mixed collection of useful scripts related to cp2k\n\nAdditional directories created during build process:\n\n- `lib`: Libraries built during compilation\n- `obj`: Objects and other intermediate compilation-time files\n- `exe`: Where the executables will be located\n\n[arch-badge]: https://img.shields.io/aur/version/cp2k\n[arch-link]: https://aur.archlinux.org/packages/cp2k\n[conda-badge]: https://img.shields.io/conda/vn/conda-forge/cp2k\n[conda-link]: https://anaconda.org/conda-forge/cp2k\n[debian-badge]: https://img.shields.io/debian/v/cp2k\n[debian-link]: https://packages.debian.org/search?keywords=cp2k\n[docker-badge]: https://img.shields.io/docker/v/cp2k/cp2k?label=docker\n[docker-link]: https://hub.docker.com/r/cp2k/cp2k\n[fedora-badge]: https://img.shields.io/fedora/v/cp2k\n[fedora-link]: https://src.fedoraproject.org/rpms/cp2k\n[homebrew-badge]: https://img.shields.io/homebrew/v/cp2k\n[homebrew-link]: https://formulae.brew.sh/formula/cp2k\n[release-badge]: https://img.shields.io/github/v/release/cp2k/cp2k\n[release-link]: https://github.com/cp2k/cp2k/releases\n[spack-badge]: https://img.shields.io/spack/v/cp2k\n[spack-link]: https://packages.spack.io/package.html?name=cp2k\n[ubuntu-badge]: https://img.shields.io/ubuntu/v/cp2k\n[ubuntu-link]: https://packages.ubuntu.com/search?keywords=cp2k\n\n",
                "dependencies": "#!-------------------------------------------------------------------------------------------------!\n#!   CP2K: A general program to perform molecular dynamics simulations                             !\n#!   Copyright 2000-2024 CP2K developers group <https://cp2k.org>                                  !\n#!                                                                                                 !\n#!   SPDX-License-Identifier: GPL-2.0-or-later                                                     !\n#!-------------------------------------------------------------------------------------------------!\n\ncmake_minimum_required(VERSION 3.22)\n\n# include our cmake snippets\nset(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ${CMAKE_CURRENT_SOURCE_DIR}/cmake)\n\n# =================================================================================================\n# REQUIRE OUT-OF-SOURCE BUILDS\nfile(TO_CMAKE_PATH \"${PROJECT_BINARY_DIR}/CMakeLists.txt\" LOC_PATH)\nif(EXISTS \"${LOC_PATH}\")\n  message(\n    FATAL_ERROR\n      \"You cannot build in a source directory (or any directory with a CMakeLists.txt file). Please make a build subdirectory.\"\n  )\nendif()\n\n# =================================================================================================\n# PROJECT AND VERSION\ninclude(CMakeDependentOption)\ninclude(GitSubmodule)\ninclude(CustomTargets)\n\ncmake_policy(SET CMP0048 NEW)\n\nif(POLICY CMP0144)\n  cmake_policy(SET CMP0144 NEW)\nendif()\n\n# !!! Keep version in sync with cp2k_info.F !!!\nproject(\n  cp2k\n  DESCRIPTION \"CP2K\"\n  HOMEPAGE_URL \"https://www.cp2k.org\"\n  VERSION \"2024.3\"\n  LANGUAGES Fortran C CXX)\n\nlist(APPEND CMAKE_MODULE_PATH \"${PROJECT_SOURCE_DIR}/cmake/modules\")\n\nif(NOT DEFINED CMAKE_CUDA_STANDARD)\n  set(CMAKE_CUDA_STANDARD 11)\n  set(CMAKE_CUDA_STANDARD_REQUIRED ON)\nendif()\n\n# set language and standard\nset(CMAKE_CXX_STANDARD 14)\nset(CMAKE_C_STANDARD 11)\n\n# remove NDEBUG flag\nstring(REPLACE \"-DNDEBUG\" \"\" CMAKE_C_FLAGS_RELEASE ${CMAKE_C_FLAGS_RELEASE})\nstring(REPLACE \"-DNDEBUG\" \"\" CMAKE_CXX_FLAGS_RELEASE ${CMAKE_CXX_FLAGS_RELEASE})\nstring(REPLACE \"-DNDEBUG\" \"\" CMAKE_Fortran_FLAGS_RELEASE\n               ${CMAKE_Fortran_FLAGS_RELEASE})\nstring(REPLACE \"-DNDEBUG\" \"\" CMAKE_C_FLAGS_RELWITHDEBINFO\n               ${CMAKE_C_FLAGS_RELWITHDEBINFO})\nstring(REPLACE \"-DNDEBUG\" \"\" CMAKE_CXX_FLAGS_RELWITHDEBINFO\n               ${CMAKE_CXX_FLAGS_RELWITHDEBINFO})\nstring(REPLACE \"-DNDEBUG\" \"\" CMAKE_Fortran_FLAGS_RELWITHDEBINFO\n               ${CMAKE_Fortran_FLAGS_RELWITHDEBINFO})\nstring(REPLACE \"-DNDEBUG\" \"\" CMAKE_C_FLAGS_MINSIZEREL\n               ${CMAKE_C_FLAGS_MINSIZEREL})\nstring(REPLACE \"-DNDEBUG\" \"\" CMAKE_CXX_FLAGS_MINSIZEREL\n               ${CMAKE_CXX_FLAGS_MINSIZEREL})\nstring(REPLACE \"-DNDEBUG\" \"\" CMAKE_Fortran_FLAGS_MINSIZEREL\n               ${CMAKE_Fortran_FLAGS_MINSIZEREL})\n\nfind_package(PkgConfig)\n\n# ##############################################################################\n# Define the paths for static libraries and executables\n# ##############################################################################\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY\n    ${cp2k_BINARY_DIR}/lib\n    CACHE PATH \"Single output directory for building all libraries.\")\n\n# Search for rocm in common locations\nforeach(__var ROCM_ROOT CRAY_ROCM_ROOT ORNL_ROCM_ROOT CRAY_ROCM_PREFIX\n              ROCM_PREFIX CRAY_ROCM_DIR)\n  if($ENV{${__var}})\n    list(APPEND CMAKE_PREFIX_PATH $ENV{__var})\n    set(ROCM_PATH\n        $ENV{__var}\n        CACHE PATH \"Path to ROCm installation\")\n  endif()\nendforeach()\n\n# =================================================================================================\n# OPTIONS\n\noption(CMAKE_POSITION_INDEPENDENT_CODE \"Enable position independent code\" ON)\noption(CP2K_ENABLE_CONSISTENCY_CHECKS\n       \"Check that the list of compiled files and files contained in src match\"\n       OFF)\noption(CP2K_DEBUG_MODE \"Enable several additional options for debugging cp2k.\"\n       OFF)\noption(CP2K_USE_DFTD4 \"Enable dispersion coorections DFTD4\" OFF)\noption(CP2K_USE_SIRIUS \"Enable plane wave dft calculations with sirius\" OFF)\noption(CP2K_USE_FFTW3 \"Use fftw3 for the calculating fast fourier transforms\"\n       ON)\noption(CP2K_USE_MPI \"Enable MPI support\" ON)\noption(CP2K_USE_ELPA \"Enable elpa support\" OFF)\noption(CP2K_USE_PEXSI \"Enable pexsi support\" OFF)\noption(CP2K_USE_SUPERLU \"Enable superlu support\" OFF)\noption(CP2K_USE_COSMA \"COSMA is a drop in replacement of scalapack dgemm\" OFF)\noption(CP2K_USE_LIBINT2 \"Enable libint2 support\" OFF)\noption(CP2K_USE_PLUMED \"Enable plumed2 support\" OFF)\noption(CP2K_USE_VORI \"Enable libvori support\" OFF)\noption(CP2K_USE_PEXSI \"Enable pexsi support\" OFF)\noption(CP2K_USE_QUIP \"Enable quip support\" OFF)\noption(CP2K_USE_SPGLIB \"Enable spglib support\" OFF)\noption(CP2K_USE_LIBXC \"Enable libxc support\" OFF)\noption(CP2K_USE_LIBTORCH \"Enable libtorch support\" OFF)\noption(CP2K_USE_STATIC_BLAS \"Link against static version of BLAS/LAPACK\" OFF)\noption(CP2K_USE_SPLA\n       \"Use SPLA offloading gemm feature to the GPU if it is beneficial. \" OFF)\noption(CP2K_USE_METIS \"enable metis library support\" OFF)\noption(\n  CP2K_USE_DLAF\n  \"Use DLA-Future for Cholesky decompositions and the solution of eigenvalue problems\"\n  OFF)\noption(CP2K_USE_LIBXSMM \"Use libxsmm for small gemms\" OFF)\n# option(CP2K_USE_LIBGRPP \"Use libgrpp\" OFF)\noption(BUILD_SHARED_LIBS \"Build cp2k shared library\" ON)\noption(\n  CP2K_USE_FFTW3_WITH_MKL\n  \"MKL has its own compatible implementation of the fftw. This option when ON will use the original implementation of the fftw library\"\n  OFF)\ncmake_dependent_option(CP2K_ENABLE_ELPA_OPENMP_SUPPORT\n                       \"Enable elpa openmp support\" ON \"CP2K_USE_ELPA\" OFF)\ncmake_dependent_option(CP2K_ENABLE_FFTW3_OPENMP_SUPPORT\n                       \"Enable FFTW openmp support\" ON \"CP2K_USE_FFTW3\" OFF)\ncmake_dependent_option(CP2K_ENABLE_FFTW3_THREADS_SUPPORT\n                       \"Enable FFTW THREADS support\" OFF \"CP2K_USE_FFTW3\" OFF)\ncmake_dependent_option(CP2K_USE_MPI_F08 \"Enable MPI Fortran 2008 interface\" OFF\n                       \"CP2K_USE_MPI\" OFF)\n\ncmake_dependent_option(\n  CP2K_USE_CUSOLVER_MP\n  \"Use Nvidia gpu accelerated eigensolver. Only active when CUDA is ON\" OFF\n  \"CP2K_USE_ACCEL MATCHES \\\"CUDA\\\"\" OFF)\n\nset(CP2K_BLAS_VENDOR\n    \"auto\"\n    CACHE STRING \"Blas library for computations on host\")\n\nset(CP2K_SCALAPACK_VENDOR_LIST \"MKL\" \"SCI\" \"GENERIC\")\nset(CP2K_SCALAPACK_VENDOR\n    \"GENERIC\"\n    CACHE STRING \"scalapack vendor/generic backend\")\nset_property(CACHE CP2K_SCALAPACK_VENDOR PROPERTY STRINGS\n                                                  ${CP2K_SCALAPACK_VENDOR_LIST})\n\nif(NOT ${CP2K_SCALAPACK_VENDOR} IN_LIST CP2K_SCALAPACK_VENDOR_LIST)\n  message(FATAL_ERROR \"Invalid scalapack vendor backend\")\nendif()\n\nset(CP2K_BUILD_OPTIONS_LIST \"CUSTOM\" \"DEFAULT\" \"MINIMAL\" \"FULL\" \"SERIAL\")\n\nset(CP2K_BUILD_OPTIONS\n    \"CUSTOM\"\n    CACHE\n      STRING\n      \"Build cp2k with a predefined set of dependencies. The default setting is full user control\"\n)\nset_property(CACHE CP2K_BUILD_OPTIONS PROPERTY STRINGS\n                                               ${CP2K_BUILD_OPTIONS_LIST})\n\nset(CP2K_DATA_DIR\n    \"default\"\n    CACHE STRING \"Set the location for cp2k data\")\n\nstring(TOUPPER ${CP2K_BUILD_OPTIONS} cp2k_build_options_up)\n\nif(NOT ${cp2k_build_options_up} IN_LIST CP2K_BUILD_OPTIONS_LIST)\n  message(FATAL_ERROR \"Invalid value for cp2k build options\")\nendif()\n\nif(${cp2k_build_options_up} STREQUAL \"MINIMAL\")\n  set(CP2K_USE_FFTW3 ON)\n  set(CP2K_USE_MPI ON)\nendif()\n\nif(${cp2k_build_options_up} STREQUAL \"FULL\")\n  set(CP2K_USE_FFTW3 ON)\n  set(CP2K_USE_MPI ON)\n  set(CP2K_USE_DFTD4 ON)\n  set(CP2K_USE_SIRIUS ON)\n  set(CP2K_USE_LIBXSMM ON)\n  set(CP2K_USE_VORI ON)\n  set(CP2K_USE_COSMA ON)\n  set(CP2K_USE_SPLA ON)\n  set(CP2K_USE_LIBXC ON)\n  set(CP2K_USE_LIBINT2 ON)\n  set(CP2K_USE_ELPA ON)\n  set(CP2K_USE_SPGLIB ON)\n  set(CP2K_USE_LIBTORCH ON)\n  set(CP2K_USE_METIS ON)\n  set(CP2K_USE_QUIP ON)\n  set(CP2K_USE_PEXSI ON)\n  set(CP2K_USE_SUPERLU ON)\n  # set(CP@K_USE_LIBGRPP ON)\nendif()\n\nif(${cp2k_build_options_up} STREQUAL \"SERIAL\")\n  set(CP2K_USE_FFTW3 ON)\n  set(CP2K_USE_MPI OFF)\n  set(CP2K_USE_SIRIUS ON)\n  set(CP2K_USE_SIRIUS ON)\n  set(CP2K_USE_LIBXSMM ON)\n  set(CP2K_USE_VORI ON)\n  set(CP2K_USE_COSMA ON)\n  set(CP2K_USE_SPLA ON)\n  set(CP2K_USE_LIBXC ON)\n  set(CP2K_USE_LIBINT2 ON)\n  set(CP2K_USE_ELPA ON)\n  set(CP2K_USE_SPGLIB ON)\n  set(CP2K_USE_LIBTORCH ON)\n  set(CP2K_USE_METIS ON)\n  set(CP2K_USE_QUIP ON)\n  set(CP2K_USE_PEXSI ON)\n  set(CP2K_USE_SUPERLU ON)\nendif()\n\nif(${cp2k_build_options_up} STREQUAL \"DEFAULT\")\n  set(CP2K_USE_FFTW3 ON)\n  set(CP2K_USE_MPI ON)\n  set(CP2K_USE_SIRIUS ON)\n  set(CP2K_USE_COSMA ON)\n  set(CP2K_USE_LIBXSMM ON)\n  set(CP2K_USE_LIBXC ON)\n  set(CP2K_USE_LIBINT2 ON)\n  set(CP2K_USE_SPGLIB ON)\nendif()\n\n# ##############################################################################\n# # gpu related options                                                    # #\n# ##############################################################################\n\nset(CP2K_SUPPORTED_ACCELERATION_TARGETS CUDA HIP NONE)\nset(CP2K_SUPPORTED_CUDA_ARCHITECTURES\n    K20X\n    K40\n    K80\n    P100\n    V100\n    A100\n    H100\n    A40)\nset(CP2K_SUPPORTED_HIP_ARCHITECTURES\n    Mi50\n    Mi100\n    Mi210\n    Mi250\n    Mi300\n    K20X\n    K40\n    K80\n    P100\n    V100\n    A100\n    H100\n    A40)\n\nset(CP2K_WITH_GPU\n    \"P100\"\n    CACHE STRING\n          \"Set the CUDA GPU architecture if HIP is enabled (default: P100)\")\n\nset_property(\n  CACHE CP2K_WITH_GPU PROPERTY STRINGS ${CP2K_SUPPORTED_CUDA_ARCHITECTURES}\n                               ${CP2K_SUPPORTED_HIP_ARCHITECTURES})\n\nset(CP2K_USE_ACCEL\n    \"NONE\"\n    CACHE STRING \"Set hardware acceleration support: CUDA, HIP\")\n\nset_property(CACHE CP2K_USE_ACCEL\n             PROPERTY STRINGS ${CP2K_SUPPORTED_ACCELERATION_TARGETS})\n\ncmake_dependent_option(CP2K_USE_NVHPC OFF \"Enable Nvidia NVHPC kit\"\n                       \"(NOT CP2K_USE_ACCEL MATCHES \\\"CUDA\\\")\" OFF)\n\ncmake_dependent_option(\n  CP2K_USE_SPLA_GEMM_OFFLOADING ON\n  \"Enable SPLA dgemm offloading (only valid with gpu support on)\"\n  \"(NOT CP2K_USE_ACCEL MATCHES \\\"NONE\\\") AND (CP2K_USE_SPLA)\" OFF)\n\n# ##############################################################################\n#\n# GPU debug options\n#\n# ##############################################################################\n\ncmake_dependent_option(\n  CP2K_DISABLE_GRID_GPU\n  OFF\n  \"disable the hardware accelerated backend for grid related functions. It is only effective when general gpu support is enabled.\"\n  \"CP2K_DEBUG_MODE\"\n  OFF)\n\ncmake_dependent_option(\n  CP2K_DISABLE_PW_GPU\n  OFF\n  \"disable the ffts accelerated (mostly GPU) backend. It is only effective when general gpu support is enabled.\"\n  \"CP2K_DEBUG_MODE\"\n  OFF)\ncmake_dependent_option(\n  CP2K_DISABLE_DBM_GPU\n  OFF\n  \"disable the dbm accelerated (mostly GPU) backend. It is only effective when general gpu support is enabled.\"\n  \"CP2K_DEBUG_MODE\"\n  OFF)\ncmake_dependent_option(\n  CP2K_DBCSR_CPU_ONLY \"Use DBCSR compiled without GPU support.\" OFF\n  \"(NOT CP2K_USE_ACCEL MATCHES \\\"NONE\\\") AND (CP2K_DEBUG_MODE)\" OFF)\n\ncmake_dependent_option(\n  CP2K_USE_UNIFIED_MEMORY \"Use CPU/GPU unified memory.\n  requires Mi250x or future AMD architectures to be fully functional\" OFF\n  \"CP2K_USE_ACCEL MATCHES \\\"HIP\\\"\" OFF)\n\ncmake_dependent_option(DBCSR_USE_OPENMP \"Enable openmp support in DBCSR\" ON\n                       \"CP2K_BUILD_DBCSR\" OFF)\n# ##############################################################################\n# specific variables for the regtests. Binaries will be created with an\n# extension               #\n# ##############################################################################\n\nset(__cp2k_ext \"\")\nset(__cp2k_cmake_name \"\")\n\nif(CP2K_USE_MPI)\n  set(__cp2k_ext \"psmp\")\nelse()\n  set(__cp2k_ext \"ssmp\")\nendif()\n\nset(__cp2k_cmake_name \"local\")\n\nif(CP2K_USE_ACCEL MATCHES \"CUDA\")\n  set(__cp2k_cmake_name \"local_cuda\")\nendif()\n\nif(CP2K_USE_ACCEL MATCHES \"HIP\")\n  set(__cp2k_cmake_name \"local_hip\")\nendif()\n\nif(CP2K_CMAKE_SUFFIX)\n  string(APPEND __cp2k_cmake_name \"${CP2K_CMAKE_SUFFIX}\")\nendif()\n\n# we can run the src consistency checks without actually searching for any\n# dependencies.\n\nif(CP2K_ENABLE_CONSISTENCY_CHECKS)\n  add_subdirectory(src)\n  # it is better to simply rm -Rf build but if someone wants to do something\n  # like\n  #\n  # cmake -DCP2K_ENABLE_CONSISTENCY_CHECKS=ON .. cmake ..\n  #\n  # he/she can\n\n  set(CP2K_ENABLE_CONSISTENCY_CHECKS\n      OFF\n      CACHE BOOL \"\" FORCE)\n  return()\nendif()\n\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY\n    ${cp2k_BINARY_DIR}/bin\n    CACHE PATH \"Single output directory for building all executables.\")\n\n# Python\n#\n# this module looks preferably for version 3 of Python. If not found, version 2\n# is searched. In CMake 3.15, if a python virtual environment is activated, it\n# will search the virtual environment for a python interpreter before searching\n# elsewhere in the system. In CMake <3.15, the system is searched before the\n# virtual environment.\n\nif(NOT Python_EXECUTABLE)\n  # If the python interpreter isn't specified as a command line option, look for\n  # it:\n  find_package(\n    Python\n    COMPONENTS Interpreter\n    REQUIRED)\nendif()\n\n# get the git hash Get the latest abbreviated commit hash of the working branch\nexecute_process(\n  COMMAND git log -1 --format=%h\n  WORKING_DIRECTORY ${CMAKE_CURRENT_LIST_DIR}\n  OUTPUT_VARIABLE CP2K_GIT_HASH\n  OUTPUT_STRIP_TRAILING_WHITESPACE)\n\nexecute_process(\n  COMMAND hostnamectl --transient\n  WORKING_DIRECTORY ${CMAKE_CURRENT_LIST_DIR}\n  OUTPUT_VARIABLE CP2K_HOST_NAME\n  OUTPUT_STRIP_TRAILING_WHITESPACE)\n\nadd_custom_target(\n  AlwaysCheckGit\n  COMMAND\n    ${CMAKE_COMMAND} -DRUN_CHECK_GIT_VERSION=1\n    -Dpre_configure_dir=${pre_configure_dir}\n    -Dpost_configure_file=${post_configure_dir}\n    -DGIT_HASH_CACHE=${GIT_HASH_CACHE} -P ${CURRENT_LIST_DIR}/CheckGit.cmake\n  BYPRODUCTS ${post_configure_file})\n\n# MPI\n\nif(CP2K_USE_MPI)\n  get_property(REQUIRED_MPI_COMPONENTS GLOBAL PROPERTY ENABLED_LANGUAGES)\n  list(REMOVE_ITEM REQUIRED_MPI_COMPONENTS CUDA) # CUDA does not have an MPI\n  # component\n  if(NOT CMAKE_CROSSCOMPILING) # when cross compiling, assume the users know\n    # what they are doing\n    set(MPI_DETERMINE_LIBRARY_VERSION TRUE)\n  endif()\n  find_package(\n    MPI\n    COMPONENTS ${REQUIRED_MPI_COMPONENTS}\n    REQUIRED)\n\n  if(NOT MPI_Fortran_HAVE_F90_MODULE)\n    message(\n      FATAL_ERROR\n        \"\\\nThe listed MPI implementation does not provide the required mpi.mod interface. \\\nWhen using the GNU compiler in combination with Intel MPI, please use the \\\nIntel MPI compiler wrappers. Check the INSTALL.md for more information.\")\n  endif()\n  if(\"${MPI_Fortran_LIBRARY_VERSION_STRING}\" MATCHES \"Open MPI v2.1\"\n     OR \"${MPI_Fortran_LIBRARY_VERSION_STRING}\" MATCHES \"Open MPI v3.1\")\n    message(\n      WARNING\n        \"RMA with ${MPI_Fortran_LIBRARY_VERSION_STRING} is not supported due to issues with its implementation.\"\n        \" Please use a newer version of OpenMPI or switch to MPICH if you plan on using MPI-RMA.\"\n    )\n  endif()\nelse()\n  if(CP2K_USE_SIRIUS\n     OR CP2K_USE_COSMA\n     OR CP2K_USE_SPLA\n     OR CP2K_USE_ELPA\n     OR CP2K_USE_METIS\n     OR CP2K_USE_PLUMED\n     OR CP2K_USE_PEXSI\n     OR CP2K_USE_QUIP)\n    message(\n      WARNING\n        \"SIRIUS, COSMA, SPLA, ELPA, METIS, PLUMED, PEXSI and QUIP require MPI support.\\n\"\n        \"Either set -DCP2K_USE_MPI to ON or turn the following dependencies off\\n\\n\"\n        \"List of dependencies with mpi support\\n\")\n    foreach(\n      __libs\n      sirius\n      COSMA\n      SPLA\n      ELPA\n      METIS\n      PLUMED\n      PEXSI\n      QUIP)\n      if(CP2K_USE_${__libs})\n        message(WARNING \" - ${__libs}\")\n      endif()\n    endforeach()\n    message(FATAL_ERROR \" -- \")\n  endif()\nendif()\n\n# BLAS & LAPACK, PkgConfig\nfind_package(Lapack REQUIRED) # also calls find_package(BLAS)\n\n# SMM (Small Matrix-Matrix multiplication)\nif(CP2K_USE_LIBXSMM)\n  find_package(LibXSMM REQUIRED)\n  message(STATUS \"Using libxsmm for Small Matrix Multiplication\")\nendif()\n\n# in practice it is always for any decent configuration. But I add a flags to\n# turn it off\nif(CP2K_USE_MPI)\n  find_package(SCALAPACK REQUIRED)\nendif()\n\n# CUDA / ROCM easy for cuda a moving target for hip\n\nif((CP2K_USE_ACCEL MATCHES CUDA) OR (CP2K_USE_ACCEL MATCHES HIP))\n  set(CP2K_GPU_ARCH_NUMBER_K20X 35)\n  set(CP2K_GPU_ARCH_NUMBER_K40 35)\n  set(CP2K_GPU_ARCH_NUMBER_K80 37)\n  set(CP2K_GPU_ARCH_NUMBER_P100 60)\n  set(CP2K_GPU_ARCH_NUMBER_V100 70)\n  set(CP2K_GPU_ARCH_NUMBER_A100 80)\n  set(CP2K_GPU_ARCH_NUMBER_H100 90)\n  set(CP2K_GPU_ARCH_NUMBER_A40 86)\n  set(CP2K_GPU_ARCH_NUMBER_Mi50 gfx906)\n  set(CP2K_GPU_ARCH_NUMBER_Mi100 gfx908)\n  set(CP2K_GPU_ARCH_NUMBER_Mi200 gfx90a)\n  set(CP2K_GPU_ARCH_NUMBER_Mi250 gfx90a)\n  set(CP2K_GPU_ARCH_NUMBER_Mi300 gfx942)\nendif()\n\nset(CP2K_USE_HIP OFF)\nset(CP2K_USE_CUDA OFF)\n\noption(CP2K_USE_PW_GPU \"Enable GPU in CUDA\" OFF)\nif(CP2K_USE_ACCEL MATCHES \"CUDA\")\n  option(CP2K_WITH_CUDA_PROFILING \"Enable CUDA profiling\" OFF)\n  # P100 is the default target.\n  set(CMAKE_CUDA_ARCHITECTURES 60)\n\n  # allow for unsupported compilers (gcc/cuda version mismatch)\n  set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -allow-unsupported-compiler\")\n\n  enable_language(CUDA)\n  if(CP2K_USE_NVHPC)\n    find_package(NVHPC REQUIRED COMPONENTS CUDA MATH HOSTUTILS NCCL)\n  else()\n    find_package(CUDAToolkit REQUIRED)\n  endif()\n\n  list(FIND CP2K_SUPPORTED_CUDA_ARCHITECTURES ${CP2K_WITH_GPU}\n       CP2K_GPU_SUPPORTED)\n\n  if(CP2K_GPU_SUPPORTED EQUAL -1)\n    message(\n      FATAL_ERROR\n        \"GPU architecture (${CP2K_WITH_GPU}) is not supported. Please choose from: ${CP2K_SUPPORTED_CUDA_ARCHITECTURES}\"\n    )\n  endif()\n\n  set(CMAKE_CUDA_ARCHITECTURES ${CP2K_GPU_ARCH_NUMBER_${CP2K_WITH_GPU}})\n  message(STATUS \"GPU target architecture: ${CP2K_WITH_GPU}\\n\"\n                 \"GPU architecture number: ${CMAKE_CUDA_ARCHITECTURES}\\n\"\n                 \"GPU profiling enabled: ${CP2K_WITH_CUDA_PROFILING}\\n\")\n  set(CP2K_ACC_ARCH_NUMBER ${CMAKE_CUDA_ARCHITECTURES})\n\n  if(WITH_CUDA_PROFILING)\n    find_library(\n      CUDA_NVTOOLSEXT nvToolsExt\n      PATHS ${CMAKE_CUDA_IMPLICIT_LINK_DIRECTORIES}\n      DOC \"Building with CUDA profiling requires the nvToolsExt CUDA library\"\n          REQUIRED)\n    message(STATUS \"Found nvToolsExt: ${CUDA_NVTOOLSEXT}\")\n  endif()\n\n  set(CP2K_USE_CUDA ON)\n  set(CP2K_USE_PW_GPU ON)\n\n  if(CP2K_USE_CUSOLVER_MP)\n    find_package(CuSolverMP REQUIRED)\n  endif()\n  message(STATUS ``\"-- CUDA compiler and libraries found\")\n\nelseif(CP2K_USE_ACCEL MATCHES \"HIP\")\n  enable_language(HIP)\n  # Find hip\n  find_package(hipfft REQUIRED IMPORTED CONFIG)\n  find_package(hipblas REQUIRED IMPORTED CONFIG)\n\n  set(CMAKE_HIP_ARCHITECTURES gfx801 gfx900 gfx90a)\n  if(NOT CMAKE_BUILD_TYPE)\n    set(HIP_RELEASE_OPTIONS \"-O3 --std=c++11\")\n  elseif(${CMAKE_BUILD_TYPE} STREQUAL \"RelWithDebInfo\")\n    set(HIP_RELEASE_OPTIONS \"-O2 -g --std=c++11\")\n  elseif(${CMAKE_BUILD_TYPE} STREQUAL \"Release\")\n    set(HIP_RELEASE_OPTIONS \"-O3 --std=c++11\")\n  elseif(${CMAKE_BUILD_TYPE} STREQUAL \"Debug\")\n    set(HIP_RELEASE_OPTIONS \"-O0 -g --std=c++11\")\n  endif()\n\n  set(CMAKE_HIP_ARCHITECTURES \"${CP2K_GPU_ARCH_NUMBER_${CP2K_WITH_GPU}}\")\n  set(CP2K_ACC_ARCH_NUMBER ${CP2K_GPU_ARCH_NUMBER_${CP2K_WITH_GPU}})\n  set(CP2K_USE_HIP ON)\n  set(CP2K_USE_PW_GPU ON)\n\n  # use hardware atomic operations on Mi250X.\n  if(CMAKE_HIP_ARCGITECTURES MATCHES \"gfx90a|gfx942\")\n    set(HIP_RELEASE_OPTIONS \"-munsafe-fp-atomics\")\n  endif()\n\n  # add the Mi300A parameters when available\n  if(CP2K_USE_UNIFIED_MEMORY)\n    if(CMAKE_HIP_ARCHITECTURES MATCHES \"gfx90a\")\n      set(CMAKE_HIP_ARCHITECTURES \"gfx90a:xnack+\")\n    endif()\n\n    if(CMAKE_HIP_ARCHITECTURES MATCHES \"gfx942\")\n      set(CMAKE_HIP_ARCHITECTURES \"gfx942:xnack+\")\n    endif()\n  endif()\nendif()\n\n# PACKAGE DISCOVERY (compiler configuration can impact package discovery)\nfind_package(OpenMP REQUIRED COMPONENTS Fortran C CXX)\n\nfind_package(DBCSR 2.6 REQUIRED)\n\n# ==================================\nif(CP2K_USE_ELPA)\n  find_package(Elpa REQUIRED)\nendif()\n\nif(CP2K_USE_LIBXC)\n  find_package(LibXC 6 REQUIRED EXACT)\nendif()\n\n# uncomment this when libgrpp cmake support is complete\n\n# if (CP2K_USE_LIBGRPP) find_package(libgrpp REQUIRED) endif()\n\nif(CP2K_USE_COSMA)\n  find_package(cosma REQUIRED)\n\n  # check that cosma::cosma_pxgemm and cosma::cosma_prefixed_pxgemm exist\n  if(NOT TARGET cosma::cosma_pxgemm OR NOT TARGET cosma::cosma_prefixed_pxgemm)\n    message(\n      FATAL_ERROR\n        \" COSMA needs to be build with scalapack offloading support. COSTA_SCALAPACK and COSMA_SCALAPACK should probably be set properly\"\n    )\n  endif()\nendif()\n\nif(CP2K_USE_VORI)\n  find_package(LibVORI REQUIRED)\nendif()\n\nif(CP2K_USE_DLAF)\n  find_package(DLAFFortran REQUIRED)\nendif()\n\n# FFTW3\n\n# we set this variable to ON when we want fftw3 support (with or without MKL).\nset(CP2K_USE_FFTW3_ OFF)\nif(CP2K_USE_FFTW3)\n  if(NOT CP2K_BLAS_VENDOR MATCHES \"MKL\" OR CP2K_USE_FFTW3_WITH_MKL)\n    find_package(Fftw REQUIRED)\n    if(CP2K_ENABLE_FFTW3_THREADS_SUPPORT AND CP2K_ENABLE_FFTW3_OPENMP_SUPPORT)\n      message(\n        FATAL_ERROR\n          \"Fftw3 threads and openmp supports can not be used at the same time\")\n    endif()\n\n    if((CP2K_ENABLE_FFTW3_THREADS_SUPPORT) AND (NOT TARGET\n                                                cp2k::FFTW3::fftw3_threads))\n      message(\n        FATAL_ERROR\n          \"fftw3 was compiled without multithreading support (--enable-threads option in the fftw build system).\"\n      )\n    endif()\n\n    if((CP2K_ENABLE_FFTW3_OPENMP_SUPPORT) AND (NOT TARGET cp2k::FFTW3::fftw3_omp\n                                              ))\n      message(\n        FATAL_ERROR\n          \"fftw3 was compiled without openmp support  (--enable-openmp option in the fftw build system).\"\n      )\n    endif()\n    set(CP2K_USE_FFTW3_ ON)\n  else()\n    message(\"-- Using the MKL implementation of FFTW3.\")\n    set(CP2K_USE_FFTW3_MKL_ ON)\n  endif()\nendif()\n\n# QUIP\nif(CP2K_USE_QUIP)\n  find_package(Quip REQUIRED)\nendif()\n\n# libint\n\nif(CP2K_USE_LIBINT2)\n  find_package(Libint2 REQUIRED)\nendif()\n\n# spglib\nif(CP2K_USE_SPGLIB)\n  find_package(Spglib CONFIG REQUIRED)\nendif()\n\nif(CP2K_USE_SPLA)\n  find_package(SPLA REQUIRED)\n  get_target_property(SPLA_INCLUDE_DIRS SPLA::spla\n                      INTERFACE_INCLUDE_DIRECTORIES)\n  if(NOT SPLA_INCLUDE_DIRS)\n    set(SPLA_INCLUDE_DIRS \"/usr/include;/usr/include/spla\")\n  endif()\n\n  if(NOT SPLA_GPU_BACKEND AND CP2K_USE_GEMM_OFFLOADING)\n    set(CP2K_USE_GEMM_OFFLOADING OFF)\n    message(\n      FATAL_ERROR\n        \"SPLA should be compiled with GPU support if the gemm offloading is requested. Use -DCP2K_USE_GEMM_OFFLOADING=OFF otherwise\"\n    )\n  endif()\nendif()\n\nif(CP2K_USE_DFTD4)\n  find_package(dftd4 REQUIRED)\nendif()\n\n# SIRIUS\nif(CP2K_USE_SIRIUS)\n  find_package(sirius REQUIRED)\nendif()\n\nif(CP2K_USE_SUPERLU)\n  find_package(SuperLU REQUIRED)\nendif()\n\nif(CP2K_USE_METIS)\n  find_package(Metis)\nendif()\n\nif(CP2K_USE_PTSCOTCH)\n  find_package(Ptscotch REQUIRED)\nendif()\n\nif(CP2K_USE_PEXSI)\n  # PEXSI 1.2 uses cmake as build system\n  find_package(PEXSI REQUIRED)\nendif()\n\nif(CP2K_USE_PLUMED)\n  find_package(Plumed REQUIRED)\nendif()\n\nif(CP2K_USE_LIBTORCH)\n  find_package(Torch REQUIRED)\nendif()\n\nif(CP2K_USE_MPI_F08 AND NOT MPI_Fortran_HAVE_F08_MODULE)\n  message(\n    FATAL_ERROR\n      \"The Fortran 2008 interface is not supported by the MPI implementation found by cmake.\"\n  )\nendif()\n\n# OPTION HANDLING\n\n# make sure that the default build type is RELEASE\nset(default_build_type \"Release\")\n\nif(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n  message(\n    STATUS\n      \"Setting build type to '${default_build_type}' as none was specified.\")\n  set(CMAKE_BUILD_TYPE\n      \"${default_build_type}\"\n      CACHE STRING\n            \"Choose the type of build, options are: Debug Release Coverage.\"\n            FORCE)\n  # set the possible values of build type for cmake-gui\n  set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\"\n                                               \"Coverage\")\nendif()\n\n# compiler configuration could have impacted package discovery (above)\ninclude(CompilerConfiguration)\ninclude(CheckCompilerSupport)\n\ninclude(GNUInstallDirs)\n\n# subdirectories\nadd_subdirectory(src)\n\nget_target_property(CP2K_LIBS cp2k_link_libs INTERFACE_LINK_LIBRARIES)\nconfigure_file(cmake/libcp2k.pc.in libcp2k.pc @ONLY)\n\nmessage(\n  \"\"\n  \"--------------------------------------------------------------------\\n\"\n  \"-                                                                  -\\n\"\n  \"-               Summary of enabled dependencies                    -\\n\"\n  \"-                                                                  -\\n\"\n  \"--------------------------------------------------------------------\\n\\n\")\n\nmessage(\n  \"  - BLAS AND LAPACK\\n\\n\"\n  \"   - vendor :              ${CP2K_BLAS_VENDOR}\\n\"\n  \"   - include directories : ${CP2K_BLAS_INCLUDE_DIR} ${LAPACK_INCLUDE_DIR}\\n\"\n  \"   - libraries :           ${CP2K_BLAS_LINK_LIBRARIES} ${CP2K_LAPACK_LINK_LIBRARIES}\\n\\n\"\n)\n\nif(CP2K_USE_MPI)\n  message(\"  - MPI\\n\" \"   - include directories :  ${MPI_INCLUDE_DIRS}\\n\"\n          \"   - libraries :           ${MPI_LIBRARIES}\\n\\n\")\n\n  if(CP2K_USE_MPI_F08)\n    message(\"   - MPI_08 :              ON\\n\")\n  endif()\n\n  if(MPI_Fortran_HAVE_F08_MODULE AND NOT CP2K_USE_MPI_F08)\n    message(\n      \"  - MPI_08 is supposed by MPI but turned off by default.\\n\"\n      \"    To use it add -DCP2K_USE_MPI_F08=ON to the cmake command line\\n\\n\")\n  endif()\n\n  message(\"  - SCALAPACK:\\n\"\n          \"    - libraries : ${CP2K_SCALAPACK_LINK_LIBRARIES}\\n\\n\")\nendif()\n\nif((CP2K_USE_ACCEL MATCHES \"CUDA\") OR (CP2K_USE_ACCEL MATCHES \"HIP\"))\n\n  message(\"  - Hardware Acceleration:\\n\")\n  if(CP2K_USE_ACCEL MATCHES \"CUDA\")\n    message(\n      \"   - CUDA:\\n\" \"     - GPU target architecture : ${CP2K_WITH_GPU}\\n\"\n      \"     - GPU architecture number : ${CP2K_ACC_ARCH_NUMBER}\\n\"\n      \"     - GPU profiling enabled :   ${CP2K_WITH_CUDA_PROFILING}\\n\\n\")\n  endif()\n\n  if(CP2K_USE_ACCEL MATCHES \"HIP\")\n    message(\"   - HIP:\\n\" \"    - GPU target architecture : ${CP2K_WITH_GPU}\\n\"\n            \"    - GPU architecture number : ${CP2K_ACC_ARCH_NUMBER}\\n\\n\")\n  endif()\n\n  message(\n    \"Note : Enabling hardware acceleration enable acceleration of the grid, pw, and dbm modules by default\\n\"\n    \"    - PW   module : ${CP2K_USE_PW_GPU}\\n\"\n    \"\\n\")\nendif()\n\nif(CP2K_USE_CUSOLVER_MP)\n  message(\n    \"   - CUSolverMP: \\n\"\n    \"     - library: ${CP2K_CUSOLVER_MP_LINK_LIBRARIES} \\n\"\n    \"     - include: ${CP2K_CUSOLVER_MP_INCLUDE_DIRS} \\n\"\n    \"     - CAL library: ${CP2K_CAL_LINK_LIBRARIES} \\n\"\n    \"     - CAL include: ${CP2K_CAL_INCLUDE_DIRS} \\n\"\n    \"     - ucc library: ${CP2K_UCC_LINK_LIBRARIES} \\n\"\n    \"     - ucx library: ${CP2K_UCX_LINK_LIBRARIES} \\n\"\n    \"     - ucc include: ${CP2K_UCC_INCLUDE_DIRS} \\n\")\nendif()\n\nif(CP2K_USE_LIBXC)\n  message(\n    \"  - LIBXC (note to package managers : libxc can be build with cmake as well)\"\n    \"   - include directories : ${CP2K_LIBXC_INCLUDE_DIRS}\\n\"\n    \"   - libraries : ${CP2K_LIBXC_LINK_LIBRARIES}\\n\\n\")\nendif()\n\nif(CP2K_USE_LIBTORCH)\n  message(\"  - LIBTORCH\\n\" \"    - libraries : ${CP2K_LIBTORCH_LIBRARIES}\\n\\n\")\nendif()\n\nif(CP2K_USE_FFTW3)\n  message(\"  - FFTW3\\n\"\n          \"    - include directories : ${CP2K_FFTW3_INCLUDE_DIRS}\\n\"\n          \"    - libraries : ${CP2K_FFTW3_LINK_LIBRARIES}\\n\\n\")\nendif()\n\nif(CP2K_USE_PLUMED)\n  message(\"  - PLUMED\\n\"\n          \"    - include directories : ${CP2K_PLUMED_INCLUDE_DIRS}\\n\"\n          \"    - libraries : ${CP2K_PLUMED_LINK_LIBRARIES}\\n\\n\")\nendif()\n\nif(CP2K_USE_LIBXSMM)\n  message(\n    \"  - libxsmm\\n\"\n    \"    - include directories : ${CP2K_LIBXSMM_INCLUDE_DIRS}\\n\"\n    \"    - libraries :           ${CP2K_LIBXSMMEXT_LINK_LIBRARIES};${CP2K_LIBXSMMF_LINK_LIBRARIES}\\n\\n\"\n  )\nendif()\n\nif(CP2K_USE_SPLA)\n  message(\" - SPLA\\n\" \"   - include directories : ${SPLA_INCLUDE_DIRS}\\n\"\n          \"   - lbraries : ${SPLA_LIBRARIES}\\n\\n\")\nendif()\n\nif(CP2K_USE_DFTD4)\n  message(\n    \" - DFTD4 :\\n\" \"   - include modules     :  ${DFTD4_DFTD4};${DFTD4_MCTC}\\n\"\n    \"   - include directories :  ${DFTD4_INCLUDE_DIR}\\n\"\n    \"   - libraries           :  ${DFTD4_LIBDIR}\\n\\n\")\nendif()\n\nif(CP2K_USE_SIRIUS)\n  message(\n    \" - SIRIUS :\\n\"\n    \"   - include directories :  ${SIRIUS_INCLUDE_DIRS}\\n\"\n    \"   - libraries           :  ${SIRIUS_LIBRARIES}\\n\"\n    \"   - dependencies :\\n\"\n    \"       - spla\\n\"\n    \"       - SpFFT\\n\"\n    \"       - SPGLIB\\n\"\n    \"       - LibXC\\n\"\n    \"       - fftw3\\n\"\n    \"       - hdf5\\n\"\n    \"       - GSL\\n\\n\")\n\n  if(CP2K_USE_VDWXC)\n    message(\"     - VDWXC\\n\")\n  endif()\nendif()\n\nif(CP2K_USE_COSMA)\n  message(\" - COSMA\\n\" \"   - include directories : ${COSMA_INCLUDE_DIRS}\\n\"\n          \"   - libraries           : ${COSMA_LIBRARIES}\\n\\n\")\nendif()\n\nif(CP2K_USE_QUIP)\n  message(\" - QUIP\\n\"\n          \"   - include directories : ${CP2K_LIBQUIP_INCLUDE_DIRS}\\n\"\n          \"   - libraries :           ${CP2K_LIBQUIP_LINK_LIBRARIES}\\n\\n\")\nendif()\n\nif(CP2K_USE_PEXSI)\n  message(\" - PEXSI\\n\\n\")\nendif()\n\nif(CP2K_USE_LIBINT2)\n  message(\" - libint2\\n\"\n          \"   - include directories : ${CP2K_LIBINT2_INCLUDE_DIRS}\\n\"\n          \"   - libraries :           ${CP2K_LIBINT2_LINK_LIBRARIES}\\n\\n\")\nendif()\n\nif(CP2K_USE_VORI)\n  message(\" - libvori\\n\"\n          \"   - include directories : ${CP2K_LIBVORI_INCLUDE_DIRS}\\n\"\n          \"   - libraries :           ${CP2K_LIBVORI_LINK_LIBRARIES}\\n\\n\")\nendif()\n\nif(CP2K_USE_ELPA)\n  message(\" - ELPA\\n\" \"   - include directories : ${CP2K_ELPA_INCLUDE_DIRS}\\n\"\n          \"   - libraries           :  ${CP2K_ELPA_LINK_LIBRARIES}\\n\\n\")\nendif()\n\nif(CP2K_USE_SUPERLU)\n  message(\" - superlu\\n\"\n          \"   - include directories : ${CP2K_SUPERLU_INCLUDE_DIRS}\\n\"\n          \"   - libraries           : ${CP2K_SUPERLU_LINK_LIBRARIES}\\n\\n\")\nendif()\n\nmessage(\n  \"--------------------------------------------------------------------\\n\"\n  \"-                                                                  -\\n\"\n  \"-        List of dependencies not included in this build           -\\n\"\n  \"-                                                                  -\\n\"\n  \"--------------------------------------------------------------------\\n\")\n\nif(NOT CP2K_USE_MPI)\n  message(\"   - MPI\")\nendif()\n\nif(NOT CP2K_USE_DFTD4)\n  message(\"   - DFTD4\")\nendif()\n\nif(NOT CP2K_USE_SIRIUS)\n  message(\"   - SIRIUS\")\nendif()\n\nif(NOT CP2K_USE_SPGLIB)\n  message(\"   - SPGLIB\")\nendif()\n\nif(NOT CP2K_USE_COSMA)\n  message(\"   - COSMA\")\nendif()\n\nif(NOT CP2K_USE_SPLA)\n  message(\"   - SPLA\")\nendif()\n\nif(${CP2K_USE_ACCEL} MATCHES \"NONE\")\n  message(\"   - GPU acceleration is disabled\")\nendif()\n\nif(NOT CP2K_USE_ELPA)\n  message(\"   - ELPA\")\nendif()\n\nif(NOT CP2K_USE_DLAF)\n  message(\"   - DLAF\")\nendif()\n\nif(NOT CP2K_USE_PLUMED)\n  message(\"   - PLUMED\")\nendif()\n\nif(NOT CP2K_USE_QUIP)\n  message(\"   - QUIP\")\nendif()\n\nif(NOT CP2K_USE_LIBXSMM)\n  message(\"   - LIBXSMM\")\nendif()\n\nif(NOT CP2K_USE_LIBINT2)\n  message(\"   - LIBINT2\")\nendif()\n\nif(NOT CP2K_USE_LIBXC)\n  message(\"   - LIBXC\")\nendif()\n\nif(NOT CP2K_USE_VORI)\n  message(\"   - LIBVORI\")\nendif()\n\nif(NOT CP2K_USE_FFTW3)\n  message(\"   - FFTW3\")\nendif()\n\nif(NOT CP2K_USE_PEXSI)\n  message(\"   - PEXSI\")\nendif()\n\nif(NOT CP2K_USE_SUPERLU)\n  message(\"   - SUPERLU\")\nendif()\n\nif(NOT CP2K_USE_LIBTORCH)\n  message(\"   - libtorch\")\nendif()\n\nmessage(\"\\n\\n\" \"To run the regtests you need to run the following commands\\n\"\n        \"\\n\\n cd ..\\n\" \" export CP2K_DATA_DIR=${CMAKE_SOURCE_DIR}/data/\\n\"\n        \" ./tests/do_regtest.py ${cp2k_BINARY_DIR}/bin ${__cp2k_ext}\\n\\n\")\n\n# files needed for cmake\n\nwrite_basic_package_version_file(\n  \"${PROJECT_BINARY_DIR}/cp2kConfigVersion.cmake\"\n  VERSION \"${CP2K_VERSION}\"\n  COMPATIBILITY SameMajorVersion)\n\nconfigure_file(\"${PROJECT_SOURCE_DIR}/cmake/cp2kConfig.cmake.in\"\n               \"${PROJECT_BINARY_DIR}/cp2kConfig.cmake\" @ONLY)\n\ninstall(FILES \"${PROJECT_BINARY_DIR}/cp2kConfig.cmake\"\n              \"${PROJECT_BINARY_DIR}/cp2kConfigVersion.cmake\"\n        DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/cp2k\")\n\ninstall(FILES \"${PROJECT_BINARY_DIR}/libcp2k.pc\"\n        DESTINATION \"${CMAKE_INSTALL_LIBDIR}/pkgconfig\")\n\ninstall(\n  DIRECTORY \"${PROJECT_SOURCE_DIR}/cmake/modules\"\n  DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/cp2k\"\n  FILES_MATCHING\n  PATTERN \"*.cmake\")\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cryogrid",
            "repo_link": "https://github.com/CryoGrid/CryoGridCommunity_source",
            "content": {
                "codemeta": "",
                "readme": "# CryoGrid community model\n\nThis is the community version of *CryoGrid*, a numerical model to investigate land surface processes in the terrestrial cryosphere. This version of *CryoGrid* is implemented in MATLAB.\n\n*Note: This is the latest development of the CryoGrid model family. It comprises the functionalities of previous versions including [CryoGrid3](https://github.com/CryoGrid/CryoGrid3), which is no longer encouraged to be used.*\n\n## Documentation\n\nThe paper [\"The CryoGrid community model - a multi-physics toolbox for climate-driven simulations in the terrestrial cryosphere\"](https://doi.org/10.5194/gmd-16-2607-2023) in published in Geoscientific Model Development and contains a description of the model and instructions to run it (Supplements 1, 3). \n\n## Getting started\n\nBoth [CryoGridCommunity_source](https://github.com/CryoGrid/CryoGridCommunity_source) and [CryoGridCommunity_run](https://github.com/CryoGrid/CryoGridCommunity_run) are required. See [CryoGridCommunity_run](https://github.com/CryoGrid/CryoGridCommunity_run) for details.\nAn instruction video on downloading the CryoGrid community model and running simple simulations is available here: https://www.youtube.com/watch?v=L1GIurc5_J4&t=372s\nThe parameter files and model forcing data for the simple simulations from the video can be downloaded here: http://files.artek.byg.dtu.dk/files/cryogrid/CryoGridExamples/CryoGrid_simpleExamples.zip \n\n## Get involved\n\nThere is an [email group](https://groups.google.com/g/cryogrid) for news, (high-level) discussions and invitation to the annual hackathon, as well as a [slack](https://join.slack.com/t/cryogrid/shared_invite/zt-2487oq3o5-ghOQCw2rLimIk13xft27ZQ) for debugging, (low-level) discussions, community building and other things CryoGrid-related!\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/crystfel",
            "repo_link": "https://gitlab.desy.de/thomas.white/crystfel/",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cuas-mpi",
            "repo_link": "https://github.com/tudasc/CUAS-MPI",
            "content": {
                "codemeta": "",
                "readme": "# CUAS-MPI &middot; [![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n## What is CUAS-MPI?\nCUAS-MPI is an MPI parallel implementation of the Confined-Unconfined Aquifer System model ([CUAS18](#ref-CUAS-2018)) for subglacial hydrology.\nThe implementation relies on the parallel data structures and linear system solvers provided by the Portable, Extensible Toolkit for Scientific Computation ([PETSc](https://petsc.org/)).\nThe model uses a one-layer equivalent porous medium (EPM) approach for efficient (channel-like) and inefficient (cavity-like) subglacial water transport.\nA two-dimensional Darcy-type groundwater flow equation with spatially and temporarily varying hydraulic transmissivity is solved considering confined and unconfined aquifer conditions.\n\n## How to use it?\n\nOne of the integration tests can be used to generate a simple setup to explore the modelling choices and command line options.\nThe example below needs `ncks` and `ncap2` from the [NCO toolkit](https://nco.sourceforge.net/) to manipulate the NetCDF files.\n\n```\n# modifiy according to your installation\nCUAS_BUILD_DIR=$CUAS_ROOT/CUAS-MPI/cmake-build-debug/\n\n# number of MPI processes\nNN=4\n\n#\n# generate simple input file from example integration test\n#\nexact=$CUAS_BUILD_DIR/test/cuascore/integration/test-exactSteadySolution\nmpirun -n $NN $exact 1000.0 101 101 31 86400 out0.nc\n# convert output to CUAS-MPI input and forcing file format\nncks -O -d time,-1 -v topg,bnd_mask,thk out0.nc input.nc\nncap2 -O -s \"bmelt=watersource * 365*24*3600\" out0.nc forcing.nc\n\n# run a simple experiment \n#\ncuas=$CUAS_BUILD_DIR/tools/cuas.exe\n\n# set-up the solver\nTOL=\"-ksp_rtol 1e-7 -ksp_atol 1e-15 -ksp_max_it 10000 -ksp_converged_use_min_initial_residual_norm\"\nexport PETSC_OPTIONS=\"-options_left -ksp_initial_guess_nonzero -pc_type bjacobi -ksp_type gmres $TOL\"\n\n# make use of many options for this example\nmpirun -n $NN  $cuas --totaltime '15 days' --dt '1 hour' --saveEvery 1 --verbose --outputSize large \\\n       --doChannels --Tmax 100  --Tmin 1.e-08 --initialHead Nzero  $opts \\\n       --conductivity 10 --layerThickness 0.1 \\\n       --flowConstant 3.4e-24 --cavityBeta 5e-4 --basalVelocityIce 1e-6 --supplyMultiplier 1.0 \\\n       --forcingFile forcing.nc  \\\n       input.nc output.nc\n```\n\n## How to install?\n\n### Requirements\n\n- c++ compiler\n  - at least support for c++ 17\n  - the code is tested using gcc/10.2.0\n- MPI\n  - we use OpenMPI\n    - https://www.open-mpi.org/\n    - version 4.0.x and 4.1.x\n- PETSc\n  - https://petsc.org/\n  - we tested version 3.14.6\n  - with MPI support\n- netcdf-c\n  - https://www.unidata.ucar.edu/software/netcdf/\n  - we use version 4.7.4\n  - with MPI and hdf5 1.8.22\n\n### Build\n\nStarting from the CUAS-MPI directory:\n```\ncmake -B build -DCMAKE_BUILD_TYPE=Release -DPETSC_DIR=<petsc-root-directory> -DNETCDF_DIR=<netcdf-root-directory>\ncmake --build build\ncmake --install build --prefix <prefix>\n```\nYou may want to use legacy cmake calls to generate Makefiles and build CUAS-MPI:\n```\nmkdir build\ncd build\ncmake .. -DCMAKE_BUILD_TYPE=Release -DPETSC_DIR=<petsc-root-directory> -DNETCDF_DIR=<netcdf-root-directory> -DCMAKE_INSTALL_PREFIX=<prefix>\nmake\nmake install\n```\n\nCUAS-MPI makes use of the following options:\n| Option | Default | Description                                                                                                  |\n| --- | :---: |--------------------------------------------------------------------------------------------------------------|\n| `CUAS_ENABLE_TESTS` | `OFF` | Enables targets building tests. |\n| `CUAS_ENABLE_DOCS` | `OFF` | Enables targets building documentation. |\n\n## References\n\n<table style=\"border:0px\">\n<tr>\n    <td valign=\"top\"><a name=\"ref-CUAS-2018\"></a>[CUAS18]</td>\n    <td>Beyer, Sebastian and Kleiner, Thomas and Aizinger, Vadym and Rückamp, Martin and Humbert, Angelika\n    <a href=https://doi.org/10.5194/tc-12-3931-2018>\n    A confined–unconfined aquifer model for subglacial hydrology and its application to the Northeast Greenland Ice Stream</a>.\n    In <i>The Cryosphere</i>, pages 3931–3947, 2018.</td>\n</tr>\n<tr>\n    <td valign=\"top\"><a name=\"ref-CUAS-2023\"></a>[CUAS23]</td>\n    <td>Fischler, Yannic and Kleiner, Thomas and Bischof, Christian and Schmiedel, Jeremie and Sayag, Roiy and\n        Emunds, Raban and Oestreich, Lennart Frederik and Humbert, Angelika\n    <a href=https://doi.org/10.5194/gmd-16-5305-2023>\n    A parallel implementation of the confined–unconfined aquifer system model for subglacial hydrology: design,\n    verification, and performance analysis (CUAS-MPI v0.1.0) </a>.\n    In <i>Geoscientific Model Development</i>, pages 5305-5322, 2023.</td>\n</tr>\n</table>\n\n## CUAS-MPI Applications\n\n- Wolovick, Micheal and Humbert, Angelika and Kleiner, Thomas and Rückamp, Martin\n  [Regularization and L-curves in ice sheet inverse models: a case study in the Filchner-Ronne catchment](https://doi.org/10.5194/tc-17-5027-2023),\n  <i>The Cryosphere</i>, vol. 17, no. 12, pages 5027–5060, 2023.\n\n",
                "dependencies": "cmake_minimum_required(VERSION 3.16)\n\nproject(\n  CUAS-MPI\n  VERSION 0.2.0\n  LANGUAGES CXX)\n\nlist(APPEND CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)\n\nfind_package(PETSc REQUIRED)\nfind_package(NetCDF REQUIRED)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\ninclude(ToolchainOptions)\n\nadd_subdirectory(lib)\nadd_subdirectory(tools)\n\n# The default options cache\noption(CUAS_ENABLE_TESTS \"Enables targets building tests.\" OFF)\noption(CUAS_ENABLE_DOCS \"Enables targets building documentation.\" OFF)\n\nif(CUAS_ENABLE_TESTS)\n  message(STATUS \"creating targets to build tests is enabled\")\n  add_subdirectory(test)\nendif()\n\nif(CUAS_ENABLE_DOCS)\n  message(STATUS \"creating targets to build documentation is enabled\")\n  add_subdirectory(docs)\nendif()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cubegui",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/perftools/cubegui",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/cubelib",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/perftools/cubelib",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/cubew",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/perftools/cubew",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/cudamemtest",
            "repo_link": "https://github.com/ComputationalRadiationPhysics/cuda_memtest",
            "content": {
                "codemeta": "",
                "readme": "# cuda_memtest\n\nThis software tests GPU memory for hardware errors and soft errors using CUDA (or OpenCL).\n\n## Note for this Fork\n\nThis is a fork of the original, yet long-time unmaintained project at https://sourceforge.net/projects/cudagpumemtest/ .\n\nAfter our fork in 2013 (v1.2.3), we primarily focused on support for newer CUDA versions and support of newer Nvidia hardware.\nPull-requests maintaining the OpenCL versions are nevertheless still welcome.\n\n## License\n\nIllinois Open Source License\n\nUniversity of Illinois/NCSA  \nOpen Source License\n\nCopyright 2009-2012,    University of Illinois.  All rights reserved.  \nCopyright 2013-2019,    The developers of PIConGPU at Helmholtz-Zentrum Dresden-Rossendorf\n\nDeveloped by:\n\n  Innovative Systems Lab  \n  National Center for Supercomputing Applications  \n  http://www.ncsa.uiuc.edu/AboutUs/Directorates/ISL.html\n\nForked and maintained for newer Nvidia GPUs since 2013 by:\n\n  Axel Huebl and Rene Widera  \n  Computational Radiation Physics Group  \n  Helmholtz-Zentrum Dresden-Rossendorf  \n  https://www.hzdr.de/crp\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of \nthis software and associated documentation files (the \"Software\"), to deal with \nthe Software without restriction, including without limitation the rights to use,\ncopy, modify, merge, publish, distribute, sublicense, and/or sell copies of the \nSoftware, and to permit persons to whom the Software is furnished to do so, subject\nto the following conditions:\n\n* Redistributions of source code must retain the above copyright notice, this list \n  of conditions and the following disclaimers.\n\n* Redistributions in binary form must reproduce the above copyright notice, this list\n  of conditions and the following disclaimers in the documentation and/or other materials\n  provided with the distribution.\n\n* Neither the names of the Innovative Systems Lab, the National Center for Supercomputing\n  Applications, nor the names of its contributors may be used to endorse or promote products\n  derived from this Software without specific prior written permission.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, \nINCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR\nPURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT \nOR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER \nDEALINGS WITH THE SOFTWARE.\n\n## Compile and Run\n\n### Compile for CUDA\n\nInside the source directory, run:\n```bash\nmkdir build\ncd build\n# build for NVIDIA architecture sm_35\ncmake -DCMAKE_CUDA_ARCHITECTURES=35 .. \nmake\n```\n\n### Compile for HIP\n\nInside the source directory, run:\n```bash\nmkdir build\ncd build\n# build for NVIDIA architecture MI2XX\ncmake -DCUDA_MEMTEST_BACKEND=hip -DGPU_TARGETS=gfx90a ..\nmake\n```\n\nNote: \n  - In CMake, `..` is the path to the source directory.\n  - You can find the architecture for your NVIDIA GPU on [this site](https://developer.nvidia.com/cuda-gpus).\n\nWe also provide the package `cuda-memtest` in the [Spack package manager](https://spack.io) .\n\n### Run\n\n```\ncuda_memtest\n```\nThe default behavior is running the test on all the GPUs available infinitely.\nThere are options to change the default behavior. \n\n```\ncuda_memtest --disable_all --enable_test 10\ncuda_memtest --stress\n```\nThis runs test 10 (the stress test). `--stress` is equivalent to `--disable_all --enable_test 10 --exit_on_error`\n\n```\ncuda_memtest --stress --num_iterations 100 --num_passes 1\n```\nThis one does a quick sanity check for GPUs with a short run of test 10. More on this later.\n\nSee help message by \n\n```\ncuda_memtest --help\n```\n\n### Sanity Check\n\nThere is a simple script `sanity_check.sh` in the directory. \nThis script does a quick check if one GPU or all GPUs are in bad health.\n\nExample usage: \n```bash\n# copy the cuda_memtest binary first into the same location as this script, e.g.\ncd ..\nmv build/cuda_memtest .\n```\n```\n./sanity_check.sh 0   //check GPU 0\n./sanity_check.sh 1   //check GPU 1 \n./sanity_check.sh     //check All GPUs in the system\n```\n\nFork note: We just run the `cuda_memtest` binary directly.\nConsider this script as a source for inspiration, or so.\n\n### Known Issues\n\n* Even if you compile with AMD HIP the tool binary will be named `cuda_memtest`. \n\n* If you run on AMD GPUs via HIP the tool will mention everywhere CUDA instead of HIP.\n\n* We are **not** maintaining the OpenCL version of this code base.\n  Pull requests restoring and updating the OpenCL capabilities are welcome.\n\n## Test Descriptions\n\n### List of all Tests\n\nRunning \n```\ncuda_memtest --list_tests\n```\nwill print out all tests and their short descriptions, as of 6/18/2009, we implemented 11 tests\n\n```\nTest0 [Walking 1 bit] \nTest1 [Own address test] \nTest2 [Moving inversions, ones&zeros] \nTest3 [Moving inversions, 8 bit pat] \nTest4 [Moving inversions, random pattern] \nTest5 [Block move, 64 moves] \nTest6 [Moving inversions, 32 bit pat] \nTest7 [Random number sequence] \nTest8 [Modulo 20, random pattern] \nTest9 [Bit fade test]  ==disabled by default==\nTest10 [Memory stress test] \n```\n\n### The General Algorithm\n\nFirst a kernel is launched to write a pattern.\nThen we exit the kernel so that the memory can be flushed. Then we start a new kernel to read\nand check if the value matches the pattern. An error is recorded if it does not match for each \nmemory location. In the same kernel, the complement of the pattern is written after the checking. \nThe third kernel is launched to read the value again and checks against the complement of the pattern. \n\n### Detailed Description\n\nTest 0 `[Walking 1 bit]`  \n\tThis test changes one bit a time in memory address to see it\n\tgoes to a different memory location. It is designed to test\n\tthe address wires. \n\nTest 1 `[Own address test]`  \n\tEach Memory location is filled with its own address. The next kernel checks if the \n\tvalue in each memory location still agrees with the address.\n\nTest 2 `[Moving inversions, ones&zeros]`  \n\tThis test uses the moving inversions algorithm with patterns of all\n\tones and zeros. \n\nTest 3 `[Moving inversions, 8 bit pat]`  \n\tThis is the same as test 1 but uses a 8 bit wide pattern of\n\t\"walking\" ones and zeros.  This test will better detect subtle errors\n\tin \"wide\" memory chips. \n\nTest 4 `[Moving inversions, random pattern]`  \n\tTest 4 uses the same algorithm as test 1 but the data pattern is a\n\trandom number and it's complement. This test is particularly effective\n\tin finding difficult to detect data sensitive errors. The random number \n\tsequence is different with each pass so multiple passes increase effectiveness.\n\nTest 5 `[Block move, 64 moves]`  \n\tThis test stresses memory by moving block memories. Memory is initialized\n\twith shifting patterns that are inverted every 8 bytes.  Then blocks\n\tof memory are moved around.  After the moves\n\tare completed the data patterns are checked.  Because the data is checked\n\tonly after the memory moves are completed it is not possible to know\n\twhere the error occurred.  The addresses reported are only for where the\n\tbad pattern was found.\n\nTest 6 `[Moving inversions, 32 bit pat]`  \n\tThis is a variation of the moving inversions algorithm that shifts the data\n\tpattern left one bit for each successive address. The starting bit position\n\tis shifted left for each pass. To use all possible data patterns 32 passes\n\tare required.  This test is quite effective at detecting data sensitive\n\terrors but the execution time is long.\n\nTest 7 `[Random number sequence]`  \n\tThis test writes a series of random numbers into memory.  A block (1 MB) of memory\n\tis initialized with random patterns. These patterns and their complements are\n\tused in moving inversions test with rest of memory.\n\nTest 8 `[Modulo 20, random pattern]`  \n\tA random pattern is generated. This pattern is used to set every 20th memory location\n\tin memory. The rest of the memory location is set to the complement of the pattern.\n\tRepeat this for 20 times and each time the memory location to set the pattern is shifted right.\n\nTest 9 `[Bit fade test, 90 min, 2 patterns]`  \n\tThe bit fade test initializes all of memory with a pattern and then\n\tsleeps for 90 minutes. Then memory is examined to see if any memory bits\n\thave changed. All ones and all zero patterns are used. This test takes\n\t3 hours to complete. The Bit Fade test is disabled by default\n\nTest 10 `[memory stress test]`  \n\tStress memory as much as we can. A random pattern is generated and a kernel of large grid size\n\tand block size is launched to set all memory to the pattern. A new read and write kernel is launched\n\timmediately after the previous write kernel to check if there is any errors in memory and set the\n\tmemory to the complement. This process is repeated for 1000 times for one pattern. The kernel is \n\twritten as to achieve the maximum bandwidth between the global memory and GPU.\n\tThis will increase the chance of catching software error. In practice, we found this test quite useful \n\tto flush hardware errors as well.\n\n",
                "dependencies": "################################################################################\n# Required cmake version\n################################################################################\n\ncmake_minimum_required(VERSION 3.16.0)\n\n################################################################################\n# Project\n################################################################################\n\ninclude(CheckLanguage) # check for CUDA/HIP language support\n\nproject(CUDA_memtest LANGUAGES CXX)\n\nif(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)\n    set(CMAKE_INSTALL_PREFIX \"${CMAKE_BINARY_DIR}\" CACHE PATH \"install prefix\" FORCE)\nendif(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)\n\n# own modules for find_packages\nset(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ${CMAKE_CURRENT_SOURCE_DIR}/cmake/)\n\nset(CUDA_MEMTEST_BACKEND \"cuda\" CACHE STRING \"Select the backend API used for the test.\")\nset_property(CACHE CUDA_MEMTEST_BACKEND PROPERTY STRINGS \"cuda;hip\")\n\noption(CUDA_MEMTEST_ADD_RPATH \"Add RPATH's to binaries.\" ON)\n\n################################################################################\n# CMake policies\n#\n# Search in <PackageName>_ROOT:\n#   https://cmake.org/cmake/help/v3.12/policy/CMP0074.html\n################################################################################\n\nif(POLICY CMP0074)\n    cmake_policy(SET CMP0074 NEW)\nendif()\n\n\nif(CUDA_MEMTEST_BACKEND STREQUAL \"cuda\")\n    ################################################################################\n    # Find CUDA\n    ################################################################################\n\n    enable_language(CUDA)\n    # cuda toolkit contains CUDA::nvml\n    find_package(CUDAToolkit)\n\n    if(SAME_NVCC_FLAGS_IN_SUBPROJECTS)\n        if(CUDA_SHOW_CODELINES)\n            target_compile_options(cuda_memtest PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:--source-in-ptx -lineinfo>)\n            set(CUDA_KEEP_FILES ON CACHE BOOL \"activate keep files\" FORCE)\n        endif(CUDA_SHOW_CODELINES)\n\n        if(CUDA_SHOW_REGISTER)\n            target_compile_options(cuda_memtest PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:-Xcuda-ptxas=-v>)\n        endif()\n\n        if(CUDA_KEEP_FILES)\n            target_compile_options(cuda_memtest PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:--keep>)\n        endif()\n    endif()\n\n    ################################################################################\n    # Find NVML\n    ################################################################################\n\n    set(GPU_DEPLOYMENT_KIT_ROOT_DIR \"$ENV{GDK_ROOT}\")\n    find_package(NVML)\nelse()\n    ################################################################################\n    # Find HIP\n    ################################################################################\n    # supported HIP version range\n    check_language(HIP)\n\n    if(CMAKE_HIP_COMPILER)\n        enable_language(HIP)\n        find_package(hip REQUIRED)\n\n        set(CUDA_MEMTEST_HIP_MIN_VER 5.2)\n        set(CUDA_MEMTEST_HIP_MAX_VER 6.1)\n\n        # construct hip version only with major and minor level\n        # cannot use hip_VERSION because of the patch level\n        # 6.0 is smaller than 6.0.1234, so CUDA_MEMTEST_HIP_MAX_VER would have to be defined with a large patch level or\n        # the next minor level, e.g. 6.1, would have to be used.\n        set(_hip_MAJOR_MINOR_VERSION \"${hip_VERSION_MAJOR}.${hip_VERSION_MINOR}\")\n\n        if(${_hip_MAJOR_MINOR_VERSION} VERSION_LESS ${CUDA_MEMTEST_HIP_MIN_VER} OR ${_hip_MAJOR_MINOR_VERSION} VERSION_GREATER ${CUDA_MEMTEST_HIP_MAX_VER})\n            message(WARNING \"HIP ${_hip_MAJOR_MINOR_VERSION} is not official supported by cuda_memtest. Supported versions: ${CUDA_MEMTEST_HIP_MIN_VER} - ${CUDA_MEMTEST_HIP_MAX_VER}\")\n        endif()\n\n    else()\n        message(FATAL_ERROR \"Optional alpaka dependency HIP could not be found!\")\n    endif()\nendif()\n\n\n################################################################################\n# Find PThreads\n################################################################################\n\nif(NOT MSVC)\n    set(THREADS_PREFER_PTHREAD_FLAG TRUE)\nendif()\n\nfind_package(Threads REQUIRED)\n\n\n################################################################################\n# Warnings\n################################################################################\n\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall\")\n\n\n################################################################################\n# Compile & Link cuda_memtest\n################################################################################\n\nif(CUDA_MEMTEST_BACKEND STREQUAL \"cuda\")\n    set_source_files_properties(tests.cpp cuda_memtest.cpp misc.cpp PROPERTIES LANGUAGE CUDA)\nendif()\n\nadd_executable(cuda_memtest\n        tests.cpp\n        misc.cpp\n        cuda_memtest.cpp\n        )\n\nif(CUDA_MEMTEST_BACKEND STREQUAL \"cuda\")\n    target_link_libraries(cuda_memtest INTERFACE CUDA::cudart)\n    target_link_libraries(cuda_memtest INTERFACE CUDA::cuda_driver)\n\n    if(TARGET CUDA::nvml)\n        message(STATUS \"nvml found\")\n        target_link_libraries(cuda_memtest PRIVATE CUDA::nvml)\n        target_compile_definitions(cuda_memtest PRIVATE \"ENABLE_NVML=1\")\n    else()\n        target_compile_definitions(cuda_memtest PRIVATE \"ENABLE_NVML=0\")\n    endif()\n\n    if(NOT CUDA_MEMTEST_RELEASE)\n        set(CMAKE_BUILD_TYPE Debug)\n    endif()\nelse()\n    target_link_libraries(cuda_memtest PRIVATE hip::host)\n    target_link_libraries(cuda_memtest PRIVATE hip::device)\n    target_compile_definitions(cuda_memtest PRIVATE \"$<$<COMPILE_LANGUAGE:CXX>:__HIP_PLATFORM_AMD__>\")\n    target_compile_definitions(cuda_memtest PRIVATE \"ENABLE_NVML=0\")\nendif()\n\nif(NOT MSVC)\n    target_link_libraries(cuda_memtest PRIVATE Threads::Threads)\nendif()\n\n## annotate with RPATH's\nif(CUDA_MEMTEST_ADD_RPATH)\n    if(NOT DEFINED CMAKE_INSTALL_RPATH)\n        if(APPLE)\n            set_target_properties(cuda_memtest PROPERTIES INSTALL_RPATH \"@loader_path\")\n        elseif(CMAKE_SYSTEM_NAME MATCHES \"Linux\")\n            set_target_properties(cuda_memtest PROPERTIES INSTALL_RPATH \"$ORIGIN\")\n        endif()\n    endif()\n    if(NOT DEFINED CMAKE_INSTALL_RPATH_USE_LINK_PATH)\n        set_target_properties(cuda_memtest PROPERTIES INSTALL_RPATH_USE_LINK_PATH ON)\n    endif()\nendif()\n\n################################################################################\n# Build type (debug, release)\n################################################################################\n\noption(CUDA_MEMTEST_RELEASE \"disable all runtime asserts\" ON)\nif(CUDA_MEMTEST_RELEASE)\n    target_compile_definitions(cuda_memtest PRIVATE NDEBUG)\nendif(CUDA_MEMTEST_RELEASE)\n\n################################################################################\n# Install cuda_memtest\n################################################################################\n\ninstall(TARGETS cuda_memtest\n        RUNTIME DESTINATION bin)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cupla",
            "repo_link": "https://github.com/alpaka-group/cupla",
            "content": {
                "codemeta": "",
                "readme": "**cupla** - C++ User interface for the Platform Independent Library alpaka\n==========================================================================\n\n[![Code Status dev](https://gitlab.com/hzdr/crp/cupla/badges/dev/pipeline.svg?key_text=dev)](https://gitlab.com/hzdr/crp/cupla/pipelines/dev/latest)\n\n![cupla Release](doc/logo/cupla_logo_320x210.png)\n\n**cupla** [[qχɑpˈlɑʔ]](https://en.wiktionary.org/wiki/Qapla%27) is a simple user\ninterface for the platform independent parallel kernel\nacceleration library\n[**alpaka**](https://github.com/alpaka-group/alpaka).\nIt follows a similar concept as the\n[NVIDIA® CUDA® API](https://developer.nvidia.com/cuda-zone) by\nproviding a software layer to manage accelerator devices.\n**alpaka** is used as backend for **cupla**.\n\nPlease keep in mind that a first, [\"find & replace\"](doc/PortingGuide.md) port\nfrom **CUDA to cupla(x86)** will result in rather bad performance. In order to\nreach decent performance on x86 systems you just need to add the **alpaka**\n[element level](doc/TuningGuide.md) to your kernels.\n\n(*Read as:* add some *tiling* to your CUDA kernels by letting the same thread\ncompute a fixed number of elements (N=4..16) instead of just computing one\n*element* per thread. Also, make the number of elements in your tiling a\n*compile-time constant* and your CUDA code (N=1) will just stay with the\nvery same performance while adding single-source performance portability for,\ne.g., x86 targets).\n\n\nSoftware License\n----------------\n\n**cupla** is licensed under **LGPLv3** or later.\n\nFor more information see [LICENSE.md](LICENSE.md).\n\n\nDependencies\n------------\n\n- **cmake 3.22.0** or higher (depends on the used alpaka version)\n- **[alpaka 1.0.0](eba6db5d8efc3c2585470085e76ba3dcab510e49)** or newer  \n  - alpaka is loaded as `git subtree` within **cupla**, see [INSTALL.md](INSTALL.md)\n\nUsage\n-----\n\n- See our notes in [INSTALL.md](INSTALL.md).\n- Checkout the [guide](doc/PortingGuide.md) how to port your project.\n- Checkout the [tuning guide](doc/TuningGuide.md) for a step further to performance\n  portable code.\n- Checkout the [interoperability guide](doc/InteroperabilityGuide.md) to learn more on\n  how to use **cupla** with software developed with an **alpaka** compatible interface.\n\n[cupla can be used as a header-only library and without the CMake build system](doc/ConfigurationHeader.md)\n\nContributing\n------------\n\nAny pull request will be reviewed by a [maintainer](https://github.com/orgs/alpaka-group/teams/alpaka-maintainers).\n\nThanks to all [active and former contributors](.rodare.json).\n\n\nTrademarks Disclaimer\n---------------------\n\nAll product names and trademarks are the property of their respective owners.\nCUDA® is a trademark of the NVIDIA Corporation.\n\n",
                "dependencies": "#\n# Copyright 2016-2021 Rene Widera, Benjamin Worpitz, Simeon Ehrig\n#\n# This file is part of cupla.\n#\n# cupla is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# cupla is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with cupla.\n# If not, see <http://www.gnu.org/licenses/>.\n#\n\n################################################################################\n# Required cmake version.\n################################################################################\n\ncmake_minimum_required(VERSION 3.22.0)\n\n################################################################################\n# Setup project information\n################################################################################\n\n# Find cupla version.\nfile(STRINGS \"${CMAKE_CURRENT_LIST_DIR}/include/cupla/version.hpp\" CUPLA_VERSION_MAJOR_HPP REGEX \"#define CUPLA_VERSION_MAJOR \")\nfile(STRINGS \"${CMAKE_CURRENT_LIST_DIR}/include/cupla/version.hpp\" CUPLA_VERSION_MINOR_HPP REGEX \"#define CUPLA_VERSION_MINOR \")\nfile(STRINGS \"${CMAKE_CURRENT_LIST_DIR}/include/cupla/version.hpp\" CUPLA_VERSION_PATCH_HPP REGEX \"#define CUPLA_VERSION_PATCH \")\n\nstring(REGEX MATCH \"([0-9]+)\" CUPLA_VERSION_MAJOR  ${CUPLA_VERSION_MAJOR_HPP})\nstring(REGEX MATCH \"([0-9]+)\" CUPLA_VERSION_MINOR  ${CUPLA_VERSION_MINOR_HPP})\nstring(REGEX MATCH \"([0-9]+)\" CUPLA_VERSION_PATCH  ${CUPLA_VERSION_PATCH_HPP})\n\nset(PACKAGE_VERSION \"${CUPLA_VERSION_MAJOR}.${CUPLA_VERSION_MINOR}.${CUPLA_VERSION_PATCH}\")\n\n\nproject(cupla VERSION      ${CUPLA_VERSION_MAJOR}.${CUPLA_VERSION_MINOR}.${CUPLA_VERSION_PATCH}\n              DESCRIPTION  \"cupla is a simple CUDA like user interface for the platform independent parallel kernel acceleration library alpaka.\"\n              HOMEPAGE_URL \"https://github.com/alpaka-group/cupla\"\n              LANGUAGES    CXX)\ninclude(GNUInstallDirs)\n\n################################################################################\n# cupla options\n################################################################################\n\noption(CUPLA_STREAM_ASYNC_ENABLE \"Enable asynchronous streams\" ON)\noption(cupla_BUILD_EXAMPLES \"Build examples\" OFF)\n\n################################################################################\n# setup alpaka\n################################################################################\n\n# the min and max. supported alpaka version is also copied to the cuplaConfig.cmake\nset(_CUPLA_MIN_ALPAKA_VERSION 1.0.0)\nset(_CUPLA_MAX_ALPAKA_VERSION 1.1.0)\n\n# do not search for alpaka if it already exists\n# for example, a project that includes alpaka via add_subdirectory before including cupla via add_subdirectory\nif(NOT TARGET alpaka::alpaka)\n  # the alpaka provider for the internal alpaka is only available,\n  # if cupla is used via add_subdirectory in another project\n  # or examples are build\n  if(cupla_BUILD_EXAMPLES OR (NOT ${CMAKE_PROJECT_NAME} STREQUAL ${PROJECT_NAME}))\n    set(cupla_ALPAKA_PROVIDER \"internal\" CACHE STRING \"Select which alpaka is used\")\n    set_property(CACHE cupla_ALPAKA_PROVIDER PROPERTY STRINGS \"internal;external\")\n    mark_as_advanced(cupla_ALPAKA_PROVIDER)\n\n    if(${cupla_ALPAKA_PROVIDER} STREQUAL \"internal\")\n      set(alpaka_BUILD_EXAMPLES OFF)\n      set(BUILD_TESTING OFF)\n      add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/alpaka)\n    else()\n      find_package(alpaka ${_CUPLA_MAX_ALPAKA_VERSION} HINTS $ENV{ALPAKA_ROOT})\n      if(NOT TARGET alpaka::alpaka)\n        message(STATUS \"Could not find alpaka ${_CUPLA_MAX_ALPAKA_VERSION}. Now searching for alpaka ${_CUPLA_MIN_ALPAKA_VERSION}\")\n        find_package(alpaka ${_CUPLA_MIN_ALPAKA_VERSION} REQUIRED HINTS $ENV{ALPAKA_ROOT})\n      endif()\n      if(alpaka_VERSION VERSION_GREATER _CUPLA_MAX_ALPAKA_VERSION)\n        message(WARNING \"Unsupported alpaka version ${alpaka_VERSION}. \"\n          \"Supported versions [${_CUPLA_MIN_ALPAKA_VERSION},${_CUPLA_MAX_ALPAKA_VERSION}].\")\n      endif()\n    endif()\n\n    if(NOT TARGET alpaka::alpaka)\n      message(FATAL_ERROR \"Required cupla dependency alpaka could not be found!\")\n    endif()\n  endif()\nendif()\n\n################################################################################\n# cupla Target.\n################################################################################\n\n# create cupla target only if the cupla is used via add_subdirectory\n# or examples are build\n# for the explanation please have a look in the cuplaConfig.cmake.in\nif(cupla_BUILD_EXAMPLES OR (NOT ${CMAKE_PROJECT_NAME} STREQUAL ${PROJECT_NAME}))\n  include(\"${CMAKE_CURRENT_LIST_DIR}/cmake/addExecutable.cmake\")\n  include(\"${CMAKE_CURRENT_LIST_DIR}/cmake/cuplaTargetHelper.cmake\")\n\n  # export HIP_HIPCC_FLAGS to the parent scope else the variable is not visible\n  # for application files\n  if(HIP_HIPCC_FLAGS)\n    set(HIP_HIPCC_FLAGS ${HIP_HIPCC_FLAGS} PARENT_SCOPE)\n  endif()\n\n  createCuplaTarget(${PROJECT_NAME}\n    ${PROJECT_SOURCE_DIR}/include # include directory path\n    ${PROJECT_SOURCE_DIR}/src # src directory path\n    )\nendif()\n\n\n################################################################################\n# add examples\n################################################################################\n\nif(cupla_BUILD_EXAMPLES)\n  add_subdirectory(example/)\nendif()\n\n################################################################################\n# install cupla\n################################################################################\n\nif(${CMAKE_PROJECT_NAME} STREQUAL ${PROJECT_NAME})\n  include(CMakePackageConfigHelpers)\n\n  set(_CUPLA_INSTALL_CMAKEDIR \"${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\")\n  set(_CUPLA_SOURCE_CMAKEDIR \"${PROJECT_SOURCE_DIR}/cmake/\")\n\n  write_basic_package_version_file(\"${PROJECT_NAME}ConfigVersion.cmake\"\n    VERSION ${PROJECT_VERSION}\n    COMPATIBILITY SameMajorVersion)\n\n  configure_package_config_file(\n    \"${_CUPLA_SOURCE_CMAKEDIR}/${PROJECT_NAME}Config.cmake.in\"\n    \"${PROJECT_BINARY_DIR}/${PROJECT_NAME}Config.cmake\"\n    INSTALL_DESTINATION ${_CUPLA_INSTALL_CMAKEDIR}\n    PATH_VARS _CUPLA_MIN_ALPAKA_VERSION _CUPLA_MAX_ALPAKA_VERSION)\n\n  install(FILES \"${PROJECT_BINARY_DIR}/${PROJECT_NAME}Config.cmake\"\n    \"${PROJECT_BINARY_DIR}/${PROJECT_NAME}ConfigVersion.cmake\"\n    DESTINATION ${_CUPLA_INSTALL_CMAKEDIR})\n\n  install(FILES \"${_CUPLA_SOURCE_CMAKEDIR}/addExecutable.cmake\"\n    \"${_CUPLA_SOURCE_CMAKEDIR}/cuplaTargetHelper.cmake\"\n    DESTINATION ${_CUPLA_INSTALL_CMAKEDIR})\n\n  install(DIRECTORY ${PROJECT_SOURCE_DIR}/include/ DESTINATION include)\n  # copy source files instead compiled library\n  # this is necessary because some functions use the ACC as a template parameter,\n  # but the ACC is not defined at the install time of cupla\n  install(DIRECTORY ${PROJECT_SOURCE_DIR}/src/ DESTINATION src/cupla)\nendif()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/damnit",
            "repo_link": "https://github.com/European-XFEL/DAMNIT",
            "content": {
                "codemeta": "",
                "readme": "# DAMNIT\n\n[![Documentation Status](https://readthedocs.org/projects/damnit/badge/?version=latest)](https://damnit.readthedocs.io/en/latest/?badge=latest)\n[![codecov](https://codecov.io/gh/European-XFEL/DAMNIT/graph/badge.svg?token=NGwo3ShLNw)](https://codecov.io/gh/European-XFEL/DAMNIT)\n\nDAMNIT is a tool developed at the European XFEL to help users create an\nautomated overview of their experiment. Check out the documentation for more\ninformation: https://damnit.rtfd.io\n\n",
                "dependencies": "[build-system]\nrequires = [\"flit_core >=3.2,<4\"]\nbuild-backend = \"flit_core.buildapi\"\n\n[project]\nname = \"damnit\"\nauthors = [\n    {name = \"Thomas Kluyver\", email = \"thomas.kluyver@xfel.eu\"},\n    {name = \"Luca Gelisio\", email = \"luca.gelisio@xfel.eu\"},\n]\ndescription = \"The Data And Metadata iNspection Interactive Thing\"\nlicense = {file = \"LICENSE\"}\nrequires-python = \">=3.9\"\nclassifiers = [\"License :: OSI Approved :: BSD License\"]\ndynamic = [\"version\"]\nreadme = \"README.md\"\ndependencies = [\n    \"h5netcdf\",\n    \"h5py\",\n    \"orjson\",  # used in plotly for faster json serialization\n    \"pandas\",\n    \"plotly\",\n    \"xarray\"\n]\n\n[project.optional-dependencies]\nbackend = [\n    \"EXtra-data\",\n    \"ipython\",\n    \"kafka-python-ng\",\n    \"kaleido\",  # used in plotly to convert figures to images\n    \"matplotlib\",\n    \"numpy\",\n    \"pyyaml\",\n    \"requests\",\n    \"supervisor\",\n    \"termcolor\"\n]\ngui = [\n    \"adeqt\",\n    \"mplcursors\",\n    \"mpl-pan-zoom\",\n    \"openpyxl\",  # for spreadsheet export\n    \"PyQt5\",\n    \"PyQtWebEngine\",\n    \"pyflakes\",  # for checking context file in editor\n    \"QScintilla==2.13\",\n    \"tabulate\",  # used in pandas to make markdown tables (for Zulip)\n]\ntest = [\n    \"pillow\",\n    \"pytest\",\n    \"pytest-qt\",\n    \"pytest-cov\",\n    \"pytest-xvfb\",\n    \"pytest-timeout\",\n    \"pytest-venv\",\n    \"testpath\",\n]\ndocs = [\n    \"mkdocs\",\n    \"mkdocs-material\",\n    \"mkdocstrings\",\n    \"mkdocstrings-python\",\n    \"pymdown-extensions\"\n]\n\n[project.urls]\nHome = \"https://github.com/European-XFEL/DAMNIT\"\n\n[project.scripts]\namore-proto = \"damnit.cli:main\"\n\n[tool.pytest.ini_options]\ntimeout = 120\nnorecursedirs = \"tests/helpers\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/dasf-messaging-python",
            "repo_link": "https://codebase.helmholtz.cloud/dasf/dasf-messaging-python",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/datadesc",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/DataDesc/",
            "content": {
                "codemeta": "",
                "readme": "<a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://raw.githubusercontent.com/OfficialCodexplosive/README_Assets/862a93188b61ab4dd0eebde3ab5daad636e129d5/FJZ_IEK-3_logo.svg\" alt=\"FZJ Logo\" width=\"300px\"></a>\n\n# DataDesc\n\nThe framework surrounding DataDesc, a metadata schema for software documentation with focus on interfaces, comes with a machine-actionable metadata exchange format and a software toolkit supporting the documentation, extraction and publication of software metadata.\n\n## Features\nDataDesc combines four distinct python software packages:\n- a form to convert general software metadata into a DataDesc compliant JSON file\n- a parser for extracting extensive metadata information from python source code to DataDesc compliant JSON\n- a tool for combining two DataDesc compliant JSON files into a single one\n- a tool for uploading DataDesc compliant files to the ORKG, GitHub, PyPI, and more\n\n## Installation\nThe framework can be installed directly via git - this will preserve the connection to the GitHub repository:\n```bash\n\tgit clone https://github.com/FZJ-IEK3-VSA/DataDesc\n```\n\nMost parts of this framework are out-of-the-box solutions which do not need to be installed. For others, a proper installation and usage instruction is provided alongside the files themselves.\n\n## License\n\nMIT License\n\nCopyright (c) 2024 Patrick Kuckertz (FZJ/IEK-3), Jan-Maris Göpfert (FZJ/IEK-3), Oliver Karras (TIB), David Neuroth (FZJ/IEK-3), Julian Schönau (FZJ/IEK-3), Rodrigo Pueblas (FZJ/IEK-3), Stephan Ferenz (University of Oldenburg/ Dept. of Computer Science), Felix Engel (TIB), Noah Pflugradt (FZJ/IEK-3), Jann Michael Weinand (FZJ/IEK-3), Leander Kotzur (FZJ/IEK-3), Astrid Nieße (University of Oldenburg/ Dept. of Computer Science), Sören Auer (TIB), Detlef Stolten (FZJ/IEK-3, RWTH)\n\nYou should have received a copy of the MIT License along with this program.\nIf not, see https://opensource.org/licenses/MIT\n\n## About Us\n\nThe [Institute of Energy and Climate Research - Techno-economic Systems Analysis (IEK-3)](https://www.fz-juelich.de/en/iek/iek-3) belongs to the [Forschungszentrum Jülich](https://www.fz-juelich.de/en). The department's interdisciplinary research is focusing on energy-related process and systems analyses. Data searches and system simulations are used to determine energy and mass balances, as well as to evaluate performance, emissions and costs of energy systems. The results are used for performing comparative assessment studies between the various systems. The current priorities include the development of energy strategies, in accordance with the German Federal Government’s greenhouse gas reduction targets, by designing new infrastructures for sustainable and secure energy supply chains and by conducting cost analysis studies for integrating new technologies into future energy market frameworks.\n\nThe [Data Science & Digtal Libraries research group](https://www.tib.eu/en/research-development/research-groups-and-labs/data-science-digital-libraries) belonging to the [TIB - Leibniz Information Centre for Science and Technology](https://www.tib.eu/en/) was established in July 2017 by the call of Prof. Dr. Sören Auer, which was jointly conducted with [Leibniz Universität Hannover](https://www.uni-hannover.de/de/). The research group serves TIB's strategic goal of improving access to and work with information, data and knowledge. The overall objective of the research group is to transform the current document-based knowledge communication in the sciences (Scholarly Communication) into knowledge-based communication (\"from papers to knowledge graphs\").\n\nThe Digitalized Energy Systems (DES) group is part of the Department of Computer Science of the University of Oldenburg. The group was established by Prof. Astrid Nieße in 2020. The group works in the field of energy informatics and focuses on distributed artificial intelligence, like agent-based systems, strategy learning in energy markets and machine learning approaches for cyber-physical energy systems (CPES).  With the high importance of data and open software in this field, research data management and how to integrate both FAIRness and industry involvement in CPES research, is a rising research area in the DES group.\n\n## Acknowledgements\nThe authors would like to thank the Federal Ministry for Economic Affairs and Energy of Germany (BMWi) for supporting this work with a grant for the project LOD-GEOSS (03EI1005B).\n\nFurthermore, the authors would like to thank the German Federal Government, the German State Governments, and the Joint Science Conference (GWK) for their funding and support as part of the NFDI4Ing consortium. Funded by the German Research Foundation (DFG) - project number: 442146713.\n\nIn addition, the work was supported by the Lower Saxony Ministry of Science and Culture within the Lower Saxony ‘‘Vorab’’ of the Volkswagen Foundation under Grant 11-76251-13-3/19–ZN3488 (ZLE), and by the Center for Digital Innovation (ZDIN).\n\nThis work was also supported by the Helmholtz Association under the program \"Energy System Design\".\n\n<a href=\"https://www.bmwk.de/Navigation/EN/Home/home.html\"><img src=\"https://www.bmwk.de/SiteGlobals/BMWI/StyleBundles/Bilder/bmwi_logo_en.svg?__blob=normal&v=13\" alt=\"BMWK Logo\" width=\"130px\"></a>\n\n",
                "dependencies": "# OpenAPI Extension Generator; General Metadata Merger\npyyaml\n\n# General Metadata Creator\njupyter\nipywidgets\n\n# ORKG Metadata Uploader\norkg\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/datalad",
            "repo_link": "https://github.com/datalad/datalad",
            "content": {
                "codemeta": "",
                "readme": "     ____            _             _                   _\n    |  _ \\    __ _  | |_    __ _  | |       __ _    __| |\n    | | | |  / _` | | __|  / _` | | |      / _` |  / _` |\n    | |_| | | (_| | | |_  | (_| | | |___  | (_| | | (_| |\n    |____/   \\__,_|  \\__|  \\__,_| |_____|  \\__,_|  \\__,_|\n                                                  Read me\n\n[![DOI](https://joss.theoj.org/papers/10.21105/joss.03262/status.svg)](https://doi.org/10.21105/joss.03262)\n[![Test Status](https://github.com/datalad/datalad/actions/workflows/test.yml/badge.svg)](https://github.com/datalad/datalad/actions/workflows/test.yml)\n[![Build status](https://ci.appveyor.com/api/projects/status/github/datalad/datalad?branch=master&svg=true)](https://ci.appveyor.com/project/mih/datalad/branch/master)\n[![Extensions](https://github.com/datalad/datalad/actions/workflows/test_extensions.yml/badge.svg)](https://github.com/datalad/datalad/actions/workflows/test_extensions.yml)\n[![Linters](https://github.com/datalad/datalad/actions/workflows/lint.yml/badge.svg)](https://github.com/datalad/datalad/actions/workflows/lint.yml)\n[![codecov.io](https://codecov.io/github/datalad/datalad/coverage.svg?branch=master)](https://codecov.io/github/datalad/datalad?branch=master)\n[![Documentation](https://readthedocs.org/projects/datalad/badge/?version=latest)](http://datalad.rtfd.org)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![GitHub release](https://img.shields.io/github/release/datalad/datalad.svg)](https://GitHub.com/datalad/datalad/releases/)\n[![Supported Python versions](https://img.shields.io/pypi/pyversions/datalad)](https://pypi.org/project/datalad/)\n[![Testimonials 4](https://img.shields.io/badge/testimonials-4-brightgreen.svg)](https://github.com/datalad/datalad/wiki/Testimonials)\n[![https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg](https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg)](https://singularity-hub.org/collections/667)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](https://github.com/datalad/datalad/blob/master/CODE_OF_CONDUCT.md)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.808846.svg)](https://doi.org/10.5281/zenodo.808846)\n[![RRID](https://img.shields.io/badge/RRID-SCR__003931-blue)](https://identifiers.org/RRID:SCR_003931)\n<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\n[![All Contributors](https://img.shields.io/badge/all_contributors-52-orange.svg?style=flat-square)](#contributors-)\n<!-- ALL-CONTRIBUTORS-BADGE:END -->\n\n## Distribution\n\n[![Anaconda](https://anaconda.org/conda-forge/datalad/badges/version.svg)](https://anaconda.org/conda-forge/datalad)\n[![Arch (AUR)](https://repology.org/badge/version-for-repo/aur/datalad.svg?header=Arch%20%28%41%55%52%29)](https://repology.org/project/datalad/versions)\n[![Debian Stable](https://badges.debian.net/badges/debian/stable/datalad/version.svg)](https://packages.debian.org/stable/datalad)\n[![Debian Unstable](https://badges.debian.net/badges/debian/unstable/datalad/version.svg)](https://packages.debian.org/unstable/datalad)\n[![Fedora Rawhide package](https://repology.org/badge/version-for-repo/fedora_rawhide/datalad.svg?header=Fedora%20%28rawhide%29)](https://repology.org/project/datalad/versions)\n[![Gentoo (::science)](https://repology.org/badge/version-for-repo/gentoo_ovl_science/datalad.svg?header=Gentoo%20%28%3A%3Ascience%29)](https://repology.org/project/datalad/versions)\n[![PyPI package](https://repology.org/badge/version-for-repo/pypi/datalad.svg?header=PyPI)](https://repology.org/project/datalad/versions)\n\n# 10000-ft. overview\n\nDataLad's purpose is to make data management and data distribution more accessible.\nTo do so, it stands on the shoulders of [Git] and [Git-annex] to deliver a\ndecentralized system for data exchange. This includes automated ingestion of\ndata from online portals and exposing it in readily usable form as Git(-annex)\nrepositories - or datasets. However, the actual data storage and permission\nmanagement remains with the original data provider(s).\n\nThe full documentation is available at http://docs.datalad.org and\nhttp://handbook.datalad.org provides a hands-on crash-course on DataLad.\n\n# Extensions\n\nA number of extensions are available that provide additional functionality for\nDataLad. Extensions are separate packages that are to be installed in addition\nto DataLad. In order to install DataLad customized for a particular domain, one\ncan simply install an extension directly, and DataLad itself will be\nautomatically installed with it. An [annotated list of\nextensions](http://handbook.datalad.org/extension_pkgs.html) is available in\nthe [DataLad handbook](http://handbook.datalad.org).\n\n\n# Support\n\nThe documentation for this project is found here:\nhttp://docs.datalad.org\n\nAll bugs, concerns, and enhancement requests for this software can be submitted here:\nhttps://github.com/datalad/datalad/issues\n\nIf you have a problem or would like to ask a question about how to use DataLad,\nplease [submit a question to\nNeuroStars.org](https://neurostars.org/new-topic?body=-%20Please%20describe%20the%20problem.%0A-%20What%20steps%20will%20reproduce%20the%20problem%3F%0A-%20What%20version%20of%20DataLad%20are%20you%20using%20%28run%20%60datalad%20--version%60%29%3F%20On%20what%20operating%20system%20%28consider%20running%20%60datalad%20plugin%20wtf%60%29%3F%0A-%20Please%20provide%20any%20additional%20information%20below.%0A-%20Have%20you%20had%20any%20luck%20using%20DataLad%20before%3F%20%28Sometimes%20we%20get%20tired%20of%20reading%20bug%20reports%20all%20day%20and%20a%20lil'%20positive%20end%20note%20does%20wonders%29&tags=datalad)\nwith a `datalad` tag.  NeuroStars.org is a platform similar to StackOverflow\nbut dedicated to neuroinformatics.\n\nAll previous DataLad questions are available here:\nhttp://neurostars.org/tags/datalad/\n\n\n# Installation\n\n## Debian-based systems\n\nOn Debian-based systems, we recommend enabling [NeuroDebian], via which we\nprovide recent releases of DataLad. Once enabled, just do:\n\n    apt-get install datalad\n\n## Gentoo-based systems\n\nOn Gentoo-based systems (i.e. all systems whose package manager can parse ebuilds as per the [Package Manager Specification]), we recommend [enabling the ::science overlay], via which we\nprovide recent releases of DataLad. Once enabled, just run:\n\n    emerge datalad\n\n## Other Linux'es via conda\n\n    conda install -c conda-forge datalad\n\nwill install the most recently released version, and release candidates are\navailable via\n\n    conda install -c conda-forge/label/rc datalad\n\n## Other Linux'es, macOS via pip\n\nBefore you install this package, please make sure that you [install a recent\nversion of git-annex](https://git-annex.branchable.com/install).  Afterwards,\ninstall the latest version of `datalad` from\n[PyPI](https://pypi.org/project/datalad). It is recommended to use\na dedicated [virtualenv](https://virtualenv.pypa.io):\n\n    # Create and enter a new virtual environment (optional)\n    virtualenv --python=python3 ~/env/datalad\n    . ~/env/datalad/bin/activate\n\n    # Install from PyPI\n    pip install datalad\n\nBy default, installation via pip installs the core functionality of DataLad,\nallowing for managing datasets etc.  Additional installation schemes\nare available, so you can request enhanced installation via\n`pip install datalad[SCHEME]`, where `SCHEME` could be:\n\n- `tests`\n     to also install dependencies used by DataLad's battery of unit tests\n- `full`\n     to install all dependencies.\n\nMore details on installation and initial configuration can be found in the\n[DataLad Handbook: Installation].\n\n# License\n\nMIT/Expat\n\n\n# Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) if you are interested in internals or\ncontributing to the project.\n\n## Acknowledgements\n\nThe DataLad project received support through the following grants:\n\n- US-German collaboration in computational neuroscience (CRCNS) project\n  \"DataGit: converging catalogues, warehouses, and deployment logistics into a\n  federated 'data distribution'\" (Halchenko/Hanke), co-funded by the US National\n  Science Foundation (NSF 1429999) and the German Federal Ministry of\n  Education and Research (BMBF 01GQ1411).\n\n- CRCNS US-German Data Sharing \"DataLad - a decentralized system for integrated\n  discovery, management, and publication of digital objects of science\"\n  (Halchenko/Pestilli/Hanke), co-funded by the US National Science Foundation\n  (NSF 1912266) and the German Federal Ministry of Education and Research\n  (BMBF 01GQ1905).\n\n- Helmholtz Research Center Jülich, FDM challenge 2022\n\n- German federal state of Saxony-Anhalt and the European Regional Development\n  Fund (ERDF), Project: Center for Behavioral Brain Sciences, Imaging Platform\n\n- ReproNim project (NIH 1P41EB019936-01A1).\n\n- Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant\n  SFB 1451 ([431549029](https://gepris.dfg.de/gepris/projekt/431549029),\n  INF project)\n\n- European Union’s Horizon 2020 research and innovation programme under grant\n  agreements:\n  - [Human Brain Project SGA3 (H2020-EU.3.1.5.3, grant no. 945539)](https://cordis.europa.eu/project/id/945539)\n  - [VirtualBrainCloud (H2020-EU.3.1.5.3, grant no. 826421)](https://cordis.europa.eu/project/id/826421)\n\nMac mini instance for development is provided by\n[MacStadium](https://www.macstadium.com/).\n\n\n### Contributors ✨\n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/glalteva\"><img src=\"https://avatars2.githubusercontent.com/u/14296143?v=4?s=100\" width=\"100px;\" alt=\"glalteva\"/><br /><sub><b>glalteva</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=glalteva\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/adswa\"><img src=\"https://avatars1.githubusercontent.com/u/29738718?v=4?s=100\" width=\"100px;\" alt=\"adswa\"/><br /><sub><b>adswa</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=adswa\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/chrhaeusler\"><img src=\"https://avatars0.githubusercontent.com/u/8115807?v=4?s=100\" width=\"100px;\" alt=\"chrhaeusler\"/><br /><sub><b>chrhaeusler</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=chrhaeusler\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/soichih\"><img src=\"https://avatars3.githubusercontent.com/u/923896?v=4?s=100\" width=\"100px;\" alt=\"soichih\"/><br /><sub><b>soichih</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=soichih\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mvdoc\"><img src=\"https://avatars1.githubusercontent.com/u/6150554?v=4?s=100\" width=\"100px;\" alt=\"mvdoc\"/><br /><sub><b>mvdoc</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=mvdoc\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mih\"><img src=\"https://avatars1.githubusercontent.com/u/136479?v=4?s=100\" width=\"100px;\" alt=\"mih\"/><br /><sub><b>mih</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=mih\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/yarikoptic\"><img src=\"https://avatars3.githubusercontent.com/u/39889?v=4?s=100\" width=\"100px;\" alt=\"yarikoptic\"/><br /><sub><b>yarikoptic</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=yarikoptic\" title=\"Code\">💻</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/loj\"><img src=\"https://avatars2.githubusercontent.com/u/15157717?v=4?s=100\" width=\"100px;\" alt=\"loj\"/><br /><sub><b>loj</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=loj\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/feilong\"><img src=\"https://avatars2.githubusercontent.com/u/2242261?v=4?s=100\" width=\"100px;\" alt=\"feilong\"/><br /><sub><b>feilong</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=feilong\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jhpoelen\"><img src=\"https://avatars2.githubusercontent.com/u/1084872?v=4?s=100\" width=\"100px;\" alt=\"jhpoelen\"/><br /><sub><b>jhpoelen</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=jhpoelen\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/andycon\"><img src=\"https://avatars1.githubusercontent.com/u/3965889?v=4?s=100\" width=\"100px;\" alt=\"andycon\"/><br /><sub><b>andycon</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=andycon\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nicholsn\"><img src=\"https://avatars3.githubusercontent.com/u/463344?v=4?s=100\" width=\"100px;\" alt=\"nicholsn\"/><br /><sub><b>nicholsn</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=nicholsn\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/adelavega\"><img src=\"https://avatars0.githubusercontent.com/u/2774448?v=4?s=100\" width=\"100px;\" alt=\"adelavega\"/><br /><sub><b>adelavega</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=adelavega\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kskyten\"><img src=\"https://avatars0.githubusercontent.com/u/4163878?v=4?s=100\" width=\"100px;\" alt=\"kskyten\"/><br /><sub><b>kskyten</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=kskyten\" title=\"Code\">💻</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/TheChymera\"><img src=\"https://avatars2.githubusercontent.com/u/950524?v=4?s=100\" width=\"100px;\" alt=\"TheChymera\"/><br /><sub><b>TheChymera</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=TheChymera\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/effigies\"><img src=\"https://avatars0.githubusercontent.com/u/83442?v=4?s=100\" width=\"100px;\" alt=\"effigies\"/><br /><sub><b>effigies</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=effigies\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jgors\"><img src=\"https://avatars1.githubusercontent.com/u/386585?v=4?s=100\" width=\"100px;\" alt=\"jgors\"/><br /><sub><b>jgors</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=jgors\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/debanjum\"><img src=\"https://avatars1.githubusercontent.com/u/6413477?v=4?s=100\" width=\"100px;\" alt=\"debanjum\"/><br /><sub><b>debanjum</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=debanjum\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nellh\"><img src=\"https://avatars3.githubusercontent.com/u/11369795?v=4?s=100\" width=\"100px;\" alt=\"nellh\"/><br /><sub><b>nellh</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=nellh\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/emdupre\"><img src=\"https://avatars3.githubusercontent.com/u/15017191?v=4?s=100\" width=\"100px;\" alt=\"emdupre\"/><br /><sub><b>emdupre</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=emdupre\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/aqw\"><img src=\"https://avatars0.githubusercontent.com/u/765557?v=4?s=100\" width=\"100px;\" alt=\"aqw\"/><br /><sub><b>aqw</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=aqw\" title=\"Code\">💻</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/vsoch\"><img src=\"https://avatars0.githubusercontent.com/u/814322?v=4?s=100\" width=\"100px;\" alt=\"vsoch\"/><br /><sub><b>vsoch</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=vsoch\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kyleam\"><img src=\"https://avatars2.githubusercontent.com/u/1297788?v=4?s=100\" width=\"100px;\" alt=\"kyleam\"/><br /><sub><b>kyleam</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=kyleam\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/driusan\"><img src=\"https://avatars0.githubusercontent.com/u/498329?v=4?s=100\" width=\"100px;\" alt=\"driusan\"/><br /><sub><b>driusan</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=driusan\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/overlake333\"><img src=\"https://avatars1.githubusercontent.com/u/28018084?v=4?s=100\" width=\"100px;\" alt=\"overlake333\"/><br /><sub><b>overlake333</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=overlake333\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/akeshavan\"><img src=\"https://avatars0.githubusercontent.com/u/972008?v=4?s=100\" width=\"100px;\" alt=\"akeshavan\"/><br /><sub><b>akeshavan</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=akeshavan\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jwodder\"><img src=\"https://avatars1.githubusercontent.com/u/98207?v=4?s=100\" width=\"100px;\" alt=\"jwodder\"/><br /><sub><b>jwodder</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=jwodder\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bpoldrack\"><img src=\"https://avatars2.githubusercontent.com/u/10498301?v=4?s=100\" width=\"100px;\" alt=\"bpoldrack\"/><br /><sub><b>bpoldrack</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=bpoldrack\" title=\"Code\">💻</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/yetanothertestuser\"><img src=\"https://avatars0.githubusercontent.com/u/19335420?v=4?s=100\" width=\"100px;\" alt=\"yetanothertestuser\"/><br /><sub><b>yetanothertestuser</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=yetanothertestuser\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/christian-monch\"><img src=\"https://avatars.githubusercontent.com/u/17925232?v=4?s=100\" width=\"100px;\" alt=\"Christian Mönch\"/><br /><sub><b>Christian Mönch</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=christian-monch\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mattcieslak\"><img src=\"https://avatars.githubusercontent.com/u/170026?v=4?s=100\" width=\"100px;\" alt=\"Matt Cieslak\"/><br /><sub><b>Matt Cieslak</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=mattcieslak\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mikapfl\"><img src=\"https://avatars.githubusercontent.com/u/7226087?v=4?s=100\" width=\"100px;\" alt=\"Mika Pflüger\"/><br /><sub><b>Mika Pflüger</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=mikapfl\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://me.ypid.de/\"><img src=\"https://avatars.githubusercontent.com/u/1301158?v=4?s=100\" width=\"100px;\" alt=\"Robin Schneider\"/><br /><sub><b>Robin Schneider</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=ypid\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://orcid.org/0000-0003-4652-3758\"><img src=\"https://avatars.githubusercontent.com/u/7570456?v=4?s=100\" width=\"100px;\" alt=\"Sin Kim\"/><br /><sub><b>Sin Kim</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=kimsin98\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/DisasterMo\"><img src=\"https://avatars.githubusercontent.com/u/49207524?v=4?s=100\" width=\"100px;\" alt=\"Michael Burgardt\"/><br /><sub><b>Michael Burgardt</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=DisasterMo\" title=\"Code\">💻</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://remi-gau.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/6961185?v=4?s=100\" width=\"100px;\" alt=\"Remi Gau\"/><br /><sub><b>Remi Gau</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=Remi-Gau\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mslw\"><img src=\"https://avatars.githubusercontent.com/u/11985212?v=4?s=100\" width=\"100px;\" alt=\"Michał Szczepanik\"/><br /><sub><b>Michał Szczepanik</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=mslw\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bpinsard\"><img src=\"https://avatars.githubusercontent.com/u/1155388?v=4?s=100\" width=\"100px;\" alt=\"Basile\"/><br /><sub><b>Basile</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=bpinsard\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/taylols\"><img src=\"https://avatars.githubusercontent.com/u/28018084?v=4?s=100\" width=\"100px;\" alt=\"Taylor Olson\"/><br /><sub><b>Taylor Olson</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=taylols\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jdkent.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/12564882?v=4?s=100\" width=\"100px;\" alt=\"James Kent\"/><br /><sub><b>James Kent</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=jdkent\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/xgui3783\"><img src=\"https://avatars.githubusercontent.com/u/19381783?v=4?s=100\" width=\"100px;\" alt=\"xgui3783\"/><br /><sub><b>xgui3783</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=xgui3783\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/tstoeter\"><img src=\"https://avatars.githubusercontent.com/u/4901704?v=4?s=100\" width=\"100px;\" alt=\"tstoeter\"/><br /><sub><b>tstoeter</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=tstoeter\" title=\"Code\">💻</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jsheunis.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/10141237?v=4?s=100\" width=\"100px;\" alt=\"Stephan Heunis\"/><br /><sub><b>Stephan Heunis</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=jsheunis\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.mmmccormick.com\"><img src=\"https://avatars.githubusercontent.com/u/25432?v=4?s=100\" width=\"100px;\" alt=\"Matt McCormick\"/><br /><sub><b>Matt McCormick</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=thewtex\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/vickychenglau\"><img src=\"https://avatars.githubusercontent.com/u/22065437?v=4?s=100\" width=\"100px;\" alt=\"Vicky C Lau\"/><br /><sub><b>Vicky C Lau</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=vickychenglau\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://chris-lamb.co.uk\"><img src=\"https://avatars.githubusercontent.com/u/133209?v=4?s=100\" width=\"100px;\" alt=\"Chris Lamb\"/><br /><sub><b>Chris Lamb</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=lamby\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/asmacdo\"><img src=\"https://avatars.githubusercontent.com/u/1028657?v=4?s=100\" width=\"100px;\" alt=\"Austin Macdonald\"/><br /><sub><b>Austin Macdonald</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=asmacdo\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://nobodyinperson.de\"><img src=\"https://avatars.githubusercontent.com/u/19148271?v=4?s=100\" width=\"100px;\" alt=\"Yann Büchau\"/><br /><sub><b>Yann Büchau</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=nobodyinperson\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/matrss\"><img src=\"https://avatars.githubusercontent.com/u/9308656?v=4?s=100\" width=\"100px;\" alt=\"Matthias Riße\"/><br /><sub><b>Matthias Riße</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=matrss\" title=\"Code\">💻</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Aksoo\"><img src=\"https://avatars.githubusercontent.com/u/141905668?v=4?s=100\" width=\"100px;\" alt=\"Aksoo\"/><br /><sub><b>Aksoo</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=Aksoo\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dguibert\"><img src=\"https://avatars.githubusercontent.com/u/1178864?v=4?s=100\" width=\"100px;\" alt=\"David Guibert\"/><br /><sub><b>David Guibert</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=dguibert\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/alliesw\"><img src=\"https://avatars.githubusercontent.com/u/72238329?v=4?s=100\" width=\"100px;\" alt=\"Alex Shields-Weber\"/><br /><sub><b>Alex Shields-Weber</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=alliesw\" title=\"Code\">💻</a></td>\n    </tr>\n  </tbody>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\n[![macstadium](https://uploads-ssl.webflow.com/5ac3c046c82724970fc60918/5c019d917bba312af7553b49_MacStadium-developerlogo.png)](https://www.macstadium.com/)\n\n[Git]: https://git-scm.com\n[Git-annex]: http://git-annex.branchable.com\n[setup.py]: https://github.com/datalad/datalad/blob/master/setup.py\n[NeuroDebian]: http://neuro.debian.net\n[Package Manager Specification]: https://projects.gentoo.org/pms/latest/pms.html\n[enabling the ::science overlay]: https://github.com/gentoo/sci#manual-install-\n\n[DataLad Handbook: Installation]: http://handbook.datalad.org/en/latest/intro/installation.html\n\n",
                "dependencies": "[build-system]\n# wheel to get more lightweight (not EASY-INSTALL) entry-points\nrequires = [\"packaging\", \"setuptools>=40.8.0\", \"wheel\"]\n\n[tool.isort]\nforce_grid_wrap = 2\ninclude_trailing_comma = true\nmulti_line_output = 3\n\n# If you want to develop, use requirements-devel.txt\n\n# Theoretically we don't want -e here but ATM pip would puke if just .[full] is provided\n# TODO -- figure it out and/or complain to pip folks\n# -e .[full]\n\n# this one should work but would copy entire . tree  so should be ran on a clean copy\n.[full]\n\n# doesn't install datalad itself\n# file://.#egg=datalad[full]\n\n#!/usr/bin/env python\n# ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\n#\n#   See COPYING file distributed along with the DataLad package for the\n#   copyright and license terms.\n#\n# ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\n\nimport sys\nfrom os.path import dirname\nfrom os.path import join as opj\n\n# This is needed for versioneer to be importable when building with PEP 517.\n# See <https://github.com/warner/python-versioneer/issues/193> and links\n# therein for more information.\nsys.path.append(dirname(__file__))\n\nimport versioneer\nfrom _datalad_build_support.setup import (\n    BuildConfigInfo,\n    BuildManPage,\n    datalad_setup,\n)\n\nrequires = {\n    'core': [\n        'platformdirs',\n        'chardet>=3.0.4',      # rarely used but small/omnipresent\n        'colorama; platform_system==\"Windows\"',\n        'distro',\n        'importlib-metadata >=3.6; python_version < \"3.10\"',\n        'iso8601',\n        'humanize',\n        'fasteners>=0.14',\n        'packaging',\n        'patool>=1.7',\n        'tqdm>=4.32.0',\n        'typing_extensions>=4.0.0; python_version < \"3.11\"',\n        'annexremote',\n        'looseversion',\n    ],\n    'downloaders': [\n        'boto3',\n        'keyring>=20.0,!=23.9.0',\n        'keyrings.alt',\n        'msgpack',\n        'requests>=1.2',\n    ],\n    'downloaders-extra': [\n        'requests_ftp',\n    ],\n    'publish': [\n        'python-gitlab',     # required for create-sibling-gitlab\n    ],\n    'misc': [\n        'argcomplete>=1.12.3',  # optional CLI completion\n        'pyperclip',         # clipboard manipulations\n        'python-dateutil',   # add support for more date formats to check_dates\n    ],\n    'tests': [\n        'BeautifulSoup4',  # VERY weak requirement, still used in one of the tests\n        'httpretty>=0.9.4',  # Introduced py 3.6 support\n        'mypy',\n        'pytest>=7.0',  # https://github.com/datalad/datalad/issues/7555\n        'pytest-cov',\n        'pytest-fail-slow~=0.2',\n        'types-python-dateutil',\n        'types-requests',\n        'vcrpy',\n    ],\n    'duecredit': [\n        'duecredit',  # needs >= 0.6.6 to be usable, but should be \"safe\" with prior ones\n    ],\n}\n\nrequires['full'] = sum(list(requires.values()), [])\n\n# Now add additional ones useful for development\nrequires.update({\n    'devel-docs': [\n        # used for converting README.md -> .rst for long_description\n        'pypandoc',\n        # Documentation\n        'sphinx>=4.3.0',\n        'sphinx-autodoc-typehints',\n        'sphinx-rtd-theme>=0.5.1',\n    ],\n    'devel-utils': [\n        'asv',        # benchmarks\n        'coverage!=7.6.5',\n        'gprof2dot',  # rendering cProfile output as a graph image\n        'psutil',\n        'pytest-xdist',  # parallelize pytest runs etc\n        # disable for now, as it pulls in ipython 6, which is PY3 only\n        #'line-profiler',\n        # necessary for accessing SecretStorage keyring (system wide Gnome\n        # keyring)  but not installable on travis, IIRC since it needs connectivity\n        # to the dbus whenever installed or smth like that, thus disabled here\n        # but you might need it\n        # 'dbus-python',\n        'scriv',  # changelog\n    ],\n})\nrequires['devel'] = sum(list(requires.values()), [])\n\n\n# let's not build manpages and examples automatically (gh-896)\n# configure additional command for custom build steps\n#class DataladBuild(build_py):\n#    def run(self):\n#        self.run_command('build_manpage')\n#        self.run_command('build_examples')\n#        build_py.run(self)\n\ncmdclass = {\n    'build_manpage': BuildManPage,\n    # 'build_examples': BuildRSTExamplesFromScripts,\n    'build_cfginfo': BuildConfigInfo,\n    # 'build_py': DataladBuild\n}\n\nsetup_kwargs = {}\n\n# normal entrypoints for the rest\n# a bit of a dance needed, as on windows the situation is different\nentry_points = {\n    'console_scripts': [\n        'datalad=datalad.cli.main:main',\n        'git-annex-remote-datalad-archives=datalad.customremotes.archives:main',\n        'git-annex-remote-datalad=datalad.customremotes.datalad:main',\n        'git-annex-remote-ria=datalad.customremotes.ria_remote:main',\n        'git-annex-remote-ora=datalad.distributed.ora_remote:main',\n        'git-credential-datalad=datalad.local.gitcredential_datalad:git_credential_datalad',\n    ],\n}\nsetup_kwargs['entry_points'] = entry_points\n\nclassifiers = [\n    'Development Status :: 5 - Production/Stable',\n    'Environment :: Console',\n    'Intended Audience :: Developers',\n    'Intended Audience :: Education',\n    'Intended Audience :: End Users/Desktop',\n    'Intended Audience :: Science/Research',\n    'License :: DFSG approved',\n    'License :: OSI Approved :: MIT License',\n    'Natural Language :: English',\n    'Operating System :: POSIX',\n    'Programming Language :: Python :: 3 :: Only',\n    'Programming Language :: Unix Shell',\n    'Topic :: Communications :: File Sharing',\n    'Topic :: Education',\n    'Topic :: Internet',\n    'Topic :: Other/Nonlisted Topic',\n    'Topic :: Scientific/Engineering',\n    'Topic :: Software Development :: Libraries :: Python Modules',\n    'Topic :: Software Development :: Version Control :: Git',\n    'Topic :: Utilities',\n]\nsetup_kwargs['classifiers'] = classifiers\n\nsetup_kwargs[\"version\"] = versioneer.get_version()\ncmdclass.update(versioneer.get_cmdclass())\n\ndatalad_setup(\n    'datalad',\n    description=\"data distribution geared toward scientific datasets\",\n    install_requires=\n        requires['core'] + requires['downloaders'] +\n        requires['publish'],\n    python_requires='>=3.9',\n    project_urls={'Homepage': 'https://www.datalad.org',\n                  'Developer docs': 'https://docs.datalad.org/en/stable',\n                  'User handbook': 'https://handbook.datalad.org',\n                  'Source': 'https://github.com/datalad/datalad',\n                  'Bug Tracker': 'https://github.com/datalad/datalad/issues',\n                  'RRID': 'https://identifiers.org/RRID:SCR_003931'},\n    extras_require=requires,\n    cmdclass=cmdclass,\n    include_package_data=True,\n    **setup_kwargs\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/datalad-container-extension",
            "repo_link": "https://github.com/datalad/datalad-container",
            "content": {
                "codemeta": "",
                "readme": "     ____          _           _                 _\n    |  _ \\   __ _ | |_   __ _ | |      __ _   __| |\n    | | | | / _` || __| / _` || |     / _` | / _` |\n    | |_| || (_| || |_ | (_| || |___ | (_| || (_| |\n    |____/  \\__,_| \\__| \\__,_||_____| \\__,_| \\__,_|\n                                       Container\n\n[![Build status](https://ci.appveyor.com/api/projects/status/k4eyq1yygcvwf7wk/branch/master?svg=true)](https://ci.appveyor.com/project/mih/datalad-container/branch/master) [![Travis tests status](https://app.travis-ci.com/datalad/datalad-container.svg?branch=master)](https://app.travis-ci.com/datalad/datalad-container) [![codecov.io](https://codecov.io/github/datalad/datalad-container/coverage.svg?branch=master)](https://codecov.io/github/datalad/datalad-container?branch=master) [![Documentation](https://readthedocs.org/projects/datalad-container/badge/?version=latest)](http://datalad-container.rtfd.org) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![GitHub release](https://img.shields.io/github/release/datalad/datalad-container.svg)](https://GitHub.com/datalad/datalad-container/releases/) [![PyPI version fury.io](https://badge.fury.io/py/datalad-container.svg)](https://pypi.python.org/pypi/datalad-container/) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3368666.svg)](https://doi.org/10.5281/zenodo.3368666) ![Conda](https://anaconda.org/conda-forge/datalad-container/badges/version.svg)\n\nThis extension enhances DataLad (http://datalad.org) for working with\ncomputational containers. Please see the [extension\ndocumentation](http://datalad-container.rtfd.org)\nfor a description on additional commands and functionality.\n\nFor general information on how to use or contribute to DataLad (and this\nextension), please see the [DataLad website](http://datalad.org) or the\n[main GitHub project page](http://datalad.org).\n\n\n## Installation\n\nBefore you install this package, please make sure that you [install a recent\nversion of git-annex](https://git-annex.branchable.com/install).  Afterwards,\ninstall the latest version of `datalad-container` from\n[PyPi](https://pypi.org/project/datalad-container). It is recommended to use\na dedicated [virtualenv](https://virtualenv.pypa.io):\n\n    # create and enter a new virtual environment (optional)\n    virtualenv --system-site-packages --python=python3 ~/env/datalad\n    . ~/env/datalad/bin/activate\n\n    # install from PyPi\n    pip install datalad_container\n\nIt is also available for conda package manager from conda-forge:\n\n    conda install -c conda-forge datalad-container\n\n\n## Support\n\nThe documentation of this project is found here:\nhttp://docs.datalad.org/projects/container\n\nAll bugs, concerns and enhancement requests for this software can be submitted here:\nhttps://github.com/datalad/datalad-container/issues\n\nIf you have a problem or would like to ask a question about how to use DataLad,\nplease [submit a question to\nNeuroStars.org](https://neurostars.org/tags/datalad) with a ``datalad`` tag.\nNeuroStars.org is a platform similar to StackOverflow but dedicated to\nneuroinformatics.\n\nAll previous DataLad questions are available here:\nhttp://neurostars.org/tags/datalad/\n\n## Acknowledgements\n\nDataLad development is supported by a US-German collaboration in computational\nneuroscience (CRCNS) project \"DataGit: converging catalogues, warehouses, and\ndeployment logistics into a federated 'data distribution'\" (Halchenko/Hanke),\nco-funded by the US National Science Foundation (NSF 1429999) and the German\nFederal Ministry of Education and Research (BMBF 01GQ1411). Additional support\nis provided by the German federal state of Saxony-Anhalt and the European\nRegional Development Fund (ERDF), Project: Center for Behavioral Brain\nSciences, Imaging Platform.  This work is further facilitated by the ReproNim\nproject (NIH 1P41EB019936-01A1).\n\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools >= 43.0.0\", \"tomli\", \"wheel\"]\n\n[tool.isort]\nforce_grid_wrap = 2\ninclude_trailing_comma = true\nmulti_line_output = 3\ncombine_as_imports = true\n\n[tool.codespell]\nskip = '.git,*.pdf,*.svg,venvs,versioneer.py,venvs'\n# DNE - do not exist\nignore-words-list = 'dne'\n\n[tool.versioneer]\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run 'versioneer.py setup' after changing this section, and commit the\n# resulting files.\nVCS = 'git'\nstyle = 'pep440'\nversionfile_source = 'datalad_container/_version.py'\nversionfile_build = 'datalad_container/_version.py'\ntag_prefix = ''\nparentdir_prefix = ''\n\n# If you want to develop, use requirements-devel.txt\n# git+https://github.com/datalad/datalad.git\n\n#!/usr/bin/env python\n\nfrom setuptools import setup\nimport versioneer\n\nfrom _datalad_buildsupport.setup import (\n    BuildManPage,\n)\n\ncmdclass = versioneer.get_cmdclass()\ncmdclass.update(build_manpage=BuildManPage)\n\nif __name__ == '__main__':\n    setup(name='datalad_container',\n          version=versioneer.get_version(),\n          cmdclass=cmdclass,\n    )\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/datalad-next-extension",
            "repo_link": "https://github.com/datalad/datalad-next",
            "content": {
                "codemeta": "",
                "readme": "# DataLad NEXT extension\n\n[![All Contributors](https://img.shields.io/github/all-contributors/datalad/datalad-next?color=ee8449&style=flat-square)](#contributors)\n[![Build status](https://ci.appveyor.com/api/projects/status/dxomp8wysjb7x2os/branch/main?svg=true)](https://ci.appveyor.com/project/mih/datalad-next/branch/main)\n[![codecov](https://codecov.io/gh/datalad/datalad-next/branch/main/graph/badge.svg?token=2P8rak7lSX)](https://codecov.io/gh/datalad/datalad-next)\n[![docs](https://github.com/datalad/datalad-next/workflows/docs/badge.svg)](https://github.com/datalad/datalad-next/actions?query=workflow%3Adocs)\n[![Documentation Status](https://readthedocs.org/projects/datalad-next/badge/?version=latest)](http://docs.datalad.org/projects/next/en/latest/?badge=latest)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![GitHub release](https://img.shields.io/github/release/datalad/datalad-next.svg)](https://GitHub.com/datalad/datalad-next/releases/)\n[![PyPI version fury.io](https://badge.fury.io/py/datalad-next.svg)](https://pypi.python.org/pypi/datalad-next/)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6833099.svg)](https://doi.org/10.5281/zenodo.6833099)\n[![Hatch project](https://img.shields.io/badge/%F0%9F%A5%9A-Hatch-4051b5.svg)](https://github.com/pypa/hatch)\n\nThis DataLad extension can be thought of as a staging area for additional\nfunctionality, or for improved performance and user experience. Unlike other\ntopical or more experimental extensions, the focus here is on functionality\nwith broad applicability. This extension is a suitable dependency for other\nsoftware packages that intend to build on this improved set of functionality.\n\n## Installation\n\n```\n# create and enter a new virtual environment (optional)\n$ virtualenv --python=python3 ~/env/dl-next\n$ . ~/env/dl-next/bin/activate\n# install from PyPi\n$ python -m pip install datalad-next\n```\n\n## How to use\n\nAdditional commands provided by this extension are immediately available\nafter installation. However, in order to fully benefit from all improvements,\nthe extension has to be enabled for auto-loading by executing:\n\n    git config --global --add datalad.extensions.load next\n\nDoing so will enable the extension to also alter the behavior the core DataLad\npackage and its commands.\n\n## Summary of functionality provided by this extension\n\n- A replacement sub-system for credential handling that is able to handle arbitrary\n  properties for annotating a secret, and facilitates determining suitable\n  credentials while minimizing avoidable user interaction, without compromising\n  configurability. A convenience method is provided that implements a standard\n  workflow for obtaining a credential.\n- A user-facing `credentials` command to set, remove, and query credentials.\n- The `create-sibling-...` commands for the platforms GitHub, GIN, GOGS, Gitea\n  are equipped with improved credential handling that, for example, only stores\n  entered credentials after they were confirmed to work, or auto-selects the\n  most recently used, matching credentials, when none are specified.\n- A `create-sibling-webdav` command for hosting datasets on a WebDAV server via\n  a sibling tandem for Git history and file storage. Datasets hosted on WebDAV\n  in this fashion are cloneable with `datalad-clone`. A full annex setup\n  for storing complete datasets with historical file content version, and an\n  additional mode for depositing single-version dataset snapshot are supported.\n  The latter enables convenient collaboration with audiences that are not using\n  DataLad, because all files are browsable via a WebDAV server's point-and-click\n  user interface.\n- Enhance `datalad-push` to automatically export files to git-annex special\n  remotes configured with `exporttree=yes`.\n- Speed-up `datalad-push` when processing non-git special remotes. This particularly\n  benefits less efficient hosting scenarios like WebDAV.\n- Enhance `datalad-siblings enable` (`AnnexRepo.enable_remote()`) to automatically\n  deploy credentials for git-annex special remotes that require them.\n- `git-remote-datalad-annex` is a Git remote helper to push/fetch to any\n  location accessible by any git-annex special remote.\n- `git-annex-backend-XDLRA` (originally available from the `mihextras` extension)\n  is a custom external git-annex backend used by `git-remote-datalad-annex`. A base\n  class to facilitate development of external backends in Python is also provided.\n- Enhance `datalad-configuration` to support getting configuration from \"global\"\n  scope without a dataset being present.\n- New modular framework for URL operations. This framework directly supports operation\n  on `http(s)`, `ssh`, and `file` URLs, and can be extended with custom functionality\n  for additional protocols or even interaction with specific individual servers.\n  The basic operations `download`, `upload`, `delete`, and `stat` are recognized,\n  and can be implemented. The framework offers uniform progress reporting and\n  simultaneous content has computation. This framework is meant to replace and\n  extend the downloader/provide framework in the DataLad core package. In contrast\n  to its predecessor it is integrated with the new credential framework, and\n  operations beyond downloading.\n- `git-annex-remote-uncurl` is a special remote that exposes the new URL\n  operations framework via git-annex. It provides flexible means to compose\n  and rewrite URLs (e.g., to compensate for storage infrastructure changes)\n  without having to modify individual URLs recorded in datasets. It enables\n  seamless transitions between any services and protocols supported by the\n  framework. This special remote can replace the `datalad` special remote\n  provided by the DataLad core package.\n- A `download` command is provided as a front-end for the new modular URL\n  operations framework.\n- A `python-requests` compatible authentication handler (`DataladAuth`) that\n  interfaces DataLad's credential system.\n- Boosted throughput of DataLad's `runner` component for command execution.\n- Substantially more comprehensive replacement for DataLad's `constraints` system\n  for type conversion and parameter validation.\n- Windows and Mac client support for RIA store access.\n- A `next-status` command that is A LOT faster than `status`, and offers\n  a `mono` recursion mode that shows modifications of nested dataset\n  hierarchies relative to the state of the root dataset.\n  Requires Git v2.31 (or later).\n\n## Summary of additional features for DataLad extension development\n\n- Framework for uniform command parameter validation. Regardless of the used\n  API (Python, CLI, or GUI), command parameters are uniformly validated. This\n  facilitates a stricter separation of parameter specification (and validation)\n  from the actual implementation of a command. The latter can now focus on a\n  command's logic only, while the former enables more uniform and more\n  comprehensive validation and error reporting. Beyond per-parameter validation\n  and type-conversion also inter-parameter dependency validation and value\n  transformations are supported.\n- Improved composition of importable functionality. Key components for `commands`,\n  `annexremotes`, `datasets` (etc) are collected in topical top-level modules that\n  provide \"all\" necessary pieces in a single place.\n- `webdav_server` fixture that automatically deploys a local WebDAV\n  server.\n- Utilities for HTTP handling\n  - `probe_url()` discovers redirects and authentication requirements for an HTTP\n    URL\n  - `get_auth_realm()` returns a label for an authentication realm that can be used\n    to query for matching credentials\n- Utilities for special remote credential management:\n  - `get_specialremote_credential_properties()` inspects a special remote and returns\n    properties for querying a credential store for matching credentials\n  - `update_specialremote_credential()` updates a credential in a store after\n    successful use\n  - `get_specialremote_credential_envpatch()` returns a suitable environment \"patch\"\n    from a credential for a particular special remote type\n- Helper for runtime-patching other datalad code (`datalad_next.utils.patch`)\n- Base class for implementing custom `git-annex` backends.\n- A set of `pytest` fixtures to:\n  - check that no global configuration side-effects are left behind by a test\n  - check that no secrets are left behind by a test\n  - provide a temporary configuration that is isolated from a user environment\n    and from other tests\n  - provide a temporary secret store that is isolated from a user environment\n    and from other tests\n  - provide a temporary credential manager to perform credential deployment\n    and manipulation isolated from a user environment and from other tests\n- An `iter_subproc()` helper that enable communication with subprocesses\n  via input/output iterables.\n- A `shell` context manager that enables interaction with (remote) shells,\n  including support for input/output iterables for each shell-command execution\n  within the context.\n\n## Patching the DataLad core package.\n\nSome of the features described above rely on a modification of the DataLad core\npackage itself, rather than coming in the form of additional commands. Loading\nthis extension causes a range of patches to be applied to the `datalad` package\nto enable them. A comprehensive description of the current set of patch is\navailable at http://docs.datalad.org/projects/next/en/latest/#datalad-patches\n\n## Developing with DataLad NEXT\n\nThis extension package moves fast in comparison to the core package. Nevertheless,\nattention is paid to API stability, adequate semantic versioning, and informative\nchangelogs.\n\n### Public vs internal API\n\nAnything that can be imported directly from any of the sub-packages in\n`datalad_next` is considered to be part of the public API. Changes to this API\ndetermine the versioning, and development is done with the aim to keep this API\nas stable as possible. This includes signatures and return value behavior.\n\nAs an example: `from datalad_next.runners import iter_git_subproc` imports a\npart of the public API, but `from datalad_next.runners.git import\niter_git_subproc` does not.\n\n### Use of the internal API\n\nDevelopers can obviously use parts of the non-public API. However, this should\nonly be done with the understanding that these components may change from one\nrelease to another, with no guarantee of transition periods, deprecation\nwarnings, etc.\n\nDevelopers are advised to never reuse any components with names starting with\n`_` (underscore). Their use should be limited to their individual subpackage.\n\n## Acknowledgements\n\nThis DataLad extension was developed with funding from the Deutsche\nForschungsgemeinschaft (DFG, German Research Foundation) under grant SFB 1451\n([431549029](https://gepris.dfg.de/gepris/projekt/431549029), INF project).\n\n\n## Contributors\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://psychoinformatics.de/\"><img src=\"https://avatars.githubusercontent.com/u/136479?v=4?s=100\" width=\"100px;\" alt=\"Michael Hanke\"/><br /><sub><b>Michael Hanke</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/issues?q=author%3Amih\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=mih\" title=\"Code\">💻</a> <a href=\"#content-mih\" title=\"Content\">🖋</a> <a href=\"#design-mih\" title=\"Design\">🎨</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=mih\" title=\"Documentation\">📖</a> <a href=\"#financial-mih\" title=\"Financial\">💵</a> <a href=\"#fundingFinding-mih\" title=\"Funding Finding\">🔍</a> <a href=\"#ideas-mih\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"#infra-mih\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"#maintenance-mih\" title=\"Maintenance\">🚧</a> <a href=\"#mentoring-mih\" title=\"Mentoring\">🧑‍🏫</a> <a href=\"#platform-mih\" title=\"Packaging/porting to new platform\">📦</a> <a href=\"#projectManagement-mih\" title=\"Project Management\">📆</a> <a href=\"https://github.com/datalad/datalad-next/pulls?q=is%3Apr+reviewed-by%3Amih\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#talk-mih\" title=\"Talks\">📢</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=mih\" title=\"Tests\">⚠️</a> <a href=\"#tool-mih\" title=\"Tools\">🔧</a> <a href=\"#userTesting-mih\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/catetrai\"><img src=\"https://avatars.githubusercontent.com/u/18424941?v=4?s=100\" width=\"100px;\" alt=\"catetrai\"/><br /><sub><b>catetrai</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/commits?author=catetrai\" title=\"Code\">💻</a> <a href=\"#design-catetrai\" title=\"Design\">🎨</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=catetrai\" title=\"Documentation\">📖</a> <a href=\"#ideas-catetrai\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=catetrai\" title=\"Tests\">⚠️</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/effigies\"><img src=\"https://avatars.githubusercontent.com/u/83442?v=4?s=100\" width=\"100px;\" alt=\"Chris Markiewicz\"/><br /><sub><b>Chris Markiewicz</b></sub></a><br /><a href=\"#maintenance-effigies\" title=\"Maintenance\">🚧</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=effigies\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mslw\"><img src=\"https://avatars.githubusercontent.com/u/11985212?v=4?s=100\" width=\"100px;\" alt=\"Michał Szczepanik\"/><br /><sub><b>Michał Szczepanik</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/issues?q=author%3Amslw\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=mslw\" title=\"Code\">💻</a> <a href=\"#content-mslw\" title=\"Content\">🖋</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=mslw\" title=\"Documentation\">📖</a> <a href=\"#example-mslw\" title=\"Examples\">💡</a> <a href=\"#ideas-mslw\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"#infra-mslw\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"#maintenance-mslw\" title=\"Maintenance\">🚧</a> <a href=\"https://github.com/datalad/datalad-next/pulls?q=is%3Apr+reviewed-by%3Amslw\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#talk-mslw\" title=\"Talks\">📢</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=mslw\" title=\"Tests\">⚠️</a> <a href=\"#tutorial-mslw\" title=\"Tutorials\">✅</a> <a href=\"#userTesting-mslw\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jsheunis.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/10141237?v=4?s=100\" width=\"100px;\" alt=\"Stephan Heunis\"/><br /><sub><b>Stephan Heunis</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/issues?q=author%3Ajsheunis\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=jsheunis\" title=\"Code\">💻</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=jsheunis\" title=\"Documentation\">📖</a> <a href=\"#ideas-jsheunis\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"#maintenance-jsheunis\" title=\"Maintenance\">🚧</a> <a href=\"#talk-jsheunis\" title=\"Talks\">📢</a> <a href=\"#userTesting-jsheunis\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bpoldrack\"><img src=\"https://avatars.githubusercontent.com/u/10498301?v=4?s=100\" width=\"100px;\" alt=\"Benjamin Poldrack\"/><br /><sub><b>Benjamin Poldrack</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/issues?q=author%3Abpoldrack\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=bpoldrack\" title=\"Code\">💻</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/yarikoptic\"><img src=\"https://avatars.githubusercontent.com/u/39889?v=4?s=100\" width=\"100px;\" alt=\"Yaroslav Halchenko\"/><br /><sub><b>Yaroslav Halchenko</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/issues?q=author%3Ayarikoptic\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=yarikoptic\" title=\"Code\">💻</a> <a href=\"#infra-yarikoptic\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"#maintenance-yarikoptic\" title=\"Maintenance\">🚧</a> <a href=\"#tool-yarikoptic\" title=\"Tools\">🔧</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/christian-monch\"><img src=\"https://avatars.githubusercontent.com/u/17925232?v=4?s=100\" width=\"100px;\" alt=\"Christian Mönch\"/><br /><sub><b>Christian Mönch</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/commits?author=christian-monch\" title=\"Code\">💻</a> <a href=\"#design-christian-monch\" title=\"Design\">🎨</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=christian-monch\" title=\"Documentation\">📖</a> <a href=\"#ideas-christian-monch\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"https://github.com/datalad/datalad-next/pulls?q=is%3Apr+reviewed-by%3Achristian-monch\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=christian-monch\" title=\"Tests\">⚠️</a> <a href=\"#userTesting-christian-monch\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/adswa\"><img src=\"https://avatars.githubusercontent.com/u/29738718?v=4?s=100\" width=\"100px;\" alt=\"Adina Wagner\"/><br /><sub><b>Adina Wagner</b></sub></a><br /><a href=\"#a11y-adswa\" title=\"Accessibility\">️️️️♿️</a> <a href=\"https://github.com/datalad/datalad-next/issues?q=author%3Aadswa\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=adswa\" title=\"Code\">💻</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=adswa\" title=\"Documentation\">📖</a> <a href=\"#example-adswa\" title=\"Examples\">💡</a> <a href=\"#maintenance-adswa\" title=\"Maintenance\">🚧</a> <a href=\"#projectManagement-adswa\" title=\"Project Management\">📆</a> <a href=\"https://github.com/datalad/datalad-next/pulls?q=is%3Apr+reviewed-by%3Aadswa\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#talk-adswa\" title=\"Talks\">📢</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=adswa\" title=\"Tests\">⚠️</a> <a href=\"#tutorial-adswa\" title=\"Tutorials\">✅</a> <a href=\"#userTesting-adswa\" title=\"User Testing\">📓</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jwodder\"><img src=\"https://avatars.githubusercontent.com/u/98207?v=4?s=100\" width=\"100px;\" alt=\"John T. Wodder II\"/><br /><sub><b>John T. Wodder II</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/commits?author=jwodder\" title=\"Code\">💻</a> <a href=\"#infra-jwodder\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=jwodder\" title=\"Tests\">⚠️</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/candleindark\"><img src=\"https://avatars.githubusercontent.com/u/12135617?v=4?s=100\" width=\"100px;\" alt=\"Isaac To\"/><br /><sub><b>Isaac To</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/commits?author=candleindark\" title=\"Code\">💻</a></td>\n    </tr>\n  </tbody>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\n",
                "dependencies": "[build-system]\nrequires = [\n  \"hatchling\",\n  \"hatch-vcs\",\n]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"datalad-next\"\ndynamic = [\"version\"]\ndescription = \"What is next in DataLad\"\nreadme = \"README.md\"\nrequires-python = \">= 3.8\"\nlicense = \"MIT\"\nkeywords = [\n  \"datalad\",\n  \"git\",\n  \"git-annex\",\n]\nauthors = [\n  { name = \"The DataLad Team and Contributors\", email = \"team@datalad.org\" },\n]\nclassifiers = [\n  \"License :: OSI Approved :: MIT License\",\n  \"Development Status :: 4 - Beta\",\n  \"Intended Audience :: Developers\",\n  \"Intended Audience :: End Users/Desktop\",\n  \"Natural Language :: English\",\n  \"Operating System :: OS Independent\",\n  \"Topic :: Software Development\",\n  \"Topic :: Software Development :: Version Control\",\n  \"Topic :: Software Development :: Version Control :: Git\",\n  \"Programming Language :: Python\",\n  \"Programming Language :: Python :: 3\",\n]\ndependencies = [\n  \"annexremote\",\n  \"datalad >= 0.18.4\",\n  \"datasalad >= 0.2.1\",\n  \"humanize\",\n  \"more-itertools\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/datalad/datalad-next\"\nDocumentation = \"https://docs.datalad.org/projects/next/en/latest/\"\nIssues = \"https://github.com/datalad/datalad-next/issues\"\nSource = \"https://github.com/datalad/datalad-next\"\nChangelog = \"https://github.com/datalad/datalad-next/blob/main/CHANGELOG.md\"\n\n[project.optional-dependencies]\ndevel = [\n  \"cheroot\",\n  \"coverage\",\n  \"psutil\",\n  \"pytest\",\n  \"pytest-cov\",\n  \"webdavclient3\",\n  \"wsgidav\",\n]\nhttpsupport = [\n  \"requests\",\n  \"requests_toolbelt\",\n]\n\n[project.scripts]\ngit-annex-backend-XDLRA = \"datalad_next.annexbackends.xdlra:main\"\ngit-annex-remote-archivist = \"datalad_next.annexremotes.archivist:main\"\ngit-annex-remote-uncurl = \"datalad_next.annexremotes.uncurl:main\"\ngit-remote-datalad-annex = \"datalad_next.gitremotes.datalad_annex:main\"\n\n[project.entry-points.\"datalad.extensions\"]\nnext = \"datalad_next:command_suite\"\n\n[tool.hatch.version]\nsource = \"vcs\"\n\n[tool.hatch.build.hooks.vcs]\nversion-file = \"datalad_next/_version.py\"\n\n[tool.hatch.build.targets.sdist]\nexclude = [\n  \".github\",\n  \"tools\",\n  \"docs/build\",\n]\n\n[tool.hatch.envs.hatch-test]\ndefault-args = [\"datalad_next\"]\nextra-dependencies = [\n  \"pytest\",\n  \"pytest-cov\",\n  \"psutil\",\n  \"webdavclient3\",\n  \"wsgidav\",\n]\n[tool.hatch.envs.hatch-test.env-vars]\n# load the extension\nDATALAD_EXTENSIONS_LOAD = \"next\"\n\n[tool.hatch.envs.tests]\ndescription = \"run tests across Python versions\"\ntemplate = \"hatch-test\"\n\n[[tool.hatch.envs.tests.matrix]]\npython = [\"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n\n[tool.hatch.envs.tests.scripts]\nrun = 'python -m pytest {args}'\n\n\n[tool.hatch.envs.types]\ndescription = \"type checking with MyPy\"\nextra-dependencies = [\n  \"mypy>=1.0.0\",\n  \"pytest\",\n]\n\n[tool.hatch.envs.types.scripts]\ncheck = [\n  \"mypy --install-types --non-interactive --python-version 3.8 --pretty --show-error-context {args:datalad_next}\",\n]\n\n[tool.hatch.envs.docs]\ndescription = \"build Sphinx-based docs\"\nextra-dependencies = [\n  \"sphinx\",\n  \"sphinx_rtd_theme\",\n  \"pytest\",\n]\n[tool.hatch.envs.docs.scripts]\nbuild = [\n  \"make -C docs html\",\n]\nclean = [\n  \"rm -rf docs/generated\",\n  \"make -C docs clean\",\n]\n\n[tool.hatch.envs.cz]\ndescription = \"commit compliance, changelog, and release generation\"\ndetached = true\nextra-dependencies = [\n  \"commitizen\",\n]\n[tool.hatch.envs.cz.scripts]\n#check-commits = [\n#  # check all commit messages since the (before) beginning\n#  \"cz check --rev-range 4b825dc642cb6eb9a060e54bf8d69288fbee4904..HEAD\",\n#]\nshow-changelog = [\n  # show the would-be changelog on stdout\n  \"cz changelog --dry-run\",\n]\nbump-version = [\n  # bump version (also tags) and update changelog\n  \"cz bump --changelog\",\n]\n\n[tool.hatch.envs.codespell]\ndescription = \"spell checking\"\ndetached = true\nextra-dependencies = [\n  \"codespell\",\n]\n[tool.hatch.envs.codespell.scripts]\ncheck = \"codespell\"\nfix = \"codespell --write-changes\"\n\n[tool.codespell]\nskip = \".git,build,.*cache,dist\"\nexclude-file = \".codespell-exclude\"\n\n[tool.pytest.ini_options]\naddopts = \"--strict-markers\"\nmarkers = [\n  # datalad-next custom markers\n  \"skip_if_no_network\",\n  # (implicitly) used markers from datalad-core, which are only declared\n  # in its tox.ini (inaccessible to pytest here)\n  \"fail_slow\",\n  \"githubci_osx\",\n  \"githubci_win\",\n  \"integration\",\n  \"known_failure\",\n  \"known_failure_githubci_osx\",\n  \"known_failure_githubci_win\",\n  \"known_failure_osx\",\n  \"known_failure_windows\",\n  \"network\",\n  \"osx\",\n  \"probe_known_failure\",\n  \"serve_path_via_http\",\n  \"skip_if_adjusted_branch\",\n  \"skip_if_no_network\",\n  \"skip_if_on_windows\",\n  \"skip_if_root\",\n  \"skip_known_failure\",\n  \"skip_nomultiplex_ssh\",\n  \"skip_ssh\",\n  \"skip_wo_symlink_capability\",\n  \"slow\",\n  \"turtle\",\n  \"usecase\",\n  \"windows\",\n  \"with_config\",\n  \"with_fake_cookies_db\",\n  \"with_memory_keyring\",\n  \"with_sameas_remotes\",\n  \"with_testrepos\",\n  \"without_http_proxy\",\n]\n\n\n[tool.coverage.run]\nsource_pkgs = [\"datalad_next\"]\nbranch = true\nparallel = true\nomit = [\n#  \"src/datasalad/__about__.py\",\n]\ndata_file = \"${COVERAGE_ROOT-.}/.coverage\"\n\n[tool.coverage.paths]\ndatalad_next = [\"src/datalad_next\", \"*/datalad_next/src/datalad_next\"]\ntests = [\"tests\", \"*/datalad_next/*/tests\"]\n\n[tool.coverage.report]\nshow_missing = true\nexclude_lines = [\n  \"no cov\",\n  \"if __name__ == .__main__.:\",\n  \"if TYPE_CHECKING:\",\n  \"raise NotImplementedError\",\n]\n\n[tool.ruff]\nexclude = [\n  # sphinx\n  \"docs\",\n]\nline-length = 88\nindent-width = 4\ntarget-version = \"py38\"\n[tool.ruff.format]\n# Prefer single quotes over double quotes.\nquote-style = \"single\"\n[tool.ruff.lint.per-file-ignores]\n\"**/test_*\" = [\n  # permit assert statements in tests\n  \"S101\",\n  # permit relative import in tests\n  \"TID252\",\n  # permit versatile function names in tests\n  \"N802\",\n]\n# permit relative import in subpackage root\n\"datalad_next/*/__init__.py\" = [\"TID252\"]\n\n[tool.commitizen]\nname = \"cz_customize\"\ntag_format = \"$version\"\nversion_scheme = \"pep440\"\nversion_provider = \"scm\"\nchangelog_incremental = true\ntemplate = \".changelog.md.j2\"\ngpg_sign = true\n\n[tool.commitizen.customize]\ncommit_parser = \"^((?P<change_type>feat|fix|rf|perf|test|doc|BREAKING CHANGE)(?:\\\\((?P<scope>[^()\\r\\n]*)\\\\)|\\\\()?(?P<breaking>!)?|\\\\w+!):\\\\s(?P<message>.*)?(?P<body>.*)?\"\nchange_type_order = [\"BREAKING CHANGE\", \"feat\", \"fix\", \"rf\", \"perf\", \"doc\", \"test\"]\nchangelog_pattern = \"^((BREAKING[\\\\-\\\\ ]CHANGE|\\\\w+)(\\\\(.+\\\\))?!?):\"\nbump_pattern = \"^((BREAKING[\\\\-\\\\ ]CHANGE|\\\\w+)(\\\\(.+\\\\))?!?):\"\nschema_pattern = \"(?s)(ci|doc|feat|fix|perf|rf|style|test|chore|revert|bump)(\\\\(\\\\S+\\\\))?!?:( [^\\\\n\\\\r]+)((\\\\n\\\\n.*)|(\\\\s*))?$\"\n\n[tool.commitizen.customize.bump_map]\n\"^\\\\w+!\" = \"MAJOR\"\n\"^BREAKING\" = \"MAJOR\"\n\"^feat\" = \"MINOR\"\n\"^fix\" = \"PATCH\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/datasail",
            "repo_link": "https://github.com/kalininalab/DataSAIL",
            "content": {
                "codemeta": "",
                "readme": "# DataSAIL: Data Splitting Against Information Leaking \n\n![testing](https://github.com/kalininalab/glyles/actions/workflows/test.yaml/badge.svg)\n[![docs-image](https://readthedocs.org/projects/glyles/badge/?version=latest)](https://datasail.readthedocs.io/en/latest/index.html)\n[![codecov](https://codecov.io/gh/kalininalab/DataSAIL/branch/main/graph/badge.svg)](https://codecov.io/gh/kalininalab/DataSAIL)\n[![anaconda](https://anaconda.org/kalininalab/datasail/badges/version.svg)](https://anaconda.org/kalininalab/datasail)\n[![update](https://anaconda.org/kalininalab/datasail/badges/latest_release_date.svg)](https://anaconda.org/kalininalab/datasail)\n[![license](https://anaconda.org/kalininalab/datasail/badges/license.svg)](https://anaconda.org/kalininalab/datasail)\n[![downloads](https://anaconda.org/kalininalab/datasail/badges/downloads.svg)](https://anaconda.org/kalininalab/datasail)\n![Python 3](https://img.shields.io/badge/python-3-blue.svg)\n[![DOI](https://zenodo.org/badge/598109632.svg)](https://doi.org/10.5281/zenodo.13938602)\n\nDataSAIL: [![platforms](https://anaconda.org/kalininalab/datasail/badges/platforms.svg)](https://anaconda.org/kalininalab/datasail)\nDataSAIL-lite: [![platforms](https://anaconda.org/kalininalab/datasail-lite/badges/platforms.svg)](https://anaconda.org/kalininalab/datasail-lite)\n\nDataSAIL is a tool that splits data while minimizing Information Leakage. This tool formulates the splitting of a \ndataset as a constrained minimization problem and computes the assignment of data points to splits while minimizing the \nobjective function that accounts for information leakage.\n\nInternally, DataSAIL uses disciplined quasi-convex programming and binary quadratic programs to formulate the \noptimization task. DataSAIL utilizes solves like [SCIP](https://scipopt.org/), one of the fastest non-commercial \nsolvers for this type of problem, and [MOSEK](https://mosek.com), a commercial solver that distributes free licenses \nfor academic use. There are other options; please check the documentation.\n\nApart from the short overview, you can find a more detailed explanation of the tool on \n[ReadTheDocs](https://datasail.readthedocs.io/en/latest/index.html). \n\n## Installation\n\nDataSAIL is installable from [conda](https://anaconda.org/kalininalab/datasail) using\n[mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html>).\nusing\n\n````shell\nmamba create -n sail -c conda-forge -c kalininalab -c bioconda datasail\nconda activate sail\npip install grakel\n````\n\nto install it into a new empty environment or\n\n````shell\nmamba install -c conda-forge -c kalininalab -c bioconda -c mosek datasail\npip install grakel\n````\n\nto install DataSAIL in an already existing environment. Alternatively, one can install DataSAIL-lite from conda. \nDataSAIL-lite is a version of DataSAIL that does not install all clustering algorithms as the standard DataSAIL.\nInstalling either package usually takes less than 5 minutes.\n\nDataSAIL is available for Python 3.8 and newer.\n\n## Usage\n\nDataSAIL is installed as a command-line tool. So, in the conda environment, DataSAIL has been installed to, you can run \n\n````shell\ndatasail --e-type P --e-data <path_to_fasta> --e-sim mmseqs --output <path_to_output_path> --technique C1e\n````\n\nto split a set of proteins that have been clustered using mmseqs. For a full list of arguments, run `datasail -h` and \ncheckout [ReadTheDocs](https://datasail.readthedocs.io/en/latest/index.html). There is a more detailed explanation of the arguments and example notebooks. The runtime largy depends on the number and type of splits to be computed and the size of the dataset. For small datasets (less then 10k samples) DataSAIL finished within minutes. On large datasets (more than 100k samples) it can take several hours to complete.\n\n## When to use DataSAIL and when not to use\n\nOne can distinguish two main ways to train a machine-learning model on biological data. \n* Either the model shall be applied to data substantially different from the data to train on. In this case, it \n  is essential to have test cases that correctly model this real-world application scenario by being as dissimilar as \n  possible to the training data. \n* Or the training dataset already covers the whole space of possible samples shown to the model.\n\nDataSAIL is created to compute complex data splits by separating data based on similarities. This makes \ncomplex data splits for the first scenario. So, you can use DataSAIL when your model is applied to data  \ndifferent from your training data but not if the data in the application is more or less the same as in the training.\n\n## Citation\n\nIf you used DataSAIL to split your data, please cite DataSAIL in your publication.\n````\n@article{joeres2023datasail,\n  title={DataSAIL: Data Splitting Against Information Leakage},\n  author={Joeres, Roman and Blumenthal, David B. and Kalinina, Olga V},\n  journal={bioRxiv},\n  pages={2023--11},\n  year={2023},\n  publisher={Cold Spring Harbor Laboratory}\n}\n````\n\n",
                "dependencies": "from setuptools import setup, find_packages\nfrom datasail.version import __version__\n\nwith open(\"README.md\", \"r\") as desc_file:\n    long_description = desc_file.read()\n\nsetup(\n    name=\"DataSAIL\",\n    version=__version__,\n    description=\"Data Splitting Against Information Leaking\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    license='MIT',\n    author=\"Roman Joeres\",\n    maintainer=\"Roman Joeres\",\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Intended Audience :: Science/Research\",\n        \"Natural Language :: English\",\n        \"Topic :: Scientific/Engineering :: Bio-Informatics\",\n    ],\n    packages=[\"datasail\"],\n    include_package_data=True,\n    # packages=find_packages(),\n    # include_package_data=False,\n    install_requires=[],\n    python_requires=\">=3.8, <4.0.0\",\n    keywords=\"bioinformatics\",\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/dcache",
            "repo_link": "https://github.com/dCache/dcache",
            "content": {
                "codemeta": "",
                "readme": "common: Test commit\nFritz Heiden\n\ndCache\n======\n\n<img src=\"dCache.png\" height=\"165\" width=\"200\">\n\n__dCache__ is a system for storing and retrieving huge amounts of data,\ndistributed among a large number of heterogeneous server nodes, under\na single virtual filesystem tree with a variety of standard access\nmethods. Depending on the Persistency Model, dCache provides methods\nfor exchanging data with backend (tertiary) Storage Systems as well\nas space management, pool attraction, dataset replication, hot spot\ndetermination and recovery from disk or node failures. Connected to\na tertiary storage system, the cache simulates unlimited direct\naccess storage space. Data exchanges to and from the underlying HSM\nare performed automatically and invisibly to the user. Beside HEP\nspecific protocols, data in dCache can be accessed via __NFSv4.1\n(pNFS)__, __FTP__ as well as through __WebDav__.\n\n[![DOI](https://zenodo.org/badge/9113580.svg)](https://zenodo.org/badge/latestdoi/9113580)\n\nDocumentation\n=============\n\n[The dCache book](docs/TheBook/src/main/markdown/index.md)\n\n[User Guide](docs/UserGuide/src/main/markdown/index.md)\n\nGetting Started\n===============\n\nThe file [BUILDING.md](BUILDING.md) describes how to compile dCache\ncode and build various packages.\n\nThe file also describes how to create the __system-test__ deployment,\nwhich provides a quick and easy way to get a working dCache.  Running\nsystem-test requires no special privileges and all the generated files\nreside within the code-base.\n\nThere are also packages of stable releases at https://www.dcache.org/downloads/.\n\nLicense\n=======\n\nThe project is licensed under __AGPL v3__. Some parts licensed under __BSD__ and __LGPL__. See the source code for details.\n\nFor more info, check the official [dCache.ORG](http://www.dcache.org) web page.\n\nContributors\n============\ndCache is a joint effort between\n[Deutsches Elektronen-Synchrotron DESY](http://www.desy.de),\n[Fermi National Accelerator Laboratory](http://www.fnal.gov)\nand [Nordic DataGrid Facility](http://www.ndgf.org).\n\nContributions are welcome! Please check out our [CONTRIBUTING guide](CONTRIBUTING.md).\n\n",
                "dependencies": "<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>org.dcache</groupId>\n    <version>11.0.0-SNAPSHOT</version>\n    <artifactId>dcache-parent</artifactId>\n    <packaging>pom</packaging>\n\n    <name>dCache parent</name>\n    <url>http://www.dcache.org/</url>\n\n    <licenses>\n        <license>\n            <name>GNU Affero General Public License Version 3</name>\n            <url>http://www.gnu.org/licenses/agpl-3.0.txt</url>\n            <distribution>manual</distribution>\n        </license>\n        <license>\n            <name>GNU Lesser General Public License Version 3</name>\n            <url>http://www.gnu.org/licenses/lgpl-3.0.txt</url>\n            <distribution>manual</distribution>\n        </license>\n        <license>\n            <name>Fermitools Software Legal Information (Modified BSD License)</name>\n            <distribution>manual</distribution>\n        </license>\n    </licenses>\n\n    <organization>\n        <name>dCache.org</name>\n        <url>http://www.dcache.org/</url>\n    </organization>\n\n    <ciManagement>\n        <system>jenkins</system>\n        <url>https://ci.dcache.org/job/dCache-master/</url>\n    </ciManagement>\n\n    <distributionManagement>\n        <downloadUrl>https://download.dcache.org/nexus/content/repositories/releases/</downloadUrl>\n\n        <repository>\n            <uniqueVersion>false</uniqueVersion>\n            <id>dcache.release.repository</id>\n            <name>dCache.org release repository</name>\n            <url>https://download.dcache.org/nexus/content/repositories/releases/</url>\n            <layout>default</layout>\n        </repository>\n        <snapshotRepository>\n            <uniqueVersion>true</uniqueVersion>\n            <id>dcache.snapshot.repository</id>\n            <name>dCache.org snapshot repository</name>\n            <url>https://download.dcache.org/nexus/content/repositories/snapshots/</url>\n            <layout>default</layout>\n        </snapshotRepository>\n    </distributionManagement>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <java.version>17</java.version>\n        <argLine>--add-modules=java.security.jgss</argLine>\n\n\n        <version.slf4j>1.7.32</version.slf4j>\n        <version.milton>2.7.3.0-dcache-1</version.milton>\n        <version.spring>5.3.39</version.spring>\n        <!-- Remember to sync aspectj version in dcache.properties -->\n        <version.aspectj>1.9.21.2</version.aspectj>\n        <version.smc>6.6.0</version.smc>\n        <version.xerces>2.12.0</version.xerces>\n        <version.jetty>9.4.55.v20240627</version.jetty>\n        <version.xrootd4j>4.6.0</version.xrootd4j>\n        <version.jersey>2.41</version.jersey>\n        <version.dcache-view>3.0.0</version.dcache-view>\n        <version.dcache-view-admin>3.0.0</version.dcache-view-admin>\n        <version.netty>4.1.108.Final</version.netty>\n        <version.dcache>${project.version}</version.dcache>\n        <version.swagger-ui>3.23.11</version.swagger-ui>\n\n        <!-- Remember to update modules/logback-test-config/pom.xml\n             when upgrading logback. -->\n        <version.logback>1.2.13</version.logback>\n        <version.logback.contrib>0.1.5</version.logback.contrib>\n        <version.jackson>2.14.0</version.jackson>\n        <version.jna>5.4.0</version.jna>\n        <jmh.version>1.35</jmh.version>\n        <version.dropwizard>4.1.29</version.dropwizard>\n        <version.nfs4j>0.26.0</version.nfs4j>\n\n\n\n\n        <version.spring-integration-kafka>5.5.11</version.spring-integration-kafka>\n        <version.spring_kafka>2.9.11</version.spring_kafka>\n        <version.kafka>3.1.0</version.kafka>\n\n        <bouncycastle.bcprov>bcprov-jdk18on</bouncycastle.bcprov>\n        <bouncycastle.bcpkix>bcpkix-jdk18on</bouncycastle.bcpkix>\n        <bouncycastle.version>1.78</bouncycastle.version>\n\n        <datanucleus-core.version>6.0.6</datanucleus-core.version>\n        <datanucleus.plugin.version>6.0.0-release</datanucleus.plugin.version>\n        <asm.version>9.5</asm.version>\n        <asm-tools.version>5.0.3</asm-tools.version>\n    </properties>\n\n    <scm>\n        <url>https://github.com/dCache/dcache</url>\n        <connection>scm:git:https://github.com/dCache/dcache.git</connection>\n        <developerConnection>scm:git:ssh://git@github.com/dCache/dcache.git</developerConnection>\n      <tag>8.2</tag>\n  </scm>\n\n    <repositories>\n        <repository>\n            <id>dcache.repository</id>\n            <url>https://download.dcache.org/nexus/content/groups/public</url>\n        </repository>\n    </repositories>\n\n    <pluginRepositories>\n        <pluginRepository>\n            <id>dcache.repository</id>\n            <url>https://download.dcache.org/nexus/content/groups/public</url>\n        </pluginRepository>\n    </pluginRepositories>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.slf4j</groupId>\n                <artifactId>slf4j-api</artifactId>\n                <version>${version.slf4j}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.slf4j</groupId>\n                <artifactId>jul-to-slf4j</artifactId>\n                <version>${version.slf4j}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.slf4j</groupId>\n                <artifactId>jcl-over-slf4j</artifactId>\n                <version>${version.slf4j}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.slf4j</groupId>\n                <artifactId>log4j-over-slf4j</artifactId>\n                <version>${version.slf4j}</version>\n            </dependency>\n            <dependency>\n                <groupId>ch.qos.logback</groupId>\n                <artifactId>logback-classic</artifactId>\n                <version>${version.logback}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>ch.qos.logback</groupId>\n                <artifactId>logback-core</artifactId>\n                <version>${version.logback}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.codehaus.janino</groupId>\n                <artifactId>janino</artifactId>\n                <version>3.0.6</version>\n            </dependency>\n            <dependency>\n                <groupId>org.apache.curator</groupId>\n                <artifactId>curator-recipes</artifactId>\n                <version>5.7.0</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>org.slf4j</groupId>\n                        <artifactId>slf4j-log4j12</artifactId>\n                    </exclusion>\n                    <exclusion>\n                        <groupId>org.apache.zookeeper</groupId>\n                        <artifactId>zookeeper</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <dependency>\n                <groupId>org.apache.zookeeper</groupId>\n                <artifactId>zookeeper</artifactId>\n                <version>3.8.4</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>com.sun.jmx</groupId>\n                        <artifactId>jmxri</artifactId>\n                    </exclusion>\n                    <exclusion>\n                        <groupId>com.sun.jdmk</groupId>\n                        <artifactId>jmxtools</artifactId>\n                    </exclusion>\n                    <exclusion>\n                        <groupId>javax.jms</groupId>\n                        <artifactId>jms</artifactId>\n                    </exclusion>\n                    <exclusion>\n                        <groupId>org.slf4j</groupId>\n                        <artifactId>slf4j-log4j12</artifactId>\n                    </exclusion>\n                    <exclusion>\n                        <groupId>log4j</groupId>\n                        <artifactId>log4j</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <!-- Required for ZooKeeper 3.6.x https://issues.apache.org/jira/browse/ZOOKEEPER-3647 -->\n            <dependency>\n                <groupId>io.dropwizard.metrics</groupId>\n                <artifactId>metrics-core</artifactId>\n                <version>${version.dropwizard}</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>org.slf4j</groupId>\n                        <artifactId>slf4j-api</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <dependency>\n                <groupId>jline</groupId>\n                <artifactId>jline</artifactId>\n                <version>2.14.6</version>\n            </dependency>\n            <dependency>\n                <groupId>com.github.spotbugs</groupId>\n                <artifactId>spotbugs-annotations</artifactId>\n                <version>3.1.12</version>\n                <scope>compile</scope>\n            </dependency>\n            <dependency>\n                <groupId>com.google.code.findbugs</groupId>\n                <artifactId>jsr305</artifactId>\n                <version>2.0.1</version>\n            </dependency>\n            <dependency>\n                <groupId>com.google.guava</groupId>\n                <artifactId>guava</artifactId>\n                <version>32.0.0-jre</version>\n            </dependency>\n            <dependency>\n                <groupId>eu.eu-emi.security</groupId>\n                <artifactId>canl</artifactId>\n                <version>2.8.3</version>\n            </dependency>\n            <dependency>\n                <groupId>org.italiangrid</groupId>\n                <artifactId>voms-api-java</artifactId>\n                <version>3.3.0</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>org.bouncycastle</groupId>\n                        <artifactId>bcprov-jdk15on</artifactId>\n                    </exclusion>\n                    <exclusion>\n                        <groupId>org.bouncycastle</groupId>\n                        <artifactId>bcpkix-jdk15on</artifactId>\n                    </exclusion>\n                    <exclusion>\n                        <groupId>eu.eu-emi.security</groupId>\n                        <artifactId>canl</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <dependency>\n                <groupId>gov.fnal</groupId>\n                <artifactId>vox-alldepends</artifactId>\n                <version>1.0</version>\n            </dependency>\n            <dependency>\n                <groupId>gov.fnal</groupId>\n                <artifactId>vox-security</artifactId>\n                <version>1.0</version>\n            </dependency>\n            <dependency>\n                <groupId>net.java.dev.jna</groupId>\n                <artifactId>jna</artifactId>\n                <version>${version.jna}</version>\n            </dependency>\n            <dependency>\n                <groupId>net.java.dev.jna</groupId>\n                <artifactId>jna-platform</artifactId>\n                <version>${version.jna}</version>\n            </dependency>\n            <dependency>\n                <groupId>javax.xml</groupId>\n                <artifactId>jaxrpc-api</artifactId>\n                <version>1.1</version>\n            </dependency>\n            <dependency>\n                <groupId>com.sun.xml.rpc</groupId>\n                <artifactId>jaxrpc-impl</artifactId>\n                <version>1.1.3_01</version>\n            </dependency>\n            <dependency>\n                <groupId>com.sun.xml.rpc</groupId>\n                <artifactId>jaxrpc-spi</artifactId>\n                <version>1.1.3_01</version>\n            </dependency>\n            <dependency>\n                <groupId>axis</groupId>\n                <artifactId>axis</artifactId>\n                <version>1.4</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>commons-logging</groupId>\n                        <artifactId>commons-logging</artifactId>\n                    </exclusion>\n                    <!-- 'servlet-api' conflicts with Jetty-supplied\n                         'javax.servlet' jar. -->\n                    <exclusion>\n                        <groupId>javax.servlet</groupId>\n                        <artifactId>servlet-api</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <dependency>\n                <groupId>terapaths</groupId>\n                <artifactId>example-client</artifactId>\n                <version>1.0</version>\n            </dependency>\n            <dependency>\n                <groupId>org.jdom</groupId>\n                <artifactId>jdom</artifactId>\n                <version>1.1</version>\n            </dependency>\n            <dependency>\n                <groupId>com.zaxxer</groupId>\n                <artifactId>HikariCP</artifactId>\n                <version>5.0.1</version>\n            </dependency>\n            <dependency>\n                <groupId>org.postgresql</groupId>\n                <artifactId>postgresql</artifactId>\n                <version>42.3.9</version>\n            </dependency>\n            <dependency>\n                <groupId>io.milton</groupId>\n                <artifactId>milton-server-ce</artifactId>\n                <version>${version.milton}</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>commons-logging</groupId>\n                        <artifactId>commons-logging</artifactId>\n                    </exclusion>\n                    <exclusion>\n                        <groupId>io.milton</groupId>\n                        <artifactId>milton-mail-api</artifactId>\n                    </exclusion>\n                    <exclusion>\n                        <groupId>io.milton</groupId>\n                        <artifactId>milton-mail-server</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <dependency>\n                <groupId>io.netty</groupId>\n                <artifactId>netty-codec-http</artifactId>\n                <version>${version.netty}</version>\n            </dependency>\n            <dependency>\n                <groupId>io.netty</groupId>\n                <artifactId>netty-codec-haproxy</artifactId>\n                <version>${version.netty}</version>\n            </dependency>\n            <dependency>\n                <groupId>io.netty</groupId>\n                <artifactId>netty-handler</artifactId>\n                <version>${version.netty}</version>\n            </dependency>\n            <dependency>\n                <groupId>io.netty</groupId>\n                <artifactId>netty-tcnative-boringssl-static</artifactId>\n                <version>2.0.48.Final</version>\n            </dependency>\n            <dependency>\n                <groupId>com.sleepycat</groupId>\n                <artifactId>je</artifactId>\n                <version>7.5.11</version>\n            </dependency>\n            <dependency>\n                <groupId>com.github.parboiled1</groupId>\n                <artifactId>grappa</artifactId>\n                <version>1.0.4</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>org.ow2.asm</groupId>\n                        <artifactId>asm-debug-all</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <dependency>\n                <groupId>org.ow2.asm</groupId>\n                <artifactId>asm</artifactId>\n                <version>${asm.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.ow2.asm</groupId>\n                <artifactId>asm-tree</artifactId>\n                <version>${asm-tools.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.ow2.asm</groupId>\n                <artifactId>asm-analysis</artifactId>\n                <version>${asm-tools.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.ow2.asm</groupId>\n                <artifactId>asm-util</artifactId>\n                <version>${asm-tools.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.datanucleus</groupId>\n                <artifactId>datanucleus-core</artifactId>\n                <version>${datanucleus-core.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.datanucleus</groupId>\n                <artifactId>datanucleus-api-jdo</artifactId>\n                <version>6.0.1</version>\n            </dependency>\n            <dependency>\n                <groupId>org.datanucleus</groupId>\n                <artifactId>datanucleus-jdo-query</artifactId>\n                <version>6.0.1</version>\n            </dependency>\n            <dependency>\n                <groupId>org.datanucleus</groupId>\n                <artifactId>javax.jdo</artifactId>\n                <version>3.2.0-release</version>\n            </dependency>\n            <dependency>\n                <groupId>org.datanucleus</groupId>\n                <artifactId>datanucleus-rdbms</artifactId>\n                <version>${datanucleus-core.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.datanucleus</groupId>\n                <artifactId>datanucleus-xml</artifactId>\n                <version>6.0.0-release</version>\n            </dependency>\n            <dependency>\n                <groupId>org.datanucleus</groupId>\n                <artifactId>datanucleus-cache</artifactId>\n                <version>6.0.0-release</version>\n            </dependency>\n            <dependency>\n                <groupId>net.sf.saxon</groupId>\n                <artifactId>saxon-dom</artifactId>\n                <version>8.7</version>\n            </dependency>\n            <dependency>\n                <groupId>org.eclipse.jetty</groupId>\n                <artifactId>jetty-util</artifactId>\n                <version>${version.jetty}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.eclipse.jetty</groupId>\n                <artifactId>jetty-io</artifactId>\n                <version>${version.jetty}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.eclipse.jetty</groupId>\n                <artifactId>jetty-server</artifactId>\n                <version>${version.jetty}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.eclipse.jetty</groupId>\n                <artifactId>jetty-deploy</artifactId>\n                <version>${version.jetty}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.eclipse.jetty</groupId>\n                <artifactId>jetty-webapp</artifactId>\n                <version>${version.jetty}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.eclipse.jetty</groupId>\n                <artifactId>jetty-servlets</artifactId>\n                <version>${version.jetty}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.eclipse.jetty</groupId>\n                <artifactId>jetty-plus</artifactId>\n                <version>${version.jetty}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.eclipse.jetty</groupId>\n                <artifactId>jetty-security</artifactId>\n                <version>${version.jetty}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.eclipse.jetty</groupId>\n                <artifactId>jetty-rewrite</artifactId>\n                <version>${version.jetty}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.eclipse.jetty.toolchain</groupId>\n                <artifactId>jetty-schemas</artifactId>\n                <version>3.1</version>\n            </dependency>\n\n            <dependency>\n                <groupId>org.glassfish.jersey.containers</groupId>\n                <artifactId>jersey-container-servlet</artifactId>\n                <version>${version.jersey}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.glassfish.jersey.core</groupId>\n                <artifactId>jersey-client</artifactId>\n                <version>${version.jersey}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.glassfish.jersey.media</groupId>\n                <artifactId>jersey-media-json-jackson</artifactId>\n                <version>${version.jersey}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.glassfish.jersey.ext</groupId>\n                <artifactId>jersey-spring5</artifactId>\n                <version>${version.jersey}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.glassfish.jersey.media</groupId>\n                <artifactId>jersey-media-sse</artifactId>\n                <version>${version.jersey}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.antlr</groupId>\n                <artifactId>ST4</artifactId>\n                <version>4.0.8</version>\n            </dependency>\n\t    <dependency>\n\t      <groupId>org.apache.sshd</groupId>\n\t      <artifactId>sshd-core</artifactId>\n\t      <version>2.13.1</version>\n\t    </dependency>\n        <!-- For ed25519 support -->\n        <dependency>\n            <groupId>net.i2p.crypto</groupId>\n            <artifactId>eddsa</artifactId>\n            <version>0.3.0</version>\n        </dependency>\n            <dependency>\n                <!-- Newer versions have two problems. A performance regression causes it to block on non-responding network\n                     interfaces (https://liquibase.jira.com/browse/CORE-2549). There is also an as of yet unreported regression\n                     affecting how quoting rules are applied during rollback. -->\n\t\t<!-- A separate issue was found with liquibase 3.5.4\n\t\t     where the 'chimera' shell's writetag command\n\t\t     failed on PostgreSQL 10.2 with the error:\n\n\t\t         ERROR: column \"ivalue\" is of type oid but\n\t\t         expression is of type bytea\n\t\t\t Hint: You will need to rewrite or cast the expression.\n\t\t     -->\n                <groupId>org.liquibase</groupId>\n                <artifactId>liquibase-core</artifactId>\n                <version>3.3.2</version>\n            </dependency>\n            <dependency>\n                <groupId>com.mattbertolini</groupId>\n                <artifactId>liquibase-slf4j</artifactId>\n                <version>2.0.0</version>\n            </dependency>\n\n            <dependency>\n                <groupId>org.springframework</groupId>\n                <artifactId>spring-beans</artifactId>\n                <version>${version.spring}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework</groupId>\n                <artifactId>spring-context</artifactId>\n                <version>${version.spring}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework</groupId>\n                <artifactId>spring-core</artifactId>\n                <version>${version.spring}</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>commons-logging</groupId>\n                        <artifactId>commons-logging</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework</groupId>\n                <artifactId>spring-expression</artifactId>\n                <version>${version.spring}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework</groupId>\n                <artifactId>spring-jdbc</artifactId>\n                <version>${version.spring}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework</groupId>\n                <artifactId>spring-orm</artifactId>\n                <version>${version.spring}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework</groupId>\n                <artifactId>spring-web</artifactId>\n                <version>${version.spring}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework</groupId>\n                <artifactId>spring-aspects</artifactId>\n                <version>${version.spring}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework</groupId>\n                <artifactId>spring-tx</artifactId>\n                <version>${version.spring}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>org.aspectj</groupId>\n                <artifactId>aspectjrt</artifactId>\n                <version>${version.aspectj}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.aspectj</groupId>\n                <artifactId>aspectjweaver</artifactId>\n                <version>${version.aspectj}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>net.sf.smc</groupId>\n                <artifactId>statemap</artifactId>\n                <version>${version.smc}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>com.h2database</groupId>\n                <artifactId>h2</artifactId>\n                <version>1.4.199</version>\n            </dependency>\n\n            <dependency>\n                <!-- https://sourceforge.net/p/hsqldb/bugs/1341/ prevents updating to 2.3.2 -->\n                <groupId>org.hsqldb</groupId>\n                <artifactId>hsqldb</artifactId>\n                <version>2.3.1</version>\n            </dependency>\n\n            <dependency>\n                <groupId>java.jndi</groupId>\n                <artifactId>nis</artifactId>\n                <version>1.2.1</version>\n            </dependency>\n\n            <dependency>\n                <groupId>org.json</groupId>\n                <artifactId>json</artifactId>\n                <version>20231013</version>\n            </dependency>\n\n            <dependency>\n                <groupId>com.thaiopensource</groupId>\n                <artifactId>jing</artifactId>\n                <version>20091111</version>\n                <scope>test</scope>\n                <exclusions>\n                    <exclusion>\n                        <groupId>xml-apis</groupId>\n                        <artifactId>xml-apis</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n\n            <dependency>\n                <groupId>org.dcache</groupId>\n                <artifactId>xrootd4j</artifactId>\n                <version>${version.xrootd4j}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.dcache</groupId>\n                <artifactId>xrootd4j-gsi</artifactId>\n                <version>${version.xrootd4j}</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>org.bouncycastle</groupId>\n                        <artifactId>bcprov-jdk15on</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <dependency>\n                <groupId>org.dcache</groupId>\n                <artifactId>xrootd4j-unix</artifactId>\n                <version>${version.xrootd4j}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.dcache</groupId>\n                <artifactId>xrootd4j-scitokens</artifactId>\n                <version>${version.xrootd4j}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.dcache</groupId>\n                <artifactId>xrootd4j-ztn</artifactId>\n                <version>${version.xrootd4j}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.dcache</groupId>\n                <artifactId>xrootd4j-authz-plugin-alice</artifactId>\n                <version>1.2.0</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>org.bouncycastle</groupId>\n                        <artifactId>bcprov-jdk15on</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n\n            <dependency>\n                <groupId>org.dcache</groupId>\n                <artifactId>dcache-view</artifactId>\n                <version>${version.dcache-view}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>org.dcache</groupId>\n                <artifactId>dcache-view-admin</artifactId>\n                <version>${version.dcache-view-admin}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>org.bouncycastle</groupId>\n                <artifactId>${bouncycastle.bcpkix}</artifactId>\n                <version>${bouncycastle.version}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>org.bouncycastle</groupId>\n                <artifactId>${bouncycastle.bcprov}</artifactId>\n                <version>${bouncycastle.version}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>xerces</groupId>\n                <artifactId>xercesImpl</artifactId>\n                <version>${version.xerces}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>com.google.code.gson</groupId>\n                <artifactId>gson</artifactId>\n                <version>2.8.9</version>\n            </dependency>\n\n            <dependency>\n              <groupId>org.apache.commons</groupId>\n              <artifactId>commons-compress</artifactId>\n              <version>1.26.0</version>\n            </dependency>\n            <dependency>\n                <groupId>org.apache.commons</groupId>\n                <artifactId>commons-math3</artifactId>\n                <version>3.6.1</version>\n            </dependency>\n            <dependency>\n                <groupId>org.python</groupId>\n                <artifactId>jython-standalone</artifactId>\n                <version>2.7.3</version>\n            </dependency>\n        <dependency>\n            <groupId>org.dcache</groupId>\n            <artifactId>nfs4j-core</artifactId>\n            <version>${version.nfs4j}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.dcache</groupId>\n            <artifactId>rquota</artifactId>\n            <version>${version.nfs4j}</version>\n        </dependency>\n        <dependency>\n            <groupId>com.github.nitram509</groupId>\n            <artifactId>jmacaroons</artifactId>\n            <version>0.3.1</version>\n        </dependency>\n            <dependency>\n                <groupId>org.apache.httpcomponents</groupId>\n                <artifactId>httpclient</artifactId>\n                <version>4.5.13</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>commons-logging</groupId>\n                        <artifactId>commons-logging</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <dependency>\n                <groupId>org.apache.httpcomponents</groupId>\n                <artifactId>httpcore</artifactId>\n                <version>4.4.6</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>commons-logging</groupId>\n                        <artifactId>commons-logging</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <dependency>\n                <groupId>com.google.jimfs</groupId>\n                <artifactId>jimfs</artifactId>\n                <version>1.1</version>\n            </dependency>\n            <dependency>\n                <groupId>org.apache.maven.archetype</groupId>\n                <artifactId>archetype-packaging</artifactId>\n                <version>2.4</version>\n            </dependency>\n            <dependency>\n                <groupId>junit</groupId>\n                <artifactId>junit</artifactId>\n                <version>4.13.1</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>org.hamcrest</groupId>\n                        <artifactId>hamcrest-core</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <dependency>\n                <groupId>org.dcache</groupId>\n                <artifactId>ldap4testing</artifactId>\n                <version>1.0</version>\n            </dependency>\n            <!-- used by alarms, needs to be deployed -->\n            <dependency>\n                <groupId>javax.mail</groupId>\n                <artifactId>mail</artifactId>\n                <version>1.4.7</version>\n            </dependency>\n            <dependency>\n                <groupId>org.mongodb</groupId>\n                <artifactId>mongodb-driver-sync</artifactId>\n                <version>4.11.1</version>\n            </dependency>\n\n            <dependency>\n                <groupId>org.springframework.integration</groupId>\n                <artifactId>spring-integration-kafka</artifactId>\n                <version>${version.spring-integration-kafka}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework.kafka</groupId>\n                <artifactId>spring-kafka</artifactId>\n                <exclusions>\n                    <exclusion>\n                        <groupId>org.slf4j</groupId>\n                        <artifactId>slf4j-log4j12</artifactId>\n                    </exclusion>\n                </exclusions>\n                <version>${version.spring_kafka}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.apache.kafka</groupId>\n                <artifactId>kafka_2.12</artifactId>\n                <version>${version.kafka}</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>org.apache.zookeeper</groupId>\n                        <artifactId>zookeeper</artifactId>\n                    </exclusion>\n                    <exclusion>\n                        <groupId>org.slf4j</groupId>\n                        <artifactId>slf4j-log4j12</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <dependency>\n                <groupId>org.apache.kafka</groupId>\n                <artifactId>kafka-clients</artifactId>\n                <exclusions>\n                    <exclusion>\n                        <groupId>org.apache.zookeeper</groupId>\n                        <artifactId>zookeeper</artifactId>\n                    </exclusion>\n                    <exclusion>\n                        <groupId>org.slf4j</groupId>\n                        <artifactId>slf4j-log4j12</artifactId>\n                    </exclusion>\n                </exclusions>\n                <version>${version.kafka}</version>\n            </dependency>\n            <dependency>\n                <groupId>com.github.danielwegener</groupId>\n                <artifactId>logback-kafka-appender</artifactId>\n                <version>0.2.0-RC1</version>\n            </dependency>\n\n            <!-- ch.qos.logback.contrib.jackson.JacksonJsonFormatter -->\n            <dependency>\n                <groupId>ch.qos.logback.contrib</groupId>\n                <artifactId>logback-jackson</artifactId>\n                <version>${version.logback.contrib}</version>\n            </dependency>\n\n            <!-- ch.qos.logback.contrib.json.classic.JsonLayout -->\n            <dependency>\n                <groupId>ch.qos.logback.contrib</groupId>\n                <artifactId>logback-json-classic</artifactId>\n                <version>${version.logback.contrib}</version>\n            </dependency>\n\n            <!-- com.fasterxml.jackson.databind.ObjectMapper -->\n            <dependency>\n                <groupId>com.fasterxml.jackson.core</groupId>\n                <artifactId>jackson-databind</artifactId>\n                <version>${version.jackson}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>com.fasterxml.jackson.core</groupId>\n                <artifactId>jackson-annotations</artifactId>\n                <version>${version.jackson}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>com.fasterxml.jackson.core</groupId>\n                <artifactId>jackson-core</artifactId>\n                <version>${version.jackson}</version>\n            </dependency>\n\n            <dependency>\n                <groupId>org.glassfish</groupId>\n                <artifactId>javax.json</artifactId>\n                <version>1.0.4</version>\n            </dependency>\n            <dependency>\n                <groupId>org.webjars</groupId>\n                <artifactId>swagger-ui</artifactId>\n                <version>${version.swagger-ui}</version>\n            </dependency>\n            <dependency>\n                <groupId>io.swagger</groupId>\n                <artifactId>swagger-jersey2-jaxrs</artifactId>\n                <version>1.6.2</version>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework.plugin</groupId>\n                <artifactId>spring-plugin-core</artifactId>\n                <version>1.2.0.RELEASE</version>\n            </dependency>\n            <dependency>\n                <!-- enforce version as javassist used powermock -->\n                <groupId>org.javassist</groupId>\n                <artifactId>javassist</artifactId>\n                <version>3.25.0-GA</version>\n            </dependency>\n            <dependency>\n                <groupId>org.reflections</groupId>\n                <artifactId>reflections</artifactId>\n                <version>0.9.12</version>\n            </dependency>\n\t    <dependency>\n                <groupId>com.github.npathai</groupId>\n                <artifactId>hamcrest-optional</artifactId>\n                <version>2.0.0</version>\n                <exclusions>\n                    <exclusion>\n                        <groupId>org.hamcrest</groupId>\n                        <artifactId>hamcrest-core</artifactId>\n                    </exclusion>\n                </exclusions>\n            </dependency>\n            <dependency>\n                <groupId>org.hamcrest</groupId>\n                <artifactId>hamcrest</artifactId>\n                <version>2.2</version>\n            </dependency>\n            <dependency>\n                <groupId>org.openjdk.jmh</groupId>\n                <artifactId>jmh-core</artifactId>\n                <version>${jmh.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.openjdk.jmh</groupId>\n                <artifactId>jmh-generator-annprocess</artifactId>\n                <version>${jmh.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>com.github.erosb</groupId>\n                <artifactId>everit-json-schema</artifactId>\n                <version>1.14.1</version>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.hamcrest</groupId>\n            <artifactId>hamcrest</artifactId>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n           <groupId>org.powermock</groupId>\n            <artifactId>powermock-api-mockito2</artifactId>\n            <version>2.0.4</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.mockito</groupId>\n            <artifactId>mockito-core</artifactId>\n            <version>3.2.4</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.powermock</groupId>\n            <artifactId>powermock-module-junit4</artifactId>\n            <version>2.0.4</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.dcache</groupId>\n            <artifactId>logback-test-config</artifactId>\n            <version>${project.version}</version>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <pluginManagement>\n            <plugins>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-site-plugin</artifactId>\n                    <version>4.0.0-M5</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-compiler-plugin</artifactId>\n                    <version>3.10.1</version>\n                    <configuration>\n                        <showDeprecation>true</showDeprecation>\n                        <release>${java.version}</release>\n                    </configuration>\n                </plugin>\n                <plugin>\n                    <!-- https://github.com/mojohaus/aspectj-maven-plugin/pull/45 -->\n                    <!-- <groupId>org.codehaus.mojo</groupId> -->\n                    <groupId>com.nickwongdev</groupId>\n                    <artifactId>aspectj-maven-plugin</artifactId>\n                    <version>1.12.6</version>\n                    <dependencies>\n                        <dependency>\n                            <groupId>org.aspectj</groupId>\n                            <artifactId>aspectjtools</artifactId>\n                            <version>${version.aspectj}</version>\n                        </dependency>\n                    </dependencies>\n                    <configuration>\n                        <XterminateAfterCompilation>true</XterminateAfterCompilation>\n                        <source>${java.version}</source>\n                        <target>${java.version}</target>\n                        <complianceLevel>${java.version}</complianceLevel>\n                        <XhasMember>true</XhasMember>\n                        <sources>\n                            <source>\n                                <basedir>${project.basedir}/src/main/aspect</basedir>\n                            </source>\n                        </sources>\n                    </configuration>\n                    <executions>\n                        <execution>\n                            <goals>\n                                <goal>compile</goal>\n                            </goals>\n                        </execution>\n                    </executions>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-resources-plugin</artifactId>\n                    <!-- Cannot update to 3.2.0 or higher due to https://github.com/spring-projects/spring-boot/issues/24346 -->\n                    <!-- Not UTF-8 Characters are not being tolerated by the plugin and hence causing the issue -->\n                    <version>3.1.0</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-dependency-plugin</artifactId>\n                    <version>3.5.0</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-assembly-plugin</artifactId>\n                    <version>3.5.0</version>\n                    <configuration>\n                        <tarLongFileMode>gnu</tarLongFileMode>\n                    </configuration>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-war-plugin</artifactId>\n                    <version>3.3.2</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-clean-plugin</artifactId>\n                    <version>3.2.0</version>\n                    <configuration>\n                        <fast>true</fast>\n                    </configuration>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-jar-plugin</artifactId>\n                    <version>3.3.0</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.codehaus.mojo</groupId>\n                    <artifactId>buildnumber-maven-plugin</artifactId>\n                    <version>3.0.0</version>\n                </plugin>\n                <plugin>\n                    <groupId>com.lukegb.mojo</groupId>\n                    <artifactId>gitdescribe-maven-plugin</artifactId>\n                    <version>3.0</version>\n                </plugin>\n                <plugin>\n                  <groupId>org.codehaus.mojo</groupId>\n                  <artifactId>build-helper-maven-plugin</artifactId>\n                  <version>3.3.0</version>\n                </plugin>\n                <plugin>\n                  <groupId>org.codehaus.mojo</groupId>\n                  <artifactId>exec-maven-plugin</artifactId>\n                  <version>3.1.0</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-surefire-plugin</artifactId>\n                    <version>3.1.2</version>\n                    <configuration>\n                        <!-- Disable stack-trace trimming as\n                             surefire makes bad decisions too often. -->\n                        <trimStackTrace>false</trimStackTrace>\n                    </configuration>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-release-plugin</artifactId>\n                    <version>3.0.0-M7</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-deploy-plugin</artifactId>\n                    <version>3.1.0</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-install-plugin</artifactId>\n                    <version>3.1.0</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.datanucleus</groupId>\n                    <artifactId>datanucleus-maven-plugin</artifactId>\n                    <version>${datanucleus.plugin.version}</version>\n                    <dependencies>\n                        <dependency>\n                            <groupId>org.datanucleus</groupId>\n                            <artifactId>datanucleus-core</artifactId>\n                            <version>${datanucleus-core.version}</version>\n                        </dependency>\n                        <dependency>\n                            <groupId>org.slf4j</groupId>\n                            <artifactId>log4j-over-slf4j</artifactId>\n                            <version>${version.slf4j}</version>\n                        </dependency>\n                        <dependency>\n                            <groupId>org.dcache</groupId>\n                            <artifactId>logback-test-config</artifactId>\n                            <version>${project.version}</version>\n                        </dependency>\n                    </dependencies>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-antrun-plugin</artifactId>\n                    <version>3.1.0</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-failsafe-plugin</artifactId>\n                    <version>3.0.0-M9</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-pmd-plugin</artifactId>\n                    <version>3.20.0</version>\n                    <configuration>\n                        <sourceEncoding>utf-8</sourceEncoding>\n                        <minimumTokens>100</minimumTokens>\n                        <targetJdk>${java.version}</targetJdk>\n                        <!-- Rely on Jenkins plugin to do source-code cross-referencing -->\n                        <linkXRef>false</linkXRef>\n                    </configuration>\n                </plugin>\n                <plugin>\n                    <groupId>org.codehaus.mojo</groupId>\n                    <artifactId>axistools-maven-plugin</artifactId>\n                    <version>1.4</version>\n                </plugin>\n                <plugin>\n                    <groupId>io.github.lukasmansour</groupId>\n                    <artifactId>patch-maven-plugin</artifactId>\n                    <version>1.0</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.jacoco</groupId>\n                    <artifactId>jacoco-maven-plugin</artifactId>\n                    <version>0.8.8</version>\n                </plugin>\n                <plugin>\n                    <groupId>com.ruleoftech</groupId>\n                    <artifactId>markdown-page-generator-plugin</artifactId>\n                    <version>2.4.0</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.codehaus.gmaven</groupId>\n                    <artifactId>groovy-maven-plugin</artifactId>\n                    <version>2.1.1</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-enforcer-plugin</artifactId>\n                    <!-- Remember to update version in\n                         modules/logback-test-config/pom.xml when\n                         upgrading -->\n                    <version>3.2.1</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.gaul</groupId>\n                    <artifactId>modernizer-maven-plugin</artifactId>\n                    <version>2.7.0</version>\n                </plugin>\n                <plugin>\n                    <groupId>com.github.spotbugs</groupId>\n                    <artifactId>spotbugs-maven-plugin</artifactId>\n                    <version>4.7.3.0</version>\n                </plugin>\n                <plugin>\n                    <groupId>de.chkal.maven</groupId>\n                    <artifactId>gitlab-code-quality-plugin</artifactId>\n                    <version>1.0.2</version>\n                </plugin>\n            </plugins>\n        </pluginManagement>\n\n        <plugins>\n            <plugin>\n                <groupId>org.gaul</groupId>\n                <artifactId>modernizer-maven-plugin</artifactId>\n                <configuration>\n                    <javaVersion>${java.version}</javaVersion>\n                    <failOnViolations>false</failOnViolations>\n                </configuration>\n                <executions>\n                    <execution>\n                        <id>modernizer</id>\n                        <phase>verify</phase>\n                        <goals>\n                            <goal>modernizer</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-enforcer-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <id>enforce-maven</id>\n                        <goals>\n                            <goal>enforce</goal>\n                        </goals>\n                        <configuration>\n                            <rules>\n                                <requireMavenVersion>\n                                    <version>3.5.0</version>\n                                </requireMavenVersion>\n                            </rules>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <configuration>\n                    <release>${java.version}</release>\n                </configuration>\n            </plugin>\n\n\n            <plugin>\n                <groupId>io.github.git-commit-id</groupId>\n                <artifactId>git-commit-id-maven-plugin</artifactId>\n                <version>5.0.0</version>\n                <executions>\n                    <execution>\n                        <goals>\n                        <goal>revision</goal>\n                        </goals>\n                    </execution>\n                </executions>\n                <configuration>\n                    <dotGitDirectory>${project.basedir}/.git</dotGitDirectory>\n                    <skipPoms>false</skipPoms>\n                    <injectAllReactorProjects>true</injectAllReactorProjects>\n                    <useNativeGit>false</useNativeGit>\n                    <gitDescribe>\n                        <always>true</always>\n                        <dirty>+dirty+${user.name}</dirty>\n                    </gitDescribe>\n                </configuration>\n            </plugin>\n\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-jar-plugin</artifactId>\n                <configuration>\n                    <archive>\n                        <index>true</index>\n                        <manifest>\n                            <addDefaultImplementationEntries>true</addDefaultImplementationEntries>\n                        </manifest>\n                        <manifestEntries>\n                            <Build-Time>${maven.build.timestamp}</Build-Time>\n                            <Implementation-Build>${git.commit.id.describe}</Implementation-Build>\n                            <Implementation-Branch>${git.branch}</Implementation-Branch>\n                        </manifestEntries>\n                    </archive>\n                </configuration>\n            </plugin>\n\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-surefire-plugin</artifactId>\n                <configuration>\n                    <includes>\n                        <include>**/*Test.class</include>\n                        <include>**/*Tests.class</include>\n                    </includes>\n                    <!-- dCache uses the singleton anti-pattern in way\n                         too many places. That unfortunately means we have\n                         to accept the overhead of forking each test run. -->\n                    <forkCount>1</forkCount>\n                    <reuseForks>false</reuseForks>\n                    <!-- Make Powermock compatible with JDK 17 -->\n                    <argLine>\n                        @{argLine}\n                        --add-opens java.base/java.lang=ALL-UNNAMED\n                        --add-opens java.base/java.util=ALL-UNNAMED\n                        --add-opens java.base/java.lang.reflect=ALL-UNNAMED\n                        --add-opens java.base/java.time=ALL-UNNAMED\n                        --add-opens java.base/java.time.format=ALL-UNNAMED\n                        --add-opens java.base/java.util.concurrent=ALL-UNNAMED\n                        --add-opens java.base/java.util.stream=ALL-UNNAMED\n                    </argLine>\n                </configuration>\n            </plugin>\n\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-release-plugin</artifactId>\n                <configuration>\n                    <autoVersionSubmodules>true</autoVersionSubmodules>\n                    <tagNameFormat>@{project.version}</tagNameFormat>\n                </configuration>\n            </plugin>\n            <plugin>\n                <groupId>com.github.spotbugs</groupId>\n                <artifactId>spotbugs-maven-plugin</artifactId>\n                <configuration>\n                    <omitVisitors>FindDeadLocalStores,UnreadFields,Naming,FindUncalledPrivateMethods,FormatStringChecker</omitVisitors>\n                </configuration>\n            </plugin>\n\n            <plugin>\n                <artifactId>maven-failsafe-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <goals>\n                            <goal>integration-test</goal>\n                            <goal>verify</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>de.chkal.maven</groupId>\n                <artifactId>gitlab-code-quality-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <phase>verify</phase>\n                        <goals>\n                            <goal>generate</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n\n    <reporting>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-project-info-reports-plugin</artifactId>\n                <version>3.4.2</version>\n                <reportSets>\n                    <reportSet>\n                        <reports>\n                            <report>cim</report>\n                            <report>scm</report>\n                        </reports>\n                    </reportSet>\n                </reportSets>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-pmd-plugin</artifactId>\n                <version>3.20.0</version>\n                <configuration>\n                    <sourceEncoding>utf-8</sourceEncoding>\n                    <minimumTokens>100</minimumTokens>\n                    <targetJdk>${java.version}</targetJdk>\n                </configuration>\n            </plugin>\n        </plugins>\n    </reporting>\n\n    <modules>\n        <module>modules/logback-test-config</module>\n        <module>modules/logback-console-config</module>\n        <module>modules/common</module>\n        <module>modules/common-cli</module>\n        <module>modules/common-security</module>\n        <module>modules/cells</module>\n        <module>modules/gplazma2</module>\n        <module>modules/gplazma2-alise</module>\n        <module>modules/gplazma2-fermi</module>\n        <module>modules/gplazma2-grid</module>\n        <module>modules/gplazma2-krb5</module>\n        <module>modules/gplazma2-jaas</module>\n        <module>modules/gplazma2-nis</module>\n        <module>modules/gplazma2-nsswitch</module>\n        <module>modules/gplazma2-banfile</module>\n        <module>modules/gplazma2-voms</module>\n        <module>modules/gplazma2-kpwd</module>\n        <module>modules/gplazma2-ldap</module>\n        <module>modules/gplazma2-htpasswd</module>\n        <module>modules/gplazma2-pyscript</module>\n        <module>modules/gplazma2-oidc</module>\n        <module>modules/gplazma2-oidc-te</module>\n        <module>modules/gplazma2-omnisession</module>\n        <module>modules/gplazma2-multimap</module>\n        <module>modules/gplazma2-roles</module>\n        <module>modules/gplazma2-scitoken</module>\n        <module>modules/dcache-vehicles</module>\n        <module>modules/dcache-nearline-spi</module>\n        <module>modules/dcache</module>\n        <module>modules/dcache-bulk</module>\n        <module>modules/dcache-chimera</module>\n        <module>modules/dcache-ftp</module>\n        <module>modules/dcache-resilience</module>\n        <module>modules/dcache-webdav</module>\n        <module>modules/dcache-xrootd</module>\n        <module>modules/dcache-dcap</module>\n        <module>modules/dcache-gplazma</module>\n        <module>modules/dcache-frontend</module>\n        <module>modules/dcache-nfs</module>\n        <module>modules/dcache-srm</module>\n        <module>modules/dcache-history</module>\n        <module>modules/dcache-info</module>\n        <module>modules/dcache-spacemanager</module>\n        <module>modules/dcache-qos</module>\n        <module>modules/ftp-client</module>\n        <module>modules/srm-common</module>\n        <module>modules/srm-server</module>\n        <module>modules/srm-client</module>\n        <module>modules/acl-vehicles</module>\n        <module>modules/acl</module>\n        <module>modules/chimera</module>\n        <module>modules/missingfiles-semsg</module>\n        <module>modules/benchmarks</module>\n        <module>plugins</module>\n        <module>docs</module>\n        <module>packages</module>\n        <module>archetypes</module>\n    </modules>\n\n    <profiles>\n      <profile>\n\t<id>code-coverage</id>\n\n\t<build>\n\t  <plugins>\n            <!-- Configured as described here:\n\t\t http://www.petrikainulainen.net/programming/maven/creating-code-coverage-reports-for-unit-and-integration-tests-with-the-jacoco-maven-plugin/\n\t    -->\n\t    <plugin>\n\t      <groupId>org.jacoco</groupId>\n\t      <artifactId>jacoco-maven-plugin</artifactId>\n\t      <executions>\n\t\t<!--\n\t\t    Prepares the property pointing to the JaCoCo runtime agent which\n\t\t    is passed as VM argument when Maven the Surefire plugin is executed.\n\t\t-->\n\t\t<execution>\n\t\t  <id>pre-unit-test</id>\n\t\t  <goals>\n                    <goal>prepare-agent</goal>\n\t\t  </goals>\n\t\t  <configuration>\n                    <!-- Sets the path to the file which contains the execution data. -->\n                    <destFile>${project.build.directory}/coverage-reports/jacoco-ut.exec</destFile>\n\t\t  </configuration>\n\t\t</execution>\n\t\t<!--\n\t\t    Ensures that the code coverage report for unit tests is created after\n\t\t    unit tests have been run.\n\t\t-->\n\t\t<execution>\n\t\t  <id>post-unit-test</id>\n\t\t  <phase>test</phase>\n\t\t  <goals>\n                    <goal>report</goal>\n\t\t  </goals>\n\t\t  <configuration>\n                    <!-- Sets the path to the file which contains the execution data. -->\n                    <dataFile>${project.build.directory}/coverage-reports/jacoco-ut.exec</dataFile>\n                    <!-- Sets the output directory for the code coverage report. -->\n                    <outputDirectory>${project.reporting.outputDirectory}/jacoco-ut</outputDirectory>\n\t\t  </configuration>\n\t\t</execution>\n\t      </executions>\n\t    </plugin>\n\t  </plugins>\n\t</build>\n      </profile>\n    </profiles>\n</project>\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/deeploki",
            "repo_link": "https://gitlab.com/qtb-hhu/marine/DeepLOKI",
            "content": {
                "codemeta": "",
                "readme": "# DeepLOKI\n\nZooplankton plays a crucial role in the ocean’s ecology, serving as a foundational component in\nthe food chain by consuming phytoplankton or other zooplankton and furthermore influencing\nnutrient cycling. This pivotal role distinguishes them from other species that reside at higher\ntrophic levels. The vertical distribution of zooplankton in the ocean is patchy, and its relation\nto hydrographical conditions cannot be fully deciphered using traditional net casts due to the\nlarge depth intervals sampled. Optical systems that continuously take images during the cast\ncan help bridge this gap. The Lightframe On-sight Keyspecies Investigation (LOKI) concentrates\nzooplankton with a net that leads to a flow-through chamber with a camera taking images with\nup to 20 frames sec−1. These high-resolution images allow for the determination of zooplankton\ntaxa, often even to genus or species level, and, in the case of copepods, developmental stages.\nEach cruise produces a substantial volume of images, ideally requiring onboard analysis, which\npresently consumes a significant amount of time and necessitates internet connectivity to access\nthe EcoTaxa Web service. To enhance the analyses, we developed an AI-based software\nframework named DeepLOKI, utilizing Deep Transfer Learning with a Convolution Neural Network\nBackbone. Our DeepLOKI image recognition tool can be applied directly on board. We trained\nand validated the model on pre-labeled images from four cruises, while images from a fifth\ncruise were used for testing. The best-performing model, utilizing the self-supervised pre-trained\nResNet18 Backbone, achieved a notable average classification accuracy of 83.9 %, surpassing\nthe regularly and frequently used method EcoTaxa (default) in this field by a factor of two. \nIn summary, we developed a tool for pre-sorting high-resolution black and white zooplankton images\nwith high accuracy, which will simplify and quicken the final annotation process. In addition, we\nprovide a user-friendly graphical interface for the DeepLOKI framework for efficient and concise\nprocesses leading up to the classification stage. Moreover, performing latent space analysis on\nthe self-supervised pre-trained ResNet18 Backbone could prove advantageous in identifying\nanomalies such as deviations in image parameter settings. This, in turn, enhances the quality\ncontrol of the data. Our methodology remains agnostic to the specific imaging end system used,\nsuch as Loki, UVP, or ZooScan, as long as there is a sufficient amount of appropriately labeled\ndata available to enable effective task performance by our algorithms.\n\n# Installation Guide\nhttps://pytorch.org/get-started/locally/\n\npip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1\n\n```\nbrew install python@3.10\npip3 install torch torchvision torchaudio\npip3 install -r requirements_.txt\n```\n\n# Usage\nFirst download the example_haul.zip, model_ckpt.zip, loki.zip. and the sort.zip from\n[Download Here](https://uni-duesseldorf.sciebo.de/s/okWh4728VwnCBGp).\nExtract them.\nloki folder in the DeepLoki_ folder on root level. (here are our models stored)\n\nCopy the update_allcruises_df_validated_5with_zoomie_20230727.csv to output.\nCopy the update_wo_artefacts_test_dataset_PS992_20230727_nicole.csv to output.\n\nCopy the example_haul folder to data/ .\nCopy the sort folder to data/5_cruises/ .\nCopy the content to saved_models/ .\n\nImage analysis: Run start_app.py\n\nImage Labeling: Run start_app_sort.py\n\n# Training - Data needed and computing power\n\nTraining: Run train_pytorch_lightning_model.py\n\nPreTraining: Run pretrain/pretrain_with_dino_paper_resnet_dino450.py\n\n# Software used\nTraining and Validation was performed on an Nvidia A$100$ (Nvidia Corp., Santa Clara, CA, USA) and on Apple M1 MAX with 32 GB (Apple, USA), depending on the computational power needed, for example self-supervised pre-training was performed on a Hyper performing cluster with Nvidia A$100$. <br>\nOn the Macbook Pro (Apple, USA) we used:<br>\nPython VERSION:3.10.5<br>\npyTorch VERSION:13.1.3<br>\nOn the cluster we used cluster specifics versions of the software:<br>\nPython VERSION:3.10.5 <br>\npyTorch VERSION:13.1.3<br>\nCUDNN VERSION:1107)<br>\n\n# Authors\nRaphael Kronberg and Ellen Oldenburg\n\n# Support \nIf you **really** like this repository and find it useful, please consider (★) **starring** it, so that it can reach a broader audience of like-minded people. It would be highly appreciated !\n\n# Contributing to DeepLOKI\nIf you find a bug, create a GitHub issue, or even better, submit a pull request. Similarly, if you have questions, simply post them as GitHub ([Link text Here](https://github.com/rakro101/DeepLOKI)) issues. \n\n\n# License , citation and acknowledgements\nPlease advice the **LICENSE.md** file. For usage of third party libraries and repositories please advise the respective distributed terms. Please cite our paper, when using this code:\n\n```\n@software{kronbergapplicationsdeeploki,\n  title={DeepLOKI- A deep learning based approach to identify Zooplankton taxa on high-resolution images from the optical plankton recorder LOKI},\n  author={Kronberg, Raphael Marvin and Oldenburg, Ellen}\n  year = {2023},\n  url = {https://github.com/rakro101/DeepLOKI},\n}\n```\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/deploy2zenodo",
            "repo_link": "https://gitlab.com/deploy2zenodo/deploy2zenodo",
            "content": {
                "codemeta": "",
                "readme": "---\nauthor: Daniel Mohr\ndate: 2024-11-29\nlicense: Apache-2.0\nhome: https://gitlab.com/deploy2zenodo/deploy2zenodo\nmirror: https://github.com/deploy2zenodo/deploy2zenodo\nlatest_release: https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest\ndoi: 10.5281/zenodo.10112959\n---\n\n# `deploy2zenodo`\n\n[[_TOC_]]\n\n## preamble\n\n[`deploy2zenodo`](https://gitlab.com/projects/51392274) is a\n[shell](https://en.wikipedia.org/wiki/Bourne_shell) script to deploy\nyour data to [Zenodo](https://zenodo.org/).\nYou can use it in a [CI pipeline](https://docs.gitlab.com/ee/ci/pipelines/) as\nan automatic workflow.\n\nEnvironmental variables allow very flexible use.\nDepending on the selected flags, the data can be curated before deployment\nin a merge request, in the zenodo web interface or not curated at all.\n\n**Note:** `deploy2zenodo` is primarily designed for the Zenodo API.\nIt may also work with other APIs like\n[Invenio RDM](https://inveniosoftware.org/products/rdm/),\nbut compatibility is not guaranteed.\nTest the script with any new API before using it.\n\n## intention\n\nTo satisfy the FAIR[^fair1] principles[^fair2], publications should be\ndeployed to an open repository. In this way the publication gets a PID\n([persistent identifier](https://en.wikipedia.org/wiki/Persistent_identifier))\nand at least the metadata is publicly accessible, findable and citable.\nFurthermore, current discussions about KPIs\n([key performance indicator](https://en.wikipedia.org/wiki/Performance_indicator))\nfor software and data publications also lead to the need to generate PIDs\nfor software and data.\n\n[^fair1]: [FAIR Principles](https://www.go-fair.org/fair-principles/)\n\n[^fair2]: [An interpretation of the FAIR principles to guide implementations in the HMC digital ecosystem.](https://doi.org/10.3289/HMC_publ_01)\n\nEspecially software usually is not citable by a PID.\nTo overcome this and make software academically significant we provide here a\ntool for automatic publication to the open repository [zenodo](https://zenodo.org/).\n\nIn principal the same is true for all kind of scientific data\n(e. g. measurements, software and results such as papers).\nFor every data managed in a version control system an automatic publication\nto an open repository is useful[^versioning].\n\n[^versioning]: [Guidance on Versioning of Digital Assets.](https://doi.org/10.3289/HMC_publ_04)\n\nSoftware in particular is subject to frequent changes, resulting in many\nversions. This leads to the urge to automate the publishing process.\nThis is not only about making the software usable through software repositories,\nbut also about the citability of individual versions.\n\n## how-to\n\nThere are many possibilities to use `deploy2zenodo` but in this how-to section\nwe will focus on a few typically use cases.\n\n### simple workflow\n\nThis workflow reflects the primary focus of `deploy2zenodo`.\n\nGo to your zenodo account and create an\n[access token](https://developers.zenodo.org/?shell#authentication).\n\nStore it in a [GitLab CI/CD variable](https://docs.gitlab.com/ee/ci/variables/)\nas `DEPLOY2ZENODO_ACCESS_TOKEN`. Use the flags\n[Mask variable](https://docs.gitlab.com/ee/ci/variables/index.html#mask-a-cicd-variable)\nand [Protect variable](https://docs.gitlab.com/ee/ci/variables/index.html#protect-a-cicd-variable).\nMasking ensures that the variable is not displayed in the CI/CD logs, and\nprotecting the variable limits access to authorized users.\nKeep in mind the token is sensitive and private information.\nTherefore you should not share it or make it public available.\n\nThen the [GitLab CI/CD pipeline](https://docs.gitlab.com/ee/ci/pipelines/)\ncould look like (we use here [sandbox.zenodo.org](https://sandbox.zenodo.org/)\ninstead of [zenodo.org](https://zenodo.org/) for testing purpose):\n\n```yaml\ninclude:\n  - remote: 'https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml'\n\nprepare_release_and_deploy2zenodo:\n  stage: build\n  image:\n    name: alpine:latest\n  variables:\n    DEPLOY2ZENODO_JSON: \"mymetadata.json\"\n  script:\n    # prepare\n    - TAG=$(grep version library.properties | cut -d \"=\" -f 2)\n    - |\n      echo '{\"metadata\":{\"creators\":[{\"name\":\"family, given\"}],\\\n        \"license\":{\"id\":\"GPL-3.0-or-later\"},\"title\":\"test script alpine\",\\\n        \"version\":\"***\",\"upload_type\":\"software\"}}' | \\\n        jq \".metadata.version = \\\"$TAG\\\"\" | tee \"$DEPLOY2ZENODO_JSON\"\n    # prepare release\n    - echo \"DESCRIPTION=README.md\" > variables.env\n    - echo \"TAG=$TAG\" >> variables.env\n    # prepare deploy2zenodo\n    - echo \"DEPLOY2ZENODO_JSON=$DEPLOY2ZENODO_JSON\" >> variables.env\n    - DEPLOY2ZENODO_UPLOAD=\"v$TAG.zip\"\n    - git archive --format zip --output \"$DEPLOY2ZENODO_UPLOAD\" \"$TAG\"\n    - echo \"DEPLOY2ZENODO_UPLOAD=$DEPLOY2ZENODO_UPLOAD\" >> variables.env\n  artifacts:\n    reports:\n      dotenv: variables.env\n    paths:\n      - $DEPLOY2ZENODO_JSON\n\nrelease_job:\n  stage: deploy\n  rules:\n    - if: $CI_COMMIT_TAG\n      when: never\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n  image:\n    registry.gitlab.com/gitlab-org/release-cli:latest\n  script:\n    - cat /etc/os-release\n  release:\n    name: 'v$TAG'\n    description: '$DESCRIPTION'\n    tag_name: '$TAG'\n    ref: '$CI_COMMIT_SHA'\n\ndeploy2zenodo:\n  rules:\n    - if: $CI_COMMIT_TAG\n      when: never\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n  variables:\n    DEPLOY2ZENODO_API_URL: \"https://sandbox.zenodo.org/api\"\n    DEPLOY2ZENODO_DEPOSITION_ID: \"create NEW record\"\n```\n\nWe use here 3 jobs:\n\n* The job `prepare_release_and_deploy2zenodo` prepares the\n  variables and data for the following jobs. You can choose how to get\n  the variables and data from your project/repository.\n  (see hints in [DEPLOY2ZENODO_JSON](#deploy2zenodo_json) and\n  [DEPLOY2ZENODO_UPLOAD](#deploy2zenodo_upload))\n* The job `release_job` uses the workflow\n  [Create release metadata in a custom script](https://docs.gitlab.com/ee/user/project/releases/release_cicd_examples.html#create-release-metadata-in-a-custom-script).\n* The job `deploy2zenodo` publishes the data to zenodo.\n\nThe variables are passed between the jobs using\n[dotenv variables](https://docs.gitlab.com/ee/ci/yaml/artifacts_reports.html#artifactsreportsdotenv).\nAnd the data are passed using\n[job artifacts](https://docs.gitlab.com/ee/ci/jobs/job_artifacts.html).\n\nAfter the first run of the above pipeline (job `deploy2zenodo`) adapt\n`DEPLOY2ZENODO_DEPOSITION_ID` to store the record id. Only then you are\nable to release new versions to zenodo.\n\nIn this example, `prepare_release_and_deploy2zenodo` always runs\nwhile the other jobs only run when the default branch is changed.\nThis makes it possible to check the artifacts during a merge request.\n\nThe used environment variables (see [script parameter](#script-parameter)) can\nbe provided in many different ways as a\n[GitLab CI/CD variable](https://docs.gitlab.com/ee/ci/variables/), e. g.:\n\n* [CI/CD variable in the UI](https://docs.gitlab.com/ee/ci/variables/#define-a-cicd-variable-in-the-ui)\n  * not stored in the repository\n  * possible to [Mask variable](https://docs.gitlab.com/ee/ci/variables/index.html#mask-a-cicd-variable)\n  * possible to [Protect variable](https://docs.gitlab.com/ee/ci/variables/index.html#protect-a-cicd-variable)\n  * used for private data (e. g. access token)\n* [CI/CD variable in the .gitlab-ci.yml](https://docs.gitlab.com/ee/ci/variables/#define-a-cicd-variable-in-the-gitlab-ciyml-file)\n  * stored in the repository\n  * in public projects also publicly accessable\n\nYou should think about which information to store at which place.\nHere a few simple considerations:\n\n| variable | private data | note |\n| ------ | ------ | ------ |\n| DEPLOY2ZENODO_API_URL | no | Should a user find your publication? |\n| DEPLOY2ZENODO_ACCESS_TOKEN | YES | Should not be shared with anyone! |\n| DEPLOY2ZENODO_DEPOSITION_ID | no | Should a user find your publication? |\n| DEPLOY2ZENODO_JSON | ? | Is the publication public? |\n| DEPLOY2ZENODO_UPLOAD | ? | Is the publication public? |\n\nSometimes it is easier to change the variable in the UI.\nFor example in your first step you should set\n`DEPLOY2ZENODO_API_URL=\"https://sandbox.zenodo.org/api\"` and\n`DEPLOY2ZENODO_DEPOSITION_ID=\"create NEW record\"` to initiate and test your\npipeline. After success you should change to\n`DEPLOY2ZENODO_API_URL=\"https://zenodo.org/api\"`.\nAnd after you have created your first record, also change\n`DEPLOY2ZENODO_DEPOSITION_ID` to the returned value to update your dataset\nnext time (and not create a new one). If you store these variables in the user\ninterface, you can change them without touching your repository.\nOn the other hand, the metadata provided via `DEPLOY2ZENODO_JSON` and the\ndata provided via `DEPLOY2ZENODO_UPLOAD` may be created dynamically and\nit could therefore make sense to create these variables dynamically as well.\n\nThere are also optional variables that can help to adapt the workflow to the\nthe individual use case.\nFor example, [DEPLOY2ZENODO_SKIP_PUBLISH](#deploy2zenodo_skip_publish) allows\nyou to curate the upload to zenodo in the zenodo web interface before\npublishing. This is especially useful if you are setting up the workflow for\nthe first time in your own project -- but can also be used at any time.\n\nDepending on where variables are defined, they have different priorities.\nFor example, CI variables defined in the UI have priority and override the\nvariables stored in the `.gitlab-ci.yml` file with the\n[keyword `variables`](https://docs.gitlab.com/ee/ci/yaml/#variables).\nVariables that are defined at job level, in the `script`, `before_script` or\n`after_script` sections, have the highest priority\n\nAn example test project is [deploy2zenodo_test_simple_workflow_update](https://gitlab.com/projects/51647607).\n\n### very simple workflow\n\nIt is not necessary to create a release for publication. But we think this\nis the typically use case for software publication.\n\nFor a very simple workflow running when creating a tag,\nyou could use something like:\n\n```yaml\ninclude:\n  - remote: 'https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml'\n\ndeploy2zenodo:\n  stage: deploy\n  rules:\n    - if: $CI_COMMIT_TAG\n  variables:\n    DEPLOY2ZENODO_API_URL: \"https://sandbox.zenodo.org/api\"\n    DEPLOY2ZENODO_JSON: \"CITATION.json\"\n    DEPLOY2ZENODO_DEPOSITION_ID: \"create NEW record\"\n    DEPLOY2ZENODO_UPLOAD: \"$CI_PROJECT_NAME-$CI_COMMIT_TAG.zip\"\n    DEPLOY2ZENODO_ADD_IsCompiledBy_DEPLOY2ZENODO: \"yes\"\n    DEPLOY2ZENODO_ADD_IsNewVersionOf: \"yes\"\n    DEPLOY2ZENODO_ADD_IsPartOf: \"yes\"\n    DEPLOY2ZENODO_GET_METADATA: \"result.json\"\n  before_script:\n    - env\n    - echo https://dl-cdn.alpinelinux.org/alpine/edge/community >> /etc/apk/repositories\n    - apk add --no-cache cffconvert curl git jq\n    - publication_date=$(echo \"$CI_COMMIT_TIMESTAMP\" | grep -Eo \"^[0-9]{4}-[0-9]{2}-[0-9]{2}\")\n    - |\n      cffconvert -i CITATION.cff -f zenodo | \\\n        jq -c '{\"metadata\": .} | .metadata += {\"upload_type\": \"software\"}' | \\\n        jq -c \".metadata.related_identifiers += [\n          {\\\"relation\\\": \\\"isDerivedFrom\\\",\n          \\\"identifier\\\": \\\"$CI_SERVER_URL/projects/$CI_PROJECT_ID\\\"}] |\n          .metadata.version = \\\"$CI_COMMIT_TAG\\\" |\n          .metadata.publication_date = \\\"$publication_date\\\"\" | \\\n        tee \"$DEPLOY2ZENODO_JSON\" | jq -C .\n    - git archive --format zip --output \"$DEPLOY2ZENODO_UPLOAD\" \"$CI_COMMIT_TAG\"\n  artifacts:\n    paths:\n      - $DEPLOY2ZENODO_JSON\n      - $DEPLOY2ZENODO_GET_METADATA\n```\n\nSuch a simple workflow uses\n\n* [deploy_deploy2zenodo_to_zenodo](https://gitlab.com/projects/52008252)\n  in the job `deploy2zenodo` to publish itself.\n* [2024-10_waw_fdm](https://codebase.helmholtz.cloud/projects/14469)\n  in the job `deploy2zenodo` to publish a poster.\n\n### triggered workflow\n\nIn many projects there is more than one maintainer. Therefore it is not\npossible to store the user token for zenodo as CI variable in the project.\nOtherwise, the user token would be shared with the other maintainers.\n\nUsing this triggered workflow allows to restrict the use of the user token\nto a specific zenodo record for other maintainers.\n\nBut the project `A` with more than one maintainer can trigger a pipeline in\nanother (private) project `B` with only one maintainer, e. g.:\n\n```yaml\ntrigger:\n  stage: .post\n  rules:\n    - if: $CI_COMMIT_TAG\n      when: never\n    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH\n  image:\n    name: alpine:latest\n  script:\n    - apk add --no-cache curl\n    - curl -X POST --fail -F token=\"$TRIGGER_TOKEN\" -F ref=main \"$TRIGGER_URL\"\n```\n\nStoring the `TRIGGER_TOKEN` as protected and\n[masked CI variable](https://docs.gitlab.com/ee/ci/variables/index.html#mask-a-cicd-variable)\n(or maybe even [hide CI variable](https://docs.gitlab.com/ee/ci/variables/index.html#hide-a-cicd-variable))\nin project `A` allows any maintainer to use it and trigger the pipeline.\n\nIn the project `B` only 1 mainainer exists and you can use\ndeploy2zenodo as normal, e. g.:\n\n```yaml\ninclude:\n  - remote: 'https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml'\n\nprepare_deploy2zenodo:\n  image:\n    name: alpine:latest\n  script:\n    - PROJECT_A_REPO=$(mktemp -d)\n    - git clone --branch main --depth 1 \"$PROJECT_A_URL\" \"$PROJECT_A_REPO\"\n    # create zip archive from latest tag\n    - |\n      (cd \"$PROJECT_A_REPO\" && \\\n       git archive --format zip -o \"$DEPLOY2ZENODO_UPLOAD\" \\\n       \"$(git tag | sort -t \".\" -n -k 3 | tail -n 1)\")\n  artifacts:\n    expire_in: 1 hrs\n    paths:\n      - $DEPLOY2ZENODO_UPLOAD\n\nmy_deploy2zenodo:\n  extends: .deploy2zenodo\n  # variables set in the script could not be overwritten by the trigger source\n  before_script:\n    - |\n      DEPLOY2ZENODO_DEPOSITION_ID=\"create NEW record\"\n      DEPLOY2ZENODO_API_URL=\"https://sandbox.zenodo.org/api\"\n    - !reference [deploy2zenodo, before_script]\n\ndeploy2zenodo:\n  rules:\n    - if: '\"0\" == \"1\"'\n      when: never\n```\n\nThere are various ways to trigger a pipeline, e. g:\n\n* [trigger a pipeline by trigger token](https://docs.gitlab.com/ee/ci/triggers/)\n* trigger using [Multi-project pipelines](https://docs.gitlab.com/ee/ci/pipelines/downstream_pipelines.html#multi-project-pipelines)\n\nIn the CI pipeline above the token method is used. In the\n[CI pipeline of deploy2zenodo](https://gitlab.com/deploy2zenodo/deploy2zenodo/-/blob/main/.gitlab-ci.yml?ref_type=heads)\nthe multi-project pipeline is used.\n\n**Be careful**:\nThe trigger job from project `A` may overwrite variables in the triggered\njob from project `B`. This could lead to security concerns.\nMaybe [Restrict who can override variables](https://docs.gitlab.com/ee/ci/variables/index.html#restrict-who-can-override-variables)\ncould help to overcome this.\n\nMore details:\nIn project `A` something exists that should be published on zenodo.\nIn project `B` the content of project `A` is published on zenodo.\nThe pipeline in project `B` can be triggered so that this happens\nautomatically when corresponding changes are made in project `A`\n(e. g. merge to default branch).\nProject `B` should rely as little as possible on project `A`.\nUnfortunately, variables can be transferred when triggering (from project `A`)\nand these are not trustworthy.\nFor example, a maintainer from project `A` could pass `DEPLOY2ZENODO_API_URL`\nin this way and thus force communication to another server.\nThis could cause the user token to be leaked.\nTo avoid this, define the variable in the script -- as shown in the example\nabove.\nHowever, it is no problem to save the user token in project `B` as\nCI variable `DEPLOY2ZENODO_ACCESS_TOKEN`.\nThis variable could then be overwritten from project `A`, but not read out.\n\nAnother possibility is to use\n[Secrets management providers](https://docs.gitlab.com/ee/ci/pipelines/pipeline_security.html#secrets-management-providers).\n\nThis triggered workflow is used in\n[file_hook_server_timestamping](https://gitlab.com/dlr-pa/file_hook_server_timestamping)\ntogether with\n[deploy_file_hook_server_timestamping_to_zenodo](https://gitlab.dlr.de/deploy2zenodo/deploy_file_hook_server_timestamping_to_zenodo).\n\n### complex workflow\n\nThis workflow splits the deploying to zenodo in steps. This allows to use\nthe zenodo record (e. g. the DOI) already in the data to publish.\n\n```yaml\ninclude:\n  - remote: 'https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml'\n\ndeploy2zenodo:\n  rules:\n    - if: '\"0\" == \"1\"'\n      when: never\n\nprepare_deploy2zenodo_step1:\n  script:\n    - ...\n\ndeploy2zenodo-step1:\n  variables:\n    - DEPLOY2ZENODO_SKIP_PUBLISH: \"true\"\n    - DEPLOY2ZENODO_GET_METADATA: \"newmetadata.json\"\n  extends: .deploy2zenodo\n  image:\n    name: alpine:latest\n  before_script:\n    - !reference [deploy2zenodo, before_script]\n  script:\n    - !reference [deploy2zenodo, script]\n    - echo \"DEPLOY2ZENODO_GET_METADATA=$DEPLOY2ZENODO_GET_METADATA\" > variables.env\n  artifacts:\n    paths:\n      - $DEPLOY2ZENODO_GET_METADATA\n    reports:\n      dotenv: variables.env\n\nprepare_release:\n  script:\n    - echo \"use the file \\\"$DEPLOY2ZENODO_GET_METADATA\\\"\"\n    - ...\n\nrelease_job:\n  script:\n    - ...\n\nprepare_deploy2zenodo_step2:\n  script:\n    - ...\n\ndeploy2zenodo-step2:\n  variables:\n    DEPLOY2ZENODO_SKIP_NEW_VERSION: \"true\"\n  extends: .deploy2zenodo\n  image:\n    name: alpine:latest\n  before_script:\n    - apk add --no-cache curl jq\n```\n\nIn the step `prepare_release` you can use [jq](https://github.com/jqlang/jq)\nto extract data. For example the preserved DOI is available by:\n\n```sh\njq .metadata.prereserve_doi.doi \"$DEPLOY2ZENODO_GET_METADATA\"\n```\n\nSuch a complex workflow uses\n[2024-10_waw_fdm_talk](https://codebase.helmholtz.cloud/projects/14501)\nto publish a poster. Instead of creating a release, as shown above, the\nposter is built in the job `build` using the DOI previously created in\nthe job `deploy2zenodo-step1`.\n\n### very complex workflow\n\n`deploy2zenodo` uses a combination of the [triggered workflow](https://gitlab.com/deploy2zenodo/deploy2zenodo#triggered-workflow)\nand the [complex workflow](https://gitlab.com/deploy2zenodo/deploy2zenodo#complex-workflow)\nto publish itself. This is described in [deploy_deploy2zenodo_to_zenodo](https://gitlab.com/projects/52008252).\n\n## script parameter\n\nInstead of command line parameters we use environment variables.\n\nYou have to provide the following variables:\n\n| variable | content |\n| ------ | ------ |\n| DEPLOY2ZENODO_API_URL | The URL of the API to use. |\n| DEPLOY2ZENODO_ACCESS_TOKEN | access token of zenodo |\n| DEPLOY2ZENODO_DEPOSITION_ID | id of the deposition/record on zenodo |\n| DEPLOY2ZENODO_JSON | file name with metadata in JSON format to upload |\n| DEPLOY2ZENODO_UPLOAD | file name(s) to upload |\n\nThere are other optional variables:\n\n| variable | content |\n| ------ | ------ |\n| DEPLOY2ZENODO_SKIP_PUBLISH | prepare record, but skip publishing |\n| DEPLOY2ZENODO_DRYRUN | skip communicating with the external URL |\n| DEPLOY2ZENODO_SKIPRUN | skip everything, only prints commands to execute |\n| DEPLOY2ZENODO_SKIP_NEW_VERSION | skip creating new version |\n| DEPLOY2ZENODO_GET_METADATA | write actual metadata to a file |\n| DEPLOY2ZENODO_SKIP_UPLOAD | skip upload of data |\n| DEPLOY2ZENODO_CURL_MAX_TIME | max time for curl |\n| DEPLOY2ZENODO_CURL_MAX_TIME_PUBLISH | max time for curl during publishing |\n| DEPLOY2ZENODO_ADD_IsCompiledBy_DEPLOY2ZENODO | reference deploy2zenodo |\n| DEPLOY2ZENODO_ADD_IsNewVersionOf | reference previous version |\n| DEPLOY2ZENODO_ADD_IsPartOf | reference DOI for all versions |\n\n### DEPLOY2ZENODO_API_URL\n\nYou can use the API of your own zenodo instance or you can use the\nofficial [zenodo instance](https://about.zenodo.org/):\n\n| state | URL |\n| ------ | ------ |\n| production | [`https://zenodo.org/api`](https://zenodo.org/api) |\n| testing | [`https://sandbox.zenodo.org/api`](https://sandbox.zenodo.org/api) |\n\n### DEPLOY2ZENODO_ACCESS_TOKEN\n\nTo access your zenodo account you have to provide an\n[access token](https://developers.zenodo.org/?shell#authentication).\n\n### DEPLOY2ZENODO_DEPOSITION_ID\n\nTo update an existing record you have to provide the `id` of this record.\n\nIf you want to create a new record please set `DEPLOY2ZENODO_DEPOSITION_ID`\nto `create NEW record`,\ne. g. `DEPLOY2ZENODO_DEPOSITION_ID=\"create NEW record\"`.\nAfter creating this record read the script output\nand adapt `DEPLOY2ZENODO_DEPOSITION_ID` for the next run with the returned\nrecord `id`.\n\n### DEPLOY2ZENODO_JSON\n\nThe given file should contain the metadata in JSON format.\n\nYou can write this file on your own, e. g.:\n\n```json\n{\n  \"metadata\": {\n    \"title\": \"foo\",\n    \"upload_type\": \"software\",\n    \"creators\": [\n      {\n        \"name\": \"ich\",\n        \"affiliation\": \"bar\"\n      }\n    ],\n    \"description\": \"foos description\"\n  }\n}\n```\n\nYou can find the necessary and possible fields on\n[zenodo: Deposit metadata](https://developers.zenodo.org/#representation).\n\nOr [cffconvert](https://github.com/citation-file-format/cffconvert) can help\nharvesting the necessary metadata in JSON format from a\n[CITATION.cff file](https://github.com/citation-file-format/citation-file-format).\nUnfortunately we need [jq](https://github.com/jqlang/jq) to correct the format,\ne. g.:\n\n```sh\ncffconvert -i CITATION.cff -f zenodo | \\\n  jq '{\"metadata\": .} | .metadata += {\"upload_type\": \"software\"}' | \\\n  tee CITATION.json\n```\n\nSince you need to adapt the output of the conversion you can also use more\ngeneral tools like [yq](https://mikefarah.gitbook.io/yq/) to convert\na CITATION.cff file (YAML format) to JSON format.\n\nThe JSON format zenodo accepts is much more general and provides many more\noptions than the Citation File Format. For many purposes the CITATION.cff\nis enough, but otherwise you can see a description of the metadata in the\nGitHub integration of\nzenodo[^githubintegration] [^githubintegration2] [^githubintegration3]\nusing `zenodo.json`, the description of the metadata in\nzenodo|Developers[^zenodoDevelopers] or InvenioRDM[^metadatareference] and\nthe unofficial description of\nzenodo upload metadata schema[^zenodouploadmetadataschema].\n\n[^githubintegration]: [developers.zenodo.org GitHub](https://developers.zenodo.org/#github)\n[^githubintegration2]: [github.com Referencing and citing content](https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content)\n[^githubintegration3]: [github: \"import\" past releases to Zenodo](https://github.com/zenodo/zenodo/issues/1463)\n\n[^zenodoDevelopers]: [zenodo|Developers](https://developers.zenodo.org/#deposit-metadata)\n\n[^metadatareference]: [InvenioRDM: Metadata reference](https://inveniordm.docs.cern.ch/reference/metadata/)\n\n[^zenodouploadmetadataschema]: [Zenodo upload metadata schema](https://github.com/zenodraft/metadata-schema-zenodo)\n\nAs `description` you can use HTML. For example you could use\n[pandoc](https://pandoc.org/) to convert your `README.md` to HTML and\n[jq](https://github.com/jqlang/jq) to add the HTML code as JSON\nvalue (`jq` will escape appropriate characters if necessary):\n\n```sh\npandoc -o README.html README.md\necho '{\"metadata\":{\"title\":\"foo\",\"upload_type\":\"software\",\n  \"creators\":[{\"name\":\"ich\",\"affiliation\":\"bar\"}],\n  \"description\":\"foos description\"}}' | \\\n  jq --rawfile README README.html '.metadata.description = $README' | \\\n  tee metadata.json\n```\n\n### DEPLOY2ZENODO_UPLOAD\n\nThe given file(s) will be uploaded as data. Typically this would be an archive.\n\nFor example you can create an archive of a tag from a git repository:\n\n```sh\nTAG=0.0.3\ngit archive --format zip --output $TAG.zip $TAG\n```\n\nFile names with spaces are not supported. Instead, if `DEPLOY2ZENODO_UPLOAD`\ncontains space(s), the string is split at the spaces.\nEach individual block represents a file and these files will be uploaded.\n\nThe reason not supporting spaces is that\n[you cannot create a CI/CD variable that is an array](https://docs.gitlab.com/ee/ci/variables/index.html#store-multiple-values-in-one-variable).\n\nIf you really not want to provide data set `DEPLOY2ZENODO_UPLOAD` to\n`do NOT provide data`, e. g. `DEPLOY2ZENODO_UPLOAD=\"do NOT provide data\"`.\nIf you want to upload 4 files with these names change the order.\n\nNot every zenodo instance supports metadata-only records\n(configured by `canHaveMetadataOnlyRecords`?).\nFor example the official [zenodo instance](https://about.zenodo.org/)\ndoes not allow metadata-only records!\nIn this case an empty dummy file is uploaded.\nIf this is the case, you should think about respecting the implicit request\nof the used zenodo instance to provide some data.\n\n### DEPLOY2ZENODO_SKIP_PUBLISH\n\nIf this variable is not empty the publishing step is skipped, e. g.:\n\n```sh\n DEPLOY2ZENODO_SKIP_PUBLISH=\"true\"\n```\n\nOnly the record is prepared -- metadata and data is uploaded -- but not\npublished.\nYou can see what will be published as a preview in the web interface of zenodo\nand initiate the publishing by pressing the button in the web interface.\n\nThis helps to integrate `deploy2zenodo` in your project.\nBut you may also want to curate the upload each time before it is published.\n\nTogether with DEPLOY2ZENODO_SKIP_NEW_VERSION this allows to split deploying\nto zenodo in steps.\n\n### DEPLOY2ZENODO_DRYRUN\n\nIf this variable is not empty the communication to the given URL is skipped.\nBut your parameters are analyzed. This could help to integrate `deploy2zenodo`\nin your project.\n\n### DEPLOY2ZENODO_SKIPRUN\n\nIf this variable is not empty nearly everything is skipped.\nOnly the commands to be executed are echoed. This is for debugging purpose.\n\n### DEPLOY2ZENODO_SKIP_NEW_VERSION\n\nIf this variable is not empty the step creating a new version is skipped.\nThis allows to split deploying to zenodo in steps.\n\nBetween creating a new version and deploying to zenodo you can\nuse the zenodo record (e. g. the DOI) already in the data to publish:\n\n```sh\njq .metadata.prereserve_doi.doi \"$DEPLOY2ZENODO_GET_METADATA\" >> README.md\n```\n\nUsing a [manual job](https://docs.gitlab.com/ee/ci/jobs/job_control.html#create-a-job-that-must-be-run-manually)\nallows you to first check the artifacts and data to be published\nbefore the last job run.\n\n### DEPLOY2ZENODO_GET_METADATA\n\nIf this variable is not empty the metadata of the record is stored in a\nfile with this name.\nThis is useful for logging or further processing after deployment.\n\nTo get these data at the end of the script an additional communication\nwith the DEPLOY2ZENODO_API_URL server is done.\n\nIn the CI pipeline you could store the result as artifacts, e. g.:\n\n```yaml\ndeploy2zenodo:\n  variables:\n    DEPLOY2ZENODO_GET_METADATA: \"result.json\"\n  artifacts:\n    paths:\n      - $DEPLOY2ZENODO_GET_METADATA\n```\n\nYou can extract values from the metadata. For example to get the DOI\nto site all versions:\n\n```yaml\nmy_deploy2zenodo:\n  extends: .deploy2zenodo\n  script:\n    - !reference [deploy2zenodo, script]\n    - jq -r .conceptdoi \"$DEPLOY2ZENODO_GET_METADATA\"\n```\n\n### DEPLOY2ZENODO_SKIP_UPLOAD\n\nIf this variable is not empty skip uploading the data. This is only\nallowed if DEPLOY2ZENODO_SKIP_PUBLISH is not empty, too.\n\nIf you split deploying to zenodo in steps using DEPLOY2ZENODO_SKIP_PUBLISH and\nDEPLOY2ZENODO_SKIP_NEW_VERSION you can avoid unnecessary traffic by using also\nDEPLOY2ZENODO_SKIP_UPLOAD.\n\n### DEPLOY2ZENODO_CURL_MAX_TIME\n\nMax time for curl (`--max-time` flag) in seconds for normal use.\nDefault value is 60.\n\n### DEPLOY2ZENODO_CURL_MAX_TIME_PUBLISH\n\nMax time for curl (`--max-time` flag) in seconds during publishing.\nDefault value is 300.\n\n### DEPLOY2ZENODO_ADD_IsCompiledBy_DEPLOY2ZENODO\n\nIf this variable is not empty a reference to deploy2zenodo is added.\nSomething like (but with the DOI of the used version) will be added to your\nprovided JSON file:\n\n```json\n{\n  \"metadata\": {\n    \"related_identifiers\": [\n      {\n        \"relation\": \"IsCompiledBy\",\n        \"identifier\": \"10.5281/zenodo.10112959\",\n        \"scheme\": \"doi\",\n        \"resource_type\": \"software\"\n      }\n    ]\n  }\n}\n```\n\n### DEPLOY2ZENODO_ADD_IsNewVersionOf\n\nIf this variable is not empty a reference to the previous version of your\nrecord is referenced.\nSomething like (but with the DOI of the old version and the appropriate\nresource_type) will be added to your provided JSON file:\n\n```json\n{\n  \"metadata\": {\n    \"related_identifiers\": [\n      {\n        \"relation\": \"IsNewVersionOf\",\n        \"identifier\": \"10.5281/zenodo.10908332\",\n        \"scheme\": \"doi\",\n        \"resource_type\": \"software\"\n      }\n    ]\n  }\n}\n```\n\nThis can only work if DEPLOY2ZENODO_SKIP_NEW_VERSION is not used! If you split\nthe run in 2 steps (the first one with DEPLOY2ZENODO_SKIP_PUBLISH and the\nsecond one with DEPLOY2ZENODO_SKIP_NEW_VERSION) you have to find the old\nversion in the first run by yourself and provide it in the second run.\nThis is done in [deploy_deploy2zenodo_to_zenodo](https://gitlab.com/projects/52008252)\nin the jobs `deploy_deploy2zenodo_step1` and `deploy_deploy2zenodo_step2`\nto publish `deploy2zenodo`. This is something like:\n\n```yaml\nstep1:\n  variables:\n    DEPLOY2ZENODO_ADD_IsNewVersionOf: \"yes\"\n  after_script:\n    - |\n      LATESTDOI=\"$(jq -r \".metadata.related_identifiers[] |\n      select(.relation==\\\"isNewVersionOf\\\") | .identifier\" \\\n      \"$DEPLOY2ZENODO_GET_METADATA\")\"\n    - |\n      LATESTUPLOADTYPE=\"$(jq -r \".metadata.related_identifiers[] |\n      select(.relation==\\\"isNewVersionOf\\\") | .resource_type\" \\\n      \"$DEPLOY2ZENODO_GET_METADATA\")\"\n    - |\n      {\n      echo \"LATESTDOI=$LATESTDOI\"\n      echo \"LATESTUPLOADTYPE=$LATESTUPLOADTYPE\"\n      } | tee variables.env\n  artifacts:\n    reports:\n      dotenv: variables.env\n```\n\n```yaml\nstep2:\n  needs:\n    - job: step1\n  variables:\n    DEPLOY2ZENODO_SKIP_NEW_VERSION: \"true\"\n  before_script:\n    - tmpjson=\"$(mktemp)\"\n    - |\n      jq \".metadata.related_identifiers += [\n      {\n      \\\"relation\\\":\\\"IsNewVersionOf\\\", \\\"identifier\\\":\\\"$LATESTDOI\\\",\n      \\\"scheme\\\":\\\"doi\\\", \\\"resource_type\\\":\\\"$LATESTUPLOADTYPE\\\"\n      }]\" \"$DEPLOY2ZENODO_JSON\" | tee \"$tmpjson\"\n    - mv \"$tmpjson\" \"$DEPLOY2ZENODO_JSON\"\n```\n\nNote that [after_script](https://docs.gitlab.com/ee/ci/yaml/#after_script)\nworks differently than `before_script` or `script` and does not affect the\njob exit code.\n\n### DEPLOY2ZENODO_ADD_IsPartOf\n\nIf this variable is not empty a reference to all versions of your record is\nreferenced.\nSomething like (but with the DOI of all versions and the appropriate\nresource_type) will be added to your provided JSON file:\n\n```json\n{\n  \"metadata\": {\n    \"related_identifiers\": [\n      {\n        \"relation\": \"IsPartOf\",\n        \"identifier\": \"10.5281/zenodo.10112959\",\n        \"scheme\": \"doi\",\n        \"resource_type\": \"software\"\n      }\n    ]\n  }\n}\n```\n\nThis only works, if DEPLOY2ZENODO_DEPOSITION_ID is not given\nas `create NEW record`.\n\n## CI pipeline\n\nUsing the keyword\n[`include`](https://docs.gitlab.com/ee/ci/yaml/index.html#include)\nit is possible to include YAML files and/or CI pipelines in your\n[GitLab](https://about.gitlab.com/) CI pipeline.\nIn this way you can use a template of `deploy2zenodo` for your CI pipeline.\n\nYou can use the latest version\n[deploy2zenodo.yaml](https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml)\nin your CI pipeline.\nOr you can use any special versions, e. g.\n[deploy2zenodo.yaml v0.1.0](https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/0.1.0/downloads/deploy2zenodo.yaml).\n\nThe provided job is called `deploy2zenodo` and you can overwrite or enhance\nthe defined job as you need (e. g. defining when to run or defining variables).\n\nA simple example choosing the stage to run could be:\n\n```yaml\ninclude:\n  - remote: 'https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml'\n\ndeploy2zenodo:\n  stage: deploy\n```\n\nThe provided GitLab CI template of `deploy2zenodo` uses\n[`alpine:latest`](https://hub.docker.com/_/alpine)\nand installs necessary software [curl](https://curl.se/) and\n[jq](https://github.com/jqlang/jq) in `before_script`.\nTo use other images you must adapt it, e. g.:\n\n```yaml\ninclude:\n  - remote: 'https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml'\n\ndeploy2zenodo:\n  image:\n    name: almalinux:latest\n  before_script:\n    - echo \"nothing to do\"\n```\n\n## script\n\nYou can use the script directly. But that is not our main focus of\n`deploy2zenodo`, so we keep it short. For example:\n\n```sh\nSCRIPTURL=https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo\nexport DEPLOY2ZENODO_API_URL=https://sandbox.zenodo.org/api\nexport DEPLOY2ZENODO_ACCESS_TOKEN=***\nexport DEPLOY2ZENODO_DEPOSITION_ID=\"create NEW record\"\nexport DEPLOY2ZENODO_JSON=metadata.json\nexport DEPLOY2ZENODO_UPLOAD=\"foo.zip bar.md\"\nexport DEPLOY2ZENODO_SKIP_PUBLISH=\"true\"\nexport DEPLOY2ZENODO_DRYRUN=\"\"\nexport DEPLOY2ZENODO_SKIPRUN=\"\"\nexport DEPLOY2ZENODO_SKIP_NEW_VERSION=\"\"\nexport DEPLOY2ZENODO_GET_METADATA=\"upload.json\"\nexport DEPLOY2ZENODO_SKIP_UPLOAD=\"\"\nexport DEPLOY2ZENODO_CURL_MAX_TIME=\"\"\nexport DEPLOY2ZENODO_CURL_MAX_TIME_PUBLISH=\"\"\ncurl -L \"$SCRIPTURL\" | tee deploy2zenodo.sh | sh\n```\n\nIt's worth noting that we try to handle private information such as the\naccess token in a secure way in the script, it is still a sensitive piece of\ninformation that should not be shared with anyone who does not need access to\nthe corresponding Zenodo account.\n\n**Important:** Using the script as described in this section (e. g. on a\ndesktop computer) does not allow for the masking of CI variables, which can\nexpose sensitive information such as access tokens. We recommend that users\ntake steps to mitigate this risk.\n\n## harvesting\n\nAs already mentioned you have to provide the metadata and the data to upload.\n\nIn my opinion, this is very dependent on the project.\nIn many programming languages, there is a convention to store metadata such\nas name, author, description, version and license in certain\nfiles (`pyproject.toml`, `library.properties`, ...).\nIn order to deploy this information to zenodo, it must be available in a\ncertain format and with a certain vocabulary. The already mentioned\n[cffconvert](https://github.com/citation-file-format/cffconvert) tries to do\nthis at least for cff files.\nOther tools such as [somesy](https://github.com/Materials-Data-Science-and-Informatics/somesy)\nhave a somewhat different focus, but they can also help in a pipeline/toolchain.\nFor example, you could use it to convert a typical python `pyproject.toml`\ninto `CITATION.cff` and then use\n[cffconvert](https://github.com/citation-file-format/cffconvert) and\n[jq](https://github.com/jqlang/jq) to get metadata for zenodo.\nHowever, [hermes](https://docs.software-metadata.pub/en/latest/) should also\nbe mentioned here. hermes tries to merge metadata from different sources and\nto provide it for zenodo.\n\n## curating\n\nYou have various options for curating the data for publication.\n\nThe typical workflow in software development is to work in developer or\nfeature branches and then merge them with the default branch (e. g. `main`).\nThis is usually done in a merge request. If the harvesting of metadata and\ndata is already taking place in a CI pipeline at this point, this can also\nbe checked in the merge request.\n\nIf publishing is prevented by using `DEPLOY2ZENODO_SKIP_PUBLISH`,\nthe preview in the zenodo web interface can be used to check the result.\n\nIf you have implemented a stable, functioning process, curation can also be\nomitted and publishing can be fully automated.\n\n## license: Apache-2.0\n\n`deploy2zenodo` has the license [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0).\n\n```txt\nCopyright 2023, 2024 Daniel Mohr and\n   Deutsches Zentrum fuer Luft- und Raumfahrt e. V., D-51170 Koeln\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/deus",
            "repo_link": "https://github.com/gfzriesgos/deus",
            "content": {
                "codemeta": "",
                "readme": "# deus\n\n[![codecov](https://codecov.io/gh/gfzriesgos/deus/branch/master/graph/badge.svg)](https://codecov.io/gh/gfzriesgos/deus)\n\n**D**amage-**E**xposure-**U**pdate-**S**ervice\n\nCommand line program for the damage computation in a multi risk scenario\npipeline for earthquakes, tsnuamis, ashfall & lahars.\n\n## Citation\nBrinckmann, Nils; Gomez-Zapata, Juan Camilo; Pittore, Massimiliano; Rüster, Matthias (2021): DEUS: Damage-Exposure-Update-Service. V. 1.0. GFZ Data Services. https://doi.org/10.5880/riesgos.2021.011\n\n## What is it?\n\nThis is the service to update a given exposure file (as it is the output\nof the assetmaster script) and update the building and damage classes\nwith given fragility functions and intensity values.\n\n## Copyright & License\nCopyright © 2021 Helmholtz Centre Potsdam GFZ German Research Centre for Geosciences, Potsdam, Germany\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n\nhttps://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n\n## Project\n\nDeus was developed in the scope of the RIESGOS project:\nMulti-Risk Analysis and Information System Components for the Andes Region (https://www.riesgos.de/en)\n\n\n## Documentation\n\nYou can look up several documentation pages:\n\n- [Setup and installation](doc/Setup.md)\n- [Example run](doc/ExposureModel.md)\n- [Shakemaps](doc/EarthQuakeShakemap.md) and [Intensity files](doc/IntensityFile.md)\n- [Exposure models](doc/ExposureModel.md)\n- [Fragility functions](doc/FragilityFunctions.md)\n- [Loss](doc/LossData.md)\n- [Schema mappings](doc/SchemaMapping.md)\n\n## Scope of deus\n\nDeus was created in the riesgos project for working in multi risk scenarios.\nIt should be provided as a web processing service by the GFZ.\n\n## You still have questions\n\nIf we don't cover important things in the documentation, please feel free to\ncreate an issue or send a mail at\n<nils.brinckmann@gfz-potsdam.de> or <pittore@gfz-potsdam.de>.\n\n## Can I use deus for xyz?\n\nYes! But you may have to code a bit yourself. The code is written against interfaces\nand already provides several implementations for some of them.\n\nAims for the following development of deus is the support of more and more\nhazards with their intensity files, their fragility functions and their schemas.\n\nYou can also take a look into the [TODOs](TODO.md).\n\n## Will there only be one deus?\n\nThere is deus and there is volcanus (a special deus version \"volcanus\"\nthat works with shapefiles for intensities - it uses a column LOAD and a unit of kPa -\nto allow a special ashfall service in the RIESGOS demonstrator).\n\nThe two services only differ in the intensity provider.\n\n",
                "dependencies": "# Copyright © 2021-2022 Helmholtz Centre Potsdam GFZ German Research Centre for\n# Geosciences, Potsdam, Germany\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n# use this file except in compliance with the License. You may obtain a copy of\n# the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations under\n# the License.\n\naffine==2.3.0\natomicwrites==1.3.0\nattrs==19.1.0\nClick==7.0\nclick-plugins==1.1.1\ncligj==0.5.0\ncoverage==4.5.4\ncycler==0.10.0\ndecorator==4.4.0\nDeprecated==1.2.6\ndescartes==1.1.0\ndocopt==0.6.2\nFiona==1.8.18\nGDAL==2.4.0\ngeopandas==0.5.1\nimageio==2.5.0\nimportlib-metadata==0.19\njoblib==0.17.0\nkiwisolver==1.1.0\nlxml==4.6.2\nmatplotlib==3.1.1\nmore-itertools==7.2.0\nmunch==2.3.2\nnetworkx==2.3\nnumpy==1.17.4\npackaging==19.1\npalettable==3.3.0\npandas==0.25.0\nPillow==7.2.0\npluggy==0.12.0\npy==1.8.0\npyparsing==2.4.2\npyproj==2.2.1\npysal==2.1.0\npytest==5.0.1\npython-dateutil==2.8.0\npytz==2019.2\nPyWavelets==1.1.1\nrasterio==1.0.28\nrasterstats==0.13.1\nRtree==0.9.7\nscikit-image==0.17.2\nscikit-learn==0.23.0\nscipy==1.3.0\nseaborn==0.9.0\nShapely==1.6.4.post2\nsimplejson==3.16.0\nsix==1.12.0\nsnuggs==1.4.6\ntqdm==4.35.0\nwcwidth==0.1.7\nwrapt==1.11.2\nzipp==0.5.2\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/digital-earth-viewer",
            "repo_link": "https://git.geomar.de/digital-earth/digital-earth-viewer",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/dirschema",
            "repo_link": "https://github.com/Materials-Data-Science-and-Informatics/dirschema",
            "content": {
                "codemeta": "{\n    \"@context\": [\n        \"https://doi.org/10.5063/schema/codemeta-2.0\",\n        \"https://w3id.org/software-iodata\",\n        \"https://raw.githubusercontent.com/jantman/repostatus.org/master/badges/latest/ontology.jsonld\",\n        \"https://schema.org\",\n        \"https://w3id.org/software-types\"\n    ],\n    \"@type\": \"SoftwareSourceCode\",\n    \"applicationCategory\": [\n        \"Scientific/Engineering\",\n        \"Software Development\"\n    ],\n    \"audience\": [\n        {\n            \"@type\": \"Audience\",\n            \"audienceType\": \"Developers\"\n        },\n        {\n            \"@type\": \"Audience\",\n            \"audienceType\": \"Science/Research\"\n        }\n    ],\n    \"author\": [\n        {\n            \"@id\": \"https://orcid.org/0000-0002-5077-7497\",\n            \"@type\": \"Person\",\n            \"affiliation\": {\n                \"@type\": \"Organization\",\n                \"legalName\": \"Forschungszentrum Jülich GmbH - Institute for Materials Data Science and Informatics (IAS9)\"\n            },\n            \"familyName\": \"Pirogov\",\n            \"givenName\": \"Anton\"\n        }\n    ],\n    \"codeRepository\": \"https://github.com/Materials-Data-Science-and-Informatics/dirschema\",\n    \"description\": \"Spec and validator for directories, files and metadata based on JSON Schema and regexes.\",\n    \"developmentStatus\": \"https://www.repostatus.org/#wip\",\n    \"identifier\": \"dirschema\",\n    \"keywords\": [\n        \"directory\",\n        \"fair\",\n        \"jsonschema\",\n        \"metadata\",\n        \"structure\",\n        \"validation\"\n    ],\n    \"license\": \"http://spdx.org/licenses/MIT\",\n    \"maintainer\": {\n        \"@type\": \"Person\",\n        \"email\": \"a.pirogov@fz-juelich.de\",\n        \"familyName\": \"Pirogov\",\n        \"givenName\": \"Anton\"\n    },\n    \"name\": \"dirschema\",\n    \"operatingSystem\": \"OS Independent\",\n    \"runtimePlatform\": \"Python 3\",\n    \"softwareHelp\": \"https://materials-data-science-and-informatics.github.io/dirschema\",\n    \"softwareRequirements\": [\n        {\n            \"@type\": \"SoftwareApplication\",\n            \"identifier\": \"entrypoints\",\n            \"name\": \"entrypoints\",\n            \"runtimePlatform\": \"Python 3\",\n            \"version\": \"^0.4\"\n        },\n        {\n            \"@type\": \"SoftwareApplication\",\n            \"identifier\": \"h5py\",\n            \"name\": \"h5py\",\n            \"runtimePlatform\": \"Python 3\",\n            \"version\": \"^3.4.0\"\n        },\n        {\n            \"@type\": \"SoftwareApplication\",\n            \"identifier\": \"jsonref\",\n            \"name\": \"jsonref\",\n            \"runtimePlatform\": \"Python 3\",\n            \"version\": \"^0.2\"\n        },\n        {\n            \"@type\": \"SoftwareApplication\",\n            \"identifier\": \"jsonschema\",\n            \"name\": \"jsonschema\",\n            \"runtimePlatform\": \"Python 3\",\n            \"version\": \"^4.4.0\"\n        },\n        {\n            \"@type\": \"SoftwareApplication\",\n            \"identifier\": \"numpy\",\n            \"name\": \"numpy\",\n            \"runtimePlatform\": \"Python 3\",\n            \"version\": \"^1.21.2\"\n        },\n        {\n            \"@type\": \"SoftwareApplication\",\n            \"identifier\": \"pydantic\",\n            \"name\": \"pydantic\",\n            \"runtimePlatform\": \"Python 3\",\n            \"version\": \"^1.8.2\"\n        },\n        {\n            \"@type\": \"SoftwareApplication\",\n            \"identifier\": \"python\",\n            \"name\": \"python\",\n            \"runtimePlatform\": \"Python 3\",\n            \"version\": \"^3.8\"\n        },\n        {\n            \"@type\": \"SoftwareApplication\",\n            \"identifier\": \"ruamel.yaml\",\n            \"name\": \"ruamel.yaml\",\n            \"runtimePlatform\": \"Python 3\",\n            \"version\": \"^0.17.16\"\n        },\n        {\n            \"@type\": \"SoftwareApplication\",\n            \"identifier\": \"typer\",\n            \"name\": \"typer\",\n            \"runtimePlatform\": \"Python 3\",\n            \"version\": \"^0.9.0\"\n        },\n        {\n            \"@type\": \"SoftwareApplication\",\n            \"identifier\": \"typing-extensions\",\n            \"name\": \"typing-extensions\",\n            \"runtimePlatform\": \"Python 3\",\n            \"version\": \"^4.5.0\"\n        }\n    ],\n    \"targetProduct\": {\n        \"@type\": \"CommandLineApplication\",\n        \"executableName\": \"dirschema\",\n        \"name\": \"dirschema\",\n        \"runtimePlatform\": \"Python 3\"\n    },\n    \"url\": \"https://materials-data-science-and-informatics.github.io/dirschema\",\n    \"version\": \"0.1.0\"\n}\n",
                "readme": "![Project status](https://img.shields.io/badge/project%20status-alpha-%23ff8000)\n[\n![Docs](https://img.shields.io/badge/read-docs-success)\n](https://materials-data-science-and-informatics.github.io/dirschema)\n[\n![CI](https://img.shields.io/github/actions/workflow/status/Materials-Data-Science-and-Informatics/dirschema/ci.yml?branch=main&label=ci)\n](https://github.com/Materials-Data-Science-and-Informatics/dirschema/actions/workflows/ci.yml)\n[\n![Test Coverage](https://materials-data-science-and-informatics.github.io/dirschema/main/coverage_badge.svg)\n](https://materials-data-science-and-informatics.github.io/dirschema/main/coverage)\n[\n![PyPIPkgVersion](https://img.shields.io/pypi/v/dirschema)\n](https://pypi.org/project/dirschema/)\n\n<!-- --8<-- [start:abstract] -->\n# dirschema\n\n<br />\n<div>\n<img style=\"center-align: middle;\" alt=\"DirSchema Logo\" src=\"https://raw.githubusercontent.com/Materials-Data-Science-and-Informatics/Logos/main/DirSchema/DirSchema_Logo_Text.png\" width=70% height=70% />\n&nbsp;&nbsp;\n</div>\n<br />\n\nA directory structure and metadata linter based on JSON Schema.\n\n[JSON Schema](https://json-schema.org/) is great for validating (files containing) JSON\nobjects that e.g. contain metadata, but these are only the smallest pieces in the\norganization of a whole directory structure, e.g. of some dataset of project.\nWhen working on datasets of a certain kind, they might contain various types of data,\neach different file requiring different accompanying metadata, based on its file type\nand/or location.\n\n**DirSchema** combines JSON Schemas and regexes into a solution to enforce structural\ndependencies and metadata requirements in directories and directory-like archives.\nWith it you can for example check that:\n\n* only files of a certain type are in a location (e.g. only `jpg` files in directory `img`)\n* for each data file there exists a metadata file (e.g. `test.jpg` has `test.jpg_meta.json`)\n* each metadata file is valid according to some JSON Schema\n\nIf validating these kinds of constraints looks appealing to you, this tool is for you!\n\n**Dirschema features:**\n\n* Built-in support for schemas and metadata stored as JSON or YAML\n* Built-in support for checking contents of ZIP and HDF5 archives\n* Extensible validation interface for advanced needs beyond JSON Schema\n* Both a Python library and a CLI tool to perform the validation\n\n<!-- --8<-- [end:abstract] -->\n<!-- --8<-- [start:quickstart] -->\n\n## Installation\n\n```\npip install dirschema\n```\n\n## Getting Started\n\nThe `dirschema` tool needs as input:\n\n* a DirSchema YAML file (containing a specification), and\n* a path to a directory or file (e.g. zip file) that should be checked.\n\nYou can run it like this:\n\n```\ndirschema my_dirschema.yaml DIRECTORY_OR_ARCHIVE_PATH\n```\n\nIf the validation was successful, there will be no output.\nOtherwise, the tool will output a list of errors (e.g. invalid metadata, missing files, etc.).\n\nYou can also use `dirschema` from other Python code as a library:\n\n```python\nfrom dirschema.validate import DSValidator\nDSValidator(\"/path/to/dirschema\").validate(\"/dataset/path\")\n```\n\nSimilarly, the method will return an error dict, which will be empty if the validation succeeded.\n\n<!-- --8<-- [end:quickstart] -->\n\n**You can find more information on using and contributing to this repository in the\n[documentation](https://materials-data-science-and-informatics.github.io/dirschema/main).**\n\n<!-- --8<-- [start:citation] -->\n\n## How to Cite\n\nIf you want to cite this project in your scientific work,\nplease use the [citation file](https://citation-file-format.github.io/)\nin the [repository](https://github.com/Materials-Data-Science-and-Informatics/dirschema/blob/main/CITATION.cff).\n\n<!-- --8<-- [end:citation] -->\n<!-- --8<-- [start:acknowledgements] -->\n\n## Acknowledgements\n\nWe kindly thank all\n[authors and contributors](https://materials-data-science-and-informatics.github.io/dirschema/latest/credits).\n\n<div>\n<img style=\"vertical-align: middle;\" alt=\"HMC Logo\" src=\"https://github.com/Materials-Data-Science-and-Informatics/Logos/raw/main/HMC/HMC_Logo_M.png\" width=50% height=50% />\n&nbsp;&nbsp;\n<img style=\"vertical-align: middle;\" alt=\"FZJ Logo\" src=\"https://github.com/Materials-Data-Science-and-Informatics/Logos/raw/main/FZJ/FZJ.png\" width=30% height=30% />\n</div>\n<br />\n\nThis project was developed at the Institute for Materials Data Science and Informatics\n(IAS-9) of the Jülich Research Center and funded by the Helmholtz Metadata Collaboration\n(HMC), an incubator-platform of the Helmholtz Association within the framework of the\nInformation and Data Science strategic initiative.\n\n<!-- --8<-- [end:acknowledgements] -->\n\n",
                "dependencies": "[tool.somesy.project]\nname = \"dirschema\"\nversion = \"0.1.0\"\ndescription = \"Spec and validator for directories, files and metadata based on JSON Schema and regexes.\"\nlicense = \"MIT\"\nrepository = \"https://github.com/Materials-Data-Science-and-Informatics/dirschema\"\nhomepage = \"https://materials-data-science-and-informatics.github.io/dirschema\"\ndocumentation = \"https://materials-data-science-and-informatics.github.io/dirschema\"\nkeywords = [\"jsonschema\", \"validation\", \"directory\", \"structure\", \"fair\", \"metadata\"]\n\n[[tool.somesy.project.people]]\nfamily-names = \"Pirogov\"\ngiven-names = \"Anton\"\nemail = \"a.pirogov@fz-juelich.de\"\norcid = \"https://orcid.org/0000-0002-5077-7497\"\n\ncontribution_begin = \"2021-09-22\"\ncontribution = \"Main author and maintainer.\"\ncontribution_types = [\"code\"]\n\nauthor = true\nmaintainer = true\n\n[tool.poetry]\n# ---- DO NOT EDIT, managed by somesy ----\nname = \"dirschema\"\nversion = \"0.1.0\"\ndescription = \"Spec and validator for directories, files and metadata based on JSON Schema and regexes.\"\nauthors = [\"Anton Pirogov <a.pirogov@fz-juelich.de>\"]\nlicense = \"MIT\"\nrepository = \"https://github.com/Materials-Data-Science-and-Informatics/dirschema\"\nhomepage = \"https://materials-data-science-and-informatics.github.io/dirschema\"\ndocumentation = \"https://materials-data-science-and-informatics.github.io/dirschema\"\nkeywords = [\"jsonschema\", \"validation\", \"directory\", \"structure\", \"fair\", \"metadata\"]\n# ----------------------------------------\nreadme = \"README.md\"\nclassifiers = [ # see https://pypi.org/classifiers/\n  \"Development Status :: 3 - Alpha\",\n  \"License :: OSI Approved :: MIT License\",\n  \"Environment :: Console\",\n  \"Operating System :: OS Independent\",\n  \"Intended Audience :: Developers\",\n  \"Intended Audience :: Science/Research\",\n  \"Topic :: Software Development\",\n  \"Topic :: Scientific/Engineering\",\n  \"Typing :: Typed\",\n]\n\n# the Python packages that will be included in a built distribution:\npackages = [{include = \"dirschema\", from = \"src\"}]\n\n# always include basic info for humans and core metadata in the distribution,\n# include files related to test and documentation only in sdist:\ninclude = [\n  \"*.md\",\n  \"LICENSE\", \"LICENSES\", \".reuse/dep5\",\n  \"CITATION.cff\", \"codemeta.json\",\n  { path = \"mkdocs.yml\", format = \"sdist\" },\n  { path = \"docs\", format = \"sdist\" },\n  { path = \"tests\", format = \"sdist\" },\n]\nmaintainers = [\"Anton Pirogov <a.pirogov@fz-juelich.de>\"]\n\n[tool.poetry.dependencies]\npython = \"^3.8,<3.11\"\npydantic = \"^1.8.2\"\n\"ruamel.yaml\" = \"^0.17.16\"\njsonref = \"^0.2\"\nh5py = { version = \"^3.4.0\", optional = true }\nnumpy = { version = \"^1.21.2\", optional = true }\njsonschema = \"^4.4.0\"\nentrypoints = \"^0.4\"\ntyping-extensions = \"^4.5.0\"\ntyper = \"^0.9.0\"\n\n[tool.poetry.group.dev.dependencies]\npoethepoet = \"^0.18.1\"\npre-commit = \"^3.1.1\"\npytest = \"^7.2.2\"\npytest-cov = \"^4.0.0\"\nhypothesis = \"^6.68.2\"\nlicensecheck = \"^2023.1.1\"\n\n[tool.poetry.group.docs.dependencies]\nmkdocs = \"^1.4.2\"\nmkdocstrings = {extras = [\"python\"], version = \"^0.21.2\"}\nmkdocs-material = \"^9.1.6\"\nmkdocs-gen-files = \"^0.4.0\"\nmkdocs-literate-nav = \"^0.6.0\"\nmkdocs-section-index = \"^0.3.5\"\nmkdocs-macros-plugin = \"^0.7.0\"\nmarkdown-include = \"^0.8.1\"\npymdown-extensions = \"^9.11\"\nmarkdown-exec = {extras = [\"ansi\"], version = \"^1.6.0\"}\nmkdocs-coverage = \"^0.2.7\"\nmike = \"^1.1.2\"\nanybadge = \"^1.14.0\"\nblack = \"^23.3.0\"\n\n[tool.poetry.group.all-extras.dependencies]\nh5py = \"^3.8.0\"\nnumpy = \"^1.24.3\"\n\n[tool.poetry.extras]\nh5 = [\"h5py\", \"numpy\"]\n\n[tool.poetry.scripts]\ndirschema = 'dirschema.cli:app'\n\n[tool.poetry.plugins.dirschema_validator]\n# dirschema supports entry-point-based validation plugins.\n# The default pydantic handler can be subclassed or\n# can serve as a template for your custom validation plugins.\npydantic = \"dirschema.json.handler_pydantic:PydanticHandler\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n# NOTE: You can run the following with \"poetry poe TASK\"\n[tool.poe.tasks]\ninit-dev = { shell = \"pre-commit install\" }\nlint = \"pre-commit run\"  # pass --all-files to check everything\ntest = \"pytest\"  # pass --cov to also collect coverage info\ndocs = \"mkdocs build\"  # run this to generate local documentation\nlicensecheck = \"licensecheck\"  # run this when you add new deps\n\n# Tool Configurations\n# -------------------\n\n[tool.pytest.ini_options]\npythonpath = [\"src\"]\naddopts = \"--cov-report=term-missing:skip-covered\"\nfilterwarnings = [\n# Example:\n# \"ignore::DeprecationWarning:importlib_metadata.*\"\n]\n\n[tool.coverage.run]\nsource = [\"dirschema\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"if self.debug:\",\n    \"if settings.DEBUG\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n    \"if 0:\",\n    \"if TYPE_CHECKING:\",\n    \"if __name__ == .__main__.:\",\n    \"class .*\\\\bProtocol\\\\):\",\n    \"@(abc\\\\.)?abstractmethod\",\n]\n\n[tool.ruff.lint]\nextend-select = [\"B\", \"D\", \"I\", \"S\"]\nignore = [\"D203\", \"D213\", \"D407\", \"B008\"]\n\n[tool.ruff.lint.per-file-ignores]\n\"**/{tests,docs}/*\" = [\"ALL\"]\n\n[tool.licensecheck]\nusing = \"poetry\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/displam",
            "repo_link": "https://gitlab.com/dlr-sy/displam",
            "content": {
                "codemeta": "",
                "readme": "[![PyPi](https://img.shields.io/pypi/v/displam?label=PyPi)](https://pypi.org/project/displam/)\n[![doi](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.12628293-red.svg)](https://zenodo.org/records/12628293)\n[![pipeline status](https://gitlab.com/dlr-sy/displam/badges/disp_python/pipeline.svg)]()\n\n# Displam\nDisplam is a legacy Fortran-based program for the calculation of the phase velocity of Lamb waves of plate composite structures.\n> Installation from source requires an active open source fortran compiler (gfortran). Intel Fortran is not yet supported. \n## Downloading\nUse GIT to get the latest code base. From the command line, use\n```\ngit clone https://gitlab.dlr.de/fa_sw/displam displam\n```\nIf you check out the repository for the first time, you have to initialize all submodule dependencies first. Execute the following from within the repository. \n```\ngit submodule update --init --recursive\n```\nTo update all refererenced submodules to the latest production level, use\n```\ngit submodule foreach --recursive 'git pull origin $(git config -f $toplevel/.gitmodules submodule.$name.branch || echo master)'\n```\n\n## Installation\nDisplam can be installed directly using pip\n```\npip install displam\n```\nor from source using [poetry](https://python-poetry.org). If you don't have [poetry](https://python-poetry.org) installed, run\n```\npip install poetry --pre --upgrade\n```\nto install the latest version of [poetry](https://python-poetry.org) within your python environment. Use\n```\npoetry update\n```\nto update all dependencies in the lock file or directly execute\n```\npoetry install\n```\nto install all dependencies from the lock file. Last, you should be able to use Displam as a CLI tool:\n```cmd\ndisplam <Input file>\n```  \nIf you omit the input file name displam will try to read the input data from a file named sample.inp. An output file disp.txt and a log file log.txt will be created. \n\n## Output\nThe output file contains the dispersion data for the problem defined in the input file. A header line is followed by one record per line. Each record consists of the following data:\n```\n  1. fd/(MHz*mm)  - frequency * plate thickness\n  2. cp/(km/sec)  - phase velocity\n  3. w/(1/�sec)   - circular frequency\n  4. k/(1/m)      - wave number\n  5. mode         - wave mode ('S' for symmetric, 'A' for antisymmetric\n                    and 'H' for quasi-horizontal shear mode)\n  6. uu0 - uu2    - complex displacement vector at the upper surface of\n                    the laminate\n  7. ul0 - ul2    - complex displacement vector at the lower surface of\n                    the laminate\n```\nThe columns are seperated by tab-characters. You can open the output file with Excel or any text editor.\n\n## Example\nCopy and paste the following text up to the end of this file to a new text file to use it as a sample input file as explained under [Installation](#Installation)\n```\n#\n# Input file for displam\n#\n############################################################### File format ###\n#\n#   Input is parsed line by line. Everything after a '#' character is ignored,\n#   so it's possible to append comments to input lines. Empty lines are \n#   ignored. White space separates keywords and input parameters and may be\n#   entered as spaces or tabs. Lines may not be longer than 1000 characters\n#   (excluding comments).\n#\n#   The following four sections must be present in this file. Each section is\n#   initiated with its keyword in capital letters in an otherwise empty line.\n#\n#     LAYUP    - stacking sequence of the plate\n#     WAVE     - wave propagation parameters\n#     RANGE    - parameter ranges for circular frequency and phase velocity\n#     MATERIAL - material database\n#\n#   The format of the input lines in each section is explained in the comments\n#   above each section.\n\n############################################################# LAYUP section ###\n#\n#   The layers of the stacking sequence are specified from top to bottom\n#   of the laminate. Three parameters must be specified per line:\n#\n#     1. material id string as specified in the MATERIAL section\n#     2. layer thickness in mm\n#     3. layer orientation in degrees\n#\n#   A layer orientation of 0 degree means that the 1-direction of local (layer\n#   material) and global (laminate) direction coincide.\n#\nLAYUP\n\tcfrp_generic   \t\t0.250\t 0.\n\ttitanium    \t\t0.500\t90.\n\tcfrp_generic\t\t0.250\t 0.\n\n############################################################## WAVE section ###\n#\n#   pa - angle of wave propagation w.r.t. global coordinates in degrees.\n#   ia - angle of incident wave w.r.t. transverse axis in degrees.\n#        90.0 is horizontal (in-plane) incident wave.\nWAVE\n\tpa\t\t 0.01\n\tia\t\t90.00\t\t# values other than 90� are not validated yet\n\n############################################################# RANGE section ###\n#\n#   cp - 3 values for start, end, and number of increments for phase velocity.\n#        Unit of phase velocity is in km / s.\n#   fq - 3 values for start, end, and number of increments for frequency.\n#        Unit of frequencies must be MHz.\nRANGE\n\tcp\t\t0.1\t\t14.0\t 200\t# unit is km/s\n\tfq\t\t0.1\t\t 3.0\t 200\t# unit is MHz\n\n########################################################## MATERIAL section ###\n#\n#   Each line starts with an identification string of up to 12 alphanumeric\n#   characters and '_'. It is followed by the number of independent stiffness\n#   constants (nsc) and the density in g / cm^3.\n#\n#   Transversely isotropic (nsc = 5) and orthotropic (nsc = 9) materials are\n#   supported. After the density the stiffness parameters must be specified\n#   in GPa for Young's and shear moduli in the following order\n#\n#     transversely isotropic  E1, E2, G12, G23, nu12\n#     orthotropic             E1, E2, E3, G12, G23, G31, nu12, nu23, nu31\n#\n#   The 3-direction is the out-of-plane direction.\n#\nMATERIAL\n#\t123456789012  nsc\t\t rho\t\tstiffness coefficients\n\talu             5\t\t2.70\t\t 69.9\t 70.1\t26.30\t26.20\t0.33\n\ttitanium        5\t\t4.50\t\t115.9\t116.1\t43.95\t43.85\t0.32\n\tcfrp_generic    5\t\t1.55\t\t150.0\t  9.00\t 5.00\t 4.00\t0.30\n\tcfrp_rose       5\t\t1.58\t\t123.9\t 11.256\t 6.73\t 3.81\t0.31\n```\n\n## Contact\n* [Marc Garbade](mailto:marc.garbade@dlr.de)\n\n",
                "dependencies": "# TOML file create Displam\n#  \n# @note: TOML file            \n# Created on 29.11.2023    \n# \n# @version:  1.0    \n# ----------------------------------------------------------------------------------------------\n# @requires:\n#        - \n# \n# @change: \n#        -    \n#    \n# @author: garb_ma                                                     [DLR-SY,STM Braunschweig]\n# ----------------------------------------------------------------------------------------------\n[build-system]\nrequires = [\"poetry-core>=1.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.poetry]\nname = \"displam\"\nversion = \"1.5.1\"\ndescription = \"Calculation of the phase velocity of Lamb waves in plate composite structures\"\nauthors = [\"Baaran, Jens <jens.baaran@haw-hamburg.de>\"]\nmaintainers = [\"Garbade, Marc <marc.garbade@dlr.de>\",\n               \"Raddatz, Florian <florian.raddatz@dlr.de>\",\n               \"Schmidt, Daniel <daniel.schmidt@dlr.de>\",\n               \"Moix-Bonet, Maria <maria.moix-bonet@dlr.de>\"]\nlicense = \"MIT\"\npackages = [{include=\"**/*\", from=\"bin\"}]\nexclude = [\"bin/*.py\"]\nrepository = \"https://gitlab.com/dlr-sy/displam\"\nkeywords = [\"analysis\",\"monitoring\",\"composite\"]\nreadme = \"README.md\"\nclassifiers = [\n    \"Development Status :: 7 - Inactive\",\n    \"Topic :: Scientific/Engineering\",\n    \"Programming Language :: Python :: 2\",\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\"\n]\n\n[[tool.poetry.source]]\nname = \"dlr-pypi\"\nurl = \"https://pypi.python.org/simple\"\npriority = \"primary\"\n\n[[tool.poetry.source]]\nname = \"dlr-sy\"\nurl = \"https://gitlab.dlr.de/api/v4/groups/541/-/packages/pypi/simple\"\npriority = \"supplemental\"\n\n[tool.poetry.build]\nscript = \"config/build.py\"\ngenerate-setup-file = false\n\n[tool.poetry.dependencies]\npython = \"~2.7 || ^3.5\"\n\n[tool.poetry.group.dev.dependencies]\npyx-core = [{version = \"^1.18\", python = \"~2.7 || ^3.5,<3.7\"},\n            {git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake.git\", develop=true, python=\"^3.7\"}]\n\t\t\t\n[tool.poetry.scripts]\ndisplam = \"displam.cli:main\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/dl4pude",
            "repo_link": "https://github.com/PedestrianDynamics/DL4PuDe",
            "content": {
                "codemeta": "",
                "readme": "# `DL4PuDe:` A hybrid framework of deep learning and visualization for pushing behavior detection in pedestrian dynamics\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8257076.svg)](https://doi.org/10.5281/zenodo.8257076\n) [![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](./LICENSE)  ![Python 3.7 | 3.8](https://img.shields.io/badge/Python-3.7|3.8-blue.svg)  ![GPU](https://img.shields.io/badge/GPU-No-yellow) ![RAM16GB](https://img.shields.io/badge/RAM-16GB-red)  \n\nThis repository is for the DL4PuDe framework, along with its  published [paper](https://www.mdpi.com/1424-8220/22/11/4040/htm), which is as follows.\n```\nAlia, Ahmed, Mohammed Maree, and Mohcine Chraibi. 2022. \"A Hybrid Deep Learning and Visualization Framework for Pushing Behavior Detection in Pedestrian Dynamics\" Sensors 22, no. 11: 4040. \n```\n## Content\n1. <a href=\"#aim\"> Framework aim. </a>\n2. <a href=\"#motivation\"> Framework Motivation. </a>\n3. <a href=\"#defention\"> Pushing Behavior Defention. </a>\n4. <a href=\"#architecture\"> Framework Architecture. </a>\n5. <a href=\"#install\"> How to install and use the framework. </a>\n6. <a href=\"#demo\"> Demo. </a>\n7. <a href=\"#videos\"> Experiments Videos. </a>\n8. <a href=\"#cnn\"> CNN-based Classifiers </a>\n    * <a href=\"#cnnsource\"> Source code for building and training CNN-based classifiers. </a>\n    * <a href=\"#trained\"> Trained CNN-based classifiers. </a>\n    * <a href=\"#evaluate\"> Source code for evaluating the trained CNN-based classifiers. </a>\n    *  <a href=\"#test\"> Test sets. </a>\n9. <a href=\"#list\"> List of papers that cited this work. </a>\n\n## Aim of `Dl4PuDe` Framework\n<a name=\"aim\">\n\n`Dl4PuDe`  aims to automatically detect and annotate pushing behavior at the patch level in video recordings of human crowds. \n\n## Motivation of `Dl4PuDe` Framework\n<a name=\"motivation\">\n\nTo assist researchers in the field of crowd dynamics in gaining a better understanding of pushing dynamics, which is crucial for effectively managing a comfortable and safe crowd.\n\n## Pushing Behavior Defention\n<a name=\"defention\">\n\nIn this article, pushing can be defined as a behavior that pedestrians use to reach a target faster.\n\n\n###### An example of pushing strategy\n<img src=\"./files/example.gif\" width=\"300px\"/>\n\n###### Entering the event faster\n<img src=\"./files/snakemotion.jpg\" width=\"300px\"/>\n \n\n## The Architecture of `DL4PuDe`\n<a name=\"architecture\">\n\n`DL4PuDe` mainly relied on the power of EfficientNet-B0-based classifier, RAFT and wheel visualization methods.\n\n<img src=\"./files/framework1.png\"/>\nKindly note that we use the <a href=\"https://github.com/princeton-vl/RAFT\" />[RAFT repository] </a> for optical flow estimation in our project.\n\n**Example**\n<table border=\"0\" width=\"100%\" align=\"center\">\n<tr>\n   <th align=\"cenetr\"> Input video </th>\n   <th align=\"cenetr\"> Output video * </th>\n   \n</tr>\n<tr>\n   <td align=\"center\"> <img src=\"./files/input150-distorted.gif\" width=\"300\"/> </td>\n   <td align=\"center\"> <img src=\"./files/output150-distorted.gif\" width=\"200\"/> </td>\n</tr>\n<tr>\n   <td colspan=\"2\"> * The framework detects pushing patches every 12 frames (12/25 s), the red boxes refer to the pushing patches. </td>\n</tr>\n</table>\n\n## Installation\n<a name=\"install\">\n\n1. Clone the repository in your directory.\n```\ngit clone https://github.com/PedestrianDynamics/DL4PuDe.git\n```\n2. Install the required libraries.\n```\npip install -r libraries.txt\n```\n3. Run the framework. \n```\npython3 run.py --video [input video path]  \n               --roi [\"x coordinate of left-top ROI corner\" \"y coordinate of left-top ROI corner\"\n               \"x coordinate of  right-bottom ROI corner\" \"y coordinate of right-bottom ROI corner\" ] \n               --patch [rows cols]    \n               --ratio [scale of video]   \n               --angle [angle in degrees for rotating the input video to make crowd flow direction\n               from left to right ---> ]\n```   \n## Demo \n<a name=\"demo\">\n\n>Run the following command\n\n```   \npython3 run.py --video ./videos/150.mp4  --roi 380 128 1356 1294 --patch 3 3 --ratio 0.5  --angle 0\n```  \n> Then, you will see the following details.\n\n<img src=\"./files/run.png\"/>\n\n> When the progress of the framework is complete, it will generate the annotated video in the framework directory. Please note that the \"150 annotated video\" is available on the directory root under the \"150-demo.mp4\" name.\n\n## Experiments Videos\n<a name=\"videos\">\n\nThe original experiments videos that are used in this work are available through the [Pedestrian Dynamics Data Archive hosted](http://ped.fz-juelich.de/da/2018crowdqueue) by the Forschungszentrum Juelich. Also, the undistorted videos are available by [this link.](https://drive.google.com/drive/folders/16eZhC9mnUQUXxUeIUXd6xwBU2fSf3qCz?usp=sharing) \n\n## CNN-based Classifiers\n<a name=\"cnn\">\n\nWe use four CNN-based classifiers for building and evaluating our classifier, including EfficientNet-B0, MobileNet, InceptionV3, and ResNet50. The source code for building, training and evaluating the CNN-based classifiers, as well as the trained classifiers are available in the below links.\n1. Source code for building and training the CNN-based classifiers. <a name=\"cnnsource\">\n   * [EfficientNet-B0-based classifier.](./CNN/CNN-Architectures/efficientNetB0.ipynb)\n   * [MobileNet-based classifier.](./CNN/CNN-Architectures/InceptionV3.ipynb)\n   * [InceptionV3-based classifier.](./CNN/CNN-Architectures/InceptionV3.ipynb)\n   * [ResNet50-based classifier.](./CNN/CNN-Architectures/ResNet50.ipynb)\n2. [Trained CNN-based classifiers.](https://drive.google.com/drive/folders/1vmgYufnt4_NNQUE9PGYZLkrn5DmErENu?usp=sharing) <a name=\"trained\">\n3. CNN-based classifiers Evaluation. <a name=\"#evaluate\">\n   * [Patch-based medium RAFT-MIM12 dataset.](./CNN/Classifiers-evaluation/patch-based-medium-RAFT-MIM12/)\n   * [Patch-based medium RAFT-MIM25 dataset.](./CNN/Classifiers-evaluation/patch-based-medium-RAFT-MIM25/)\n   * [Patch-based small RAFT-MIM12 dataset.](./CNN/Classifiers-evaluation/patch-based-small-RAFT-MIM12/)\n   * [Patch-based small RAFT-MIM25 dataset.](./CNN/Classifiers-evaluation/patch-based-small-RAFT-MIM25/)\n   * [Patch-based medium FB-MIM12 dataset.](./CNN/Classifiers-evaluation/patch-based-medium-FB-MM12/)\n   * [Frame-based RAFT-MIM12 dataset.](./CNN/Classifiers-evaluation/frame-based-RAFT-MIM12/)\n   * [Frame-based RAFT-MIM25 dataset.](./CNN/Classifiers-evaluation/frame-based-RAFT-MIM25/)\n4. [Patch-based MIM test sets.](./CNN/Classifiers-evaluation/test-sets/) <a name=\"test\">\n5. MIM training and validation sets are available from the corresponding authors upon request.\n \n## List of papers that cited this work \n<a name=\"list\">\n\nTo access the list of papers citing this work, kindly click on this [link.](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14553227952079022657&as_sdt=5)\n\n## Citation\n\nIf you utilize this framework or the generated dataset in your work, please cite it using the following BibTex entry:\n```\nAlia, Ahmed, Mohammed Maree, and Mohcine Chraibi. 2022. \"A Hybrid Deep Learning and Visualization Framework for Pushing Behavior Detection in Pedestrian Dynamics\" Sensors 22, no. 11: 4040. \n```\n\n\n## Acknowledgments\n* This work was funded by the German Federal Ministry of Education and Research (BMBF: funding number 01DH16027) within the Palestinian-German Science Bridge project framework, and partially by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)—491111487.\n\n* Thanks to the Forschungszentrum Juelich, Institute for Advanced Simulation-7, for making the Pedestrian Dynamics Data Archive publicly accessible under the CC Attribution 4.0 International license.\n\n* Thanks to Anna Sieben, Helena Lügering, and Ezel Üsten for developing the rating system and annotating the pushing behavior in the video experiments.\n\n* Thanks to the authors of the paper titled ``RAFT: Recurrent All Pairs Field Transforms for Optical Flow'' for making the RAFT source code available.\n\n\n \n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/earth-system-model-evaluation-tool-esmvaltool",
            "repo_link": "https://github.com/ESMValGroup/ESMValTool",
            "content": {
                "codemeta": "",
                "readme": "[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity)\n[![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/)\n[![Documentation Status](https://readthedocs.org/projects/esmvaltool/badge/?version=latest)](https://esmvaltool.readthedocs.io/en/latest/?badge=latest)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3401363.svg)](https://doi.org/10.5281/zenodo.3401363)\n[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/#ESMValGroup_Lobby:gitter.im)\n[![CircleCI](https://circleci.com/gh/ESMValGroup/ESMValTool/tree/main.svg?style=svg)](https://circleci.com/gh/ESMValGroup/ESMValTool/tree/main)\n[![Test in Full Development Mode](https://github.com/ESMValGroup/ESMValTool/actions/workflows/test-development.yml/badge.svg)](https://github.com/ESMValGroup/ESMValTool/actions/workflows/test-development.yml)\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/79bf6932c2e844eea15d0fb1ed7e415c)](https://app.codacy.com/gh/ESMValGroup/ESMValTool/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade)\n[![Docker Build Status](https://img.shields.io/docker/automated/esmvalgroup/esmvaltool)](https://hub.docker.com/r/esmvalgroup/esmvaltool/)\n[![Anaconda-Server Badge](https://img.shields.io/conda/vn/conda-forge/ESMValTool?color=blue&label=conda-forge&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/esmvaltool)\n![stand with Ukraine](https://badgen.net/badge/stand%20with/UKRAINE/?color=0057B8&labelColor=FFD700)\n\n![esmvaltoollogo](https://raw.githubusercontent.com/ESMValGroup/ESMValTool/main/doc/sphinx/source/figures/ESMValTool-logo-2-glow.png)\n\n- [**Documentation**](https://docs.esmvaltool.org/en/latest/)\n- [**ESMValTool Website**](https://www.esmvaltool.org/)\n- [**ESMValTool Tutorial**](https://tutorial.esmvaltool.org/index.html)\n- [**ESMValGroup Project on GitHub**](https://github.com/ESMValGroup)\n- [**Gallery**](https://docs.esmvaltool.org/en/latest/gallery.html)\n- [**`conda-forge` package feedstock**](https://github.com/conda-forge/esmvaltool-suite-feedstock)\n\n# Introduction\n\nESMValTool is a community-developed climate model diagnostics and evaluation software package, driven\nboth by computational performance and scientific accuracy and reproducibility. ESMValTool is open to both\nusers and developers, encouraging open exchange of diagnostic source code and evaluation results from the\nCoupled Model Intercomparison Project [CMIP](https://www.wcrp-climate.org/wgcm-cmip) ensemble. For a\ncomprehensive introduction to ESMValTool please visit our\n[documentation](https://docs.esmvaltool.org/en/latest/introduction.html) page.\n\n# Running esmvaltool\n\nDiagnostics from ESMValTool are run using [recipe](https://docs.esmvaltool.org/en/latest/recipes/index.html)\nfiles that contain pointers to the requested data types, directives for the preprocessing steps that data\nwill be subject to, and directives for the actual diagnostics that will be run with the now preprocessed data.\nData preprocessing is done via the [ESMValCore](https://docs.esmvaltool.org/projects/ESMValCore/en/latest/quickstart/index.html) package, a pure Python, highly-optimized scientific library, developed by the ESMValTool core developers,\nand that performs a number of common analysis tasks\nsuch as regridding, masking, levels extraction etc. [Diagnostics](https://docs.esmvaltool.org/en/latest/develop/diagnostic.html) are written in a variety of programming languages (Python, NCL, R, Julia) and are developed by the wider\nscientific community, and included after a scientific and technical review process.\n\n# Input data\n\nESMValTool can run with the following types of [data as input](https://docs.esmvaltool.org/en/latest/input.html):\n\n- CMIP6\n- CMIP5\n- CMIP3\n- [observational and re-analysis datasets](https://docs.esmvaltool.org/en/latest/input.html#supported-datasets-for-which-a-cmorizer-script-is-available)\n- obs4MIPs\n- ana4mips\n- CORDEX ([work in progress](https://docs.esmvaltool.org/en/latest/input.html#cordex-note))\n\n# Getting started\n\nPlease see [getting started](https://docs.esmvaltool.org/en/latest/quickstart/index.html) on our instance of Read the Docs as well as [ESMValTool tutorial](https://tutorial.esmvaltool.org/index.html). The tutorial is a set of lessons that together teach skills needed to work with ESMValTool in climate-related domains.\n\n## Getting help\n\nThe easiest way to get help, if you cannot find the answer in the documentation in our [docs](https://docs.esmvaltool.org), is to open an [issue on GitHub](https://github.com/ESMValGroup/ESMValTool/issues).\n\n## Contributing\n\nIf you would like to contribute a new diagnostic or feature, please have a look at our [contribution guidelines](https://docs.esmvaltool.org/en/latest/community/index.html).\n\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools >= 40.6.0\", \"wheel\", \"setuptools_scm>=6.2\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools_scm]\nversion_scheme = \"release-branch-semver\"\n\n#!/usr/bin/env python\n\"\"\"ESMValTool installation script.\"\"\"\nimport json\nimport os\nimport re\nimport sys\nfrom pathlib import Path\n\nfrom setuptools import Command, setup\n\nPACKAGES = [\n    'esmvaltool',\n]\n\nREQUIREMENTS = {\n    # Installation script (this file) dependencies\n    'setup': [\n        'setuptools_scm',\n    ],\n    # Installation dependencies\n    # Use with pip install . to install from source\n    'install': [\n        'aiohttp',\n        'cartopy',\n        'cdo',\n        'cdsapi',\n        'cf-units',\n        'cfgrib',\n        'cftime',\n        'cmocean',\n        'dask!=2024.8.0',  # https://github.com/dask/dask/issues/11296\n        'distributed',\n        'ecmwf-api-client',\n        'eofs',\n        'ESMPy',  # not on PyPI\n        'esmvalcore',\n        'esmf-regrid>=0.10.0',  # iris-esmf-regrid #342\n        'fiona',\n        'fire',\n        'fsspec',\n        'GDAL',\n        'jinja2',\n        'joblib',\n        'lime',\n        'mapgenerator>=1.0.5',\n        'matplotlib',\n        'natsort',\n        'nc-time-axis',\n        'netCDF4',\n        'numba',\n        'numpy!=1.24.3',  # severe masking bug\n        'openpyxl',\n        'packaging',\n        'pandas==2.1.4',  # see note in environment.yml\n        'progressbar2',\n        'psyplot>=1.5.0',  # psy*<1.5.0 are not py312 compat\n        'psy-maps>=1.5.0',\n        'psy-reg>=1.5.0',\n        'psy-simple>=1.5.0',\n        'pyproj>=2.1',\n        'pys2index',\n        'python-dateutil',\n        'pyyaml',\n        'rasterio>=1.3.10',\n        'requests',\n        'ruamel.yaml',\n        'scikit-image',\n        'scikit-learn>=1.4.0',  # github.com/ESMValGroup/ESMValTool/issues/3504\n        'scipy',\n        'scitools-iris>=3.11',\n        'seaborn',\n        'seawater',\n        'shapely>=2',\n        'xarray>=0.12.0',\n        'xesmf>=0.7.1',\n        'xgboost>1.6.1',  # github.com/ESMValGroup/ESMValTool/issues/2779\n        'xlsxwriter',\n        'zarr',\n    ],\n    # Test dependencies (unit tests)\n    # Execute `pip install .[test]` once and then use `pytest` to run tests\n    'test': [\n        'flake8',\n        'pytest>=3.9,!=6.0.0rc1,!=6.0.0',\n        'pytest-cov>=2.10.1',\n        'pytest-env',\n        'pytest-html!=2.1.0',\n        'pytest-metadata>=1.5.1',\n        'pytest-mock',\n        'pytest-xdist',\n    ],\n    # Documentation dependencies\n    'doc': [\n        'autodocsumm>=0.2.2',\n        'nbsphinx',\n        'sphinx>=6.1.3',\n        'pydata-sphinx-theme',\n    ],\n    # Development dependencies\n    # Use pip install -e .[develop] to install in development mode\n    'develop': [\n        'codespell',\n        'docformatter',\n        'imagehash',\n        'isort',\n        'pre-commit',\n        'prospector[with_pyroma]>=1.12',\n        'vprof',\n        'yamllint',\n        'yapf',\n    ],\n}\n\n\ndef discover_python_files(paths, ignore):\n    \"\"\"Discover Python files.\"\"\"\n\n    def _ignore(path):\n        \"\"\"Return True if `path` should be ignored, False otherwise.\"\"\"\n        return any(re.match(pattern, path) for pattern in ignore)\n\n    for path in sorted(set(paths)):\n        for root, _, files in os.walk(path):\n            if _ignore(path):\n                continue\n            for filename in files:\n                filename = os.path.join(root, filename)\n                if (filename.lower().endswith('.py')\n                        and not _ignore(filename)):\n                    yield filename\n\n\nclass RunLinter(Command):\n    \"\"\"Class to run a linter and generate reports.\"\"\"\n\n    user_options = []\n\n    def initialize_options(self):\n        \"\"\"Do nothing.\"\"\"\n\n    def finalize_options(self):\n        \"\"\"Do nothing.\"\"\"\n\n    def install_deps_temp(self):\n        \"\"\"Try to temporarily install packages needed to run the command.\"\"\"\n        if self.distribution.install_requires:\n            self.distribution.fetch_build_eggs(\n                self.distribution.install_requires)\n        if self.distribution.tests_require:\n            self.distribution.fetch_build_eggs(self.distribution.tests_require)\n\n    def run(self):\n        \"\"\"Run prospector and generate a report.\"\"\"\n        check_paths = PACKAGES + [\n            'setup.py',\n            'tests',\n            'util',\n        ]\n        ignore = [\n            'doc/',\n        ]\n\n        # try to install missing dependencies and import prospector\n        try:\n            from prospector.run import main\n        except ImportError:\n            # try to install and then import\n            self.distribution.fetch_build_eggs(['prospector[with_pyroma]'])\n            from prospector.run import main\n\n        self.install_deps_temp()\n\n        # run linter\n\n        # change working directory to package root\n        package_root = os.path.abspath(os.path.dirname(__file__))\n        os.chdir(package_root)\n\n        # write command line\n        files = discover_python_files(check_paths, ignore)\n        sys.argv = ['prospector']\n        sys.argv.extend(files)\n\n        # run prospector\n        errno = main()\n\n        sys.exit(errno)\n\n\ndef read_authors(filename):\n    \"\"\"Read the list of authors from .zenodo.json file.\"\"\"\n    with Path(filename).open() as file:\n        info = json.load(file)\n        authors = []\n        for author in info['creators']:\n            name = ' '.join(author['name'].split(',')[::-1]).strip()\n            authors.append(name)\n        return ', '.join(authors)\n\n\ndef read_description(filename):\n    \"\"\"Read the description from .zenodo.json file.\"\"\"\n    with Path(filename).open() as file:\n        info = json.load(file)\n        return info['description']\n\n\nsetup(\n    name='ESMValTool',\n    author=read_authors('.zenodo.json'),\n    description=read_description('.zenodo.json'),\n    long_description=Path('README.md').read_text(),\n    long_description_content_type='text/markdown',\n    url='https://www.esmvaltool.org',\n    download_url='https://github.com/ESMValGroup/ESMValTool',\n    license='Apache License, Version 2.0',\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Environment :: Console',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: Apache Software License',\n        'Natural Language :: English',\n        'Operating System :: POSIX :: Linux',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3.12',\n        'Topic :: Scientific/Engineering',\n        'Topic :: Scientific/Engineering :: Atmospheric Science',\n        'Topic :: Scientific/Engineering :: GIS',\n        'Topic :: Scientific/Engineering :: Hydrology',\n        'Topic :: Scientific/Engineering :: Physics',\n    ],\n    packages=PACKAGES,\n    # Include all version controlled files\n    include_package_data=True,\n    setup_requires=REQUIREMENTS['setup'],\n    install_requires=REQUIREMENTS['install'],\n    tests_require=REQUIREMENTS['test'],\n    extras_require={\n        'develop':\n        REQUIREMENTS['develop'] + REQUIREMENTS['test'] + REQUIREMENTS['doc'],\n        'doc':\n        REQUIREMENTS['doc'],\n        'test':\n        REQUIREMENTS['test'],\n    },\n    entry_points={\n        'console_scripts': [\n            'nclcodestyle = esmvaltool.utils.nclcodestyle.nclcodestyle:_main',\n            'test_recipe = '\n            'esmvaltool.utils.testing.recipe_settings.install_expand_run:main',\n        ],\n        'esmvaltool_commands': [\n            'colortables = '\n            'esmvaltool.utils.color_tables.show_color_tables:ColorTables',\n            'install = esmvaltool.install:Install',\n            'data = esmvaltool.cmorizers.data.cmorizer:DataCommand'\n        ]\n    },\n    cmdclass={\n        'lint': RunLinter,\n    },\n    zip_safe=False,\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/easywave",
            "repo_link": "https://git.gfz-potsdam.de/id2/geoperil/easyWave",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/egsim",
            "repo_link": "https://github.com/rizac/egsim",
            "content": {
                "codemeta": "",
                "readme": "eGSIM is a web service for selecting and testing  ground shaking models (GSIM) \nin Europe, developed by the [GFZ](https://www.gfz-potsdam.de/) \nin the framework of the Thematic Core Services for Seismology of \n[EPOS](https://www.epos-eu.org/) under the umbrella of \n[EFEHR](http://www.efehr.org/en/home/)\n\n<p align=\"middle\">\n    <a title='EFEHR' href='www.efehr.org'><img height='50' src='http://www.efehr.org/export/system/modules/ch.ethz.sed.bootstrap.efehr2021/resources/img/logos/efehr.png'></a>\n    &nbsp;\n    <a title='GFZ' href='https://www.gfz-potsdam.de/'><img height='50' src='https://www.gfz-potsdam.de/fileadmin/gfz/GFZ.svg'></a>\n    &nbsp;\n    <a title='EPOS' href='https://www.epos-eu.org/'><img height='50' src='https://www.epos-eu.org/themes/epos/logo.svg'></a>\n    <br>\n</p>\n\nThe web portal (and API documentation) is available at:\n\n# https://egsim.gfz-potsdam.de\n\n## Citation\n\n> Zaccarelli, Riccardo; Weatherill, Graeme (2020): eGSIM - a Python library and web application to select and test Ground Motion models. GFZ Data Services. https://doi.org/10.5880/GFZ.2.6.2023.007\n\n# Table of contents\n\n   * [Installation](#installation)\n   * [Usage](#usage)\n   * [Packages upgrade](#packages-upgrade)\n   * [Django](#django)\n     * [Starting a Python terminal shell](#starting-a-python-terminal-shell)\n     * [Complete DB reset](#Complete-DB-reset)\n     * [Repopulating the DB](#Re-populating-the-DB)\n     * [Admin panel](#admin-panel)\n     * [Create a custom management command](#Create-a-custom-management-command)  \n     * [Add new predefined flatfiles](#Add-new-predefined-flatfiles)\n     * [Add new regionalization](#Add-new-regionalization)\n     \n\nDISCLAIMER: **This document does not cover the server installation of \nthe web app**, which is publicly available at the URL above. \n**Here you can find instructions on**:\n\n - How to install  eGSIM as local Python library \n   (`import egsim.smtk` in your code)\n - (For developers and contributors) How to install the Django app locally for testing,\n   features addition, maintenance\n\n\n# Installation\n\n\n## Requirements\n\n```bash\nsudo apt-get update # pre-requisite\nsudo apt-get install gcc  # optional\nsudo apt-get install git python3-venv python3-pip python3-dev\n```\n\n(The command above are Ubuntu specific, in macOS install brew and type\n`brew install` instead of `apt-get install`. *Remove python3-dev as it does not\nexist on macOS*).\n\nThis web service uses a *specific* version of Python (Open `setup.py` and \ncheck `python_requires=`. As of January 2022, it's `>=3.11`) *which you must \ninstall* in addition to the Python version required by your system, and use\nit. Any command `python3` hereafter will refer to the required Python version.\n\n\n## Clone repository\n\nSelect a `root directory` (e.g. `/root/path/to/egsim`), and clone egsim into the\nso-called egsim directory:\n\n```bash\ngit clone https://github.com/rizac/eGSIM.git egsim\n```\n\n## Create and activate Python virtual env\n\nMove to whatever directory you want (usually the egsim directory above) and then:\n\n```bash\npython3 -m venv .env/<ENVNAME>  # create python virtual environment (venv)\nsource .env/<ENVNAME>/bin/activate  # activate venv\n```\n\n**NOTE: From now on, all following operations must have the virtualenv \nactivated FIRST**\n\n## Install\n\nAssuming you are in the egsim directory with a virtualenv <VENVNAME>:\n\n```console\nsource .env/<ENVNAME>/bin/activate\npip install -r ./requirements.txt\n```\n\n### eGSIM as local library\n\nIf you want to use eGSIM locally using the \nstrong motion toolkit package only (`from egsim.smtk import ...`\nin your code):\n\n```console\nsource .env/<ENVNAME>/bin/activate\npip install -r ./requirements.lib.txt\n```\n\n#### Run tests \n\n(remember to `pip install pytest` first)\n```bash\npytest -vvv ./tests/smtk\n```\n\n## Run Test\n\n(web app tests. For testing the library only, see above)\n\n> Note: the value of `DJANGO_SETTINGS_MODULE` in the examples below \n> must be changed in production\n\nMove in the `egsim directory` and type:\n\n```bash\nexport DJANGO_SETTINGS_MODULE=egsim.settings_debug; pytest -xvvv ./tests/\n```\n(x=stop at first error, v*=increase verbosity). \n\nwith coverage report:\n\n```bash\nexport DJANGO_SETTINGS_MODULE=egsim.settings_debug; pytest --cov=egsim --cov-report=html -xvvv ./tests/\n```\n\n<details>\n<summary>Configure PyCharm</summary>\nFor **PyCharm users**, you need to configure the environment variable\nfor all tests. Go to:\n\n- Run\n  - Edit Configurations\n    - Python tests\n    \nAnd then under **Environment variables:** add:\n\n`DJANGO_SETTINGS_MODULE=egsim.settings_debug`\n\n(type several env vars separated by ;)\n\n</details>\n\n\n# Usage\n\n> Note: the value of `DJANGO_SETTINGS_MODULE` in the examples below \n> must be changed in production\n\nIf you didn't do already, perform \na [Complete DB reset](#Complete-DB-reset)\n(**one-time only operation**)\n\nIf you want to access the admin panel, see [the admin panel](#admin-panel).\n\n**To run the program in your local browser**, type:\n\n```bash\nexport DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py runserver \n```\n\n<details>\n<summary>Configure PyCharm</summary>\nFor **PyCharm users**, you can implement a service, which can be run as any\nPyCharm configuration in debug mode, allowing to open the browser \nand stop at specific point in the code (the PyCharm window will popup \nautomatically in case). \nTo implement a service, go to:\n\n- Run\n  - Edit Configurations\n    - Add new configuration\n\nthen under **Run**:\n - between `script` and `module` (should be a combo box) choose `script`,\n   and in the next text field put `manage.py`\n - script parameters: `runserver`\n - And then under **Environment variables:** add:\n   `DJANGO_SETTINGS_MODULE=egsim.settings_debug`\n   (type several env vars separated by ;)\n\nYou should see in the `Services` tab appearing the script name, so you can\nrun / debug it normally\n\n</details>\n\n\n## Packages upgrade\n\n\n```console\nsource .env/<ENVNAME>/bin/activate\npip install --upgrade pip setuptools\n```\n\nUpgrade OpenQuake (**optional**). The operation below should be performed in\nvery specific cases only (important bugfixes or features) because\n**being OpenQuake often backward incompatible** it might require additional \ncode fixes and feedbacks from scientific experts or OpenQuake developers.\nFirst, **open `setup.py` and comment the line of `install_requires` where OpenQuake\nis installed** (should be starting with `openquake.engine`). Then \n(note that `pip install openquake` works but is not the recommended way):\n```console\npip install -r \"https://raw.githubusercontent.com/gem/oq-engine/master/requirements-py311-macos_x86_64.txt\"\n# pip install -r \"https://raw.githubusercontent.com/gem/oq-engine/master/requirements-py311-linux64.txt\"\n```\n\n\nInstall eGSIM Python library, upgrading its dependencies:\n```console\npip install -U . && pip freeze >./requirements.lib.txt && pip install pytest\n```\n\nRun tests:\n```console\npytest -vvv ./tests/smtk\n```\n\nInstall eGSIM web app, upgrading its dependencies:\n```console\npip install -U --upgrade-strategy eager \".[web]\"\npip freeze > ./requirements.txt\n```\n\nRun tests:\n```console\nexport DJANGO_SETTINGS_MODULE=egsim.settings_debug; pytest -xvvv ./tests/\n```\n\nChange `setup.py` and set the current OpenQuake version in \n`install_requires` (uncomment it if commented). Optionally,\nremove egsim from requirements.txt (it might interfere with Django web?*).\n\nEventually, **commit and push**\n\n\n# Django\n\nRemember to **activate the Python virtualenv** in all examples below\n\n<details>\n<summary>\nBrief Introduction to some important concepts and key terms (click to show)\n</summary>\n\n - [Settings file](https://docs.djangoproject.com/en/stable/topics/settings/): \n   A Django settings file contains all the configuration of your Django \n   installation. The settings file referred in this document, \n   included in this git repo, is for debug and local deployment only.\n   On production, a separate settings file is used, located on the server \n   outside the git repo and **not shared for security reasons**.\n\n   \n - [manage.py](https://docs.djangoproject.com/en/stable/ref/django-admin/) or\n   `django-admin` is Django’s command-line utility for administrative tasks.\n   It is invoked from the terminal within your Python virtualenv (see examples\n   in this document) by providing the settings file via:\n   ```bash\n   export DJANGO_SETTINGS_MODULE=<settings_file_path> python manage.py <command>\n   ```\n   Django allows also the implementation of custom management commands.\n   eGSIM implements `egsim-init` in order to populate the db (more details \n   below)\n\n\n - [app](https://docs.djangoproject.com/en/stable/intro/reusable-apps/) a \n   Django app is a Python package that is specifically intended for use in \n   a Django project. An application may use common Django conventions, such as \n   having models, tests, urls, and views submodules. In our case, the Django\n   project is the egsim root directory (created with the command\n   `django-admin startproject egsim`), and the *Django apps* inside it are \n   \"api\" (the core web API) and \"app\" (the *web app*, i.e. the part of eGSIM\n   delivered over the Internet through a browser interface), that relies on \n   the \"api\" code.\n   Inside the settings file (variable `INSTALLED_APPS`) is configured the list \n   of all applications that are enabled in the eGSIM project. This includes not \n   only our \"api\" app, that tells Django to create the eGISM tables when\n   initializing the database, but also several builtin Django apps, e.g. the \n   Django `admin` app, visible through the [Admin panel](#admin-panel).\n\n</details>\n\n## Starting a Python terminal shell\n\n> Note: the value of `DJANGO_SETTINGS_MODULE` in the examples below \n> must be changed in production\n\nTyping `python` on the terminal does not work as one needs to\ninitialize Django settings. The Django `shell` command does this:\n\n```bash\nexport DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py shell \n```\n\n## Get Django uploaded files directory\n\nIf you did not set it explicitly in `settings.FILE_UPLOAD_TEMP_DIR` \n(by default is missing), then Django will put uploaded files \nin the standard temporary directory which you can get easily by \ntyping:\n\n```bash\npython -c \"import tempfile;print(tempfile.gettempdir())\"\n```\n\n## Complete DB reset\n\n> Note: the value of `DJANGO_SETTINGS_MODULE` in the examples below \n> must be changed in production\n\nWe perform a complete DB reset every time we change something \nin the Database schema (see `egsim.api.models.py`), e.g. a table, \na column, a constraint.\n\n<details>\n<summary>(if you wonder why we do not use DB migrations, click here)</summary>\n\nThe usual way to change a DB in a web app is to create and run\nmigrations \n([full details here](https://docs.djangoproject.com/en/stable/topics/migrations/)),\nwhich allow to keep track of all changes (moving back and forth if necessary) \nwhilst preserving the data stored in the DB. \nHowever, none of those features is required in eGSIM: DB data is predefined\nand would be regenerated from scratch in any case after any new migration.\nConsequently, **upon changes in the DB, a complete DB reset is an easier \nprocedure**.\n\nIn any case (**just for reference**), the steps to create and run migrations \nin eGSIM are the following:\n\n```bash\nexport DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py makemigrations egsim --name <migration_name>\nexport DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py migrate egsim\n```\nAnd then repopulate the db:\n```bash\nexport DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py egsim_init\n```\n\nNotes: \n  - The `make_migration` command just generates a migration file, it doesn't \n    change the db. The `migrate` command does that, by means of the migration \n    files generated. For details on Django migrations, see:\n    - https://realpython.com/django-migrations-a-primer/#changing-models\n    - https://docs.djangoproject.com/en/stable/topics/migrations/#workflow \n  - <migration_name> will be a suffix appended to the migration file, use it\n    like you would use a commit message in `git`).\n  - When running `migrate`, if the migration \n    will introduce new non-nullable fields, maybe better to run \n    `manage.py flush` first to empty all tables, to avoid conflicts\n    \"egsim\" above is the app name. If you omit the app, all apps will be \n    migrated. The command `migrate` does nothing if it detects that there is \n    nothing to migrate\n</details>\n\nTo perform a complete db reset:\n\n - delete or rename the database of the settings file used and *all* migration \n   files. In dev mode they are:\n   - `egsim/db.sqlite3`\n   - `egsim/api/migrations/0001_initial.py` (there should be only one. If there \n     are others, delete all of them)\n - Execute:\n   ```bash\n   export DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py makemigrations && python manage.py migrate && python manage.py egsim_init\n   ```\n - `git add` the newly created migration file (in dev mode it's \n   `egsim/api/migrations/0001_initial.py`)\n - [**Optional**] re-add the Django admin superuser(s) as explained in the\n   [admin panel](#admin-panel)\n\nNotes:\n - Commands explanation:\n   - `makemigrations` creates the necessary migration file(s) from Python \n     code and existing migration file(s)\n   - `migrate` re-create the DB via the generated migration file(s)\n   - `egsim_init` repopulates the db with eGSIM data\n\n\n## Re-populating the DB\n \nWe repopulate the DB when  **its schema has not changed** but its data \nneeds to, e.g., OpenQuake is upgraded, or new data is implemented \n(new regionalization or flatfile), or a bug in the code has been \nfixed. The operations are similar but simpler than a complete Db Rest:\n\n- delete or rename the database of the settings file used:\n   - `egsim/db.sqlite3`\n- Execute: \n  ```bash\n  export DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py migrate && python manage.py egsim_init\n  ```\n- [**Optional**] most likely (not tested, please check) you need to re-add \n  the Django admin superuser(s) as explained in the [admin panel](#admin-panel)\n   \n\n## Admin panel\n\n> Note: the value of `DJANGO_SETTINGS_MODULE` in the examples below \n> must be changed in production\n\nThis command allows the user to check database data from the \nweb browser. For further details, check the \n[Django doc](https://docs.djangoproject.com/en/stable/ref/django-admin/)\n\nThe database must have been created and populated (see [Usage](#usage)). \n\nCreate a superuser (to be done **once only** ):\n```bash\nexport DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py createsuperuser\n```\nand follow the instructions.\n\nStart the program (see [Usage](#Usage)) and then navigate in the browser to \n`[SITE_URL]/admin` (in development mode, `http://127.0.0.1:8000/admin/`)\n\n*Note: Theoretically, you can modify db data from the browser, e.g., hide some \nmodel, regionalization or predefined flatfile. Persistent changes should be\nimplemented in Python code and then run a [Complete DB reset](#Complete-DB-reset)*\n\n\n## Create a custom management command\n\nSee `egsim/api/management/commands/README.md`.\n\nThe next two sections will describe how to store\nnew data (regionalizations and flatfiles) that will be\nmade available in eGSIM with the `egsim_init` command\n(see [Complete DB reset](#Complete-DB-reset) for details)\n\n\n## Add new predefined flatfiles\n\n- Add the file (CSV or zipped CSV) in\n  `managements/commands/data/flatfiles`. \n  If the file is too big try to zip it. \n  **If it is more than few tens of Mb, then do not commit it** (explain in \n  the section `details` - see below - how to get the source file). \n  When zipping in macOS you will probably need to\n  [exclude or remove (after zipping) the MACOSX folder](https://stackoverflow.com/q/10924236)~~\n- \n- Implement a new `FlatfileParser` class in \n  `management/commands/flatfile_parsers`. Take another parser, copy it \n  and follow instructions.\n  The parser goal is to read the file and convert it into a harmonized HDF \n  table\n\n- Add binding file -> parser in the Python `dict`:\n  `management.commands._egsim_flatfiles.Command.PARSER`\n\n- (Optional) Add the file refs \n  in `management/commands/data/references.yaml`, e.g. reference, url, the \n  file name that will be used in the API (if missing, defaults to the file \n  name without extension)\n\n- Repopulate all eGSIM tables (command `egsim_init`)\n\nImplemented flatfiles sources (click on the items below to expand)\n\n<details>\n<summary>ESM 2018 flatfile</summary>\n\n- Go to https://esm.mi.ingv.it//flatfile-2018/flatfile.php\n(with username and password, you must be registered \n  beforehand it's relatively fast and simple)\n\n- Download `ESM_flatfile_2018.zip`, uncompress and extract\n  `ESM_flatfile_SA.csv` from there \n  \n- `ESM_flatfile_SA.csv` is our raw flatfile, compress it \n  again (it's big) into this directory as \n  `ESM_flatfile_2018_SA.zip`\n \n- If on macOS, type the command above to remove the\n  macOS folder from the zip\n</details>\n\n\n## Add new regionalization\n\n- Add two files *with the same basename* and extensions in \n  `managements/commands/data/regionalization_files`:\n\n  - <name>.geojson (regionalization, aka regions collection) and\n  - <name>.json (region -> gsim mapping)\n  \n  See already implemented files for an example\n\n- (Optional) Add the file refs \n  in `management/commands/data/references.yaml`, e.g. reference, url, the \n  file name that will be used in the API (if missing, defaults to the file \n  name without extension)\n\n- Repopulate all eGSIM tables (command `egsim_init`)\n\n\n",
                "dependencies": "asgiref==3.8.1\nastroid==3.1.0\nblosc2==2.6.2\ncertifi==2024.2.2\ncharset-normalizer==3.3.2\ncontourpy==1.2.1\ncoverage==7.4.4\ncycler==0.12.1\ndecorator==5.1.1\ndill==0.3.8\nDjango==5.0.4\ndocutils==0.21.1\n# -e git+https://github.com/rizac/eGSIM.git@704f51b24afad75745c7985e9fdac00fad775c31#egg=egsim\nfonttools==4.51.0\nh5py==3.11.0\nidna==3.7\niniconfig==2.0.0\nisort==5.13.2\nkaleido==0.2.1\nkiwisolver==1.4.5\nmatplotlib==3.8.4\nmccabe==0.7.0\nmsgpack==1.0.8\nndindex==1.8\nnumexpr==2.10.0\nnumpy==1.26.4\nopenquake.engine==3.15.0\npackaging==24.0\npandas==2.2.2\npillow==10.3.0\nplatformdirs==4.2.0\nplotly==5.20.0\npluggy==1.4.0\npsutil==5.9.8\npy-cpuinfo==9.0.0\npylint==3.1.0\npyparsing==3.1.2\npyproj==3.6.1\npytest==8.1.1\npytest-cov==5.0.0\npytest-django==4.8.0\npython-dateutil==2.9.0.post0\npytz==2024.1\nPyYAML==6.0.1\npyzmq==25.1.2\nrequests==2.31.0\nscipy==1.13.0\nshapely==2.0.3\nsix==1.16.0\nsqlparse==0.4.4\ntables==3.9.2\ntenacity==8.2.3\ntoml==0.10.2\ntomlkit==0.12.4\ntzdata==2024.1\nurllib3==2.2.1\n\nfrom setuptools import setup, find_packages\n\n_README = \"\"\"\nPython and OpenQuake-based web service for selecting, comparing and testing\nGround Shaking Intensity Models.\n\"\"\"\n\nsetup(\n    name='egsim',\n    version='2.1.0',\n    description=_README,\n    url='https://github.com/rizac/eGSIM',\n    packages=find_packages(exclude=['tests', 'tests.*']),\n    python_requires='>=3.11',\n    # Minimal requirements for the library (egsim.smtk package).\n    # FOR DEV/TESTS, add: `pip install pytest`\n    install_requires=[\n        'openquake.engine==3.15.0',  # 1st tested version was >3.5.0\n        'pyyaml>=6.0',\n        'tables>=3.8.0',\n    ],\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). You can install these using the following syntax,\n    # for example:\n    # $ pip install -e \".[web]\"\n    extras_require={\n        'web': [\n            'Django>=4.1.2',\n            'plotly>=5.10.0',\n            'kaleido>=0.2.1',  # required by plotly to save images\n            # test packages:\n            'pytest',\n            'pylint>=2.3.1',\n            'pytest-django>=3.4.8',\n            'pytest-cov>=2.6.1'\n        ]\n    },\n    author='r. zaccarelli',\n    author_email='',\n    maintainer='r. zaccarelli',\n    maintainer_email='',\n    classifiers=[\n        'Development Status :: 1 - Beta',\n        'Intended Audience :: Education',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: GNU Affero General Public License v3',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python :: 3',\n        'Topic :: Scientific/Engineering',\n    ],\n    keywords=[\n        \"seismic hazard\",\n        \"ground shaking intensity model\",\n        \"gsim\",\n        \"gmpe\",\n        \"ground motion database\",\n        \"flatfile\"\n    ],\n    license=\"AGPL3\",\n    platforms=[\"any\"],\n    # package_data={\"smtk\": [\n    #    \"README.md\", \"LICENSE\"]},\n    # include_package_data=True,\n    zip_safe=False,\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ehrapy",
            "repo_link": "https://github.com/theislab/ehrapy",
            "content": {
                "codemeta": "",
                "readme": "[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Build](https://github.com/theislab/ehrapy/actions/workflows/build.yml/badge.svg)](https://github.com/theislab/ehrapy/actions/workflows/build.yml)\n[![Codecov](https://codecov.io/gh/theislab/ehrapy/branch/master/graph/badge.svg)](https://codecov.io/gh/theislab/ehrapy)\n[![License](https://img.shields.io/github/license/theislab/ehrapy)](https://opensource.org/licenses/Apache2.0)\n[![PyPI](https://img.shields.io/pypi/v/ehrapy.svg)](https://pypi.org/project/ehrapy/)\n[![Python Version](https://img.shields.io/pypi/pyversions/ehrapy)](https://pypi.org/project/ehrapy)\n[![Read the Docs](https://img.shields.io/readthedocs/ehrapy/latest.svg?label=Read%20the%20Docs)](https://ehrapy.readthedocs.io/)\n[![Test](https://github.com/theislab/ehrapy/actions/workflows/test.yml/badge.svg)](https://github.com/theislab/ehrapy/actions/workflows/test.yml)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n\n<img src=\"https://user-images.githubusercontent.com/21954664/156930990-0d668468-0cd9-496e-995a-96d2c2407cf5.png\" alt=\"ehrapy logo\">\n\n# ehrapy overview\n\n![fig1](https://github.com/user-attachments/assets/7927aa20-751c-4e73-8939-1e4b1c465570)\n\n## Features\n\n-   Exploratory and targeted analysis of Electronic Health Records\n-   Quality control & preprocessing\n-   Visualization & Exploration\n-   Clustering & trajectory inference\n\n## Installation\n\nYou can install _ehrapy_ via [pip] from [PyPI]:\n\n```console\n$ pip install ehrapy\n```\n\n## Usage\n\nPlease have a look at the [Usage documentation](https://ehrapy.readthedocs.io/en/latest/usage/usage.html) and the [tutorials](https://ehrapy.readthedocs.io/en/latest/tutorials/index.html).\n\n```python\nimport ehrapy as ep\n```\n\n## Citation\n\n[Heumos, L., Ehmele, P., Treis, T. et al. An open-source framework for end-to-end analysis of electronic health record data. Nat Med (2024). https://doi.org/10.1038/s41591-024-03214-0](https://www.nature.com/articles/s41591-024-03214-0).\n\n",
                "dependencies": "[build-system]\nbuild-backend = \"hatchling.build\"\nrequires = [\"hatchling\"]\n\n[project]\nname = \"ehrapy\"\nversion = \"0.11.0\"\ndescription = \"Electronic Health Record Analysis with Python.\"\nreadme = \"README.md\"\nrequires-python = \">=3.10,<3.13\"\nlicense = {file = \"LICENSE\"}\nauthors = [\n    {name = \"Lukas Heumos\"},\n    {name = \"Philipp Ehmele\"},\n    {name = \"Eljas Roellin\"},\n    {name = \"Lilly May\"},\n    {name = \"Tim Treis\"},\n    {name = \"Altana Namsaraeva\"},\n    {name = \"Vladimir Shitov\"},\n    {name = \"Luke Zappia\"},\n    {name = \"Xinyue Zhang\"},\n]\nmaintainers = [\n    {name = \"Lukas Heumos\", email = \"lukas.heumos@posteo.net\"},\n]\nurls.Documentation = \"https://ehrapy.readthedocs.io\"\nurls.Source = \"https://github.com/theislab/ehrapy\"\nurls.Home-page = \"https://github.com/theislab/ehrapy\"\n\nclassifiers = [\n    \"License :: OSI Approved :: Apache Software License\",\n    \"Development Status :: 4 - Beta\",\n    \"Environment :: Console\",\n    \"Framework :: Jupyter\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n    \"Natural Language :: English\",\n    \"Operating System :: MacOS :: MacOS X\",\n    \"Operating System :: POSIX :: Linux\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering :: Bio-Informatics\",\n    \"Topic :: Scientific/Engineering :: Visualization\",\n]\n\ndependencies = [\n    \"session-info\",\n    \"lamin_utils\",\n    \"rich\",\n    \"scanpy[leiden]\",\n    \"requests\",\n    \"pynndescent\",\n    \"miceforest\",\n    \"scikit-misc\",\n    \"lifelines\",\n    \"missingno\",\n    \"thefuzz[speedup]\",\n    \"dowhy\",\n    \"fhiry\",\n    \"pyampute\",\n    \"tableone\",\n    \"imbalanced-learn\",\n    \"fknni\",\n    \"python-dateutil\",\n    \"filelock\",\n]\n\n[project.optional-dependencies]\nmedcat = [\n    \"medcat\",\n]\ndask = [\n    \"dask\",\n    \"dask-ml\",\n]\ndev = [\n    \"pre-commit\",\n]\ndocs = [\n    \"docutils\",\n    \"sphinx\",\n    \"furo\",\n    \"myst-nb\",\n    \"myst-parser\",\n    \"sphinxcontrib-bibtex\",\n    \"sphinx-gallery\",\n    \"sphinx-autodoc-typehints\",\n    \"sphinx-copybutton\",\n    \"sphinx-remove-toctrees\",\n    \"sphinx-design\",\n    \"sphinx-last-updated-by-git\",\n    \"sphinx-automodapi\",\n    \"sphinxext-opengraph\",\n    \"pygments\",\n    \"pyenchant\",\n    \"nbsphinx\",\n    \"nbsphinx-link\",\n    \"ipykernel\",\n    \"ipython\",\n    \"ehrapy[dask,medcat]\",\n]\ntest = [\n    \"ehrapy[dask]\",\n    \"pytest\",\n    \"pytest-cov\",\n    \"pytest-mock\"\n]\n\n\n[tool.hatch.version]\nsource = \"vcs\"\n\n[tool.coverage.run]\nsource_pkgs = [\"ehrapy\"]\nomit = [\n    \"**/test_*.py\",\n    \"ehrapy/data/_datasets.py\",  # Difficult to test\n]\n\n[tool.pytest.ini_options]\ntestpaths = \"tests\"\nxfail_strict = true\naddopts = [\n    \"--import-mode=importlib\",  # allow using test files with same name\n]\nmarkers = [\n    \"conda: marks a subset of tests to be ran on the Bioconda CI.\",\n    \"extra: marks tests that require extra dependencies.\"\n]\nfilterwarnings = [\n    \"ignore::DeprecationWarning\",\n    \"ignore::anndata.OldFormatWarning:\",\n    \"ignore:X converted to numpy array with dtype object:UserWarning\",\n    \"ignore:`flavor='seurat_v3'` expects raw count data, but non-integers were found:UserWarning\",\n    \"ignore:All-NaN slice encountered:RuntimeWarning\",\n    \"ignore:Observation names are not unique. To make them unique, call `.obs_names_make_unique`.:UserWarning\"\n]\nminversion = 6.0\nnorecursedirs = [ '.*', 'build', 'dist', '*.egg', 'data', '__pycache__']\n\n[tool.ruff]\nline-length = 120\n\n[tool.ruff.format]\ndocstring-code-format = true\n\n[tool.ruff.lint]\nselect = [\n    \"F\",  # Errors detected by Pyflakes\n    \"E\",  # Error detected by Pycodestyle\n    \"W\",  # Warning detected by Pycodestyle\n    \"I\",  # isort\n    #\"D\",  # pydocstyle\n    \"B\",  # flake8-bugbear\n    \"TID\",  # flake8-tidy-imports\n    \"C4\",  # flake8-comprehensions\n    \"BLE\",  # flake8-blind-except\n    \"UP\",  # pyupgrade\n    \"RUF100\",  # Report unused noqa directives\n    \"TCH\",  # Typing imports\n    # \"NPY\",  # Numpy specific rules\n    \"PTH\"  # Use pathlib\n]\nignore = [\n    # line too long -> we accept long comment lines; black gets rid of long code lines\n    \"E501\",\n    # Do not assign a lambda expression, use a def -> lambda expression assignments are convenient\n    \"E731\",\n    # allow I, O, l as variable names -> I is the identity matrix\n    \"E741\",\n    # Missing docstring in public package\n    \"D104\",\n    # Missing docstring in public module\n    \"D100\",\n    # Missing docstring in __init__\n    \"D107\",\n    # Errors from function calls in argument defaults. These are fine when the result is immutable.\n    \"B008\",\n    # __magic__ methods are are often self-explanatory, allow missing docstrings\n    \"D105\",\n    # first line should end with a period [Bug: doesn't work with single-line docstrings]\n    \"D400\",\n    # First line should be in imperative mood; try rephrasing\n    \"D401\",\n    ## Disable one in each pair of mutually incompatible rules\n    # We don’t want a blank line before a class docstring\n    \"D203\",\n    # We want docstrings to start immediately after the opening triple quote\n    \"D213\",\n    # Imports unused\n    \"F401\",\n    # camcelcase imported as lowercase\n    \"N813\",\n    # module import not at top level of file\n    \"E402\",\n]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n[tool.ruff.lint.per-file-ignores]\n\"docs/*\" = [\"I\"]\n\"tests/*\" = [\"D\"]\n\"*/__init__.py\" = [\"F401\"]\n\n[tool.mypy]\nstrict = false\npretty = true\nshow_column_numbers = true\nshow_error_codes = true\nshow_error_context = true\nignore_missing_imports = true\nno_strict_optional = true\n\n[tool.cruft]\nskip = [\n    \"tests\",\n    \"src/**/__init__.py\",\n    \"src/**/basic.py\",\n    \"docs/api.md\",\n    \"docs/changelog.md\",\n    \"docs/references.bib\",\n    \"docs/references.md\",\n    \"docs/notebooks/example.ipynb\"\n]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/electrode",
            "repo_link": "https://github.com/robeme/lammps",
            "content": {
                "codemeta": "",
                "readme": "This is the LAMMPS software package.\n\nLAMMPS stands for Large-scale Atomic/Molecular Massively Parallel\nSimulator.\n\nCopyright (2003) Sandia Corporation.  Under the terms of Contract\nDE-AC04-94AL85000 with Sandia Corporation, the U.S. Government retains\ncertain rights in this software.  This software is distributed under\nthe GNU General Public License.\n\n----------------------------------------------------------------------\n\nLAMMPS is a classical molecular dynamics simulation code designed to\nrun efficiently on parallel computers.  It was developed at Sandia\nNational Laboratories, a US Department of Energy facility, with\nfunding from the DOE.  It is an open-source code, distributed freely\nunder the terms of the GNU Public License (GPL) version 2.\n\nThe code is maintained by the LAMMPS development team who can be emailed\nat developers@lammps.org.  The LAMMPS WWW Site at www.lammps.org has\nmore information about the code and its uses.\n\nThe LAMMPS distribution includes the following files and directories:\n\nREADME                     this file\nLICENSE                    the GNU General Public License (GPL)\nbench                      benchmark problems\ncmake                      CMake build files\ndoc                        documentation\nexamples                   simple test problems\nfortran                    Fortran wrapper for LAMMPS\nlib                        additional provided or external libraries\npotentials                 interatomic potential files\npython                     Python wrappers for LAMMPS\nsrc                        source files\ntools                      pre- and post-processing tools\n\nPoint your browser at any of these files to get started:\n\nhttps://docs.lammps.org/Manual.html         LAMMPS manual\nhttps://docs.lammps.org/Intro.html          hi-level introduction\nhttps://docs.lammps.org/Build.html          how to build LAMMPS\nhttps://docs.lammps.org/Run_head.html       how to run LAMMPS\nhttps://docs.lammps.org/Commands_all.html   Table of available commands\nhttps://docs.lammps.org/Library.html        LAMMPS library interfaces\nhttps://docs.lammps.org/Modify.html         how to modify and extend LAMMPS\nhttps://docs.lammps.org/Developer.html      LAMMPS developer info\n\nYou can also create these doc pages locally:\n\n% cd doc\n% make html                # creates HTML pages in doc/html\n% make pdf                 # creates Manual.pdf\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/elephant",
            "repo_link": "https://github.com/NeuralEnsemble/elephant",
            "content": {
                "codemeta": "{\n    \"@context\": \"https://doi.org/10.5063/schema/codemeta-2.0\",\n    \"@type\": \"SoftwareSourceCode\",\n    \"license\": \"https://spdx.org/licenses/BSD-3-Clause\",\n    \"codeRepository\": \"git+https://github.com/NeuralEnsemble/elephant.git\",\n    \"contIntegration\": \"https://github.com/NeuralEnsemble/elephant/actions\",\n    \"dateCreated\": \"2022-03-14\",\n    \"datePublished\": \"2015-04-08\",\n    \"dateModified\": \"2024-03-19\",\n    \"downloadUrl\": \"https://files.pythonhosted.org/packages/cb/b5/893fadd5505e638a4c8788bf0a2f5a211f59f45203f3dfa3919469e83ed4/elephant-1.0.0.tar.gz\",\n    \"issueTracker\": \"https://github.com/NeuralEnsemble/elephant/issues\",\n    \"name\": \"Elephant\",\n    \"version\": \"1.1.0\",\n    \"identifier\": \"https://doi.org/10.5281/zenodo.1186602\",\n    \"description\": \"Elephant (Electrophysiology Analysis Toolkit) is an open-source, community centered library for the analysis of electrophysiological data in the Python programming language. The focus of Elephant is on generic analysis functions for spike train data and time series recordings from electrodes, such as the local field potentials (LFP) or intracellular voltages.In addition to providing a common platform for analysis code from different laboratories, the Elephant project aims to provide a consistent and homogeneous analysis framework that is built on a modular foundation. \\nElephant is the direct successor to Neurotools and maintains ties to complementary projects such as OpenElectrophy and spykeviewer.\",\n    \"applicationCategory\": \"library\",\n    \"releaseNotes\": \"https://github.com/NeuralEnsemble/elephant/releases/tag/v1.1.0\",\n    \"funding\": \"EU Grant 604102 (HBP), EU Grant 720270(HBP), EU Grant 785907(HBP), EU Grant 945539(HBP)\",\n    \"developmentStatus\": \"active\",\n    \"keywords\": [\n        \"neuroscience\",\n        \"neurophysiology\",\n        \"electrophysiology\",\n        \"statistics\",\n        \"data-analysis\"\n    ],\n    \"programmingLanguage\": [\n        \"Python3\",\n        \"\"\n    ],\n    \"operatingSystem\": [\n        \"Linux\",\n        \"Windows\",\n        \"MacOS\"\n    ],\n    \"softwareRequirements\": [\n        \"https://github.com/NeuralEnsemble/elephant/tree/v1.1.0/requirements\"\n    ],\n    \"relatedLink\": [\n        \"http://python-elephant.org\",\n        \"http://elephant.readthedocs.org/\"\n    ],\n    \"author\": [\n        {\n            \"@type\": \"Person\",\n            \"@id\": \"https://orcid.org/0000-0003-1255-7300\",\n            \"givenName\": \"Michael\",\n            \"familyName\": \"Denker\",\n            \"affiliation\": {\n                \"@type\": \"Organization\",\n                \"name\": \"Institute for Advanced Simulation (IAS-6), Jülich Research Centre, Jülich, Germany\"\n            }\n        },\n        {\n            \"@type\": \"Person\",\n            \"@id\": \"https://orcid.org/0000-0001-7292-1982\",\n            \"givenName\": \"Moritz\",\n            \"familyName\": \"Kern\",\n            \"affiliation\": {\n                \"@type\": \"Organization\",\n                \"name\": \"Institute for Advanced Simulation (IAS-6), Jülich Research Centre, Jülich, Germany\"\n            }\n        },\n        {\n            \"@type\": \"Person\",\n            \"@id\": \"https://orcid.org/0009-0003-9352-9826\",\n            \"givenName\": \"Felician\",\n            \"familyName\": \"Richter\",\n            \"affiliation\": {\n                \"@type\": \"Organization\",\n                \"name\": \"BioMEMS Lab, University of Applied Sciences Aschaffenburg, Germany\"\n            }\n        }\n    ]\n}\n\n",
                "readme": "# Elephant - Electrophysiology Analysis Toolkit\n\n[![Coverage Status](https://coveralls.io/repos/github/NeuralEnsemble/elephant/badge.svg?branch=master)](https://coveralls.io/github/NeuralEnsemble/elephant?branch=master)\n[![Documentation Status](https://readthedocs.org/projects/elephant/badge/?version=latest)](https://elephant.readthedocs.io/en/latest/?badge=latest)\n[![![PyPi]](https://img.shields.io/pypi/v/elephant)](https://pypi.org/project/elephant/)\n[![Statistics](https://img.shields.io/pypi/dm/elephant)](https://seladb.github.io/StarTrack-js/#/preload?r=neuralensemble,elephant)\n[![Gitter](https://badges.gitter.im/python-elephant/community.svg)](https://gitter.im/python-elephant/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![DOI Latest Release](https://zenodo.org/badge/10311278.svg)](https://zenodo.org/badge/latestdoi/10311278)\n[![tests](https://github.com/NeuralEnsemble/elephant/actions/workflows/CI.yml/badge.svg)](https://github.com/NeuralEnsemble/elephant/actions/workflows/CI.yml)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/6191/badge)](https://bestpractices.coreinfrastructure.org/projects/6191)\n[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu)\n[![Twitter](https://img.shields.io/twitter/follow/PyElephant?style=social)](https://twitter.com/pyelephant)\n[![Fosstodon](https://img.shields.io/badge/@elephant-6364FF?logo=mastodon&logoColor=fff&style=flat)](https://fosstodon.org/@elephant)\n\n\n*Elephant* package analyses all sorts of neurophysiological data:\nspike trains, LFP, analog signals. The input-output data format is either\n[Neo](https://github.com/NeuralEnsemble/python-neo), Quantity or Numpy array.\n\n\n### More Information\n\n* Documentation: https://elephant.readthedocs.io/en/latest/\n* Mailing list: https://groups.google.com/group/neuralensemble\n\n\n#### Visualization of Elephant analysis objects\n\nViziphant package (https://github.com/INM-6/viziphant) is developed by Elephant\nteam for plotting and visualization of the output of Elephant functions in a\nfew lines of code.\n\n\n#### License\n \nModified BSD License, see [LICENSE.txt](LICENSE.txt) for details.\n\n\n#### Copyright\n\n:copyright: 2014-2024 by the [Elephant team](doc/authors.rst).\n\n\n#### Acknowledgments\n\nSee [acknowledgments](doc/acknowledgments.rst).\n\n\n#### Citation\n\nSee [citations](doc/citation.rst).\n\n",
                "dependencies": "# -*- coding: utf-8 -*-\nimport os.path\nimport platform\nimport sys\n\nfrom setuptools import setup, Extension\nfrom setuptools.command.install import install\nfrom setuptools.command.develop import develop\n\nwith open(os.path.join(os.path.dirname(__file__),\n                       \"elephant\", \"VERSION\")) as version_file:\n    version = version_file.read().strip()\n\nwith open(\"README.md\") as f:\n    long_description = f.read()\nwith open('requirements/requirements.txt') as fp:\n    install_requires = fp.read().splitlines()\nextras_require = {}\nfor extra in ['extras', 'docs', 'tests', 'tutorials', 'cuda', 'opencl']:\n    with open('requirements/requirements-{0}.txt'.format(extra)) as fp:\n        extras_require[extra] = fp.read()\n\nif platform.system() == \"Windows\":\n    fim_module = Extension(\n        name='elephant.spade_src.fim',\n        sources=['elephant/spade_src/src/fim.cpp'],\n        include_dirs=['elephant/spade_src/include'],\n        language='c++',\n        libraries=[],\n        extra_compile_args=[\n            '-DMODULE_NAME=fim', '-DUSE_OPENMP', '-DWITH_SIG_TERM',\n            '-Dfim_EXPORTS', '-fopenmp', '/std:c++17'],\n        optional=True\n    )\nelif platform.system() == \"Darwin\":\n    fim_module = Extension(\n        name='elephant.spade_src.fim',\n        sources=['elephant/spade_src/src/fim.cpp'],\n        include_dirs=['elephant/spade_src/include'],\n        language='c++',\n        libraries=['pthread', 'omp'],\n        extra_compile_args=[\n            '-DMODULE_NAME=fim', '-DUSE_OPENMP', '-DWITH_SIG_TERM',\n            '-Dfim_EXPORTS', '-O3', '-pedantic', '-Wextra',\n            '-Weffc++', '-Wunused-result', '-Werror', '-Werror=return-type',\n            '-Xpreprocessor',\n            '-fopenmp', '-std=gnu++17'],\n        optional=True\n    )\nelif platform.system() == \"Linux\":\n    fim_module = Extension(\n        name='elephant.spade_src.fim',\n        sources=['elephant/spade_src/src/fim.cpp'],\n        include_dirs=['elephant/spade_src/include'],\n        language='c++',\n        libraries=['pthread', 'gomp'],\n        extra_compile_args=[\n            '-DMODULE_NAME=fim', '-DUSE_OPENMP', '-DWITH_SIG_TERM',\n            '-Dfim_EXPORTS', '-O3', '-pedantic', '-Wextra',\n            '-Weffc++', '-Wunused-result', '-Werror',\n            '-fopenmp', '-std=gnu++17'],\n        optional=True\n    )\n\nsetup_kwargs = {\n    \"name\": \"elephant\",\n    \"version\": version,\n    \"packages\": ['elephant', 'elephant.test'],\n    \"include_package_data\": True,\n    \"install_requires\": install_requires,\n    \"extras_require\": extras_require,\n    \"author\": \"Elephant authors and contributors\",\n    \"author_email\": \"contact@python-elephant.org\",\n    \"description\": \"Elephant is a package for analysis of electrophysiology data in Python\",  # noqa\n    \"long_description\": long_description,\n    \"long_description_content_type\": \"text/markdown\",\n    \"license\": \"BSD\",\n    \"url\": 'http://python-elephant.org',\n    \"project_urls\": {\n            \"Bug Tracker\": \"https://github.com/NeuralEnsemble/elephant/issues\",\n            \"Documentation\": \"https://elephant.readthedocs.io/en/latest/\",\n            \"Source Code\": \"https://github.com/NeuralEnsemble/elephant\",\n        },\n    \"python_requires\": \">=3.8\",\n    \"classifiers\": [\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: BSD License',\n        'Natural Language :: English',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3 :: Only',\n        'Topic :: Scientific/Engineering']\n}\n\n# no compile options and corresponding extensions\noptions = {\"--no-compile\": None, \"--no-compile-spade\": fim_module}\n# check if any option was specified\nif not any([True for key in options.keys() if key in sys.argv]):\n    if platform.system() in [\"Windows\", \"Linux\"]:\n        setup_kwargs[\"ext_modules\"] = [fim_module]\nelse:  # ...any option was specified\n    # select extensions accordingly\n    extensions = [module for flag, module in options.items() if\n                  flag not in sys.argv]\n    if None in extensions:  # None indicates \"--no-compile\" not in sys.argv\n        extensions.remove(None)\n        setup_kwargs[\"ext_modules\"] = extensions\n\n\nclass CommandMixin(object):\n    \"\"\"\n    This class acts as a superclass to integrate new commands in setuptools.\n    \"\"\"\n    user_options = [\n        ('no-compile', None, 'do not compile any C++ extension'),\n        ('no-compile-spade', None, 'do not compile spade related C++ extension')  # noqa\n    ]\n\n    def initialize_options(self):\n        \"\"\"\n        The method is responsible for setting default values for\n        all the options that the command supports.\n\n        Option dependencies should not be set here.\n        \"\"\"\n\n        super().initialize_options()\n        # Initialize options\n        self.no_compile_spade = None\n        self.no_compile = None\n\n    def finalize_options(self):\n        \"\"\"\n        Overriding a required abstract method.\n\n        This method is responsible for setting and checking the\n        final values and option dependencies for all the options\n        just before the method run is executed.\n\n        In practice, this is where the values are assigned and verified.\n        \"\"\"\n\n        super().finalize_options()\n\n    def run(self):\n        \"\"\"\n        Sets global which can later be used in setup.py to remove c-extensions\n        from setup call.\n        \"\"\"\n        # Use options\n        global no_compile_spade\n        global no_compile\n        no_compile_spade = self.no_compile_spade\n        no_compile = self.no_compile\n\n        super().run()\n\n\nclass InstallCommand(CommandMixin, install):\n    \"\"\"\n    This class extends setuptools.command.install class, adding user options.\n    \"\"\"\n    user_options = getattr(\n        install, 'user_options', []) + CommandMixin.user_options\n\n\nclass DevelopCommand(CommandMixin, develop):\n    \"\"\"\n    This class extends setuptools.command.develop class, adding user options.\n    \"\"\"\n    user_options = getattr(\n        develop, 'user_options', []) + CommandMixin.user_options\n\n\n# add classes to setup-kwargs to add the user options\nsetup_kwargs['cmdclass'] = {'install': InstallCommand,\n                            'develop': DevelopCommand}\n\nsetup(**setup_kwargs)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/emipy",
            "repo_link": "https://jugit.fz-juelich.de/network-science-group/emipy",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/enpt",
            "repo_link": "https://git.gfz-potsdam.de/EnMAP/GFZ_Tools_EnMAP_BOX/EnPT",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/enrichedheatmap",
            "repo_link": "https://github.com/jokergoo/EnrichedHeatmap",
            "content": {
                "codemeta": "",
                "readme": "# Make Enriched Heatmaps\n\n[![R-CMD-check](https://github.com/jokergoo/EnrichedHeatmap/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/EnrichedHeatmap/actions)\n[![codecov](https://img.shields.io/codecov/c/github/jokergoo/EnrichedHeatmap.svg)](https://codecov.io/github/jokergoo/EnrichedHeatmap)\n[![bioc](https://bioconductor.org/shields/downloads/devel/EnrichedHeatmap.svg)](https://bioconductor.org/packages/stats/bioc/EnrichedHeatmap/) \n[![bioc](http://www.bioconductor.org/shields/years-in-bioc/EnrichedHeatmap.svg)](http://bioconductor.org/packages/devel/bioc/html/EnrichedHeatmap.html)\n\n \n\nEnriched heatmap is a special type of heatmap which visualizes the enrichment of genomic signals on specific target regions. It is broadly used to visualize e.g. how histone marks are enriched to specific sites.\n\nThere are several tools that can make such heatmap (e.g. [ngs.plot](https://github.com/shenlab-sinai/ngsplot) or [deepTools](https://github.com/fidelram/deepTools)). Here we implement Enriched heatmap by [ComplexHeatmap](https://github.com/jokergoo/ComplexHeatmap) package. Since this type of heatmap is just a normal heatmap but with some fixed settings, with the functionality of ComplexHeatmap, it would be much easier to customize the heatmap as well as concatenating a list of heatmaps to show correspondance between differnet data sources.\n\n### Citation\n\nZuguang Gu, et al., EnrichedHeatmap: an R/Bioconductor package for comprehensive visualization of genomic signal associations, 2018. BMC Genomics. [link](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-4625-x)\n\n### Install\n\n**EnrichedHeatmap** is available on [Bioconductor](http://bioconductor.org/packages/devel/bioc/html/EnrichedHeatmap.html), you can install it by:\n\n```{r}\nif (!requireNamespace(\"BiocManager\", quietly=TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"EnrichedHeatmap\") \n```\n\nIf you want the latest version, install it directly from GitHub:\n\n```{r}\nlibrary(devtools)\ninstall_github(\"jokergoo/ComplexHeatmap\")\ninstall_github(\"jokergoo/EnrichedHeatmap\")\n```\n\n### Example\n\nLike other tools, the task involves two steps:\n\n1. Normalize the accosiations between genomic signals and target regions to a matrix.\n2. Draw heatmaps.\n\n```{r}\nmat1 = normalizeToMatrix(H3K4me3, tss, value_column = \"coverage\", \n    extend = 5000, mean_mode = \"w0\", w = 50)\nmat2 = normalizeToMatrix(meth, tss, value_column = \"meth\", mean_mode = \"absolute\",\n    extend = 5000, w = 50, background = NA, smooth = TRUE)\n```\n\n```{r}\npartition = kmeans(mat1, centers = 3)$cluster\nlgd = Legend(at = c(\"cluster1\", \"cluster2\", \"cluster3\"), title = \"Clusters\", \n    type = \"lines\", legend_gp = gpar(col = 2:4))\nht_list = Heatmap(partition, col = structure(2:4, names = as.character(1:3)), name = \"partition\",\n              show_row_names = FALSE, width = unit(3, \"mm\")) +\n          EnrichedHeatmap(mat1, col = c(\"white\", \"red\"), name = \"H3K4me3\", row_split = partition,\n              top_annotation = HeatmapAnnotation(lines = anno_enriched(gp = gpar(col = 2:4))), \n              column_title = \"H3K4me3\") + \n          EnrichedHeatmap(mat2, name = \"methylation\",\n              top_annotation = HeatmapAnnotation(lines = anno_enriched(gp = gpar(col = 2:4))), \n              column_title = \"Methylation\") +\n          Heatmap(log2(rpkm+1), col = c(\"white\", \"orange\"), name = \"log2(rpkm+1)\", \n              show_row_names = FALSE, width = unit(5, \"mm\"))\ndraw(ht_list, main_heatmap = \"H3K4me3\", gap = unit(c(2, 10, 2), \"mm\"))\n```\n\n![image](https://cloud.githubusercontent.com/assets/449218/14768684/41a6d534-0a49-11e6-800a-36ce15ad83ca.png)\n\nAlso when signals are discreate values. E.g. chromatin states:\n\n![test](https://user-images.githubusercontent.com/449218/36900761-e3d2ff86-1e24-11e8-865c-2cedb2674707.png)\n\nActually you can generate rather complex heatmaps:\n\n<img width=\"1043\" alt=\"screen shot 2017-10-13 at 10 42 42\" src=\"https://user-images.githubusercontent.com/449218/31608873-50c497d6-b272-11e7-8d81-cd88156d18aa.png\">\n\n\n### License\n\nMIT @ Zuguang Gu\n\n",
                "dependencies": "Package: EnrichedHeatmap\nType: Package\nTitle: Making Enriched Heatmaps\nVersion: 1.33.1\nDate: 2024-02-27\nAuthors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"),\n                  comment = c('ORCID'=\"0000-0002-7395-8709\"))\nDepends: R (>= 4.0.0), methods, grid, ComplexHeatmap (>= 2.11.0), GenomicRanges\nImports: matrixStats, stats, GetoptLong, Rcpp, utils, locfit, circlize (>= 0.4.5), IRanges\nSuggests: testthat (>= 0.3), knitr, markdown, rmarkdown, genefilter, RColorBrewer\nVignetteBuilder: knitr\nDescription: Enriched heatmap is a special type of heatmap which \n  visualizes the enrichment of genomic signals on specific target regions.\n  Here we implement enriched heatmap by ComplexHeatmap package. \n  Since this type of heatmap is just a normal heatmap but with some special settings, \n  with the functionality of ComplexHeatmap, it would be much easier\n  to customize the heatmap as well as concatenating to a list of heatmaps to \n  show correspondance between different data sources.\nbiocViews: Software, Visualization, Sequencing, GenomeAnnotation, Coverage\nURL: https://github.com/jokergoo/EnrichedHeatmap\nLicense: MIT + file LICENSE\nLinkingTo: Rcpp\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/esm-tools",
            "repo_link": "https://github.com/esm-tools/esm_tools",
            "content": {
                "codemeta": "",
                "readme": "=========\nESM Tools\n=========\n\nDocumentation\n-------------\n\n.. image:: https://readthedocs.org/projects/esm-tools/badge/?version=latest\n\nFor our complete documentation, please check https://esm-tools.readthedocs.io/en/latest/index.html.\n\nHow to cite this software\n-------------------------\nTo cite ESM-Tools, please use the following DOI: https://zenodo.org/doi/10.5281/zenodo.3737927. This DOI represents all versions of the software, and will always pointing to the latest version available on https://zenodo.org.\n\n\nBefore you continue\n-------------------\n\nYou will need python 3 (possibly version 3.6 or newer), a version of git that is not ancient (everything newer than 2.10 should be good), and up-to-date pip (``pip install -U pip``) to install the `esm_tools`. That means that on the supported machines, you could for example use the following settings:\n\nalbedo::\n\n    $ module load git\n    $ module load python\n\nlevante.dkrz.de::\n\n    $ module load git\n    $ module load python3\n\nglogin.hlrn.de / blogin.hlrn.de::\n\n    $ module load git\n    $ module load anaconda3\n\njuwels.fz-juelich.de::\n\n    $ module load Stages/2022\n    $ module load git\n    $ module load Python/3.9.6\n\naleph::\n\n    $ module load git\n    $ module load python\n\nNote that some machines might raise an error ``conflict netcdf_c`` when loading ``anaconda3``. In that case you will need to swap ``netcdf_c`` with ``anaconda3``::\n\n    $ module unload netcdf_c\n    $ module load anaconda3\n\n\n\nInstalling\n----------\n\n1. First, make sure you add the following lines to one of your login or profile files, i.e. ``~/.bash_profile``, ``~/.bashrc``, ``~/.profile``, etc.::\n\n        $ export PATH=$PATH:~/.local/bin\n        $ export LC_ALL=en_US.UTF-8\n        $ export LANG=en_US.UTF-8\n\n2. Inside the same login or profile file, add also the ``module`` commands necessary for the HPC system you are using (find the lines in the section above).\n\n3. You can choose to source now your login or profile file, so that the ``module`` and ``export`` commands are run (e.g. ``$ source ~/.bash_profile``).\n\n4. To use the new version of the ESM-Tools, now rewritten in Python, clone this repository::\n\n        $ git clone https://github.com/esm-tools/esm_tools.git\n\n5. Then, run the ``install.sh``::\n\n        $ ./install.sh\n\nYou should now have the command line tools ``esm_master`` and ``esm_runscripts``, which replace the old version.\n\n",
                "dependencies": "#!/usr/bin/env bash\n\n# boolean variable to exit the program on error\nshall_exit=false\n\n\n# prints the error message as the first argument and exits the program with non-zero status\nfunction quit_install () {\n    echo \"\"\n    echo \"$(tput setaf 1)ERROR: ${1} $(tput sgr 0)\"\n    echo \"please set the LANG and LC_ALL variables in your shell startup script (eg. .bashrc, .bash_profile) to the following values: \"\n    echo \"    export LC_ALL=en_US.UTF-8\"\n    echo \"    export LANG=en_US.UTF-8\"\n    echo \"and re-execute them so that the changes take place. Exiting the installation process\"\n\n    exit 1\n}\n\n\n# check if LANG environment is set to the correct value\nif [[ -z ${LANG+x} ]]; then\n    err_msg=\"LANG environment variable is not set\"\n    shall_exit=true\nfi\n\n\n# check if LC_ALL variable is set to the correct value\nif [[ -z ${LC_ALL+x} ]]; then\n    err_msg=\"LC_ALL variable is not set\"\n    shall_exit=true\nfi\n\n# we have an error, terminate the script\nif [[ ${shall_exit} == true ]]; then\n    quit_install \"${err_msg}\"\nfi\n\n\n\ngit_error_message=\"You need git version >= 2.13 to install the esm_tools (see README.rst).\"\nif hash git 2>/dev/null; then\n\tgit_version=`git --version | rev | cut -d' ' -f 1 | rev`\n\tmajor_git_version=`git --version | rev | cut -d' ' -f 1 | rev | cut -d'.' -f 1`\n\tminor_git_version=`git --version | rev | cut -d' ' -f 1 | rev | cut -d'.' -f 2`\n\tif test ${major_git_version} -lt \"2\"; then\n\t\techo $git_error_message\n\t\techo \"git version found: ${git_version}\"\n\telse\n\t\tif test ${minor_git_version} -lt \"10\"; then\n\t\t\techo $git_error_message\n\t\t\techo \"git version found: ${git_version}\"\n\t\tfi\n\tfi\nelse\n\techo $git_error_message\n\techo \"No installed git version found.\"\nfi\n\n# See here: https://tinyurl.com/5b57knvx\nif [ ! -z ${VIRTUAL_ENV+x} ]; then\n    echo \"Detected virtual environment $VIRTUAL_ENV\"\n    pip install -e .\nelif [ ! -z ${CONDA_PREFIX+x} ]; then\n    echo \"=======================\"\n    echo \"Using CONDA environment\"\n    echo \"=======================\"\n    echo \"WARNING: The use of a conda environment is currently not recommended. Use only for testing purposes!\"\n    ${CONDA_PREFIX}/bin/pip install -e .\nelse\n    echo \"Standard install to user directory (likely ${HOME}/.local)\"\n    pip install --user -e .\nfi\n\n#!/usr/bin/env python\n\n\"\"\"The setup script.\"\"\"\n\nfrom os import getenv\n\nfrom setuptools import find_packages, setup\n\nwith open(\"README.rst\") as readme_file:\n    readme = readme_file.read()\n\nwith open(\"HISTORY.rst\") as history_file:\n    history = history_file.read()\n\nrequirements = [\n    \"Click>=8.0.4\",  # Maximum version for Python 3.6 support\n    \"PyGithub==1.55\",\n    \"colorama==0.4.5\",\n    \"coloredlogs==15.0.1\",  # NOTE(PG): Should be removed during cleanup for loguru instead\n    \"emoji==1.7.0\",\n    \"f90nml==1.4.2\",\n    \"gfw-creator==0.2.2\",\n    \"gitpython==3.1.41\",  # Maximum version for Python 3.6 support\n    \"jinja2==3.1.4\",\n    \"loguru==0.6.0\",\n    \"numpy>=1.19.5\",  # Maximum version for Python 3.6 support\n    \"packaging==21.3\",\n    \"pandas>=1.1.5\",  # Correct compatiability with xarray for Python 3.6\n    \"psutil==5.9.1\",\n    \"pytest==7.1.2\",\n    \"pyyaml==6.0.1\",\n    \"questionary==1.10.0\",\n    \"ruamel.yaml==0.17.32\",\n    \"semver==2.13.0\",\n    \"sqlalchemy>=1.4.39\",\n    \"tabulate==0.8.10\",\n    \"tqdm==4.66.3\",\n    \"typing_extensions>=4.1.1\",  # Maximum number for Python 3.6 support\n    \"xdgenvpy==2.3.5\",\n    \"pydantic>=1.10.13\",\n    \"h5netcdf>=0.8.1\",\n]\n\nsetup_requirements = []\n\ntest_requirements = [\n    \"pyfakefs==4.6.0\",\n]\n\nsetup(\n    author=\"The ESM Tools Team\",\n    author_email=[\n        \"dirk.barbi@awi.de\",\n        \"paul.gierz@awi.de\",\n        \"miguel.andres-martinez@awi.de\",\n        \"deniz.ural@awi.de\",\n        \"jan.streffing@awi.de\",\n        \"sebastian.wahl@geomar.de\",\n        \"kai.himstedt@dkrz.de\",\n    ],\n    python_requires=\">=3.6, <=3.11\",\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: GNU General Public License v2 (GPLv2)\",\n        \"Natural Language :: English\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n    ],\n    description=\"ESM Tools external infrastructure for Earth System Modelling\",\n    entry_points={\n        \"console_scripts\": [\n            \"esm_archive=esm_archiving.cli:main\",\n            \"esm_cleanup=esm_cleanup.cli:main\",\n            \"esm_database=esm_database.cli:main\",\n            \"esm_master=esm_master.cli:main\",\n            \"esm_plugins=esm_plugin_manager.cli:main\",\n            \"esm_runscripts=esm_runscripts.cli:main\",\n            \"esm_tests=esm_tests.cli:main\",\n            \"esm_tools=esm_tools.cli:main\",\n            \"esm_utilities=esm_utilities.cli:main\",\n        ],\n    },\n    install_requires=requirements,\n    license=\"GNU General Public License v2\",\n    long_description=readme + \"\\n\\n\" + history,\n    include_package_data=True,\n    keywords=\"esm_tools\",\n    name=\"esm-tools\",\n    packages=find_packages(\"src\")\n    + [\n        \"esm_tools\",\n        \"esm_tools.configs\",\n        \"esm_tools.namelists\",\n        \"esm_tools.runscripts\",\n        \"esm_tools.couplings\",\n    ],\n    package_dir={\n        \"\": \"src\",\n        \"esm_tools.configs\": \"configs\",\n        \"esm_tools.namelists\": \"namelists\",\n        \"esm_tools.runscripts\": \"runscripts\",\n        \"esm_tools.couplings\": \"couplings\",\n    },\n    package_data={\n        \"esm_tools.configs\": [\"../configs/*\"],\n        \"esm_tools.namelists\": [\"../namelists/*\"],\n        \"esm_tools.runscripts\": [\"../runscripts/*\"],\n        \"esm_tools.couplings\": [\"../couplings/*\"],\n    },\n    setup_requires=setup_requirements,\n    test_suite=\"tests\",\n    tests_require=test_requirements,\n    url=\"https://github.com/esm-tools/esm_tools\",\n    version=\"6.43.5\",\n    zip_safe=False,\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/esmvalcore",
            "repo_link": "https://github.com/ESMValGroup/ESMValCore",
            "content": {
                "codemeta": "",
                "readme": "# ESMValCore package\n\n[![Documentation Status](https://readthedocs.org/projects/esmvalcore/badge/?version=latest)](https://esmvaltool.readthedocs.io/en/latest/?badge=latest)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3387139.svg)](https://doi.org/10.5281/zenodo.3387139)\n[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/#ESMValGroup_Lobby:gitter.im)\n[![CircleCI](https://circleci.com/gh/ESMValGroup/ESMValCore/tree/main.svg?style=svg)](https://circleci.com/gh/ESMValGroup/ESMValCore/tree/main)\n[![codecov](https://codecov.io/gh/ESMValGroup/ESMValCore/branch/main/graph/badge.svg?token=wQnDzguwq6)](https://codecov.io/gh/ESMValGroup/ESMValCore)\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/5d496dea9ef64ec68e448a6df5a65783)](https://app.codacy.com/gh/ESMValGroup/ESMValCore/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade)\n[![Anaconda-Server Badge](https://img.shields.io/conda/vn/conda-forge/ESMValCore?color=blue&label=conda-forge&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/esmvalcore)\n[![Github Actions Test](https://github.com/ESMValGroup/ESMValCore/actions/workflows/run-tests.yml/badge.svg)](https://github.com/ESMValGroup/ESMValCore/actions/workflows/run-tests.yml)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/ESMValGroup/ESMValCore/main.svg)](https://results.pre-commit.ci/latest/github/ESMValGroup/ESMValCore/main)\n\n![esmvaltoollogo](https://raw.githubusercontent.com/ESMValGroup/ESMValCore/main/doc/figures/ESMValTool-logo-2-glow.png)\n\nESMValCore: core functionalities for the ESMValTool, a community diagnostic\nand performance metrics tool for routine evaluation of Earth System Models\nin the Climate Model Intercomparison Project (CMIP).\n\n# Getting started\n\nPlease have a look at the\n[documentation](https://docs.esmvaltool.org/projects/esmvalcore/en/latest/quickstart/install.html)\nto get started.\n\n## Using the ESMValCore package to run recipes\n\nThe ESMValCore package provides the `esmvaltool` command, which can be used to run\n[recipes](https://docs.esmvaltool.org/projects/esmvalcore/en/latest/recipe/overview.html)\nfor working with CMIP-like data.\nA large collection of ready to use\n[recipes and diagnostics](https://docs.esmvaltool.org/en/latest/recipes/index.html)\nis provided by the\n[ESMValTool](https://github.com/ESMValGroup/ESMValTool)\npackage.\n\n## Using ESMValCore as a Python library\n\nThe ESMValCore package provides various functions for:\n\n-   Finding data in a directory structure typically used for CMIP data.\n\n-   Reading CMIP/CMOR tables and using those to check model and observational data.\n\n-   ESMValTool preprocessor functions based on\n    [iris](https://scitools-iris.readthedocs.io) for e.g. regridding,\n    vertical interpolation, statistics, correcting (meta)data errors, extracting\n    a time range, etcetera.\n\nread all about it in the\n[API documentation](https://docs.esmvaltool.org/projects/esmvalcore/en/latest/api/esmvalcore.html).\n\n## Getting help\n\nThe easiest way to get help if you cannot find the answer in the documentation\non [readthedocs](https://docs.esmvaltool.org), is to open an\n[issue on GitHub](https://github.com/ESMValGroup/ESMValCore/issues).\n\n## Contributing\n\nContributions are very welcome, please read our\n[contribution guidelines](https://docs.esmvaltool.org/projects/ESMValCore/en/latest/contributing.html)\nto get started.\n\n",
                "dependencies": "[build-system]\nrequires = [\n    \"setuptools >= 40.6.0\",\n    \"setuptools_scm>=6.2\",\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nauthors = [\n    {name = \"ESMValTool Development Team\", email = \"esmvaltool-dev@listserv.dfn.de\"}\n]\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Environment :: Console\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: Apache Software License\",\n    \"Natural Language :: English\",\n    \"Operating System :: POSIX :: Linux\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Programming Language :: Python :: 3.13\",\n    \"Topic :: Scientific/Engineering\",\n    \"Topic :: Scientific/Engineering :: Atmospheric Science\",\n    \"Topic :: Scientific/Engineering :: GIS\",\n    \"Topic :: Scientific/Engineering :: Hydrology\",\n    \"Topic :: Scientific/Engineering :: Physics\",\n]\ndynamic = [\n    \"readme\",\n    \"version\",\n]\ndependencies = [\n    \"cartopy\",\n    \"cf-units\",\n    \"dask[array,distributed]!=2024.8.0\",  # ESMValCore/issues/2503\n    \"dask-jobqueue\",\n    \"esgf-pyclient>=0.3.1\",\n    \"esmf-regrid>=0.11.0\",\n    \"esmpy\",  # not on PyPI\n    \"filelock\",\n    \"fiona\",\n    \"fire\",\n    \"geopy\",\n    \"humanfriendly\",\n    \"iris-grib>=0.20.0\",  # github.com/ESMValGroup/ESMValCore/issues/2535\n    \"isodate>=0.7.0\",\n    \"jinja2\",\n    \"nc-time-axis\",  # needed by iris.plot\n    \"nested-lookup\",\n    \"netCDF4\",\n    \"numpy!=1.24.3\",\n    \"packaging\",\n    \"pandas\",\n    \"pillow\",\n    \"prov\",\n    \"psutil\",\n    \"py-cordex\",\n    \"pybtex\",\n    \"pyyaml\",\n    \"requests\",\n    \"rich\",\n    \"scipy>=1.6\",\n    \"scitools-iris>=3.11\",  # 3.11 first to support Numpy 2 and Python 3.13\n    \"shapely>=2.0.0\",\n    \"stratify>=0.3\",\n    \"yamale\",\n]\ndescription = \"A community tool for pre-processing data from Earth system models in CMIP and running analysis scripts\"\nlicense = {text = \"Apache License, Version 2.0\"}\nname = \"ESMValCore\"\nrequires-python = \">=3.10\"\n\n[project.optional-dependencies]\ntest = [\n    \"pytest>6.0.0\",\n    \"pytest-cov>=2.10.1\",\n    \"pytest-env\",\n    \"pytest-html!=2.1.0\",\n    \"pytest-metadata>=1.5.1\",\n    \"pytest-mock\",\n    \"pytest-xdist\",\n    \"ESMValTool_sample_data==0.0.3\",\n]\ndoc = [\n    \"autodocsumm>=0.2.2\",\n    \"ipython\",\n    \"nbsphinx\",\n    \"sphinx>=6.1.3\",\n    \"pydata_sphinx_theme\",\n]\ndevelop = [\n    \"esmvalcore[test,doc]\",\n    \"pre-commit\",\n    \"pylint\",\n    \"pydocstyle\",\n    \"vprof\",\n]\n\n[project.scripts]\nesmvaltool = \"esmvalcore._main:run\"\n\n[project.urls]\nCode = \"https://github.com/ESMValGroup/ESMValCore\"\nCommunity = \"https://github.com/ESMValGroup/Community\"\nDocumentation = \"https://docs.esmvaltool.org\"\nHomepage = \"https://esmvaltool.org\"\nIssues = \"https://github.com/ESMValGroup/ESMValCore/issues\"\n\n[tool.setuptools]\ninclude-package-data = true\nlicense-files = [\"LICENSE\"]\npackages = [\"esmvalcore\"]\nzip-safe = false\n\n[tool.setuptools.dynamic]\nreadme = {file = \"README.md\", content-type = \"text/markdown\"}\n\n[tool.setuptools_scm]\nversion_scheme = \"release-branch-semver\"\n\n# Configure tests\n\n[tool.pytest.ini_options]\naddopts = [\n    \"-ra\",\n    \"--strict-config\",\n    \"--strict-markers\",\n    \"--doctest-modules\",\n    \"--ignore=esmvalcore/cmor/tables/\",\n    \"--cov-report=xml:test-reports/coverage.xml\",\n    \"--cov-report=html:test-reports/coverage_html\",\n    \"--html=test-reports/report.html\",\n]\nlog_cli_level = \"INFO\"\nenv = {MPLBACKEND = \"Agg\"}\nlog_level = \"WARNING\"\nminversion = \"6\"\nmarkers = [\n    \"installation: Test requires installation of dependencies\",\n    \"use_sample_data: Run functional tests using real data\",\n]\ntestpaths = [\"tests\"]\nxfail_strict = true\n\n[tool.coverage.run]\nparallel = true\nsource = [\"esmvalcore\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"if __name__ == .__main__.:\",\n    \"if TYPE_CHECKING:\",\n]\n\n# Configure type checks\n\n[tool.mypy]\n# See https://mypy.readthedocs.io/en/stable/config_file.html\nignore_missing_imports = true\nenable_error_code = [\n    \"truthy-bool\",\n]\n\n# Configure linters\n\n[tool.codespell]\nskip = \"*.ipynb,esmvalcore/config/extra_facets/ipslcm-mappings.yml,tests/sample_data/iris-sample-data/LICENSE\"\nignore-words-list = \"vas,hist,oce\"\n\n[tool.ruff]\nline-length = 79\n[tool.ruff.lint]\nselect = [\n    \"B\",\n    \"D\",       # pydocstyle\n    \"E\",       # pycodestyle\n    \"F\",       # pyflakes\n    \"I\",       # isort\n    \"ISC001\",  # pycodestyle\n    \"W\",       # pycodestyle\n]\nignore = [\n    \"E501\",  # Disable line-too-long as this is taken care of by the formatter.\n    \"D105\",  # Disable Missing docstring in magic method as these are well defined.\n]\n[tool.ruff.lint.per-file-ignores]\n\"tests/**.py\" = [\n    \"B011\",  # `assert False` is valid test code.\n    # Docstrings in tests are only needed if the code is not self-explanatory.\n    \"D100\",  # Missing docstring in public module\n    \"D101\",  # Missing docstring in public class\n    \"D102\",  # Missing docstring in public method\n    \"D103\",  # Missing docstring in public function\n    \"D104\",  # Missing docstring in public package\n]\n[tool.ruff.lint.isort]\nknown-first-party = [\"esmvalcore\"]\n[tool.ruff.lint.pydocstyle]\nconvention = \"numpy\"\n\n# Configure linters that are run by Prospector\n# TODO: remove once we have enabled all ruff rules for the tools provided by\n# Prospector, see https://github.com/ESMValGroup/ESMValCore/issues/2528.\n\n[tool.pylint.main]\njobs = 1  # Running more than one job in parallel crashes prospector.\nignore-paths = [\n    \"doc/conf.py\",  # Sphinx configuration file\n]\n[tool.pylint.basic]\ngood-names = [\n    \"_\",            # Used by convention for unused variables\n    \"i\", \"j\", \"k\",  # Used by convention for indices\n    \"logger\",       # Our preferred name for the logger\n]\n[tool.pylint.format]\nmax-line-length = 79\n[tool.pylint.\"messages control\"]\ndisable = [\n    \"import-error\",      # Needed because Codacy does not install dependencies\n    \"file-ignored\",      # Disable messages about disabling checks\n    \"line-too-long\",     # Disable line-too-long as this is taken care of by the formatter.\n    \"locally-disabled\",  # Disable messages about disabling checks\n]\n\n[tool.pydocstyle]\nconvention = \"numpy\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ethosfine-framework-for-integrated-energy-system-assessment",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/FINE",
            "content": {
                "codemeta": "",
                "readme": "<!-- markdownlint-disable line-length no-inline-html -->\n# ETHOS.FINE - Framework for Integrated Energy System Assessment\n\n[![Build Status](https://travis-ci.com/FZJ-IEK3-VSA/FINE.svg?branch=master)](https://travis-ci.com/FZJ-IEK3-VSA/FINE)\n[![Version](https://img.shields.io/pypi/v/FINE.svg)](https://pypi.python.org/pypi/FINE)\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/fine.svg)](https://anaconda.org/conda-forge/fine)\n[![Documentation Status](https://readthedocs.org/projects/vsa-fine/badge/?version=latest)](https://vsa-fine.readthedocs.io/en/latest/)\n[![PyPI - License](https://img.shields.io/pypi/l/FINE)](https://github.com/FZJ-IEK3-VSA/FINE/blob/master/LICENSE.txt)\n[![codecov](https://codecov.io/gh/FZJ-IEK3-VSA/FINE/branch/master/graph/badge.svg)](https://codecov.io/gh/FZJ-IEK3-VSA/FINE)\n\n<a href=\"https://www.fz-juelich.de/en/ice/ice-2\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/JSA-Header.svg?raw=True\" alt=\"Forschungszentrum Juelich Logo\" width=\"300px\"></a>\n\nThe ETHOS.FINE python package provides a framework for modeling, optimizing and assessing energy systems. With the provided framework, systems with multiple regions, commodities, time steps and investment periods can be modeled. Target of the optimization is the minimization of the systems net present value (NPV) while considering technical and environmental constraints. If only one investment period is considered, the net present value is equal to the total annual costs (TAC). Besides using the full temporal resolution, an interconnected typical period storage formulation can be applied, that reduces the complexity and computational time of the model.\n\nThis Readme provides information on the installation of the package. For further information have a look at the [documentation](https://vsa-fine.readthedocs.io/en/latest/).\n\nETHOS.FINE is used for the modelling of a diverse group of optimization problems within the [Energy Transformation PatHway Optimization Suite (ETHOS) at ICE-2](https://www.fz-juelich.de/de/ice/ice-2/leistungen/model-services).  \n\nIf you want to use ETHOS.FINE in a published work, please [**kindly cite following publication**](https://www.sciencedirect.com/science/article/pii/S036054421830879X) which gives a description of the first stages of the framework. The python package which provides the time series aggregation module and its corresponding literature can be found [here](https://github.com/FZJ-IEK3-VSA/tsam).\n\n## Installation \nThere are several options for the installation of ETHOS.FINE. You can install it via PyPI or from conda-forge.\nFor detailed information, have a look at the [installation documentation](https://vsa-fine.readthedocs.io/en/latest/installationDoc.html).\n\nNOTE: If you want to work on the source code of FINE, see [Editable install from conda-forge](#editable-install-from-conda-forge).\n\nIf you would like to run ETHOS.FINE for your analysis we recommend to install it directly from conda-forge into a new Python environment with\n\n```bash\nmamba create --name fine --channel conda-forge fine\n```\n\n**Note on Mamba vs.Conda:** `mamba` commands can be substitued with `conda`. We highly recommend using [(Micro-)Mamba](https://mamba.readthedocs.io/en/latest/) instead of Conda. The recommended way to use Mamba on your system is to install the [Miniforge distribution](https://github.com/conda-forge/miniforge#miniforge3). They offer installers for Windows, Linux and OS X. In principle, Conda and Mamba are interchangeable. The commands and concepts are the same. The distributions differ in the methodology for determining dependencies when installing Python packages. Mamba relies on a more modern methodology, which (with the same result) leads to very significant time savings during the installation of ETHOS.FINE. Switching to Mamba usually does not lead to any problems, as it is virtually identical to Conda in terms of operation.\n\n**Note on the solver:** \nThe functionality of ETHOS.FINE depends on the following C libraries that need to be installed on your system. If you do not know how to install those, consider installing from conda-forge. The mamba/conda installation comes with [GLPK](https://www.gnu.org/software/glpk/) [(installation for Windows)](https://sourceforge.net/projects/winglpk/files/latest/download) as Mixed Integer Linear Programming (MILP) solver. If you want to solve large problems it is highly recommended to install [GUROBI](http://www.gurobi.com/). See [\"Installation of an optimization solver\"](https://vsa-fine.readthedocs.io/en/latest/installationDoc.html#installation-of-an-optimization-solver) in the documentation for more information.\n\n\n## Examples\n\nA number of [examples](https://github.com/FZJ-IEK3-VSA/FINE/tree/develop/examples) shows the capabilities of ETHOS.FINE.\n\n- [00_Tutorial](https://github.com/FZJ-IEK3-VSA/FINE/tree/develop/examples/00_Tutorial)\n  - In this application, an energy supply system, consisting of two regions, is modeled and optimized. Recommended as starting point to get to know to ETHOS.FINE.\n- [01_1node_Energy_System_Workflow](https://github.com/FZJ-IEK3-VSA/FINE/tree/develop/examples/01_1node_Energy_System_Workflow)\n  - In this application, a single region energy system is modeled and optimized. The system includes only a few technologies.\n- [02_EnergyLand](https://github.com/FZJ-IEK3-VSA/FINE/tree/develop/examples/02_EnergyLand)\n  - In this application, a single region energy system is modeled and optimized. Compared to the previous examples, this example includes a lot more technologies considered in the system.\n- [03_Multi-regional_Energy_System_Workflow](https://github.com/FZJ-IEK3-VSA/FINE/tree/develop/examples/03_Multi-regional_Energy_System_Workflow)\n  - In this application, an energy supply system, consisting of eight regions, is modeled and optimized. The example shows how to model multi-regional energy systems. The example also includes a notebook to get to know the optional performance summary. The summary shows how the optimization performed.\n- [04_Model_Run_from_Excel](https://github.com/FZJ-IEK3-VSA/FINE/tree/develop/examples/04_Model_Run_from_Excel)\n  - ETHOS.FINE can also be run by excel. This example shows how to read and run a model using excel files.\n- [05_District_Optimization](https://github.com/FZJ-IEK3-VSA/FINE/tree/develop/examples/05_District_Optimization)\n  - In this application, a small district is modeled and optimized. This example also includes binary decision variables.\n- [06_Water_Supply_System](https://github.com/FZJ-IEK3-VSA/FINE/tree/develop/examples/06_Water_Supply_System)\n  - The application cases of ETHOS.FINE are not limited. This application shows how to model the water supply system.\n- [07_NetCDF_to_save_and_set_up_model_instance](https://github.com/FZJ-IEK3-VSA/FINE/tree/develop/examples/07_NetCDF_to_save_and_set_up_model_instance)\n  - This example shows how to save the input and optimized results of an energy system Model instance to netCDF files to allow reproducibility.\n- [08_Spatial_and_technology_aggregation](https://github.com/FZJ-IEK3-VSA/FINE/tree/develop/examples/08_Spatial_and_technology_aggregation)\n  - These two examples show how to reduce the model complexity. Model regions can be aggregated to reduce the number of regions (spatial aggregation). Input parameters are automatically adapted. Furthermore, technologies can be aggregated to reduce complexity, e.g. reducing the number of different PV components (technology aggregation). Input parameters are automatically adapted.\n- [09_Stochastic_Optimization](https://github.com/FZJ-IEK3-VSA/FINE/tree/develop/examples/9_Stochastic_Optimizatio)\n  - In this application, a stochastic optimization is performed. It is possible to perform the optimization of an energy system model with different input parameter sets to receive a more robust solution.\n- [10_PerfectForesight](https://github.com/FZJ-IEK3-VSA/FINE/tree/develop/examples/10_PerfectForesight)\n  - In this application, a transformation pathway of an energy system is modeled and optimized showing how to handle several investment periods with time-dependent assumptions for costs and operation.\n- [11_Partload](https://github.com/FZJ-IEK3-VSA/FINE/tree/develop/examples/11_Partload)\n  - In this application, a hydrogen system is modeled and optimized considering partload behavior of the electrolyzer.\n\n## Notes for developers\n\n### Editable install from conda-forge\n\nIt is recommended to create a clean environment with conda to use ETHOS.FINE because it requires many dependencies.\n\n```bash\nmamba env create --name fine --file requirements_dev.yml\nmamba activate fine\n```\n\nInstall ETHOS.FINE as editable install and without checking the dependencies from pypi with\n\n```bash\npython -m pip install --no-deps --editable .\n```\n\n### Editable install from pypi\n\nIf you do not want to use conda-forge consider the steps in section [Installation from pipy](#Installation-from-pipy) and install ETHOS.FINE as editable install and with developer dependencies with\n\n```bash\npython -m pip install --editable .[develop]\n```\n\n### Good coding style\n\nWe use [ruff](https://docs.astral.sh/ruff) to ensure good coding style. Make\nsure to use it before contributing to the code base with\n\n```bash\nruff check fine\n```\n\n## License\n\nMIT License\n\nCopyright (C) 2016-2024 FZJ-ICE-2\n\nActive Developers: Theresa Groß, Kevin Knosala, Noah Pflugradt, Johannes Behrens, Julian Belina, Arne Burdack, Toni Busch, Philipp Dunkel, David Franzmann, Patrick Freitag, Thomas Grube, Heidi Heinrichs, Maximilian Hoffmann, Jason Hu, Shitab Ishmam, Sebastian Kebrich, Felix Kullmann, Jochen Linßen, Rachel Maier, Shruthi Patil, Jan Priesmann, Julian Schönau, Maximilian Stargardt, Lovindu Wijesinghe, Christoph Winkler, Detlef Stolten\n\nAlumni: Robin Beer, Henrik Büsing, Dilara Caglayan, Timo Kannengießer, Leander Kotzur, Stefan Kraus, Peter Markewitz, Lars Nolting,Stanley Risch, Martin Robinius, Bismark Singh, Andreas Smolenko, Peter Stenzel, Chloi Syranidou, Johannes Thürauf, Lara Welder, Michael Zier\n\nYou should have received a copy of the MIT License along with this program.\nIf not, see https://opensource.org/licenses/MIT\n\n\n## About Us \n\n<a href=\"https://www.fz-juelich.de/en/ice/ice-2\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/iek3-square.png?raw=True\" alt=\"Institute image ICE-2\" width=\"280\" align=\"right\" style=\"margin:0px 10px\"/></a>\n\nWe are the <a href=\"https://www.fz-juelich.de/en/ice/ice-2\">Institute of Climate and Energy Systems (ICE) - Jülich Systems Analysis</a> belonging to the <a href=\"https://www.fz-juelich.de/en\">Forschungszentrum Jülich</a>. Our interdisciplinary department's research is focusing on energy-related process and systems analyses. Data searches and system simulations are used to determine energy and mass balances, as well as to evaluate performance, emissions and costs of energy systems. The results are used for performing comparative assessment studies between the various systems. Our current priorities include the development of energy strategies, in accordance with the German Federal Government’s greenhouse gas reduction targets, by designing new infrastructures for sustainable and secure energy supply chains and by conducting cost analysis studies for integrating new technologies into future energy market frameworks.\n\n## Contributions and Support\nEvery contributions are welcome:\n- If you have a question, you can start a [Discussion](https://github.com/FZJ-IEK3-VSA/FINE/discussions). You will get a response as soon as possible.\n- If you want to report a bug, please open an [Issue](https://github.com/FZJ-IEK3-VSA/FINE/issues/new). We will then take care of the issue as soon as possible.\n- If you want to contribute with additional features or code improvements, open a [Pull request](https://github.com/FZJ-IEK3-VSA/FINE/pulls).\n\n## Code of Conduct\nPlease respect our [code of conduct](CODE_OF_CONDUCT.md).\n\n## Acknowledgement\nThis work was initially supported by the Helmholtz Association under the Joint Initiative [\"Energy System 2050   A Contribution of the Research Field Energy\"](https://www.helmholtz.de/en/research/energy/energy_system_2050/). \n\nThe authors also gratefully acknowledge financial support by the Federal Ministry for Economic Affairs and Energy of Germany as part of the project [METIS](http://www.metis-platform.net/) (project number 03ET4064, 2018-2022).\n\nThis work was supported by the Helmholtz Association under the program [\"Energy System Design\"](https://www.helmholtz.de/en/research/research-fields/energy/energy-system-design/).\n\n<p float=\"left\">\n<a href=\"https://www.helmholtz.de/en/\"><img src=\"https://www.helmholtz.de/fileadmin/user_upload/05_aktuelles/Marke_Design/logos/HG_LOGO_S_ENG_RGB.jpg\" alt=\"Helmholtz Logo\" width=\"200px\"></a>\n</p>\n\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools>=61.0.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n\n[project]\nname = \"fine\"\nversion = \"2.4.0\"\ndescription = \"Framework for integrated energy systems assessment\"\nreadme = \"README.md\"\nauthors = [\n    { name = \"Johannes Behrens\", email = \"j.behrens@fz-juelich.de\"},\n    { name = \"Theresa Groß\", email = \"t.gross@fz-juelich.de\" },\n]\nmaintainers = [\n    { name = \"Johannes Behrens\", email = \"j.behrens@fz-juelich.de\"},\n    { name = \"Theresa Groß\", email = \"t.gross@fz-juelich.de\" },\n]\nlicense = { file = \"LICENSE\" }\n\nclassifiers = [\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Natural Language :: English\",\n    \"Operating System :: OS Independent\",\n    \"Programming Language :: Python\",\n    \"Programming Language :: Python :: 3\",\n    \"Topic :: Scientific/Engineering :: Mathematics\",\n    \"Topic :: Software Development :: Libraries :: Python Modules\",\n]\nkeywords = [\"energy assesment\", \"energy system\", \"optimization\"]\ndependencies = [\n    \"geopandas<=0.14.4\",\n    \"openpyxl<=3.1.5\",\n    \"matplotlib<=3.9.2\",\n    \"xlrd<=2.0.1\",\n    \"pyomo<=6.8.0\",\n    \"numpy<=1.26.4\",\n    \"pandas>=2,<=2.2.3\",\n    \"scipy<=1.14.1\",\n    \"scikit-learn>=1.2,<=1.5.2\",\n    \"xarray<=2024.3\",\n    \"rasterio<=1.4.1\",\n    \"netcdf4<=1.7.2\",\n    \"tsam\",\n    \"pwlf<=2.2.1\",\n    \"psutil<=5.9.8\",\n    \"gurobi-logtools<=3.1.0\",\n    \"ipykernel<=6.29.5\",\n]\n\nrequires-python = \">=3.10,<3.13\"\n\n[project.optional-dependencies]\ndevelop = [\n    \"sphinx<=7.4.4\",\n    \"sphinx_rtd_theme<=2.0.0\",\n    \"myst-parser<=2.0.0\",\n    \"pytest<=8.3.3\",\n    \"pytest-cov<=4.1.0\",\n    \"pytest-xdist<=3.6.1\",\n    \"nbval<=0.11.0\",\n    \"ruff<0.7.0\",\n]\n\n#Configureation options\n# https://docs.pytest.org/en/7.1.x/reference/reference.html#configuration-options\n[tool.pytest.ini_options]\ntestpaths = [\"test\"]\nconsole_output_style = \"progress\"\n# How to configure Filterwarning:\n# https://docs.python.org/3/library/warnings.html#warning-filter\n# action:message:category:module:line\n# Omit a field by add ing \":\" for each omitted field\n# Actions are: \"default\"\n# \"error\", \"ignore\", \"always\", \"module\", \"once\"\nfilterwarnings = []\n\n[project.urls]\nhomepage = \"https://www.fz-juelich.de/en/ice/ice-2/research-1/open_source/fine\"\nrepository = \"https://github.com/FZJ-IEK3-VSA/FINE\"\ndocumentation = \"https://vsa-fine.readthedocs.io/en/master/\"\n\n[tool.ruff]\nextend-include = [\"*.ipynb\"]\n\n[tool.ruff.lint]\nselect = [\"E4\", \"E7\", \"E9\", \"F\", \"PL\"]\n# See https://docs.astral.sh/ruff/rules for explanations\nignore = [\n    \"F403\",    # ‘from module import *’ used; unable to detect undefined names\n    \"PLR0913\", # Too many arguments in function definition ( > 5)\n    \"PLR0912\", # Too many branches ( > 12) (if, else statements)\n    \"PLR0915\", # Too many statements ( > 50)\n    \"PLR0911\", # Too many return statements ( > 6)\n    \"PLR2004\", # Magic value used in comparison, consider replacing `2` with a constant variable\n    \"PLR1714\", # Consider merging multiple comparisons. Use a `set` if the elements are hashable.\n]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/hisim",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/HiSim",
            "content": {
                "codemeta": "",
                "readme": " [![PyPI Version](https://img.shields.io/pypi/v/hisim.svg)](https://pypi.python.org/pypi/hisim)\n [![PyPI - License](https://img.shields.io/pypi/l/hisim)](LICENSE)\n \n <a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://www.fz-juelich.de/static/media/Logo.2ceb35fc.svg\" alt=\"Forschungszentrum Juelich Logo\" width=\"230px\"></a> \n\n# ETHOS.HiSim - Household Infrastructure and Building Simulator\n\nETHOS.HiSim is a Python package for simulation and analysis of household scenarios and building systems using modern\ncomponents as alternative to fossil fuel based ones. This package integrates load profiles generation of electricity\nconsumption, heating demand, electricity generation, and smart strategies of modern components, such as\nheat pump, battery, electric vehicle or thermal energy storage. ETHOS.HiSim is a package under development by\nForschungszentrum Jülich und Hochschule Emden/Leer. For detailed documentation, please\naccess [ReadTheDocs](https://household-infrastructure-simulator.readthedocs.io/en/latest/) of this repository.\n\n\n# Install Graphviz\n\nIf you want to use the feature that generates system charts, you need to install GraphViz in your system. If you don't\nhave Graphviz installed, you will experience error messages about a missing dot.exe under Windows.\n\nFollow the installation instructions from here:\nhttps://www.graphviz.org/download/\n\n(or simply disable the system charts)\n\nClone Repository\n-----------------------\nTo clone this repository, enter the following command to your terminal:\n\n```python\ngit clone https://github.com/FZJ-IEK3-VSA/HiSim.git\n```\n\nVirtual Environment\n-----------------------\nBefore installing `ETHOS.Hisim`, it is recommended to set up a Python virtual environment. Let `hisimvenv` be the name of\nvirtual environment to be created. For Windows users, setting the virtual environment in the path `\\Hisim` is done with\nthe command line:\n\n```python\npython -m venv hisimvenv\n```\n\nAfter its creation, the virtual environment can be activated in the same directory:\n\n```python \nhisimvenv\\Scripts\\activate\n```\n\nFor Linux/Mac users, the virtual environment is set up and activated as follows:\n\n```python \nvirtual hisimvenv source hisimvenv/bin/activate\n```\n\nAlternatively, Anaconda can be used to set up and activate the virtual environment:\n\n```python \nconda create -n hisimvenv python=3.9\nconda activate hisimvenv\n```\n\nWith the successful activation, `ETHOS.HiSim` is ready to be locally installed.\n\nInstall Package\n------------------------\nAfter setting up the virtual environment, install the package to your local libraries:\n\n```python\npip install -e .\n```\n\nOptional: Set Environment Variables\n-----------------------\nCertain components might access APIs to retrieve data. In order to use them, you need to set the url and key as environment variables. This can be done with an `.env` file wihtin the HiSim root folder or with system tools. The environment variables are:\n\n```\nUTSP_URL\nUTSP_API_KEY\n```\n\nRun Simple System Setups\n-----------------------\nRun the python interpreter in the `HiSim/system_setups` directory with the following command:\n\n```python\npython ../hisim/hisim_main.py simple_system_setup_one.py\n```\nor\n\n```python\npython ../hisim/hisim_main.py simple_system_setup_two.py\n```\n\nThis command executes `hisim_main.py` on the setup function `setup_function` implemented in the files `simple_system_setup_one.py`\nand `simple_system_setup_two.py` that are stored in `HiSim/system_setups`.\nThe results can be visualized under directory `results` created under the same directory where the script with the setup\nfunction is located.\n\nRun Basic Household System Setup\n-----------------------\nThe directory `HiSim/system_setups` also contains a basic household configuration in the script `basic_household.py`.\nIt can be executed with the following command:\n\n```python\npython ../hisim/hisim_main.py basic_household.py\n```\n\nThe system is set up with the following elements:\n\n* Occupancy (Residents' Demands)\n* Weather\n* Photovoltaic System\n* Building\n* Heat Pump\n\nHence, photovoltaic modules and the heat pump are responsible to cover the electricity the thermal energy demands as\nbest as possible. As the name of the setup function says, the components are explicitly connected to each other, binding\ninputs correspondingly to its output sequentially. This is difference then automatically connecting inputs and outputs\nbased its similarity. For a better understanding of explicit connection, proceed to session `IO Connecting Functions`.\n\nGeneric Setup Function Walkthrough\n---------------------\nThe basic structure of a setup function follows:\n\n1. Set the simulation parameters (See `SimulationParameters` class in `hisim/hisim/component.py`)\n1. Create a `Component` object and add it to `Simulator` object\n    1. Create a `Component` object from one of the child classes implemented in `hisim/hisim/components`\n        1. Check if `Component` class has been correctly imported\n    1. If necessary, connect your object's inputs with previous created `Component` objects' outputs.\n    1. Finally, add your `Component` object to `Simulator` object\n1. Repeat step 2 while all the necessary components have been created, connected and added to the `Simulator` object.\n\nOnce you are done, you can run the setup function according to the description in the simple system setup run.\n\nPackage Structure\n-----------\nThe main program is executed from `hisim/hisim/hisim_main.py`. The `Simulator`(`simulator.py`) object groups `Component`\ns declared and added from the setups functions. The `ComponentWrapper`(`simulator.py`) gathers together the `Component`s\ninside an `Simulator` Object. The `Simulator` object performs the entire simulation under the\nfunction `run_all_timesteps` and stores the results in a Python pickle `data.pkl` in a subdirectory\nof `hisim/hisim/results` named after the executed setup function. Plots and the report are automatically generated from\nthe pickle by the class `PostProcessor` (`hisim/hisim/postprocessing/postprocessing.py`).\n\nComponent Class\n-----------\nA child class inherits from the `Component` class in `hisim/hisim/component.py` and has to have the following methods\nimplemented:\n\n* i_save_state: updates previous state variable with the current state variable\n* i_restore_state: updates current state variable with the previous state variable\n* i_simulate: performs a timestep iteration for the `Component`\n* i_doublecheck: checks if the values are expected throughout the iteration\n\nThese methods are used by `Simulator` to execute the simulation and generate the results.\n\nList of `Component` Children\n-----------\nTheses classes inherent from `Component` (`component.py`) class and can be used in your setup function to customize\ndifferent configurations. All `Component` class children are stored in `hisim/hisim/components` directory. Some of these\nclasses are:\n\n- `RandomNumbers` (`random_numbers.py`)\n- `SimpleController` (`simple_controller.py`)\n- `SimpleSotrage` (`simple_storage.py`)\n- `Transformer` (`transformer.py`)\n- `PVSystem` (`pvs.py`)\n- `CHPSystem` (`chp_system.py`)\n- `Csvload` (`csvload.py`)\n- `SumBuilderForTwoInputs` (`sumbuilder.py`)\n- `SumBuilderForThreeInputs` (`sumbuilder.py`)\n- ToDo: more components to be added\n\nConnecting Input/Outputs\n-----------\nLet `my_home_electricity_grid` and `my_appliance` be Component objects used in the setup function. The\nobject `my_apppliance` has an output `ElectricityOutput` that has to be connected to an object `ElectricityGrid`. The\nobject `my_home_electricity_grid` has an input `ElectricityInput`, where this connection takes place. In the setup\nfunction, the connection is performed with the method `connect_input` from the `Simulator` class:\n\n```python\nmy_home_electricity_grid.connect_input(input_fieldname=my_home_electricity_grid.ELECTRICITY_INPUT,\n                                       src_object_name=my_appliance.component_name,\n                                       src_field_name=my_appliance.ELECTRICITY_OUTPUT)\n```\n\nConfiguration Automator\n-----------\nA configuration automator is under development and has the goal to reduce connections calls among similar components.\n\nPost Processing\n-----------\nAfter the simulator runs all time steps, the post processing (`postprocessing.py`) reads the persistent saved results,\nplots the data and\ngenerates a report.\n\n## Contributions and Collaborations\nETHOS.HiSim welcomes any kind of feedback, contributions, and collaborations. \nIf you are interested in joining the project, adding new features, or providing valuable insights, feel free to reach out (email to k.rieck@fz-juelich.de) and participate in our HiSim developer meetings held every second Monday. Additionally, we encourage you to utilize our Issue section to share feedback or report any bugs you encounter.\nWe look forward to your contributions and to making meaningful improvements. \nHappy coding!\n\n## License\n\nMIT License\n\nCopyright (C) 2020-2021 Noah Pflugradt, Leander Kotzur, Detlef Stolten, Tjarko Tjaden, Kevin Knosala, Sebastian Dickler, Katharina Rieck, David Neuroth, Johanna Ganglbauer, Vitor Zago, Frank Burkard, Maximilian Hillen, Marwa Alfouly, Franz Oldopp, Markus Blasberg\n\nYou should have received a copy of the MIT License along with this program.\nIf not, see https://opensource.org/licenses/MIT\n\n## About Us\n\n<a href=\"https://www.fz-juelich.de/iek/iek-3/DE/Home/home_node.html\"><img src=\"https://www.fz-juelich.de/SharedDocs/Bilder/IEK/IEK-3/Abteilungen2015/VSA_DepartmentPicture_2019-02-04_459x244_2480x1317.jpg?__blob=normal\" alt=\"Institut TSA\"></a>\n\nWe are\nthe [Institute of Energy and Climate Research - Techno-economic Systems Analysis (IEK-3)](https://www.fz-juelich.de/iek/iek-3/DE/Home/home_node.html)\nbelonging to the [Forschungszentrum Jülich](www.fz-juelich.de/). Our interdisciplinary institute's research is focusing\non energy-related process and systems analyses. Data searches and system simulations are used to determine energy and\nmass balances, as well as to evaluate performance, emissions and costs of energy systems. The results are used for\nperforming comparative assessment studies between the various systems. Our current priorities include the development of\nenergy strategies, in accordance with the German Federal Government’s greenhouse gas reduction targets, by designing new\ninfrastructures for sustainable and secure energy supply chains and by conducting cost analysis studies for integrating\nnew technologies into future energy market frameworks.\n\n## Contributions and Users\n\nDevelopment Partners:\n\n**Hochschule Emden/Leer** inside the project \"Piegstrom\".\n\n**4ward Energy** inside the EU project \"WHY\" \n\n## Acknowledgement\n\nThis work was supported by the Helmholtz Association under the Joint\nInitiative [\"Energy System 2050   A Contribution of the Research Field Energy\"](https://www.helmholtz.de/en/research/energy/energy_system_2050/)\n.\n\nFor this work weather data is based on data from [\"German Weather Service (Deutscher Wetterdienst-DWD)\"](https://www.dwd.de/DE/Home/home_node.html/) and [\"NREL National Solar Radiation Database\"](https://nsrdb.nrel.gov/data-viewer/download/intro/) (License: Creative Commons Attribution 3.0 United States License); individual values are averaged.\n\n<a href=\"https://www.helmholtz.de/en/\"><img src=\"https://www.helmholtz.de/fileadmin/user_upload/05_aktuelles/Marke_Design/logos/HG_LOGO_S_ENG_RGB.jpg\" alt=\"Helmholtz Logo\" width=\"200px\" style=\"float:right\"></a>\n\n<a href=\"https://www.dwd.de/\"><img src=\"https://www.dwd.de/SharedDocs/bilder/DE/logos/dwd/dwd_logo_258x69.png?__blob=normal&v=1\" alt=\"DWD Logo\" width=\"200px\" style=\"float:right\"></a>\n\nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 891943. \n\n<img src=\"eulogo.png\" alt=\"EU Logo\" width=\"200px\" style=\"float:right\"></a>\n\n<a href=\"https://www.why-h2020.eu/\"><img src=\"whylogo.jpg\" alt=\"WHY Logo\" width=\"200px\" style=\"float:right\"></a>\n\n=====\nHiSim\n=====\n\n\n.. image:: https://img.shields.io/pypi/v/hisim.svg\n        :target: https://pypi.python.org/pypi/hisim\n\n.. image:: https://img.shields.io/travis/audreyr/hisim.svg\n        :target: https://travis-ci.com/audreyr/hisim\n\n.. image:: https://readthedocs.org/projects/hisim/badge/?version=latest\n        :target: https://hisim.readthedocs.io/en/latest/?badge=latest\n        :alt: Documentation Status\n\n\n\n\nHiSim is a house infrastructure simulator\n\n\n* Free software: MIT license\n* Documentation: https://hisim.readthedocs.io.\n\n\nFeatures\n--------\n\n* TODO\n\nCredits\n-------\n\nThis package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template.\n\n.. _Cookiecutter: https://github.com/audreyr/cookiecutter\n.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage\n\n",
                "dependencies": "[tool.black]\nline-length = 120\n\nnumpy\npandas\nmatplotlib\nseaborn\nreportlab\npvlib # ==0.10.2\nopenpyxl\npytest\nsphinx\nsphinx-rtd-theme\ndataclasses_json\nhplib==1.9\nbslib==0.6\npsutil\npydot\ngraphviz\ndataclass_wizard\nutspclient==0.1.6\npyam-iamc\nhtml2image\ncontrol\ncasadi\nplotly\nordered_set\ntyping_extensions\npython-dotenv\nwindpowerlib  # ==0.2.1\nwetterdienst\ncdsapi\nxarray\n#!/usr/bin/env python\n\n\"\"\"The setup script.\"\"\"\n# clean\nfrom setuptools import setup, find_packages\n\nwith open(\"README.md\", encoding=\"utf-8\") as readme_file:\n    readme = readme_file.read()\n\nwith open(\"requirements.txt\", encoding=\"utf-8\") as requirements_file:\n    requirements = requirements_file.read().splitlines()\n\nsetup_requirements = [\n    \"pytest-runner\",\n]\n\ntest_requirements = [\n    \"pytest>=3\",\n]\n\n\nsetup(\n    author=\"Noah Pflugradt\",\n    author_email=\"n.pflugradt@fz-juelich.de\",\n    python_requires=\">=3.5\",\n    classifiers=[\n        \"Development Status :: 2 - Pre-Alpha\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Natural Language :: English\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.5\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n    ],\n    description=\"ETHOS.HiSim is a house infrastructure simulator\",\n    entry_points={\n        \"console_scripts\": [\n            \"hisim=hisim.cli:main\",\n        ],\n    },\n    install_requires=requirements,\n    license=\"MIT license\",\n    long_description=readme,\n    long_description_content_type=\"text/markdown\",\n    package_data={\"hisim\": [\"inputs/*\"]},\n    include_package_data=True,\n    keywords=\"hisim\",\n    name=\"hisim\",\n    packages=find_packages(include=[\"hisim\", \"hisim.*\"]),\n    setup_requires=setup_requirements,\n    test_suite=\"tests\",\n    tests_require=test_requirements,\n    url=\"https://github.com/FZJ-IEK3-VSA/HiSim\",\n    version=\"1.2.2\",\n    zip_safe=False,\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ethospenalps",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/ETHOS_PeNALPS",
            "content": {
                "codemeta": "",
                "readme": "| Name | Version | Platforms | Daily Tests |\n|---|---|---|---|\n|[![Conda Recipe](https://img.shields.io/badge/recipe-ethos_penalps-green.svg)](https://anaconda.org/conda-forge/ethos_penalps)|[![Conda Version](https://img.shields.io/conda/vn/conda-forge/ethos_penalps.svg)](https://anaconda.org/conda-forge/ethos_penalps)|[![Conda Platforms](https://img.shields.io/conda/pn/conda-forge/ethos_penalps.svg)](https://anaconda.org/conda-forge/ethos_penalps) |![example workflow](https://github.com/FZJ-IEK3-VSA/ETHOS_PeNALPS/actions/workflows/daily_tests.yml/badge.svg)\n\n<a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/FJZ_IEK-3_logo.svg?raw=True\" alt=\"Forschungszentrum Juelich Logo\" width=\"300px\"></a> \n# ETHOS.PeNALPS\n\nETHOS.PeNALPS (Petri Net Agent based Load Profile Simulator) is a Python library for the simulation of load profiles of industrial manufacturing processes. It is part of [ETHOS (Energy Transformation Pathway Optimization Suite)](https://go.fzj.de/ethos_suite). Load profiles are energy demand time series. Processes that can be simulated using ETHOS.PeNALPS include, for example, steel, paper, and industrial food production. One or multiple product orders are passed to the model which starts the simulation and eventually creates the desired load profiles.\n\n# Working Principle\n\nThe figure below shows the main conceptual objects of ETHOS.PeNALPS which are:\n\n- Generic model objects\n- Material flow simulations\n- Production plans\n- Result load profiles\n\nThe model of the material flow simulation is created by users based on generic simulation\nobjects. After the material flow simulation is completed, a set of production orders is passed to the model to start the simulation. The simulation generates a production plan that tracks the activity of each node to fulfill the requested set of orders. Based on the activity in the production plan, the load profiles are created for each node in therein. \n\n\n![Main Component Overview](paper/main_component_overview.png)\n*Depiction of the main components and workflow of ETHOS.PeNALPS*\n\nThe [HTML documentation provides a tutorial](https://ethospenalps.readthedocs.io/en/latest/ethos_penalps_tutorial/0_overview.html) for ETHOS.PeNALPS. The executable files for the tutorial are located in the example section of this repository. Also two examples for a [toffee production process](https://ethospenalps.readthedocs.io/en/latest/examples/toffee_example.html) and a [b-pillar production process](https://ethospenalps.readthedocs.io/en/latest/examples/b_pillar_example.html) are available.\n\n\n# Installation\n\n## Requirements\nThe installation process uses a Conda-based Python package manager. We highly recommend using Mamba instead of Anaconda. The recommended way to use Mamba on your system is to install the Miniforge distribution. They offer installers for Windows, Linux and OS X. Have a look at the [Mamba installation guide](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html) for further details. If you prefer to stick to Anaconda you should install the [libmamba solver which is a lot faster than the classic conda solver](https://www.anaconda.com/blog/a-faster-conda-for-a-growing-community). Otherwise the installation of ETHOS.PeNALPS might take very long or does not succeed at all.  \n\n```\nconda install -n base conda-libmamba-solver\nconda config --set solver libmamba\n```\n\nPlease note that the installation time of the solver can be very long if you have installed a lot of other packages into you conda base environment. In the following the commands mamba and conda are exchangeable if you prefer to use conda.\n\n## Installation via conda-forge\nThe simplest way ist to install ETHOS.PeNALPS into a fresh environment from conda-forge with:\n\nCreate a new environment\n```python\nmamba create -n penalps_env \n```\n\nActivate the environment\n```python\nmamba activate penalps_env\n```\n\nInstall ETHOS.PeNALPS from conda forge\n```python\nmamba install -c conda-forge ethos_penalps\n```\n\n## Installation from Github for Development\n\nFirst the repository must be cloned from Github\n\n```python\ngit clone https://github.com/FZJ-IEK3-VSA/ETHOS_PeNALPS.git\n```\nThen change the directory to the root folder of the repository.\n```python\ncd ETHOS_PeNALPS\n```\n\nCreate a new environment from the environment.yml file with all required dependencies.\n```python\nmamba env create --file=environment.yml\n```\n\nActivate the new environment.\n```python\nmamba activate ethos_penalps\n```\n\nInstall ethos_penalps locally in editable to mode for development.\n```python\npip install -e .\n```\n\n# Tests\n\nThe library can be tested by running pytest with the following command from the root folder.\n\n```python\npytest\n```\n\n# Documentation \n\nA ReadTheDocs Documentation can be found [here](http://ethospenalps.readthedocs.io/).\n\n\n# Contributing\n\nContributions are welcome, and they are greatly appreciated! Every little bit\nhelps, and credit will always be given.\n\nYou can contribute in many ways:\n\n## Types of Contributions\n\n\n### Report Bugs\n\n\nReport bugs at https://github.com/FZJ-IEK3-VSA/ETHOS_PeNALPS/issues.\n\nIf you are reporting a bug, please include:\n\n* Your operating system name and version.\n* Any details about your local setup that might be helpful in troubleshooting.\n* Detailed steps to reproduce the bug.\n\n### Fix Bugs\n\nLook through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help\nwanted\" is open to whoever wants to implement it.\n\n### Implement Features\n\nLook through the GitHub issues for features. Anything tagged with \"enhancement\"\nand \"help wanted\" is open to whoever wants to implement it.\n\n### Write Documentation\n\nETHOS.PeNALPS could always use more documentation, whether as part of the\nofficial ETHOS.PeNALPS docs, in docstrings, or even on the web in blog posts,\narticles, and such.\n\n### Submit Feedback\n\n\nThe best way to send feedback is to file an issue at https://github.com/FZJ-IEK3-VSA/ETHOS_PeNALPS/issues.\n\nIf you are proposing a feature:\n\n- Explain in detail how it would work.\n- Keep the scope as narrow as possible, to make it easier to implement.\n- Remember that this is a volunteer-driven project, and that contributions\n  are welcome :)\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools>=64.0.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools.package-data]\n\"ethos_penalps\" = [\"py.typed\"]\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n\n[project]\nname = \"ethos_penalps\"\nversion = \"1.0.7\"\ndescription = \"A package to create energy load curves for industry locations in Germany\"\nreadme = \"README.md\"\nauthors = [\n    { name = \"Julian Belina\", email = \"j.belina@fz-juelich.de\" },\n    { name = \"Noah Pflugradt\", email = \"n.pflugradt@fz-juelich.de\" },\n    { name = \"Detlef Stolten\", email = \"d.stolten@fz-juelich.de\" },\n]\nlicense = { file = \"LICENSE\" }\n\nkeywords = [\n    \"Load Profile\",\n    \"Simulator\",\n    \"Energy Simulation\",\n    \"Industrial Process\",\n]\nrequires-python = \">=3.10\"\n#Configureation options\n# https://docs.pytest.org/en/7.1.x/reference/reference.html#configuration-options\n\n[tool.pytest.ini_options]\nfilterwarnings = [\n    # \"ignore::DeprecationWarning:matplotlib\",\n    # \"ignore::DeprecationWarning:pkg_resources\",\n    # \"ignore::DeprecationWarning:distutils\",\n    # \"ignore::DeprecationWarning:requests_toolbelt\",\n    # \"ignore::DeprecationWarning:jupyter_client\",\n    # \"ignore::DeprecationWarning:importlib\",\n    \"ignore:invalid value encountered in cast:RuntimeWarning:numpy\",\n    \"ignore::marshmallow.warnings.RemovedInMarshmallow4Warning\",\n    \"ignore:The distutils package is deprecated and slated for removal in Python 3.12.:DeprecationWarning\",\n    \"ignore: Deprecated API features detected!\",\n    \"ignore: np.find_common_type is deprecated\",\n    \"ignore:the load_module()\",\n    \"ignore:the 'timedelta' type is not supported,\",\n    \"ignore:the imp module is deprecated in favour\",\n    \"ignore:Deprecated call to `pkg_resources.declare_namespace\",\n    \"ignore: Deprecated call to `pkg_resources.declare_namespace\",\n    \"ignore: distutils Version classes are deprecated.\",\n    \"ignore: pkg_resources is deprecated as an API.\",\n    \"ignore: Deprecated API features detected!\"\n    \n\n] # How to configure Filterwarning:minversion = \"6.0\"\n# https://docs.pytest.org/en/7.1.x/reference/reference.html#confval-pythonpath\ntestpaths = [\"test\"] # Sets the path where to look for tests\npythonpath =[\"test\"] # Sets the path which should be prepended to pythonpath relative to the root folder\n\nconsole_output_style = \"count\"\n# https://docs.python.org/3/library/warnings.html#warning-filter\n# action:message:category:module:line\n# Ommit a field by add ing \":\" for each ommited field\n# Actions are: \"default\"\n# \"error\", \"ignore\", \"always\", \"module\", \"once\"\nmarkers = [\"load_profile_entry_analyzer\",\"carpet_plot_tests\",\"matrix_resample_and_compression_tests\",\"resample_load_profile_meta_data\",\"production_plan_input_output\",\"test_load_profile_post_processing\"]\n\n[tool.mypy]\npython_version = \"3.10\"\nwarn_return_any = true\nwarn_unused_configs = true\ncheck_untyped_defs= true\n\n[[tool.mypy.overrides]]\nmodule = [\"cloudpickle.*\",\"datapane.*\",\"matplotlib.*\"]\nignore_missing_imports = true\n\n\n[tool.ruff.lint]\nignore = [\"F401\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ethosreflow",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/ethos.REFLOW",
            "content": {
                "codemeta": "",
                "readme": "# REFLOW: Renewable Energy potentials workFLOW manager\r\n\r\n<a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/FJZ_IEK-3_logo.svg?raw=True\" alt=\"Forschungszentrum Juelich Logo\" width=\"300px\"></a>\r\n\r\nREFLOW is a workflow manager tool designed to streamline and automate tasks related to renewable energy potential analyses. It is built with Luigi and provides an automated, robust framework for data acquisition, processing, land/sea eligibility analysis, technology placements, simulations and visualizations. It is build with transparency and reproducibility in mind. \r\n\r\n## Requirements\r\n* Python\r\n* An IDE (e.g. PyCharm, Visual Studio Code, etc.)\r\n* Unix-like system or bash on Windows\r\n* *optional*: Docker Desktop if running in container\r\n\r\n## Getting Started\r\n\r\n### Try an example project - Aachen technical wind potential\r\n\r\nWe highly recommend starting with the example workflow to get a feel for how REFLOW works. The example project is a simple technical wind energy potential analysis for a small region in Germany. \r\n\r\nTo run this analysis, follow these steps:\r\n1. Clone this repository to your local machine using:\r\n    ```bash\r\n    git clone https://github.com/FZJ-IEK3-VSA/ethos.REFLOW.git\r\n    ```\r\n\r\n2. Navigate to the example project directory:\r\n    ```bash\r\n    cd reflow/example_workflows/aachen_technical\r\n    ```\r\n3. Follow the instructions in the README.md file in the example project directory.\r\n\r\n\r\n### Initial Setup for a new project\r\n\r\nTo start your own new project using REFLOW, follow these steps:\r\n\r\n1. Clone this repository to your local machine using:\r\n    ```bash\r\n    git clone https://github.com/FZJ-IEK3-VSA/ethos.REFLOW.git\r\n    ```\r\n\r\n2. **Initialize a new Project:** Navigate to the **main REFLOW repo (this repo)** and run the initialize_project.py script by executing:\r\n    ```bash\r\n    python initialize_project.py\r\n    ```\r\n    You will be prompted to enter the name of your new project and the parent directory where it should be created.\r\n\r\n3. Create the main REFLOW python environment by running:\r\n    ```bash\r\n    conda env create -f required_software/requirements-reflow.yml\r\n    ```\r\n    Activate the environment by running:\r\n    ```bash\r\n    conda activate reflow\r\n    ```\r\n\r\n4. We recommend using a seperate conda environment for each software package which needs to be run outside of the main REFLOW environment. For example, if you are using WAsP, you can create a new environment which contains the PyWAsP package by:\r\n\r\n    4.1. Creating an environment file for the WAsP python package under the `required_software` directory. \r\n    You can use the provided `requirements-glaes.yml` file from the Aachen technical example as a template.\r\n\r\n    4.2. If the new environment file is in the required_software directory, the environment will automatically be created during the first task of the REFLOW workflow. \r\n\r\n    4.3. You can then run whichever task's script is needed inside the environment within your main REFLOW workflow. Do this by using a wrapper script which activates the environment, runs the task, and then deactivates the environment. (Again, see the Aachen technical example for reference.)\r\n\r\n**Optional but recommended - work with GIT:**\r\n\r\n5. **Create a New Git Repository**: Navigate into your new project directory and initilize it as a git repository:\r\n    ```bash\r\n    cd path/to/your-project-name\r\n    git init\r\n    git add .\r\n    git commit -m \"Initial commit\"\r\n    ```\r\n6. **Create an Empty Repository on Github** (or any other Git hosting service): Ensure the repository name matches your project's name. \r\n    Do not initialize the repository with a README, .gitignore or license.\r\n\r\n7. **Link your local repository to the remote repository**: Make sure you are in your new project directory and run the following commands:\r\n    ```bash\r\n    git remote add origin https://github.com/your-username/your-repo-name.git\r\n    git branch -M main\r\n    git push -u origin main\r\n    ```\r\n\r\nYou can now start working on your project and push your changes to the remote repository.\r\n\r\n## Examples\r\n\r\nThe example workflows are located in the `example_workflows` directory. Each example contains a README.md file with detailed instructions on how to run the workflow.\r\n\r\nWe recommend starting with the [Aachen technical wind potential example](example_workflows/aachen_technical/) to get a feel for how REFLOW works.\r\n\r\n## License\r\n\r\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE.txt) file for details.\r\n\r\nCopyright (c) 2024 Tristan Pelser (FZJ IEK-3), Jann Michael Weinand (FZJ IEK-3), Patrick Kuckertz (FZJ IEK-3), Detlef Stolten (FZJ IEK-3)\r\n\r\nYou should have received a copy of the MIT License along with this program.\r\nIf not, see https://opensource.org/licenses/MIT\r\n\r\n## About Us\r\n\r\n<a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/iek3-square.png?raw=True\" alt=\"Institute image IEK-3\" width=\"280\" align=\"right\" style=\"margin:0px 10px\"/></a>\r\n\r\nWe are the [Complex Energy System Modesl and Data Structures](https://www.fz-juelich.de/en/iek/iek-3/research/integrated-models-and-strategies/complex-energy-system-models-and-data-structures) department at the [Institute of Energy and Climate Research: Techno-economic Systems Analysis (IEK-3)](https://www.fz-juelich.de/en/iek/iek-3), belonging to the Forschungszentrum Jülich. Our interdisciplinary department's research is focusing on energy-related process and systems analyses. Data searches and system simulations are used to determine energy and mass balances, as well as to evaluate performance, emissions and costs of energy systems. The results are used for performing comparative assessment studies between the various systems. Our current priorities include the development of energy strategies, in accordance with the German Federal Government’s greenhouse gas reduction targets, by designing new infrastructures for sustainable and secure energy supply chains and by conducting cost analysis studies for integrating new technologies into future energy market frameworks.\r\n\r\n## Acknowledgements\r\n\r\nThe authors would like to thank the German Federal Government, the German State Governments, and the Joint Science Conference (GWK) for their funding and support as part of the NFDI4Ing consortium. Funded by the German Research Foundation (DFG) – 442146713, this work was also supported by the Helmholtz Association as part of the program “Energy System Design”.\r\n\r\n<p float=\"left\">\r\n<a href=\"https://www.helmholtz.de/en/\"><img src=\"https://www.helmholtz.de/fileadmin/user_upload/05_aktuelles/Marke_Design/logos/HG_LOGO_S_ENG_RGB.jpg\" alt=\"Helmholtz Logo\" width=\"200px\"></a>\r\n</p>\r\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ethoszoomin",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/ETHOS.zoomin",
            "content": {
                "codemeta": "",
                "readme": "ETHOS.zoomin: A spatial disaggregation workflow tool developed within the LOCALISED project.\n==============================\n\nA workflow tool that:\n1. Pulls data from a database.\n2. Disaggregates it based on proxy specifications present in the database.\n3. Evaulates quality rating - based on quality of the data to be disaggregated, proxy data and \nthe confidence in the assigned proxy.  \n4. Dumps the disaggregated data into the database.\n\n--------\n\nInstallation steps \n------------\n\n0. Before you begin:\n\nPlease make sure you have mamba installed in your base environment\n    ```bash\n    conda install mamba -c conda-forge\n    ```\nAlso create the initial database. Steps to create the Database will follow soon. \n\n\n1. Clone this repository:\n    ```bash\n    git clone https://jugit.fz-juelich.de/iek-3/shared-code/localised/ETHOS.zoomin.git\n    ```\n\n2. Install dependencies and the repo in a clean conda environment:\n    ```bash\n    cd ETHOS.zoomin\n    mamba env create -n zoomin --file=requirements.yml\n    conda activate zoomin\n    pip install -e .\n    ```\n\n3. Run the workflow from command line:\n    ```bash\n    bash run_deployment.sh\n    ```\n\n<p><small>Project based on the <a target=\"_blank\" href=\"https://drivendata.github.io/cookiecutter-data-science/\">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>\n",
                "dependencies": "\"\"\"Setup file for python packaging.\"\"\"\nimport os\nimport setuptools\n\ndir_path = os.path.dirname(os.path.realpath(__file__))\nwith open(os.path.join(dir_path, \"README.md\"), mode=\"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=\"zoomin\",\n    version=\"0.1.0\",\n    author=\"Shruthi Patil\",\n    author_email=\"s.patil@fz-juelich.de\",\n    description=\"Spatially disaggregates techo-economic data.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://www.localised-project.eu/\",\n    include_package_data=True,\n    packages=setuptools.find_packages(),\n    setup_requires=[\"setuptools-git\"],\n    classifiers=[\n        \"Development Status :: 2 - Pre-Alpha\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Natural Language :: English\",\n        \"Operating System :: Microsoft :: Windows\",\n        \"Operating System :: POSIX :: Linux\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Topic :: Scientific/Engineering :: Information Analysis\",\n    ],\n    keywords=[\"data import\", \"spatial disaggregation\", \"proxy metrics\"],\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/elias",
            "repo_link": "https://gitlab.kit.edu/julian.quinting/elias-2.0",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/beta-faircore4eosc",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/beta-faircore4eosc",
            "content": {
                "codemeta": "{\n    \"@context\": \"https://doi.org/10.5063/schema/codemeta-2.0\",\n    \"@type\": \"SoftwareSourceCode\",\n    \"description\": \"Beta Release - EOSC Research Software APIs and Connectors (Dagstuhl WP 6.2) \",\n    \"name\": \"beta-faircore4eosc\",\n    \"dateCreated\": \"2023-10-06\",\n    \"datePublished\": \"2023-10-09\",\n    \"license\": \"https://spdx.org/licenses/MIT.html\",\n    \"readme\": \"https://github.com/dagstuhl-publishing/beta-faircore4eosc/blob/main/README.md\",\n    \"softwareVersion\": \"1.0\",\n    \"installUrl\": \"https://github.com/dagstuhl-publishing/beta-faircore4eosc/blob/main/README.md#installation-steps\",\n    \"buildInstructions\": \"https://github.com/dagstuhl-publishing/beta-faircore4eosc/blob/main/README.md#b-deployment-steps-local-webserver-instance\",\n    \"dateModified\": \"2023-10-13\",\n    \"codeRepository\": \"https://github.com/dagstuhl-publishing/beta-faircore4eosc\",\n    \"issueTracker\": \"https://github.com/dagstuhl-publishing/faircore4eosc/issues\",\n    \"contIntegration\": \"https://github.com/dagstuhl-publishing/faircore4eosc/projects?query=is%3Aopen\",\n    \"developmentStatus\": \"active\",\n    \"operatingSystem\": \"Cross-platform\",\n    \"author\": [\n        {\n            \"@type\": \"Person\",\n            \"@id\": \"https://orcid.org/my-orcid?orcid=0000-0002-8504-3549\",\n            \"affiliation\": {\n                \"@type\": \"Organization\",\n                \"name\": \"Schloss Dagstuhl - Leibniz Center for Informatics, Wadern\"\n            },\n            \"email\": \"ramy.ahmed@dagstuhl.de\",\n            \"familyName\": \"Ahmed\",\n            \"givenName\": \"Ramy-Badr\"\n        }\n    ],\n    \"programmingLanguage\": [\n        \"PHP\",\n        \"JavaScript\",\n        \"Blade\"\n    ],\n    \"softwareRequirements\": [\n        \"PHP: ^8.1\",\n        \"Laravel/Framework: ^10.10\",\n        \"Livewire/Livewire: ^2.12\",\n        \"guzzlehttp/guzzle: ^7.2\",\n        \"laravel/tinker: ^2.8\",\n        \"Laravel/Octane: ^2.0\",\n        \"spiral/roadrunner-http: ^3.0.1\"\n    ],\n    \"funder\": {\n        \"@type\": \"Organization\",\n        \"@id\": \"https://doi.org/10.3030/101057264\",\n        \"funding\": \"EU-Horizon: 101057264\",\n        \"name\": \"European Comission\"\n    },\n    \"relatedLink\": [\n        \"https://faircore4eosc.dagstuhl.de/\",\n        \"https://faircore4eosc.eu/\"\n    ],\n    \"keywords\": [\n        \"Dagstuhl\",\n        \"Software Heritage\"\n    ],\n    \"applicationCategory\": [\n        \"MetaData\",\n        \"API Connectors\",\n        \"Research Software\"\n    ],\n    \"maintainer\": {\n        \"@type\": \"Person\",\n        \"@id\": \"https://orcid.org/my-orcid?orcid=0000-0002-8504-3549\",\n        \"affiliation\": {\n            \"@type\": \"Organization\",\n            \"name\": \"Schloss Dagstuhl - Leibniz Center for Informatics, Wadern\"\n        },\n        \"email\": \"ramy.ahmed@dagstuhl.de\",\n        \"familyName\": \"Ahmed\",\n        \"givenName\": \"Ramy-Badr\"\n    }\n}\n\n",
                "readme": "# beta-faircore4eosc\n\n![LARAVEL](https://img.shields.io/badge/LARAVEL-%23CC342D.svg?style=plastic&logo=laravel&logoColor=white) ![Livewire](https://img.shields.io/badge/Livewire-purple?style=plastic&logo=laravel&logoColor=white) ![PHP](https://img.shields.io/badge/PHP-777BB4?style=plastic&logo=php&logoColor=white)\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808833.svg)](https://doi.org/10.5281/zenodo.12808833)\n\n> [!Note]\n>  A demonstrable version can be accessed here: <a href=\"https://1959e979-c58a-4d3c-86bb-09ec2dfcec8a.ka.bw-cloud-instance.org/\" target=\"_blank\">**Demo Version**</a>\n\n#### Sample snapshot of the codemeta generator and converter demo:\n\n![snap.PNG](snap.PNG)\n\n#### Sample archiving repositories demo:\n\n![archive-samp.PNG](archive-samp.PNG)\n\n## Installation Steps:\n\n    1) Clone this project.\n    \n    2) Open a console session and navigate to the cloned directory, then:\n    \n        2.1) Run \"composer install\".\n        \n        2.2) Run \"npm install\".         \n        \n    3) (Optional) Create your local branch.\n    \n    4) (Optional) Acquire SWH tokens for increased SWH-API Rate-Limits.\n    \n    5) Prepare .env file:   \n    \n        5.1) Rename/Copy the cloned \".env.example\" file\n                cp .env.example .env   \n                \n        5.2) ADD these Keys:\n        \n                SWH_TOKEN=Your_TOKEN_FROM_SWH_ACCOUNT                   # As set in step 4)                \n                SWH_API_URL=https://archive.softwareheritage.org                \n                SWH_TOKEN_STAGING=Your_STAGING_TOKEN_FROM_SWH_ACCOUNT   # As set in step 4)                \n                SWH_API_URL_STAGING=https://webapp.staging.swh.network\n\n    You can now proceed to either I) or II)\n\n#### I) SWH API First Steps:\n\n    1) In a console session inside the cloned directory.    \n    \n        - Run 'php artisan tinker' to interact with the API modules.\n        \n    2) Initialise a new API session:\n    \n        - Write:\n                namespace App\\Modules\\SwhApi;                 \n                use App\\Modules\\SwhApi;\n                \n> - You can now proceed to the [SWH API Client documentation](https://github.com/Ramy-Badr-Ahmed/beta-faircore4eosc/blob/dev-cont/app/Modules/SwhApi/README.md)\n> - SWH Client as a standalone php library: https://github.com/Ramy-Badr-Ahmed/swh-client/wiki\n___\n\n#### II) Deployment Steps (Run your own local webserver instance):\n    \n    1) Setup a Database (e.g. MariaDB/MySQL/PostgreSQL) and create a new DB schema relevant for this project.\n    \n    2) Edit .env file\n    \n        2.1) (Optional) Generate an Application Key --> Navigate to the cloned directory and Run \"php artisan key:generate\"\n        \n        2.2) Edit Keys:\n                APP_NAME=beta-faircore4eosc\n                APP_ENV=local \n                APP_DEBUG=true\n                APP_URL=http://localhost\n\n                DB_CONNECTION=mysql             # As set in step 1)\n                DB_HOST=127.0.0.1               # As set in step 1)\n                DB_PORT=3306                    # As set in step 1)\n                DB_DATABASE=beta-faircore4eosc  # As set in step 1): Name of your database schema for this project\n                DB_USERNAME=root                # As set in step 1)\n                DB_PASSWORD=                    # As set in step 1)\n\n    3) Run \"php artisan migrate\".    \n    \n    4) (Optional) Checkout the first Commit: \"Initial Setup\".\n    \n    5) Navigate to the cloned directory and Run \"php artisan serve\" in your console.     \n    \n    6) Visit \"http://127.0.0.1:8000\" from your browser.\n\n#### III) Quick Test After II) Deployment:\n\n    1) Navigate to an Archive page, e.g.: /beta/archival-view-3\n    \n    2) Insert a group of repository URLs.\n    \n    3) Navigate to the previously cloned directory and then Run \"php artisan swh:updateCron\" in your console to synchronise with SWH.\n    \n       Note: you may need to set up a CRON/Scheduler in your OS.\n\n",
                "dependencies": "{\n    \"name\": \"laravel/laravel\",\n    \"type\": \"project\",\n    \"description\": \"The skeleton application for the Laravel framework.\",\n    \"keywords\": [\"laravel\", \"framework\"],\n    \"license\": \"MIT\",\n    \"require\": {\n        \"php\": \"^8.1\",\n        \"ext-dom\": \"*\",\n        \"ext-pdo\": \"*\",\n        \"composer/spdx-licenses\": \"^1.5\",\n        \"guzzlehttp/guzzle\": \"^7.2\",\n        \"laravel/framework\": \"^10.10\",\n        \"laravel/octane\": \"^2.0\",\n        \"laravel/sanctum\": \"^3.2\",\n        \"laravel/tinker\": \"^2.8\",\n        \"laravel/ui\": \"^4.2\",\n        \"livewire/livewire\": \"^2.12\",\n        \"php-ds/php-ds\": \"^1.4\",\n        \"seld/jsonlint\": \"^1.9\",\n        \"spatie/array-to-xml\": \"^3.2\",\n        \"spiral/roadrunner-cli\": \"^2.5.0\",\n        \"spiral/roadrunner-http\": \"^3.0.1\"\n    },\n    \"require-dev\": {\n        \"barryvdh/laravel-ide-helper\": \"^2.13\",\n        \"fakerphp/faker\": \"^1.9.1\",\n        \"laravel/pint\": \"^1.0\",\n        \"laravel/sail\": \"^1.18\",\n        \"mockery/mockery\": \"^1.4.4\",\n        \"nunomaduro/collision\": \"^7.0\",\n        \"phpunit/phpunit\": \"^10.1\",\n        \"spatie/laravel-ignition\": \"^2.0\"\n    },\n    \"autoload\": {\n        \"psr-4\": {\n            \"App\\\\\": \"app/\",\n            \"Database\\\\Factories\\\\\": \"database/factories/\",\n            \"Database\\\\Seeders\\\\\": \"database/seeders/\"\n        }\n    },\n    \"autoload-dev\": {\n        \"psr-4\": {\n            \"Tests\\\\\": \"tests/\"\n        }\n    },\n    \"scripts\": {\n        \"post-autoload-dump\": [\n            \"Illuminate\\\\Foundation\\\\ComposerScripts::postAutoloadDump\",\n            \"@php artisan package:discover --ansi\"\n        ],\n        \"post-update-cmd\": [\n            \"@php artisan vendor:publish --tag=laravel-assets --ansi --force\"\n        ],\n        \"post-root-package-install\": [\n            \"@php -r \\\"file_exists('.env') || copy('.env.example', '.env');\\\"\"\n        ],\n        \"post-create-project-cmd\": [\n            \"@php artisan key:generate --ansi\"\n        ]\n    },\n    \"extra\": {\n        \"laravel\": {\n            \"dont-discover\": []\n        }\n    },\n    \"config\": {\n        \"optimize-autoloader\": true,\n        \"preferred-install\": \"dist\",\n        \"sort-packages\": true,\n        \"allow-plugins\": {\n            \"pestphp/pest-plugin\": true,\n            \"php-http/discovery\": true\n        }\n    },\n    \"minimum-stability\": \"stable\",\n    \"prefer-stable\": true\n}\n\n{\n    \"private\": true,\n    \"type\": \"module\",\n    \"scripts\": {\n        \"dev\": \"vite\",\n        \"build\": \"vite build\"\n    },\n    \"devDependencies\": {\n        \"@popperjs/core\": \"^2.11.6\",\n        \"@vitejs/plugin-react\": \"^4.0\",\n        \"axios\": \"^1.7\",\n        \"bootstrap\": \"^5.2.3\",\n        \"chokidar\": \"^3.5.3\",\n        \"laravel-vite-plugin\": \"^0.8.0\",\n        \"lodash\": \"^4.17.19\",\n        \"postcss\": \"^8.1.14\",\n        \"react\": \"^18.2.0\",\n        \"react-dom\": \"^18.2.0\",\n        \"sass\": \"^1.56.1\",\n        \"vite\": \"^4.5.5\",\n        \"follow-redirects\": \"^1.15.4\"\n    }\n}\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fairmq",
            "repo_link": "https://github.com/FairRootGroup/FairMQ",
            "content": {
                "codemeta": "{\n  \"@context\": \"https://doi.org/10.5063/schema/codemeta-2.0\",\n  \"@type\": \"SoftwareSourceCode\",\n  \"name\": \"FairMQ\",\n  \"description\": \"C++ Message Queuing Library and Framework\",\n  \"license\": \"./COPYRIGHT\",\n  \"datePublished\": \"2018-04-15\",\n  \"developmentStatus\": \"active\",\n  \"softwareVersion\": \"master\",\n  \"releaseNotes\": \"https://github.com/FairRootGroup/FairMQ/releases\",\n  \"codeRepository\": \"https://github.com/FairRootGroup/FairMQ/\",\n  \"readme\": \"https://github.com/FairRootGroup/FairMQ/#readme\",\n  \"issueTracker\": \"https://github.com/FairRootGroup/FairMQ/issues\",\n  \"identifier\": \"https://doi.org/10.5281/zenodo.1689985\",\n  \"maintainer\": [\n    {\n      \"@type\": \"ResearchOrganisation\",\n      \"@id\": \"https://ror.org/02k8cbn47\",\n      \"name\": \"GSI Helmholtz Centre for Heavy Ion Research\"\n    }\n  ],\n  \"author\": [\n    {\n      \"@type\": \"Person\",\n      \"@id\": \"https://orcid.org/0000-0002-8071-4497\",\n      \"givenName\": \"Mohammad\",\n      \"familyName\": \"Al-Turany\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"@id\": \"https://orcid.org/0000-0003-3787-1910\",\n      \"givenName\": \"Dennis\",\n      \"familyName\": \"Klein\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Thorsten\",\n      \"familyName\": \"Kollegger\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"@id\": \"https://orcid.org/0000-0002-6249-155X\",\n      \"givenName\": \"Alexey\",\n      \"familyName\": \"Rybalchenko\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Nicolas\",\n      \"familyName\": \"Winckler\"\n    }\n  ],\n  \"contributor\": [\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Laurent\",\n      \"familyName\": \"Aphecetche\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Sebastien\",\n      \"familyName\": \"Binet\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Giulio\",\n      \"familyName\": \"Eulisse\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Radoslaw\",\n      \"familyName\": \"Karabowicz\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Matthias\",\n      \"familyName\": \"Kretz\",\n      \"email\": \"kretz@kde.org\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Mikolaj\",\n      \"familyName\": \"Krzewicki\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Andrey\",\n      \"familyName\": \"Lebedev\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Teo\",\n      \"familyName\": \"Mrnjavac\",\n      \"email\": \"teo.m@cern.ch\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Gvozden\",\n      \"familyName\": \"Neskovic\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Matthias\",\n      \"familyName\": \"Richter\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"@id\": \"https://orcid.org/0000-0002-5321-8404\",\n      \"givenName\": \"Christian\",\n      \"familyName\": \"Tacke\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Florian\",\n      \"familyName\": \"Uhlig\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Sandro\",\n      \"familyName\": \"Wenzel\"\n    }\n  ]\n}\n\n",
                "readme": "<!-- {#mainpage} -->\n# FairMQ\n\n[![license](https://alfa-ci.gsi.de/shields/badge/license-LGPL--3.0-orange.svg)](COPYRIGHT)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1689985.svg)](https://doi.org/10.5281/zenodo.1689985)\n[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/6915/badge)](https://bestpractices.coreinfrastructure.org/projects/6915)\n[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8B%20%20%E2%97%8F%20%20%E2%97%8F-yellow)](https://github.com/FairRootGroup/FairMQ/actions/workflows/fair-software.yml)\n[![Spack package](https://repology.org/badge/version-for-repo/spack/fairmq.svg)](https://repology.org/project/fairmq/versions)\n\nC++ Message Queuing Library and Framework\n\nDocs: [Book](https://github.com/FairRootGroup/FairMQ/blob/dev/README.md#documentation)\n\nFind all FairMQ releases [here](https://github.com/FairRootGroup/FairMQ/releases).\n\n## Introduction\n\nFairMQ is designed to help implementing large-scale data processing workflows needed in next-generation Particle Physics experiments. FairMQ is written in C++ and aims to\n  * provide **an asynchronous message passing abstraction** of different data transport technologies,\n  * provide a reasonably **efficient data transport** service (zero-copy, high throughput),\n  * be **data format agnostic**, and\n  * provide **basic building blocks** that can be used to implement higher level data processing workflows.\n\nThe core of FairMQ provides an abstract asynchronous message passing API with scalability protocols\ninspired by [ZeroMQ](https://github.com/zeromq/libzmq) (e.g. PUSH/PULL, PUB/SUB).\nFairMQ provides multiple implementations for its API (so-called \"transports\",\ne.g. `zeromq` and `shmem` (latest release of the `ofi` transport in v1.4.56, removed since v1.5+)) to cover\na variety of use cases\n(e.g. inter-thread, inter-process, inter-node communication) and machines (e.g. Ethernet, Infiniband).\nIn addition to this core functionality FairMQ provides a framework for creating \"devices\" - actors which\nare communicating through message passing. FairMQ does not only allow the user to use different transport\nbut also to mix them; i.e: A Device can communicate using different transport on different channels at the\nsame time. Device execution is modelled as a simple state machine that shapes the integration points for\nthe user task. Devices also incorporate a plugin system for runtime configuration and control.\nNext to the provided [devices](https://github.com/FairRootGroup/FairMQ/tree/master/fairmq/devices) and\n[plugins](https://github.com/FairRootGroup/FairMQ/tree/master/fairmq/plugins) the user can extend FairMQ\nby developing his own plugins to integrate his devices with external configuration and control services.\n\nFairMQ has been developed in the context of its mother project [FairRoot](https://github.com/FairRootGroup/FairRoot) -\na simulation, reconstruction and analysis framework.\n\n## Installation from Source\n\nRecommended:\n\n```bash\ngit clone https://github.com/FairRootGroup/FairMQ fairmq_source\ncmake -S fairmq_source -B fairmq_build -GNinja -DCMAKE_BUILD_TYPE=Release\ncmake --build fairmq_build\nctest --test-dir fairmq_build --output-on-failure --schedule-random -j<ncpus>\ncmake --install fairmq_build --prefix $(pwd)/fairmq_install\n```\n\nPlease consult the [manpages of your CMake version](https://cmake.org/cmake/help/latest/manual/cmake.1.html) for more options.\n\nIf dependencies are not installed in standard system directories, you can hint the installation location via\n`-DCMAKE_PREFIX_PATH=...` or per dependency via `-D{DEPENDENCY}_ROOT=...` (`*_ROOT` variables can also be environment variables).\n\n\n## Usage\n\nFairMQ ships as a CMake package, so in your `CMakeLists.txt` you can discover it like this:\n\n```cmake\nfind_package(FairCMakeModules 1.0 REQUIRED)\ninclude(FairFindPackage2)\nfind_package2(FairMQ)\nfind_package2_implicit_dependencies()\n```\n\nThe [`FairFindPackage2` module](https://fairrootgroup.github.io/FairCMakeModules/latest/module/FairFindPackage2.html) is part of the [`FairCMakeModules` package](https://fairrootgroup.github.io/FairCMakeModules).\n\nIf FairMQ is not installed in system directories, you can hint the installation:\n\n```cmake\nlist(PREPEND CMAKE_PREFIX_PATH /path/to/fairmq_install)\n```\n\n## Dependencies\n\n  * [Boost](https://www.boost.org/)\n  * [CMake](https://cmake.org/)\n  * [Doxygen](http://www.doxygen.org/)\n  * [FairCMakeModules](https://github.com/FairRootGroup/FairCMakeModules) (optionally bundled)\n  * [FairLogger](https://github.com/FairRootGroup/FairLogger)\n  * [GTest](https://github.com/google/googletest) (optionally bundled)\n  * [ZeroMQ](http://zeromq.org/)\n\n  Which dependencies are required depends on which components are built.\n\n  Supported platform is Linux. macOS is supported on a best-effort basis.\n\n## CMake options\n\nOn command line:\n\n  * `-DDISABLE_COLOR=ON` disables coloured console output.\n  * `-DBUILD_TESTING=OFF` disables building of tests.\n  * `-DBUILD_EXAMPLES=OFF` disables building of examples.\n  * `-DBUILD_DOCS=ON` enables building of API docs.\n  * `-DFAIRMQ_CHANNEL_DEFAULT_AUTOBIND=OFF` disable channel `autoBind` by default\n  * You can hint non-system installations for dependent packages, see the #installation-from-source section above\n\nAfter the `find_package(FairMQ)` call the following CMake variables are defined:\n\n| Variable | Info |\n| --- | --- |\n| `${FairMQ_PACKAGE_DEPENDENCIES}` | the list of public package dependencies |\n| `${FairMQ_<dep>_VERSION}` | the minimum `<dep>` version FairMQ requires |\n| `${FairMQ_<dep>_COMPONENTS}` | the list of `<dep>` components FairMQ depends on |\n| `${FairMQ_PACKAGE_COMPONENTS}` | the list of components FairMQ consists of |\n| `${FairMQ_#COMPONENT#_FOUND}` | `TRUE` if this component was built |\n| `${FairMQ_VERSION}` | the version in format `MAJOR.MINOR.PATCH` |\n| `${FairMQ_GIT_VERSION}` | the version in the format returned by `git describe --tags --dirty --match \"v*\"` |\n| `${FairMQ_PREFIX}` | the actual installation prefix |\n| `${FairMQ_BINDIR}` | the installation bin directory |\n| `${FairMQ_INCDIR}` | the installation include directory |\n| `${FairMQ_LIBDIR}` | the installation lib directory |\n| `${FairMQ_DATADIR}` | the installation data directory (`../share/fairmq`) |\n| `${FairMQ_CMAKEMODDIR}` | the installation directory of shipped CMake find modules |\n| `${FairMQ_BUILD_TYPE}` | the value of `CMAKE_BUILD_TYPE` at build-time |\n| `${FairMQ_CXX_FLAGS}` | the values of `CMAKE_CXX_FLAGS` and `CMAKE_CXX_FLAGS_${CMAKE_BUILD_TYPE}` at build-time |\n\n## Documentation\n\n1. [Device](docs/Device.md#1-device)\n   1. [Topology](docs/Device.md#11-topology)\n   2. [Communication Patterns](docs/Device.md#12-communication-patterns)\n   3. [State Machine](docs/Device.md#13-state-machine)\n   4. [Multiple devices in the same process](docs/Device.md#15-multiple-devices-in-the-same-process)\n2. [Transport Interface](docs/Transport.md#2-transport-interface)\n   1. [Message](docs/Transport.md#21-message)\n      1. [Ownership](docs/Transport.md#211-ownership)\n   2. [Channel](docs/Transport.md#22-channel)\n   3. [Poller](docs/Transport.md#23-poller)\n3. [Configuration](docs/Configuration.md#3-configuration)\n    1. [Device Configuration](docs/Configuration.md#31-device-configuration)\n    2. [Communication Channels Configuration](docs/Configuration.md#32-communication-channels-configuration)\n        1. [JSON Parser](docs/Configuration.md#321-json-parser)\n        2. [SuboptParser](docs/Configuration.md#322-suboptparser)\n    3. [Introspection](docs/Configuration.md#33-introspection)\n4. [Development](docs/Development.md#4-development)\n   1. [Testing](docs/Development.md#41-testing)\n   2. [Static Analysis](docs/Development.md#42-static-analysis)\n      1. [CMake Integration](docs/Development.md#421-cmake-integration)\n      2. [Extra Compiler Arguments](docs/Development.md#422-extra-compiler-arguments)\n5. [Logging](docs/Logging.md#5-logging)\n   1. [Log severity](docs/Logging.md#51-log-severity)\n   2. [Log verbosity](docs/Logging.md#52-log-verbosity)\n   3. [Color for console output](docs/Logging.md#53-color)\n   4. [File output](docs/Logging.md#54-file-output)\n   5. [Custom sinks](docs/Logging.md#55-custom-sinks)\n6. [Examples](docs/Examples.md#6-examples)\n7. [Plugins](docs/Plugins.md#7-plugins)\n   1. [Usage](docs/Plugins.md#71-usage)\n   2. [Development](docs/Plugins.md#72-development)\n   3. [Provided Plugins](docs/Plugins.md#73-provided-plugins)\n       1. [PMIx](docs/Plugins.md#731-pmix)\n\n",
                "dependencies": "################################################################################\n# Copyright (C) 2018-2023 GSI Helmholtzzentrum fuer Schwerionenforschung GmbH  #\n#                                                                              #\n#              This software is distributed under the terms of the             #\n#              GNU Lesser General Public Licence (LGPL) version 3,             #\n#                  copied verbatim in the file \"LICENSE\"                       #\n################################################################################\n\n\n# Project ######################################################################\ncmake_minimum_required(VERSION 3.15 FATAL_ERROR)\ncmake_policy(VERSION 3.15...3.26)\n\nlist(PREPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/cmake)\ninclude(GitHelper)\nget_git_version()\n\nproject(FairMQ VERSION ${PROJECT_VERSION} LANGUAGES CXX)\ninclude(FairMQProjectSettings)\n################################################################################\n\n\n# Build options ################################################################\ninclude(FairMQBuildOption)\n\nfairmq_build_option(BUILD_FAIRMQ        \"Build FairMQ library and devices.\"\n                                         DEFAULT ON)\nfairmq_build_option(BUILD_TESTING       \"Build tests.\"\n                                         DEFAULT OFF REQUIRES \"BUILD_FAIRMQ\")\nfairmq_build_option(BUILD_EXAMPLES      \"Build FairMQ examples.\"\n                                         DEFAULT ON  REQUIRES \"BUILD_FAIRMQ\")\nfairmq_build_option(BUILD_TIDY_TOOL     \"Build the fairmq-tidy tool.\"\n                                         DEFAULT OFF)\nfairmq_build_option(BUILD_DOCS          \"Build FairMQ documentation.\"\n                                         DEFAULT OFF)\nfairmq_build_option(USE_EXTERNAL_GTEST  \"Do not use bundled GTest. Not recommended.\"\n                                         DEFAULT OFF)\nfairmq_build_option(FAIRMQ_DEBUG_MODE   \"Compile in debug mode (may decrease performance).\"\n                                         DEFAULT OFF)\n################################################################################\n\n\n# Dependencies #################################################################\ninclude(CTest)\ninclude(FairMQDependencies)\n################################################################################\n\n\n# Targets ######################################################################\nif(BUILD_FAIRMQ)\n  add_subdirectory(fairmq)\nendif()\n\nif(BUILD_TESTING)\n  add_subdirectory(test)\nendif()\n\nif(BUILD_EXAMPLES)\n  add_subdirectory(examples)\nendif()\n\nif(BUILD_DOCS)\n  set(DOXYGEN_OUTPUT_DIRECTORY doxygen)\n  set(DOXYGEN_PROJECT_NUMBER ${PROJECT_GIT_VERSION})\n  set(DOXYGEN_PROJECT_BRIEF \"C++ Message Queuing Library and Framework\")\n  set(DOXYGEN_USE_MDFILE_AS_MAINPAGE README.md)\n  set(DOXYGEN_HTML_FOOTER docs/footer.html)\n  doxygen_add_docs(doxygen README.md fairmq)\n  add_custom_target(docs ALL DEPENDS doxygen)\nendif()\n\nif(BUILD_TIDY_TOOL)\n  add_subdirectory(fairmq/tidy)\nendif()\n################################################################################\n\n\n# Package components ###########################################################\nif(BUILD_FAIRMQ)\n  list(APPEND PROJECT_PACKAGE_COMPONENTS fairmq)\nendif()\nif(BUILD_TESTING)\n  list(APPEND PROJECT_PACKAGE_COMPONENTS tests)\nendif()\nif(BUILD_EXAMPLES)\n  list(APPEND PROJECT_PACKAGE_COMPONENTS examples)\nendif()\nif(BUILD_DOCS)\n  list(APPEND PROJECT_PACKAGE_COMPONENTS docs)\nendif()\nif(BUILD_TIDY_TOOL)\n  list(APPEND PROJECT_PACKAGE_COMPONENTS tidy_tool)\nendif()\n################################################################################\n\n\n# Installation #################################################################\nif(BUILD_FAIRMQ)\n  install(FILES cmake/FindZeroMQ.cmake\n    DESTINATION ${PROJECT_INSTALL_CMAKEMODDIR}\n  )\nendif()\nif(BUILD_DOCS)\n  install(DIRECTORY ${CMAKE_BINARY_DIR}/doxygen/html\n    DESTINATION ${PROJECT_INSTALL_DATADIR}/docs\n  )\nendif()\nif(BUILD_TIDY_TOOL)\n  install(FILES cmake/FairMQTidy.cmake\n    DESTINATION ${PROJECT_INSTALL_CMAKEMODDIR}\n  )\nendif()\n\ninclude(FairMQPackage)\ninstall_cmake_package()\n################################################################################\n\n\n# Summary ######################################################################\ninclude(FairMQSummary)\n\nmessage(STATUS \"${BWhite}${PROJECT_NAME}${CR} ${PROJECT_GIT_VERSION} from ${PROJECT_DATE}\")\nfair_summary_global_cxx_flags_standard()\nfair_summary_build_types()\nfair_summary_package_dependencies()\nfairmq_summary_components()\nfairmq_summary_static_analysis()\nfairmq_summary_install_prefix()\nfairmq_summary_debug_mode()\nfairmq_summary_compile_definitions()\nmessage(STATUS \"  \")\n################################################################################\n\nspack:\n  specs:\n  - boost+container+program_options+filesystem+date_time+regex\n  - faircmakemodules\n  - fairlogger+pretty\n  - fmt\n  - libzmq\n  view: true\n  concretizer:\n    unify: true\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fame",
            "repo_link": "https://gitlab.com/fame-framework/fame-io",
            "content": {
                "codemeta": "",
                "readme": "<!-- SPDX-FileCopyrightText: 2024 German Aerospace Center <fame@dlr.de>\n\nSPDX-License-Identifier: Apache-2.0 -->\n[![PyPI version](https://badge.fury.io/py/fameio.svg)](https://badge.fury.io/py/fameio)\n[![JOSS](https://joss.theoj.org/papers/10.21105/joss.04958/status.svg)](https://doi.org/10.21105/joss.04958)\n[![Zenodo](https://zenodo.org/badge/DOI/10.5281/zenodo.4314337.svg)](https://doi.org/10.5281/zenodo.4314337)\n[![PyPI license](https://img.shields.io/pypi/l/fameio.svg)](https://badge.fury.io/py/fameio)\n[![pipeline status](https://gitlab.com/fame-framework/fame-io/badges/main/pipeline.svg)](https://gitlab.com/fame-framework/fame-io/commits/main)\n[![coverage report](https://gitlab.com/fame-framework/fame-io/badges/main/coverage.svg)](https://gitlab.com/fame-framework/fame-io/-/commits/main)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![REUSE status](https://api.reuse.software/badge/gitlab.com/fame-framework/fame-io)](https://api.reuse.software/info/gitlab.com/fame-framework/fame-io)\n[![Common Changelog](https://common-changelog.org/badge.svg)](https://common-changelog.org)\n![GitLab last commit](https://img.shields.io/gitlab/last-commit/fame-framework%2Ffame-io)\n![GitLab closed issues by-label](https://img.shields.io/gitlab/issues/closed/fame-framework%2Ffame-io)\n\n# FAME-Io\n\n*Tools for input preparation and output digestion of FAME models*\n\nFAME-Io compiles input for FAME models in protobuf format and extracts model outputs to human-readable files.\nPlease visit the [FAME-Wiki](https://gitlab.com/fame-framework/wiki/-/wikis/home) to get an explanation of FAME and its components.\n\n# Installation\n\nWe recommend installing `fameio` using PyPI:\n\n    pip install fameio\n\nYou may also use `pipx`. For detailed information please refer to the\nofficial `pipx` [documentation](https://github.com/pypa/pipx).\n\n    pipx install fameio\n\n`fameio` is currently developed and tested for Python 3.8 or higher.\nSee the `pyproject.toml` for a complete listing of dependencies.\n\n# Usage\n\nFAME-Io currently offers two main scripts `makeFameRunConfig` and `convertFameResults`.\nBoth are automatically installed with the package.\nThe first one creates a protobuf file for FAME applications using YAML definition files and CSV files.\nThe latter one reads output files from FAME applications in protobuf format and converts them to CSV files.\n\nYou may use the [example data](https://gitlab.com/dlr-ve/esy/amiris/examples) provided for\nthe [AMIRIS](https://gitlab.com/dlr-ve/esy/amiris/amiris) model which can be used to simulate electricity markets\nin [Germany](https://gitlab.com/dlr-ve/esy/amiris/examples/-/tree/main/Germany2019), [Austria](https://gitlab.com/dlr-ve/esy/amiris/examples/-/tree/main/Austria2019),\nand a simple [proof-of-concept model](https://gitlab.com/dlr-ve/esy/amiris/examples/-/tree/main/Simple).\n\n## Make a FAME run configuration\n\nDigests configuration files in YAML format, combines them with CSV data files and creates a single input file for FAME\napplications in protobuf format.\nCall structure:\n\n    makeFameRunConfig -f <path/to/scenario.yaml>\n\nYou may also specify any of the following arguments:\n\n| Command                | Action                                                                                                                                   |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| `-l` or `--log`        | Sets the logging level. Default is `info`. Options are `debug`, `info`, `warning`, `warn`, `error`, `critical`.                          |\n| `-lf` or `--logfile`   | Sets the logging file. Default is `None`. If `None` is provided, all logs get only printed to the console.                               |\n| `-o` or `--output`     | Sets the path of the compiled protobuf output file. Default is `config.pb`.                                                              |\n| `-enc` or `--encoding` | Sets the encoding of all yaml files to the given one (e.g. 'utf8' or 'cp1252'. Default is `None`, i.e. your operating system's standard. |\n\nThis could look as follows:\n\n    makeFameRunConfig -f <path/to/scenario.yaml> -l debug -lf <path/to/scenario.log> -o <path/to/config.pb>\n\nYou may also call the configuration builder from any Python script with\n\n```python\nfrom fameio.scripts.make_config import Options, run as make_config\n\nmake_config({Options.FILE: \"path/to/scenario.yaml\", })\n```\n\nSimilar to the console call you may also specify custom run config arguments and add it in a dictionary to the function\ncall.\n\n```python\nfrom fameio.scripts.make_config import Options, run as make_config\n\nrun_config = {Options.FILE: \"path/to/scenario.yaml\",\n              Options.LOG_LEVEL: \"info\",\n              Options.OUTPUT: \"output.pb\",\n              Options.LOG_FILE: \"scenario.log\",\n              }\n\nmake_config(run_config)\n```\n\nYou can also use the associated argument parser, to extract the run_config dynamically from a string:\n\n```python\nfrom fameio.scripts.make_config import Options, run as make_config\nfrom fameio.cli.make_config import handle_args\n\nmy_defaults = {Options.FILE: \"path/to/scenario.yaml\",\n               Options.LOG_LEVEL: \"info\",\n               Options.OUTPUT: \"output.pb\",\n               Options.LOG_FILE: \"scenario.log\",\n               }\nmy_arg_string = ['-f', 'my/other/scenario.yaml', '-l', 'error']\n\nrun_config = handle_args(my_arg_string, my_defaults)\nmake_config(run_config)\n```\n\n### Scenario YAML\n\nThe \"scenario.yaml\" file contains all configuration options for a FAME-based simulation.\nIt consists of the sections `Schema`, `GeneralProperties`, `Agents` and `Contracts`, and the optional\nsection `StringSets`.\nAll of them are described below.\n\n#### Schema\n\nThe Schema describes a model's components such as its types of agents, their inputs, what data they exchange, etc.\nIt is also used to validate the model inputs provided in the `scenario.yaml`.\nSince the Schema is valid until the model itself is changed, it is recommended to defined it in a separate file and\ninclude the file here.\n\nCurrently, the schema specifies:\n\n* which type of Agents can be created\n* what type of input attributes an Agent uses\n* what type of Products an Agent can send in Contracts, and\n* the names of the Java packages for the classes corresponding to Agents, DataItems and Portables.\n\nThe Schema consists of the sections `JavaPackages` and `AgentTypes`.\n\n##### JavaPackages\n\nThis section defines the name of the Java packages in which the model code is located.\nA similar data set was formerly specified in the `fameSetup.yaml`, but is now specified in the schema.\nEach of the three sections `Agents`, `DataItems`, and `Portables` contain a list of fully qualified java package names\nof your model's classes.\nPackage names can occur in multiple lists and may overlap.\nIt is not necessary (but possible) to specify the nearest enclosing package for each Agent, DataItem or Portable.\nSpecifying any super-package will also work.\nAlso, package names occur on multiple lists for Agent, DataItem or Portable.\n\nFor example, for a project with all its\n\n* Agent-derived java classes located in packages below the package named \"agents\",\n* DataItem implementation classes in a subpackage named \"msg\",\n* Portable implementation classes in a subpackages named \"portableItems\" and \"otherPortables\",\n\nthe corresponding section in the schema would look like this:\n\n```yaml\nJavaPackages:\n  Agents:\n    - \"agents\"\n  DataItems:\n    - \"msg\"\n  Portables:\n    - \"portableItems\"\n    - \"otherPortables\"\n```\n\nOne can leave out the `DataItems` specifications, but `Agents` and `Portables` are required and must not be empty.\n\n##### AgentTypes\n\nHere, each type of agent that can be created in your FAME-based application is listed, its attributes and its available\nProducts for Contracts.\nThe structure of this section\n\n```yaml\nAgentTypes:\n  MyAgentType:\n    Attributes:\n      MyAttribute:\n        ...\n      MyOtherAttribute:\n        ...\n    Products: [ 'Product1', 'Product2', 'Product3' ]\n    Outputs: [ 'Column1', 'Column2', 'Column3' ]\n    Metadata:\n      Some: \"Dict with Metadata that you would like to add\"\n  MyOtherAgentWithoutProductsOrAttributes:\n```\n\n* `MyAgentType` Java's simple class name of the Agent type\n* `Attributes` indicates that beginning of the attribute definition section for this Agent type\n* `MyAttribute` Name of an attribute as specified in the corresponding Java source code of this Agent type (annotated\n  with \"@Input\")\n* `MyOtherAttribute` Name of another attribute derived from Java source code\n* `Products` list or dictionary of Products that this Agent can send in Contracts; derived from Java source code of this\n  Agent type (annotated with \"@Product\")\n* `Outputs` list or dictionary of Output columns that this Agent can write to; derived from Java source code of this\n  Agent type (annotated with \"@Output\")\n* `Metadata` dictionary with any content that is assigned to this Agent type as additional information\n* `MyOtherAgentWithoutProductsOrAttributes` an Agent type that requires neither Attributes nor Products\n\nAttributes, Products, Outputs and Metadata are optional - there may be useful Agents that require none of them.\nProducts and Outputs can both be lists of Strings, or dictionaries with additional Metadata.\nFor example, you could write the above in the following way:\n\n```yaml\nProducts:\n  Product1:\n    Metadata:\n      Any: \"information you would like to add to Product1 using a dictionary form\"\n  Product2:\n  Product3:\nOutputs:\n  Column1:\n  Column2:\n    ThisEntry: \"is ignored, as it is not below the keyword: 'Metadata'\"\n    Metadata:\n      My: \"Metadata\"\n      That: \"will be saved to Column2\"\n  Column3:\n```\n\nHere, \"Product1\" and \"Column2\" have additional, optional Metadata assigned to them (using the keyword \"Metadata\").\nThe other Products and Columns have no metadata assigned to them - which is also ok.\n\nIn the AgentType definition example above attribute definition was not shown explicitly (indicated by `...`).\nThe next example provides details on how to define an attribute:\n\n```yaml\nMySimpleAttribute:\n  AttributeType: enum\n  Mandatory: true\n  List: false\n  Values: [ 'AllowedValue1', 'AllowedValue2' ]\n  Default: 'AllowedValue1'\n  Help: 'My help text'\n  Metadata:\n    Go: \"here\"\n\nMyComplexAttribute:\n  AttributeType: block\n  NestedAttributes:\n    InnerAttributeA:\n      AttributeType: integer\n      Values:\n        1:\n          Metadata:\n            Explain: \"1 is a allowed value\"\n        2:\n          Metadata:\n            Comment: \"2 is also allowed, but consider using 1\"\n    InnerAttributeB:\n      AttributeType: double\n```\n\n* `MySimpleAttribute`, `MyDoubleList`, `MyComplexAttribute` Names of the attributes as specified in the Java enum\n  annotated with \"@Input\"\n* `AttributeType` (required) data type of the attribute; see options in table below\n* `Mandatory` (optional - true by default) if true: the attribute is required for this agent and validation will fail if\n  the attribute is missing in the scenario **and** no default is provided\n* `List` (optional - false by default)\n    * `AttributeType: time_series` cannot be true\n    * `AttributeType: block`\n        * if true: any nested element in the scenario must be part of a list element and thus can appear multiple times\n        * if false: any nested element in the scenario can only appear once\n    * any other AttributeType: the attribute is interpreted as list, i.e. multiple values can be assigned to this\n      attribute in the scenario\n* `NestedAttributes` (required only if `AttributeType: block`, otherwise disallowed) starts an inner Attribute\n  definition block - defined Attributes are sub-elements of `MyComplexAttribute`\n* `Values` (optional - None by default):\n  * if present, defines a list or dictionary of allowed values for this attribute\n  * if a dictionary is used, individual Metadata can be assigned to each allowed value using the `Metadata` keyword\n* `Default` (optional - None by default):\n  * if present, defines a default value to be used if the scenario does not specify one\n  * must match one of the entries in `Values` in case those are defined\n  * can be a list if the attribute is a list\n* `Help` (optional - None by default): if present, defines a help text for your Attribute\n* `Metadata` (optional - None by default): if present, defines additional metadata assigned to the Attribute\n\n| AttributeType | value                                                                                                                   |\n|---------------|-------------------------------------------------------------------------------------------------------------------------|\n| `integer`     | a 32-bit integer value                                                                                                  |\n| `double`      | a 64-bit floating-point value (integers also allowed)                                                                   |\n| `long`        | a 64-bit integer value                                                                                                  |\n| `time_stamp`  | either a FAME time stamp string or 64-bit integer value                                                                 |\n| `string`      | any string                                                                                                              |\n| `string_set`  | a string from a set of allowed `Values` defined in `StringSet` section in `scenario`                                    |\n| `enum`        | a string from a set of allowed `Values` defined in `schema`                                                             |\n| `time_series` | either a path to a .csv-file or a single 64-bit floating-point value; does not support `List: true`                     |\n| `block`       | this attribute has no value of its own but hosts a group of nested Attributes; implies `NestedAttributes` to be defined |\n\n#### GeneralProperties\nSpecifies FAME-specific properties of the simulation. Structure:\n\n```yaml\nGeneralProperties:\n  RunId: 1\n  Simulation:\n    StartTime: 2011-12-31_23:58:00\n    StopTime: 2012-12-30_23:58:00\n    RandomSeed: 1\n```\n\nParameters:\n\n* `RunId` an ID that can be given to the simulation; use at your discretion\n* `StartTime` time stamp in the format YYYY-MM-DD_hh:mm:ss; first moment of the simulation.\n* `StopTime` time stamp in the format YYYY-MM-DD_hh:mm:ss; last moment of the simulation - i.e. simulation terminates\n  after passing that time stamp\n* `RandomSeed` seed to initialise random number generation; each value leads to a unique series of random numbers.\n\n#### Agents\n\nSpecifies all Agents to be created in the simulation in a list. Each Agent has its own entry.\nStructure:\n\n```yaml\nAgents:\n  - Type: MyAgentWithInputs\n    Id: 1\n    Attributes:\n      MyEnum: SAME_SHARES\n      MyInteger: 2\n      MyDouble: 4.2\n      MyTimeSeries: \"./path/to/time_series.csv\"\n    Metadata:\n      Can: \"also be assigned\"\n\n  - Type: MyAgentWithoutInputs\n    Id: 2\n```\n\nAgent Parameters:\n\n* `Type` Mandatory; Java's simple class name of the agent to be created\n* `Id` Mandatory; simulation-unique id of this agent; if two agents have the same ID, the configuration process will\n  stop.\n* `Attributes` Optional; if the agent has any attributes, specify them here in the format \"AttributeName: value\"; please\n  see attribute table above\n* `Metadata` Optional; can be assigned to each instance of an Agent, as well as to each of its Attributes\n\nThe specified `Attributes` for each agent must match the specified `Attributes` options in the linked Schema (see above).\nFor better structure and readability of the `scenario.yaml`, `Attributes` may also be specified in a nested way as demonstrated below.\n\n```yaml\nAgents:\n  - Type: MyAgentWithInputs\n    Id: 1\n    Attributes:\n      Parent:\n        MyEnum: SAME_SHARES\n        MyInteger: 2\n      Parent2:\n        MyDouble: 4.2\n        Child:\n          MyTimeSeries: \"./path/to/time_series.csv\"\n```\n\nIn case Attributes are defined with `List: true` option, lists are assigned to an Attribute or Group:\n\n```yaml\nAttributes:\n  MyDoubleList: [ 5.2, 4.5, 7, 9.9 ]\n  MyListGroup:\n    - IntValueA: 5\n      IntValueB: 42\n    - IntValueA: 7\n      IntValueB: 100\n```\n\nHere, `MyDoubleList` and `MyListGroup` need to specify `List: true` in the corresponding Schema.\nThe shorter `[]`-notation was used to assign a list of floating-point values to `MyDoubleList`.\nNested items `IntValueA` and `IntValueB` of `MyListGroup` are assigned within a list, allowing the specification of\nthese nested items several times.\n\n##### Attribute Metadata\nMetadata can be assigned to any value, list item, or superstructure.\nTo assign Metadata to a primitive value, create a dictionary from it, set the actual value with the inner keyword `Value` and add the keyword `Metadata` like this:\n\n```yaml\nValueWithoutMetadata: 1\nSameValueWithMetadata:\n  Value: 1\n  Metadata: # describe `SameValueWithMetadata` herein\n```\n\nYou can assign Metadata to a list of primitive values using the keyword `Values` like this:\n\n```yaml\nValueListWithoutMetadata: [1,2,3]\nSameValueListWithListMetadata:\n  Values: [1,2,3]\n  Metadata: # describe the whole list of values with Metadata here\n```\n\nor specify Metadata for each (or just some) value individually, like this:\n\n```yaml\nValueListWithoutMetadata: [1,2,3]\nSameValueListWithMetadataAtEachElement:\n  - Value: 1\n    Metadata: # describe this specific value \"1\" with Metadata here\n  - Value: 2  # this value has no Metadata attached, but you can still use the keyword `Value`\n  - 3 # or use in the actual directly since this value has no Metadata anyway\n```\n\nor assign Metadata to both the list and any of its list entries, like this:\n\n```yaml\nValueListWithoutMetadata: [1,2,3]\nSameValueListWithAllMetadata:\n  Metadata: # Recommendation: place the Metadata of the list first if the list of values is extensive, as in this case\n  Values:\n    - Value: 1\n      Metadata: # describe this specific value \"1\" with Metadata here\n    - Value: 2\n      Metadata: # describe this specific value \"2\" with Metadata here\n    - Value: 3\n      Metadata: # describe this specific value \"3\" with Metadata here\n```\n\nYou can assign Metadata directly to a nested element by adding the Metadata keyword:\n\n```yaml\nNestedItemWithoutMetadata:\n  A: 1\n  B: 2\nSameNestedItemWithMetadata:\n  A: 1\n  B: 2\n  Metadata: # These Metadata describe `SameNestedItemWithMetadata`\n```\n\nSimilar to lists of values, you can assign Metadata to a list of nested elements using the `Values` keyword, like this:\n\n```yaml\nListOfNestedItemsWithoutMetadata:\n  - A: 1\n    B: 10\n  - A: 2\n    B: 20\nSameListOfNestedItemsWithGeneralMetadata:\n  Values:\n    - A: 1\n      B: 10\n    - A: 2\n      B: 20\n  Metadata: # These Metadata describe `SameListOfNestedItemsWithGeneralMetadata` as a whole\n```\n\nand, similar to nested elements, you can assign Metadata directly to any list element, like this:\n\n```yaml\nListOfNestedItemsWithoutMetadata:\n  - A: 1\n    B: 10\n  - A: 2\n    B: 20\nSameListOfNestedItemsWithGeneralMetadata:\n  - A: 1\n    B: 10\n    Metadata: # These Metadata describe the first list item\n  - A: 2\n    B: 20\n    Metadata: # These Metadata describe the second list item\n```\n\nAgain, you may apply both variants and apply Metadata to the list and each of its items if you wish.\n\n#### Contracts\nSpecifies all Contracts, i.e. repetitive bilateral transactions in between agents.\nContracts are given as a list.\nWe recommend moving Contracts to separate files and to use the `!include` command to integrate them in the scenario.\n\n```yaml\nContracts:\n  - SenderId: 1\n    ReceiverId: 2\n    ProductName: ProductOfAgent_1\n    FirstDeliveryTime: -25\n    DeliveryIntervalInSteps: 3600\n    Metadata:\n      Some: \"additional information can go here\"\n\n  - SenderId: 2\n    ReceiverId: 1\n    ProductName: ProductOfAgent_2\n    FirstDeliveryTime: -22\n    DeliveryIntervalInSteps: 3600\n    Attributes:\n      ProductAppendix: value\n      TimeOffset: 42\n```\n\nContract Parameters:\n\n* `SenderId` unique ID of agent sending the product\n* `ReceiverId` unique ID of agent receiving the product\n* `ProductName` name of the product to be sent\n* `FirstDeliveryTime` first time of delivery in the format \"seconds after the January 1st 2000, 00:00:00\"\n* `DeliveryIntervalInSteps` delay time in between deliveries in seconds\n* `Metadata` can be assigned to add further helpful information about a Contract\n* `Attributes` can be set to include additional information as `int`, `float`, `enum`, or `dict` data types\n\n##### Definition of Multiple Similar Contracts\nOften, scenarios contain multiple agents of similar type that also have similar chains of contracts.\nTherefore, FAME-Io supports a compact definition of multiple similar contracts.\n`SenderId` and `ReceiverId` can both be lists and support One-to-N, N-to-One and N-to-N relations like in the following\nexample:\n\n```yaml\nContracts:\n  # effectively 3 similar contracts (0 -> 11), (0 -> 12), (0 -> 13)\n  # with otherwise identical ProductName, FirstDeliveryTime & DeliveryIntervalInSteps\n  - SenderId: 0\n    ReceiverId: [ 11, 12, 13 ]\n    ProductName: MyOtherProduct\n    FirstDeliveryTime: 100\n    DeliveryIntervalInSteps: 3600\n\n  # effectively 3 similar contracts (1 -> 10), (2 -> 10), (3 -> 10)\n  # with otherwise identical ProductName, FirstDeliveryTime & DeliveryIntervalInSteps\n  - SenderId: [ 1, 2, 3 ]\n    ReceiverId: 10\n    ProductName: MyProduct\n    FirstDeliveryTime: 100\n    DeliveryIntervalInSteps: 3600\n\n  # effectively 3 similar contracts (1 -> 11), (2 -> 12), (3 -> 13)\n  # with otherwise identical ProductName, FirstDeliveryTime & DeliveryIntervalInSteps\n  - SenderId: [ 1, 2, 3 ]\n    ReceiverId: [ 11, 12, 13 ]\n    ProductName: MyThirdProduct\n    FirstDeliveryTime: 100\n    DeliveryIntervalInSteps: 3600\n```\n\nCombined with YAML anchors complex contract chains can be easily reduced to a minimum of required configuration.\nThe following example is equivalent to the previous one and allows a quick extension of contracts to a new couple of\nagents e.g. (4;14):\n\n```yaml\nGroups:\n  - &agentList1: [ 1,2,3 ]\n  - &agentList2: [ 11,12,13 ]\n\nContracts:\n  - SenderId: 0\n    ReceiverId: *agentList2\n    ProductName: MyOtherProduct\n    FirstDeliveryTime: 100\n    DeliveryIntervalInSteps: 3600\n\n  - SenderId: *agentList1\n    ReceiverId: 10\n    ProductName: MyProduct\n    FirstDeliveryTime: 100\n    DeliveryIntervalInSteps: 3600\n\n  - SenderId: *agentList1\n    ReceiverId: *agentList2\n    ProductName: MyThirdProduct\n    FirstDeliveryTime: 100\n    DeliveryIntervalInSteps: 3600\n```\n\n#### StringSets\n\nThis optional section defines values of type `string_set`.\nIn contrast to `enum` values, which are **statically** defined in the `Schema`, `string_set` values can be **dynamically\n** defined in this section.\nIf an agent attribute is of type `string_set` and the attribute is set in the `scenario`, then\n\n1. the section `StringSets` in the `scenario` must contain an entry named exactly like the attribute, and\n2. the attribute value must be contained in the string set's `Values` declaration.\n\nFor instance:\n\nIn `schema`:\n\n``` yaml\nAgentTypes:\n  FuelsMarket:\n    Attributes:\n      FuelType:\n        AttributeType: string_set\n```\n\nIn `scenario`:\n\n``` yaml\nStringSets:\n  FuelType:\n    Values: ['OIL', 'HARD_COAL', 'LIGNITE']\n\nAgents:\n - Type: FuelsMarket\n   Id: 1\n   Attributes:\n     FuelType: OIL\n```\n\nImportant: If different types of Agents shall refer to the same StringSet, their attributes in schema must have the\n**exact** same name.\n\n### CSV files\n\nTIME_SERIES inputs are not directly fed into the Scenario YAML file.\nInstead, TIME_SERIES reference a CSV file that can be stored some place else.\nThese CSV files follow a specific structure:\n\n* They should contain exactly two columns - any other columns are ignored.\n  A warning is raised if more than two non-empty columns are detected.\n* The first column must be a time stamp in form `YYYY-MM-DD_hh:mm:ss`\n* The second column must be a numerical value (either integer or floating-point)\n* The separator of the two columns is a semicolon\n* The data must **not** have headers, except for comments marked with `#`\n\nYou may add comments using `#`.\nExemplary content of a valid CSV file:\n\n    # If you want an optional header, you must use a comment\n    2012-01-01_00:00:00;400\n    2013-01-01_00:00:00;720.5\n    2014-01-01_00:00:00;650\n    2015-01-01_00:00:00;99.27772\n    2016-01-01_00:00:00;42  # optional comment on this particular data point\n    2017-01-01_00:00:00;0.1\n\nPlease refer also to the detailed article about `TimeStamps` in\nthe [FAME-Wiki](https://gitlab.com/fame-framework/wiki/-/wikis/TimeStamp).\n\n### Split and join multiple YAML files\n\nThe user may include other YAML files into a YAML file to divide the content across files as convenient.\nWe explicitly recommend using this feature for the `Schema` and `Contracts` sections.\nOtherwise, the scenario.yaml may become crowded.\n\n#### Command: !Include\n\nTo hint YAML to load the content of another file use `!include \"path/relative/to/including/yaml/file.yml\"`.\nYou can concatenate !include commands and can use !include in the included file as well.\nThe path to the included file is always relative to the file using the !include command.\nSo with the following file structure\n\n###### file-structure\n\n```\na.yaml\nfolder/b.yaml\nfolder/c.yaml\nfolder/deeper_folder/d.yaml\n```\n\nthe following !include commands work\n\n###### in a.yaml\n\n```\nToBe: !include \"folder/b.yaml\"\nOrNot: !include \"folder/deeper_folder/d.yaml\"\n```\n\n###### in b.yaml\n\n```\nThatIs: !include \"c.yaml\"\nTheQuestion: !include \"deeper_folder/d.yaml\"\n```\n\nProvided that\n\n###### in c.yaml\n\n```\nOr: maybe\n```\n\n###### d.yaml\n\n```\nnot: \"?\"\n```\n\nthe resulting file would look like this:\n\n###### THe Joined file a.yaml\n\n```\nToBe:\n  ThatIs:\n    Or: maybe\n  TheQuestion:\n    not: \"?\"\nOrNot:\n  not: \"?\"\n```\n\nYou may also specify absolute file paths if preferred by starting with a \"/\".\n\nWhen specifying only a file path, the complete content of the file is assigned to the given key.\nYou always need a key to assign the !include command to.\nHowever, you cannot combine the value returned from !include with other values in the same key.\nThus, the following combinations do not work:\n\n###### caveats.yml\n\n```\n!include \"file.yaml\" # no key assigned\n\nKey:\n  Some: OtherItem\n  !include \"file.yaml\" # cannot join with other named items\n\nList:\n  - an: entry\n  !include \"file.yaml\" # cannot directly join with list items, even if !include returns a list\n```\n\n#### Integrate specific nodes of YAML files\n\nInstead of including *all* content in the included file, you may also pick a specific node within that file.\nFor this use `!include [<relative/path/to/file.yaml>, Path:To:Field:In:Yaml]`.\nHere, `:` is used in the node-specifying string to select a sequence of nodes to follow - with custom depth.\nConsider the following two files:\n\n###### file_to_be_included.yaml\n\n```yaml\nSet1:\n  Subset1:\n    Key: Value\nSet2:\n  OtherKey: OtherValue\n```\n\n###### including_file.yaml\n\n```yaml\n- Type: MyAgentWithInputs\n  Id: 1\n  Attributes: !include_node [ file_to_be_included.yaml, Set1:Subset1 ]\n```\n\nCompiling \"including_file.yaml\" results in\n\n###### resulting_file.yaml\n\n```yaml\n- Type: MyAgentWithInputs\n  Id: 1\n  Attributes:\n    Key: Value\n```\n\n#### Load multiple files\n\nUsing wildcards in the given path (e.g. \"path/to/many/*.yaml\") will lead to loading multiple files and assigning their\ncontent to the same key.\nYou can make use of this feature with or without specifying a node selector.\nHowever, the elements to be joined across multiple files must be lists.\nThese lists are then concatenated into a single list and then assigned to the key in the file calling !include.\nThis feature is especially useful for Contracts: You can split the Contracts list into several files and place them in a\nseparate folder.\nThen use !include to re-integrate them into your configuration. An example:\n\n###### my_contract1.yaml\n\n```\nContracts:\n - ContractA\n - ContractB\n```\n\n###### my_contract2.yaml\n\n```\nContracts:\n - ContractC\n - ContractD\n - ContractE\n```\n\n###### including_file.yaml\n\n```\nContracts: [!include \"my_contract*.yaml\", \"Contracts\"]\n```\n\nresults in\n\n###### result.yaml\n\n```\nContracts:\n - ContractA\n - ContractB\n - ContractC\n - ContractD\n - ContractE\n```\n\n#### Ignoring files\n\nFiles that have their name start with \"IGNORE_\" are not included with the !include command.\nYou will see a debug output to notify you that the file was ignored.\nUse this to temporarily take files out ouf your configuration without deleting or moving them.\n\n## Read FAME results\n\nTakes an output file in protobuf format of FAME-based applications and converts it into files in CSV format.\nAn individual file for each type of Agent is created in a folder named after the protobuf input file.\nCall structure:\n\n    convertFameResults -f <./path/to/protobuf_file.pb>\n\nYou may also specify any of the following arguments:\n\n| Command                                       | Action                                                                                                                                                                                                |\n|-----------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `-l` or `--log` <option>                      | Sets the logging level. Default is `WARNING`. Options are `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`.                                                                                            |\n| `-lf` or `--logfile` <file>                   | Sets the logging file. Default is `None`. If `None` is provided, all logs get only printed to the console.                                                                                            |\n| `-a` or `--agents` <list-of-agents>           | If specified, only a subset of agents is extracted from the protobuf file. Default is to extract all agents.                                                                                          |\n| `-o` or `--output`                            | Sets the path to where the generated output files are written to. If not specified, the folder's name is derived from the input file's name. Folder will be created if it does not exist.             |\n| `-se` or `--single-export`                    | Enables export of individual agents to individual files, when present. If not present (the default) one file per `AgentType` is created.                                                              |\n| `-m` or `--memory-saving`                     | When specified, reduces memory usage profile at the cost of runtime. Use only when necessary.                                                                                                         |\n| `-cc` or `--complex-column` <option>          | Defines how to deal with complex indexed output columns (if any). `IGNORE` ignores complex columns. `SPLIT` creates a separate file for each complex indexed output column.                           |\n| `-t` or `--time` <option>                     | Option to define conversion of time steps to given format (default=`UTC`) by `-t/--time {UTC, INT, FAME}`                                                                                             |\n| `--input-recovery` or `--no-input-recovery`   | If True, all input data are recovered in addition to the outputs (default=False).                                                                                                                     |\n| `-mt` or `--merge-times` <list-of-parameters> | Option to merge `TimeSteps` of a certain range of steps in the output files to associate multiple time steps with a common logical time in your simulation and reduce number of lines in output files |\n\nThe option `--merge-times` requires exactly three integer arguments separated by spaces:\n\n| Position | Name         | Meaning                                                                                  |\n|----------|--------------|------------------------------------------------------------------------------------------|\n| First    | Focal point  | TimeStep on which `steps-before` earlier and `steps-after` later TimeSteps are merged on |\n| Second   | Steps before | Range of TimeSteps before the `focal-point` they get merged to, must be Zero or positive |\n| Third    | Steps after  | Range of TimeSteps after the `focal-point` they get merged to, must be Zero or positive  |\n\n\nThis could look as follows:\n\n    convertFameResults -f <./path/to/protobuf_file.pb> -l debug -lf <path/to/output.log> -a AgentType1 AgentType2 -o myCsvFolder -m -cc SPLIT --merge-times 0 1799 1800\n\nMake sure that in the range of time steps you specify for merging, there is only one value per column in the merged time range.\nIf multiple values per column are merged values will get concatenated and might yield unexpected results.\n\nYou may also call the conversion script from any Python script with:\n\n```python\nfrom fameio.scripts.convert_results import Options, run as convert_results\n\nconvert_results({Options.FILE: \"./path/to/protobuf_file.pb\"})\n```\n\nSimilar to the console call you may also specify custom run config arguments and add it in a dictionary to the function\ncall.\n\n```python\nfrom fameio.scripts.convert_results import Options, run as convert_results\n\nrun_config = {Options.FILE: \"./path/to/protobuf_file.pb\",\n              Options.LOG_LEVEL: \"info\",\n              Options.LOG_FILE: \"scenario.log\",\n              Options.OUTPUT: \"Output\",\n              Options.AGENT_LIST: ['AgentType1', 'AgentType2'],\n              Options.MEMORY_SAVING: False,\n              Options.SINGLE_AGENT_EXPORT: False,\n              Options.RESOLVE_COMPLEX_FIELD: \"SPLIT\",\n              Options.TIME: \"INT\",\n              Options.TIME_MERGING: {},\n              }\n\nconvert_results(run_config)\n```\n\nYou can also use the associated argument parser, to extract the run_config dynamically from a string:\n\n```python\nfrom fameio.scripts.convert_results import Options, run as convert_results\nfrom fameio.cli.convert_results import handle_args\n\nmy_defaults = {Options.FILE: \"./path/to/protobuf_file.pb\",\n               Options.LOG_LEVEL: \"info\",\n               Options.LOG_FILE: \"scenario.log\",\n               Options.OUTPUT: \"Output\",\n               Options.AGENT_LIST: ['AgentType1', 'AgentType2'],\n               Options.MEMORY_SAVING: False,\n               Options.SINGLE_AGENT_EXPORT: False,\n               Options.RESOLVE_COMPLEX_FIELD: \"SPLIT\",\n               Options.TIME: \"INT\",\n               Options.TIME_MERGING: {},\n               }\nmy_arg_string = ['-f', 'my/other/scenario.yaml', '-l', 'error']\n\nrun_config = handle_args(my_arg_string, my_defaults)\nconvert_results(run_config)\n```\n\n## Cite FAME-Io\n\nIf you use FAME-Io for academic work, please cite as follows.\n\nBibtex entry:\n\n```\n@article{fameio2023joss,\n  author  = {Felix Nitsch and Christoph Schimeczek and Ulrich Frey and Benjamin Fuchs},\n  title   = {FAME-Io: Configuration tools for complex agent-based simulations},\n  journal = {Journal of Open Source Software},\n  year    = {2023},\n  doi     = {doi: https://doi.org/10.21105/joss.04958}\n}\n```\n\n## Available Support\n\nThis is a purely scientific project by (at the moment) one research group.\nThus, there is no paid technical support available.\nHowever, we will give our best to answer your questions and provide support.\n\nIf you experience any trouble with FAME-Io, you may contact the developers via [fame@dlr.de](mailto:fame@dlr.de).\nPlease report bugs and make feature requests by filing issues following the provided templates (see\nalso [Contribute](CONTRIBUTING.md)).\nFor substantial enhancements, we recommend that you contact us via [fame@dlr.de](mailto:fame@dlr.de) for working\ntogether on the code in common projects or towards common publications and thus further develop FAME-Io.\n\n",
                "dependencies": "# SPDX-FileCopyrightText: 2024 German Aerospace Center <fame@dlr.de>\n#\n# SPDX-License-Identifier: Apache-2.0\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.poetry]\nname = \"fameio\"\nversion = \"3.0.0\"\ndescription = \"Tools for input preparation and output digestion of FAME models\"\nlicense = \"Apache-2.0\"\nauthors = [\n    \"Felix Nitsch <fame@dlr.de>\",\n    \"Christoph Schimeczek <fame@dlr.de>\",\n    \"Ulrich Frey <fame@dlr.de>\",\n    \"Benjamin Fuchs <fame@dlr.de>\",\n]\nmaintainers = [\n    \"Felix Nitsch <fame@dlr.de>\",\n]\nreadme = \"README.md\"\nhomepage = \"https://gitlab.com/fame-framework/wiki/-/wikis/home\"\nrepository = \"https://gitlab.com/fame-framework/fame-io/\"\nkeywords = [\"FAME\", \"fameio\", \"agent-based modelling\", \"energy systems\"]\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Operating System :: OS Independent\",\n    \"Topic :: Scientific/Engineering\",\n    \"Environment :: Console\",\n    \"Intended Audience :: Science/Research\",\n]\npackages = [{include = \"fameio\", from = \"src\"}]\ninclude = [\"CHANGELOG.md\"]\n\n[tool.poetry.dependencies]\npython = \"^3.9\"\nfameprotobuf = \"^2.0.2\"\npandas = \">= 1.0, <3.0\"\npyyaml = \"^6.0\"\n\n[tool.poetry.group.dev]\noptional = true\n\n[tool.poetry.group.dev.dependencies]\npytest = \"^8.3\"\nmockito = \"^1.5\"\npre-commit = \"^3.8\"\ncoverage = {version = \"^7.6\", extras = [\"toml\"]}\nblack = \"^24.8\"\n\n[tool.poetry.scripts]\nmakeFameRunConfig = \"fameio.scripts:makeFameRunConfig\"\nconvertFameResults = \"fameio.scripts:convertFameResults\"\n\n[tool.black]\nline-length = 120\n\n[tool.coverage.run]\nbranch = true\nsource = [\"fameio\"]\nomit = [\"./src/fameio/scripts/*\"]\ncommand_line = \"-m pytest\"\n\n[tool.coverage.report]\nshow_missing = true\nskip_covered = true\nskip_empty = true\nprecision = 2\nsort = \"Cover\"\n\n[tool.coverage.xml]\noutput = \"coverage.xml\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fastscape-toolbox",
            "repo_link": "https://github.com/fastscape-lem/fastscape",
            "content": {
                "codemeta": "",
                "readme": "Fastscape\n=========\n\n|Build Status| |Doc Status| |Zenodo|\n\nA fast, versatile and user-friendly landscape evolution model.\n\nFastscape is a Python package that provides a lot a small model\ncomponents (i.e., processes) to use with the xarray-simlab_ modeling\nframework. Those components can readily be combined together in order\nto create custom Landscape Evolution Models (LEMs).\n\nRoutines from the fastscapelib_ library are used for fast model\nexecution.\n\n.. |Build Status| image:: https://github.com/fastscape-lem/fastscape/actions/workflows/tests.yml/badge.svg?branch=master\n   :target: https://github.com/fastscape-lem/fastscape/actions/workflows/tests.yml\n   :alt: Build Status\n.. |Doc Status| image:: https://readthedocs.org/projects/fastscape/badge/?version=latest\n   :target: https://fastscape.readthedocs.io/en/latest/?badge=latest\n   :alt: Documentation Status\n.. |Zenodo| image:: https://zenodo.org/badge/133702738.svg\n   :target: https://zenodo.org/badge/latestdoi/133702738\n   :alt: Citation\n\n.. _xarray-simlab: https://github.com/benbovy/xarray-simlab\n.. _fastscapelib: https://github.com/fastscape-lem/fastscapelib-fortran\n\nDocumentation\n-------------\n\nDocumentation is hosted on ReadTheDocs:\nhttps://fastscape.readthedocs.io\n\nLicense\n-------\n\n3-clause (\"Modified\" or \"New\") BSD license. See the LICENSE file for details.\n\nAcknowledgment\n--------------\n\nFastscape is developed at the `Earth Surface Process Modelling`__ group of\nthe GFZ Helmholtz Centre Potsdam.\n\n__ http://www.gfz-potsdam.de/en/section/earth-surface-process-modelling/\n\nCiting fastscape\n----------------\n\nIf you use xarray-simlab in a scientific publication, we would\nappreciate a `citation`_.\n\n.. _`citation`: http://fastscape.readthedocs.io/en/latest/cite.html\n\n",
                "dependencies": "[build-system]\nbuild-backend = \"setuptools.build_meta\"\nrequires = [\n  \"setuptools>=42\",\n  \"setuptools-scm>=7\",\n]\n\n[tool.setuptools.packages.find]\ninclude = [\n    \"fastscape\",\n    \"fastscape.*\",\n]\n\n[tool.setuptools_scm]\nfallback_version = \"9999\"\n\n[project]\nname = \"fastscape\"\ndynamic = [\"version\"]\nauthors = [\n    {name = \"Benoît Bovy\", email = \"benbovy@gmail.com\"},\n]\nmaintainers = [\n    {name = \"Fastscape contributors\"},\n]\nlicense = {text = \"BSD-3-Clause\"}\ndescription = \"A fast, versatile and user-friendly landscape evolution model\"\nkeywords = [\"simulation\", \"toolkit\", \"modeling\", \"landscape\", \"geomorphology\"]\nreadme = \"README.rst\"\nclassifiers = [\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: BSD License\",\n    \"Programming Language :: Python :: 3\",\n]\nrequires-python = \">=3.9\"\ndependencies = [\n    \"xarray-simlab >= 0.5.0\",\n    \"numba\",\n]\n\n[project.optional-dependencies]\ndev = [\"pytest\"]\n\n[project.urls]\nDocumentation = \"https://fastscape.readthedocs.io\"\nRepository = \"https://github.com/fastscape-lem/fastscape\"\n\n[tool.black]\nline-length = 100\n\n[tool.ruff]\n# E402: module level import not at top of file\n# E501: line too long - let black worry about that\n# E731: do not assign a lambda expression, use a def\nignore = [\n  \"E402\",\n  \"E501\",\n  \"E731\",\n]\nselect = [\n  \"F\", # Pyflakes\n  \"E\", # Pycodestyle\n  \"W\",\n  \"I\", # isort\n  \"UP\", # Pyupgrade\n]\nexclude = [\".eggs\", \"doc\"]\ntarget-version = \"py39\"\n\n[tool.ruff.isort]\nknown-first-party = [\"fastscape\"]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fastsurfer",
            "repo_link": "https://github.com/deep-MI/FastSurfer/",
            "content": {
                "codemeta": "",
                "readme": "[![DOI](https://zenodo.org/badge/211859022.svg)](https://zenodo.org/badge/latestdoi/211859022)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Deep-MI/FastSurfer/blob/stable/Tutorial/Tutorial_FastSurferCNN_QuickSeg.ipynb)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Deep-MI/FastSurfer/blob/stable/Tutorial/Complete_FastSurfer_Tutorial.ipynb)\n\n<!-- start of content -->\n# Welcome to FastSurfer!\n##  Overview\n\nThis README contains all information needed to run FastSurfer - a fast and accurate deep-learning based neuroimaging pipeline. FastSurfer provides a fully compatible [FreeSurfer](https://freesurfer.net/) alternative for volumetric analysis (within minutes) and surface-based thickness analysis (within only around 1h run time). \nFastSurfer is transitioning to sub-millimeter resolution support throughout the pipeline.\n\nThe FastSurfer pipeline consists of two main parts for segmentation and surface reconstruction.  \n\n- the segmentation sub-pipeline (`seg`) employs advanced deep learning networks for fast, accurate segmentation and volumetric calculation of the whole brain and selected substructures.\n- the surface sub-pipeline (`recon-surf`) reconstructs cortical surfaces, maps cortical labels and performs a traditional point-wise and ROI thickness analysis. \n\n\n### Segmentation Modules \n- approximately 5 minutes (GPU), `--seg_only` only runs this part. \n \nModules (all run by default):\n1. `asegdkt:` [FastSurferVINN](FastSurferCNN/README.md) for whole brain segmentation (deactivate with `--no_asegdkt`)\n   - the core, outputs anatomical segmentation and cortical parcellation and statistics of 95 classes, mimics FreeSurfer’s DKTatlas.\n   - requires a T1w image ([notes on input images](#requirements-to-input-images)), supports high-res (up to 0.7mm, experimental beyond that).\n   - performs bias-field correction and calculates volume statistics corrected for partial volume effects (skipped if `--no_biasfield` is passed).\n2. `cereb:` [CerebNet](CerebNet/README.md) for cerebellum sub-segmentation (deactivate with `--no_cereb`)\n   - requires `asegdkt_segfile`, outputs cerebellar sub-segmentation with detailed WM/GM delineation.\n   - requires a T1w image ([notes on input images](#requirements-to-input-images)), which will be resampled to 1mm isotropic images (no native high-res support).\n   - calculates volume statistics corrected for partial volume effects (skipped if `--no_biasfield` is passed).\n3. `hypothal`: [HypVINN](HypVINN/README.md) for hypothalamus subsegmentation (deactivate with `--no_hypothal`)\n   - outputs a hypothalamic subsegmentation including 3rd ventricle, c. mammilare, fornix and optic tracts.\n   - a T1w image is highly recommended ([notes on input images](#requirements-to-input-images)), supports high-res (up to 0.7mm, but experimental beyond that).\n   - allows the additional passing of a T2w image with `--t2 <path>`, which will be registered to the T1w image (see `--reg_mode` option).\n   - calculates volume statistics corrected for partial volume effects based on the T1w image (skipped if `--no_bias_field` is passed).\n\n### Surface reconstruction\n- approximately 60-90 minutes, `--surf_only` runs only [the surface part](recon_surf/README.md).\n- supports high-resolution images (up to 0.7mm, experimental beyond that).\n\n<!-- start of image requirements -->\n### Requirements to input images\nAll pipeline parts and modules require good quality MRI images, preferably from a 3T MR scanner.\nFastSurfer expects a similar image quality as FreeSurfer, so what works with FreeSurfer should also work with FastSurfer. \nNotwithstanding module-specific limitations, resolution should be between 1mm and 0.7mm isotropic (slice thickness should not exceed 1.5mm). Preferred sequence is Siemens MPRAGE or multi-echo MPRAGE. GE SPGR should also work. See `--vox_size` flag for high-res behaviour.\n<!-- end of image requirements -->\n\n![](doc/images/teaser.png)\n\n<!-- start of getting started -->\n## Getting started\n\n### Installation \nThere are two ways to run FastSurfer (links are to installation instructions):\n\n1. In a container ([Singularity](doc/overview/INSTALL.md#singularity) or [Docker](doc/overview/INSTALL.md#docker)) (OS: [Linux](doc/overview/INSTALL.md#linux), [Windows](doc/overview/INSTALL.md#windows), [MacOS on Intel](doc/overview/INSTALL.md#docker-currently-only-supported-for-intel-cpus)),\n2. As a [native install](doc/overview/INSTALL.md#native-ubuntu-2004-or-ubuntu-2204) (all OS for segmentation part). \n\nWe recommended you use Singularity or Docker on a Linux host system with a GPU. The images we provide on [DockerHub](https://hub.docker.com/r/deepmi/fastsurfer) conveniently include everything needed for FastSurfer. You will also need a [FreeSurfer license](https://surfer.nmr.mgh.harvard.edu/fswiki/License) file for the [Surface pipeline](#surface-reconstruction). We have detailed per-OS Installation instructions in the [INSTALL.md](doc/overview/INSTALL.md) file.\n\n### Usage\n\nAll installation methods use the `run_fastsurfer.sh` call interface (replace `*fastsurfer-flags*` with [FastSurfer flags](doc/overview/FLAGS.md#required-arguments)), which is the general starting point for FastSurfer. However, there are different ways to call this script depending on the installation, which we explain here:\n\n1. For container installations, you need to define the hardware and mount the folders with the input (`/data`) and output data (`/output`):  \n   (a) For __singularity__, the syntax is \n    ```\n    singularity exec --nv \\\n                     --no-home \\\n                     -B /home/user/my_mri_data:/data \\\n                     -B /home/user/my_fastsurfer_analysis:/output \\\n                     -B /home/user/my_fs_license_dir:/fs_license \\\n                     ./fastsurfer-gpu.sif \\\n                     /fastsurfer/run_fastsurfer.sh \n                     *fastsurfer-flags*\n   ```\n   The `--nv` flag is needed to allow FastSurfer to run on the GPU (otherwise FastSurfer will run on the CPU).\n\n   The `--no-home` flag tells singularity to not mount the home directory (see [Singularity documentation](Singularity/README.md#mounting-home) for more info).\n\n   The `-B` flag is used to tell singularity, which folders FastSurfer can read and write to.\n \n   See also __[Example 2](doc/overview/EXAMPLES.md#example-2-fastsurfer-singularity)__ for a full singularity FastSurfer run command and [the Singularity documentation](Singularity/README.md#fastsurfer-singularity-image-usage) for details on more singularity flags.  \n\n   (b) For __docker__, the syntax is\n    ```\n    docker run --gpus all \\\n               -v /home/user/my_mri_data:/data \\\n               -v /home/user/my_fastsurfer_analysis:/output \\\n               -v /home/user/my_fs_license_dir:/fs_license \\\n               --rm --user $(id -u):$(id -g) \\\n               deepmi/fastsurfer:latest \\\n               *fastsurfer-flags*\n    ```\n   The `--gpus` flag is needed to allow FastSurfer to run on the GPU (otherwise FastSurfer will run on the CPU).\n\n   The `-v` flag is used to tell docker, which folders FastSurfer can read and write to.\n \n   See also __[Example 1](doc/overview/EXAMPLES.md#example-1-fastsurfer-docker)__ for a full FastSurfer run inside a Docker container and [the Docker documentation](Docker/README.md#docker-flags) for more details on the docker flags including `--rm` and `--user`.\n\n2. For a __native install__, you need to activate your FastSurfer environment (e.g. `conda activate fastsurfer_gpu`) and make sure you have added the FastSurfer path to your `PYTHONPATH` variable, e.g. `export PYTHONPATH=$(pwd)`. \n\n   You will then be able to run fastsurfer with `./run_fastsurfer.sh *fastsurfer-flags*`.\n\n   See also [Example 3](doc/overview/EXAMPLES.md#example-3-native-fastsurfer-on-subjectx-with-parallel-processing-of-hemis) for an illustration of the commands to run the entire FastSurfer pipeline (FastSurferCNN + recon-surf) natively.\n\n<!-- start of flags -->\n### FastSurfer_Flags\nPlease refer to [FASTSURFER_FLAGS](doc/overview/FLAGS.md).\n\n\n## Examples\nAll the examples can be found here: [FASTSURFER_EXAMPLES](doc/overview/EXAMPLES.md)\n- [Example 1: FastSurfer Docker](doc/overview/EXAMPLES.md#example-1-fastsurfer-docker)\n- [Example 2: FastSurfer Singularity](doc/overview/EXAMPLES.md#example-2-fastsurfer-singularity)\n- [Example 3: Native FastSurfer on subjectX with parallel processing of hemis](doc/overview/EXAMPLES.md#example-3-native-fastsurfer-on-subjectx-with-parallel-processing-of-hemis)\n- [Example 4: FastSurfer on multiple subjects](doc/overview/EXAMPLES.md#example-4-fastsurfer-on-multiple-subjects)\n- [Example 5: Quick Segmentation](doc/overview/EXAMPLES.md#example-5-quick-segmentation)\n- [Example 6: Running FastSurfer on a SLURM cluster via Singularity](doc/overview/EXAMPLES.md#example-6-running-fastsurfer-on-a-slurm-cluster-via-singularity)\n\n\n## Output files\n\nModules output can be found here: [FastSurfer_Output_Files](doc/overview/OUTPUT_FILES.md)\n- [Segmentation module](doc/overview/OUTPUT_FILES.md#segmentation-module)\n- [Cerebnet module](doc/overview/OUTPUT_FILES.md#cerebnet-module)\n- [Surface module](doc/overview/OUTPUT_FILES.md#surface-module)\n\n<!-- start of system requirements -->\n## System Requirements\n\nRecommendation: At least 8 GB system memory and 8 GB NVIDIA graphics memory ``--viewagg_device gpu``  \n\nMinimum: 7 GB system memory and 2 GB graphics memory ``--viewagg_device cpu --vox_size 1``\n\nMinimum CPU-only: 8 GB system memory (much slower, not recommended) ``--device cpu --vox_size 1`` \n\n### Minimum Requirements:\n\n|       | --viewagg_device | Min GPU (in GB) | Min CPU (in GB) |\n|:------|------------------|----------------:|----------------:|\n| 1mm   | gpu              |               5 |               5 |\n| 1mm   | cpu              |               2 |               7 |\n| 0.8mm | gpu              |               8 |               6 |\n| 0.8mm | cpu              |               3 |               9 |\n| 0.7mm | gpu              |               8 |               6 |\n| 0.7mm | cpu              |               3 |               9 |\n\n\n## Expert usage\nIndividual modules and the surface pipeline can be run independently of the full pipeline script documented in this documentation. \nThis is documented in READMEs in subfolders, for example: [whole brain segmentation only with FastSurferVINN](FastSurferCNN/README.md), [cerebellum sub-segmentation](CerebNet/README.md), [hypothalamic sub-segmentation](HypVINN/README.md) and [surface pipeline only (recon-surf)](recon_surf/README.md).\n\nSpecifically, the segmentation modules feature options for optimized parallelization of batch processing.\n\n\n## FreeSurfer Downstream Modules\n\nFreeSurfer provides several Add-on modules for downstream processing, such as subfield segmentation ( [hippocampus/amygdala](https://surfer.nmr.mgh.harvard.edu/fswiki/HippocampalSubfieldsAndNucleiOfAmygdala), [brainstem](https://surfer.nmr.mgh.harvard.edu/fswiki/BrainstemSubstructures), [thalamus](https://freesurfer.net/fswiki/ThalamicNuclei) and [hypothalamus](https://surfer.nmr.mgh.harvard.edu/fswiki/HypothalamicSubunits) ) as well as [TRACULA](https://surfer.nmr.mgh.harvard.edu/fswiki/Tracula). We now provide symlinks to the required files, as FastSurfer creates them with a different name (e.g. using \"mapped\" or \"DKT\" to make clear that these file are from our segmentation using the DKT Atlas protocol, and mapped to the surface). Most subfield segmentations require `wmparc.mgz` and work very well with FastSurfer,  so feel free to run those pipelines after FastSurfer. TRACULA requires `aparc+aseg.mgz` which we now link, but have not tested if it works, given that [DKT-atlas](https://mindboggle.readthedocs.io/en/latest/labels.html) merged a few labels. You should source FreeSurfer 7.3.2 to run these modules. \n\n\n## Intended Use\n\nThis software can be used to compute statistics from an MR image for research purposes. Estimates can be used to aggregate population data, compare groups etc. The data should not be used for clinical decision support in individual cases and, therefore, does not benefit the individual patient. Be aware that for a single image, produced results may be unreliable (e.g. due to head motion, imaging artefacts, processing errors etc). We always recommend to perform visual quality checks on your data, as also your MR-sequence may differ from the ones that we tested. No contributor shall be liable to any damages, see also our software [LICENSE](LICENSE). \n\n<!-- start of references -->\n## References\n\nIf you use this for research publications, please cite:\n\n_Henschel L, Conjeti S, Estrada S, Diers K, Fischl B, Reuter M, FastSurfer - A fast and accurate deep learning based neuroimaging pipeline, NeuroImage 219 (2020), 117012. https://doi.org/10.1016/j.neuroimage.2020.117012_\n\n_Henschel L*, Kuegler D*, Reuter M. (*co-first). FastSurferVINN: Building Resolution-Independence into Deep Learning Segmentation Methods - A Solution for HighRes Brain MRI. NeuroImage 251 (2022), 118933. http://dx.doi.org/10.1016/j.neuroimage.2022.118933_\n\n_Faber J*, Kuegler D*, Bahrami E*, et al. (*co-first). CerebNet: A fast and reliable deep-learning pipeline for detailed cerebellum sub-segmentation. NeuroImage 264 (2022), 119703. https://doi.org/10.1016/j.neuroimage.2022.119703_\n\n_Estrada S, Kuegler D, Bahrami E, Xu P, Mousa D, Breteler MMB, Aziz NA, Reuter M. FastSurfer-HypVINN: Automated sub-segmentation of the hypothalamus and adjacent structures on high-resolutional brain MRI. Imaging Neuroscience 2023; 1 1–32. https://doi.org/10.1162/imag_a_00034_\n\nStay tuned for updates and follow us on [X/Twitter](https://twitter.com/deepmilab).\n\n<!-- start of acknowledgements -->\n## Acknowledgements\n\nThis project is partially funded by:\n- [Chan Zuckerberg Initiative](https://chanzuckerberg.com/eoss/proposals/fastsurfer-ai-based-neuroimage-analysis-package/)\n- [German Federal Ministry of Education and Research](https://www.gesundheitsforschung-bmbf.de/de/deepni-innovative-deep-learning-methoden-fur-die-rechnergestutzte-neuro-bildgebung-10897.php)\n\nThe recon-surf pipeline is largely based on [FreeSurfer](https://surfer.nmr.mgh.harvard.edu/fswiki/FreeSurferMethodsCitation).\n\n",
                "dependencies": "[build-system]\nrequires = ['setuptools >= 61.0.0']\nbuild-backend = 'setuptools.build_meta'\n\n[project]\nname = 'fastsurfer'\nversion = '2.4.0-dev'\ndescription = 'A fast and accurate deep-learning based neuroimaging pipeline'\nreadme = 'README.md'\nlicense = {file = 'LICENSE'}\nrequires-python = '>=3.10'\nauthors = [{name = 'Martin Reuter et al.'}]\nmaintainers = [{name = 'FastSurfer Developers'}]\nkeywords = [\n    'python',\n    'Deep learning',\n    'Segmentation',\n    'Brain segmentation',\n    'Brain analysis',\n    'volumetry',\n]\nclassifiers = [\n    'Operating System :: Microsoft :: Windows',\n    'Operating System :: Unix',\n    'Operating System :: MacOS',\n    'Programming Language :: Python :: 3 :: Only',\n    'Programming Language :: Python :: 3.10',\n    'Programming Language :: Python :: 3.11',\n    'Programming Language :: Python :: 3.12',\n    'Natural Language :: English',\n    'License :: OSI Approved :: Apache Software License',\n    'Intended Audience :: Science/Research',\n]\ndependencies = [\n    'h5py>=3.7',\n    'lapy>=1.1.0',\n    'matplotlib>=3.7.1',\n    'nibabel>=5.1.0',\n    'numpy>=1.25,<2',\n    'pandas>=1.5.3',\n    'pyyaml>=6.0',\n    'requests>=2.31.0',\n    'scikit-image>=0.19.3',\n    'scikit-learn>=1.2.2',\n    'scipy>=1.10.1,!=1.13.0',\n    'simpleitk>=2.2.1',\n    'tensorboard>=2.12.1',\n    'torch>=2.0.1',\n    'torchio>=0.18.83',\n    'torchvision>=0.15.2',\n    'tqdm>=4.65',\n    'yacs>=0.1.8',\n]\n\n[project.optional-dependencies]\ndoc = [\n    'furo!=2023.8.17',\n    'matplotlib',\n    'memory-profiler',\n    'myst-parser',\n    'numpydoc',\n    # sphinx 8 handles importing of files differently in some manner and will cause the\n    # build of the doc to fail. This will need to be addressed before we up to sphinx 8.\n    'sphinx>=7.3,<8',\n    'sphinxcontrib-bibtex',\n    'sphinxcontrib-programoutput',\n    'sphinx-argparse',\n    'sphinx-copybutton',\n    'sphinx-design',\n    'sphinx-gallery',\n    'sphinx-issues',\n    'pypandoc',\n    'nbsphinx',\n    'IPython', # For syntax highlighting in notebooks\n    'ipykernel',\n    'scikit-image',\n    'torchvision',\n    'scikit-learn',\n]\nstyle = [\n    'bibclean',\n    'codespell',\n    'pydocstyle[toml]',\n    'ruff',\n]\nquicktest = [\n    'pytest>=8.2.2',\n]\nall = [\n    'fastsurfer[doc]',\n    'fastsurfer[style]',\n    'fastsurfer[quicktest]',\n]\nfull = [\n    'fastsurfer[all]',\n]\n\n[project.urls]\nhomepage = 'https://fastsurfer.org'\ndocumentation = 'https://fastsurfer.org'\nsource = 'https://github.com/Deep-MI/FastSurfer'\ntracker = 'https://github.com/Deep-MI/FastSurfer/issues'\n\n[tool.setuptools]\npackages = ['FastSurferCNN','CerebNet','recon_surf']\n\n[tool.pydocstyle]\nconvention = 'numpy'\nignore-decorators = '(copy_doc|property|.*setter|.*getter|pyqtSlot|Slot)'\nmatch = '^(?!setup|__init__|test_).*\\.py'\nmatch-dir = '^FastSurferCNN.*,^CerebNet.*,^recon-surf.*'\nadd_ignore = 'D100,D104,D107'\n\n[tool.ruff]\nline-length = 120\ntarget-version = \"py310\"\nextend-exclude = [\n    \"build\",\n    \"checkpoints\",\n    \"doc\",\n    \"env\",\n]\n\n[tool.ruff.lint]\n# https://docs.astral.sh/ruff/linter/#rule-selection\nselect = [\n    \"E\",   # pycodestyle\n    \"F\",   # Pyflakes\n    \"UP\",  # pyupgrade\n    \"B\",   # flake8-bugbear\n    \"I\",   # isort\n    # \"SIM\", # flake8-simplify\n]\n\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\"F401\"]\n\"Tutorial/*.ipynb\" = [\"E501\"]  # exclude \"Line too long\"\n\"FastSurferCNN/utils/logging.py\" = [\"F401\"]  # exclude \"Imported but unused\"\n\"FastSurferCNN/utils/parser_defaults.py\" = [\"F401\"]  # exclude \"Imported but unused\"\n\n[tool.pytest.ini_options]\nminversion = '6.0'\naddopts = '--durations 20 --junit-xml=junit-results.xml --verbose'\nfilterwarnings = []\n\n[tool.coverage.run]\nbranch = true\ncover_pylib = false\nomit = [\n    '**/__init__.py',\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    'pragma: no cover',\n    'if __name__ == .__main__.:',\n]\nprecision = 2\n\n\n#\n# This file is autogenerated by kueglerd from deepmi/fastsurfer:cuda-v2.3.3\n# by the following command from FastSurfer:\n#\n#    ./requirements.txt deepmi/fastsurfer:cuda-v2.3.3\n#\n# Which ran the following command:\n#    docker run --rm -u <user_id>:<group_id> --entrypoint /bin/bash deepmi/fastsurfer:cuda-v2.3.3 -c 'python --version && pip list --format=freeze --no-color --disable-pip-version-check --no-input'\n#\n#\n# Image was configured for cu124 using python version 3.10.15\n#\n--extra-index-url https://download.pytorch.org/whl/cu124\n\n# Python 3.10.15\nabsl-py==2.1.0\nBrotli==1.1.0\ncached-property==1.5.2\ncertifi==2024.8.30\ncffi==1.17.1\ncharset-normalizer==3.3.2\nclick==8.1.7\ncolorama==0.4.6\ncontourpy==1.3.0\ncycler==0.12.1\nDeprecated==1.2.14\nfilelock==3.16.1\nfonttools==4.54.1\nfsspec==2024.9.0\ngrpcio==1.65.5\nh2==4.1.0\nh5py==3.11.0\nhpack==4.0.0\nhumanize==4.11.0\nhyperframe==6.0.1\nidna==3.10\nimagecodecs==2024.6.1\nimageio==2.35.1\nimportlib_metadata==8.5.0\nimportlib_resources==6.4.5\nJinja2==3.1.4\njoblib==1.4.2\nkiwisolver==1.4.7\nlapy==1.1.0\nlazy_loader==0.4\nMarkdown==3.6\nmarkdown-it-py==3.0.0\nMarkupSafe==2.1.5\nmatplotlib==3.9.2\nmdurl==0.1.2\nmpmath==1.3.0\nmunkres==1.1.4\nnetworkx==3.3\nnibabel==5.2.1\nnumpy==1.26.4\nnvidia-cublas-cu12==12.4.2.65\nnvidia-cuda-cupti-cu12==12.4.99\nnvidia-cuda-nvrtc-cu12==12.4.99\nnvidia-cuda-runtime-cu12==12.4.99\nnvidia-cudnn-cu12==9.1.0.70\nnvidia-cufft-cu12==11.2.0.44\nnvidia-curand-cu12==10.3.5.119\nnvidia-cusolver-cu12==11.6.0.99\nnvidia-cusparse-cu12==12.3.0.142\nnvidia-nccl-cu12==2.20.5\nnvidia-nvjitlink-cu12==12.4.99\nnvidia-nvtx-cu12==12.4.99\npackaging==24.1\npandas==2.2.2\npillow==10.4.0\npip==24.2\nplotly==5.24.1\nprotobuf==5.27.5\npsutil==6.0.0\npycparser==2.22\nPygments==2.18.0\npyparsing==3.1.4\nPySide6==6.7.3\nPySocks==1.7.1\npython-dateutil==2.9.0\npytz==2024.2\nPyWavelets==1.7.0\nPyYAML==6.0.2\nrequests==2.32.3\nrich==13.9.2\nscikit-image==0.24.0\nscikit-learn==1.5.1\nscikit-sparse==0.4.14\nscipy==1.14.1\nsetuptools==72.2.0\nshellingham==1.5.4\nshiboken6==6.7.3\nSimpleITK==2.4.0\nsix==1.16.0\nsympy==1.13.3\ntenacity==9.0.0\ntensorboard==2.17.1\ntensorboard-data-server==0.7.0\nthreadpoolctl==3.5.0\ntifffile==2024.9.20\ntorch==2.4.0+cu124\ntorchio==0.19.9\ntorchvision==0.19.0+cu124\ntornado==6.4.1\ntqdm==4.66.5\ntriton==3.0.0\ntyper==0.12.5\ntyping_extensions==4.12.2\ntzdata==2024.2\nunicodedata2==15.1.0\nurllib3==2.2.3\nWerkzeug==3.0.6\nwheel==0.44.0\nwrapt==1.16.0\nyacs==0.1.8\nzipp==3.20.2\nzstandard==0.23.0\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fatsegnet",
            "repo_link": "https://github.com/Deep-MI/FatSegNet",
            "content": {
                "codemeta": "",
                "readme": "\n# FatSegNet : A Fully Automated Deep Learning Pipeline for Adipose Segmentation on Abdominal Dixon MRI\n\nThis repository contains the tool  designed for the [Rhineland Study](https://www.rheinland-studie.de/) \nfor segmenting visceral and subcuteneous adipose tissue on fat \nimages from a two-point Dixon sequence. \n\nIf you use this tool please cite:\n\nEstrada, Santiago, et al. \"FatSegNet: A fully automated deep learning pipeline for adipose tissue segmentation on abdominal dixon MRI.\" Magnetic resonance in medicine 83.4 (2020): 1471-1483. [https:// doi.org/10.1002/mrm.28022](https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.28022)\n```\n@article{estrada2020fatsegnet,\n  title={FatSegNet: A fully automated deep learning pipeline for adipose tissue segmentation on abdominal dixon MRI},\n  author={Estrada, Santiago and Lu, Ran and Conjeti, Sailesh and Orozco-Ruiz, Ximena and Panos-Willuhn, Joana and Breteler, Monique MB and Reuter, Martin},\n  journal={Magnetic resonance in medicine},\n  volume={83},\n  number={4},\n  pages={1471--1483},\n  year={2020},\n  publisher={Wiley Online Library}\n}\n```\n\n## Usage\n\nWe wrap our tool on a docker image, so there is no need to install any library dependencies or drivers, \nthe only requirement is to have docker (cpu) or \nnvidia-docker(gpu) installed.\n\nPrerequisites:\n\n* Docker (For running on CPU) (https://docs.docker.com/install/)\n* NVIDIA-Docker (For running on GPU ) (https://github.com/nvidia/nvidia-docker/wiki)\n\n\n\n## Tool installation \n\n If the tool is run for the first time the FatSegNet docker image has to be created. Run the following steps\n \n 1. Run on the terminal `sudo git clone https://github.com/reuter-lab/FatSegNet.git`  or download .zip file from the github repository \n 2. From the download repository directory run on the terminal: \n\n* `bash build_docker_cpu.sh` for CPU (In case GPU is not available)<br/> \n* `bash build_docker_gpu.sh` for GPU <br/>\n\nFor checking that the FatSegNet image was created correctly type on the terminal<br/>\n`docker images`\n\nit should appear a repository with the name **adipose_tool** and the tag v1 for gpu or cpu_v1 for cpu.\n\n**Example** \n``` bash\nREPOSITORY        TAG       IMAGE ID      CREATED     SIZE\nadipose_tool      v1        xxxxxxxx      xxxxxx      xxxx\nadipose_tool      cpu_v1    xxxxxxxx      xxxxxx      xxxx    \n```\n\n**Note:** Both docker images for CPU and GPU can be created on the same machine. \n\n \n## Running the tool \n\n### **Input Data format**\nFor running the tool the input data is expected to be a nifti volume with size of [256,224,72], if the scans have a \ndifferent size they will be crop or padd to the correct size. Additionally \nthe scans have to be arrange as follows(or see [example_data_folder](./example_data_folder), **NOTE** :This folder \ncontain a ilustrative example of how images have to organized for FatSegNet to work.\nThe Fat and water images scans are empty) :\n\n ```\n #Input  Scheme                            \n|-- my_dataset                                                             \n    participants.csv                         \n    |-- Subject_1                                \n        |-- FatImaging_F.nii.gz                      \n        |-- FatImaging_W.nii.gz                                                                  \n    |-- Subject_2                                            \n        |-- FatImaging_F.nii.gz                                         \n        |-- FatImaging_W.nii.gz              \n    |-- Subject_3                            \n        |-- FatImaging_F.nii.gz                  \n        |-- FatImaging_W.nii.gz                      \n    ...........                                     \n    |-- Subject_xx                                    \n        |-- FatImaging_F.nii.gz                      \n        |-- FatImaging_W.nii.gz\n ``` \nThe fat and water scans should have the same name, the name can be defined by the user,  the default names are \n**FatImaging_F.nii.gz (Fat)** and **FatImaging_W.nii.gz(water)**.\n\n**Participants file (participants.csv)** : the purpose of this file is to configure the participants scans \nthat should be process. The file has a one compulsory column  that consist of the name of folder containing \nthe water and fat scans.\n \n`participants.csv` example : \n\n```\nSubject_1\nSubject_2\nSubject_3\nSubject_xx\n```\n\n \n### Running FatSegNet\n\nFor executing FatSegNet  is necesary to configure the docker run options and the script input arguments \nas follows :<br/>\n```\n#For gpu\nnvidia-docker run [OPTIONS] adipose_tool:v1 [ARGUMENTS]\n#For Cpu\ndocker run [OPTIONS] adipose_tool:v1 [ARGUMENTS]\n```\n\n#### Options\nA docker container doesnt have access to the system files so volumes has to be mounted. For our tool \nis necessary to mount  the main data directory `my_dataset` to `/tool/Data` and the desire  local output\n folder to `/tool/Output`. The output folder \nis where all pipeline output are going to be store (the input and output folder can be the same). We additionally recommend to use the following docker flags:<br/>\n * `--rm` : Automatically clean up the container and remove the file system when the container exits\n * `--user , -u `: Username or UID (format: <name|uid>[:<group|gid>])\n * `--name` : Assign a name to the container\n * `--volume , -v`: Bind mount a volume\n \n\n**Example** \n``` bash\n#For Gpu\nnvidia-docker run --rm --name fatsegnet -u $(id -u) -v ../my_dataset/:/tool/Data -v ../my_dataset_output/:/tool/Output  adipose_tool:v1 [Arguments]\n# For CPU\ndocker run -it --rm --name fatsegnet -u $(id -u) -v ../my_dataset/:/tool/Data -v ../my_dataset_output/:/tool/Output  adipose_tool:cpu_v1 [Arguments]\n```\n\n #### Arguments\n * `--file,-f` : csv file containing the participants to process (default: participants.csv)\n * `--output_folder,-outp` : Parent folder for the scripts outputs (see output seccion) \n * `--fat_image,-fat` : Name of the fat image (default :FatImaging_F.nii.gz)\n * `--water_image,-water`: Name of the water image (default :FatImaging_W.nii.gz)\n * `--control_images,-No_QC` : Not to plot subjects predictions for visual quality control\n * `--run_localization,-loc` : run abdominal region localization model , by default the localization model is not run\n * `--axial,-axial` : run only axial segmentation model\n * `--order,-order` : Interpolation order (0=nearest,1=linear(default),2=quadratic,3=cubic),the tool standardizes the input resolutions to [2mm,2mm,5mm]; if the axial flag is selected only the axial plane is sample.  \n * `--compartments,-comp` : Number of equal compartments to calculate the statistics (default=0.0)\n * `--increase_threshold,-AAT` : Warning flag for an increase in AAT over the define threhold between consecutive scans (default=0.4)\n * `--sat_to_vat_threshold,-ratio`: Warning flag for a vat to sat ratio higher than the define threshold (default=2.0)\n * `--runs_stats,-stats` : the AAT segmentations model are not deploy only image biomarkers are calculated,a fat scan and VAT and SAT segmentation map  is required (AAT_pred.nii.gz)\n * `--gpu_id, -gpu_id` :  GPU device ID, the container will only use the specified Gpu  (default `device ID 0`). ***Note*** the script organize the GPU IDs by pci bus IDs.\n \n \n**Example**\n```\n# Run paper implementation \nnvidia-docker run --rm --name fatsegnet -u $(id -u) -v ../my_dataset/:/tool/Data -v ../my_dataset_output/:/tool/Output  adipose_tool:v1 -loc\n\n# Change Participants files \nnvidia-docker run [Options]  adipose_tool:v1 -f new_participants.csv -loc\n\n# Change name of water and fat images to search\nnvidia-docker run [Options]  adipose_tool:v1  -fat fat_image.nii.gz -water water_image.nii.gz -loc\n\n# Select a specific GPU (ex: device ID 2)\nnvidia-docker run [Options]  adipose_tool:v1  -loc -gpu_id 2\n\n# run only the segmentation models on the axial plane and define interpolation order\nnvidia-docker run [Options]  adipose_tool:v1  -axial -order 3\n\n```\n\n### **Output Data format**\n```  bash\n#Output Scheme \n|-- my_dataset_output                                   \n    |-- Subject_1\n        |-- MRI (Only created if the images are resize or sample)\n           |-- FatImaging_F.nii.gz (Fat_Scans)\n           |-- FatImaging_W.nii.gz (Water_Scans)\n        |-- QC\n           |-- QC_[0-3].png (Quality control images)\n        |-- Segmentations                                                 \n           |-- AAT_pred.nii.gz (Only adipose tissues prediction map)\n           |-- ALL_pred.nii.gz (adipose tissues and auxilary classes prediction map)         \n           |-- AAT_variables_summary.json  (Calculated Image Biomarkers) \n    |-- Subject_2\n        |-- MRI (Only created if the images are resize or sample)\n           |-- FatImaging_F.nii.gz (Fat_Scans)\n           |-- FatImaging_W.nii.gz (Water_Scans)\n        |-- QC\n           |-- QC_[0-3].png (Quality control images)\n        |-- Segmentations                                                 \n           |-- AAT_pred.nii.gz (Only adipose tissues prediction map)\n           |-- ALL_pred.nii.gz (adipose tissues and auxilary classes prediction map)          \n           |-- AAT_variables_summary.json  (Calculated Image Biomarkers)                      \n    ...............\n    |-- Subject_xx\n        |-- MRI (Only created if the images are resize or sample)\n           |-- FatImaging_F.nii.gz (Fat_Scans)\n           |-- FatImaging_W.nii.gz (Water_Scans)\n        |-- QC\n           |-- QC_[0-3].png (Quality control images)\n        |-- Segmentations    \n           |-- AAT_pred.nii.gz (Only adipose tissues prediction map)\n           |-- ALL_pred.nii.gz (adipose tissues and auxilary classes prediction map)    \n           |-- AAT_variables_summary.json  (Calculated Image Biomarkers)\n\n ``` \n\n**Image Biomarkers**\n\nFor more information on the pipeline image biomarkers reported in the `AAT_variables_summary.json ` file \nplease check the document [FatSegNet_Variables.pdf](./FatSegNet_Variables.pdf)\n\n**Quality Control Image Example**\n\nBy default the tool creates 4 images for visually control of the input scan and predicted segmentation, as the one shown below.\nTop row fat images from axial, coronal ,sagittal view centered on the red dot; bottom row predicted segmentations \n(blue: SAT, green : VAT).    \n\n![](Images/QC_3.png)\n\n\n\n\n--------\nFor any questions and feedback, feel free to contact santiago.estrada(at).dzne.de<br/>\n\n--------\n\n\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fesom",
            "repo_link": "https://github.com/FESOM/fesom2",
            "content": {
                "codemeta": "",
                "readme": "The Finite Element Sea Ice-Ocean Model (FESOM2) \n======\n[![Build Status](https://github.com/FESOM/fesom2/workflows/FESOM2%20main%20test/badge.svg)](https://github.com/FESOM/fesom2/actions)\n\nMulti-resolution ocean general circulation model that solves the equations of motion describing the ocean and sea ice using finite-element and finite-volume methods on unstructured computational grids. The model is developed and supported by researchers at the Alfred Wegener Institute, Helmholtz Centre for Polar and Marine Research (AWI), in Bremerhaven, Germany.\n\n**Website:** [fesom.de](https://fesom.de/)\n\n**Documentation:** [fesom2.readthedocs.io](https://fesom2.readthedocs.io/en/latest/index.html)\n\n**Basic tutorial:** [Getting started](https://fesom2.readthedocs.io/en/latest/getting_started/getting_started.html)\n\n\nReferences\n----------\n\n[Complete list of references on fesom.de](https://fesom.de/publications/)\n\n* **[Ocean model formulation]** Danilov, S., Sidorenko, D., Wang, Q., and Jung, T.: The Finite-volumE Sea ice–Ocean Model (FESOM2), Geosci. Model Dev., 10, 765–789, https://doi.org/10.5194/gmd-10-765-2017, 2017. \n\n* **[Sea ice model formulation]** Danilov, S., Q. Wang, R. Timmermann, N. Iakovlev, D. Sidorenko, M. Kimmritz, T. Jung, and Schröter, J. (2015), Finite-Element Sea Ice Model (FESIM), version 2, Geosci. Model Dev., 8, 1747–1761, http://www.geosci-model-dev.net/8/1747/2015/\n\n* **[Evaluation of standard sumulations]** Scholz, P., Sidorenko, D., Gurses, O., Danilov, S., Koldunov, N., Wang, Q., Sein, D., Smolentseva, M., Rakowsky, N., and Jung, T.: Assessment of the Finite-volumE Sea ice-Ocean Model (FESOM2.0) – Part 1: Description of selected key model elements and comparison to its predecessor version, Geosci. Model Dev., 12, 4875–4899, https://doi.org/10.5194/gmd-12-4875-2019, 2019.\n\n* **[Evaluation of computational performance]** Koldunov, N. V., Aizinger, V., Rakowsky, N., Scholz, P., Sidorenko, D., Danilov, S., and Jung, T.: Scalability and some optimization of the Finite-volumE Sea ice–Ocean Model, Version 2.0 (FESOM2), Geosci. Model Dev., 12, 3991–4012, https://doi.org/10.5194/gmd-12-3991-2019, 2019. \n\n* **[Version coupled with ECHAM6 atmosphere]** Sidorenko, D., Goessling, H. F., Koldunov, N. V., Scholz, P., Danilov, S., Barbi, D., et al ( 2019). Evaluation of FESOM2.0 coupled to ECHAM6.3: Pre‐industrial and HighResMIP simulations. Journal of Advances in Modeling Earth Systems, 11. https://doi.org/10.1029/2019MS001696\n\n* **[Version with ICEPACK sea ice thermodynamics]** Zampieri, Lorenzo, Frank Kauker, Jörg Fröhle, Hiroshi Sumata, Elizabeth C. Hunke, and Helge Goessling. Impact of Sea-Ice Model Complexity on the Performance of an Unstructured-Mesh Sea-ice/ocean Model Under Different Atmospheric Forcings. Washington: American Geophysical Union, 2020. https://dx.doi.org/10.1002/essoar.10505308.1.\n\n* **[Version coupled with OpenIFS atmosphere]** Streffing, J., Sidorenko, D., Semmler, T., Zampieri, L., Scholz, P., Andrés-Martínez, M., et al ( 2022). AWI-CM3 coupled climate model: description and evaluation experiments for a prototype post-CMIP6 model. Geoscientific Model Development, 15. https://doi.org/10.5194/gmd-15-6399-2022\n\n",
                "dependencies": "cmake_minimum_required(VERSION 3.16)\n\n# set default build type cache entry (do so before project(...) is called, which would create this cache entry on its own)\nif(NOT CMAKE_BUILD_TYPE)\n  message(STATUS \"setting default build type: Release\")\n  set(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"Choose the type of build, options are: None(CMAKE_CXX_FLAGS or CMAKE_C_FLAGS used) Debug Release RelWithDebInfo MinSizeRel.\")\nendif()\n\nproject(FESOM2.0)\n\nset(BUILD_SHARED_LIBS ON CACHE BOOL \"Default to using shared libs\")\nset(TOPLEVEL_DIR ${CMAKE_CURRENT_LIST_DIR})\nset(FESOM_COUPLED OFF CACHE BOOL \"compile fesom standalone or with oasis support (i.e. coupled)\")\nset(OIFS_COUPLED OFF CACHE BOOL \"compile fesom coupled to OpenIFS. (Also needs FESOM_COUPLED to work)\")\nset(CRAY OFF CACHE BOOL \"compile with cray ftn\")\nset(USE_ICEPACK OFF CACHE BOOL \"compile fesom with the Iceapck modules for sea ice column physics.\")\nset(OPENMP_REPRODUCIBLE OFF CACHE BOOL \"serialize OpenMP loops that are critical for reproducible results\")\nset(RECOM_COUPLED OFF CACHE BOOL \"compile fesom including biogeochemistry, REcoM3\")\nset(CISO_COUPLED OFF CACHE BOOL \"compile ciso coupled to REcoM3. RECOM_COUPLED has to be active\")\nset(USE_MULTIO OFF CACHE BOOL \"Use MULTIO for IO, either grib or binary for now. This also means path to MULTIO installation has to provided using env MULTIO_INSTALL_PATH='..' and multio configuration yamls must be present to run the model with MULTIO\")\nset(OASIS_WITH_YAC OFF CACHE BOOL \"Useing a version of OASIS compiled with YAC instead of SCRIP for interpolation?\")\nset(ASYNC_ICEBERGS ON CACHE BOOL \"compile fesom with or without support for asynchronous iceberg computations\")\nset(VERBOSE OFF CACHE BOOL \"toggle debug output\")\n#add_subdirectory(oasis3-mct/lib/psmile)\nadd_subdirectory(src)\n\nforeach( _file fesom-config.cmake fesom-config-version.cmake fesom-targets.cmake )\n  execute_process( COMMAND ${CMAKE_COMMAND} -E create_symlink ${PROJECT_BINARY_DIR}/src/${_file} ${PROJECT_BINARY_DIR}/${_file}  )\nendforeach()\n\n# Define ${PROJECT_NAME}_DIR in PARENT_SCOPE so that a `find_package( <this-project> )` in a bundle\n# will easily find the project without requiring a `HINT <this-project>_BINARY_DIR` argument\nif( NOT CMAKE_SOURCE_DIR STREQUAL PROJECT_SOURCE_DIR )\n    # Guard needed because PARENT_SCOPE cannot be used in top-level CMake project\n\n    set( fesom_DIR ${fesom_DIR} PARENT_SCOPE )\nendif()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fishinspector",
            "repo_link": "https://github.com/sscholz-UFZ/FishInspector",
            "content": {
                "codemeta": "",
                "readme": "# FishInspector\n**Annotation of features from zebrafish embryos**\n\nThe software FishInspector allows annotation of features in images of zebrafish embryos. The recent version requires images of a lateral position. It is important that the position is precise since deviation may confound with feature annotations. Images from any source can be used. However, depending on the image properties parameters may have to be adjusted. Furthermore, images obtained with normal microscope and not using an automated position system with embryos in glass capillaries require conversion using a KNIME workflow (available [here](https://github.com/eteixido/Knime-workflows-FishInspector)). As a result of the analysis the software provides JSON files that contain the coordinates of the features. Coordinates are provided for eye, fish contour, notochord , otoliths, yolk sac, pericard and swimbladder. Furthermore, pigment cells in the notochord area are detected. Additional features can be manually annotated. It is the aim of the software to provide the coordinates, which may then be analysed subsequently to identify and quantify changes in the morphology of zebrafish embryos.\n\n## [Available for Download Here](https://github.com//sscholz-UFZ/FishInspector/releases)\n\n## User Guide\n\nThe complete user guide can be checked [here](https://github.com/sscholz-UFZ/FishInspector/blob/master/docs/Index.md)\n\n## Referencing\n\nCitations can be made using the following DOI:\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1422642.svg)](https://doi.org/10.5281/zenodo.1422642)\n\n*Teixido, E., Kießling, T.R., Krupp, E., Quevedo, C., Muriana, A., Scholz, S., 2018. Automated morphological feature assessment for zebrafish embryo developmental toxicity screens. Tox. Sci. accepted.*\n\n## License\n\nThis project is licensed under a GNU General Public License - see the [LICENSE](LICENSE) file for details and also this [file](License.txt). \n\n\n\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fiware-deployment-kit",
            "repo_link": "https://jugit.fz-juelich.de/iek-10/public/ict-platform/deployment",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/fleur",
            "repo_link": "https://iffgit.fz-juelich.de/fleur/fleur",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/formatfuzzer",
            "repo_link": "https://github.com/uds-se/FormatFuzzer",
            "content": {
                "codemeta": "",
                "readme": "README.md\n# FormatFuzzer\n\n`FormatFuzzer` is a framework for *high-efficiency, high-quality generation and parsing of binary inputs.*\nIt takes a *binary template* that describes the format of a binary input and generates an *executable* that produces and parses the given binary format.\nFrom a binary template for GIF, for instance, `FormatFuzzer` produces a GIF generator - also known as *GIF fuzzer*.\n\nGenerators produced by `FormatFuzzer` are highly efficient, producing thousands of valid test inputs per second - in sharp contrast to mutation-based fuzzers, where the large majority of inputs is invalid. Inputs generated by `FormatFuzzer` are independent from the program under test (or actually, any program), so you can also use them in black-box settings. However, `FormatFuzzer` also integrates with AFL++ to produce valid inputs that also aim for maximum coverage. In our experiments, this \"best of two worlds\" approach surpasses all other settings; see [our paper](https://arxiv.org/abs/2109.11277) for details.\n\nThe binary templates used by FormatFuzzer come from the [010 editor](https://www.sweetscape.com/010editor/).\nThere are more than [170 binary templates](https://www.sweetscape.com/010editor/templates.html), which either can be used directly for `FormatFuzzer` or adapted for its use. Out of the box, `FormatFuzzer` produces formats such as AVI, BMP, GIF, JPG, MIDI, MP3, MP4, PCAP, PNG, WAV, and ZIP; and we keep on extending this list every week.\n\nContributors are welcome! Visit the [FormatFuzzer project page](https://github.com/uds-se/FormatFuzzer) for filing ideas and issues, or adding pull requests. For details on how `FormatFuzzer` works and how it compares, read [our paper](https://arxiv.org/abs/2109.11277) for more info.\n\n\n## Getting\n\nFormatFuzzer is available from the [FormatFuzzer project page](https://github.com/uds-se/FormatFuzzer). You can download and unpack the latest release from [the releases page](https://github.com/uds-se/FormatFuzzer/releases).\n\nFor the very latest and greatest, you can also clone its git repository:\n```\ngit clone https://github.com/uds-se/FormatFuzzer.git\n```\nAll further actions take place in its main folder:\n```\ncd FormatFuzzer\n```\n\n## Prerequisites\n\nTo run FormatFuzzer, you need the following:\n* Python 3\n* A C++ compiler with GNU libraries (notably `getopt_long()`) such as `clang` or `gcc`\n* The Python packages `py010parser`, `six`, and `intervaltree`\n* A `zlib` library (for compression functions)\n* A `boost` library (for checksum functions)\n\nIf you plan to edit the build and configuration scripts (`.ac` and `.am` files), you will also need\n* GNU autoconf\n* GNU automake\n\n### Installing Requirements on Linux (Debian Packages)\n\n```\nsudo apt install git g++ make automake python3-pip zlib1g-dev libboost1.71-dev\npip3 install py010parser six intervaltree\n```\n\n### Installing Requirements on MacOS (with Xcode & Homebrew)\n\n```\nxcode-select --install\nbrew install python3 automake boost\npip3 install py010parser six intervaltree\n```\n\n### Installing Python Packages Only (All Operating Systems)\n\nOn all systems, using `pip`:\n```\npip install py010parser\npip install six\npip install intervaltree\n```\n\n\n## Building\n\nNote: all building commands require you to be in the same folder as this `README` file. Building a fuzzer outside of this folder is not yet supported.\n\n### Method 1: Using the build.sh script\n\nThere's a `build.sh` script which automates all construction steps.\nSimply run\n```\n./build.sh gif\n```\nto create a GIF fuzzer.\n\nThis works for all file formats provided in `templates/`; if there is a file `templates/FOO.bt`, then `./build.sh FOO` will build a fuzzer.\n\n\n### Method 2: Using Make\n\nThere's a `Makefile` (source in `Makefile.am`) which automates all construction steps.\n(Requires `GNU make`.)\nFirst do\n```\ntouch configure Makefile.in\n```\nthen\n```\n./configure\n```\nand then\n```\nmake gif-fuzzer\n```\nto create a GIF fuzzer.\n\nThis works for all file formats provided in `templates/`; if there is a file `templates/FOO.bt`, then `make FOO-fuzzer` will build a fuzzer.\n\n\n### Method 3: Manual steps\n\nIf the above `make` method does not work, or if you want more control, you may have to proceed manually.\n\n#### Step 1: Compiling Binary Template Files into C++ code\n\nRun the `ffcompile` compiler to compile the binary template into C++ code. It takes two arguments: the `.bt` binary template, and a `.cpp` C++ file to be generated.\n```\n./ffcompile templates/gif.bt gif.cpp\n```\n\n\n#### Step 2: Compiling the C++ code\n\nUse the following commands to create a fuzzer `gif-fuzzer`.\nFirst, compile the generic command-line driver:\n\n```\ng++ -c -I . -std=c++17 -g -O3 -Wall fuzzer.cpp\n```\n(`-I .` denotes the location of the `bt.h` file; `-std=c++17` sets the C++ standard.)\n\nThen, compile the binary parser/compiler:\n\n```\ng++ -c -I . -std=c++17 -g -O3 -Wall gif.cpp\n```\n\nFinally, link the binary parser/compiler with the command-line driver to obtain an executable. If you use any extra libraries (such as `-lz`), be sure to specify these here too.\n```\ng++ -O3 gif.o fuzzer.o -o gif-fuzzer -lz\n```\n\n\n## Running the Fuzzer\n\nFormatFuzzer can be run as a standalone parser, generator or mutator of specific formats.\nIn addition, it can called by general-purpose fuzzers such as [AFL++](https://github.com/uds-se/AFLplusplus) to integrate those format-specific capabilities into the fuzzing process (see the section below on AFL++ integration).\n\nThe generated fuzzer takes a _command_ as first argument, followed by options and arguments to that command.\n\nThe most important command is `fuzz`, for producing outputs.  Its arguments are files to be generated in the appropriate format.\n\nRun the generator as\n```\n./gif-fuzzer fuzz output.gif\n```\nto create a random binary file `output.gif`, or\n```\n./gif-fuzzer fuzz out1.gif out2.gif out3.gif\n```\nto create three GIF files `out1.gif`, `out2.gif`, and `out3.gif`.\n\nNote that the `gif.bt` template we provide has been augmented with special functions to make generation of valid files easier. If you use an original `.bt` template files without adaptations, you may get warnings during generation and create invalid files.\n\n\n## Running Parsers\n\nYou can also run the fuzzer as a _parser_ for binary files, using the `parse` command. This is useful if you want to test the accuracy of the binary template, or if you want to mutate an input (see `Decision Files', below).\n\nTo run the parser, use\n```\n./gif-fuzzer parse input.gif\n```\nYou will see error messages if `input.gif` cannot be successfully parsed.\n\n\n## Decision Files\n\nWhile parsing, you can also store all parsing decisions (i.e. which parsing alternatives were taken) in a _decision file_. This is a sequence of bytes enumerating the decisions taken.\nEach byte stands for a single parsing decision. A byte value of `0` means that the first alternative was taken, a byte value of `1` means that the second alternative was taken, and so on.\n\nYou can generate such a decision file when parsing an input:\n```\n./gif-fuzzer parse --decisions input.dec input.gif\n```\nHere, `input.dec` stores the decisions made for parsing `input.gif'.\n\nYou can also use such a decision file when _generating_ inputs. The fuzzer will then take the exact same decisions as found during parsing. The following command generates a new GIF file using the decisions determined while parsing `input.gif':\n```\n./gif-fuzzer fuzz --decisions input.dec input2.gif\n```\nIf everything works well, both files should be identical:\n```\ncmp input.gif input2.gif\n```\nBy _mutating_ a decision file (e.g. replacing individual bytes), you can create inputs that are _similar_ to the original file parsed. This is useful for interfacing with specific testing strategies and fuzzers such as AFL, where you can use `gif-fuzzer` and the like as _translators_ from decision files to binary files and back: AFL would mutate decision files, and the program under test would run on the translated binary files. In contrast to mutating binary files directly (as AFL would normally do), this would have the advantage of always having valid inputs - and thus progressing much faster towards coverage.\n\n\n## AFL++ Integration\n\nIn addition to the format-specific fuzzers, such as `gif-fuzzer`, FormatFuzzer can also be compiled into format-specific shared libraries, such as `gif.so` (for that, simply run `./build.sh gif` or `make gif.so`).\nThose shared libraries can be loaded by general-purpose fuzzers, such as [AFL++](https://github.com/AFLplusplus/AFLplusplus).\n\nTo run AFL++ with FormatFuzzer, just follow the instructions on [our modified version of AFL++](https://github.com/uds-se/AFLplusplus).\nWe support different fuzzing strategies, including:\n\n  * AFL+FFMut: runs AFL++ using FormatFuzzer to provide format-specific smart mutations.\n\n  * AFL+FFGen: uses FormatFuzzer as a format-specific generator, while AFL++ mutates its decision seeds.\n\n\n## Creating and Customizing Binary Templates\n\nTo write your own `.bt` binary templates (and thus create a high-efficiency fuzzer/parser for this format), read the section [Introduction to Templates and Scripts](https://www.sweetscape.com/010editor/manual/IntroTempScripts.htm) from the [010 Editor Manual](https://www.sweetscape.com/010editor/manual/).\n\nIn many cases, a template of the format you are looking for (or a similar one) may already exist. Have a look at the 010 editor [binary template collection](https://www.sweetscape.com/010editor/templates.html) whether there is something that you can use or base your format on.\n\nNote that the `.bt` files provided in the repository generally target _parsing_ files. They _can_ be used for _generating_ files, too; but they often lack exact information which parts of the input are required.\n\nIn this section, we discuss some of the ways in which you can customize `.bt` files to work well with `FormatFuzzer`.\n\nFor example, for the GIF format, the file [templates/gif-orig.bt](templates/gif-orig.bt) shows the original binary template, which was only designed for parsing, while the file [templates/gif.bt](templates/gif.bt) is a modified version which is capable of generating valid GIFs. Comparing the two files, we see that a small number changes was required to achieve this.\n\nIf you have created a `gif-fuzzer`, either by running `make gif-fuzzer` or by using the `ffcompile` tool, you have already obtained a C++ file `gif.cpp` which contains an implementation of the GIF generator and parser. This is useful to see how the changes you make to the binary template are translated into executable code. More details on the C++ code are presented on the next section.\n\nThe GIF binary template makes use of _lookahead_ functions `ReadUByte()` and `ReadUShort()` to look ahead at the values of the next bytes in the file before actually parsing them into a struct field. At generation time, we allow those functions to receive an additional argument specifying a set of good known values to pick for the bytes that we look ahead. In addition, we also allow specifying a global set of good known values to always use when calling a particular lookahead function, such as `ReadUByte()`. Those are stored in the `ReadUByteInitValues` vector.\n\nBy default, our translation procedure `ffcompile` tries to mine interesting values which have been used in comparisons against lookahead bytes and use them as a global set of known values. When running\n```\n./ffcompile templates/gif.bt gif.cpp\n```\na printed message shows the lookahead functions identified, as well as the mined interesting values:\n```\nFinished creating cpp generator.\n\nLookahead functions found:\n\nReadUByte\nReadUShort\n\nMined interesting values:\n\nGlobalColorTableFlag: ['1']\nLocalColorTableFlag: ['1']\nReadUByte: ['0x3B', '0x2C']\nReadUShort: ['0xF921', '0xFE21', '0x0121', '0xFF21']\nSignature: ['\"GIF\"']\n```\n\nFor GIF generation, however, it is better to specify the set of good known values for `ReadUByte()` individually at each call to the function. So we define an empty array (size 0)\n```\nconst local UBYTE ReadUByteInitValues[0];\n```\nto overwrite the set of global `ReadUByteInitValues` and for each call to `ReadUByte()`, we use an additional argument to specify the set of good values to use for that particular location.\nThe binary template language is also powerful enough to allow this choice to be made based on runtime conditions. For example, in the following code we show how the choice of appropriate values for a `ReadUByte()` call can depend on the current GIF version we are generating. A GIF version `89a` allows one extra possible value for the byte (0x21).\n```\n\tif(GifHeader.Version == \"89a\")\n\t\tlocal UBYTE values[] = { 0x3B, 0x2C, 0x21 };\n\telse\n\t\tlocal UBYTE values[] = { 0x3B, 0x2C };\n\n\twhile (ReadUByte(FTell(), values) != 0x3B) {\n\t\t...\n\t}\n```\n\nThe remaining edits required for the GIF binary template are similar. For example, for each struct field can also specify a set of known good values. For example this specifies the correct values for the `Version` field: `87a` and `89a`.\n```\n\tchar\tVersion[3] = { {\"87a\"}, {\"89a\"} };\n```\n\n\n## Understanding the Generated C++ Code\n\nFor debugging purposes, as well as for understanding how to make appropriate changes to improve your generators and parsers, it may be useful to understand some inner workings of the generated C++ code.\nIdeally, you should be able to edit the binary template files until they can be used to generate valid files with high probability, so you wouldn't have to edit the generated C++ code.\n\nThe C++ code creates a class for each `struct` and `union` defined in the binary template, as well as for native types, such as `int`.\n\nAt construction time, when initializing a variable, we can define a set of good known values that this variable can assume. For example, the constructor call\n```\nchar_array_class cname(cname_element, { \"IHDR\", \"tEXt\", \"PLTE\", \"cHRM\", \"sRGB\", \"iEXt\", \"zEXt\", \"tIME\", \"pHYs\", \"bKGD\", \"sBIT\", \"sPLT\", \"acTL\", \"fcTL\", \"fdAT\", \"IHDR\", \"IEND\" });\n```\nwould specify 17 good values to use for variable `cname`. But this is often not enough, since the choice of appropriate chunk types is context sensitive.\nSo we also allow specifying a set of good values at generation time when generating a new chunk.\nFor example, this call could be used to generate an instance of `chunk` for the first chunk, which must have type IHDR.\n```\nGENERATE(chunk, ::g->chunk.generate({ \"IHDR\" }, false));\n```\nWhen generating the second chunk, we might use this long list of possible chunks that can come between the IHDR chunk and the PLTE chunk:\n```\nGENERATE(chunk, ::g->chunk.generate({ \"iCCP\", \"sRGB\", \"sBIT\", \"gAMA\", \"cHRM\", \"pHYs\", \"sPLT\", \"tIME\", \"zTXt\", \"tEXt\", \"iTXt\", \"eXIf\", \"oFFs\", \"pCAL\", \"sCAL\", \"acTL\", \"fcTL\", \"fdAT\", \"fRAc\", \"gIFg\", \"gIFt\", \"gIFx\", \"sTER\" }, true));\n```\nThe generator will then uniformly pick one of the good known values to use for the new instance. We also allow the choice of an evil value which is not one of the good known values with small probability 1/128.\nThis feature can be enabled or disabled any time by using the method `set_evil_bit`.\n\nAll the random choices taken by the generator are done by calling the `rand_int()` method.\n```\nlong long rand_int(unsigned long long x, std::function<long long (unsigned char*)> parse);\n```\nWhen running the program as a generator, this method samples an integer from 0 to x-1 by reading bytes from the random buffer.\nWhen running the program as a parser, this method uses the `parse()` function to find out which random bytes must be present in the random buffer in order to generate the target file, and then writes those bytes to the random buffer.\nThe `parse` function receives as an argument the buffer at the current position of the file and must then return which value would have to be returned by the current call to `rand_int()` in order to generate this exact file configuration.\n\n\n## Authors\n\nFormatFuzzer was designed and written by Rafael Dutra &lt;rafael.dutra@cispa.de&gt;.\n\nThe concept of a fuzzer compiler was introduced by Rahul Gopinath &lt;rahul.gopinath@cispa.de&gt; and Andreas Zeller &lt;zeller@cispa.de&gt;.\n\n\n\n## Copyright and Licenses\n\nFormatFuzzer is Copyright &copy; 2020, 2021 by [CISPA Helmholtz Center for Information Security](https://cispa.de/). The following licenses apply:\n\n* _The FormatFuzzer code_ (notably, all C++ code and code related to its generation) is subject to the GNU GENERAL PUBLIC LICENSE, as found in [COPYING](COPYING).\n\n* As an exception to the above, _C++ code generated by FormatFuzzer_ (i.e., fuzzers and parsers for specific formats) is in the public domain.\n\n* _The original_ [pfp](https://github.com/d0c-s4vage/pfp) _code_, which FormatFuzzer is based upon, is subject to an MIT license, as found in [LICENSE-pfp](LICENSE-pfp).\n\n",
                "dependencies": "py010parser>=0.1.17\nsix>=1.10.0,<2.0.0\nintervaltree>=3.0.2,<4.0.0\n\n#!/usr/bin/env python\n# encoding: utf-8\n\nimport os, sys\nfrom setuptools import setup\n\nsetup(\n    # metadata\n    name=\"pfp\",\n    description=\"An 010 template interpreter for Python\",\n    long_description=\"\"\"\n        pfp is an 010 template interpreter for Python. It accepts an\n        input data stream and an 010 template and returns a modifiable\n        DOM of the parsed data. Extensions have also been added to the\n        010 template syntax to allow for linked fields (e.g. checksums,\n        length calculations, etc), sub structures in compressed data,\n        etc.\n    \"\"\",\n    license=\"MIT\",\n    version=\"{{VERSION}}\",\n    author=\"James Johnson\",\n    maintainer=\"James Johnson\",\n    author_email=\"d0c.s4vage@gmail.com\",\n    url=\"https://github.com/d0c-s4vage/pfp\",\n    platforms=\"Cross Platform\",\n    download_url=\"https://github.com/d0c-s4vage/pfp/tarball/v{{VERSION}}\",\n    install_requires=open(\n        os.path.join(os.path.dirname(__file__), \"requirements.txt\")\n    )\n    .read()\n    .split(\"\\n\"),\n    classifiers=[\n        \"Programming Language :: Python :: 2\",\n        \"Programming Language :: Python :: 3\",\n    ],\n    entry_points={\n        \"console_scripts\": [\"pfp = pfp.__main__:main\"]\n    },\n    packages=[\"pfp\", \"pfp.native\", \"pfp.fuzz\"],\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fracspy",
            "repo_link": "https://github.com/FraCSPy/FraCSPy",
            "content": {
                "codemeta": "",
                "readme": "# FraCSPy\n\n[![DOI](https://zenodo.org/badge/619447827.svg)](https://zenodo.org/badge/latestdoi/619447827)\n\nFraCSPy stands for Python Framework for Conventional microSeismic Processing.\n\n![FraCSPy logo](logo/fracspy_logo.png)\n\nIt is a single python toolbox for the full microseismic pipeline from modelling to post-event analysis.\nThis library is a single location leveraging the excellent work of other scientists (software developers) and adapts them for the specific use case of microseismic monitoring.\n\nSome functionalities include:\n\n- modelling script generation (for accompanying [SOFI3D](https://docs.csc.fi/apps/sofi3d/))\n- event imaging: detection, location\n- moment tensor inversion\n\nSome python libraries that are utilised include:\n\n- pylops\n- torch\n- obspy\n- and more...\n\n## Requirements\n\nInstallation requires either [pip](https://pypi.org/project/pip/) package installer or [Conda](https://conda.io) package manager, e.g. one can use [miniforge](https://github.com/conda-forge/miniforge).\n\n## Install using pip\n\n```bash\npip install fracspy\n```\n\n## Install using conda\n\n### Linux\n\nSimply run\n\n```bash\nmake install\n```\n\nIt will create a new conda environment `fracspy` with all the required packages:\n\nSimilarly, on Linux you can run:\n\n```bash\n./install.sh\n```\n\n### Windows\n\nOn Windows, the best way is to use [miniforge](https://github.com/conda-forge/miniforge) prompt and run:\n\n```cmd\ninstall.bat\n```\n\nIt will install the package to environment `fracspy` and activate it.\n\nTo install development version use\n\n```cmd\ninstall-dev.bat\n```\n\nNow you are ready to use the package.\n\n### Uninstall\n\nIf you need to add/change packages:\n\n```bash\nconda deactivate\nconda remove -n fracspy -all\n```\n\n## Documentation\n\nThe latest stable documentation based on [Sphinx](https://www.sphinx-doc.org) is available online at: <https://fracspy.github.io/FraCSPy>\n\nOne can also build the documentation locally:\n\n```bash\ncd docs\nmake html\n```\n\nIf you want to rebuild the documentation:\n\n```bash\nmake clean\nmake html\n```\n\nAfter a successful build, one can serve the documentation website locally:\n\n```bash\ncd build/html\npython -m http.server\n```\n\nand open in browser: <http://localhost:8000>\n\nTo build/rebuild documentation on Windows you can simply run\n\n```cmd\nbuild_docs.bat\n```\n\n**Note:** check the exact port number in the output\n\n",
                "dependencies": "#!/bin/bash\n# \n# Installer for fracspy\n# \n# Run: ./install.sh\n# \n# C. Birnie, 13/04/2023\n# Updated by D. Anikiev 30/05/2024\n\nENV_YAML=environment.yml\nENV_NAME=fracspy\nPACKAGE_NAME=fracspy\n\necho 'Creating $(ENV_NAME) environment'\n\n# create conda env\nconda env create -f $ENV_YAML\nsource $CONDA_PREFIX/etc/profile.d/conda.sh\nconda activate $ENV_NAME\npip install -e .\nconda env list\necho 'Created and activated environment $(ENV_YAML):' $(which python)\n\n# Check\necho 'Checking $(PACKAGE_NAME) version...'\npython -c 'import fracspy as fp; print(fp.__version__)'\n\necho 'Done!'\n\n\n\n[build-system]\nrequires = [\n    \"setuptools >= 65\",\n    \"setuptools_scm[toml]\",\n    \"wheel\",\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"fracspy\"\ndescription = \"A python package for general microseismic modelling, monitoring and analysis\"\nreadme = \"README.md\"\nauthors = [\n    {name = \"Claire Emma Birnie\", email = \"claire.birnie@kaust.edu.sa\"},\n    {name = \"Denis Anikiev\", email = \"denis.anikiev@gfz-potsdam.de\"},\n    {name = \"Omar Sad Aly\", email = \"omar.sadaly@kaust.edu.sa\"},\n    {name = \"Matteo Ravasi\", email = \"matteo.ravasi@kaust.edu.sa\"},\n]\nlicense = {file = \"LICENSE.md\"}\nkeywords = [\"geophysics\", \"signal processing\", \"microseismic\"]\nclassifiers = [\n    \"Development Status :: 1 - Planning\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n    \"Intended Audience :: Education\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Natural Language :: English\",\n    \"Operating System :: OS Independent\",\n    \"Programming Language :: Python :: 3 :: Only\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n]\ndependencies = [\n    \"numpy >= 1.15.0\",\n    \"scipy >= 1.8.0\",\n    \"matplotlib\",\n    \"pylops >= 2.0.0\",\n    \"torch\",\n    \"cmcrameri\",\n    \"openpyxl\",\n    \"tqdm\",\n    \"obspy\",\n]\ndynamic = [\"version\"]\n\n[project.optional-dependencies]\ntest = [\n    \"pytest\",\n    \"pytest-cov\",\n]\n\n[tool.setuptools.packages.find]\nexclude = [\"pytests\"]\n\n[tool.setuptools_scm]\nversion_file = \"fracspy/version.py\"\n\n.\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fsqc",
            "repo_link": "https://github.com/Deep-MI/fsqc",
            "content": {
                "codemeta": "",
                "readme": "# fsqc toolbox\n\n## Description\n\nThis package provides quality assurance / quality control scripts for FastSurfer- or\nFreeSurfer-processed structural MRI data. It will check outputs of these two software \npackages by means of quantitative and visual summaries. Prior processing of data using \neither FastSurfer or FreeSurfer is required, i.e. the software cannot be used on raw images.\n\nIt is a revision, extension, and translation to the Python language of the\n[Freesurfer QA Tools](https://surfer.nmr.mgh.harvard.edu/fswiki/QATools). It has\nbeen augmented by additional functions from the [MRIQC toolbox](https://github.com/poldracklab/mriqc),\nand with code derived from the [LaPy](https://github.com/Deep-MI/lapy) and\n[BrainPrint](https://github.com/Deep-MI/brainprint) toolboxes.\n\nThis page provides general, usage, and installation information. See [here](https://deep-mi.org/fsqc/dev/index.html)\nfor the full documentation.\n___\n\n## Contents\n\n- [Functionality](#functionality)\n- [Development](#development)\n  - [News](#news)\n  - [Main and development branches](#main-and-development-branches)\n  - [Roadmap](#roadmap)\n- [Usage](#usage)\n  - [As a command line tool](#as-a-command-line-tool)\n  - [As a Python package](#as-a-python-package)\n  - [As a Docker image](#as-a-docker-image)\n- [Installation](#installation)\n   - [Installation as a Python package](#installation-as-a-python-package)\n   - [Installation from GitHub](#installation-from-github)\n   - [Download from GitHub](#download-from-github)\n- [Requirements](#requirements)\n- [Known issues](#known-issues)\n- [Authors](#authors)\n- [Citations](#citations)\n- [License](#license)\n\n___\n\n## Functionality\n\nThe core functionality of this toolbox is to compute the following features:\n\nvariable       |   description\n---------------|----------------------------------------------------------------\nsubject        |   subject ID\nwm_snr_orig    |   signal-to-noise ratio for white matter in orig.mgz\ngm_snr_orig    |   signal-to-noise ratio for gray matter in orig.mgz\nwm_snr_norm    |   signal-to-noise ratio for white matter in norm.mgz\ngm_snr_norm    |   signal-to-noise ratio for gray matter in norm.mgz\ncc_size        |   relative size of the corpus callosum\nlh_holes       |   number of holes in the left hemisphere\nrh_holes       |   number of holes in the right hemisphere\nlh_defects     |   number of defects in the left hemisphere\nrh_defects     |   number of defects in the right hemisphere\ntopo_lh        |   topological fixing time for the left hemisphere\ntopo_rh        |   topological fixing time for the right hemisphere\ncon_lh_snr     |   wm/gm contrast signal-to-noise ratio in the left hemisphere\ncon_rh_snr     |   wm/gm contrast signal-to-noise ratio in the right hemisphere\nrot_tal_x      |   rotation component of the Talairach transform around the x axis\nrot_tal_y      |   rotation component of the Talairach transform around the y axis\nrot_tal_z      |   rotation component of the Talairach transform around the z axis\n\nThe program will use an existing output directory (or try to create it) and\nwrite a csv table into that location. The csv table will contain the above\nmetrics plus a subject identifier.\n\nThe program can also be run on images that were processed with [FastSurfer](https://github.com/Deep-MI/FastSurfer)\n(v1.1 or later) instead of FreeSurfer. In that case, simply add a `--fastsurfer`\nswitch to your shell command. Note that FastSurfer's full processing stream must\nhave been run, including surface reconstruction (i.e. brain segmentation alone\nis not sufficient).\n\nIn addition to the core functionality of the toolbox there are several optional\nmodules that can be run according to need:\n\n- screenshots module\n\nThis module allows for the automated generation of cross-sections of the brain\nthat are overlaid with the anatomical segmentations (asegs) and the white and\npial surfaces. These images will be saved to the 'screenshots' subdirectory\nthat will be created within the output directory. These images can be used for\nquickly glimpsing through the processing results. Note that no display manager\nis required for this module, i.e. it can be run on a remote server, for example.\n\n- surfaces module\n\nThis module allows for the automated generation of surface renderings of the\nleft and right pial and inflated surfaces, overlaid with the aparc annotation.\nThese images will be saved to the 'surfaces' subdirectory that will be created\nwithin the output directory. These images can be used for quickly glimpsing\nthrough the processing results. Note that no display manager is required for\nthis module, i.e. it can be run on a remote server, for example.\n\n- skullstrip module\n\nThis module allows for the automated generation cross-sections of the brain\nthat are overlaid with the colored and semi-transparent brainmask. This allows\nto check the quality of the skullstripping in FreeSurfer. The resulting images\nwill be saved to the 'skullstrip' subdirectory that will be created within the\noutput directory.\n\n- fornix module\n\nThis is a module to assess potential issues with the segmentation of the\ncorpus callosum, which may incorrectly include parts of the fornix. To assess\nsegmentation quality, a screenshot of the contours of the corpus callosum\nsegmentation overlaid on the norm.mgz will be saved as 'cc.png' for each\nsubject within the 'fornix' subdirectory of the output directory.\n\n- modules for the amygdala, hippocampus, and hypothalamus\n\nThese modules evaluate potential missegmentations of the amygdala, hippocampus,\nand hypothalamus. To assess segmentation quality, screenshots will be created\nThese modules require prior processing of the MR images with FreeSurfer's\ndedicated toolboxes for the segmentation of the amygdala and hippocampus, and\nthe hypothalamus, respectively.\n\n- shape module\n\nThe shape module will run a shapeDNA / brainprint analysis to compute distances\nof shape descriptors between lateralized brain structures. This can be used\nto identify discrepancies and irregularities between pairs of corresponding\nstructures. The results will be included in the main csv table, and the output\ndirectory will also contain a 'brainprint' subdirectory.\n\n- outlier module\n\nThis is a module to detect extreme values among the subcortical ('aseg')\nsegmentations as well as the cortical parcellations. If present, hypothalamic\nand hippocampal subsegmentations will also be included.\n\nThe outlier detection is based on comparisons with the\ndistributions of the sample as well as normative values taken from the\nliterature (see References).\n\nFor comparisons with the sample distributions, extreme values are defined in\ntwo ways: nonparametrically, i.e. values that are 1.5 times the interquartile\nrange below or above the 25th or 75th percentile of the sample, respectively,\nand parametrically, i.e. values that are more than 2 standard deviations above\nor below the sample mean. Note that a minimum of 10 supplied subjects is\nrequired for running these analyses, otherwise `NaNs` will be returned.\n\nFor comparisons with the normative values, lower and upper bounds are computed\nfrom the 95% prediction intervals of the regression models given in Potvin et\nal., 2016, and values exceeding these bounds will be flagged. As an\nalternative, users may specify their own normative values by using the\n'--outlier-table' argument. This requires a custom csv table with headers\n`label`, `upper`, and `lower`, where `label` indicates a column of anatomical\nnames. It can be a subset and the order is arbitrary, but naming must exactly\nmatch the nomenclature of the 'aseg.stats' and/or '[lr]h.aparc.stats' file.\nIf cortical parcellations are included in the outlier table for a comparison\nwith aparc.stats values, the labels must have a 'lh.' or 'rh.' prefix. `upper`\nand `lower` are user-specified upper and lower bounds.\n\nThe main csv table will be appended with the following summary variables, and\nmore detailed output about will be saved as csv tables in the 'outliers'\nsubdirectory of the main output directory.\n\nvariable                 |   description\n-------------------------|---------------------------------------------------\nn_outliers_sample_nonpar | number of structures that are 1.5 times the IQR above/below the 75th/25th percentile\nn_outliers_sample_param  | number of structures that are 2 SD above/below the mean\nn_outliers_norms         | number of structures exceeding the upper and lower bounds of the normative values\n\n___\n\n\n## Development\n\n### Current status\n\nWe are happy to announce the release of version 2.0 of the fsqc toolbox. With\nthis release comes a change of the project name from `qatools` to `fsqc`, to\nreflect increased independence from the original FreeSurfer QA tools, and\napplicability to other neuroimaging analysis packages - such as [Fastsurfer](https://github.com/Deep-MI/FastSurfer).\n\nRecent changes include the addition of the hippocampus and hypothalamus modules\nas well as the addition of surface and skullstrip visualization modules.\nTechnical changes include how the package is installed, imported, and run, see\n[below](https://github.com/Deep-MI/fsqc#usage) for details.\n\nA list of changes is available [here](CHANGES.md).\n\n### Main and development branches\n\nThis repository contains multiple branches, reflecting the ongoing\ndevelopment of the toolbox. The two primary branches are the main branch\n(`stable`) and the development branch (`dev`). New features will first be added\nto the development branch, and eventually be merged with the main branch. \n\n### Roadmap\n\nThe goal of the `fsqc` project is to create a modular and extensible software\npackage that provides quantitative metrics and visual information for the\nquality control of FreeSurfer- or Fastsurfer-processed MR images. The package\nis currently under development, and new features are continuously added.\n\nNew features will initially be available in the [development branch](https://github.com/Deep-MI/fsqc/tree/dev)\nof this toolbox and will be included in the [main branch](https://github.com/Deep-MI/fsqc/tree/stable)\nafter a period of testing and evaluation. Unless explicitly announced, all new\nfeatures will preserve compatibility with earlier versions.\n\nFeedback, suggestions, and [contributions](CONTRIBUTINNG.md) are always welcome,\npreferably via [issues](https://github.com/Deep-MI/fsqc/issues) and [pull requests](https://github.com/Deep-MI/fsqc/pulls).\n\n___\n\n## Usage\n\n### As a command line tool\n\n```\nrun_fsqc --subjects_dir <directory> --output_dir <directory>\n    [--subjects SubjectID [SubjectID ...]]\n    [--subjects-file <file>] [--screenshots]\n    [--screenshots-html] [--surfaces] [--surfaces-html]\n    [--skullstrip] [--skullstrip-html]\n    [--fornix] [--fornix-html] [--hippocampus]\n    [--hippocampus-html] [--hippocampus-label ... ]\n    [--hypothalamus] [--hypothalamus-html] [--shape]\n    [--outlier] [--fastsurfer] [--no-group]\n    [--group-only] [--exit-on-error]\n    [--skip-existing] [-h] [--more-help]\n    [...]\n\n\nrequired arguments:\n  --subjects_dir <directory>\n                         subjects directory with a set of Freesurfer- or\n                         Fastsurfer-processed individual datasets.\n  --output_dir <directory>\n                         output directory\n\noptional arguments:\n  --subjects SubjectID [SubjectID ...]\n                         list of subject IDs\n  --subjects-file <file> filename of a file with subject IDs (one per line)\n  --screenshots          create screenshots of individual brains\n  --screenshots-html     create screenshots of individual brains incl.\n                         html summary page\n  --surfaces             create screenshots of individual brain surfaces\n  --surfaces-html        create screenshots of individual brain surfaces\n                         and html summary page\n  --skullstrip           create screenshots of individual brainmasks\n  --skullstrip-html      create screenshots of individual brainmasks and\n                         html summary page\n  --fornix               check fornix segmentation\n  --fornix-html          check fornix segmentation and create html summary\n                         page of fornix evaluation\n  --hypothalamus         check hypothalamic segmentation\n  --hypothalamus-html    check hypothalamic segmentation and create html\n                         summary page\n  --hippocampus          check segmentation of hippocampus and amygdala\n  --hippocampus-html     check segmentation of hippocampus and amygdala\n                         and create html summary page\n  --hippocampus-label    specify label for hippocampus segmentation files\n                         (default: T1.v21). The full filename is then\n                         [lr]h.hippoAmygLabels-<LABEL>.FSvoxelSpace.mgz\n  --shape                run shape analysis\n  --outlier              run outlier detection\n  --outlier-table        specify normative values (only in conjunction with\n                         --outlier)\n  --fastsurfer           use FastSurfer instead of FreeSurfer output\n  --no-group             run script in subject-level mode. will compute\n                         individual files and statistics, but not create\n                         group-level summaries.\n  --group-only           run script in group mode. will create group-level\n                         summaries from existing inputs\n  --exit-on-error        terminate the program when encountering an error;\n                         otherwise, try to continue with the next module or\n                         case\n  --skip-existing        skips processing for a given case if output\n                         already exists, even with possibly different\n                         parameters or settings\n\ngetting help:\n  -h, --help            display this help message and exit\n  --more-help           display extensive help message and exit\n\nexpert options:\n  --screenshots_base <image>\n                        filename of an image that should be used instead of\n                        norm.mgz as the base image for the screenshots. Can be\n                        an individual file (which would not be appropriate for\n                        multi-subject analysis) or can be a file without\n                        pathname and with the same filename across subjects\n                        within the 'mri' subdirectory of an individual\n                        FreeSurfer results directory (which would be appropriate\n                        for multi-subject analysis).\n  --screenshots_overlay <image>\n                        path to an image that should be used instead of aseg.mgz\n                        as the overlay image for the screenshots; can also be\n                        none. Can be an individual file (which would not be\n                        appropriate for multi-subject analysis) or can be a file\n                        without pathname and with the same filename across\n                        subjects within the 'mri' subdirectory of an individual\n                        FreeSurfer results directory (which would be appropriate\n                        for multi-subject analysis).\n  --screenshots_surf <surf> [<surf> ...]\n                        one or more surface files that should be used instead\n                        of [lr]h.white and [lr]h.pial; can also be none. Can be\n                        one or more individual file(s) (which would not be\n                        appropriate for multi-subject analysis) or can be a\n                        (list of) file(s) without pathname and with the same\n                        filename across subjects within the 'surf' subdirectory\n                        of an individual FreeSurfer results directory (which\n                        would be appropriate for multi-subject analysis).\n  --screenshots_views <view> [<view> ...]\n                        one or more views to use for the screenshots in the form\n                        of x=<numeric> y=<numeric> and/or z=<numeric>. Order\n                        does not matter. Default views are x=-10 x=10 y=0 z=0.\n  --screenshots_layout <rows> <columns>\n                        layout matrix for screenshot images.\n\n```\n\n*Examples:*\n\n- Run the QC pipeline for all subjects found in `/my/subjects/directory`:\n\n```bash\nrun_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory\n```\n\n- Run the QC pipeline for two specific subjects that need to be present in `/my/subjects/directory`:\n\n```bash\nrun_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --subjects mySubjectID1 mySubjectID2\n```\n\n- Run the QC pipeline for all subjects found in `/my/subjects/directory` after full FastSurfer processing:\n\n```bash\nrun_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --fastsurfer\n```\n\n- Run the QC pipeline plus the screenshots module for all subjects found in `/my/subjects/directory`:\n\n```bash\nrun_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --screenshots\n```\n\n- Run the QC pipeline plus the fornix pipeline for all subjects found in `/my/subjects/directory`:\n\n```bash\nrun_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --fornix\n```\n\n- Run the QC pipeline plus the shape analysis pipeline for all subjects found in `/my/subjects/directory`:\n\n```bash\nrun_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --shape\n```\n\n- Run the QC pipeline plus the outlier detection module for all subjects found in `/my/subjects/directory`:\n\n```bash\nrun_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --outlier\n```\n\n- Run the QC pipeline plus the outlier detection module with a user-specific table of normative values for all subjects found in `/my/subjects/directory`:\n\n```bash\nrun_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --outlier --outlier-table /my/table/with/normative/values.csv\n```\n\n- Note that the `--screenshots`, `--fornix`, `--shape`, and `--outlier` (and other) arguments can also be used in conjunction.\n\n### As a Python package\n\nAs an alternative to their command-line usage, the fsqc scripts can also be run\nwithin a pure Python environment, i.e. installed and imported as a Python package.\n\nUse `import fsqc` (or sth. equivalent) to import the package within a\nPython environment, and use the `run_fsqc` function from the `fsqc` module to\nrun an analysis.\n\nIn its most basic form:\n\n```python\nimport fsqc\nfsqc.run_fsqc(subjects_dir='/my/subjects/dir', output_dir='/my/output/dir')\n```\n\nSpecify subjects as a list:\n\n```python\nimport fsqc\nfsqc.run_fsqc(subjects_dir='/my/subjects/dir', output_dir='/my/output/dir', subjects=['subject1', 'subject2', 'subject3'])\n```\n\nAnd as a more elaborate example:\n\n```python\nimport fsqc\nfsqc.run_fsqc(subjects_dir='/my/subjects/dir', output_dir='/my/output/dir', subject_file='/my/subjects/file.txt', screenshots_html=True, surfaces_html=True, skullstrip_html=True, fornix_html=True, hypothalamus_html=True, hippocampus_html=True, hippocampus_label=\"T1.v21\", shape=True, outlier=True)\n```\n\n\nCall `help(fsqc.run_fsqc)` for further usage info and additional options.\n\n### As a Docker image\n\nWe provide configuration files that can be used to create a Docker or\nSingularity image for the fsqc scripts. Documentation can be found on the\n[Docker](docker/Docker.md) and [Singularity](singularity/Singularity.md) pages.\n\n___\n\n## Installation\n\n### Installation as a Python package\n\nUse:\n\n```bash\npip install fsqc\n```\n\nto install the fsqc package and all of its dependencies. This is the recommended\nway of installing the package, and allows for both command-line execution and\nexecution as a Python function. We also recommend to do this installation within\na Python virtual environment, which can be created and activated as follows:\n\n```bash\nvirtualenv /path/to/my/virtual/environment\nsource /path/to/my/virtual/environment/bin/activate\n```\n\n### Installation from GitHub\n\nUse the following code to download, build and install the fsqc package from its\nGitHub repository into your local Python package directory:\n\n```bash\npip install git+https://github.com/deep-mi/fsqc.git\n```\n\nThis can be useful if you want to install a particular branch - such as the `dev`\nbranch in the following example:\n\n```bash\npip install git+https://github.com/deep-mi/fsqc.git@dev\n```\n\n### Download from GitHub\n\nThis software can also be downloaded from its GitHub repository at `https://github.com/Deep-MI/fsqc`,\nor cloned directly via `git clone https://github.com/Deep-MI/fsqc`.\n\nThe `run_fsqc` script will then be executable from the command line, as\ndetailed above. Note, however, that the required dependencies will have to be\ninstalled manually. See the [requirements](#requirements) section for\ninstructions.\n\n\n___\n\n## Requirements\n\n- At least one structural MR image that was processed with Freesurfer 6.0, 7.x,\n  or FastSurfer 1.1 or later (including the surface pipeline).\n\n- A Python version >= 3.8 is required to run this script.\n\n- Required packages include (among others) the nibabel and skimage package for\n  the core functionality, plus the matplotlib, pandas, and transform3d\n  packages for some optional functions and modules. See the `requirements.txt`\n  file for a complete list. Use `pip install -r requirements.txt` to install\n  these packages.\n\n- If installing the toolbox as a Python package or if using the Docker image,\n  all required packages will be installed automatically and manual installation\n  as detailed above will not be necessary.\n\n- This software has been tested on Ubuntu 20.04 and 22.04.\n\n- A working [FreeSurfer](https://freesurfer.net) installation (version 6 or\n  newer) is required for running the 'shape' module of this toolbox. Also make\n  sure that FreeSurfer is sourced (i.e., `FREESURFER_HOME` is set as an\n  environment variable) before running an analysis.\n\n___\n\n## Known issues\n\n- Aborted / restarted recon-all runs: the program will analyze recon-all\n  logfiles, and may fail or return erroneous results if the logfile is\n  appended by multiple restarts of recon-all runs. Ideally, the logfile should\n  therefore consist of just a single, successful recon-all run.\n\n___\n\n## Authors\n\n- fsqc toolbox: Kersten Diers, Tobias Wolff, and Martin Reuter.\n- Freesurfer QA Tools: David Koh, Stephanie Lee, Jenni Pacheco, Vasanth Pappu,\n  and Louis Vinke.\n- lapy and brainprint toolboxes: Martin Reuter.\n\n___\n\n## Citations\n\n- Esteban O, Birman D, Schaer M, Koyejo OO, Poldrack RA, Gorgolewski KJ; 2017;\n  MRIQC: Advancing the Automatic Prediction of Image Quality in MRI from Unseen\n  Sites; PLOS ONE 12(9):e0184661; doi:10.1371/journal.pone.0184661.\n\n- Wachinger C, Golland P, Kremen W, Fischl B, Reuter M; 2015; BrainPrint: a\n  Discriminative Characterization of Brain Morphology; Neuroimage: 109, 232-248;\n  doi:10.1016/j.neuroimage.2015.01.032.\n\n- Reuter M, Wolter FE, Shenton M, Niethammer M; 2009; Laplace-Beltrami\n  Eigenvalues and Topological Features of Eigenfunctions for Statistical Shape\n  Analysis; Computer-Aided Design: 41, 739-755; doi:10.1016/j.cad.2009.02.007.\n\n- Potvin O, Mouiha A, Dieumegarde L, Duchesne S, & Alzheimer's Disease\n  Neuroimaging Initiative; 2016; Normative data for subcortical regional volumes\n  over the lifetime of the adult human brain; Neuroimage: 137, 9-20; doi.org/10.1016/j.neuroimage.2016.05.016\n\n___\n\n## License\n\nThis software is licensed under the MIT License, see associated LICENSE file\nfor details.\n\nCopyright (c) 2019 Image Analysis Group, DZNE e.V.\n\n",
                "dependencies": "[build-system]\nrequires = ['setuptools >= 61.0.0']\nbuild-backend = 'setuptools.build_meta'\n\n[project]\nname = 'fsqc'\ndescription = 'Quality control scripts for FastSurfer and FreeSurfer structural MRI data'\nlicense = {file = 'LICENSE'}\nrequires-python = '>=3.9'\nauthors = [\n    {name = 'Kersten Diers', email = 'kersten.diers@dzne.de'},\n    {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'}\n]\nmaintainers = [\n    {name = 'Kersten Diers', email = 'kersten.diers@dzne.de'},\n    {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'}\n]\nkeywords = [\n    'FastSurfer',\n    'FreeSurfer',\n    'Quality control',\n    'Quality assurance',\n]\nclassifiers = [\n    'Operating System :: Unix',\n    'Operating System :: MacOS',\n    'Programming Language :: Python :: 3 :: Only',\n    'Programming Language :: Python :: 3.9',\n    'Programming Language :: Python :: 3.10',\n    'Programming Language :: Python :: 3.11',\n    'Programming Language :: Python :: 3.12',\n    'Natural Language :: English',\n    'License :: OSI Approved :: MIT License',\n    'Intended Audience :: Science/Research',\n]\ndynamic = [\"version\", \"readme\", \"dependencies\"]\n\n[project.optional-dependencies]\nbuild = [\n    'build',\n    'twine',\n]\ndoc = [\n    'furo!=2023.8.17',\n    'matplotlib',\n    'memory-profiler',\n    'numpydoc',\n    'sphinx!=7.2.*',\n    'sphinxcontrib-bibtex',\n    'sphinx-copybutton',\n    'sphinx-design',\n    'sphinx-gallery',\n    'sphinx-issues',\n    'pypandoc',\n    'nbsphinx',\n    'IPython', # For syntax highlighting in notebooks\n    'ipykernel',\n]\nstyle = [\n    'bibclean',\n    'codespell',\n    'pydocstyle[toml]',\n    'ruff',\n]\ntest = [\n    'pytest',\n    'pytest-cov',\n    'pytest-timeout',\n]\nall = [\n    'fsqc[build]',\n    'fsqc[doc]',\n    'fsqc[style]',\n]\nfull = [\n    'fsqc[all]',\n]\n\n[project.urls]\nhomepage = 'https://github.com/Deep-MI/fsqc'\ndocumentation = 'https://github.com/Deep-MI/fsqc'\nsource = 'https://github.com/Deep-MI/fsqc'\ntracker = 'https://github.com/Deep-MI/fsqc/issues'\n\n[project.scripts]\nrun_fsqc = 'fsqc.cli:main'\nfsqc-sys_info = 'fsqc.commands.sys_info:run'\n\n[tool.setuptools.dynamic]\nversion = {file = 'VERSION'}\nreadme = {file = 'DESCRIPTION.md', content-type = \"text/markdown\"}\ndependencies = {file = 'requirements.txt'}\n\n#[tool.setuptools] ### probably not needed\n#include-package-data = false\n\n[tool.setuptools.packages.find]\ninclude = ['fsqc', 'fsqc.cli', 'fsqc.commands', 'fsqc.utils']\nexclude = ['docker', 'singularity']\n\n[tool.pydocstyle]\nconvention = 'numpy'\nignore-decorators = '(copy_doc|property|.*setter|.*getter|pyqtSlot|Slot)'\nmatch = '^(?!setup|__init__|test_).*\\.py'\nmatch-dir = '^fsqc.*'\nadd_ignore = 'D100,D104,D107'\n\n[tool.ruff]\nline-length = 88\nextend-exclude = [\n    \".github\",\n    \"doc\",\n    \"docker\",\n    \"setup.py\",\n    \"singularity\",\n]\nignore = [\"E501\"] # line too long (should be enforced soon)\n\n\n[tool.ruff.lint]\n# https://docs.astral.sh/ruff/linter/#rule-selection\nselect = [\n    \"E\",   # pycodestyle\n    \"F\",   # Pyflakes\n    \"UP\",  # pyupgrade\n    \"B\",   # flake8-bugbear\n    \"I\",   # isort\n    # \"SIM\", # flake8-simplify\n]\n\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\"F401\"]\n\n[tool.pytest.ini_options]\nminversion = '6.0'\nfilterwarnings = []\naddopts = [\n    \"--import-mode=importlib\",\n    \"--junit-xml=junit-results.xml\",\n    \"--durations=20\",\n    \"--verbose\",\n]\n\n[tool.coverage.run]\nbranch = true\ncover_pylib = false\nomit = [\n    '**/__init__.py',\n    '**/fsqc/_version.py',\n    '**/fsqc/commands/*',\n    '**/tests/**',\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    'pragma: no cover',\n    'if __name__ == .__main__.:',\n]\nprecision = 2\n\nbrainprint==0.4.0\nlapy>=1.0.0,<2\nkaleido\nmatplotlib\nnibabel\nnumpy\npandas\nscipy\nscikit-image\ntransforms3d\n\n# https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html\n# If compatibility with legacy builds or versions of tools that don’t support\n# certain packaging standards (e.g. PEP 517 or PEP 660), a simple setup.py\n# script can be added to your project [1] (while keeping the configuration in\n# pyproject.toml):\n\nfrom setuptools import setup\n\nsetup()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/gaia",
            "repo_link": "https://gitlab.dlr.de/pf-plp/gaia-group",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/gasnetsim",
            "repo_link": "https://jugit.fz-juelich.de/iek-10/public/simulation/gasnetsim",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/geomultisens",
            "repo_link": "https://git.gfz-potsdam.de/geomultisens",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/gfzrnx",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/ginkgo",
            "repo_link": "https://github.com/ginkgo-project/ginkgo/",
            "content": {
                "codemeta": "",
                "readme": "  <p align=\"center\"><img src=\"/assets/logo.png\" alt=\"Ginkgo\" width=\"60%\" height=\"60%\"></p>\n\n<div align=\"center\">\n\n[![License](https://img.shields.io/github/license/ginkgo-project/ginkgo.svg)](./LICENSE)|[![c++ standard](https://img.shields.io/badge/c%2B%2B-17-blue.svg)](https://en.wikipedia.org/wiki/C%2B%2B#Standardization)|[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://ginkgo-project.github.io/ginkgo-generated-documentation/doc/develop/)|[![DOI](https://joss.theoj.org/papers/10.21105/joss.02260/status.svg)](https://doi.org/10.21105/joss.02260)\n|:-:|:-:|:-:|:-:|\n\n\n[![Build status](https://gitlab.com/ginkgo-project/ginkgo-public-ci/badges/develop/pipeline.svg)](https://gitlab.com/ginkgo-project/ginkgo-public-ci/-/pipelines?page=1&scope=branches&ref=develop)|[![OSX-build](https://github.com/ginkgo-project/ginkgo/actions/workflows/osx.yml/badge.svg)](https://github.com/ginkgo-project/ginkgo/actions/workflows/osx.yml)|[![codecov](https://codecov.io/gh/ginkgo-project/ginkgo/branch/develop/graph/badge.svg)](https://codecov.io/gh/ginkgo-project/ginkgo)|[![Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=ginkgo-project_ginkgo&metric=sqale_rating)](https://sonarcloud.io/dashboard?id=ginkgo-project_ginkgo)|[![Reliability Rating](https://sonarcloud.io/api/project_badges/measure?project=ginkgo-project_ginkgo&metric=reliability_rating)](https://sonarcloud.io/dashboard?id=ginkgo-project_ginkgo)|[![CDash dashboard](https://img.shields.io/badge/CDash-Access-blue.svg)](https://my.cdash.org/index.php?project=Ginkgo+Project)\n|:-:|:-:|:-:|:-:|:-:|:-:|\n\n</div>\n\n\nGinkgo is a high-performance numerical linear algebra library for many-core systems, with a\nfocus on solution of sparse linear systems. It is implemented using modern C++\n(you will need an at least C++17 compliant compiler to build it), with GPU kernels\nimplemented for NVIDIA, AMD and Intel GPUs.\n\n---\n\n**[Prerequisites](#prerequisites)** |\n**[Building Ginkgo](#building-and-installing-ginkgo)** |\n**[Tests, Examples, Benchmarks](#tests-examples-and-benchmarks)** |\n**[Bug reports](#bug-reports-and-support)** |\n**[Licensing](#licensing)** |\n**[Contributing](#contributing-to-ginkgo)** |\n**[Citing](#citing-ginkgo)**\n\n\n# Prerequisites\n\n### Linux and Mac OS\n\nFor Ginkgo core library:\n\n*   _cmake 3.16+_\n*   C++17 compliant compiler, one of:\n    *   _gcc 7+_\n    *   _clang 5+_\n    *   _Intel compiler 2019+_\n    *   _Apple Clang 15.0_ is tested. Earlier versions might also work.\n    *   _Cray Compiler 14.0.1+_\n    *   _NVHPC Compiler 22.7+_\n\nThe Ginkgo CUDA module has the following __additional__ requirements:\n\n*   _cmake 3.18+_ (If CUDA was installed through the NVIDIA HPC Toolkit, we require _cmake 3.22+_)\n*   _CUDA 11.0+_ or _NVHPC Package 22.7+_\n*   Any host compiler restrictions your version of CUDA may impose also apply\n    here. For the newest CUDA version, this information can be found in the\n    [CUDA installation guide for Linux](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html)\n    or [CUDA installation guide for Mac Os X](https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html)\n\nThe Ginkgo HIP module has the following __additional__ requirements:\n\n* _ROCm 4.5+_\n* the HIP, hipBLAS, hipSPARSE, hip/rocRAND and rocThrust packages compiled with the ROCm backend\n* if the hipFFT package is available, it is used to implement the FFT LinOps.\n* _cmake 3.21+_\n\nThe Ginkgo DPC++(SYCL) module has the following __additional__ requirements:\n\n* _oneAPI 2023.1+_\n* Set `dpcpp` or `icpx` as the `CMAKE_CXX_COMPILER`\n* The following oneAPI packages should be available:\n    * oneMKL\n    * oneDPL\n\nThe Ginkgo MPI module has the following __additional__ requirements:\n\n* MPI 3.1+, ideally GPU-Aware, for best performance\n\nIn addition, if you want to contribute code to Ginkgo, you will also need the\nfollowing:\n\n*   _clang-format 14_ (downloaded automatically by `pre-commit`)\n*   _clang-tidy_ (optional, when setting the flag `-DGINKGO_WITH_CLANG_TIDY=ON`)\n*   _iwyu_ (Include What You Use, optional, when setting the flag `-DGINKGO_WITH_IWYU=ON`)\n\n### Windows\n\n*   _cmake 3.16+_\n*   C++17 compliant 64-bit compiler:\n    *   _MinGW : gcc 7+_\n    *   _Microsoft Visual Studio : VS 2019+_\n\nThe Ginkgo CUDA module has the following __additional__ requirements:\n\n*   _CUDA 11.0+_\n*   _Microsoft Visual Studio_\n*   Any host compiler restrictions your version of CUDA may impose also apply\n    here. For the newest CUDA version, this information can be found in the\n    [CUDA installation guide for Windows](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html)\n\nThe Ginkgo OMP module has the following __additional__ requirements:\n*  _MinGW_\n\nIn these environments, two problems can be encountered, the solution for which is described in the\n[windows section in INSTALL.md](INSTALL.md#building-ginkgo-in-windows):\n* `ld: error: export ordinal too large` needs the compilation flag `-O1`\n* `cc1plus.exe: out of memory allocating 65536 bytes` requires a modification of the environment\n\n__NOTE:__ Some restrictions will also apply on the version of C and C++ standard\nlibraries installed on the system. This needs further investigation.\n\n# Building and Installing Ginkgo\n\nTo build Ginkgo, you can use the standard CMake procedure.\n\n```sh\nmkdir build; cd build\ncmake -G \"Unix Makefiles\" .. && cmake --build .\ncmake --install .\n```\n\nBy default, `GINKGO_BUILD_REFERENCE` is enabled. You should be able to run\nexamples with this executor. By default, Ginkgo tries to enable the relevant\nmodules depending on your machine environment (present of CUDA, ...). You can\nalso explicitly compile with the OpenMP, CUDA, HIP or DPC++(SYCL) modules enabled to\nrun the examples with these executors.\n\nPlease refer to the [Installation page](./INSTALL.md) for more details.\n\nTip: After installation, in your CMake project, Ginkgo can be added with `find_package(Ginkgo)` and the the target that is exported is `Ginkgo::ginkgo`.\nAn example can be found in the [`test_install`](test/test_install/CMakeLists.txt).\n\n# Tests, Examples and Benchmarks\n\n### Testing\n\nGinkgo does comprehensive unit tests using Google Tests. These tests are enabled by default and can be disabled if necessary by passing the `-DGINKGO_BUILD_TESTS=NO` to the cmake command. More details about running tests can be found in the [TESTING.md page](./TESTING.md).\n\n### Running examples\n\nVarious examples are available for you to understand and play with Ginkgo within the `examples/` directory. They can be compiled by passing the `-DGINKGO_BUILD_EXAMPLES=ON` to the cmake command. Each of the examples have commented code with explanations and this can be found within the [online documentation](https://ginkgo-project.github.io/ginkgo-generated-documentation/doc/develop/Examples.html).\n\n### Benchmarking\n\nA unique feature of Ginkgo is the ability to run benchmarks and view your results\nwith the help of the [Ginkgo Performance Explorer (GPE)](https://ginkgo-project.github.io/gpe/).\n\nMore details about this can be found in the [BENCHMARKING.md page](./BENCHMARKING.md)\n\n# Bug reports and Support\n\nIf you have any questions about using Ginkgo, please use [Github discussions](https://github.com/ginkgo-project/ginkgo/discussions).\n\nIf you would like to request a feature, or have encountered a bug, please [create an issue](https://github.com/ginkgo-project/ginkgo/issues/new). Please be sure to describe your problem and provide as much information as possible.\n\nYou can also send an email to [Ginkgo's main email address](mailto:ginkgo.library@gmail.com).\n\n# Licensing\n\nGinkgo is available under the [3-clause BSD license](LICENSE). All contributions\nto the project are added under this license.\n\nDepending on the configuration options used when building Ginkgo, third party\nsoftware may be pulled as additional dependencies, which have their own\nlicensing conditions. Refer to [ABOUT-LICENSING.md](ABOUT-LICENSING.md) for\ndetails.\n\n# Contributing to Ginkgo\n\nWe are glad that that you would like to contribute to Ginkgo and we are happy to help you with any questions you may have.\n\nIf you are contributing for the first time, please add yourself to the list of external contributors with the following info\n\n``` text\nName Surname <email@domain> Institution(s)\n```\n\n#### Declaration\n\nGinkgo's source is distributed with a BSD-3 clause license. By contributing to Ginkgo and adding yourself to the contributors list, you agree to the following statement (also in [contributors.txt](contributors.txt)):\n\n``` text\nI hereby place all my contributions in this codebase under a BSD-3-Clause\nlicense, as specified in the repository's LICENSE file.\n```\n\n#### Contribution Guidelines\n\nWhen contributing to Ginkgo, to ease the review process, please follow the guidelines mentioned in [CONTRIBUTING.md](CONTRIBUTING.md).\n\nIt also contains other general recommendations such as writing proper commit messages, understanding Ginkgo's library design, relevant C++ information etc.\n\n\n# Citing Ginkgo\n\nThe main Ginkgo paper describing Ginkgo's purpose, design and interface is\navailable through the following reference:\n\n``` bibtex\n@article{ginkgo-toms-2022,\ntitle = {{Ginkgo: A Modern Linear Operator Algebra Framework for High Performance Computing}},\nvolume = {48},\ncopyright = {All rights reserved},\nissn = {0098-3500},\nshorttitle = {Ginkgo},\nurl = {https://doi.org/10.1145/3480935},\ndoi = {10.1145/3480935},\nnumber = {1},\nurldate = {2022-02-17},\njournal = {ACM Transactions on Mathematical Software},\nauthor = {Anzt, Hartwig and Cojean, Terry and Flegar, Goran and Göbel, Fritz and Grützmacher, Thomas and Nayak, Pratik and Ribizel, Tobias and Tsai, Yuhsiang Mike and Quintana-Ortí, Enrique S.},\nmonth = feb,\nyear = {2022},\nkeywords = {ginkgo, healthy software lifecycle, High performance computing, multi-core and manycore architectures},\npages = {2:1--2:33}\n}\n```\n\nFor more information on topical subjects, please refer to the [CITING.md\npage](CITING.md).\n\n",
                "dependencies": "cmake_minimum_required(VERSION 3.16)\n\nproject(Ginkgo LANGUAGES CXX VERSION 1.10.0 DESCRIPTION \"A numerical linear algebra library targeting many-core architectures\")\nset(Ginkgo_VERSION_TAG \"develop\")\nset(PROJECT_VERSION_TAG ${Ginkgo_VERSION_TAG})\nif(Ginkgo_VERSION_TAG STREQUAL \"master\")\n    set(GINKGO_VERSION_TAG_DEPRECATED ON)\nelse()\n    set(GINKGO_VERSION_TAG_DEPRECATED OFF)\nendif()\nif(GINKGO_VERSION_TAG_DEPRECATED)\n    message(\n            WARNING\n            \"The branch ${Ginkgo_VERSION_TAG} is deprecated and will stop receiving updates after 2025. \"\n            \"Please use the main branch for the latest release, or the develop branch for the latest development updates.\")\nendif()\n# Cuda and Hip also look for Threads. Set it before any find_package to ensure the Threads setting is not changed.\nset(THREADS_PREFER_PTHREAD_FLAG ON)\n\n# Determine which modules can be compiled\ninclude(cmake/autodetect_executors.cmake)\n\nlist(APPEND CMAKE_MODULE_PATH \"${PROJECT_SOURCE_DIR}/cmake/Modules/\")\ninclude(cmake/autodetect_system_libs.cmake)\n\n# rename helper\ninclude(cmake/rename.cmake)\n\n# Ginkgo configuration options\noption(GINKGO_DEVEL_TOOLS \"Add development tools to the build system\" OFF)\noption(GINKGO_BUILD_TESTS \"Generate build files for unit tests\" ON)\noption(GINKGO_BUILD_EXAMPLES \"Build Ginkgo's examples\" ON)\noption(GINKGO_BUILD_BENCHMARKS \"Build Ginkgo's benchmarks\" ON)\noption(GINKGO_BUILD_REFERENCE \"Compile reference CPU kernels\" ON)\noption(GINKGO_BUILD_OMP \"Compile OpenMP kernels for CPU\" ${GINKGO_HAS_OMP})\noption(GINKGO_BUILD_MPI \"Compile the MPI module\" ${GINKGO_HAS_MPI})\ngko_rename_cache(GINKGO_BUILD_DPCPP GINKGO_BUILD_SYCL BOOL \"Compile SYCL kernels for Intel GPUs or other SYCL enabled hardware\")\noption(GINKGO_BUILD_SYCL\n    \"Compile SYCL kernels for Intel GPUs or other SYCL enabled hardware\" ${GINKGO_HAS_SYCL})\noption(GINKGO_BUILD_CUDA \"Compile kernels for NVIDIA GPUs\" ${GINKGO_HAS_CUDA})\noption(GINKGO_BUILD_HIP \"Compile kernels for AMD or NVIDIA GPUs\" ${GINKGO_HAS_HIP})\noption(GINKGO_BUILD_DOC \"Generate documentation\" OFF)\noption(GINKGO_FAST_TESTS \"Reduces the input size for a few tests known to be time-intensive\" OFF)\noption(GINKGO_TEST_NONDEFAULT_STREAM \"Uses non-default streams in CUDA and HIP tests\" OFF)\noption(GINKGO_MIXED_PRECISION \"Instantiate true mixed-precision kernels (otherwise they will be conversion-based using implicit temporary storage)\" OFF)\noption(GINKGO_ENABLE_HALF \"Enable the use of half precision\" ON)\n# We do not support half precision in MSVC and msys2 (MINGW).\nif(MSVC OR MINGW)\n    message(STATUS \"We do not support half precision in MSVC and MINGW.\")\n    set(GINKGO_ENABLE_HALF OFF CACHE BOOL \"Enable the use of half precision\" FORCE)\nendif()\noption(GINKGO_SKIP_DEPENDENCY_UPDATE\n    \"Do not update dependencies each time the project is rebuilt\" ON)\noption(GINKGO_WITH_CLANG_TIDY \"Make Ginkgo call `clang-tidy` to find programming issues.\" OFF)\noption(GINKGO_WITH_IWYU \"Make Ginkgo call `iwyu` (Include What You Use) to find include issues.\" OFF)\noption(GINKGO_WITH_CCACHE \"Use ccache if available to speed up C++ and CUDA rebuilds by caching compilations.\" ON)\noption(GINKGO_CHECK_CIRCULAR_DEPS\n    \"Enable compile-time checks detecting circular dependencies between libraries and non-self-sufficient headers.\"\n    OFF)\noption(GINKGO_CONFIG_LOG_DETAILED\n    \"Enable printing of detailed configuration log to screen in addition to the writing of files,\" OFF)\noption(GINKGO_BENCHMARK_ENABLE_TUNING\n    \"Enable tuning variables in the benchmarks. For specific use cases, manual code changes could be required.\"\n    OFF)\nset(GINKGO_VERBOSE_LEVEL \"1\" CACHE STRING\n    \"Verbosity level. Put 0 to turn off. 1 activates a few important messages.\")\nset(GINKGO_CUDA_ARCHITECTURES \"Auto\" CACHE STRING\n    \"A list of target NVIDIA GPU architectures. See README.md for more detail.\")\n# the details of fine/coarse grain memory and unsafe atomic are available https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#floating-point-fp-atomic-operations-and-coarse-fine-grained-memory-allocations\noption(GINKGO_HIP_AMD_UNSAFE_ATOMIC \"Compiler uses unsafe floating point atomic (only for AMD GPU and ROCM >= 5). Default is ON because we use hipMalloc, which is always on coarse grain. Must turn off when allocating memory on fine grain\" ON)\noption(GINKGO_SPLIT_TEMPLATE_INSTANTIATIONS \"Split template instantiations for slow-to-compile files. This improves parallel build performance\" ON)\nmark_as_advanced(GINKGO_SPLIT_TEMPLATE_INSTANTIATIONS)\noption(GINKGO_JACOBI_FULL_OPTIMIZATIONS \"Use all the optimizations for the CUDA Jacobi algorithm\" OFF)\noption(BUILD_SHARED_LIBS \"Build shared (.so, .dylib, .dll) libraries\" ON)\noption(GINKGO_BUILD_HWLOC \"Build Ginkgo with HWLOC. Default is OFF.\" OFF)\noption(GINKGO_BUILD_PAPI_SDE \"Build Ginkgo with PAPI SDE. Enabled if a system installation is found.\" ${PAPI_SDE_FOUND})\noption(GINKGO_DPCPP_SINGLE_MODE \"Do not compile double kernels for the DPC++ backend.\" OFF)\noption(GINKGO_INSTALL_RPATH \"Set the RPATH when installing its libraries.\" ON)\noption(GINKGO_INSTALL_RPATH_ORIGIN \"Add $ORIGIN (Linux) or @loader_path (MacOS) to the installation RPATH.\" ON)\noption(GINKGO_INSTALL_RPATH_DEPENDENCIES \"Add dependencies to the installation RPATH.\" OFF)\noption(GINKGO_FORCE_GPU_AWARE_MPI \"Assert that the MPI library is GPU aware. This forces Ginkgo to assume that GPU aware functionality is available (OFF (default) or ON), but may fail\n     catastrophically in case the MPI implementation is not GPU Aware, and GPU aware functionality has been forced\" OFF)\nset(GINKGO_CI_TEST_OMP_PARALLELISM \"4\" CACHE STRING\n    \"The number of OpenMP threads to use for a test binary during CTest resource file-constrained test.\")\noption(GINKGO_EXTENSION_KOKKOS_CHECK_TYPE_ALIGNMENT \"Enables mapping to Kokkos types to check the alignment of the source and target type.\" ON)\ngko_rename_cache(GINKGO_COMPILER_FLAGS CMAKE_CXX_FLAGS BOOL \"Flags used by the CXX compiler during all build types.\")\ngko_rename_cache(GINKGO_CUDA_COMPILER_FLAGS CMAKE_CUDA_FLAGS BOOL \"Flags used by the CUDA compiler during all build types.\")\n\n# load executor-specific configuration\nif(GINKGO_BUILD_CUDA)\n    include(cmake/cuda.cmake)\n    if(CUDAToolkit_VERSION VERSION_LESS 11.6)\n        message(STATUS \"Disable custom thrust namespace for cuda before 11.6 because it has no effect in the thrust shipped by cuda before 11.6\")\n        set(GINKGO_CUDA_CUSTOM_THRUST_NAMESPACE OFF)\n    else()\n        message(STATUS \"Enable custom thrust namespace for cuda\")\n        set(GINKGO_CUDA_CUSTOM_THRUST_NAMESPACE ON)\n    endif()\nendif()\nif(GINKGO_BUILD_HIP)\n    include(cmake/hip.cmake)\n    if(GINKGO_HIP_PLATFORM_AMD AND GINKGO_HIP_VERSION VERSION_LESS 5.7)\n        # Hip allow custom namespace but does not fully make everything in the custom namespace before rocm-5.7\n        # more specific pr: https://github.com/ROCm/rocThrust/pull/286\n        message(STATUS \"Disable custom thrust namespace for hip before 5.7 because hip does not fully support it before 5.7\")\n        set(GINKGO_HIP_CUSTOM_THRUST_NAMESPACE OFF)\n    else()\n        message(STATUS \"Enable custom thrust namespace for hip\")\n        set(GINKGO_HIP_CUSTOM_THRUST_NAMESPACE ON)\n    endif()\nendif()\nif(GINKGO_BUILD_SYCL)\n    include(cmake/sycl.cmake)\nendif()\nif(GINKGO_BUILD_OMP)\n    find_package(OpenMP 3.0 REQUIRED)\nendif()\n\nfind_package(Threads REQUIRED)\ninclude(cmake/build_type_helpers.cmake)\n\n# Load other CMake helpers\ninclude(cmake/build_helpers.cmake)\ninclude(cmake/install_helpers.cmake)\n\nif(MSVC)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /bigobj\")\nendif()\nif(MINGW OR CYGWIN)\n    if(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n        # Otherwise, dynamic_cast to the class marked by final will be failed.\n        # https://reviews.llvm.org/D154658 should be relevant\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fno-assume-unique-vtables\")\n    else()\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wa,-mbig-obj\")\n    endif()\nendif()\n\n# For now, PGI/NVHPC nvc++ compiler doesn't seem to support\n# `#pragma omp declare reduction`\n#\n# The math with optimization level -O2 doesn't follow IEEE standard, so we\n# enable that back as well.\nif (CMAKE_CXX_COMPILER_ID MATCHES \"PGI|NVHPC\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Kieee\")\nendif()\n\nset(GINKGO_CIRCULAR_DEPS_FLAGS \"-Wl,--no-undefined\")\n\n# Use ccache as compilation launcher\nif(GINKGO_WITH_CCACHE)\n    find_program(CCACHE_PROGRAM ccache)\n    if(CCACHE_PROGRAM)\n        set(CMAKE_CXX_COMPILER_LAUNCHER \"${CCACHE_PROGRAM}\")\n        if(GINKGO_BUILD_CUDA)\n            set(CMAKE_CUDA_COMPILER_LAUNCHER \"${CCACHE_PROGRAM}\")\n        endif()\n    endif()\nendif()\n\nif(GINKGO_BENCHMARK_ENABLE_TUNING)\n    # In this state, the tests and examples cannot be compiled without extra\n    # complexity/intrusiveness, so we simply disable them.\n    set(GINKGO_BUILD_TESTS OFF)\n    set(GINKGO_BUILD_EXAMPLES OFF)\nendif()\n\nif(GINKGO_BUILD_TESTS)\n    message(STATUS \"GINKGO_BUILD_TESTS is ON, enabling GINKGO_BUILD_REFERENCE\")\n    set(GINKGO_BUILD_REFERENCE ON CACHE BOOL \"Compile reference CPU kernels\" FORCE)\nendif()\n\nif(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n    message(STATUS \"Setting build type to 'Release' as none was specified.\")\n    set(CMAKE_BUILD_TYPE Release CACHE STRING \"Choose the type of build.\" FORCE)\nendif()\n\n# Ensure we have a debug postfix\nif(NOT DEFINED CMAKE_DEBUG_POSTFIX)\n    set(CMAKE_DEBUG_POSTFIX \"d\")\nendif()\n\nif(GINKGO_BUILD_TESTS)\n    # Configure CTest\n    configure_file(\n        ${CMAKE_CURRENT_LIST_DIR}/cmake/CTestCustom.cmake.in\n        ${CMAKE_CURRENT_BINARY_DIR}/CTestCustom.cmake @ONLY)\n\n    # For testing, we need some special matrices\n    add_subdirectory(matrices)\n\n    enable_testing()\n    include(CTest)\n    add_custom_target(quick_test \"${CMAKE_CTEST_COMMAND}\" -R 'core|reference')\nendif()\n\nif(GINKGO_WITH_CLANG_TIDY)\n    find_program(GINKGO_CLANG_TIDY_PATH clang-tidy)\nendif()\n\nif(GINKGO_WITH_IWYU)\n    find_program(GINKGO_IWYU_PATH iwyu)\nendif()\n\n# Find important header files, store the definitions in\n# include/ginkgo/config.h.in For details, see\n# https://gitlab.kitware.com/cmake/community/wikis/doc/tutorials/How-To-Write-Platform-Checks\ninclude(CheckIncludeFileCXX)\ncheck_include_file_cxx(cxxabi.h GKO_HAVE_CXXABI_H)\n\n# Automatically find TAU\nset(GINKGO_HAVE_TAU 0)\nfind_package(PerfStubs QUIET)\nif(PerfStubs_FOUND)\n    set(GINKGO_HAVE_TAU 1)\nendif()\n# Automatically find VTune\nset(GINKGO_HAVE_VTUNE 0)\nfind_package(VTune)\nif(VTune_FOUND)\n    set(GINKGO_HAVE_VTUNE 1)\nendif()\n# Automatically find METIS\nset(GINKGO_HAVE_METIS 0)\nfind_package(METIS)\nif(METIS_FOUND)\n    set(GINKGO_HAVE_METIS 1)\nendif()\n# Automatically detect ROCTX (see hip.cmake)\nset(GINKGO_HAVE_ROCTX 0)\nif(GINKGO_BUILD_HIP AND ROCTX_FOUND)\n    set(GINKGO_HAVE_ROCTX 1)\nendif()\n\n# Switch off HWLOC for Windows and MacOS\nif(GINKGO_BUILD_HWLOC AND (MSVC OR WIN32 OR CYGWIN OR APPLE))\n    set(GINKGO_BUILD_HWLOC OFF CACHE BOOL \"Build Ginkgo with HWLOC. Default is OFF. Ginkgo does not support HWLOC on Windows/MacOS\" FORCE)\n    message(WARNING \"Ginkgo does not support HWLOC on Windows/MacOS, switch GINKGO_BUILD_HWLOC to OFF\")\nendif()\n\nset(GINKGO_HAVE_GPU_AWARE_MPI OFF)\nset(GINKGO_FORCE_SPMV_BLOCKING_COMM OFF)\nif(GINKGO_BUILD_MPI)\n    find_package(MPI 3.1 COMPONENTS CXX REQUIRED)\n    if(GINKGO_FORCE_GPU_AWARE_MPI)\n        set(GINKGO_HAVE_GPU_AWARE_MPI ON)\n    else()\n        set(GINKGO_HAVE_GPU_AWARE_MPI OFF)\n    endif()\n\n    # use try_compile instead of try_run to prevent cross-compiling issues\n    try_compile(uses_openmpi\n        ${Ginkgo_BINARY_DIR}\n        ${Ginkgo_SOURCE_DIR}/cmake/openmpi_test.cpp\n                COMPILE_DEFINITIONS -DCHECK_HAS_OPEN_MPI=1\n        LINK_LIBRARIES MPI::MPI_CXX\n        )\n    if(uses_openmpi)\n        try_compile(valid_openmpi_version\n                    ${Ginkgo_BINARY_DIR}\n                    ${Ginkgo_SOURCE_DIR}/cmake/openmpi_test.cpp\n                    COMPILE_DEFINITIONS -DCHECK_OPEN_MPI_VERSION=1\n                    LINK_LIBRARIES MPI::MPI_CXX\n        )\n        if(NOT valid_openmpi_version)\n            message(WARNING\n                \"OpenMPI v4.0.x has a bug that forces us to use blocking communication in our distributed \"\n                \"matrix class. To enable faster, non-blocking communication, consider updating your OpenMPI version or \"\n                \"switch to a different vendor.\")\n            set(GINKGO_FORCE_SPMV_BLOCKING_COMM ON)\n        endif()\n    endif()\nendif()\n\n# Try to find the third party packages before using our subdirectories\nif(GINKGO_BUILD_TESTS)\n    find_package(GTest 1.10.0) # No need for QUIET as CMake ships FindGTest\nendif()\nif(GINKGO_BUILD_BENCHMARKS)\n    find_package(gflags 2.2.2 QUIET)\nendif()\nif(GINKGO_BUILD_TESTS OR GINKGO_BUILD_BENCHMARKS OR GINKGO_BUILD_EXAMPLES)\n    find_package(nlohmann_json 3.9.1 QUIET)\nendif()\n\n# System provided, third party libraries (not bundled!)\nset(GINKGO_HAVE_HWLOC 0)\nif(GINKGO_BUILD_HWLOC)\n    find_package(HWLOC 2.1 REQUIRED)\n    set(GINKGO_HAVE_HWLOC 1)\n    message(WARNING \"The GINKGO_BUILD_HWLOC option has no beneficial effect. Consider setting it to GINKGO_BUILD_HWLOC=OFF.\")\nendif()\n\nset(GINKGO_HAVE_PAPI_SDE 0)\nif(GINKGO_BUILD_PAPI_SDE)\n    find_package(PAPI 7.0.1.0 COMPONENTS sde)\n    if (PAPI_SDE_FOUND)\n        set(GINKGO_HAVE_PAPI_SDE 1)\n    else()\n        message(WARNING \"PAPI (SDE) could not be found. PAPI_SDE support will be disabled.\")\n        set(GINKGO_BUILD_PAPI_SDE OFF CACHE BOOL \"PAPI_SDE support was disabled because a system package could not be found.\" FORCE)\n    endif()\nendif()\n\n# Bundled third party libraries\nadd_subdirectory(third_party)    # Third-party tools and libraries\n\nif(MSVC)\n    if(BUILD_SHARED_LIBS)\n        set(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS TRUE)\n    else()\n        set(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS FALSE)\n    endif()\nendif()\n\nif(GINKGO_BUILD_SYCL)\n    ginkgo_extract_dpcpp_version(${CMAKE_CXX_COMPILER} GINKGO_DPCPP_MAJOR_VERSION __LIBSYCL_MAJOR_VERSION)\n    ginkgo_extract_dpcpp_version(${CMAKE_CXX_COMPILER} GINKGO_DPCPP_MINOR_VERSION __LIBSYCL_MINOR_VERSION)\n    ginkgo_extract_dpcpp_version(${CMAKE_CXX_COMPILER} GINKGO_DPCPP_VERSION __SYCL_COMPILER_VERSION)\nelse()\n    set(GINKGO_DPCPP_MAJOR_VERSION \"0\")\n    set(GINKGO_DPCPP_MINOR_VERSION \"0\")\nendif()\nconfigure_file(${Ginkgo_SOURCE_DIR}/include/ginkgo/config.hpp.in\n    ${Ginkgo_BINARY_DIR}/include/ginkgo/config.hpp @ONLY)\nconfigure_file(${Ginkgo_SOURCE_DIR}/include/ginkgo/extensions/kokkos/config.hpp.in\n        ${Ginkgo_BINARY_DIR}/include/ginkgo/extensions/kokkos/config.hpp\n        @ONLY\n)\n\n# Ginkgo core libraries\n# Needs to be first in order for `CMAKE_CUDA_DEVICE_LINK_EXECUTABLE` to be\n# propagated to the other parts of Ginkgo in case of building as static libraries\nadd_subdirectory(devices)        # Basic device functionalities. Always compiled.\nadd_subdirectory(common)         # Import list of unified kernel source files\nif(GINKGO_BUILD_CUDA)\n    add_subdirectory(cuda)       # High-performance kernels for NVIDIA GPUs\nendif()\nif(GINKGO_BUILD_REFERENCE)\n    add_subdirectory(reference)  # Reference kernel implementations\nendif()\nif(GINKGO_BUILD_HIP)\n    add_subdirectory(hip)        # High-performance kernels for AMD or NVIDIA GPUs\nendif()\nif(GINKGO_BUILD_SYCL)\n    add_subdirectory(dpcpp)        # High-performance DPC++ kernels\nendif()\nif(GINKGO_BUILD_OMP)\n    add_subdirectory(omp)        # High-performance omp kernels\nendif()\nadd_subdirectory(core)           # Core Ginkgo types and top-level functions\nadd_subdirectory(include)        # Public API self-contained check\nif(GINKGO_BUILD_TESTS)\n    add_subdirectory(test)       # Tests running on all executors\nendif()\n\n# Non core directories and targets\nadd_subdirectory(extensions)\n\nif(GINKGO_BUILD_EXAMPLES)\n    add_subdirectory(examples)\nendif()\n\nif(GINKGO_BUILD_BENCHMARKS)\n    add_subdirectory(benchmark)\nendif()\n\nif(GINKGO_DEVEL_TOOLS)\n    find_program(PRE_COMMIT pre-commit)\n    if(NOT PRE_COMMIT)\n        message(FATAL_ERROR \"The pre-commit command was not found. It is necessary if you want to commit changes to Ginkgo. \"\n                \"If that is not the case, set GINKGO_DEVEL_TOOLS=OFF. \"\n                \"Otherwise install pre-commit via pipx (or pip) using:\\n\"\n                \"    pipx install pre-commit\")\n    endif()\n\n    execute_process(COMMAND \"${PRE_COMMIT}\" \"install\"\n                    WORKING_DIRECTORY ${Ginkgo_SOURCE_DIR}\n                    RESULT_VARIABLE pre-commit-result\n                    OUTPUT_VARIABLE pre-commit-output\n                    ERROR_VARIABLE pre-commit-error)\n    if(pre-commit-result)\n        message(FATAL_ERROR\n                \"Failed to install the git hooks via pre-commit. Please check the error message:\\n\"\n                \"${pre-commit-output}\\n${pre-commit-error}\")\n    endif()\n    if(pre-commit-output MATCHES \"^Running in migration mode with existing hooks\")\n        message(WARNING\n                \"An existing git hook was encountered during `pre-commit install`. The old git hook \"\n                \"will also be executed. Consider removing it with `pre-commit install -f`\")\n    elseif(NOT pre-commit-output MATCHES \"^pre-commit installed at\")\n        message(WARNING\n                \"`pre-commit install` did not exit normally. Please check the output message:\\n\"\n                \"${pre-commit-output}\")\n    endif()\n\n    add_custom_target(format\n                      COMMAND bash -c \"${PRE_COMMIT} run\"\n                      WORKING_DIRECTORY ${Ginkgo_SOURCE_DIR}\n                      VERBATIM)\nendif()\n\n# MacOS needs to install bash, gnu-sed, findutils and coreutils\n# format_header needs clang-format 6.0.0+\nfind_program(BASH bash)\nif(NOT \"${BASH}\" STREQUAL \"BASH-NOTFOUND\" AND GINKGO_DEVEL_TOOLS)\n    add_custom_target(generate_ginkgo_header ALL\n        COMMAND ${Ginkgo_SOURCE_DIR}/dev_tools/scripts/update_ginkgo_header.sh\n        WORKING_DIRECTORY ${Ginkgo_SOURCE_DIR})\nendif()\nunset(BASH CACHE)\n\n\n# Installation\ninclude(cmake/information_helpers.cmake)\nginkgo_pkg_information()\nginkgo_git_information()\n\ninclude(cmake/get_info.cmake)\n\nif(GINKGO_BUILD_DOC)\n    add_subdirectory(doc)\nendif()\n\n\n\n# WINDOWS NVCC has \" inside the string, add escape character\n# to avoid config problem.\nginkgo_modify_flags(CMAKE_CUDA_FLAGS)\nginkgo_modify_flags(CMAKE_CUDA_FLAGS_DEBUG)\nginkgo_modify_flags(CMAKE_CUDA_FLAGS_RELEASE)\nginkgo_install()\nginkgo_export_binary_dir()\n\nset(GINKGO_TEST_INSTALL_SRC_DIR \"${Ginkgo_SOURCE_DIR}/test/test_install/\")\nset(GINKGO_TEST_INSTALL_BIN_DIR \"${Ginkgo_BINARY_DIR}/test/test_install/\")\nset(GINKGO_TEST_EXPORTBUILD_SRC_DIR \"${Ginkgo_SOURCE_DIR}/test/test_exportbuild/\")\nset(GINKGO_TEST_EXPORTBUILD_BIN_DIR \"${Ginkgo_BINARY_DIR}/test/test_exportbuild/\")\nset(GINKGO_TEST_PKGCONFIG_SRC_DIR \"${Ginkgo_SOURCE_DIR}/test/test_pkgconfig/\")\nset(GINKGO_TEST_PKGCONFIG_BIN_DIR \"${Ginkgo_BINARY_DIR}/test/test_pkgconfig/\")\nget_property(GINKGO_USE_MULTI_CONFIG GLOBAL PROPERTY GENERATOR_IS_MULTI_CONFIG)\n# GINKGO_CONFIG_PREFIX contains / in the end.\nset(GINKGO_CONFIG_PREFIX \"$<$<BOOL:${GINKGO_USE_MULTI_CONFIG}>:$<CONFIG>/>\")\nset(GINKGO_TEST_INSTALL_CMD ${GINKGO_TEST_INSTALL_BIN_DIR}/${GINKGO_CONFIG_PREFIX}test_install)\nset(GINKGO_TEST_EXPORTBUILD_CMD ${GINKGO_TEST_EXPORTBUILD_BIN_DIR}/${GINKGO_CONFIG_PREFIX}test_exportbuild)\nset(GINKGO_TEST_PKGCONFIG_CMD ${GINKGO_TEST_PKGCONFIG_BIN_DIR}/${GINKGO_CONFIG_PREFIX}test_pkgconfig)\nif(GINKGO_BUILD_CUDA)\n    set(GINKGO_TEST_INSTALL_CUDA_CMD ${GINKGO_TEST_INSTALL_BIN_DIR}/${GINKGO_CONFIG_PREFIX}test_install_cuda)\nendif()\nif(GINKGO_BUILD_HIP)\n    set(GINKGO_TEST_INSTALL_HIP_CMD ${GINKGO_TEST_INSTALL_BIN_DIR}/${GINKGO_CONFIG_PREFIX}test_install_hip)\nendif()\n\nfile(MAKE_DIRECTORY \"${GINKGO_TEST_INSTALL_BIN_DIR}\")\nfile(MAKE_DIRECTORY \"${GINKGO_TEST_EXPORTBUILD_BIN_DIR}\")\nset(TOOLSET \"\")\nif(NOT \"${CMAKE_GENERATOR_TOOLSET}\" STREQUAL \"\")\n    set(TOOLSET \"-T${CMAKE_GENERATOR_TOOLSET}\")\nendif()\nadd_custom_target(test_install\n    COMMAND ${CMAKE_COMMAND} -G${CMAKE_GENERATOR} ${TOOLSET}\n    -S${GINKGO_TEST_INSTALL_SRC_DIR}\n    -B${GINKGO_TEST_INSTALL_BIN_DIR}\n    -DCMAKE_BUILD_TYPE=${CMAKE_BUILD_TYPE}\n    -DCMAKE_PREFIX_PATH=${CMAKE_INSTALL_PREFIX}\n    -DCMAKE_CXX_COMPILER=${CMAKE_CXX_COMPILER}\n    -DCMAKE_CUDA_COMPILER=${CMAKE_CUDA_COMPILER}\n    -DCMAKE_CXX_FLAGS=${CMAKE_CXX_FLAGS}\n    # `--config cfg` is ignored by single-configuration generator.\n    # `$<CONFIG>` is always be the same as `CMAKE_BUILD_TYPE` in\n    # single-configuration generator.\n    COMMAND ${CMAKE_COMMAND}\n    --build ${GINKGO_TEST_INSTALL_BIN_DIR}\n    --config $<CONFIG>\n    COMMAND ${GINKGO_TEST_INSTALL_CMD}\n    COMMAND ${GINKGO_TEST_INSTALL_CUDA_CMD}\n    COMMAND ${GINKGO_TEST_INSTALL_HIP_CMD}\n    WORKING_DIRECTORY ${GINKGO_TEST_INSTALL_BIN_DIR}\n    COMMENT \"Running a test on the installed binaries. \"\n    \"This requires running `(sudo) make install` first.\")\n\nadd_custom_target(test_exportbuild\n    COMMAND ${CMAKE_COMMAND} -G${CMAKE_GENERATOR} ${TOOLSET}\n    -S${GINKGO_TEST_EXPORTBUILD_SRC_DIR}\n    -B${GINKGO_TEST_EXPORTBUILD_BIN_DIR}\n    -DCMAKE_CXX_COMPILER=${CMAKE_CXX_COMPILER}\n    -DCMAKE_CUDA_COMPILER=${CMAKE_CUDA_COMPILER}\n    -DCMAKE_CXX_FLAGS=${CMAKE_CXX_FLAGS}\n    -DGinkgo_ROOT=${Ginkgo_BINARY_DIR}\n    # `--config cfg` is ignored by single-configuration generator.\n    # `$<CONFIG>` is always be the same as `CMAKE_BUILD_TYPE` in\n    # single-configuration generator.\n    COMMAND ${CMAKE_COMMAND}\n    --build ${GINKGO_TEST_EXPORTBUILD_BIN_DIR}\n    --config $<CONFIG>\n    COMMAND ${GINKGO_TEST_EXPORTBUILD_CMD}\n    COMMENT \"Running a test on Ginkgo's exported build directory.\")\n\n# static linking with pkg-config is not possible with HIP, since\n# some linker information cannot be expressed in pkg-config files\nif (BUILD_SHARED_LIBS OR NOT GINKGO_BUILD_HIP)\n    add_custom_target(test_pkgconfig\n        COMMAND ${CMAKE_COMMAND} -G${CMAKE_GENERATOR} ${TOOLSET}\n        -S${GINKGO_TEST_PKGCONFIG_SRC_DIR}\n        -B${GINKGO_TEST_PKGCONFIG_BIN_DIR}\n        -DCMAKE_CXX_COMPILER=${CMAKE_CXX_COMPILER}\n        -DCMAKE_CUDA_COMPILER=${CMAKE_CUDA_COMPILER}\n        -DCMAKE_CXX_FLAGS=${CMAKE_CXX_FLAGS}\n        # `--config cfg` is ignored by single-configuration generator.\n        # `$<CONFIG>` is always be the same as `CMAKE_BUILD_TYPE` in\n        # single-configuration generator.\n        COMMAND ${CMAKE_COMMAND}\n        --build ${GINKGO_TEST_PKGCONFIG_BIN_DIR}\n        --config $<CONFIG>\n        COMMAND ${GINKGO_TEST_PKGCONFIG_CMD}\n        COMMENT \"Running a test on Ginkgo's PkgConfig\"\n        \"This requires installing Ginkgo first\")\nendif()\n\n\n# Setup CPack\nset(CPACK_PACKAGE_DESCRIPTION_FILE \"${Ginkgo_SOURCE_DIR}/README.md\")\nset(CPACK_RESOURCE_FILE_LICENSE \"${Ginkgo_SOURCE_DIR}/LICENSE\")\nset(CPACK_PACKAGE_ICON \"${Ginkgo_SOURCE_DIR}/assets/logo.png\")\nset(CPACK_PACKAGE_CONTACT \"ginkgo.library@gmail.com\")\ninclude(CPack)\n\n# And finally, print the configuration to screen:\nif(GINKGO_CONFIG_LOG_DETAILED)\n    FILE(READ ${PROJECT_BINARY_DIR}/detailed.log GINKGO_LOG_SUMMARY)\nelse()\n    FILE(READ ${PROJECT_BINARY_DIR}/minimal.log GINKGO_LOG_SUMMARY)\nendif()\nMESSAGE(STATUS \"${GINKGO_LOG_SUMMARY}\")\n\n# make sure no build files get committed accidentally\nif(NOT EXISTS ${CMAKE_CURRENT_BINARY_DIR}/.gitignore)\n    file(WRITE ${CMAKE_CURRENT_BINARY_DIR}/.gitignore \"*\")\nendif()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/gipptools",
            "repo_link": "https://git.gfz-potsdam.de/gipp/gipptools",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/glaes",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/geokit",
            "content": {
                "codemeta": "",
                "readme": "﻿|                                                          master                                                           |                                                          dev                                                           |\n| :-----------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------: |\n| [![Build Status](https://travis-ci.com/FZJ-IEK3-VSA/geokit.svg?branch=master)](https://travis-ci.com/FZJ-IEK3-VSA/geokit) | [![Build Status](https://travis-ci.com/FZJ-IEK3-VSA/geokit.svg?branch=dev)](https://travis-ci.com/FZJ-IEK3-VSA/geokit) |\n\n---\n\n<a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/FJZ_IEK-3_logo.svg?raw=True\" alt=\"Forschungszentrum Juelich Logo\" width=\"300px\"></a> \n\n# GeoKit - **Geo**spatial tool**kit** for Python\n\nGeoKit communicates directly with functions and objects within the Geospatial Data Abstraction Library (<a href=\"www.gdal.org\">GDAL</a>) and exposes them in such a way that is particularly useful for programmatic general purpose geospatial analyses.\nIt gives low overhead control of fundamental operations; such as reading, writing, and mutating geospatial data sets, manipulating and translating geometries, warping and resampling raster data, and much more.\nVia the RegionMask object, GeoKit even allows for seamless integration of information expressed across multiple geospatial datasets in many formats and reference systems into the context of a single region.\n\nGeoKit is not intended to replace the GDAL library, as only very small subset of GDAL's capabilities are exposed. Nor is it intended to compete with other libraries with similar functionalities.\nInstead GeoKit evolved in an ad hoc manner in order to realize the Geospatial Land Eligibility for Energy Systems (<a href=\"https://github.com/FZJ-IEK3-VSA/glaes\">GLAES</a>) model which is intended for rapid land eligibility analyses of renewable energy systems and is also available on GitHub.\nNevertheless, GeoKit quickly emerged as a general purpose GIS toolkit with capabilities far beyond computing land eligibility.\nTherefore, it is our pleasure to offer it to anyone who is interested in its use.\n\n[![DOI](https://zenodo.org/badge/114900977.svg)](https://zenodo.org/badge/latestdoi/114900977)\n\n## Features\n\n- Direct exposure of functions and objects in the GDAL library\n- Reading, writing, and manipulating raster and vector datasets\n- Translation between data formats and projection systems\n- Direct conversion of raster data into NumPy matrices\n\n## Installation\n\n### Installation via conda-forge\nThe easiest way to install GeoKit into a new environment is from `conda-forge` with:\n\n```bash\nconda create -n geokit -c conda-forge geokit\n```\n\nor into an existing environment with:\n```bash\nconda install -c conda-forge geokit\n```\n\n### Installation from a local folder\n\n1. First clone a local copy of the repository to your computer, and move into the created directory\n\n```\ngit clone https://github.com/FZJ-IEK3-VSA/geokit.git\ncd geokit\n```\n\n1. (Alternative) If you want to use the 'dev' branch (or another branch) then use:\n\n```\ngit checkout dev\n```\n\n2. When using [Anaconda](https://www.anaconda.com/) / [(Micro-)Mamba](https://mamba.readthedocs.io/en/latest/) (recommended), GeoKit should be installable to a new environment with:\n\n```\nconda env create --file requirements.yml\nconda activate geokit\npip install . --no-deps\n```\n\n2. (Alternative) Or into an existing environment with:\n\n```\nconda env update --file requirements.yml -n <ENVIRONMENT-NAME>\nconda activate geokit\npip install . --no-deps\n```\n\n2. (Alternative) If you want to install GeoKit in editable mode, and also with jupyter notebook and with testing functionalities use:\n\n```\nconda env create --file requirements-dev.yml\nconda activate geokit\npip install . --no-deps -e\n```\n\n## Examples\n\nSee the [Examples page](Examples/)\n\n## Docker\n\nWe are trying to get GeoKit to work within a Docker container. Try it out!\n\n- First pull the image with:\n\n```bash\ndocker pull sevberg/geokit:latest\n```\n\n- You can then start a basic python interpreter with:\n\n```bash\ndocker run -it sevberg/geokit:latest -c \"python\"\n```\n\n- Or you can start a jupyter notebook using:\n\n```bash\ndocker run -it \\\n    -p 8888:8888 \\\n    sevberg/geokit:latest \\\n    -c \"jupyter notebook --ip='*' --port=8888 --no-browser --allow-root\"\n```\n\n- Which can then be connected to at the address \"localhost:8888:<API-KEY>\"\n- The API Key can be found from the output of the earlier command\n\n* Finally, you might want to mount a volume to access geospatial data. For this you can use:\n\n```bash\ndocker run -it \\\n    --mount target=/notebooks,type=bind,src=<PATH-TO-DIRECTORY> \\\n    -p 8888:8888 \\\n    sevberg/geokit:latest  \\\n    -c \"jupyter notebook --notebook-dir=/notebooks --ip='*' --port=8888 --no-browser --allow-root\"\n```\n\n## License\n\nMIT License\n\nActive Developers: Julian Schönau, Rachel Maier, Christoph Winkler, Shitab Ishmam, David Franzmann, Julian Belina, Noah Pflugradt, Heidi Heinrichs, Jochen Linßen, Detlef Stolten \n\nAlumni: David Severin Ryberg, Martin Robinius, Stanley Risch\n\nYou should have received a copy of the MIT License along with this program.  \nIf not, see <https://opensource.org/licenses/MIT>\n\n## About Us\n\n<a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/iek3-square.png?raw=True\" alt=\"Institute image IEK-3\" width=\"280\" align=\"right\" style=\"margin:0px 10px\"/></a>\n\nWe are the <a href=\"https://www.fz-juelich.de/en/iek/iek-3\">Institute of Energy and Climate Research - Techno-economic Systems Analysis (IEK-3)</a> belonging to the <a href=\"https://www.fz-juelich.de/en\">Forschungszentrum Jülich</a>. Our interdisciplinary department's research is focusing on energy-related process and systems analyses. Data searches and system simulations are used to determine energy and mass balances, as well as to evaluate performance, emissions and costs of energy systems. The results are used for performing comparative assessment studies between the various systems. Our current priorities include the development of energy strategies, in accordance with the German Federal Government’s greenhouse gas reduction targets, by designing new infrastructures for sustainable and secure energy supply chains and by conducting cost analysis studies for integrating new technologies into future energy market frameworks.\n\n## Acknowledgment\n\nThis work was supported by the Helmholtz Association under the Joint Initiative [\"Energy System 2050   A Contribution of the Research Field Energy\"](https://www.helmholtz.de/en/research/energy/energy_system_2050/).\n\n<a href=\"https://www.helmholtz.de/en/\"><img src=\"https://www.helmholtz.de/fileadmin/user_upload/05_aktuelles/Marke_Design/logos/HG_LOGO_S_ENG_RGB.jpg\" alt=\"Helmholtz Logo\" width=\"200px\" style=\"float:right\"></a>\n\n",
                "dependencies": "from setuptools import setup, find_packages\n\nsetup(\n    name='geokit',\n    version='1.4.0',\n    author='GeoKit Developer Team',\n    url='https://github.com/FZJ-IEK3-VSA/geokit',\n    packages=find_packages(),\n    include_package_data=True,\n    install_requires=[\n        \"gdal>=2.4.0, ==3.4.*\",\n        \"numpy\",\n        \"descartes\",\n        \"pandas\",\n        \"scipy\",\n        \"matplotlib\",\n        \"smopy\",\n    ]\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/global-benchmark-database-gbd",
            "repo_link": "https://github.com/Udopia/gbd",
            "content": {
                "codemeta": "",
                "readme": "# Global Benchmark Database (GBD)\n\n[![DOI](https://zenodo.org/badge/141396410.svg)](https://zenodo.org/doi/10.5281/zenodo.10213943)\n\nGBD is a comprehensive suite of tools for provisioning and sustainably maintaining benchmark instances and their metadata for empirical research on hard algorithmic problem classes.\nFor an introduction to the GBD concept, the underlying data model, and specific use cases, please refer to our [2024 SAT Tool Paper](https://doi.org/10.4230/LIPIcs.SAT.2024.18).\n\n## GBD contributes data to your algorithmic evaluations\n\nGBD provides benchmark instance identifiers, feature extractors, and instance transformers for hard algorithmic problem domains, now including propositional satisfiability (SAT) and optimization (MaxSAT), and pseudo-Boolean optimization (PBO).\n\n## GBD solves several problems\n\n- benchmark instance identification\n- identification of equivalence classes of benchmark instances\n- distribution of benchmark instances and benchmark metadata\n- initialization and maintenance of instance feature databases\n- transformation algorithms for benchmark instances\n\nGBD provides an extensible set of problem domains, feature extractors, and instance transformers.\nFor a description of those currently supported, see the [GBDC documentation](https://udopia.github.io/gbdc/doc/Index.html).\nGBDC is a Python extension module for GBD's performance-critical code (written in C++), maintained in a separate [repository](https://github.com/Udopia/gbdc).\n\n## Installation and Configuration\n\n- Run `pip install gbd-tools`\n- Run `pip install gbdc` (optional, installation of extension module gbdc)\n- Obtain a GBD database, e.g. download [https://benchmark-database.de/getdatabase/meta.db](https://benchmark-database.de/getdatabase/meta.db).\n- Configure your environment by registering paths to databases like this `export GBD_DB=path/to/database1:path/to/database2`.\n- Test the command line interface with the `gbd info` and `gbd --help` commands.\n\n## GBD Interfaces\n\nGBD provides the command-line tool `gbd`, the web interface `gbd serve`, and the Python interface `gbd_core.api.GBD`.\n\n### GBD Command-Line Interface\n\nCentral commands in gbd are those for data access `gbd get` and database initialization `gbd init`.\nSee `gbd --help` for more commands.\nOnce a database is registered in the environment variable `GBD_DB`, the `gbd get` command can be used to access data.\nSee `gbd get --help` for more information.\n`gbd init` provides access to registered feature extractors, such as those provided by the `gdbc` extension module.\nAll initialization routines can be run in parallel, and resource limits can be set per process.\nSee `gbd init --help` for more information.\n\n### GBD Server\n\nThe GBD server can be started locally with gbd serve. Our instance of the GBD server is hosted at [https://benchmark-database.de/](https://benchmark-database.de/).\nYou can download benchmark instances and prebuilt feature databases from there.\n\n### GBD Python Interface\n\nThe GBD Python interface is used by all programs in the GBD ecosystem. Important here is the query command, which returns GBD data in the form of a Pandas dataframe for further analysis, as shown in the following example.\n\n```Python\nfrom gbd_core.api import GBD\nwith GBD(['path/to/database1', 'path/to/database2', ..] as gbd:\n    df = gbd.query(\"family = hardware-bmc\", resolve=['verified-result', 'runtime-kissat'])\n```\n\nScripts and use cases of GBD's Python interface are available on [https://udopia.github.io/gbdeval/](https://udopia.github.io/gbdeval/).\nThe [evaluation demo](https://udopia.github.io/gbdeval/demo_evaluation.html) demonstrates portfolio analysis and subsequent category-wise performance evaluation using the 2023 SAT competition data.\nThe [prediction demo](https://udopia.github.io/gbdeval/demo_prediction.html) demonstrates category prediction from instance features and subsequent feature importance evaluation.\n\n\n",
                "dependencies": "from setuptools import setup, find_packages\n\nsetup(name='gbd_tools',\n  version='4.9.7',\n  description='GBD Tools: Maintenance and Distribution of Benchmark Instances and their Attributes',\n  long_description=open('README.md', 'rt').read(),\n  long_description_content_type=\"text/markdown\",\n  url='https://github.com/Udopia/gbd',\n  author='Markus Iser',\n  author_email='markus.iser@kit.edu',\n  packages=[\n    \"gbd_core\", \n    \"gbd_init\",\n    \"gbd_server\"\n  ],\n  scripts=[\n    \"gbd.py\"\n  ],\n  include_package_data=True,\n  setup_requires=[\n    'wheel',\n    'setuptools'\n  ],\n  install_requires=[\n    'flask',\n    'tatsu',\n    'pandas',\n    'waitress',\n    'pebble',\n    'gbdc'\n  ],\n  install_obsoletes=['global-benchmark-database-tool'],\n  classifiers=[\n    \"License :: OSI Approved :: GNU General Public License v3 (GPLv3)\",\n    \"Programming Language :: Python :: 3\"\n  ],\n  entry_points={\n    \"console_scripts\": [\n        \"gbd = gbd:main\"\n    ]\n  }\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/gmgpolar",
            "repo_link": "https://github.com/mknaranja/GMGPolar",
            "content": {
                "codemeta": "",
                "readme": "GMGPolar\n=======\n\nGMGPolar is a performant geometric multigrid solver using implicit extrapolation to raise the convergence order. It is based on meshes in tensor- or product-format. GMGPolar's focus applications are geometries that can be described by polar or curvilinear coordinates for which suited smoothing procedures have been developed.\n\nIf using GMGPolar, please cite:\n\nM. J. Kühn, C. Kruse, U. Rüde. Implicitly extrapolated geometric multigrid on disk-like domains for the gyrokinetic Poisson equation from fusion plasma applications. Journal of Scientific Computing, 91 (28). Springer (2022). Link: https://link.springer.com/article/10.1007/s10915-022-01802-1\n\nTested plateforms\n-----------------\n\nWorking\n=======\n\n* Linux x86_64 with GNU 9.3.0  compilers.    \n\nObtaining the source code\n-------------------------\n\nThe GmgPolar Solver does not require any external libraries.\nIt is possible to link the code with the sparse direct solver ``MUMPS``.\n\n* ``MUMPS`` is optional. However, it is absolutely recommended if large grids are considered.\n  Otherwise, the nonoptimal backup solver will be used for factorization of the matrices and will\n  slow down the setup phase significantly. To use it, compile the code with option -DGMGPOLAR_USE_MUMPS. \n  It is recommended to use the latest version (currently 5.4.1) but any version ulterior \n  to 5.1.0 should be okay. MUMPS is available freely on demand on the MUMPS consortium \n  website \"mumps-solver.org\".\n\t\nThe installation can be done by typing the following commands in your terminal\n\n    # download the latest stable version\n    # it will create a directory named GMGPolar\n\n    git clone https://github.com/mknaranja/GMGPolar\n\nNow that everything is ready, we can compile the solver.\nEdit the file ``Makefile.in`` so that it reflects your configuration (path to libraries, file \nnames, etc).\n\n\nBuilding the library\n--------------------\n          \nThe build process is done using CMake:\n\n    # Create build directory\n    mkdir -p build && cd build\n    # Configure\n    cmake ..\n    # Build\n    cmake --build .\n\nCurrently, the default build process only supports gnu compiler although Intel compiler\nhas been successfully tested for some configurations.\n\nRunning GmgPolar\n------------\n\nYou can run the solver without having to write a code (as we do in the next section). After building \nthe library, a binary is created called ``./build/gmgpolar_simulation``, it takes parameters directly from command-line.\n\n   \n    # To try GmgPolar on a small problem size, without having to write any code,\n    # ./build/gmgpolar_simulation uses default parameters with a grid 49 x 64.\n\n    ./build/gmgpolar_simulation\n\n    # For more details on the available parameters, see the main.cpp source code.\n    # You can control the number of OpenMP threads used by changing the environment variable.\n    # Note that only MUMPS is parallelized at the moment.\n\n    export OMP_NUM_THREADS=4\n  \n\nExecuting an example\n-------------------------------------------------\n\nOnce the library is built, you can run the examples:\n\n    # the verbose option defines the extent of the output\n\n    ./build/gmgpolar_simulation --verbose 2\n\n    # the option --debug 1 turns on internal debugging and compares the results of the C++ code \n    # with the results from the previous matlab implementation.\n   \n    ./build/gmgpolar_simulation --debug 1\n\n\nIssue tracker\n-------------\nIf you find any bug, didn't understand a step in the documentation, or if you\nhave a feature request, submit your issue on our\n`Issue Tracker`: https://github.com/mknaranja/GMGPolar/issues\nby giving:\n\n- reproducible parameters\n- computing environment (compiler, etc.)\n\n\nRelease Notes\n-------------\n* GmgPolar 1.0\n1) Working multigrid cycle\n2) In-house solver and possibility to link with MUMPS for the smoothing and coarse grid solution\n3) Extrapolation strategies:\n   \n\ta. No extrapolation (--extrapolation 0)\n\n\tb. Default implicit extrapolation (--extrapolation 1)\n\n\tc. Non-default implicit extrapolation with smoothing of all nodes on the finest level [experimental, use with care, convergence cannot be observed with residual] (--extrapolation 2)\n6) Optimization of apply_A / build_rhs / apply_prolongation / build_Asc / apply_Asc_ortho\n\n",
                "dependencies": "cmake_minimum_required(VERSION 3.16.3)\n\nproject(GMGPolar VERSION 1.0.0)\n\noption(GMGPOLAR_BUILD_TESTS \"Build GMGPolar unit tests.\" ON)\noption(GMGPOLAR_USE_MUMPS \"Use MUMPS to compute matrix factorizations.\" OFF)\noption(GMGPOLAR_USE_LIKWID \"Use LIKWID to measure code (regions).\" OFF)\n\n\nset(CMAKE_MODULE_PATH \"${PROJECT_SOURCE_DIR}/cmake\" ${CMAKE_MODULE_PATH})\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\n\nset(CMAKE_CXX_FLAGS \"\")\nset(CMAKE_LINKER_FLAGS \"\")\n\nadd_subdirectory(src)\n\n# code coverage analysis\n# Note: this only works under linux and with make\n# Ninja creates different directory names which do not work together with this scrupt\n# as STREQUAL is case-sensitive https://github.com/TriBITSPub/TriBITS/issues/131, also allow DEBUG as accepted input\noption(GMGPOLAR_TEST_COVERAGE \"Enable GCov coverage analysis (adds a 'coverage' target)\" OFF)\n\nif(CMAKE_BUILD_TYPE STREQUAL \"Debug\" OR CMAKE_BUILD_TYPE STREQUAL \"DEBUG\")\n    if(GMGPOLAR_TEST_COVERAGE)\n        message(STATUS \"Coverage enabled\")\n        include(CodeCoverage)\n        append_coverage_compiler_flags()\n        setup_target_for_coverage_lcov(\n            NAME coverage\n            EXECUTABLE tests/gmgpolar_tests\n            EXCLUDE \"${CMAKE_SOURCE_DIR}/tests*\" \"${CMAKE_SOURCE_DIR}/src/test_cases*\" \"${CMAKE_BINARY_DIR}/*\" \"/usr*\"\n        )\n    endif()\nendif()\n\n\nadd_library(GMGPolar ${SOURCES_SRC})\n\nadd_executable(gmgpolar_simulation ./src/main.cpp)\n\nconfigure_file(${CMAKE_SOURCE_DIR}/include/config_internal.h.in ${CMAKE_SOURCE_DIR}/include/config_internal.h)\n\ntarget_include_directories(gmgpolar_simulation PRIVATE ${CMAKE_SOURCE_DIR}/include ${CMAKE_SOURCE_DIR}/include/test_cases )\ntarget_include_directories(GMGPolar PRIVATE ${CMAKE_SOURCE_DIR}/include ${CMAKE_SOURCE_DIR}/include/test_cases )\n\nif(GMGPOLAR_USE_LIKWID)\n\n    find_package(LIKWID REQUIRED)\n\n    target_include_directories(GMGPolar PUBLIC ${LIKWID_INCLUDE_DIRS})\n    target_link_libraries(GMGPolar PUBLIC ${LIKWID_LIBRARIES})\n    target_compile_definitions(GMGPolar PUBLIC \"-DLIKWID_PERFMON\")\n\nendif()\n\n\n\nif(GMGPOLAR_USE_MUMPS)\n\n    set(INC_DIRS\n        /home/kueh_mj/.spack/rev.23.05/install/linux-rocky8-zen2/gcc-10.4.0/mumps-5.4.1-fftqkl/include\n        /sw/rev/23.05/linux-rocky8-zen2/gcc-10.4.0/metis-5.1.0-akhgsf/include\n    )\n\n    set(LIB_DIRS\n        /home/kueh_mj/.spack/rev.23.05/install/linux-rocky8-zen2/gcc-10.4.0/mumps-5.4.1-fftqkl/lib\n        /sw/rev/23.05/linux-rocky8-zen2/gcc-10.4.0/metis-5.1.0-akhgsf/lib\n    )\n\n    include_directories(\n        ${INC_DIRS}\n    )\n\n    target_link_directories(\n        GMGPolar\n        PUBLIC\n        ${LIB_DIRS}\n    )\n\n    set(LIBS\n        mpiseq\n        dmumps\n        mumps_common\n        metis\n    )\n\n    target_link_libraries(\n        GMGPolar\n        PUBLIC\n        ${LIBS}\n    ) \nendif()\n\n\nfind_package(OpenMP)\n\n#currently works on GNU compiler\nif((CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\") AND (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 7))\n\n    string(APPEND CMAKE_CXX_FLAGS \" -O2 -Wall -MMD -MP -Wwrite-strings\")\n    string(APPEND CMAKE_LINKER_FLAGS \" -O2 -Wall -MMD -MP -Wwrite-strings\")\n\n    if(OPENMP_FOUND)\n        string(APPEND CMAKE_CXX_FLAGS \" -fopenmp\")\n        string(APPEND CMAKE_LINKER_FLAGS \" -fopenmp\")\n        \n    else()\n        message(FATAL_ERROR \"OpenMP needed\")\n    endif()\nelse()\n    message(FATAL_ERROR \"Please use GNU compiler or change CMakeLists manually\")\nendif()\n\n\ntarget_link_libraries(gmgpolar_simulation PRIVATE GMGPolar)\n\n\ninclude(thirdparty/CMakeLists.txt)\nadd_subdirectory(tests)\n\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/goac",
            "repo_link": "https://iffgit.fz-juelich.de/k.koester/goac",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/golem",
            "repo_link": "https://github.com/ajacquey/golem",
            "content": {
                "codemeta": "",
                "readme": "<h1 align=\"center\">\n  <br>\n  <a href=\"https://github.com/ajacquey/Golem\"><img src=\"images/golem_logo.png\" alt=\"GOLEM\" width=\"600\"></a>\n  <br>\n  A MOOSE-based application\n  <br>\n</h1>\n\n<h4 align=\"center\">A numerical simulator for modelling coupled THM processes in faulted geothermal reservoirs based on <a href=\"http://mooseframework.org/\" target=\"blank\">MOOSE</a>.</h4>\n\n<p align=\"center\">\n  <a href=\"LICENSE\">\n    <img src=\"https://img.shields.io/badge/license-GPLv3-blue.svg\"\n         alt=\"GitHub License\">\n  </a>\n  <a href=\"https://gitter.im/Golem-Moose/golem\">\n    <img src=\"https://img.shields.io/gitter/room/nwjs/nw.js.svg\"\n         alt=\"Gitter\">\n  </a>\n  <a href=\"https://zenodo.org/record/999401#.Wc5NqBdx1pg\">\n    <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.999401.svg\"\n         alt=\"DOI\">\n  </a>\n</p>\n\n## About\nGOLEM is a numerical simulator for modelling coupled Thermo-Hydro-Mechanical processes in faulted geothermal reservoirs.\nThe simulator is developed by [Antoine Jacquey](http://www.gfz-potsdam.de/en/staff/antoine-jacquey/) <a href=\"https://orcid.org/0000-0002-6259-4305\" target=\"orcid.widget\" rel=\"noopener noreferrer\" style=\"vertical-align:top;\"><img src=\"https://orcid.org/sites/default/files/images/orcid_16x16.png\" style=\"width:1em;margin-right:.5em;\" alt=\"ORCID iD icon\"></a><a href=\"https://github.com/ajacquey/\" target=\"github.widget\" rel=\"noopener noreferrer\" style=\"vertical-align:top;\"><img src=\"images/GitHub-Mark-32px.png\" width=\"16\" margin-right=\".5em;\" alt=\"GitHub icon id\"></a> and [Mauro Cacace](http://www.gfz-potsdam.de/en/section/basin-modeling/staff/profil/mauro-cacace/) <a href=\"https://orcid.org/0000-0001-6101-9918\" target=\"orcid.widget\" rel=\"noopener noreferrer\" style=\"vertical-align:top;\"><img src=\"https://orcid.org/sites/default/files/images/orcid_16x16.png\" style=\"width:1em;margin-right:.5em;\" alt=\"ORCID iD icon\"></a><a href=\"https://github.com/mcacace\" target=\"github.widget\" rel=\"noopener noreferrer\" style=\"vertical-align:top;\"><img src=\"images/GitHub-Mark-32px.png\" width=\"16\" margin-right=\".5em;\" alt=\"GitHub icon id\"></a> at the [GFZ German Research Centre for Geosciences](http://www.gfz-potsdam.de/en/home/) from the section [Basin Modelling](http://www.gfz-potsdam.de/en/section/basin-modeling/).\n\n\nGOLEM is a MOOSE-based application. Visit the [MOOSE framework](http://mooseframework.org) page for more information.\n\n## Licence\nGOLEM is distributed under the [GNU GENERAL PUBLIC LICENSE v3](https://github.com/ajacquey/Golem/blob/master/LICENSE).\n\n\n## Getting Started\n\n#### Minimum System Requirements\nThe following system requirements are from the MOOSE framework (see [Getting Started](http://mooseframework.inl.gov/getting_started/) for more information):\n* Compiler: C++11 Compliant GCC 4.8.4, Clang 3.4.0, Intel20130607\n* Python 2.7+\n* Memory: 16 GBs (debug builds)\n* Processor: 64-bit x86\n* Disk: 30 GBs\n* OS: UNIX compatible (OS X, most flavors of Linux)\n\n#### 1. Setting Up a MOOSE Installation\nTo install GOLEM, you need first to have a working and up-to-date installation of the MOOSE framework.  \nTo do so, please visit the [Getting Started](http://mooseframework.inl.gov/getting_started/) page of the MOOSE framework and follow the instructions. If you encounter difficulties at this step, you can ask for help on the [MOOSE-users Google group](https://groups.google.com/forum/#!forum/moose-users).\n\n#### 2. Clone GOLEM\nGOLEM can be cloned directly from [GitHub](https://github.com/ajacquey/Golem) using [Git](https://git-scm.com/). In the following, we refer to the directory `projects` which you created during the MOOSE installation (by default `~/projects`):  \n\n    cd ~/projects\n    git clone https://github.com/ajacquey/Golem.git\n    cd ~/projects/golem\n    git checkout master\n\n*Note: the \"master\" branch of GOLEM is the \"stable\" branch which is updated only if all tests are passing.*\n\n#### 3. Compile GOLEM\nYou can compile GOLEM by following these instructions:\n\n    cd ~/projects/golem\n    make -j4\n\n#### 4. Test GOLEM\nTo make sure that everything was installed properly, you can run the tests suite of GOLEM:\n\n    cd ~/projects/golem\n    ./run_tests -j2\n\nIf all the tests passed, then your installation is working properly. You can now use the GOLEM simulator!\n\n## Usage\nTo run GOLEM from the command line with multiple processors, use the following command:\n\n    mpiexec -n <nprocs> ~/projects/golem/golem-opt -i <input-file>\n\nWhere `<nprocs>` is the number of processors you want to use and `<input-file>` is the path to your input file (extension `.i`).  \n\nInformation about the structure of the GOLEM input files can be found in the documentation (link to follow).\n## Cite\n\nIf you use GOLEM for your work please cite:\n* This repository:  \nAntoine B. Jacquey, & Mauro Cacace. (2017, September 29). GOLEM, a MOOSE-based application. Zenodo. http://doi.org/10.5281/zenodo.999401\n* The publication presenting GOLEM:  \n Cacace, M. and Jacquey, A. B.: Flexible parallel implicit modelling of coupled thermal–hydraulic–mechanical processes in fractured rocks, Solid Earth, 8, 921-941, https://doi.org/10.5194/se-8-921-2017, 2017.  \n\n\nPlease read the [CITATION](https://github.com/ajacquey/Golem/blob/master/CITATION) file for more information.\n\n## Publications using GOLEM\n\n* Freymark, J., Bott, J., Cacace, M., Ziegler, M., Scheck-Wenderoth, M.: Influence of the Main Border Faults on the 3D Hydraulic Field of the Central Upper Rhine Graben, *Geofluids*, 2019.\n* Blöcher, G.,  Cacace, M.,  Jacquey, A. B.,  Zang, A.,  Heidbach, O.,  Hofmann, H.,  Kluge, C.,  Zimmermann, G.: Evaluating Micro-Seismic Events Triggered by Reservoir Operations at the Geothermal Site of Groß Schönebeck (Germany), *Rock Mechanics and Rock Engineering*, 2018.\n* Jacquey, A. B.,  Urpi, L.,  Cacace, M.,  Blöcher, G.,  Zimmermann, G.,  Scheck-Wenderoth, M.: Far field poroelastic response of geothermal reservoirs to hydraulic stimulation treatment: Theory and application at the Groß Schönebeck geothermal research facility, *International Journal of Rock Mechanics and Mining Sciences*, 2018.\n* Peters, E., Blöcher, G., Salimzadeh, S., Egberts, P. J. P., Cacace, M.: Modelling of multi-lateral well geometries for geothermal applications, *Advances in Geosciences*, 2018.\n* Magri, F., Cacace, M., Fischer, T., Kolditz, O., Wang, W., Watanabe, N.: Thermal convection of viscous fluids in a faulted system: 3D benchmark for numerical codes, *Energy Procedia*, 2017.\n* Cacace, M. and Jacquey, A. B.: Flexible parallel implicit modelling of coupled Thermal-Hydraulic-Mechanical processes in fractured rocks, Solid Earth, 2017.\n* Jacquey, A. B.: Coupled Thermo-Hydro-Mechanical Processes in Geothermal Reservoirs: a Multiphysic and Multiscale Approach Linking Geology and 3D Numerical Modelling, PhD thesis, RWTH Aachen, 2017.\n* Jacquey, A. B., Cacace, M., Blöcher, G.: Modelling coupled fluid flow and heat transfer in fractured reservoirs: description of a 3D benchmark numerical case, Energy Procedia, 2017.\n* Jacquey, A. B., Cacace, M., Blöcher, G., Milsch, H., Deon, F., Scheck-Wenderoth, M.: Processes Responsible for Localized Deformation within Porous Rocks: Insights from Laboratory Experiments and Numerical Modelling, 6th Biot Conference on Poromechanics, Paris 2017.\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/gravis",
            "repo_link": "https://git.gfz-potsdam.de/gravis",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/gr-framework",
            "repo_link": "https://github.com/sciapp/gr",
            "content": {
                "codemeta": "",
                "readme": "GR - a universal framework for visualization applications\n=========================================================\n\n[![MIT license](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE.md)\n[![GitHub tag](https://img.shields.io/github/tag/jheinen/gr.svg)](https://github.com/jheinen/gr/releases)\n[![PyPI version](https://img.shields.io/pypi/v/gr.svg)](https://pypi.python.org/pypi/gr)\n[![DOI](https://zenodo.org/badge/17747322.svg)](https://zenodo.org/badge/latestdoi/17747322)\n\n*GR* is a universal framework for cross-platform visualization applications.\nIt offers developers a compact, portable and consistent graphics library for\ntheir programs. Applications range from publication quality 2D graphs to the\nrepresentation of complex 3D scenes.\n\n*GR* is essentially based on an implementation of a Graphical Kernel System (GKS).\nAs a self-contained system it can quickly and easily be integrated into existing\napplications (i.e. using the `ctypes` mechanism in Python or `ccall` in Julia).\n\nThe *GR* framework can be used in imperative programming systems or integrated\ninto modern object-oriented systems, in particular those based on GUI toolkits.\n*GR* is characterized by its high interoperability and can be used with modern\nweb technologies. The *GR* framework is especially suitable for real-time\nor signal processing environments.\n\n*GR* was developed by the Scientific IT-Systems group at the Peter Grünberg\nInstitute at Forschunsgzentrum Jülich. The main development has been done\nby Josef Heinen who currently maintains the software, but there are other\ndevelopers who currently make valuable contributions. Special thanks to\nFlorian Rhiem (*GR3*) and Christian Felder (qtgr, setup.py).\n\nStarting with release 0.6 *GR* can be used as a backend\nfor [Matplotlib](http://matplotlib.org) and significantly improve\nthe performance of existing Matplotlib or PyPlot applications written\nin Python or Julia, respectively.\nIn [this](http://gr-framework.org/tutorials/matplotlib.html) tutorial\nsection you can find some examples.\n\nBeginning with version 0.10.0 *GR* supports inline graphics which shows\nup in IPython's Qt Console or interactive computing environments for *Python*\nand *Julia*, such as [IPython](http://ipython.org) and\n[Jupyter](https://jupyter.org). An interesting example can be found\n[here](http://pgi-jcns.fz-juelich.de/pub/doc/700K_460.html).\n\n## Installation and Getting Started\n\nTo install *GR* and try it using *Python*, *Julia* or *C*, please see the corresponding documentation:\n\n- [Python package gr](https://gr-framework.org/python.html)\n- [Julia package GR](https://gr-framework.org/julia.html)\n- [C library GR](https://gr-framework.org/c.html)\n- [Ruby package GR](https://github.com/red-data-tools/GR.rb)\n\n## Documentation\n\nYou can find more information about *GR* on the [GR home page](http://gr-framework.org).\n\n## Contributing\n\nIf you want to improve *GR*, please read the [contribution guide](https://github.com/sciapp/gr/blob/develop/CONTRIBUTING.md) for a few notes on how to report issues or submit changes.\n\n## Support\n\nIf you have any questions about *GR* or run into any issues setting up or running GR, please [open an issue on GitHub](https://github.com/sciapp/gr/issues/new), either in this repo or in the repo for the language binding you are using ([Python](https://github.com/sciapp/python-gr/issues/new), [Julia](https://github.com/jheinen/GR.jl/issues/new), [Ruby](https://github.com/red-data-tools/GR.rb/issues/new)).\n\n",
                "dependencies": "cmake_minimum_required(VERSION 3.1 FATAL_ERROR)\n\nlist(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\")\ninclude(GetVersionFromGit)\nget_version_from_git(GR_VERSION GR_VERSION_FULL)\n\nproject(\n  GR\n  VERSION ${GR_VERSION}\n  LANGUAGES C CXX\n)\n\ninclude(GNUInstallDirs)\ninclude(CheckCXXCompilerFlag)\n\n# Honor `C_VISIBILITY_PRESET hidden` and `CXX_VISIBILITY_PRESET hidden` in static libraries\ncmake_policy(SET CMP0063 NEW)\n\ncheck_cxx_compiler_flag(\"-Werror=implicit\" ERROR_IMPLICIT_SUPPORTED)\nif(ERROR_IMPLICIT_SUPPORTED)\n  set(COMPILER_OPTION_ERROR_IMPLICIT\n      \"-Werror=implicit\"\n      CACHE INTERNAL \"Compiler flag for generating errors on implicit declarations\"\n  )\nelse()\n  set(COMPILER_OPTION_ERROR_IMPLICIT\n      \"\"\n      CACHE INTERNAL \"Compiler flag for generating errors on implicit declarations\"\n  )\nendif()\n\nif(UNIX)\n  if(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)\n    set(CMAKE_INSTALL_PREFIX\n        \"/usr/local/gr\"\n        CACHE PATH \"GR install prefix\" FORCE\n    )\n  endif()\nendif()\nset(GR_DIRECTORY\n    \"${CMAKE_INSTALL_PREFIX}\"\n    CACHE STRING \"Default value for GRDIR\"\n)\noption(GR_BUILD_DEMOS \"Build demos for GR\" OFF)\noption(GR_BUILD_GKSM \"Build GKS metafile reader for GR\" OFF)\noption(GR_INSTALL \"Create installation target for GR\" ON)\noption(GR_USE_BUNDLED_LIBRARIES \"Use thirdparty libraries bundled with GR\" OFF)\noption(GR_MANUAL_MOC_AND_RCC \"Manually run moc and rcc instead of relying on AUTOMOC and AUTORCC\" OFF)\noption(GR_FIND_QT5_BY_VARIABLES \"Find Qt5 based on the variables Qt5_INCLUDE_DIR and Qt5_LIBRARY_DIR\" OFF)\noption(GR_PREFER_XCODEBUILD \"Prefer xcodebuild to build macOS applications if available\" ON)\n\nif(GR_USE_BUNDLED_LIBRARIES)\n  list(APPEND CMAKE_FIND_ROOT_PATH \"${CMAKE_CURRENT_LIST_DIR}/3rdparty/build/\")\n  set(GR_THIRDPARTY_LIBRARY_PREFIX ${CMAKE_STATIC_LIBRARY_PREFIX})\n  set(GR_THIRDPARTY_LIBRARY_SUFFIX ${CMAKE_STATIC_LIBRARY_SUFFIX})\nelse()\n  set(GR_THIRDPARTY_LIBRARY_PREFIX ${CMAKE_SHARED_LIBRARY_PREFIX})\n  set(GR_THIRDPARTY_LIBRARY_SUFFIX ${CMAKE_SHARED_LIBRARY_SUFFIX})\nendif()\n\nif(GR_FIND_QT5_BY_VARIABLES)\n  list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake/optional/qt5\")\nendif()\n\nif(WIN32)\n  set(GR_PLUGIN_SUFFIX \".dll\")\nelse()\n  set(GR_PLUGIN_SUFFIX \".so\")\nendif()\n\nif(APPLE)\n  set(DEFAULT_SHARED_LIBRARY_SUFFIX \".dylib\")\nelse()\n  set(DEFAULT_SHARED_LIBRARY_SUFFIX \"${GR_PLUGIN_SUFFIX}\")\nendif()\n\nset(GR_SHARED_LIBRARY_SUFFIX\n    \"${DEFAULT_SHARED_LIBRARY_SUFFIX}\"\n    CACHE STRING \"File suffix for shared libraries\"\n)\n\nset(CMAKE_FIND_PACKAGE_PREFER_CONFIG\n    TRUE\n    CACHE BOOL \"\"\n)\n\n# Find the following packages always in system locations even if `GR_USE_BUNDLED_LIBRARIES` is set, because they are not\n# located in 3rdparty\nfind_package(X11)\nfind_package(Fontconfig)\nfind_package(OpenGL OPTIONAL_COMPONENTS OpenGL)\nif(${CMAKE_VERSION} VERSION_GREATER \"3.16.0\")\n  find_package(\n    Qt6\n    OPTIONAL_COMPONENTS\n      Widgets\n      Core\n      Network\n      Gui\n      PrintSupport\n  )\nendif()\nfind_package(\n  Qt5\n  OPTIONAL_COMPONENTS\n    Widgets\n    Core\n    Network\n    Gui\n    PrintSupport\n)\nfind_package(Qt4)\n\n# Find the following packages only in 3rdparty, if `GR_USE_BUNDLED_LIBRARIES` is set\nif(GR_USE_BUNDLED_LIBRARIES)\n  set(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)\n  set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)\n  set(CMAKE_FIND_ROOT_PATH_MODE_PACKAGE ONLY)\n  set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM ONLY)\nendif()\nfind_package(Freetype)\nif(TARGET freetype AND NOT TARGET Freetype::Freetype)\n  add_library(Freetype::Freetype ALIAS freetype)\nendif()\nfind_package(Jpeg REQUIRED)\nfind_package(Libpng REQUIRED)\nfind_package(ZLIB REQUIRED)\nfind_package(Qhull REQUIRED)\nif(TARGET Qhull::qhullstatic_r AND NOT TARGET Qhull::qhull_r)\n  add_library(Qhull::qhull_r ALIAS Qhull::qhullstatic_r)\nendif()\nfind_package(Tiff)\nfind_package(Ffmpeg 57.48.100)\nfind_package(glfw3)\nif(TARGET glfw AND NOT TARGET Glfw::Glfw)\n  add_library(Glfw::Glfw ALIAS glfw)\nendif()\nfind_package(ZeroMQ)\nif(NOT TARGET ZeroMQ::ZeroMQ)\n  if(TARGET libzmq)\n    add_library(ZeroMQ::ZeroMQ ALIAS libzmq)\n  elseif(TARGET libzmq-static)\n    add_library(ZeroMQ::ZeroMQ ALIAS libzmq-static)\n    get_target_property(ZeroMQ_LIBRARY libzmq-static LOCATION)\n  endif()\nendif()\nfind_package(Cairo)\nfind_package(Agg)\nfind_package(Gs)\nfind_package(XercesC)\n\nif(APPLE)\n  set(INSTALL_RPATH \"${GR_DIRECTORY}/lib/;@loader_path/.\")\nelse()\n  set(INSTALL_RPATH \"${GR_DIRECTORY}/lib/;$ORIGIN/.\")\nendif()\n\nif(Qt4_FOUND\n   OR (Qt5Widgets_FOUND\n       AND Qt5Core_FOUND\n       AND Qt5Network_FOUND\n      )\n   OR (Qt6Widgets_FOUND\n       AND Qt6Core_FOUND\n       AND Qt6Network_FOUND\n      )\n)\n  if(GR_MANUAL_MOC_AND_RCC)\n    if(NOT QT_MOC_EXECUTABLE)\n      find_program(QT_MOC_EXECUTABLE moc CMAKE_FIND_ROOT_PATH_BOTH)\n    endif()\n    if(NOT QT_MOC_EXECUTABLE)\n      message(FATAL_ERROR \"Could not find moc but GR_MANUAL_MOC_AND_RCC is set.\")\n    endif()\n    if(NOT QT_RCC_EXECUTABLE)\n      find_program(QT_RCC_EXECUTABLE rcc CMAKE_FIND_ROOT_PATH_BOTH)\n    endif()\n    if(NOT QT_RCC_EXECUTABLE)\n      message(FATAL_ERROR \"Could not find rcc but GR_MANUAL_MOC_AND_RCC is set.\")\n    endif()\n  endif()\nendif()\nif(Qt4_FOUND)\n  set(QT4_MOC_INCLUDE_FLAGS \"\")\n  foreach(DIR IN LISTS QT_INCLUDE_DIR)\n    set(QT4_MOC_INCLUDE_FLAGS ${QT4_MOC_INCLUDE_FLAGS} -I${DIR})\n  endforeach()\n  if(NOT DEFINED Qt4_LIBRARY_DIR)\n    get_filename_component(Qt4_LIBRARY_DIR \"${QT_LIBRARY_DIR}/../..\" ABSOLUTE)\n  endif()\nendif()\nif(Qt5Widgets_FOUND\n   AND Qt5Core_FOUND\n   AND Qt5Network_FOUND\n)\n  set(QT5_MOC_INCLUDE_FLAGS \"\")\n  foreach(DIR IN LISTS Qt5Core_INCLUDE_DIRS Qt5Gui_INCLUDE_DIRS Qt5Widgets_INCLUDE_DIRS)\n    set(QT5_MOC_INCLUDE_FLAGS ${QT5_MOC_INCLUDE_FLAGS} -I${DIR})\n  endforeach()\n  if(NOT DEFINED Qt5_LIBRARY_DIR)\n    get_filename_component(Qt5_LIBRARY_DIR \"${Qt5_DIR}/../..\" ABSOLUTE)\n  endif()\nendif()\nif(Qt6Widgets_FOUND\n   AND Qt6Core_FOUND\n   AND Qt6Network_FOUND\n)\n  set(QT6_MOC_INCLUDE_FLAGS \"\")\n  foreach(DIR IN LISTS Qt6Core_INCLUDE_DIRS Qt6Gui_INCLUDE_DIRS Qt6Widgets_INCLUDE_DIRS)\n    set(QT6_MOC_INCLUDE_FLAGS ${QT6_MOC_INCLUDE_FLAGS} -I${DIR})\n  endforeach()\n  if(NOT DEFINED Qt6_LIBRARY_DIR)\n    get_filename_component(Qt6_LIBRARY_DIR \"${Qt6_DIR}/../..\" ABSOLUTE)\n  endif()\nendif()\n\nif(X11_FOUND)\n  # older versions of FindX11.cmake set variables but do not create targets\n  if(NOT TARGET X11::X11\n     AND X11_INCLUDE_DIR\n     AND X11_X11_LIB\n     AND X11_Xft_LIB\n  )\n    add_library(X11::X11 UNKNOWN IMPORTED)\n    set_target_properties(\n      X11::X11\n      PROPERTIES INTERFACE_INCLUDE_DIRECTORIES \"${X11_INCLUDE_DIR}/\"\n                 IMPORTED_LINK_INTERFACE_LANGUAGES \"C\"\n                 IMPORTED_LOCATION \"${X11_X11_LIB}\"\n                 INTERFACE_LINK_LIBRARIES \"${X11_Xft_LIB}\"\n    )\n  endif()\n  if(NOT TARGET X11::Xt\n     AND X11_INCLUDE_DIR\n     AND X11_Xt_LIB\n     AND TARGET X11::X11\n  )\n    add_library(X11::Xt UNKNOWN IMPORTED)\n    set_target_properties(\n      X11::Xt\n      PROPERTIES INTERFACE_INCLUDE_DIRECTORIES \"${X11_INCLUDE_DIR}/\"\n                 IMPORTED_LINK_INTERFACE_LANGUAGES \"C\"\n                 IMPORTED_LOCATION \"${X11_Xt_LIB}\"\n                 INTERFACE_LINK_LIBRARIES \"X11::X11\"\n    )\n  endif()\nendif()\n\nset(GKS_SOURCES\n    lib/gks/afm.c\n    lib/gks/font.c\n    lib/gks/socket.c\n    lib/gks/ft.c\n    lib/gks/malloc.c\n    lib/gks/util.c\n    lib/gks/compress.c\n    lib/gks/gks.c\n    lib/gks/mf.c\n    lib/gks/win.c\n    lib/gks/gksforbnd.c\n    lib/gks/pdf.c\n    lib/gks/wiss.c\n    lib/gks/dl.c\n    lib/gks/plugin.c\n    lib/gks/error.c\n    lib/gks/io.c\n    lib/gks/ps.c\n    lib/gks/resample.c\n)\n\nadd_library(gks_static STATIC ${GKS_SOURCES})\nadd_library(gks_shared SHARED ${GKS_SOURCES})\n\nforeach(LIBRARY gks_static gks_shared)\n  if(LIBRARY MATCHES \"static\")\n    set(GKS_LINK_MODE PUBLIC)\n  else()\n    set(GKS_LINK_MODE PRIVATE)\n  endif()\n  if(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    target_compile_definitions(${LIBRARY} PRIVATE _CRT_SECURE_NO_WARNINGS)\n  endif()\n  if(NOT (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\"))\n    target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} pthread)\n    target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} m)\n  endif()\n  if(FREETYPE_FOUND)\n    target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} Freetype::Freetype)\n  else()\n    target_compile_definitions(\n      ${LIBRARY}\n      ${GKS_LINK_MODE}\n      NO_FT\n    )\n  endif()\n  target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} ZLIB::ZLIB)\n  target_compile_definitions(\n    ${LIBRARY}\n    ${GKS_LINK_MODE}\n    HAVE_ZLIB\n  )\n  if(UNIX)\n    target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} dl)\n  elseif(WIN32)\n    target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} ws2_32)\n    target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} msimg32)\n    target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} gdi32)\n  endif()\n  if(NOT (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\"))\n    target_compile_options(${LIBRARY} ${GKS_LINK_MODE} -pthread)\n  endif()\n  target_compile_options(${LIBRARY} PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  target_include_directories(\n    ${LIBRARY} PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/>\n                      $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>\n  )\n  target_compile_definitions(${LIBRARY} PRIVATE GRDIR=\"${GR_DIRECTORY}\")\n  set_target_properties(\n    ${LIBRARY}\n    PROPERTIES C_STANDARD 90\n               C_EXTENSIONS OFF\n               C_STANDARD_REQUIRED ON\n               POSITION_INDEPENDENT_CODE ON\n               PREFIX \"lib\"\n               IMPORT_PREFIX \"lib\"\n               OUTPUT_NAME GKS\n               INSTALL_RPATH \"${INSTALL_RPATH}\"\n  )\nendforeach()\nunset(GKS_LINK_MODE)\nif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")\n  set_target_properties(gks_static PROPERTIES OUTPUT_NAME GKSstatic)\nendif()\nset_target_properties(gks_shared PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" EXPORT_NAME GKS)\nadd_library(GR::GKS ALIAS gks_static)\nfile(WRITE ${CMAKE_CURRENT_BINARY_DIR}/gr_version.h\n     \"#ifndef GR_VERSION\\n#define GR_VERSION \\\"${GR_VERSION_FULL}\\\"\\n#endif\\n\"\n)\nset_source_files_properties(${CMAKE_CURRENT_BINARY_DIR}/gr_version.h PROPERTIES GENERATED TRUE)\n\nset(GR_SOURCES\n    lib/gr/boundary.c\n    lib/gr/contour.c\n    lib/gr/contourf.c\n    lib/gr/delaunay.c\n    lib/gr/gr.c\n    lib/gr/grforbnd.c\n    lib/gr/gridit.c\n    lib/gr/image.c\n    lib/gr/import.c\n    lib/gr/interp2.c\n    lib/gr/stream.c\n    lib/gr/md5.c\n    lib/gr/shade.c\n    lib/gr/spline.c\n    lib/gr/strlib.c\n    lib/gr/text.c\n    lib/gr/mathtex2.c\n    lib/gr/mathtex2_kerning.c\n    lib/gr/mathtex2.tab.c\n    lib/gr/threadpool.c\n    ${CMAKE_CURRENT_BINARY_DIR}/gr_version.h\n)\n\nadd_library(gr_static STATIC ${GR_SOURCES})\nadd_library(gr_shared SHARED ${GR_SOURCES})\n\nforeach(LIBRARY gr_static gr_shared)\n  if(LIBRARY MATCHES \"static\")\n    set(GR_LINK_MODE PUBLIC)\n  else()\n    set(GR_LINK_MODE PRIVATE)\n  endif()\n  target_link_libraries(${LIBRARY} ${GR_LINK_MODE} gks_static)\n  if(NOT (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\"))\n    target_link_libraries(${LIBRARY} ${GR_LINK_MODE} m)\n  endif()\n  target_link_libraries(${LIBRARY} ${GR_LINK_MODE} Qhull::qhull_r)\n  target_link_libraries(${LIBRARY} ${GR_LINK_MODE} Jpeg::Jpeg)\n  target_link_libraries(${LIBRARY} ${GR_LINK_MODE} Libpng::Libpng)\n  if(WIN32)\n    target_link_libraries(${LIBRARY} ${GR_LINK_MODE} ws2_32)\n  endif()\n  target_include_directories(\n    ${LIBRARY} PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/>\n                      $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>\n  )\n  target_include_directories(${LIBRARY} PRIVATE $<BUILD_INTERFACE:${CMAKE_CURRENT_BINARY_DIR}>)\n  if(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    target_compile_definitions(${LIBRARY} PRIVATE _CRT_SECURE_NO_WARNINGS)\n  endif()\n  target_compile_definitions(${LIBRARY} PRIVATE GRDIR=\"${GR_DIRECTORY}\")\n  target_compile_options(${LIBRARY} PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  set_target_properties(\n    ${LIBRARY}\n    PROPERTIES C_STANDARD 90\n               C_EXTENSIONS OFF\n               C_STANDARD_REQUIRED ON\n               POSITION_INDEPENDENT_CODE ON\n               INSTALL_RPATH \"${INSTALL_RPATH}\"\n               PREFIX \"lib\"\n               IMPORT_PREFIX \"lib\"\n               OUTPUT_NAME GR\n  )\nendforeach()\nunset(GR_LINK_MODE)\nif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")\n  set_target_properties(gr_static PROPERTIES OUTPUT_NAME GRstatic)\nendif()\nset_target_properties(gr_shared PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" EXPORT_NAME GR)\nadd_library(GR::GR ALIAS gr_shared)\n\nset(GR3_SOURCES\n    lib/gr3/gr3.c\n    lib/gr3/gr3_convenience.c\n    lib/gr3/gr3_gr.c\n    lib/gr3/gr3_html.c\n    lib/gr3/gr3_jpeg.c\n    lib/gr3/gr3_mc.c\n    lib/gr3/gr3_png.c\n    lib/gr3/gr3_povray.c\n    lib/gr3/gr3_slices.c\n    lib/gr3/gr3_sr.c\n)\n\nadd_library(gr3_static STATIC ${GR3_SOURCES})\nadd_library(gr3_shared SHARED ${GR3_SOURCES})\nforeach(LIBRARY gr3_static gr3_shared)\n  if(LIBRARY MATCHES \"static\")\n    set(GR3_LINK_MODE PUBLIC)\n  else()\n    set(GR3_LINK_MODE PRIVATE)\n  endif()\n  if(APPLE)\n    target_sources(${LIBRARY} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr3/gr3_cgl.c)\n  elseif(UNIX AND NOT APPLE)\n    target_sources(${LIBRARY} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr3/gr3_glx.c)\n  elseif(WIN32)\n    target_sources(${LIBRARY} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr3/gr3_win.c)\n  endif()\n  target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} GR::GR)\n  target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} GR::GKS)\n  target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} Jpeg::Jpeg)\n  target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} Libpng::Libpng)\n  if(NOT (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\"))\n    target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} pthread)\n    target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} m)\n  endif()\n  if((APPLE OR WIN32) AND OpenGL_FOUND)\n    target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} OpenGL::GL)\n  endif()\n  if(APPLE)\n    # Apple has deprecated OpenGL in macOS 10.14\n    target_compile_definitions(${LIBRARY} PRIVATE GL_SILENCE_DEPRECATION)\n  endif()\n  if(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n    target_compile_definitions(${LIBRARY} PRIVATE _CRT_SECURE_NO_WARNINGS)\n  endif()\n  target_compile_definitions(${LIBRARY} PRIVATE GRDIR=\"${GR_DIRECTORY}\")\n  if(NOT (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\"))\n    target_compile_options(${LIBRARY} ${GR3_LINK_MODE} -pthread)\n  endif()\n  target_compile_options(${LIBRARY} PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  target_include_directories(\n    ${LIBRARY} PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/lib/gr3/>\n                      $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>\n  )\n  set_target_properties(\n    ${LIBRARY}\n    PROPERTIES C_STANDARD 90\n               C_EXTENSIONS OFF\n               C_STANDARD_REQUIRED ON\n               POSITION_INDEPENDENT_CODE ON\n               INSTALL_RPATH \"${INSTALL_RPATH}\"\n               PREFIX \"lib\"\n               IMPORT_PREFIX \"lib\"\n               OUTPUT_NAME GR3\n  )\nendforeach()\nunset(GR3_LINK_MODE)\nif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")\n  set_target_properties(gr3_static PROPERTIES OUTPUT_NAME GR3static)\nendif()\nset_target_properties(gr3_shared PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" EXPORT_NAME GR3)\nadd_library(GR::GR3 ALIAS gr3_shared)\n\nif(UNIX AND NOT APPLE)\n  if(TARGET X11::X11\n     AND TARGET OpenGL::GLX\n     AND TARGET OpenGL::GL\n  )\n    add_library(gr3platform SHARED lib/gr3/gr3_platform_glx.c)\n    target_link_libraries(gr3platform PUBLIC GR::GR)\n    target_link_libraries(gr3platform PUBLIC X11::X11)\n    target_link_libraries(gr3platform PUBLIC OpenGL::GLX)\n    target_link_libraries(gr3platform PUBLIC OpenGL::GL)\n    target_compile_options(gr3platform PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n    set_target_properties(\n      gr3platform\n      PROPERTIES C_STANDARD 90\n                 C_EXTENSIONS OFF\n                 C_STANDARD_REQUIRED ON\n                 POSITION_INDEPENDENT_CODE ON\n                 INSTALL_RPATH \"${INSTALL_RPATH}\"\n                 OUTPUT_NAME GR3platform\n    )\n  else()\n    target_compile_definitions(gr3_static PUBLIC -DNO_GL)\n    target_compile_definitions(gr3_shared PUBLIC -DNO_GL)\n    message(WARNING \"libGR3platform.so requires X11 and will not be built.\")\n  endif()\nelseif(NOT OpenGL_FOUND)\n  target_compile_definitions(gr3_static PUBLIC -DNO_GL)\n  target_compile_definitions(gr3_shared PUBLIC -DNO_GL)\nendif()\n\nset(GRM_SOURCES\n    lib/grm/src/grm/args.c\n    lib/grm/src/grm/backtrace.c\n    lib/grm/src/grm/base64.c\n    lib/grm/src/grm/bson.c\n    lib/grm/src/grm/dump.c\n    lib/grm/src/grm/dynamic_args_array.c\n    lib/grm/src/grm/error.c\n    lib/grm/src/grm/event.c\n    lib/grm/src/grm/interaction.cxx\n    lib/grm/src/grm/json.c\n    lib/grm/src/grm/layout_c.cxx\n    lib/grm/src/grm/layout.cxx\n    lib/grm/src/grm/layout_error.cxx\n    lib/grm/src/grm/logging.c\n    lib/grm/src/grm/memwriter.c\n    lib/grm/src/grm/net.c\n    lib/grm/src/grm/plot.cxx\n    lib/grm/src/grm/util.c\n    lib/grm/src/grm/import.cxx\n    lib/grm/src/grm/utilcpp.cxx\n    lib/grm/src/grm/datatype/double_map.c\n    lib/grm/src/grm/datatype/size_t_list.c\n    lib/grm/src/grm/datatype/string_array_map.c\n    lib/grm/src/grm/datatype/string_list.c\n    lib/grm/src/grm/datatype/string_map.c\n    lib/grm/src/grm/datatype/uint_map.c\n    lib/grm/src/grm/dom_render/context.cxx\n    lib/grm/src/grm/dom_render/render.cxx\n    lib/grm/src/grm/dom_render/Drawable.cxx\n    lib/grm/src/grm/dom_render/graphics_tree/Comment.cxx\n    lib/grm/src/grm/dom_render/graphics_tree/Document.cxx\n    lib/grm/src/grm/dom_render/graphics_tree/Element.cxx\n    lib/grm/src/grm/dom_render/graphics_tree/Node.cxx\n    lib/grm/src/grm/dom_render/graphics_tree/Value.cxx\n    lib/grm/src/grm/dom_render/graphics_tree/util.cxx\n    lib/grm/src/grm/dom_render/ManageCustomColorIndex.cxx\n    lib/grm/src/grm/dom_render/ManageGRContextIds.cxx\n    lib/grm/src/grm/dom_render/ManageZIndex.cxx\n)\n\nadd_library(grm_static STATIC ${GRM_SOURCES})\nadd_library(grm_shared SHARED ${GRM_SOURCES})\nadd_library(grm_shared_internal SHARED ${GRM_SOURCES})\n\nforeach(LIBRARY grm_static grm_shared grm_shared_internal)\n  if(LIBRARY MATCHES \"static\")\n    set(GRM_LINK_MODE PUBLIC)\n  else()\n    set(GRM_LINK_MODE PRIVATE)\n  endif()\n  if(NOT LIBRARY MATCHES \"internal\")\n    set_target_properties(${LIBRARY} PROPERTIES C_VISIBILITY_PRESET hidden CXX_VISIBILITY_PRESET hidden)\n  endif()\n  target_link_libraries(${LIBRARY} ${GRM_LINK_MODE} GR::GKS)\n  target_link_libraries(${LIBRARY} ${GRM_LINK_MODE} GR::GR)\n  target_link_libraries(${LIBRARY} ${GRM_LINK_MODE} GR::GR3)\n  if(TARGET XercesC::XercesC)\n    target_link_libraries(${LIBRARY} ${GRM_LINK_MODE} XercesC::XercesC)\n  else()\n    target_compile_definitions(${LIBRARY} PRIVATE NO_XERCES_C)\n  endif()\n  if(NOT (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\"))\n    target_link_libraries(${LIBRARY} ${GRM_LINK_MODE} m)\n  endif()\n  if(WIN32)\n    target_link_libraries(${LIBRARY} ${GRM_LINK_MODE} ws2_32)\n  endif()\n  target_include_directories(\n    ${LIBRARY}\n    PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/include/>\n           $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/src/> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>\n  )\n  target_compile_definitions(${LIBRARY} PRIVATE BUILDING_GR)\n  target_compile_definitions(${LIBRARY} PRIVATE GRDIR=\"${GR_DIRECTORY}\")\n  target_compile_options(${LIBRARY} PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")\n    target_compile_options(${LIBRARY} PRIVATE /permissive-)\n  endif()\n  set_target_properties(\n    ${LIBRARY}\n    PROPERTIES C_STANDARD 90\n               C_EXTENSIONS OFF\n               C_STANDARD_REQUIRED ON\n               CXX_STANDARD 17\n               CXX_EXTENSIONS OFF\n               CXX_STANDARD_REQUIRED ON\n               POSITION_INDEPENDENT_CODE ON\n               INSTALL_RPATH \"${INSTALL_RPATH}\"\n               PREFIX \"lib\"\n               IMPORT_PREFIX \"lib\"\n               OUTPUT_NAME GRM\n  )\nendforeach()\nunset(GRM_LINK_MODE)\nset_target_properties(grm_shared_internal PROPERTIES C_VISIBILITY_PRESET default)\nset_target_properties(grm_shared_internal PROPERTIES OUTPUT_NAME GRM_int)\ntarget_include_directories(grm_shared_internal PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/>)\nif(WIN32)\n  target_compile_definitions(grm_static PRIVATE GR_STATIC_LIB)\n  target_compile_definitions(grm_shared PRIVATE BUILDING_DLL)\n  target_compile_definitions(grm_shared_internal PRIVATE BUILDING_DLL)\nendif()\nif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")\n  set_target_properties(grm_static PROPERTIES OUTPUT_NAME GRMstatic)\nendif()\nset_target_properties(grm_shared PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" EXPORT_NAME GRM)\nset_target_properties(grm_shared_internal PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\")\nadd_library(GR::GRM ALIAS grm_shared)\n\nset(GR_REPORT \"GKS plugins:\\n\")\n\nadd_library(cairoplugin SHARED lib/gks/plugin/cairoplugin.c)\ntarget_link_libraries(cairoplugin PUBLIC gks_static)\nif(Cairo_FOUND)\n  target_link_libraries(cairoplugin PRIVATE Cairo::Cairo)\n  target_link_libraries(cairoplugin PRIVATE Jpeg::Jpeg)\n  string(APPEND GR_REPORT \"- cairoplugin:\\n\")\n  string(APPEND GR_REPORT \"\\tPNG output: Yes\\n\")\n  string(APPEND GR_REPORT \"\\tBMP output: Yes\\n\")\n  string(APPEND GR_REPORT \"\\tJPEG output: Yes\\n\")\n\n  if(Tiff_FOUND)\n    target_link_libraries(cairoplugin PRIVATE Tiff::Tiff)\n    string(APPEND GR_REPORT \"\\tTiff output: Yes\\n\")\n  else()\n    string(APPEND GR_REPORT \"\\tTiff output: No (libtiff not found)\\n\")\n    target_compile_definitions(cairoplugin PRIVATE NO_TIFF)\n  endif()\n  target_compile_options(cairoplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  string(APPEND GR_REPORT \"\\tSixel output: Yes\\n\")\n  string(APPEND GR_REPORT \"\\tMemory output: Yes\\n\")\n  string(APPEND GR_REPORT \"\\tX11 output: No (not available in CMake build)\\n\")\nelse()\n  target_compile_definitions(cairoplugin PRIVATE NO_CAIRO)\n  string(APPEND GR_REPORT \"- cairoplugin: No (Cairo not found)\\n\")\nendif()\n# Cairo X11 support is disabled to allow users to generate images using Cairo on systems without X11 installed\ntarget_compile_definitions(cairoplugin PRIVATE NO_X11)\nset_target_properties(\n  cairoplugin\n  PROPERTIES C_STANDARD 90\n             C_EXTENSIONS OFF\n             C_STANDARD_REQUIRED ON\n             PREFIX \"\"\n             SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n)\n\nadd_library(aggplugin SHARED lib/gks/plugin/aggplugin.cxx)\ntarget_link_libraries(aggplugin PUBLIC gks_static)\nif(Agg_FOUND)\n  target_link_libraries(aggplugin PRIVATE Agg::Agg)\n  target_link_libraries(aggplugin PRIVATE Jpeg::Jpeg)\n  target_link_libraries(aggplugin PRIVATE Libpng::Libpng)\n  string(APPEND GR_REPORT \"- aggplugin: Yes\\n\")\n  target_compile_options(aggplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\nelse()\n  target_compile_definitions(aggplugin PRIVATE NO_AGG)\n  string(APPEND GR_REPORT \"- aggplugin: No (Agg not found)\\n\")\nendif()\nset_target_properties(\n  aggplugin\n  PROPERTIES CXX_STANDARD 11\n             CXX_EXTENSIONS OFF\n             CXX_STANDARD_REQUIRED ON\n             PREFIX \"\"\n             SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n)\n\nadd_library(videoplugin SHARED lib/gks/plugin/gif.c lib/gks/plugin/vc.c lib/gks/plugin/videoplugin.c)\ntarget_link_libraries(videoplugin PUBLIC gks_static)\nif(NOT Cairo_FOUND)\n  target_compile_definitions(videoplugin PRIVATE NO_CAIRO)\n  target_compile_definitions(videoplugin PRIVATE NO_AV)\n  string(APPEND GR_REPORT \"- videoplugin: No (Cairo not found)\\n\")\nelseif(NOT Ffmpeg_FOUND)\n  target_compile_definitions(videoplugin PRIVATE NO_CAIRO)\n  target_compile_definitions(videoplugin PRIVATE NO_AV)\n  string(APPEND GR_REPORT \"- videoplugin: No (ffmpeg / ogg / theora / vpx / openh264 not found)\\n\")\nelse()\n  string(APPEND GR_REPORT \"- videoplugin:\\n\")\n  string(APPEND GR_REPORT \"\\tMP4 output: Yes\\n\")\n  string(APPEND GR_REPORT \"\\tMOV output: Yes\\n\")\n  string(APPEND GR_REPORT \"\\tWEBM output: Yes\\n\")\n  string(APPEND GR_REPORT \"\\tOGG output: Yes\\n\")\n  string(APPEND GR_REPORT \"\\tGIF output: Yes\\n\")\n  string(APPEND GR_REPORT \"\\tAPNG output: Yes\\n\")\n  target_link_libraries(videoplugin PUBLIC Ffmpeg::Ffmpeg)\nendif()\ntarget_compile_options(videoplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\nset_target_properties(\n  videoplugin\n  PROPERTIES C_STANDARD 99\n             C_EXTENSIONS OFF\n             C_STANDARD_REQUIRED ON\n             LINKER_LANGUAGE CXX\n             PREFIX \"\"\n             SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n)\n\nadd_library(pgfplugin SHARED lib/gks/plugin/pgfplugin.c)\ntarget_link_libraries(pgfplugin PUBLIC gks_static)\ntarget_link_libraries(pgfplugin PUBLIC Libpng::Libpng)\ntarget_compile_options(pgfplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\nset_target_properties(\n  pgfplugin\n  PROPERTIES C_STANDARD 90\n             C_EXTENSIONS OFF\n             C_STANDARD_REQUIRED ON\n             PREFIX \"\"\n             SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n)\nstring(APPEND GR_REPORT \"- pgfplugin:\\n\")\nstring(APPEND GR_REPORT \"\\tTeX output: Yes\\n\")\n\nadd_library(wmfplugin SHARED lib/gks/plugin/wmfplugin.c)\ntarget_link_libraries(wmfplugin PUBLIC gks_static)\ntarget_compile_options(wmfplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\nset_target_properties(\n  wmfplugin\n  PROPERTIES C_STANDARD 90\n             C_EXTENSIONS OFF\n             C_STANDARD_REQUIRED ON\n             PREFIX \"\"\n             SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n)\nstring(APPEND GR_REPORT \"- wmfplugin:\\n\")\nstring(APPEND GR_REPORT \"\\tWMF output: Yes\\n\")\n\nadd_library(gsplugin SHARED lib/gks/plugin/gsplugin.c)\ntarget_link_libraries(gsplugin PUBLIC gks_static)\nif(NOT TARGET X11::Xt)\n  string(APPEND GR_REPORT \"- gsplugin: No (X11 / Xt not found)\\n\")\n  target_compile_definitions(gsplugin PRIVATE NO_GS)\nelseif(NOT Gs_FOUND)\n  string(APPEND GR_REPORT \"- gsplugin: No (ghostscript not found)\\n\")\n  target_compile_definitions(gsplugin PRIVATE NO_GS)\nelse()\n  target_link_libraries(gsplugin PUBLIC X11::Xt)\n  target_link_libraries(gsplugin PUBLIC Gs::Gs)\n  string(APPEND GR_REPORT \"- gsplugin:\\n\")\n  string(APPEND GR_REPORT \"\\tPNG outout: Yes\\n\")\n  string(APPEND GR_REPORT \"\\tBMP outout: Yes\\n\")\n  string(APPEND GR_REPORT \"\\tJPEG outout: Yes\\n\")\n  string(APPEND GR_REPORT \"\\tTiff outout: Yes\\n\")\nendif()\ntarget_compile_options(gsplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\nset_target_properties(\n  gsplugin\n  PROPERTIES C_STANDARD 90\n             C_EXTENSIONS OFF\n             C_STANDARD_REQUIRED ON\n             PREFIX \"\"\n             SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n)\n\nadd_library(svgplugin SHARED lib/gks/plugin/svgplugin.c)\ntarget_link_libraries(svgplugin PUBLIC gks_static)\ntarget_link_libraries(svgplugin PUBLIC Libpng::Libpng)\ntarget_compile_options(svgplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\nset_target_properties(\n  svgplugin\n  PROPERTIES C_STANDARD 90\n             C_EXTENSIONS OFF\n             C_STANDARD_REQUIRED ON\n             PREFIX \"\"\n             SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n)\nstring(APPEND GR_REPORT \"- svgplugin:\\n\")\nstring(APPEND GR_REPORT \"\\tSVG output: Yes\\n\")\n\nadd_library(glplugin SHARED lib/gks/plugin/glplugin.c)\ntarget_link_libraries(glplugin PUBLIC gks_static)\nif(TARGET Glfw::Glfw AND OpenGL_FOUND)\n  string(APPEND GR_REPORT \"- glplugin: Yes\\n\")\n  target_link_libraries(glplugin PUBLIC Glfw::Glfw)\n  target_link_libraries(glplugin PUBLIC OpenGL::GL)\nelse()\n  string(APPEND GR_REPORT \"- glplugin: No (GLFW / OpenGL not found)\\n\")\n  target_compile_definitions(glplugin PRIVATE NO_GLFW)\nendif()\nif(NOT Freetype_FOUND)\n  target_compile_definitions(glplugin PRIVATE NO_FT)\nendif()\nif(APPLE)\n  # Apple has deprecated OpenGL in macOS 10.14\n  target_compile_definitions(glplugin PRIVATE GL_SILENCE_DEPRECATION)\nendif()\ntarget_compile_options(glplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\nset_target_properties(\n  glplugin\n  PROPERTIES C_STANDARD 90\n             C_EXTENSIONS OFF\n             C_STANDARD_REQUIRED ON\n             PREFIX \"\"\n             SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n)\n\nadd_library(zmqplugin SHARED lib/gks/plugin/zmqplugin.c)\ntarget_link_libraries(zmqplugin PUBLIC gks_static)\nif(ZeroMQ_FOUND)\n  string(APPEND GR_REPORT \"- zmqplugin: Yes\\n\")\n  target_link_libraries(zmqplugin PUBLIC ZeroMQ::ZeroMQ)\nelse()\n  string(APPEND GR_REPORT \"- zmqplugin: No (ZeroMQ not found)\\n\")\n  target_compile_definitions(zmqplugin PRIVATE NO_ZMQ)\nendif()\ntarget_compile_options(zmqplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\nset_target_properties(\n  zmqplugin\n  PROPERTIES C_STANDARD 90\n             C_EXTENSIONS OFF\n             C_STANDARD_REQUIRED ON\n             PREFIX \"\"\n             SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n)\n\nadd_library(x11plugin SHARED lib/gks/plugin/x11plugin.c)\ntarget_link_libraries(x11plugin PUBLIC gks_static)\nif(NOT Freetype_FOUND)\n  target_compile_definitions(x11plugin PRIVATE NO_FT)\nendif()\nif(TARGET X11::Xt)\n  if(Xft_FOUND AND Fontconfig_FOUND)\n    target_link_libraries(x11plugin PUBLIC Fontconfig::Fontconfig)\n  else()\n    target_compile_definitions(x11plugin PRIVATE NO_XFT)\n  endif()\n  target_link_libraries(x11plugin PUBLIC X11::Xt)\n  string(APPEND GR_REPORT \"- x11plugin: Yes\\n\")\nelse()\n  target_compile_definitions(x11plugin PRIVATE NO_X11)\n  target_compile_definitions(x11plugin PRIVATE NO_XFT)\n  string(APPEND GR_REPORT \"- x11plugin: No (X11 / Xft / Xt not found)\\n\")\nendif()\ntarget_compile_options(x11plugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\nset_target_properties(\n  x11plugin\n  PROPERTIES C_STANDARD 90\n             C_EXTENSIONS OFF\n             C_STANDARD_REQUIRED ON\n             CXX_STANDARD 11\n             CXX_EXTENSIONS OFF\n             CXX_STANDARD_REQUIRED ON\n             PREFIX \"\"\n             SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n)\n\nadd_library(qt5plugin SHARED lib/gks/plugin/qt5plugin.cxx)\ntarget_link_libraries(qt5plugin PUBLIC gks_static)\nif(Qt5Widgets_FOUND)\n  string(APPEND GR_REPORT \"- qt5plugin: Yes\\n\")\n  target_link_libraries(qt5plugin PUBLIC Qt5::Widgets)\nelse()\n  string(APPEND GR_REPORT \"- qt5plugin: No (Qt5 not found)\\n\")\n  target_compile_definitions(qt5plugin PRIVATE NO_QT5)\nendif()\ntarget_compile_options(qt5plugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\nset_target_properties(\n  qt5plugin\n  PROPERTIES CXX_STANDARD 11\n             CXX_EXTENSIONS OFF\n             CXX_STANDARD_REQUIRED ON\n             PREFIX \"\"\n             SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n)\n\nadd_library(qt6plugin SHARED lib/gks/plugin/qt6plugin.cxx)\ntarget_link_libraries(qt6plugin PUBLIC gks_static)\nif(Qt6Widgets_FOUND AND ${CMAKE_VERSION} VERSION_GREATER \"3.16.0\")\n  string(APPEND GR_REPORT \"- qt6plugin: Yes\\n\")\n  target_link_libraries(qt6plugin PUBLIC Qt6::Widgets)\n  set_target_properties(\n    qt6plugin\n    PROPERTIES CXX_STANDARD 17\n               CXX_EXTENSIONS OFF\n               CXX_STANDARD_REQUIRED ON\n               PREFIX \"\"\n               SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n  )\nelse()\n  if(${CMAKE_VERSION} VERSION_LESS \"3.16.0\")\n    string(APPEND GR_REPORT \"- qt6plugin: No (CMake version < 3.16.0)\\n\")\n  else()\n    string(APPEND GR_REPORT \"- qt6plugin: No (Qt6 not found)\\n\")\n  endif()\n  set_target_properties(\n    qt6plugin\n    PROPERTIES CXX_STANDARD 11\n               CXX_EXTENSIONS OFF\n               CXX_STANDARD_REQUIRED ON\n               PREFIX \"\"\n               SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n  )\n  target_compile_definitions(qt6plugin PRIVATE NO_QT6)\nendif()\ntarget_compile_options(qt6plugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n\nadd_library(qtplugin SHARED lib/gks/plugin/qtplugin.cxx)\ntarget_link_libraries(qtplugin PUBLIC gks_static)\nif(Qt4_FOUND)\n  string(APPEND GR_REPORT \"- qtplugin: Yes\\n\")\n  target_link_libraries(qtplugin PUBLIC Qt4::QtGui)\nelse()\n  string(APPEND GR_REPORT \"- qtplugin: No (Qt4 not found)\\n\")\n  target_compile_definitions(qtplugin PRIVATE NO_QT)\nendif()\ntarget_compile_options(qtplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\nset_target_properties(\n  qtplugin\n  PROPERTIES CXX_STANDARD 11\n             CXX_EXTENSIONS OFF\n             CXX_STANDARD_REQUIRED ON\n             PREFIX \"\"\n             SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n)\n\nadd_library(gtkplugin SHARED lib/gks/plugin/gtkplugin.c)\ntarget_link_libraries(gtkplugin PUBLIC gks_static)\nset_target_properties(\n  gtkplugin\n  PROPERTIES C_STANDARD 90\n             C_EXTENSIONS OFF\n             C_STANDARD_REQUIRED ON\n             PREFIX \"\"\n             SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n)\ntarget_compile_options(gtkplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\nstring(APPEND GR_REPORT \"- gtkplugin: No (not implemented yet)\\n\")\n\nif(APPLE)\n  add_library(quartzplugin SHARED lib/gks/plugin/quartzplugin.m)\n  target_link_libraries(quartzplugin PUBLIC gks_static)\n  if(ZeroMQ_FOUND)\n    target_link_libraries(quartzplugin PUBLIC ZeroMQ::ZeroMQ)\n    target_link_libraries(quartzplugin PUBLIC \"-framework Foundation -framework ApplicationServices -framework AppKit\")\n    target_compile_definitions(quartzplugin PRIVATE GRDIR=\"${GR_DIRECTORY}\")\n  else()\n    target_compile_definitions(quartzplugin PRIVATE NO_GKSTERM)\n  endif()\n  target_compile_options(quartzplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  set_target_properties(\n    quartzplugin\n    PROPERTIES C_STANDARD 90\n               C_EXTENSIONS OFF\n               C_STANDARD_REQUIRED ON\n               PREFIX \"\"\n               SUFFIX \"${GR_PLUGIN_SUFFIX}\"\n  )\nendif()\n\nif(Qt4_FOUND)\n  add_library(qt4gr SHARED lib/gr/qtgr/grwidget.cxx)\n  target_link_libraries(qt4gr PUBLIC GR::GR)\n  target_link_libraries(qt4gr PUBLIC Qt4::QtCore Qt4::QtGui)\n  if(MINGW AND CMAKE_SYSTEM_PROCESSOR STREQUAL \"i686\")\n    target_compile_options(qt4gr PRIVATE -fno-exceptions)\n  endif()\n  target_compile_options(qt4gr PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  set_target_properties(qt4gr PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" INSTALL_RPATH \"${INSTALL_RPATH}\")\n  if(GR_MANUAL_MOC_AND_RCC)\n    set_target_properties(qt4gr PROPERTIES AUTOMOC OFF AUTORCC OFF)\n    add_custom_command(\n      OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_qt4_grwidget.cxx\n      COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${QT4_MOC_INCLUDE_FLAGS}\n              ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.h -o ${CMAKE_CURRENT_BINARY_DIR}/moc_qt4_grwidget.cxx\n      DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.cxx\n    )\n    target_sources(qt4gr PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/moc_qt4_grwidget.cxx)\n  else()\n    set_target_properties(qt4gr PROPERTIES AUTOMOC ON AUTORCC ON)\n    target_sources(qt4gr PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.h)\n  endif()\n  add_library(GR::qt4gr ALIAS qt4gr)\nendif()\n\nif(Qt5Widgets_FOUND AND Qt5Core_FOUND)\n  add_library(qt5gr SHARED lib/gr/qtgr/grwidget.cxx)\n  target_link_libraries(qt5gr PUBLIC GR::GR)\n  target_link_libraries(qt5gr PUBLIC Qt5::Core Qt5::Widgets)\n  if(MINGW AND CMAKE_SYSTEM_PROCESSOR STREQUAL \"i686\")\n    target_compile_options(qt5gr PRIVATE -fno-exceptions)\n  endif()\n  target_compile_options(qt5gr PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  set_target_properties(qt5gr PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" INSTALL_RPATH \"${INSTALL_RPATH}\")\n  if(GR_MANUAL_MOC_AND_RCC)\n    set_target_properties(qt5gr PROPERTIES AUTOMOC OFF AUTORCC OFF)\n    add_custom_command(\n      OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_qt5_grwidget.cxx\n      COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${QT5_MOC_INCLUDE_FLAGS}\n              ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.h -o ${CMAKE_CURRENT_BINARY_DIR}/moc_qt5_grwidget.cxx\n      DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.cxx\n    )\n    target_sources(qt5gr PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/moc_qt5_grwidget.cxx)\n  else()\n    set_target_properties(qt5gr PROPERTIES AUTOMOC ON AUTORCC ON)\n    target_sources(qt5gr PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.h)\n  endif()\n  add_library(GR::qt5gr ALIAS qt5gr)\nendif()\n\nif(Qt6Widgets_FOUND AND Qt6Core_FOUND)\n  add_library(qt6gr SHARED lib/gr/qtgr/grwidget.cxx)\n  target_link_libraries(qt6gr PUBLIC GR::GR)\n  target_link_libraries(qt6gr PUBLIC Qt6::Core Qt6::Widgets)\n  if(MINGW AND CMAKE_SYSTEM_PROCESSOR STREQUAL \"i686\")\n    target_compile_options(qt6gr PRIVATE -fno-exceptions)\n  endif()\n  target_compile_options(qt6gr PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  set_target_properties(qt6gr PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" INSTALL_RPATH \"${INSTALL_RPATH}\")\n  if(GR_MANUAL_MOC_AND_RCC)\n    set_target_properties(qt6gr PROPERTIES AUTOMOC OFF AUTORCC OFF)\n    add_custom_command(\n      OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_qt6_grwidget.cxx\n      COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${QT6_MOC_INCLUDE_FLAGS}\n              ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.h -o ${CMAKE_CURRENT_BINARY_DIR}/moc_qt6_grwidget.cxx\n      DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.cxx\n    )\n    target_sources(qt6gr PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/moc_qt6_grwidget.cxx)\n  else()\n    set_target_properties(qt6gr PROPERTIES AUTOMOC ON AUTORCC ON)\n    target_sources(qt6gr PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.h)\n  endif()\n  add_library(GR::qt6gr ALIAS qt6gr)\nendif()\n\nstring(APPEND GR_REPORT \"\\nGKS applications:\\n\")\n\nif(APPLE)\n  if(ZeroMQ_FOUND)\n    if(GR_PREFER_XCODEBUILD)\n      find_program(XCODE_BUILD xcodebuild CMAKE_FIND_ROOT_PATH_BOTH)\n      if(XCODE_BUILD)\n        execute_process(COMMAND ${XCODE_BUILD} -version RESULT_VARIABLE XCODE_BUILD_CHECK_RESULT)\n        if(NOT XCODE_BUILD_CHECK_RESULT EQUAL 0)\n          set(XCODE_BUILD \"XCODE_BUILD-NOTFOUND\")\n        endif()\n      endif()\n    endif()\n    if(GR_PREFER_XCODEBUILD AND XCODE_BUILD)\n      set(GKSTERM_SOURCES\n          lib/gks/quartz/GKSTerm.h\n          lib/gks/quartz/GKSTerm.icns\n          lib/gks/quartz/GKSTerm.m\n          lib/gks/quartz/GKSTerm_Prefix.pch\n          lib/gks/quartz/GKSView.h\n          lib/gks/quartz/GKSView.m\n          lib/gks/quartz/Info.plist\n          lib/gks/quartz/MacOSXBundleInfo.plist.in\n          lib/gks/quartz/main.m\n          lib/gks/quartz/English.lproj/InfoPlist.strings\n          lib/gks/quartz/English.lproj/ExtendSavePanel.nib/designable.nib\n          lib/gks/quartz/English.lproj/ExtendSavePanel.nib/keyedobjects.nib\n          lib/gks/quartz/English.lproj/MainMenu.nib/designable.nib\n          lib/gks/quartz/English.lproj/MainMenu.nib/keyedobjects.nib\n          lib/gks/quartz/GKSTerm.xcodeproj/project.pbxproj\n          lib/gks/quartz/GKSTerm.xcodeproj/project.xcworkspace\n          lib/gks/quartz/GKSTerm.xcodeproj/project.xcworkspace/contents.xcworkspacedata\n      )\n      add_custom_target(\n        GKSTerm ALL\n        COMMAND ${XCODE_BUILD} -arch ${CMAKE_SYSTEM_PROCESSOR} -project GKSTerm.xcodeproj\n                CONFIGURATION_BUILD_DIR=${CMAKE_CURRENT_BINARY_DIR}/GKSTerm\n        WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm-build/quartz\n        DEPENDS ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm-build/quartz/GKSTerm.xcodeproj ZeroMQ::ZeroMQ gks_static\n      )\n      # Disable autoformat to not break the `sed` command\n      # cmake-format: off\n      add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm-build/quartz/GKSTerm.xcodeproj\n        COMMAND ${CMAKE_COMMAND} -E copy_directory lib/gks ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm-build\n        COMMAND sed -e s%../../../3rdparty/build/lib/libzmq.a%${ZeroMQ_LIBRARY}% -e s%../../../3rdparty/build/include/zmq.h%${ZeroMQ_INCLUDE_DIR}/zmq.h% ${CMAKE_SOURCE_DIR}/lib/gks/quartz/GKSTerm.xcodeproj/project.pbxproj >${CMAKE_CURRENT_BINARY_DIR}/GKSTerm-build/quartz/GKSTerm.xcodeproj/project.pbxproj\n        WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}\n        DEPENDS ${GKS_SOURCES} ${GKSTERM_SOURCES}\n      )\n      # cmake-format: on\n      string(APPEND GR_REPORT \"- GKSTerm: Yes (Xcode build)\\n\")\n    else()\n      add_executable(GKSTerm MACOSX_BUNDLE lib/gks/quartz/GKSTerm.m lib/gks/quartz/GKSView.m lib/gks/quartz/main.m)\n      target_link_libraries(GKSTerm PUBLIC ZeroMQ::ZeroMQ)\n      target_link_libraries(\n        GKSTerm PUBLIC gks_static\n                       \"-framework CoreGraphics -framework CoreFoundation -framework CoreVideo -framework Cocoa\"\n      )\n\n      add_custom_command(\n        TARGET GKSTerm\n        POST_BUILD\n        COMMAND\n          ${CMAKE_COMMAND} -E copy_directory ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/quartz/English.lproj\n          ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm.app/Contents/Resources/English.lproj DEPENDS\n          ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/quartz/English.lproj\n        BYPRODUCTS ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm.app/Contents/Resources/English.lproj\n      )\n      target_sources(GKSTerm PRIVATE lib/gks/quartz/GKSTerm.icns)\n      set_source_files_properties(lib/gks/quartz/GKSTerm.icns PROPERTIES MACOSX_PACKAGE_LOCATION Resources/)\n      target_compile_options(GKSTerm PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n      set_target_properties(\n        GKSTerm\n        PROPERTIES MACOSX_BUNDLE_BUNDLE_NAME \"GKSTerm\"\n                   MACOSX_BUNDLE_GUI_IDENTIFIER \"de.fz-juelich.GKSTerm\"\n                   MACOSX_BUNDLE_ICON_FILE \"GKSTerm\"\n                   MACOSX_BUNDLE_BUNDLE_VERSION \"${GR_VERSION}\"\n                   MACOSX_BUNDLE_INFO_PLIST \"${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/quartz/MacOSXBundleInfo.plist.in\"\n      )\n      set(GKSTerm_MACOSX_BUNDLE_SIGNATURE \"GKST\")\n      set(GKSTerm_MACOSX_MAIN_NIB_FILE \"MainMenu\")\n      string(APPEND GR_REPORT \"- GKSTerm: Yes (Build without Xcode)\\n\")\n    endif()\n  else()\n    string(APPEND GR_REPORT \"- GKSTerm: No (ZeroMQ not found)\\n\")\n  endif()\nendif()\n\nif(Qt4_FOUND\n   OR (Qt5Widgets_FOUND\n       AND Qt5Core_FOUND\n       AND Qt5Network_FOUND\n      )\n   OR (Qt6Widgets_FOUND\n       AND Qt6Core_FOUND\n       AND Qt6Network_FOUND\n      )\n)\n  add_executable(gksqt WIN32 MACOSX_BUNDLE lib/gks/qt/gksqt.cxx lib/gks/qt/gksserver.cxx lib/gks/qt/gkswidget.cxx)\n  target_link_libraries(gksqt PUBLIC gks_static)\n  if(Qt6Widgets_FOUND\n     AND Qt6Core_FOUND\n     AND Qt6Network_FOUND\n  )\n    target_link_libraries(gksqt PUBLIC Qt6::Widgets Qt6::Core Qt6::Network)\n    set(gksqt_INSTALL_RPATH \"${INSTALL_RPATH};${Qt6_LIBRARY_DIR}\")\n  elseif(\n    Qt5Widgets_FOUND\n    AND Qt5Core_FOUND\n    AND Qt5Network_FOUND\n  )\n    target_link_libraries(gksqt PUBLIC Qt5::Widgets Qt5::Core Qt5::Network)\n    set(gksqt_INSTALL_RPATH \"${INSTALL_RPATH};${Qt5_LIBRARY_DIR}\")\n  elseif(Qt4_FOUND)\n    target_link_libraries(gksqt PUBLIC Qt4::QtCore Qt4::QtGui Qt4::QtNetwork)\n    set(gksqt_INSTALL_RPATH \"${INSTALL_RPATH};${Qt4_LIBRARY_DIR}\")\n  endif()\n  set_target_properties(\n    gksqt PROPERTIES CXX_STANDARD 11 CXX_EXTENSIONS OFF CXX_STANDARD_REQUIRED ON INSTALL_RPATH \"${gksqt_INSTALL_RPATH}\"\n  )\n  if(MINGW AND CMAKE_SYSTEM_PROCESSOR STREQUAL \"i686\")\n    target_compile_options(gksqt PRIVATE -fno-exceptions)\n  endif()\n  target_compile_options(gksqt PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  if(GR_MANUAL_MOC_AND_RCC)\n    set_target_properties(gksqt PROPERTIES AUTOMOC OFF AUTORCC OFF)\n    if(Qt6Widgets_FOUND\n       AND Qt6Core_FOUND\n       AND Qt6Network_FOUND\n    )\n      set(MOC_INCLUDE_FLAGS ${QT6_MOC_INCLUDE_FLAGS})\n    elseif(\n      Qt5Widgets_FOUND\n      AND Qt5Core_FOUND\n      AND Qt5Network_FOUND\n    )\n      set(MOC_INCLUDE_FLAGS ${QT5_MOC_INCLUDE_FLAGS})\n    else()\n      set(MOC_INCLUDE_FLAGS ${QT4_MOC_INCLUDE_FLAGS})\n    endif()\n    add_custom_command(\n      OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_gkswidget.cxx\n      COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS}\n              ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gkswidget.h -o ${CMAKE_CURRENT_BINARY_DIR}/moc_gkswidget.cxx\n      DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gkswidget.h\n    )\n    add_custom_command(\n      OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_gksserver.cxx\n      COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS}\n              ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gksserver.h -o ${CMAKE_CURRENT_BINARY_DIR}/moc_gksserver.cxx\n      DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gksserver.h\n    )\n    add_custom_command(\n      OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/qrc_gksqt.cxx\n      COMMAND ${QT_RCC_EXECUTABLE} ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gksqt.qrc -o\n              ${CMAKE_CURRENT_BINARY_DIR}/qrc_gksqt.cxx\n      DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gksqt.qrc\n    )\n    target_sources(\n      gksqt PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/moc_gksserver.cxx ${CMAKE_CURRENT_BINARY_DIR}/moc_gkswidget.cxx\n                    ${CMAKE_CURRENT_BINARY_DIR}/qrc_gksqt.cxx\n    )\n  else()\n    set_target_properties(gksqt PROPERTIES AUTOMOC ON AUTORCC ON)\n    target_sources(\n      gksqt PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gksserver.h\n                    ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gkswidget.h ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gksqt.qrc\n    )\n  endif()\n  if(APPLE)\n    target_sources(gksqt PRIVATE lib/gks/qt/gksqt.icns)\n    set_source_files_properties(lib/gks/qt/gksqt.icns PROPERTIES MACOSX_PACKAGE_LOCATION Resources/)\n    set_target_properties(\n      gksqt\n      PROPERTIES MACOSX_BUNDLE_BUNDLE_NAME \"gksqt\"\n                 MACOSX_BUNDLE_GUI_IDENTIFIER \"de.fz-juelich.gksqt\"\n                 MACOSX_BUNDLE_ICON_FILE \"gksqt\"\n                 MACOSX_BUNDLE_BUNDLE_VERSION \"${GR_VERSION}\"\n    )\n    set(gksqt_MACOSX_BUNDLE_SIGNATURE \"gksqt\")\n    set(gksqt_MACOSX_MAIN_NIB_FILE \"\")\n  endif()\n  string(APPEND GR_REPORT \"- gksqt: Yes\\n\")\nelse()\n  string(APPEND GR_REPORT \"- gksqt: No (Qt4 / Qt5 / Qt6 not found)\\n\")\nendif()\n\nif((Qt6Widgets_FOUND\n    AND Qt6Core_FOUND\n    AND Qt6Gui_FOUND\n   )\n   OR (Qt5Widgets_FOUND\n       AND Qt5Core_FOUND\n       AND Qt5Gui_FOUND\n      )\n)\n\n  if(NOT MINGW OR NOT CMAKE_SYSTEM_PROCESSOR STREQUAL \"i686\")\n    add_executable(\n      grplot WIN32 MACOSX_BUNDLE\n      lib/grm/grplot/gredit/AddElementWidget.cpp\n      lib/grm/grplot/gredit/Bounding_logic.cpp\n      lib/grm/grplot/gredit/Bounding_object.cpp\n      lib/grm/grplot/gredit/CustomTreeWidgetItem.cpp\n      lib/grm/grplot/gredit/EditElementWidget.cpp\n      lib/grm/grplot/gredit/TableWidget.cpp\n      lib/grm/grplot/gredit/TreeWidget.cpp\n      lib/grm/grplot/grplot.cxx\n      lib/grm/grplot/grplot_mainwindow.cxx\n      lib/grm/grplot/grplot_widget.cxx\n      lib/grm/grplot/qtterm/grm_args_t_wrapper.cpp\n      lib/grm/grplot/qtterm/receiver_thread.cpp\n      lib/grm/grplot/util.cxx\n    )\n    if(Qt6Widgets_FOUND AND Qt6Core_FOUND)\n      target_link_libraries(grplot PRIVATE Qt6::Widgets Qt6::Core Qt6::Gui grm_static)\n      set(grplot_INSTALL_RPATH \"${INSTALL_RPATH};${Qt6_LIBRARY_DIR}\")\n    else()\n      target_link_libraries(grplot PRIVATE Qt5::Widgets Qt5::Core Qt5::Gui grm_static)\n      set(grplot_INSTALL_RPATH \"${INSTALL_RPATH};${Qt5_LIBRARY_DIR}\")\n    endif()\n    target_compile_definitions(grplot PRIVATE GRDIR=\"${GR_DIRECTORY}\")\n    if(NOT TARGET XercesC::XercesC)\n      target_compile_definitions(grplot PRIVATE NO_XERCES_C)\n    endif()\n    set_target_properties(\n      grplot PROPERTIES CXX_STANDARD 17 CXX_EXTENSIONS OFF CXX_STANDARD_REQUIRED ON INSTALL_RPATH\n                                                                                    \"${grplot_INSTALL_RPATH}\"\n    )\n    if(WIN32)\n      if(MINGW)\n        target_link_options(grplot PRIVATE -mconsole)\n      else()\n        target_link_options(grplot PRIVATE /SUBSYSTEM:CONSOLE)\n      endif()\n    endif()\n    if(GR_MANUAL_MOC_AND_RCC)\n      set_target_properties(grplot PROPERTIES AUTOMOC OFF AUTORCC OFF)\n      if(Qt6Widgets_FOUND AND Qt6Core_FOUND)\n        set(MOC_INCLUDE_FLAGS ${QT6_MOC_INCLUDE_FLAGS})\n      else()\n        set(MOC_INCLUDE_FLAGS ${QT5_MOC_INCLUDE_FLAGS})\n      endif()\n      add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_grplot_widget.cxx\n        COMMAND\n          ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS}\n          ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplot_widget.hxx -o\n          ${CMAKE_CURRENT_BINARY_DIR}/moc_grplot_widget.cxx\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplot_widget.hxx\n      )\n      add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_grplot_mainwindow.cxx\n        COMMAND\n          ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS}\n          ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplot_mainwindow.hxx -o\n          ${CMAKE_CURRENT_BINARY_DIR}/moc_grplot_mainwindow.cxx\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplot_mainwindow.hxx\n      )\n      add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_grm_args_t_wrapper.cpp\n        COMMAND\n          ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS}\n          ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/qtterm/grm_args_t_wrapper.h -o\n          ${CMAKE_CURRENT_BINARY_DIR}/moc_grm_args_t_wrapper.cpp\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/qtterm/grm_args_t_wrapper.h\n      )\n      add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_receiver_thread.cpp\n        COMMAND\n          ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS}\n          ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/qtterm/receiver_thread.h -o\n          ${CMAKE_CURRENT_BINARY_DIR}/moc_receiver_thread.cpp\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/qtterm/receiver_thread.h\n      )\n\n      add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_Bounding_logic.cpp\n        COMMAND\n          ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS}\n          ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/Bounding_logic.h -o\n          ${CMAKE_CURRENT_BINARY_DIR}/moc_Bounding_logic.cpp\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/Bounding_logic.h\n      )\n      add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_Bounding_object.cpp\n        COMMAND\n          ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS}\n          ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/Bounding_object.h -o\n          ${CMAKE_CURRENT_BINARY_DIR}/moc_Bounding_object.cpp\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/Bounding_object.h\n      )\n      add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_CustomTreeWidgetItem.cpp\n        COMMAND\n          ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS}\n          ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/CustomTreeWidgetItem.h -o\n          ${CMAKE_CURRENT_BINARY_DIR}/moc_CustomTreeWidgetItem.cpp\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/CustomTreeWidgetItem.h\n      )\n      add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_TreeWidget.cpp\n        COMMAND\n          ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS}\n          ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/TreeWidget.h -o\n          ${CMAKE_CURRENT_BINARY_DIR}/moc_TreeWidget.cpp\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/TreeWidget.h\n      )\n      add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_AddElementWidget.cpp\n        COMMAND\n          ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS}\n          ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/AddElementWidget.h -o\n          ${CMAKE_CURRENT_BINARY_DIR}/moc_AddElementWidget.cpp\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/AddElementWidget.h\n      )\n      add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_EditElementWidget.cpp\n        COMMAND\n          ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS}\n          ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/EditElementWidget.h -o\n          ${CMAKE_CURRENT_BINARY_DIR}/moc_EditElementWidget.cpp\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/EditElementWidget.h\n      )\n      add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_TableWidget.cpp\n        COMMAND\n          ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS}\n          ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/TableWidget.h -o\n          ${CMAKE_CURRENT_BINARY_DIR}/moc_TableWidget.cpp\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/TableWidget.h\n      )\n      target_sources(\n        grplot\n        PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/moc_grplot_mainwindow.cxx\n                ${CMAKE_CURRENT_BINARY_DIR}/moc_grplot_widget.cxx\n                ${CMAKE_CURRENT_BINARY_DIR}/moc_AddElementWidget.cpp\n                ${CMAKE_CURRENT_BINARY_DIR}/moc_Bounding_logic.cpp\n                ${CMAKE_CURRENT_BINARY_DIR}/moc_Bounding_object.cpp\n                ${CMAKE_CURRENT_BINARY_DIR}/moc_CustomTreeWidgetItem.cpp\n                ${CMAKE_CURRENT_BINARY_DIR}/moc_EditElementWidget.cpp\n                ${CMAKE_CURRENT_BINARY_DIR}/moc_TableWidget.cpp\n                ${CMAKE_CURRENT_BINARY_DIR}/moc_TreeWidget.cpp\n                ${CMAKE_CURRENT_BINARY_DIR}/moc_grm_args_t_wrapper.cpp\n                ${CMAKE_CURRENT_BINARY_DIR}/moc_receiver_thread.cpp\n      )\n    else()\n      set_target_properties(grplot PROPERTIES AUTOMOC ON AUTORCC ON)\n      target_sources(\n        grplot\n        PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplot_mainwindow.hxx\n                ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplot_widget.hxx\n                ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/AddElementWidget.h\n                ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/Bounding_logic.h\n                ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/Bounding_object.h\n                ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/CustomTreeWidgetItem.h\n                ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/EditElementWidget.h\n                ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/TableWidget.h\n                ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/TreeWidget.h\n                ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/qtterm/grm_args_t_wrapper.h\n                ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/qtterm/receiver_thread.h\n      )\n    endif()\n    if(APPLE)\n      set_target_properties(grplot PROPERTIES MACOSX_BUNDLE_GUI_IDENTIFIER \"de.fz-juelich.grplot\")\n    endif()\n    if(WIN32)\n      target_compile_definitions(grplot PRIVATE GR_STATIC_LIB)\n    endif()\n    target_compile_options(grplot PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  else()\n    add_executable(grplot lib/grm/grplot/grplot.cxx)\n  endif()\n  string(APPEND GR_REPORT \"- grplot: Yes\\n\")\nelse()\n  string(APPEND GR_REPORT \"- grplot: No (Qt6 and Qt5 not found)\\n\")\nendif()\n\nstring(APPEND GR_REPORT \"\\nGRM integrations:\\n\")\nif(TARGET XercesC::XercesC)\n  string(APPEND GR_REPORT \"- Xerces-C++: Yes\\n\")\nelse()\n  string(APPEND GR_REPORT \"- Xerces-C++: No\\n\")\nendif()\n\nif(GR_BUILD_DEMOS)\n  add_executable(gksdemo lib/gks/demo.c)\n  target_link_libraries(gksdemo PUBLIC gks_static)\n  target_compile_options(gksdemo PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  set_target_properties(gksdemo PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON)\n\n  add_executable(grdemo lib/gr/demo.c)\n  target_link_libraries(grdemo PUBLIC GR::GR)\n  target_link_libraries(grdemo PUBLIC GR::GKS)\n  target_compile_options(grdemo PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  set_target_properties(grdemo PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON)\n\n  add_subdirectory(lib/grm/test/public_api/grm grm_test_public_api)\n  add_subdirectory(lib/grm/test/internal_api/grm grm_test_internal_api)\nendif()\n\nif(GR_BUILD_GKSM)\n  add_executable(gksm lib/gks/gksm.c)\n  target_link_libraries(gksm PUBLIC gks_static)\n  target_compile_options(gksm PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT})\n  set_target_properties(gksm PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON)\nendif()\n\nif(GR_INSTALL)\n  install(FILES LICENSE.md DESTINATION ${CMAKE_INSTALL_DOCDIR}/)\n  install(\n    FILES lib/grm/src/grm/dom_render/graphics_tree/schema.xsd\n    DESTINATION ${CMAKE_INSTALL_DATADIR}/xml/GRM\n    RENAME grm_graphics_tree_schema.xsd\n  )\n  install(\n    FILES lib/grm/src/grm/dom_render/graphics_tree/private_schema.xsd\n    DESTINATION ${CMAKE_INSTALL_DATADIR}/xml/GRM\n    RENAME grm_graphics_tree_private_schema.xsd\n  )\n  include(CMakePackageConfigHelpers)\n  configure_package_config_file(\n    \"cmake/Config.cmake.in\" \"GRConfig.cmake\" INSTALL_DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/GR\"\n  )\n  write_basic_package_version_file(\n    ${CMAKE_CURRENT_BINARY_DIR}/GRConfigVersion.cmake\n    VERSION ${GR_VERSION}\n    COMPATIBILITY SameMajorVersion\n  )\n  install(FILES \"${CMAKE_CURRENT_BINARY_DIR}/GRConfig.cmake\" \"${CMAKE_CURRENT_BINARY_DIR}/GRConfigVersion.cmake\"\n          DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/GR\"\n  )\n  install(\n    TARGETS gks_shared gr_shared gr3_shared grm_shared\n    EXPORT GRTargets\n    ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n    LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n    RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n  )\n  if(TARGET qt4gr)\n    install(\n      TARGETS qt4gr\n      EXPORT GRTargets\n      ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n    )\n  endif()\n  if(TARGET qt5gr)\n    install(\n      TARGETS qt5gr\n      EXPORT GRTargets\n      ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n    )\n  endif()\n  if(TARGET qt6gr)\n    install(\n      TARGETS qt6gr\n      EXPORT GRTargets\n      ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n    )\n  endif()\n  install(\n    EXPORT GRTargets\n    DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/GR\"\n    NAMESPACE GR::\n  )\n  foreach(PACKAGE gr gr3 gks grm)\n    configure_file(\"${PACKAGE}.pc.in\" \"${CMAKE_CURRENT_BINARY_DIR}/${PACKAGE}.pc\" @ONLY)\n    install(FILES \"${CMAKE_CURRENT_BINARY_DIR}/${PACKAGE}.pc\" DESTINATION \"${CMAKE_INSTALL_LIBDIR}/pkgconfig/\")\n  endforeach()\n  install(\n    TARGETS gks_static\n            gr_static\n            gr3_static\n            grm_static\n            aggplugin\n            cairoplugin\n            glplugin\n            gsplugin\n            gtkplugin\n            pgfplugin\n            svgplugin\n            videoplugin\n            wmfplugin\n            x11plugin\n            zmqplugin\n            qtplugin\n            qt5plugin\n            qt6plugin\n    ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n    LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n    RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n  )\n  if(TARGET gr3platform)\n    install(\n      TARGETS gr3platform\n      LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n    )\n  endif()\n  if(TARGET qt4gr)\n    install(\n      TARGETS qt4gr\n      ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n    )\n  endif()\n  if(TARGET qt5gr)\n    install(\n      TARGETS qt5gr\n      ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n    )\n  endif()\n  if(TARGET qt6gr)\n    install(\n      TARGETS qt6gr\n      ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n    )\n  endif()\n  if(TARGET quartzplugin)\n    install(\n      TARGETS quartzplugin\n      ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n      RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n    )\n  endif()\n  if(TARGET GKSTerm)\n    if(XCODE_BUILD)\n      install(\n        DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm/GKSTerm.app\n        DESTINATION Applications\n        USE_SOURCE_PERMISSIONS\n      )\n    else()\n      install(TARGETS GKSTerm BUNDLE DESTINATION Applications)\n    endif()\n  endif()\n  if(TARGET gksqt)\n    install(\n      TARGETS gksqt\n      RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n      BUNDLE DESTINATION Applications\n    )\n  endif()\n  if(TARGET grplot)\n    install(\n      TARGETS grplot\n      RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n      BUNDLE DESTINATION Applications\n    )\n    install(\n      FILES lib/grm/grplot/README.md\n      DESTINATION ${CMAKE_INSTALL_DATAROOTDIR}/doc/grplot\n      RENAME grplot.man.md\n    )\n  endif()\n  install(FILES lib/gr/gr.h lib/gks/gks.h lib/gr3/gr3.h lib/grm/include/grm.h DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/)\n  install(DIRECTORY lib/grm/include/grm DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/)\n  if(TARGET qt4gr\n     OR TARGET qt5gr\n     OR TARGET qt6gr\n  )\n    install(FILES lib/gr/qtgr/grwidget.h DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/)\n  endif()\n  install(\n    DIRECTORY lib/gks/fonts\n    DESTINATION ${CMAKE_INSTALL_PREFIX}/\n    USE_SOURCE_PERMISSIONS\n  )\nendif()\n\nmessage(${GR_REPORT})\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/gstools",
            "repo_link": "https://github.com/GeoStat-Framework/GSTools",
            "content": {
                "codemeta": "",
                "readme": "# Welcome to GSTools\n[![GMD](https://img.shields.io/badge/GMD-10.5194%2Fgmd--15--3161--2022-orange)](https://doi.org/10.5194/gmd-15-3161-2022)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1313628.svg)](https://doi.org/10.5281/zenodo.1313628)\n[![PyPI version](https://badge.fury.io/py/gstools.svg)](https://badge.fury.io/py/gstools)\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/gstools.svg)](https://anaconda.org/conda-forge/gstools)\n[![Build Status](https://github.com/GeoStat-Framework/GSTools/workflows/Continuous%20Integration/badge.svg?branch=main)](https://github.com/GeoStat-Framework/GSTools/actions)\n[![Coverage Status](https://coveralls.io/repos/github/GeoStat-Framework/GSTools/badge.svg?branch=main)](https://coveralls.io/github/GeoStat-Framework/GSTools?branch=main)\n[![Documentation Status](https://readthedocs.org/projects/gstools/badge/?version=latest)](https://geostat-framework.readthedocs.io/projects/gstools/en/stable/?badge=stable)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/GeoStat-Framework/GSTools/main/docs/source/pics/gstools.png\" alt=\"GSTools-LOGO\" width=\"251px\"/>\n</p>\n\n<p align=\"center\"><b>Get in Touch!</b></p>\n<p align=\"center\">\n<a href=\"https://github.com/GeoStat-Framework/GSTools/discussions\"><img src=\"https://img.shields.io/badge/GitHub-Discussions-f6f8fa?logo=github&style=flat\" alt=\"GH-Discussions\"/></a>\n<a href=\"https://swung.slack.com/messages/gstools\"><img src=\"https://img.shields.io/badge/Swung-Slack-4A154B?style=flat&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAABoAAAAaCAYAAACpSkzOAAAABmJLR0QA%2FwD%2FAP%2BgvaeTAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAAB3RJTUUH5AYaFSENGSa5qgAABmZJREFUSMeFlltsVNcVhr%2B1z5m7Zzy%2BxaBwcQrGQOpgCAkKtSBQIqJepKhPBULpQ6sKBVWVKqXtSy%2BR0qYXqa2qRmlDCzjBEZGKUCK1TWqlNiGIEKDQBtf4Fki4OIxnxrex53LOXn2YwbjEtOvlHG3tvX%2Btf%2B21%2Fl%2BYJ1QVEbn1vwLYBWwCVgG1lW0ZoA%2FoAQ6LSP%2BdZ%2BeGzAMiIqK%2Bem0GpxNYVeBj3j2b4NCfM2QnfAAaa11al4fZuCZK24owQJ9v%2BbLryIVbd9wVSNUaEWNVtQPYfXHmAD0T32ZJeBM1Q8d0zzMDUpMwAFgLJU%2BxClURw9NfqedLWxMAHSKyR1WNiNhPAM0B6c%2FbdPORTLuOeUMSNkmMBHgyeo32bwwRDMh8bDM%2BZVl0j6uvPrdYknFnSESWzwUzt%2BkyVlUHx7zh5j%2BmPkXBjosjLkWdominiMQ%2BoiEZxuq8OFRXGXJ5K5%2Fde5nha8VlqjooIlZVBcBUiqeqemjGppd1ptfhSpS8pmmN7GVf4whPNY4Di9m%2BMcR03nK3sBbCQeFbv7gBsExVOyp3l6nz1VtjcM4fTK3Uok5IXtPsrHuPevcBXk8d4dWPX6I%2BsIB9wf1s%2B2Y%2FVbFynUIBIeDeplIECiXl5Iv3kbLogogRgbWukfNumT%2FnlYszBxj3hwXg0cQvqXcfYNu5tVyYPE%2B1G8dXn%2BfW72fH49U8sSlOPGr4SccoF4cKs3WzFrY%2BFCMUNmz%2Ba0aeWR1l15JwJ7DaVPpk1YnJ7xIxtQRNjDXRvTx%2F9ef0Tl0g6SYQhAlvmkH%2Fgv74qUaiTSG8ewJ0%2FGgRK5aG8Cts5ouWDa1RxoDRovK9i9MAq1S12QA7b5ROUdBxBIeQ1ACG49m%2FEXPis7Qk3ChHbx6Qw1dgXVeWB7uyDOctP%2Fx6w2zdrIVIyFCyiq8wXlJOZzyAXQbY%2FGGhC8EAilJ%2BVg7ufxU6IAHeSvewfQEadiDuCr%2B6NE1LU4hwUFAF1xFGRkvEjVDlgiPwVqoEsNkAq0ZKp3EIYrFM2xGm7Uc8u%2FzXjHkTmHIHoCiDM73E3IIsDCtRV3gn7QHQ0hTCt0ooKLw%2FWCAM1AcNISOcHSsBrDRAbc7eQMQBFFciHM18kaZIMz3r%2F0HO5mazytsiw%2FmTtCYiGGCkQlltwkEVjMDVmyUA6oIGR%2BDGjAWoM3f2giHAhH%2BFI5nPsDrWxqWNE9S4tUz5k1S7cQ5df4k9S6qY9JRipXtr4w5WQYH0eHkWrqxy8FTn3AvpmFmIqj%2B76EiQjNfHH1JNWFKc3vABj9V9npw%2FRXfmBNsaoTRnRAQDAgqqMJr1KBWUtUmHaR8WRgzAqAH6FgYexqd4R2Yuns5wcLSFK4U36bj%2FdbbUbGdoZoCi3uS%2Bqtt73TlNWygpqXGfZTGXnKesrwkA9BmgZ0noMZT5R0tQ4hzLfo4rhS46W%2F%2BCAn3T7%2BhDySiWMl2RkHArP8dAesKjPixYVbbUBwB6DHB4QWADIamuHPtkhE0t3ZP7ANhe9zgvXP2dfK0pymRJmQLiEYNW6mEVljYGuDzlkwwaHq51AQ4bERkAetvjP2XCT6H480AJeZsB4N7QYt7OnuSROtRXJV2wNNS4qIJvlbUtERJxhxcv5%2FlNWwygV0QGyzKBv%2FP%2ByFfZXf%2ButoR3UuXcS95mKNgxSjpN3qZZFHwUgFPjx5n2c9wo9ktrtcOZtMeWB2NEw4b2thivPLuIS1M%2BAzmrTy4O4ys7Zv1B5fsnVdWCr7PxYf7vej73ex2YeU1VVY9nu7ShG63vRo%2Fe%2FK1%2B518FbXkjo3OjO1XU2LFRzRZ9VdWDczFQ1VsCOHgpd1G%2FcG6jHrj2vPbn%2BjVdHNfr%2BRH92eXva2MPuvxEQpe%2BHdEnzm%2FQf4%2BrRo%2BldMUbGd393oS2dWU0cDSlw1OequrALVG9Q8rLsquqg2OlzLL2Myu1N5eShgB4CjEnSMSJYrX8Oj0t8UH7NMnX0iSDwmhBWRl3tKs9IcmgGRSRZqtqzFwpL4uWWKvWiMjyZKC24%2F1HbsrLn95Pwk3gCpS0yIw%2Fg6clPC2RLc3QmzvJupoARQsvrItxZmtSkkFz6E6Q%2F2m3PFta44jbCaw%2BO3GK7uybnJs8xfXC1fLYCdTz9NIfsCS0mYVhAHp9ZYdr5J%2F%2F127dxUA2AzuBzRUDWVfZlq4YyG6gs9ImdzWQ%2FwFNRlgCFdG5bAAAAABJRU5ErkJggg%3D%3D\" alt=\"Slack-Swung\"/></a>\n<a href=\"https://gitter.im/GeoStat-Framework/GSTools\"><img src=\"https://img.shields.io/badge/Gitter-GeoStat--Framework-ed1965?logo=gitter&style=flat\" alt=\"Gitter-GSTools\"/></a>\n<a href=\"mailto:info@geostat-framework.org\"><img src=\"https://img.shields.io/badge/Email-GeoStat--Framework-468a88?style=flat&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbDpzcGFjZT0icHJlc2VydmUiIHdpZHRoPSI1MDAiIGhlaWdodD0iNTAwIj48cGF0aCBkPSJNNDQ4IDg4SDUyYy0yNyAwLTQ5IDIyLTQ5IDQ5djIyNmMwIDI3IDIyIDQ5IDQ5IDQ5aDM5NmMyNyAwIDQ5LTIyIDQ5LTQ5VjEzN2MwLTI3LTIyLTQ5LTQ5LTQ5em0xNiA0OXYyMjZsLTIgNy0xMTUtMTE2IDExNy0xMTd6TTM2IDM2M1YxMzdsMTE3IDExN0wzOCAzNzBsLTItN3ptMjE5LTYzYy0zIDMtNyAzLTEwIDBMNjYgMTIxaDM2OHptLTc5LTIzIDQ2IDQ2YTM5IDM5IDAgMCAwIDU2IDBsNDYtNDYgMTAxIDEwMkg3NXoiIHN0eWxlPSJmaWxsOiNmNWY1ZjU7ZmlsbC1vcGFjaXR5OjEiLz48L3N2Zz4=\" alt=\"Email\"/></a>\n<a href=\"https://twitter.com/GSFramework\"><img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/GSFramework?style=social\"></a>\n</p>\n\n<p align=\"center\"><b>Youtube Tutorial on GSTools</b><br></p>\n\n<p align=\"center\">\n<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=qZBJ-AZXq6Q\" target=\"_blank\">\n<img src=\"http://img.youtube.com/vi/qZBJ-AZXq6Q/0.jpg\" alt=\"GSTools Transform 22 tutorial\" width=\"480\" height=\"360\" border=\"0\" />\n</a>\n</p>\n\n## Purpose\n\n<img align=\"right\" width=\"450\" src=\"https://raw.githubusercontent.com/GeoStat-Framework/GSTools/main/docs/source/pics/demonstrator.png\" alt=\"\">\n\nGeoStatTools provides geostatistical tools for various purposes:\n- random field generation, including periodic boundaries\n- simple, ordinary, universal and external drift kriging\n- conditioned field generation\n- incompressible random vector field generation\n- (automated) variogram estimation and fitting\n- directional variogram estimation and modelling\n- data normalization and transformation\n- many readily provided and even user-defined covariance models\n- metric spatio-temporal modelling\n- plotting and exporting routines\n\n\n## Installation\n\n\n### conda\n\nGSTools can be installed via [conda][conda_link] on Linux, Mac, and Windows.\nInstall the package by typing the following command in a command terminal:\n\n    conda install gstools\n\nIn case conda forge is not set up for your system yet, see the easy to follow\ninstructions on [conda forge][conda_forge_link]. Using conda, the parallelized\nversion of GSTools should be installed.\n\n\n### pip\n\nGSTools can be installed via [pip][pip_link] on Linux, Mac, and Windows.\nOn Windows you can install [WinPython][winpy_link] to get Python and pip\nrunning. Install the package by typing the following command in a command terminal:\n\n    pip install gstools\n\nTo install the latest development version via pip, see the\n[documentation][doc_install_link].\nOne thing to point out is that this way, the non-parallel version of GSTools\nis installed. In case you want the parallel version, follow these easy\n[steps][doc_install_link].\n\n\n## Citation\n\nIf you are using GSTools in your publication please cite our paper:\n\n> Müller, S., Schüler, L., Zech, A., and Heße, F.:\n> GSTools v1.3: a toolbox for geostatistical modelling in Python,\n> Geosci. Model Dev., 15, 3161–3182, https://doi.org/10.5194/gmd-15-3161-2022, 2022.\n\nYou can cite the Zenodo code publication of GSTools by:\n\n> Sebastian Müller & Lennart Schüler. GeoStat-Framework/GSTools. Zenodo. https://doi.org/10.5281/zenodo.1313628\n\nIf you want to cite a specific version, have a look at the [Zenodo site](https://doi.org/10.5281/zenodo.1313628).\n\n\n## Documentation for GSTools\n\nYou can find the documentation under [geostat-framework.readthedocs.io][doc_link].\n\n\n### Tutorials and Examples\n\nThe documentation also includes some [tutorials][tut_link], showing the most important use cases of GSTools, which are\n\n- [Random Field Generation][tut1_link]\n- [The Covariance Model][tut2_link]\n- [Variogram Estimation][tut3_link]\n- [Random Vector Field Generation][tut4_link]\n- [Kriging][tut5_link]\n- [Conditioned random field generation][tut6_link]\n- [Field transformations][tut7_link]\n- [Geographic Coordinates][tut8_link]\n- [Spatio-Temporal Modelling][tut9_link]\n- [Normalizing Data][tut10_link]\n- [Miscellaneous examples][tut0_link]\n\nThe associated python scripts are provided in the `examples` folder.\n\n\n## Spatial Random Field Generation\n\nThe core of this library is the generation of spatial random fields. These fields are generated using the randomisation method, described by [Heße et al. 2014][rand_link].\n\n[rand_link]: https://doi.org/10.1016/j.envsoft.2014.01.013\n\n\n### Examples\n\n#### Gaussian Covariance Model\n\nThis is an example of how to generate a 2 dimensional spatial random field with a gaussian covariance model.\n\n```python\nimport gstools as gs\n# structured field with a size 100x100 and a grid-size of 1x1\nx = y = range(100)\nmodel = gs.Gaussian(dim=2, var=1, len_scale=10)\nsrf = gs.SRF(model)\nsrf((x, y), mesh_type='structured')\nsrf.plot()\n```\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/GeoStat-Framework/GSTools/main/docs/source/pics/gau_field.png\" alt=\"Random field\" width=\"600px\"/>\n</p>\n\nGSTools also provides support for [geographic coordinates](https://en.wikipedia.org/wiki/Geographic_coordinate_system).\nThis works perfectly well with [cartopy](https://scitools.org.uk/cartopy/docs/latest/index.html).\n\n```python\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport gstools as gs\n# define a structured field by latitude and longitude\nlat = lon = range(-80, 81)\nmodel = gs.Gaussian(latlon=True, len_scale=777, geo_scale=gs.KM_SCALE)\nsrf = gs.SRF(model, seed=12345)\nfield = srf.structured((lat, lon))\n# Orthographic plotting with cartopy\nax = plt.subplot(projection=ccrs.Orthographic(-45, 45))\ncont = ax.contourf(lon, lat, field, transform=ccrs.PlateCarree())\nax.coastlines()\nax.set_global()\nplt.colorbar(cont)\n```\n\n<p align=\"center\">\n<img src=\"https://github.com/GeoStat-Framework/GeoStat-Framework.github.io/raw/master/img/GS_globe.png\" alt=\"lat-lon random field\" width=\"600px\"/>\n</p>\n\nA similar example but for a three dimensional field is exported to a [VTK](https://vtk.org/) file, which can be visualized with [ParaView](https://www.paraview.org/) or [PyVista](https://docs.pyvista.org) in Python:\n\n```python\nimport gstools as gs\n# structured field with a size 100x100x100 and a grid-size of 1x1x1\nx = y = z = range(100)\nmodel = gs.Gaussian(dim=3, len_scale=[16, 8, 4], angles=(0.8, 0.4, 0.2))\nsrf = gs.SRF(model)\nsrf((x, y, z), mesh_type='structured')\nsrf.vtk_export('3d_field') # Save to a VTK file for ParaView\n\nmesh = srf.to_pyvista() # Create a PyVista mesh for plotting in Python\nmesh.contour(isosurfaces=8).plot()\n```\n\n<p align=\"center\">\n<img src=\"https://github.com/GeoStat-Framework/GeoStat-Framework.github.io/raw/master/img/GS_pyvista.png\" alt=\"3d Random field\" width=\"600px\"/>\n</p>\n\n\n## Estimating and Fitting Variograms\n\nThe spatial structure of a field can be analyzed with the variogram, which contains the same information as the covariance function.\n\nAll covariance models can be used to fit given variogram data by a simple interface.\n\n### Example\n\nThis is an example of how to estimate the variogram of a 2 dimensional unstructured field and estimate the parameters of the covariance\nmodel again.\n\n```python\nimport numpy as np\nimport gstools as gs\n# generate a synthetic field with an exponential model\nx = np.random.RandomState(19970221).rand(1000) * 100.\ny = np.random.RandomState(20011012).rand(1000) * 100.\nmodel = gs.Exponential(dim=2, var=2, len_scale=8)\nsrf = gs.SRF(model, mean=0, seed=19970221)\nfield = srf((x, y))\n# estimate the variogram of the field\nbin_center, gamma = gs.vario_estimate((x, y), field)\n# fit the variogram with a stable model. (no nugget fitted)\nfit_model = gs.Stable(dim=2)\nfit_model.fit_variogram(bin_center, gamma, nugget=False)\n# output\nax = fit_model.plot(x_max=max(bin_center))\nax.scatter(bin_center, gamma)\nprint(fit_model)\n```\n\nWhich gives:\n\n```python\nStable(dim=2, var=1.85, len_scale=7.42, nugget=0.0, anis=[1.0], angles=[0.0], alpha=1.09)\n```\n\n<p align=\"center\">\n<img src=\"https://github.com/GeoStat-Framework/GeoStat-Framework.github.io/raw/master/img/GS_vario_est.png\" alt=\"Variogram\" width=\"600px\"/>\n</p>\n\n\n## Kriging and Conditioned Random Fields\n\nAn important part of geostatistics is Kriging and conditioning spatial random\nfields to measurements. With conditioned random fields, an ensemble of field realizations with their variability depending on the proximity of the measurements can be generated.\n\n### Example\nFor better visualization, we will condition a 1d field to a few \"measurements\", generate 100 realizations and plot them:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gstools as gs\n\n# conditions\ncond_pos = [0.3, 1.9, 1.1, 3.3, 4.7]\ncond_val = [0.47, 0.56, 0.74, 1.47, 1.74]\n\n# conditioned spatial random field class\nmodel = gs.Gaussian(dim=1, var=0.5, len_scale=2)\nkrige = gs.krige.Ordinary(model, cond_pos, cond_val)\ncond_srf = gs.CondSRF(krige)\n# same output positions for all ensemble members\ngrid_pos = np.linspace(0.0, 15.0, 151)\ncond_srf.set_pos(grid_pos)\n\n# seeded ensemble generation\nseed = gs.random.MasterRNG(20170519)\nfor i in range(100):\n    field = cond_srf(seed=seed(), store=f\"field_{i}\")\n    plt.plot(grid_pos, field, color=\"k\", alpha=0.1)\nplt.scatter(cond_pos, cond_val, color=\"k\")\nplt.show()\n```\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/GeoStat-Framework/GSTools/main/docs/source/pics/cond_ens.png\" alt=\"Conditioned\" width=\"600px\"/>\n</p>\n\n## User Defined Covariance Models\n\nOne of the core-features of GSTools is the powerful\n[CovModel][cov_link]\nclass, which allows to easy define covariance models by the user.\n\n### Example\n\nHere we re-implement the Gaussian covariance model by defining just a\n[correlation][cor_link] function, which takes a non-dimensional distance ``h = r/l``:\n\n```python\nimport numpy as np\nimport gstools as gs\n# use CovModel as the base-class\nclass Gau(gs.CovModel):\n    def cor(self, h):\n        return np.exp(-h**2)\n```\n\nAnd that's it! With ``Gau`` you now have a fully working covariance model,\nwhich you could use for field generation or variogram fitting as shown above.\n\nHave a look at the [documentation ][doc_link] for further information on incorporating\noptional parameters and optimizations.\n\n\n## Incompressible Vector Field Generation\n\nUsing the original [Kraichnan method][kraichnan_link], incompressible random\nspatial vector fields can be generated.\n\n\n### Example\n\n```python\nimport numpy as np\nimport gstools as gs\nx = np.arange(100)\ny = np.arange(100)\nmodel = gs.Gaussian(dim=2, var=1, len_scale=10)\nsrf = gs.SRF(model, generator='VectorField', seed=19841203)\nsrf((x, y), mesh_type='structured')\nsrf.plot()\n```\n\nyielding\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/GeoStat-Framework/GSTools/main/docs/source/pics/vec_srf_tut_gau.png\" alt=\"vector field\" width=\"600px\"/>\n</p>\n\n\n[kraichnan_link]: https://doi.org/10.1063/1.1692799\n\n\n## VTK/PyVista Export\n\nAfter you have created a field, you may want to save it to file, so we provide\na handy [VTK][vtk_link] export routine using the `.vtk_export()` or you could\ncreate a VTK/PyVista dataset for use in Python with to `.to_pyvista()` method:\n\n```python\nimport gstools as gs\nx = y = range(100)\nmodel = gs.Gaussian(dim=2, var=1, len_scale=10)\nsrf = gs.SRF(model)\nsrf((x, y), mesh_type='structured')\nsrf.vtk_export(\"field\") # Saves to a VTK file\nmesh = srf.to_pyvista() # Create a VTK/PyVista dataset in memory\nmesh.plot()\n```\n\nWhich gives a RectilinearGrid VTK file ``field.vtr`` or creates a PyVista mesh\nin memory for immediate 3D plotting in Python.\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/GeoStat-Framework/GSTools/main/docs/source/pics/pyvista_export.png\" alt=\"pyvista export\" width=\"600px\"/>\n</p>\n\n\n## Requirements:\n\n- [NumPy >= 1.20.0](https://www.numpy.org)\n- [SciPy >= 1.1.0](https://www.scipy.org/scipylib)\n- [hankel >= 1.0.0](https://github.com/steven-murray/hankel)\n- [emcee >= 3.0.0](https://github.com/dfm/emcee)\n- [pyevtk >= 1.1.1](https://github.com/pyscience-projects/pyevtk)\n- [meshio >= 5.1.0](https://github.com/nschloe/meshio)\n\n### Optional\n\n- [GSTools-Core >= 0.2.0](https://github.com/GeoStat-Framework/GSTools-Core)\n- [matplotlib](https://matplotlib.org)\n- [pyvista](https://docs.pyvista.org/)\n\n\n## Contact\n\nYou can contact us via <info@geostat-framework.org>.\n\n\n## License\n\n[LGPLv3][license_link] © 2018-2024\n\n[pip_link]: https://pypi.org/project/gstools\n[conda_link]: https://docs.conda.io/en/latest/miniconda.html\n[conda_forge_link]: https://github.com/conda-forge/gstools-feedstock#installing-gstools\n[conda_pip]: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-pkgs.html#installing-non-conda-packages\n[pipiflag]: https://pip-python3.readthedocs.io/en/latest/reference/pip_install.html?highlight=i#cmdoption-i\n[winpy_link]: https://winpython.github.io/\n[license_link]: https://github.com/GeoStat-Framework/GSTools/blob/main/LICENSE\n[cov_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/generated/gstools.covmodel.CovModel.html#gstools.covmodel.CovModel\n[stable_link]: https://en.wikipedia.org/wiki/Stable_distribution\n[doc_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/\n[doc_install_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/#pip\n[tut_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/tutorials.html\n[tut1_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/01_random_field/index.html\n[tut2_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/02_cov_model/index.html\n[tut3_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/03_variogram/index.html\n[tut4_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/04_vector_field/index.html\n[tut5_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/05_kriging/index.html\n[tut6_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/06_conditioned_fields/index.html\n[tut7_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/07_transformations/index.html\n[tut8_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/08_geo_coordinates/index.html\n[tut9_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/09_spatio_temporal/index.html\n[tut10_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/10_normalizer/index.html\n[tut0_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/00_misc/index.html\n[cor_link]: https://en.wikipedia.org/wiki/Autocovariance#Normalization\n[vtk_link]: https://www.vtk.org/\n\n",
                "dependencies": "[build-system]\nrequires = [\n    \"setuptools>=64\",\n    \"setuptools_scm>=7\",\n    \"numpy>=2.0.0rc1,<2.3; python_version >= '3.9'\",\n    \"oldest-supported-numpy; python_version < '3.9'\",\n    \"Cython>=3.0.10,<3.1.0\",\n    \"extension-helpers>=1\",\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nrequires-python = \">=3.8\"\nname = \"gstools\"\ndescription = \"GSTools: A geostatistical toolbox.\"\nauthors = [\n    {name = \"Sebastian Müller, Lennart Schüler\", email = \"info@geostat-framework.org\"},\n]\nreadme = \"README.md\"\nlicense = {text = \"LGPL-3.0\"}\ndynamic = [\"version\"]\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: End Users/Desktop\",\n    \"Intended Audience :: Science/Research\",\n    \"Intended Audience :: Education\",\n    \"License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)\",\n    \"Natural Language :: English\",\n    \"Operating System :: Unix\",\n    \"Operating System :: Microsoft\",\n    \"Operating System :: MacOS\",\n    \"Programming Language :: Python\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3 :: Only\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering\",\n    \"Topic :: Scientific/Engineering :: GIS\",\n    \"Topic :: Scientific/Engineering :: Hydrology\",\n    \"Topic :: Scientific/Engineering :: Mathematics\",\n    \"Topic :: Scientific/Engineering :: Physics\",\n    \"Topic :: Utilities\",\n]\ndependencies = [\n    \"emcee>=3.0.0\",\n    \"hankel>=1.0.0\",\n    \"meshio>=5.1.0\",\n    \"numpy>=1.20.0\",\n    \"pyevtk>=1.1.1\",\n    \"scipy>=1.1.0\",\n]\n\n[project.optional-dependencies]\ndoc = [\n    \"m2r2>=0.2.8\",\n    \"matplotlib>=3.7\",\n    \"meshzoo>=0.7\",\n    \"numpydoc>=1.1\",\n    \"pykrige>=1.5,<2\",\n    \"pyvista>=0.40\",\n    \"sphinx>=7\",\n    \"sphinx-gallery>=0.8\",\n    \"sphinx-rtd-theme>=2\",\n    \"sphinxcontrib-youtube>=1.1\",\n]\nplotting = [\n    \"matplotlib>=3.7\",\n    \"pyvista>=0.40\",\n]\nrust = [\"gstools_core>=1.0.0\"]\ntest = [\"pytest-cov>=3\"]\nlint = [\n    \"black>=24\",\n    \"pylint\",\n    \"isort[colors]\",\n    \"cython-lint\",\n]\n\n[project.urls]\nChangelog = \"https://github.com/GeoStat-Framework/GSTools/blob/main/CHANGELOG.md\"\nConda-Forge = \"https://anaconda.org/conda-forge/gstools\"\nDocumentation = \"https://gstools.readthedocs.io\"\nHomepage = \"https://geostat-framework.org/#gstools\"\nSource = \"https://github.com/GeoStat-Framework/GSTools\"\nTracker = \"https://github.com/GeoStat-Framework/GSTools/issues\"\n\n[tool.setuptools]\nlicense-files = [\"LICENSE\"]\n\n[tool.setuptools_scm]\nwrite_to = \"src/gstools/_version.py\"\nwrite_to_template = \"__version__ = '{version}'\"\nlocal_scheme = \"no-local-version\"\nfallback_version = \"0.0.0.dev0\"\n\n[tool.isort]\nprofile = \"black\"\nmulti_line_output = 3\nline_length = 79\n\n[tool.black]\nline-length = 79\ntarget-version = [\n    \"py38\",\n    \"py39\",\n    \"py310\",\n    \"py311\",\n    \"py312\",\n]\n\n[tool.coverage]\n    [tool.coverage.run]\n    source = [\"gstools\"]\n    omit = [\n        \"*docs*\",\n        \"*examples*\",\n        \"*tests*\",\n        \"*/src/gstools/covmodel/plot.py\",\n        \"*/src/gstools/field/plot.py\",\n    ]\n\n    [tool.coverage.report]\n    exclude_lines = [\n        \"pragma: no cover\",\n        \"def __repr__\",\n        \"def __str__\",\n    ]\n\n[tool.pylint]\n    [tool.pylint.main]\n    extension-pkg-whitelist = [\n        \"numpy\",\n        \"scipy\",\n        \"gstools_core\",\n    ]\n    ignore = \"_version.py\"\n    load-plugins = [\n        \"pylint.extensions.no_self_use\",\n    ]\n\n    [tool.pylint.message_control]\n    disable = [\n        \"R0801\",\n    ]\n\n    [tool.pylint.reports]\n    output-format = \"colorized\"\n\n    [tool.pylint.design]\n    max-args = 20\n    max-locals = 50\n    max-branches = 30\n    max-statements = 85\n    max-attributes = 25\n    max-public-methods = 80\n\n[tool.cibuildwheel]\n# Switch to using build\nbuild-frontend = \"build\"\n# Disable building PyPy wheels on all platforms, 32bit for py3.10/11/12, musllinux builds, py3.6/7\nskip = [\"cp36-*\", \"cp37-*\", \"pp*\", \"*-win32\", \"*-manylinux_i686\", \"*-musllinux_*\"]\n# Run the package tests using `pytest`\ntest-extras = \"test\"\ntest-command = \"pytest -v {package}/tests\"\n\n\"\"\"GSTools: A geostatistical toolbox.\"\"\"\n\nimport os\n\nimport numpy as np\nfrom Cython.Build import cythonize\nfrom extension_helpers import add_openmp_flags_if_available\nfrom setuptools import Extension, setup\n\n# cython extensions\nCY_MODULES = [\n    Extension(\n        name=f\"gstools.{ext}\",\n        sources=[os.path.join(\"src\", \"gstools\", *ext.split(\".\")) + \".pyx\"],\n        include_dirs=[np.get_include()],\n        define_macros=[(\"NPY_NO_DEPRECATED_API\", \"NPY_1_7_API_VERSION\")],\n    )\n    for ext in [\"field.summator\", \"variogram.estimator\", \"krige.krigesum\"]\n]\n# you can set GSTOOLS_BUILD_PARALLEL=0 or GSTOOLS_BUILD_PARALLEL=1\nopen_mp = False\nif int(os.getenv(\"GSTOOLS_BUILD_PARALLEL\", \"0\")):\n    added = [add_openmp_flags_if_available(mod) for mod in CY_MODULES]\n    if any(added):\n        open_mp = True\n    print(f\"## GSTools setup: OpenMP used: {open_mp}\")\nelse:\n    print(\"## GSTools setup: OpenMP not wanted by the user.\")\n\n# setup - do not include package data to ignore .pyx files in wheels\nsetup(\n    ext_modules=cythonize(CY_MODULES, compile_time_env={\"OPENMP\": open_mp}),\n    include_package_data=False,\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/habitat-sampler",
            "repo_link": "https://git.gfz-potsdam.de/habitat-sampler/HabitatSampler",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/hcocena",
            "repo_link": "https://github.com/MarieOestreich/hCoCena",
            "content": {
                "codemeta": "",
                "readme": "# hCoCena - Horizontal integration and analysis of transcriptomics datasets [[paper](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btac589/6677225)]\n\nhCoCena is an R-package that allows you to integrate and jointly analyse multiple transcriptomic datasets or simply analyse a single dataset if you don't want to do any data integration! hCoCena uses network representations of the data as the basis for integration. You can find more details of how that works in our [paper](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btac589/6677225) . Below, you will find some info on how to install the package and tips for using it. \n\n## Installation\nTo install hcocena (v1.1.1) from this repo, run the codeline provided in the `install_hcocena.R` script.\nTo install versioned dependencies, use the script `install_versioned_dependecies.R`.\n\n## Usage\n**hCoCena is divided into 2 parts:** \n\n**1.** the main analysis that comprises the mandatory steps to process and integrate the data and\n\n**2.** the satellite functions that offer you a plethora of analysis options in a pick & mix kind of fashion. \n\nThe figure below illustrates this: the main analysis is at the center, while the satellite functions can be found in the orbits around it. \nA step-by-step walkthrough of the main analysis steps can be found in the `hcocena_main.Rmd`, the satellite functions are in the `hcocena_saltellite.Rmd`. \n\nhCoCena was written with user-friendliness and customizability in mind. We are doing our best to provide you with plenty of supplementary information that make the usage of the tool easy for you. You can also always extend the tool's functionalities with your on custom scripts and functions to adapt the analysis to your needs! For more details on hCoCena's object structure and where to find the outputs of different analysis steps for customization, please refer to the overview in the [Wiki](https://github.com/MarieOestreich/hCoCena/wiki/Structure-of-the-hcobject) and the extensive function documentations you can access from within R Studio.\n\n\n![hCoCenaFig1](https://user-images.githubusercontent.com/50077786/158609782-2048c06e-0420-4c3f-8680-5d99f91d6905.jpg)\n*Marie Oestreich, Lisa Holsten, Shobhit Agrawal, Kilian Dahm, Philipp Koch, Han Jin, Matthias Becker, Thomas Ulas, hCoCena: horizontal integration and analysis of transcriptomics datasets, Bioinformatics, Volume 38, Issue 20, 15 October 2022, Pages 4727–4734, https://doi.org/10.1093/bioinformatics/btac589*\n\n## Showcase\nTo rerun the showcase example from our original publication, please refer to the branch of version [1.0.1](https://github.com/MarieOestreich/hCoCena/tree/v-1.0.1).\n\n## Wiki\nFor loads of additional information regarding the [satellite functions](https://github.com/MarieOestreich/hCoCena/wiki/Satellite-Functions), [community detection](https://github.com/MarieOestreich/hCoCena/wiki/Background-Info-on-the-Community-Detection-Algorithms) algorithms etc. please check out our carefully curated Wiki pages!\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/heat",
            "repo_link": "https://github.com/helmholtz-analytics/heat",
            "content": {
                "codemeta": "",
                "readme": "<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/helmholtz-analytics/heat/main/doc/images/logo.png\">\n</div>\n\n---\n\nHeat is a distributed tensor framework for high performance data analytics.\n\n# Project Status\n\n[![CPU/CUDA/ROCm tests](https://codebase.helmholtz.cloud/helmholtz-analytics/ci/badges/heat/base/pipeline.svg)](https://codebase.helmholtz.cloud/helmholtz-analytics/ci/-/commits/heat/base)\n[![Documentation Status](https://readthedocs.org/projects/heat/badge/?version=latest)](https://heat.readthedocs.io/en/latest/?badge=latest)\n[![coverage](https://codecov.io/gh/helmholtz-analytics/heat/branch/main/graph/badge.svg)](https://codecov.io/gh/helmholtz-analytics/heat)\n[![license: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Version](https://img.shields.io/pypi/v/heat)](https://pypi.org/project/heat/)\n[![Downloads](https://pepy.tech/badge/heat)](https://pepy.tech/project/heat)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/heat/badges/version.svg)](https://anaconda.org/conda-forge/heat)\n[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/helmholtz-analytics/heat/badge)](https://securityscorecards.dev/viewer/?uri=github.com/helmholtz-analytics/heat)\n[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7688/badge)](https://bestpractices.coreinfrastructure.org/projects/7688)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2531472.svg)](https://doi.org/10.5281/zenodo.2531472)\n[![Benchmarks](https://img.shields.io/badge/Grafana-Benchmarks-2ea44f)](https://57bc8d92-72f2-4869-accd-435ec06365cb.ka.bw-cloud-instance.org:3000/d/adjpqduq9r7k0a/heat-cb?orgId=1)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![JuRSE Code Pick of the Month](https://img.shields.io/badge/JuRSE_Code_Pick-August_2024-blue)](https://www.fz-juelich.de/en/rse/jurse-community/jurse-code-of-the-month/august-2024)\n\n# Table of Contents\n  - [What is Heat for?](#what-is-heat-for)\n  - [Features](#features)\n  - [Getting Started](#getting-started)\n  - [Installation](#installation)\n    - [Requirements](#requirements)\n    - [pip](#pip)\n    - [conda](#conda)\n  - [Support Channels](#support-channels)\n  - [Contribution guidelines](#contribution-guidelines)\n    - [Resources](#resources)\n  - [License](#license)\n  - [Citing Heat](#citing-heat)\n  - [FAQ](#faq)\n  - [Acknowledgements](#acknowledgements)\n\n\n# What is Heat for?\n\nHeat builds on [PyTorch](https://pytorch.org/) and [mpi4py](https://mpi4py.readthedocs.io) to provide high-performance computing infrastructure for memory-intensive applications within the NumPy/SciPy ecosystem.\n\n\nWith Heat you can:\n- port existing NumPy/SciPy code from single-CPU to multi-node clusters with minimal coding effort;\n- exploit the entire, cumulative RAM of your many nodes for memory-intensive operations and algorithms;\n- run your NumPy/SciPy code on GPUs (CUDA, ROCm, coming up: Apple MPS).\n\nFor a example that highlights the benefits of multi-node parallelism, hardware acceleration, and how easy this can be done with the help of Heat, see, e.g., our [blog post on trucated SVD of a 200GB data set](https://helmholtz-analytics.github.io/heat/2023/06/16/new-feature-hsvd.html).\n\nCheck out our [coverage tables](coverage_tables.md) to see which NumPy, SciPy, scikit-learn functions are already supported.\n\n If you need a functionality that is not yet supported:\n  - [search existing issues](https://github.com/helmholtz-analytics/heat/issues) and make sure to leave a comment if someone else already requested it;\n  - [open a new issue](https://github.com/helmholtz-analytics/heat/issues/new/choose).\n\n\nCheck out our [features](#features) and the [Heat API Reference](https://heat.readthedocs.io/en/latest/autoapi/index.html) for a complete list of functionalities.\n\n# Features\n\n* High-performance n-dimensional arrays\n* CPU, GPU, and distributed computation using MPI\n* Powerful data analytics and machine learning methods\n* Seamless integration with the NumPy/SciPy ecosystem\n* Python array API (work in progress)\n\n\n# Getting Started\n\nGo to [Quick Start](quick_start.md) for a quick overview. For more details, see [Installation](#installation).\n\n**You can test your setup** by running the [`heat_test.py`](https://github.com/helmholtz-analytics/heat/blob/main/scripts/heat_test.py) script:\n\n```shell\nmpirun -n 2 python heat_test.py\n```\n\nIt should print something like this:\n\n```shell\nx is distributed:  True\nGlobal DNDarray x:  DNDarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=ht.int32, device=cpu:0, split=0)\nGlobal DNDarray x:\nLocal torch tensor on rank  0 :  tensor([0, 1, 2, 3, 4], dtype=torch.int32)\nLocal torch tensor on rank  1 :  tensor([5, 6, 7, 8, 9], dtype=torch.int32)\n```\n\nCheck out our Jupyter Notebook [**Tutorials**](https://github.com/helmholtz-analytics/heat/blob/main/tutorials/), choose `local` to try things out on your machine, or `hpc` if you have access to an HPC system.\n\nThe complete documentation of the latest version is always deployed on\n[Read the Docs](https://heat.readthedocs.io/).\n\n\n<!-- # Goals\n\nHeat is a flexible and seamless open-source software for high performance data\nanalytics and machine learning. It provides highly optimized algorithms and data structures for tensor computations using CPUs, GPUs, and distributed cluster systems on top of MPI. The goal of Heat is to fill the gap between single-node data analytics and machine learning libraries, and  high-performance computing (HPC). Heat's interface integrates seamlessly with the existing data science ecosystem and makes  writing scalable\nscientific and data science applications as effortless as using NumPy.\n\nHeat allows you to tackle your actual Big Data challenges that go beyond the\ncomputational and memory needs of your laptop and desktop.\n -->\n# Installation\n\n## Requirements\n\n### Basics\n- python >= 3.9\n- MPI (OpenMPI, MPICH, Intel MPI, etc.)\n- mpi4py >= 3.0.0\n- pytorch >= 2.0.0\n\n### Parallel I/O\n- h5py\n- netCDF4\n\n### GPU support\nIn order to do computations on your GPU(s):\n- your CUDA or ROCm installation must match your hardware and its drivers;\n- your [PyTorch installation](https://pytorch.org/get-started/locally/) must be compiled with CUDA/ROCm support.\n\n### HPC systems\nOn most HPC-systems you will not be able to install/compile MPI or CUDA/ROCm yourself. Instead, you will most likely need to load a pre-installed MPI and/or CUDA/ROCm module from the module system. Maybe, you will even find PyTorch, h5py, or mpi4py as (part of) such a module. Note that for optimal performance on GPU, you need to usa an MPI library that has been compiled with CUDA/ROCm support (e.g., so-called \"CUDA-aware MPI\").\n\n\n## pip\nInstall the latest version with\n\n```bash\npip install heat[hdf5,netcdf]\n```\nwhere the part in brackets is a list of optional dependencies. You can omit\nit, if you do not need HDF5 or NetCDF support.\n\n## **conda**\n\nThe conda build includes all dependencies **including OpenMPI**.\n```bash\n conda install -c conda-forge heat\n ```\n\n# Support Channels\n\nGo ahead and ask questions on [GitHub Discussions](https://github.com/helmholtz-analytics/heat/discussions). If you found a bug or are missing a feature, then please file a new [issue](https://github.com/helmholtz-analytics/heat/issues/new/choose). You can also get in touch with us on [Mattermost](https://mattermost.hzdr.de/signup_user_complete/?id=3sixwk9okpbzpjyfrhen5jpqfo) (sign up with your GitHub credentials). Once you log in, you can introduce yourself on the `Town Square` channel.\n\n\n# Contribution guidelines\n\n**We welcome contributions from the community, if you want to contribute to Heat, be sure to review the [Contribution Guidelines](contributing.md) and [Resources](#resources)  before getting started!**\n\nWe use [GitHub issues](https://github.com/helmholtz-analytics/heat/issues) for tracking requests and bugs, please see [Discussions](https://github.com/helmholtz-analytics/heat/discussions) for general questions and discussion. You can also get in touch with us on [Mattermost](https://mattermost.hzdr.de/signup_user_complete/?id=3sixwk9okpbzpjyfrhen5jpqfo) (sign up with your GitHub credentials). Once you log in, you can introduce yourself on the `Town Square` channel.\n\nIf you’re unsure where to start or how your skills fit in, reach out! You can ask us here on GitHub, by leaving a comment on a relevant issue that is already open.\n\n**If you are new to contributing to open source, [this guide](https://opensource.guide/how-to-contribute/) helps explain why, what, and how to get involved.**\n\n\n## Resources\n\n* [Heat Tutorials](https://github.com/helmholtz-analytics/heat/tree/main/tutorials)\n* [Heat API Reference](https://heat.readthedocs.io/en/latest/autoapi/index.html)\n\n### Parallel Computing and MPI:\n\n* David Henty's [course](https://www.archer2.ac.uk/training/courses/200514-mpi/)\n* Wes Kendall's [Tutorials](https://mpitutorial.com/tutorials/)\n* Rolf Rabenseifner's [MPI course material](https://www.hlrs.de/training/self-study-materials/mpi-course-material) (including C, Fortran **and** Python via `mpi4py`)\n\n### mpi4py\n\n* [mpi4py docs](https://mpi4py.readthedocs.io/en/stable/tutorial.html)\n* [Tutorial](https://www.kth.se/blogs/pdc/2019/08/parallel-programming-in-python-mpi4py-part-1/)\n# License\n\nHeat is distributed under the MIT license, see our\n[LICENSE](LICENSE) file.\n\n# Citing Heat\n\n<!-- If you find Heat helpful for your research, please mention it in your publications. You can cite: -->\n\nPlease do mention Heat in your publications if it helped your research. You can cite:\n\n* Götz, M., Debus, C., Coquelin, D., Krajsek, K., Comito, C., Knechtges, P., Hagemeier, B., Tarnawa, M., Hanselmann, S., Siggel, S., Basermann, A. & Streit, A. (2020). HeAT - a Distributed and GPU-accelerated Tensor Framework for Data Analytics. In 2020 IEEE International Conference on Big Data (Big Data) (pp. 276-287). IEEE, DOI: 10.1109/BigData50022.2020.9378050.\n\n```\n@inproceedings{heat2020,\n    title={{HeAT -- a Distributed and GPU-accelerated Tensor Framework for Data Analytics}},\n    author={\n      Markus Götz and\n      Charlotte Debus and\n      Daniel Coquelin and\n      Kai Krajsek and\n      Claudia Comito and\n      Philipp Knechtges and\n      Björn Hagemeier and\n      Michael Tarnawa and\n      Simon Hanselmann and\n      Martin Siggel and\n      Achim Basermann and\n      Achim Streit\n    },\n    booktitle={2020 IEEE International Conference on Big Data (Big Data)},\n    year={2020},\n    pages={276-287},\n    month={December},\n    publisher={IEEE},\n    doi={10.1109/BigData50022.2020.9378050}\n}\n```\n# FAQ\nWork in progress...\n\n  <!-- - Users\n  - Developers\n  - Students\n  - system administrators -->\n\n## Acknowledgements\n\n*This work is supported by the [Helmholtz Association Initiative and\nNetworking Fund](https://www.helmholtz.de/en/about_us/the_association/initiating_and_networking/)\nunder project number ZT-I-0003 and the Helmholtz AI platform grant.*\n\n*This project has received funding from Google Summer of Code (GSoC) in 2022.*\n\n*This work is partially carried out under a [programme](https://activities.esa.int/index.php/4000144045) of, and funded by, the European Space Agency.\nAny view expressed in this repository or related publications can in no way be taken to reflect the official opinion of the European Space Agency.*\n\n---\n\n<div align=\"center\">\n  <a href=\"https://www.dlr.de/EN/Home/home_node.html\"><img src=\"https://raw.githubusercontent.com/helmholtz-analytics/heat/main/doc/images/dlr_logo.svg\" height=\"50px\" hspace=\"3%\" vspace=\"20px\"></a><a href=\"https://www.fz-juelich.de/portal/EN/Home/home_node.html\"><img src=\"https://raw.githubusercontent.com/helmholtz-analytics/heat/main/doc/images/fzj_logo.svg\" height=\"40px\" hspace=\"3%\" vspace=\"20px\"></a><a href=\"http://www.kit.edu/english/index.php\"><img src=\"https://raw.githubusercontent.com/helmholtz-analytics/heat/main/doc/images/kit_logo.svg\" height=\"40px\" hspace=\"3%\" vspace=\"5px\"></a><a href=\"https://www.helmholtz.de/en/\"><img src=\"https://raw.githubusercontent.com/helmholtz-analytics/heat/main/doc/images/helmholtz_logo.svg\" height=\"50px\" hspace=\"3%\" vspace=\"5px\"></a><a href=\"https://www.esa.int/\"><img src=\"https://github.com/user-attachments/assets/2ee251b4-733e-44ea-8d1c-8b75928eef55\" height=\"45px\" hspace=\"3%\" vspace=\"20px\"></a>\n\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.black]\nline-length = 100\n\nfrom setuptools import setup, find_packages\nimport codecs\n\n\nwith codecs.open(\"README.md\", \"r\", \"utf-8\") as handle:\n    long_description = handle.read()\n\n__version__ = None  # appeases flake, assignment in exec() below\nwith open(\"./heat/core/version.py\") as handle:\n    exec(handle.read())\n\nsetup(\n    name=\"heat\",\n    packages=find_packages(exclude=(\"*tests*\", \"*benchmarks*\")),\n    package_data={\"heat.datasets\": [\"*.csv\", \"*.h5\", \"*.nc\"]},\n    version=__version__,\n    description=\"A framework for high-performance data analytics and machine learning.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    author=\"Helmholtz Association\",\n    author_email=\"martin.siggel@dlr.de\",\n    url=\"https://github.com/helmholtz-analytics/heat\",\n    keywords=[\"data\", \"analytics\", \"tensors\", \"distributed\", \"gpu\"],\n    python_requires=\">=3.9\",\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Intended Audience :: Science/Research\",\n        \"Topic :: Scientific/Engineering\",\n    ],\n    install_requires=[\n        \"mpi4py>=3.0.0\",\n        \"numpy>=1.22.0, <2\",\n        \"torch>=2.0.0, <2.5.2\",\n        \"scipy>=1.10.0\",\n        \"pillow>=6.0.0\",\n        \"torchvision>=0.15.2, <0.20.2\",\n    ],\n    extras_require={\n        \"docutils\": [\"docutils>=0.16\"],\n        \"hdf5\": [\"h5py>=2.8.0\"],\n        \"netcdf\": [\"netCDF4>=1.5.6\"],\n        \"dev\": [\"pre-commit>=1.18.3\"],\n        \"examples\": [\"scikit-learn>=0.24.0\", \"matplotlib>=3.1.0\"],\n        \"cb\": [\"perun>=0.2.0\"],\n        \"pandas\": [\"pandas>=1.4\"],\n    },\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/heatnetsim",
            "repo_link": "https://jugit.fz-juelich.de/iek-10/public/simulation/heatnetsim",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/heliport",
            "repo_link": "https://codebase.helmholtz.cloud/heliport/heliport",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/hifis-rsd",
            "repo_link": "https://codebase.helmholtz.cloud/research-software-directory/RSD-as-a-service",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/higgs-dataset-training",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Higgs-Dataset-Training",
            "content": {
                "codemeta": "",
                "readme": "![Python](https://img.shields.io/badge/Python-3670A0?style=plastic&logo=python&logoColor=ffdd54)  ![Pandas](https://img.shields.io/badge/Pandas-%23150458.svg?style=plastic&logo=pandas&logoColor=white) ![NumPy](https://img.shields.io/badge/Numpy-777BB4.svg?style=plastic&logo=numpy&logoColor=white) ![Matplotlib](https://img.shields.io/badge/Matplotlib-%233F4F75.svg?style=plastic&logo=plotly&logoColor=white) ![Keras](https://img.shields.io/badge/Keras-%23D00000.svg?style=plastic&logo=Keras&logoColor=white) ![scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=plastic&logo=scikit-learn&logoColor=white) ![Dask](https://img.shields.io/badge/Dask-%23870000.svg?style=plastic&logo=dask&logoColor=white) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/Higgs-Dataset-Training?style=plastic)\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13133945.svg)](https://doi.org/10.5281/zenodo.13133945)\n\n\n# Model Training and Evaluation for Higgs Dataset\n\n## Overview\n\nThis repository demonstrates training and evaluating a Keras model using the Higgs dataset available from the UCI ML Repository.\n\n> [Higgs Dataset](http://archive.ics.uci.edu/ml/datasets/HIGGS)\n\nThe dataset has been studied in this publication:\n\n> [Searching for Exotic Particles in High-energy Physics with Deep Learning.<br>Baldi, P., P. Sadowski, and D. Whiteson. \nNature Communications 5, 4308 (2014)](https://www.nature.com/articles/ncomms5308)\n\nThe ML pipeline includes downloading the dataset, data preparation, model training, evaluation, feature importance analysis, and visualization of results. Dask is utilised for handling this large datasets for parallel processing.\n\n### Installation\n\n1) Create and source virtual environment:\n```shell\npython -m venv env\nsource env/bin/activate  # On Windows use `env\\Scripts\\activate`\n```\n2) Install the dependencies:\n```shell\npip install -r requirements.txt\n```\n\n### Data\n\nThe Higgs dataset can be downloaded directly from the provided scripts in separate steps\n\n- `download_data.py`  ~ 2.6 GB\n- `data_extraction.py` ~ 7 GB\n- `data_preparation.py` ~ test dataset: 240 MB, trained dataset: 5 GB\n\nAlternatively, you can run directly the main script from the `data/src/main.py`:\n\n```shell\npython data/src/main.py\n```\n\n#### Downloading Data\nDownload a dataset file from the specified URL with a progress bar.\n\n##### Script\n```shell\npython data/download_data.py\n```\n\n##### Example Usage\n```python\nzipDataUrl = 'https://archive.ics.uci.edu/static/public/280/higgs.zip'      # Higgs dataset URL\nzipPath = '../higgs/higgs.zip'\ndownloadDataset(zipDataUrl, zipPath)\ncleanUp(zipPath)        # Clean up downloaded zip file (~ 2.6 GB)\n```\n\n#### Data Extraction\nExtract the contents of a zip dataset and decompress the .gz dataset file to a specified output path.\n\n##### Script\n```shell\npython data/data_extraction.py\n```\n\n##### Example Usage\n```python\nzipDataUrl = 'https://archive.ics.uci.edu/static/public/280/higgs.zip'      # Higgs dataset URL\nextractTo = '../higgs'\nzipPath = os.path.join(extractTo, 'higgs.zip')\ngzCsvPath = os.path.join(extractTo, 'higgs.csv.gz')\nfinalCsvPath = os.path.join(extractTo, 'higgs.csv')\n\nextractZippedData(zipPath, extractTo)\ndecompressGzFile(gzCsvPath, finalCsvPath)\ncleanUp(gzCsvPath)      # Clean up gzipped file (~ 2.6 GB)\n```\n\n#### Data Preparation\nSet column names and separates the test set from the training data based on the dataset description (500,000 test sets).\n\nDataset Description: The first column is the class label (1 for signal, 0 for background), followed by the 28 features (21 low-level features then 7 high-level features). The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features.\n\n##### Script\n```shell\npython data/data_preparation.py\n```\n\n##### Example Usage\n```python\nprepareFrom = '../higgs'\ncsvPath = os.path.join(prepareFrom, 'higgs.csv')\npreparedCsvPath = os.path.join(prepareFrom, 'prepared-higgs.csv')\nprepareData(csvPath, preparedCsvPath)\ncleanUp(csvPath)         # Clean up gzipped file (~ 7.5 GB)\n```\n\n### Loading Data\n\n#### Using Pandas\n\nUse the `dataLoader/data_loader.py` script to load the prepared dataset into a Pandas DataFrame.\n\n##### Script\n```shell\npython data/src/data_loader.py\n```\n\n##### Example Usage\n```python\nfilepath = '../data/higgs/prepared-higgs_train.csv'   # prepared-higgs_test.csv\ndataLoader = DataLoader(filepath)\ndataFrame = dataLoader.loadData()\ndataLoader.previewData(dataFrame)\n```\n\n#### Using Dask\n\nUse the `dataLoader/data_loader_dask.py` script to load the prepared dataset into a Dask DataFrame, which is beneficial for this large dataset.\n\n##### Script\n```shell\npython data/src/data_loader_dask.py\n```\n##### Example Usage:\n```python\nfilepath = '../data/higgs/prepared-higgs_train.csv'   # prepared-higgs_test.csv\ndataLoader = DataLoaderDask(filepath)\ndataFrame = dataLoader.loadData()\ndataLoader.previewData(dataFrame)\n```\n\n### Exploratory Data Analysis (EDA)\n\nProvides various functions for performing EDA, including visualising correlations, checking missing values, and plotting feature distributions.\nThe data analysis plots are saved under `eda/plots`.\n\n##### Script\n```shell\npython exploration/eda.py\n```\n\n##### Example Usage:\n```python\nfilepath = '../data/higgs/prepared-higgs_train.csv'   # prepared-higgs_test.csv\n\n    # using Dask data frame\ndataLoaderDask = DataLoaderDask(filepath)\ndataFrame = dataLoaderDask.loadData()\n\neda = EDA(dataFrame)\neda.describeData()\neda.checkMissingValues()\neda.visualiseFeatureCorrelation()\n\neda.visualizeTargetDistribution()\neda.visualizeFeatureDistribution('feature_1')\neda.visualizeAllFeatureDistributions()\neda.visualizeFeatureScatter('feature_1', 'feature_2')\neda.visualizeTargetDistribution()\neda.visualizeFeatureBoxplot('feature_2')\n```\n\n### Usage\n\n#### Training the Model\nThe model is defined using Keras with the following default architecture for binary classification:\n\n- Input layer with 128 neurons (dense)\n- Hidden layer with 64 neurons (dense)\n- Output layer with 1 neuron (activation function: sigmoid)\n\nYou can customise the model architecture by providing a different `modelBuilder` callable in the ModelTrainer class.\n\nThe trained models and training loss plots are saved under `kerasModel/trainer/trainedModels`.\n\n##### Script\n```shell\npython kerasModel/trainer/model_trainer.py\n```\n\n##### Example Usage:\n```python\nfilePath = '../../data/higgs/prepared-higgs_train.csv'\n\ndef customModel(inputShape: int) -> Model:\n    \"\"\"Example of a custom model builder function for classification\"\"\"\n    model = keras.Sequential([\n        layers.Input(shape=(inputShape,)),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.3),\n        layers.Dense(256, activation='relu'),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n    ])\n    return model\n        \ndataLoaderDask = DataLoaderDask(filePath)\ndataFrame = dataLoaderDask.loadData()\n\n## Optional: Define model training/compiling/defining parameters as a dictionary and pass it to the class constructor\nparams = {\n    \"epochs\": 10,\n    \"batchSize\": 32,\n    \"minSampleSize\": 100000,\n    \"learningRate\": 0.001,\n    \"modelBuilder\": customModel,     # callable\n    \"loss\": 'binary_crossentropy',    \n    \"metrics\": ['accuracy']\n}\ntrainer = ModelTrainer(dataFrame, params)\ntrainer.trainKerasModel()           # optional: Train the Keras model with sampling, Set: trainKerasModel(sample = true, frac = 0.1).\ntrainer.plotTrainingHistory()\n```\n\n#### Evaluating the Model\nThe evaluation script computes metrics like:\n\n- Accuracy\n- Precision\n- Recall (Sensitivity\n- F1 Score\n- Classification Report\n\nThe evaluation includes visualizations such as\n- Confusion Matrix\n- ROC Curve\n\nThe evaluation results are logged and saved to a file under `kerasModel/evaluator/evaluationPlots`.\n\n##### Script\n```shell\npython kerasModel/evaluator/model_evaluator.py\n```\n\n##### Example Usage:\n```python\nmodelPath = '../trainer/trainedModels/keras_model_trained_dataset.keras'\nfilePath = '../../data/higgs/prepared-higgs_train.csv'\n\ndataLoaderDask = DataLoaderDask(filePath)\ndataFrame = dataLoaderDask.loadData()\n\nevaluator = ModelEvaluator(modelPath, dataFrame)\nevaluator.evaluate()\n```\n\n#### Feature Importance Analysis\n\nThe feature importance is computed using permutation importance and visualised using a bar chart. It is implemented once using the Pandas approach (with SciKit) and another using Dask for parallel processing.\n\nThe chart and the result CSV file are saved under `kerasModel/featureImportance/featureImportancePlots`.\n\n##### Script\n```shell\npython kerasModel/featureImportance/feature_importance.py\n```\n\n##### Example Usage:\n```python\nmodelPath = '../trainer/trainedModels/keras_model_test_dataset.keras'\nfilePath = '../../data/higgs/prepared-higgs_test.csv'\n\ndataLoaderDask = DataLoaderDask(filePath)\ndataFrame = dataLoaderDask.loadData()\n\nevaluator = FeatureImportanceEvaluator(modelPath, dataFrame)\nevaluator.evaluate()\n        \n        # Alternatively\nevaluator = FeatureImportanceEvaluator(modelPath, dataFrame, sampleFraction = 0.1, nRepeats=32)  # with sampling\nevaluator.evaluate(withDask = False)        # with pandas\n```\n\n",
                "dependencies": "numpy~=1.26.4\nmatplotlib~=3.9.1\npandas~=2.2.2\nrequests~=2.32.3\ntqdm~=4.66.4\nseaborn~=0.13.2\nscikit-learn~=1.5.1\ndask[complete]~=2024.7.1\ntensorflow~=2.17.0\ncupy-cuda11x~=13.2.0\njoblib~=1.4.2\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/hilbertcurve",
            "repo_link": "https://github.com/jokergoo/HilbertCurve",
            "content": {
                "codemeta": "",
                "readme": "# HilbertCurve <img width=\"300\" alt=\"image\" src=\"https://github.com/jokergoo/HilbertCurve/assets/449218/e40159f5-bbca-4d61-960b-1ba1d744f9e2\" align=\"right\">\n\n\n\n[![R-CMD-check](https://github.com/jokergoo/HilbertCurve/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/HilbertCurve/actions)\n[![codecov](https://img.shields.io/codecov/c/github/jokergoo/HilbertCurve.svg)](https://codecov.io/github/jokergoo/HilbertCurve) \n[![bioc](https://bioconductor.org/shields/downloads/devel/HilbertCurve.svg)](https://bioconductor.org/packages/stats/bioc/HilbertCurve/)\n[![bioc](http://www.bioconductor.org/shields/years-in-bioc/HilbertCurve.svg)](http://bioconductor.org/packages/devel/bioc/html/HilbertCurve.html)\n\n\n\n\n\n[Hilbert curve](https://en.wikipedia.org/wiki/Hilbert_curve) is a type of space-filling curves\nthat fold one dimensional axis into a two dimensional space, but with still keeping the locality.\nIt has advantages to visualize data with long axis in following two aspects:\n\n1. greatly improve resolution for the visualization;\n2. easy to visualize clusters because generally data points in the cluster will also be close in the Hilbert curve. \n\nThis package aims to provide an easy and flexible way to visualize data through Hilbert curve.\nThe implementation and example figures are based on following sources:\n\n- http://mkweb.bcgsc.ca/hilbert/\n- http://corte.si/posts/code/hilbert/portrait/index.html\n- http://bioconductor.org/packages/devel/bioc/html/HilbertVis.html\n\n### Citation\n\nZuguang Gu, et al., [HilbertCurve: an R/Bioconductor package for high-resolution visualization of genomic data.](https://doi.org/10.1093/bioinformatics/btw161)\nBioinformatics 2016\n\n### Install\n\nThe package is at [Bioconductor](http://bioconductor.org/packages/devel/bioc/html/HilbertCurve.html) now\nand you can install the newest version by:\n\n```r\nlibrary(devtools)\ninstall_github(\"jokergoo/ComplexHeatmap\")  # in order to get the newest version of ComplexHeatmap\ninstall_github(\"jokergoo/HilbertCurve\")\n```\n\n### Usage\n\nBasically, there are two steps to make a Hilbert curve.\n\n1. Initialize the curve and also map the one-dimensional axis to the curve.\n2. add low-level graphics by `hc_points()`, `hc_segments()`, ... by giving the positions of the graphics.\n\n```r\nhc = HilbertCurve(1, 100, level = 4)\nhc_points(hc, ...)\nhc_segments(hc, ...)\nhc_rect(hc, ...)\nhc_text(hc, ...)\n```\n\nThere is another 'pixel' mode which provides a high resolution for visualizing genomic data by the Hilbert curve.\n\n```r\nhc = HilbertCurve(1, 100000000000, level = 10)\nhc_layer(hc, ...) # this can be repeated several times to add multiple layers on the curve\nhc_png(hc, ...)\n```\n\n### Examples\n\nRainbow color spectrum:\n\n![](https://cloud.githubusercontent.com/assets/449218/12678993/f184c4de-c6a1-11e5-8c8c-ed3ed938c487.png)\n\nChinese dynasty:\n\n![](https://cloud.githubusercontent.com/assets/449218/12678995/f18981cc-c6a1-11e5-8b66-6222bed67c63.png)\n\nGC percent and genes on chromosome 1:\n\n![](https://cloud.githubusercontent.com/assets/449218/12678996/f18a6646-c6a1-11e5-9e0b-c99cc7a93f0e.png)\n\nAssociation between H3K36me3 histone modification and gene bodies:\n\n![](https://cloud.githubusercontent.com/assets/449218/12678992/f1848320-c6a1-11e5-8225-e6fef169f29b.png)\n\nMethylation on chromosome 1:\n\n![](https://cloud.githubusercontent.com/assets/449218/12678994/f186827e-c6a1-11e5-884a-b9135f24146e.png)\n\nCopy number alterations in 22 chromosomes:\n\n![](https://cloud.githubusercontent.com/assets/449218/12678997/f18e405e-c6a1-11e5-9478-3d8fdc4bc834.png)\n\n### License\n\nMIT @ Zuguang Gu\n\n",
                "dependencies": "Package: HilbertCurve\nType: Package\nTitle: Making 2D Hilbert Curve\nVersion: 1.99.2\nDate: 2024-10-08\nAuthors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"),\n                  comment = c('ORCID'=\"0000-0002-7395-8709\"))\nDepends: R (>= 4.0.0), grid\nImports: methods, utils, png, grDevices, circlize (>= 0.3.3), \n         IRanges, GenomicRanges, polylabelr, Rcpp\nSuggests: knitr, testthat (>= 1.0.0), ComplexHeatmap (>= 1.99.0), markdown,\n          RColorBrewer, RCurl, GetoptLong, rmarkdown\nVignetteBuilder: knitr\nDescription: Hilbert curve is a type of space-filling curves\n    that fold one dimensional axis into a two dimensional space, \n    but with still preserves the locality. This package aims to provide \n    an easy and flexible way to visualize data through Hilbert curve.\nbiocViews: Software, Visualization, Sequencing, Coverage, GenomeAnnotation\nURL: https://github.com/jokergoo/HilbertCurve, https://jokergoo.github.io/HilbertCurve/\nLicense: MIT + file LICENSE\nLinkingTo: Rcpp\nCollate: 00_S4_generic_methods.R\n         HilbertCurve.R\n         hc_polygon.R\n         hc_legend.R\n         utils.R\n         GenomicHilbertCurve.R\n         RcppExports.R\n         zzz.R\n\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/hipsta",
            "repo_link": "https://github.com/Deep-MI/hipsta",
            "content": {
                "codemeta": "",
                "readme": "# Hippocampal Shape and Thickness Analysis\n\n\n## Purpose:\n\nThis repository contains the Hipsta package, a collection of scripts for\nhippocampal shape and thickness analysis as described in our recent [publication](https://doi.org/10.1016/j.neuroimage.2023.120182).\n\n\n## Documentation:\n\nPlease see the documentation pages for a general overview, usage information and\nexamples, and output description. Brief usage information is also available [here](hipsta/doc/DOCUMENTATION.md).\nSome suggestions for running the script can be found in the [tutorial](TUTORIAL.md).\n\n\n## Current status:\n\nThe hippocampal shape and thickness analysis package is currently in its beta\nstage, which means that it's open for testing, but may still contain unresolved\nissues. Future changes with regard to the algorithms, interfaces, and package\nstructure are to be expected.\n\n\n## Feedback:\n\nQuestions, suggestions, and feedback are welcome, and should preferably be\nsubmitted as an [issue](https://github.com/Deep-MI/Hipsta/issues).\n\n\n## Installation:\n\nIt is recommended to run this pipeline within its own virtual environment. A\nvirtual environment can, for example, be created using Python's `virtualenv`\ncommand:\n\n`virtualenv /path/to/a/new/directory/of/your/choice`\n\nActivate the virtual environment as follows:\n\n`source /path/to/a/new/directory/of/your/choice/bin/activate`\n\nThe package is available on `pypi.org`, and can be installed as follows \n(including all required dependencies):\n\n`pip install hipsta`\n\nAlternatively, the following code can be used to download this package from its\nGitHub repository (this will create a 'Hipsta' directory within the current\nworking directory):\n\n`git clone https://github.com/Deep-MI/Hipsta.git`\n\n\nUse the following code to install the downloaded files as a Python package\n(after changing into the 'Hipsta' directory). It will also install all required\ndependencies:\n\n`pip install .`\n\nThe above steps are not necessary when running the [Docker](docker/Docker.md) or\n[Singularity](singularity/Singularity.md) versions of the package.\n\n\n## Requirements:\n\nUnless using the [Docker](docker/Docker.md) or [Singularity](singularity/Singularity.md)\nversions of the package, the following conditions need to be met for running an\nanalysis:\n\n1. A FreeSurfer version (6.x or 7.x) must be sourced, i.e. FREESURFER_HOME must\nexist as an environment variable and point to a valid FreeSurfer installation.\n\n2. A hippocampal subfield segmentation created by FreeSurfer 7.11 or later\nor the ASHS software. A custom segmentation is also permissible (some restrictions\nand settings apply; see [Supported Segmentations](https://github.com/Deep-MI/Hipsta#supported-segmentations)).\n\n3. Python 3.8 or higher including the lapy, numpy, scipy, nibabel, pyvista, and\npyacvd libraries, among others. See `requirements.txt` for a full list, and use\n`pip install -r requirements.txt` to install.\n\n4. The gmsh package (version 2.x; http://gmsh.info) must be installed. Can be\ndownloaded e.g. as binaries for [linux](https://gmsh.info/bin/Linux/gmsh-2.16.0-Linux64.tgz) or\n[MacOSX](https://gmsh.info/bin/MacOSX/gmsh-2.16.0-MacOSX.dmg) . The 'gmsh' binary must\nbe on the $PATH:\n\n    `export PATH=${PATH}:/path/to/gmsh-directory/bin`\n\n\n## References:\n\nPlease cite the following publications if you use these scripts in your work:\n\n- Diers, K., Baumeister, H., Jessen, F., Düzel, E., Berron, D., & Reuter, M. (2023). An automated, geometry-based method for hippocampal shape and thickness analysis. Neuroimage, 276:120182. doi: [10.1016/j.neuroimage.2023.120182](https://doi.org/10.1016/j.neuroimage.2023.120182).\n\nPlease also consider citing the these publications:\n\n- Geuzaine, C., & Remacle, J.-F. (2009). Gmsh: a three-dimensional finite element mesh generator with built-in pre- and post-processing facilities. International Journal for Numerical Methods in Engineering, 79, 1309-1331.\n\n- Andreux, M., Rodola, E., Aubry, M., & Cremers, D. (2014). Anisotropic Laplace-Beltrami operators for shape analysis. In European Conference on Computer Vision (pp. 299-312). Springer, Cham.\n\n- Iglesias, J. E., Augustinack, J. C., Nguyen, K., Player, C. M., Player, A., Wright, M., ... & Fischl, B. (2015). A computational atlas of the hippocampal formation using ex vivo, ultra-high resolution MRI: application to adaptive segmentation of in vivo MRI. Neuroimage, 115, 117-137.\n\n- Yushkevich, P. A., Pluta, J., Wang, H., Ding, S.L., Xie, L., Gertje, E., Mancuso, L., Kliot, D., Das, S. R., & Wolk, D.A. (2015). Automated Volumetry and Regional Thickness Analysis of Hippocampal Subfields and Medial Temporal Cortical Structures in Mild Cognitive Impairment. Human Brain Mapping, 36, 258-287.\n\n",
                "dependencies": "[build-system]\nrequires = ['setuptools >= 61.0.0']\nbuild-backend = 'setuptools.build_meta'\n\n[project]\nname = 'hipsta'\ndescription = 'A python package for hippocampal shape and thickness analysis'\nlicense = {file = 'LICENSE'}\nrequires-python = '>=3.9'\nauthors = [\n    {name = 'Kersten Diers', email = 'kersten.diers@dzne.de'},\n    {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'}\n]\nmaintainers = [\n    {name = 'Kersten Diers', email = 'kersten.diers@dzne.de'},\n    {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'}\n]\nkeywords = [\n    'hippocampus',\n    'shape',\n    'thickness',\n    'geometry',\n]\nclassifiers = [\n    'Operating System :: Unix',\n    'Operating System :: MacOS',\n    'Programming Language :: Python :: 3 :: Only',\n    'Programming Language :: Python :: 3.9',\n    'Programming Language :: Python :: 3.10',\n    'Programming Language :: Python :: 3.11',\n    'Programming Language :: Python :: 3.12',\n    'Natural Language :: English',\n    'License :: OSI Approved :: MIT License',\n    'Intended Audience :: Science/Research',\n]\ndynamic = [\"version\", \"readme\", \"dependencies\"]\n\n[project.optional-dependencies]\nbuild = [\n    'build',\n    'twine',\n]\ndoc = [\n    'furo!=2023.8.17',\n    'matplotlib',\n    'memory-profiler',\n    'numpydoc',\n    'sphinx!=7.2.*',\n    'sphinxcontrib-bibtex',\n    'sphinx-copybutton',\n    'sphinx-design',\n    'sphinx-gallery',\n    'sphinx-issues',\n    'myst-parser',\n    'pypandoc',\n    'nbsphinx',\n    'IPython', # For syntax highlighting in notebooks\n    'ipykernel',\n]\nstyle = [\n    'bibclean',\n    'codespell',\n    'pydocstyle[toml]',\n    'ruff',\n]\ntest = [\n    'pytest',\n    'pytest-cov',\n    'pytest-timeout',\n]\nall = [\n    'hipsta[doc]',\n    'hipsta[build]',\n    'hipsta[style]',\n]\nfull = [\n    'hipsta[all]',\n]\n\n[project.urls]\nhomepage = 'https://github.com/Deep-MI/Hipsta'\ndocumentation = 'https://github.com/Deep-MI/Hipsta'\nsource = 'https://github.com/Deep-MI/Hipsta'\ntracker = 'https://github.com/Deep-MI/Hipsta/issues'\n\n[project.scripts]\nrun_hipsta = 'hipsta.cli:main'\n\n[tool.setuptools.dynamic]\nversion = {file = 'VERSION'}\nreadme = {file = 'README.md', content-type = \"text/markdown\"}\ndependencies = {file = 'requirements.txt'}\n\n[tool.setuptools.packages.find]\ninclude = ['hipsta*']\nexclude = ['docker', 'singularity']\n\n[tool.setuptools.package-data]\n\"hipsta.doc\" = [\"*.md\"]\n\n[tool.pydocstyle]\nconvention = 'numpy'\nignore-decorators = '(copy_doc|property|.*setter|.*getter|pyqtSlot|Slot)'\nmatch = '^(?!setup|__init__|test_).*\\.py'\nmatch-dir = '^fsqc.*'\nadd_ignore = 'D100,D104,D107'\n\n[tool.ruff]\nline-length = 120\nextend-exclude = [\n    \".github\",\n    \"doc\",\n    \"docker\",\n    \"images\",\n    \"singularity\",\n]\n\n[tool.ruff.lint]\n# https://docs.astral.sh/ruff/linter/#rule-selection\nselect = [\n    \"E\",   # pycodestyle\n    \"F\",   # Pyflakes\n    \"UP\",  # pyupgrade\n    \"B\",   # flake8-bugbear\n    \"I\",   # isort\n    # \"SIM\", # flake8-simplify\n]\n\n[tool.ruff.per-file-ignores]\n\"__init__.py\" = [\"F401\"]\n\"computeThickness.py\" = [\"E501\"]   # long lines for easier readability \n\n[tool.pytest.ini_options]\nminversion = '6.0'\nfilterwarnings = []\naddopts = [\n    \"--import-mode=importlib\",\n#    \"--junit-xml=junit-results.xml\",\n#    \"--durations=20\",\n#    \"--verbose\",\n]\n\nnibabel>=5.2\nnumpy>=1.22.3\npandas>=1.3.0\nplotly>=5.6.0\npyacvd>=0.2.9\npyvista>=0.37.0\nscipy>=1.8.0, !=1.13.0\nnilearn>=0.8.1\nlapy>=1.0.0\nkaleido>=0.2.1\nscikit-image\nimportlib_resources\npykdtree!=1.3.13\n\n# https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html\n# If compatibility with legacy builds or versions of tools that don’t support\n# certain packaging standards (e.g. PEP 517 or PEP 660), a simple setup.py\n# script can be added to your project [1] (while keeping the configuration in\n# pyproject.toml):\n\nfrom setuptools import setup\n\nsetup()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/holowizard",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/hybridmt",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/icgem",
            "repo_link": "https://git.gfz-potsdam.de/icgem",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/ideal-equilibrium-oxygen-membrane-reactor",
            "repo_link": "https://github.com/KabitGit/Ideal-Equilibrium-Oxygen-Membrane-Reactor",
            "content": {
                "codemeta": "",
                "readme": "Prequisites\n--\n- Numpy\n- SciPy\n- Cantera (tested with version 2.6.0)\n- Matplotlib (used for plotting in the examples)\n\n\nDescription\n--\nThe Python script OMR_model.py allows to simulate oxygen membrane reactors with continuous gas flow rates as described in [1]. Model input values are the initial condition of the feed and sweep gas entering the chambers separated by the membrane, while the output values refer to the chemical equilibrium state in both chambers, which is affected by the oxygen flux through the membrane.  \n\nChemical equilibrium and perfect mixing is assumed in the entire sweep and feed chamber on both sides of the membrane. The oxygen flux through the membrane is modelled using the Wagner equation and included into the chemical equilibrium calculation.\nThe entire problem is then solved as a nested problem: The inner problem is the equilibrium calculation including an assumed oxygen flux using Cantera [2]. \nThe outer problem is a root-finding problem to find the oxygen flux satisfying the Wagner equation in the equilibrium state which is solved using SciPy [3].\nA detailed explanation of the assumptions, limitations and equations including experimental validation can be found in [1].\n\nThe implementation published here uses thermodynamic data from the Gri 3.0 mechanism [4] and subsequently considers 53 species. \nNotable species herein include: [O2, H2O, H2, CH4, CO, CO2, N2, AR]\n\n\n\nUsage\n--------------------------------------------------------------------------------------------------------------------------------------\nTo import the script into your Python code, download OMR_model.py and import the function Simulate_OMR at the beginning of the code as:\n\n\n`from OMR_model import Simulate_OMR`\n\nAfter defining the input parameters for the model [T,N_f0,x_f0,P_f,N_s0,x_s0,P_s,A_mem,sigma,L,Lc], a simulation can then be performed by calling:\n\n`N_f, x_f, p_o2_f, N_s, x_s, p_o2_s, N_o2, dH, x_comp, conv = Simulate_OMR(T,N_f0,x_f0,P_f,N_s0,x_s0,P_s,A_mem,sigma,L,Lc)`\n\nExamples for using the function with scalar input parameters, as well as array shaped input parameters are given in the Examples folder.\n\n\n\nInputs and outputs of the model\n--\n\n    Parameters (Inputs)\n    ----------\n    T : Float (scalar or array with length n)\n        Temperature in °C.\n    N_f0 : Float (scalar or array with length n)\n        Initial feed gas molar flow rate in mol/min.\n    x_f0 : String or list of strings with length n\n        Initial feed gas mole fractions specified as a string in the format 'A:x_A_f0, B:x_B_f0, C:x_C_f0, ...', where x_A_f0, x_B_f0, x_C_f0 are the mole fractions of the respective species.\n    P_f : Float (scalar or array with length n)\n        Feed gas pressure in Pa.\n    N_s0 : Float (scalar or array with length n)\n        Initial sweep gas molar flow rate in mol/min.\n    x_s0 : String or list of strings with length n\n        Initial sweep gas mole fractions specified as a string in the format 'A:x_A_s0, B:x_B_s0, C:x_C_s0, ...', where x_A_s0, x_B_s0, x_C_s0 are the mole fractions of the respective species.\n    P_s : Float (scalar or array with length n)\n        Sweep gas pressure in Pa.\n    A_mem : Float (scalar or array with length n)\n        Active membrane area in cm^2.\n    sigma : Float (scalar or array with length n)\n        Ambipolar conductivity in S/m.\n    L : Float (scalar or array with length n)\n        Membrane thickness in mum.\n    Lc : Float (scalar or array with length n)\n        Characteristic length in mum.\n\n    Returns (Outputs)\n    -------\n    N_f : Float (array with length n)\n        Feed gas molar flow rate in mol/min.\n    x_f : List of array of floats with size n*53\n        Mole fraction array, consisting of the mole fractions of the feed gas, related to the composition array.\n    p_o2_f : Float (array with length n)\n        Feed gas oxygen partial pressure in Pa.\n    N_s : Float (array with length n)\n        Sweep gas molar flow rate in mol/min.\n    x_s : List of array of floats with size n*53\n        Mole fraction array, consisting of the mole fractions of the sweep gas, related to the composition array.\n    p_o2_s : Float (array with length n)\n        Sweep gas oxygen partial pressure in Pa.\n    p_o2_f : Float (array with length n)\n        Feed gas oxygen partial pressure in Pa.\n    N_o2 : Float (array with length n)\n        Molar oxygen flux through the membrane in mol/min.\n    dH : List of array of floats with size n*53\n        Outlet-Inlet enthalpy flow difference (reaction heat) in W; If positive: The reaction is endothermic; If negative: The reaction is exothermic.\n    x_comp : List of strings with size 53\n        Composition array consisting of the considered species in the calculation.\n    conv : Integer (array with length n)\n        Check whether convergence was achieved for the respective array index; Equal to 1 if converged.\n\n\n\nReferences\n--\n[1] Bittner, K., Margaritis, N., Schulze-Küppers, F., Wolters, J., & Natour, G. (2023). \n    A mathematical model for initial design iterations and feasibility studies of oxygen membrane reactors by minimizing Gibbs free energy. \n    Journal of Membrane Science, 685, 121955.\n    DOI: https://doi.org/10.1016/j.memsci.2023.121955\n\n[2] David G. Goodwin, Harry K. Moffat, Ingmar Schoegl, Raymond L. Speth, and Bryan W. Weber. Cantera: An object-oriented software toolkit for chemical kinetics, \n    thermodynamics, and transport processes. \n    https://www.cantera.org, 2023. Version 3.0.0. DOI: 10.5281/zenodo.8137090\n\n[3] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson,\n    Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, \n    Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, CJ Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, \n    Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E.A. Quintero, Charles R Harris, Anne M. Archibald, Antônio H. Ribeiro,\n    Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. \n    (2020) SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17(3), 261-272.\n    DOI: https://doi.org/10.1038/s41592-019-0686-2\n\n[4] G.P. Smith, D.M. Golden, M. Frenklach, N.W. Moriarty, B. Eiteneer, M. Goldenberg, C.T. Bowman, R.K. Hanson, S. Song, \n    W.C.J. Gardiner, V.V. Lissianski, Z. Qin, Gri-Mech 3.0\n\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/igmas",
            "repo_link": "https://git.gfz-potsdam.de/igmas/igmas-releases",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/interactivecomplexheatmap",
            "repo_link": "https://github.com/jokergoo/InteractiveComplexHeatmap",
            "content": {
                "codemeta": "",
                "readme": "# Make Interactive Complex Heatmaps\n\n[![R-CMD-check](https://github.com/jokergoo/InteractiveComplexHeatmap/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/InteractiveComplexHeatmap/actions)\n[![bioc](http://www.bioconductor.org/shields/downloads/devel/InteractiveComplexHeatmap.svg)](https://bioconductor.org/packages/stats/bioc/InteractiveComplexHeatmap/) \n[![bioc](http://www.bioconductor.org/shields/years-in-bioc/InteractiveComplexHeatmap.svg)](http://bioconductor.org/packages/devel/bioc/html/InteractiveComplexHeatmap.html)\n\n<img width=\"1150\" alt=\"Screenshot 2021-07-19 at 21 31 14\" src=\"https://user-images.githubusercontent.com/449218/126217251-8eee8ce8-7e7f-4251-b844-800dd72481bd.png\">\n\n\n**InteractiveComplexHeatmap** is an R package that converts static heatmaps produced from\n[**ComplexHeatmap**](https://github.com/jokergoo/ComplexHeatmap) package into an interactive\nShiny app only with one extra line of code.\n\nThe first example is the default layout of the interactive complex heatmap widget.\n\n<img src=\"https://user-images.githubusercontent.com/449218/110212910-e6147e00-7e9d-11eb-94ed-0ac549247888.gif\"  width='100%' border=\"black\" />\n\nThe second example demonstrates a DESeq2 result with integrating the package **shinydashboard**.\n\n<img src=\"https://user-images.githubusercontent.com/449218/111832925-b16ae280-88f1-11eb-8530-290374f9f2c2.gif\" width=\"100%\" border=\"black\" />\n\n\n### Citation\n\nZuguang Gu, et al., Make Interactive Complex Heatmaps in R, 2021, Bioinformatics, https://doi.org/10.1093/bioinformatics/btab806\n\n## Install\n\n**InteractiveComplexHeatmap** is available on\n[Bioconductor](http://bioconductor.org/packages/devel/bioc/html/InteractiveComplexHeatmap.html),\nyou can install it by:\n\n```r\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"InteractiveComplexHeatmap\")\n```\n\nIf you want the latest version, install it directly from GitHub:\n\n```r\nlibrary(devtools)\ninstall_github(\"jokergoo/InteractiveComplexHeatmap\")\n```\n\n## Documentation\n\nThere are the following vignettes along with the package:\n\n1. [How to visualize heatmaps interactively](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/InteractiveComplexHeatmap.html)\n2. [How interactive complex heatmap is implemented](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/implementation.html)\n3. [Functions for Shiny app development](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/shiny_dev.html)\n4. [Decorations on heatmaps](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/decoration.html)\n5. [Interactivate heatmaps indirectly generated by pheatmap(), heatmap.2() and heatmap()](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/interactivate_indirect.html)\n6. [A Shiny app for visualizing DESeq2 results](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/deseq2_app.html)\n7. [Implement interactive heatmap from scratch](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/from_scratch.html)\n8. [Share interactive heatmaps to collaborators](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/share.html)\n\nA printer-friendly version of the documentation is available at [bioRxiv](https://doi.org/10.1101/2021.03.08.434289).\n\n\n## Usage\n\n### Directly turn heatmaps interactive\n\nWith any `Heatmap`/`HeatmapList` object, directly send to `htShiny()` to create a Shiny app for your heatmap(s):\n\n```r\nhtShiny(ht_list)\n```\n\nIf the heatmaps are already drawn, `ht_list` can be omitted and the last heatmap object is retrieved automatically:\n\n```r\nHeatmap(...) + other_heatmaps_or_annotations # or other functions that internally use Heatmap()\nhtShiny()\n```\n\n### Shiny app development\n\nThere are also two functions for Shiny app development:\n\n- `InteractiveComplexHeatmapOutput()`: for the UI on the client side.\n- `makeInteractiveComplexHeatmap()`: for processing on the sever side.\n\n```r\nlibrary(InteractiveComplexHeatmap)\nlibrary(ComplexHeatmap)\n\nht = Heatmap(m)\nht = draw(ht)\n\nui = fluidPage(\n    InteractiveComplexHeatmapOutput()\n)\n\nserver = function(input, output, session) {\n    makeInteractiveComplexHeatmap(input, output, session, ht)\n}\n\nshiny::shinyApp(ui, server)\n```\n\nYou can also put multiple interactive heatmaps widgets in the same Shiny app:\n\n```r\nht1 = Heatmap(m, col = c(\"white\", \"blue\"))\nht1 = draw(ht1)\nht2 = Heatmap(m, col = c(\"white\", \"red\"))\nht2 = draw(ht2)\n\nui = fluidPage(\n    h3(\"The first heatmap\"),\n    InteractiveComplexHeatmapOutput(\"ht1\"),\n    hr(),\n    h3(\"The second heatmap\"),\n    InteractiveComplexHeatmapOutput(\"ht2\")\n)\n\nserver = function(input, output, session) {\n    makeInteractiveComplexHeatmap(input, output, session, ht1, \"ht1\")\n    makeInteractiveComplexHeatmap(input, output, session, ht2, \"ht2\")\n}\n\nshiny::shinyApp(ui, server)\n```\n\nTwo additional functions to let you dynamically load interactive heatmap widgets:\n\n- `InteractiveComplexHeatmapModal()`: The interactive heatmap widget is inserted as a \"modal\".\n- `InteractiveComplexHeatmapWidget()`: The interactive heatmap widget is inserted into a place defined by users.\n\n```r\nm = matrix(rnorm(100), 10)\nht = Heatmap(m)\n    \nui = fluidPage(\n    actionButton(\"show_heatmap\", \"Generate_heatmap\"),\n)\n\nserver = function(input, output, session) {\n    observeEvent(input$show_heatmap, {\n        InteractiveComplexHeatmapModal(input, output, session, ht)\n    })\n}\nshiny::shinyApp(ui, server)\n\n# or use InteractiveComplexHeatmapWidget()\nui = fluidPage(\n    actionButton(\"show_heatmap\", \"Generate_heatmap\"),\n    htmlOutput(\"heatmap_output\")\n)\n\nserver = function(input, output, session) {\n    observeEvent(input$show_heatmap, {\n        InteractiveComplexHeatmapWidget(input, output, session, ht,\n            output_id = \"heatmap_output\")\n    })\n}\nshiny::shinyApp(ui, server)\n```\n\n## Interactivate pheatmap(), heatmap.2() and heatmap()\n\nIf you directly use these three funtions, simply replace them with\n`ComplexHeatmap::pheatmap()`, `ComplexHeatmap:::heatmap.2()` and\n`ComplexHeatmap:::heatmap()`. If the three functions are used indirectly, e.g.\na function `foo()` (maybe from another packages or other people's functions)\nwhich internally uses these three heatmap functions, check the vignette\n[\"Interactivate indirect use of pheatmap(), heatmap.2() and heatmap()\"](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/interactivate_indirect.html) to find out how.\n\n## Live examples\n\nFollowing lists several live examples of interactive heatmaps. Details\ncan be found in the package vignette.\n\n- https://jokergoo.shinyapps.io/interactive_complexheatmap/\n- https://jokergoo.shinyapps.io/interactive_complexheatmap_vertical/\n- https://jokergoo.shinyapps.io/interactive_densityheatmap/\n- https://jokergoo.shinyapps.io/interactive_oncoprint/\n- https://jokergoo.shinyapps.io/interactive_enrichedheatmap/\n- https://jokergooo.shinyapps.io/interactive_upset/\n- https://jokergooo.shinyapps.io/interactive_pheatmap/\n- https://jokergooo.shinyapps.io/interactive_heatmap/\n- https://jokergooo.shinyapps.io/interactive_heatmap_2/\n- https://jokergooo.shinyapps.io/interactive_tidyheatmap/\n\nThere are also many other examples provided in the package.\n\n```r\nhtShinyExample()\n```\n\n```\nThere are following examples. Individual example can be run by e.g. htShinyExample(1.1).\n\n──────── 1. Simple examples ─────────────────────────────────────────────────────────\n 1.1 A single heatmap with minimal arguments.\n 1.2 A single heatmap from a character matrix.\n 1.3 A single heatmap with annotations on both rows and columns.\n 1.4 A single heatmap where rows and columns are split.\n 1.5 A list of two heatmaps.\n 1.6 A list of two vertically concatenated heatmaps\n 1.7 Use last generated heatmap, an example from cola package.\n 1.8 Use last generated heatmap, an app with three interactive heatmaps\n 1.9 Demonstrate hover, click and dblclick actions to select cells.\n 1.10 Only response to one of click/hover/dblclick/hover events. Please use\n      htShinyExample('1.10') to get this example (quote the index, or else\n      htShinyExample(1.10) will be treated as the same as htShinyExample(1.1)).\n 1.11 Interactive heatmap under compact mode.\n\n──────── 2. On other plots and packages ─────────────────────────────────────────────\n 2.1 A density heatmap.\n 2.2 An oncoPrint.\n 2.3 A UpSet plot.\n 2.4 An interactive heatmap from pheatmap().\n 2.5 An interactive heatmap from heatmap().\n 2.6 An interactive heatmap from heatmap.2().\n 2.7 A heatmap produced from tidyHeatmap package.\n 2.8 Genome-scale heatmap.\n 2.9 A package-dependency heatmap. You can try to control \"Fill figure region\"\n     and \"Remove empty rows and columns\" in the tools under the sub-heatmap.\n\n──────── 3. Enriched heatmaps ───────────────────────────────────────────────────────\n 3.1 A single enriched heatmap.\n 3.2 A list of enriched heatmaps.\n 3.3 An enriched heatmap with discrete signals.\n\n──────── 4. On public datasets ──────────────────────────────────────────────────────\n 4.1 An example from Lewis et al 2019.\n 4.2 Visualize cell heterogeneity from single cell RNASeq.\n 4.3 Correlations between methylation, expression and other genomic features.\n\n──────── 5. Shiny app development ───────────────────────────────────────────────────\n 5.1 A single Shiny app with two interactive heatmap widgets.\n 5.2 Self-define the output. The selected sub-matrix is shown as a text table.\n 5.3 Self-define the output. Additional annotations for the selected genes are\n     shown.\n 5.4 Visualize Gene Ontology similarities. A list of selected GO IDs as well as\n     their descriptions are shown in the output.\n 5.5 Interactive correlation heatmap. Clicking on the cell generates a\n     scatterplot of the two corresponding variables.\n 5.6 A heatmap on Jaccard coefficients for a list of genomic regions. Clicking\n     on the cell generates a Hilbert curve of how the two sets of genomic\n     regions overlap.\n 5.7 Implement interactivity from scratch. Instead of generating the whole\n     interactive heatmap widget, it only returns the information of rows and\n     columns that user have selected on heatmap and users can use this\n     information to build their own interactive heatmap widgets.\n 5.8 Implement interactivity from scratch. A visualization of 2D density\n     distribution. Brushing on heatmap triggers a new 2D density estimation\n     only on the subset of data.\n\n──────── 6. Dynamically generate heatmap widget in Shiny app ────────────────────────\n 6.1 The matrix with different dimensions is dynamically generated.\n 6.2 Reorder by a column that is specified by user.\n 6.3 Dynamically generate the widget with InteractiveComplexHeatmapModal(). The\n     modal is triggered by an action button.\n 6.4 Dynamically select interactive heatmaps. The modal is triggered by radio\n     buttons.\n 6.5 Dynamically generate the widget. A customized Javascript code is inserted\n     after the UI to change the default behavior of the action button.\n 6.6 The widget is generated by InteractiveComplexHeatmapWidget() where the UI\n     is directly put in the place defined by htmlOutput().\n 6.7 The widget is generated by InteractiveComplexHeatmapWidget() and a\n     customized Javascript code is inserted after the UI.\n\n──────── 7. Interactive R markdown document ─────────────────────────────────────────\n 7.1 Integrate in an interactive R Markdown document.\n 7.2 Integrate in an interactive R Markdown document where the heatmap widgets\n     are dynamically generated.\n\n──────── 8. Interactivate heatmaps indirectly generated by heatmap()/heatmap.2()/pheatmap()\n 8.1 Indirect use of pheatmap().\n 8.2 Indirect use of heatmap.2().\n 8.3 Two interactive heatmap widgets from indirect use of pheatmap().\n\n──────── 9. Float output UI along with mouse positions ──────────────────────────────\n 9.1 A simple example that demonstrates output UI floating with the three\n     actions: hover, click and dblclick.\n 9.2 Floating self-defined outputs.\n 9.3 Floating output only from one event on heatmap, i.e.\n     hover/click/dblclick/brush-output.\n\n──────── 10. Work with shinydashboard ────────────────────────────────────────────────\n 10.1 Separate the three UI components into three boxes.\n 10.2 The three UI components are draggable.\n 10.3 A Shiny dashboard with two tabs.\n 10.4 Only contain the original heatmap where output floats.\n 10.5 A complex dashboard that visualizes a DESeq2 results.\n```\n\n## License\n\nMIT @ Zuguang Gu\n\n\n",
                "dependencies": "Package: InteractiveComplexHeatmap\nType: Package\nTitle: Make Interactive Complex Heatmaps\nVersion: 1.11.1\nDate: 2024-02-27\nAuthors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"),\n                  comment = c('ORCID'=\"0000-0002-7395-8709\"))\nDepends: R (>= 4.0.0), ComplexHeatmap (>= 2.11.0)\nImports: grDevices, stats, shiny, grid, GetoptLong, \n    S4Vectors (>= 0.26.1), digest, IRanges, kableExtra (>= 1.3.1), utils, svglite,\n    htmltools, clisymbols, jsonlite, RColorBrewer, fontawesome\nSuggests: knitr, rmarkdown, testthat, EnrichedHeatmap, GenomicRanges, data.table,\n    circlize, GenomicFeatures, tidyverse, tidyHeatmap, cluster,\n    org.Hs.eg.db, simplifyEnrichment, GO.db, SC3, GOexpress, SingleCellExperiment,\n    scater, gplots, pheatmap, airway, DESeq2, DT, cola, BiocManager, gridtext,\n    HilbertCurve (>= 1.21.1), shinydashboard, SummarizedExperiment, pkgndep, ks\nVignetteBuilder: knitr\nDescription: This package can easily make heatmaps which are produced \n    by the ComplexHeatmap package into interactive applications. It provides two types of interactivities: \n    1. on the interactive graphics device, and 2. on a Shiny app. It also provides \n    functions for integrating the interactive heatmap widgets for more complex Shiny app development.\nbiocViews: Software, Visualization, Sequencing\nURL: https://github.com/jokergoo/InteractiveComplexHeatmap\nBugReports: https://github.com/jokergoo/InteractiveComplexHeatmap/issues\nLicense: MIT + file LICENSE\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/interactivevis",
            "repo_link": "https://github.com/martindyrba/DeepLearningInteractiveVis",
            "content": {
                "codemeta": "",
                "readme": "# Deep Learning Interactive Visualization\n\nThis project contains all code to learn a convolutional neural network model to detect Alzheimer's disease and visualize contributing brain regions with high relevance.\n \n**Further details on the procedures including samples, image processing, neural network modeling, evaluation, and validation were published in:**\n\nDyrba et al. (2021) Improving 3D convolutional neural network comprehensibility via interactive visualization of relevance maps: evaluation in Alzheimer’s disease. *Alzheimer's research & therapy* 13. DOI: [10.1186/s13195-021-00924-2](https://doi.org/10.1186/s13195-021-00924-2).\n\n\n![Screenshot of the InteractiveVis app](InteractiveVis.png)*Screenshot of the InteractiveVis app*\n\n\n***\n\n\n\n### Running the interactive visualization\n\nThe interactive Bokeh web application [InteractiveVis](InteractiveVis) can be used for deriving and inspecting the relevance maps overlaid on the original input images.\n\nTo run it, there are three options.\n\n1. **We set up a public web service to quickly try it out:** <https://explaination.net/demo>\n\n2. Alternatively, download the docker container from DockerHub: `sudo docker pull martindyrba/interactivevis`\nThen use the scripts `sudo ./run_docker_intvis.sh` and `sudo ./stop_docker_intvis.sh` to run or stop the Bokeh app. (You find both files above in this repository.)\nAfter starting the docker container, the app will be available from your web browser: <http://localhost:5006/InteractiveVis>\n\n3. Download this Git repository. Install the required Python modules (see below). Then point the Anaconda prompt or terminal console to the DeepLearningInteractiveVis main directory and run the Bokeh app using:\n`bokeh serve InteractiveVis --show`\n\n\n\n### Requirements and installation:\n\nTo be able to run the interactive visualization from the Git sources, you will need Python <3.8, in order to install tensorflow==1.15.\nAlso, we recommend to first create a new Python environment (using [Anaconda](https://www.anaconda.com/download) or [virtualenv/venv](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)) to avoid messing up your local Python modules/versions when you have other coding projects or a system shared by multiple users.\n```console\n# for Anaconda:\nconda create -n InteractiveVis python=3.7\nconda activate InteractiveVis\n```\n\nRun pip to install the dependencies:\n```console\npip install -r requirements.txt\n```\n\nThen you can start the Bokeh application:\n```console\nbokeh serve InteractiveVis --show\n```\n\n\n\n***\n\n\n\n### CNN model training and performance evaluation\n\nThe code for training the CNN models and evaluation is provided in this repository in the subdirectory [scripts](scripts).\nThe order of script execution was as follows:\n\n- [1_CreateResiduals_ADNI2_StoreModels.ipynb](scripts/1_CreateResiduals_ADNI2_StoreModels.ipynb) and other scripts for the validation samples [4_CreateResiduals_DELCODE_applying_ADNI2_regr_model.ipynb](scripts/4_CreateResiduals_DELCODE_applying_ADNI2_regr_model.ipynb) (execution time: each 15-30 minutes).\n- [2_Train_3D_CNN_ADNI2_xVal_wb_mwp1_CAT12_MNI_shuffle_checkpoint.ipynb](scripts/2_Train_3D_CNN_ADNI2_xVal_wb_mwp1_CAT12_MNI_shuffle_checkpoint.ipynb) for model training based on tenfold cross-validation to evaluate general model accuracy for the residualized data (execution time: 2-10 hrs with CUDA-GPU) and [3_Train_3D_CNN_ADNI2_whole_dataset_wb_mwp1_CAT12_MNI_shuffle.ipynb](scripts/3_Train_3D_CNN_ADNI2_whole_dataset_wb_mwp1_CAT12_MNI_shuffle.ipynb) for training the model based on the whole ADNI-GO/2 dataset.\n- [5_Validate_3D_CNN_xVal_wb_mwp1_CAT12_MNI_DELCODE.ipynb](scripts/5_Validate_3D_CNN_xVal_wb_mwp1_CAT12_MNI_DELCODE.ipynb) and [6_Validate_3D_CNN_whole_ds_wb_mwp1_CAT12_MNI_DELCODE.ipynb](scripts/6_Validate_3D_CNN_whole_ds_wb_mwp1_CAT12_MNI_DELCODE.ipynb) for the evaluation of the models using the validation data sets (execution time: each 15-30 minutes with CUDA-GPU).\n- [7_Train_3D_CNN_ADNI2_xVal_wb_rawdat_mwp1_CAT12_MNI_shuffle_checkpoint.ipynb](scripts/7_Train_3D_CNN_ADNI2_xVal_wb_rawdat_mwp1_CAT12_MNI_shuffle_checkpoint.ipynb) and [8_Train_3D_CNN_ADNI2_whole_dataset_wb_rawdat_mwp1_CAT12_MNI_shuffle.ipynb](scripts/8_Train_3D_CNN_ADNI2_whole_dataset_wb_rawdat_mwp1_CAT12_MNI_shuffle.ipynb) for training the models for the raw datasets (execution time: each 2-10 hrs with CUDA-GPU).\n- [9_Validate_3D_CNN_whole_ds_wb_rawdat_mwp1_CAT12_MNI_DELCODE.ipynb](scripts/9_Validate_3D_CNN_whole_ds_wb_rawdat_mwp1_CAT12_MNI_DELCODE.ipynb) and [9_Validate_3D_CNN_xVal_wb_mwp1_CAT12_MNI_DELCODE.ipynb](scripts/9_Validate_3D_CNN_xVal_wb_mwp1_CAT12_MNI_DELCODE.ipynb) for the evaluation of the models using the validation data sets (execution time: each 15-30 minutes with CUDA-GPU).\n- [x_extract_hippocampus_relevance_lrpCMP_DELCODE.ipynb](scripts/x_extract_hippocampus_relevance_lrpCMP_DELCODE.ipynb) to extract the hippocampus relevance for all models (execution time: 15-30 minutes with CUDA-GPU).\n- [x_extract_relevance_maps_as_nifti_DELCODE.ipynb](scripts/x_extract_relevance_maps_as_nifti_DELCODE.ipynb) to extract the relevance maps directly as nifti file for all participants/scans (execution time: 30 minutes with CUDA-GPU).\n- [hippocampus_volume_relevance_analysis_DELCODE.html](scripts/hippocampus_volume_relevance_analysis_DELCODE.html) for the baseline group separation analysis of hippocampus volume and the correlation analysis of hippocampus volume and relevance (see also other R/Rmd scripts).\n- [y_occlusion_analysis.ipynb](scripts/y_occlusion_analysis.ipynb) code for the occlusion sensitivity analysis (execution time: 90 minutes with CUDA-GPU).\n- [z_CreateResiduals_demo_dataset_applying_ADNI2_regr_model.ipynb](scripts/z_CreateResiduals_demo_dataset_applying_ADNI2_regr_model.ipynb) to create the example files being used by the InteractiveVis demo. It contains a sample of 15 people per diagnostic group, representatively selected from the ADNI-2 phase based on the criteria: amyloid status (positive for Alzheimer's dementia and amnestic mild cogntive impairment, negative for controls), MRI field strength of 3 Tesla, RID greater than 4000, and age of 65 or older. \n\n\n***\n\n\n\n### InteractiveVis architecture overview\n\n*InteractiveVis UML class diagram (v4)*\n\n![InteractiveVis class diagram (v4)](InteractiveVis_class_diagram_v4.svg)\n\n*Select subject UML sequence diagram (v3)*\n\n![Select subject sequence diagram (v3)](select_subject_sequence_diagram_v3.svg)\n\n\n\n***\n\n\n\n### License:\n\nCopyright (c) 2020 Martin Dyrba martin.dyrba@dzne.de, German Center for Neurodegenerative Diseases (DZNE), Rostock, Germany\n\nThis project and included source code is published under the MIT license. See [LICENSE](LICENSE) for details.\n\n",
                "dependencies": "protobuf==3.20\ninnvestigate==1.0.9\nkeras==2.2.4\ntensorflow~=1.15\njinja2==3.0\nbokeh<=2.2.3\nnumpy==1.18.5\nopenpyxl==3.0.5\nh5py==2.10.0\npandas==1.3.5\nmatplotlib==3.3.3\nnibabel==3.2.1\nscikit-learn==0.23.2\nscikit-image==0.17.2\nscipy==1.6.0\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ioproc",
            "repo_link": "https://gitlab.com/dlr-ve/esy/ioproc",
            "content": {
                "codemeta": "",
                "readme": "[![PyPI version](https://badge.fury.io/py/ioproc.svg)](https://badge.fury.io/py/ioproc)\n[![PyPI license](https://img.shields.io/pypi/l/ioproc.svg)](https://badge.fury.io/py/ioproc)\n[![pipeline status](https://gitlab.dlr.de/ioproc/ioproc/badges/development/pipeline.svg)](https://gitlab.dlr.de/ioproc/ioproc/-/commits/development)\n[![coverage report](https://gitlab.dlr.de/ioproc/ioproc/badges/development/coverage.svg)](https://gitlab.dlr.de/ioproc/ioproc/-/commits/development) \n\n# The ioProc workflow manager\n`ioproc` is a light-weight workflow manager for Python ensuring robust, scalable and reproducible data pipelines. The tool is developed at the German Aerospace Center (DLR) for and in the scientific context of energy systems analysis, however, it is widely applicable in other scientific fields.\n\n## how-to install\nSetup a new Python environment and install ioProc using \n\n    pip install ioproc   \n\n## how-to configure\n\nConfigure your pipeline in the `user.yaml`. The `workflow` is defined by a list of actions. These must\ncontain the fields `project`, `call` and `data` (with sub fields `read_from_dmgr`, and `write_to_dmgr`). The user\nmay specify additional fields for each action under the optional key `args`.  \nYou may get inspiration from the default actions in `general.py`.\n\nYou may also have a look into the [snippets](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets) section where several basic `ioproc` functionalities are described:\n- [Set up your first workflow](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327213)\n- [Define your first action](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327210)\n- [Make use of checkpoints](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327214)\n- [Define an action making use of the ioproc datamanger](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327212)\n- [Add additional yaml files to your workflow](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327209)\n- [Define global parameters](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327207)\n- [Starting ioproc workflow via command line with additional input parameters](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327208) \n\n## default actions provided by ioProc\n\n### `readExcel`\nThis function is used to parse Excel files and storing it in the Data manager.\n\n```python\n@action('general')\ndef parse_excel(dmgr, config, params):\n    '''\n    Parses given `excelFile` for specified `excelSheets` as dataframe object and stores it in the datamanager by the \n    key specified in `write_to_dmgr`.\n    `excelHeader` can be set to `True` or `False`.\n    \n    The action may be specified in the user.yaml as follows:\n    - action:\n        project: general\n        call: parse_excel\n        data:\n            read_from_dmgr: null\n            write_to_dmgr: parsedData\n        args:  \n            excelFile: spreadsheet.xlsx\n            excelSheet: sheet1\n            excelHeader: True\n    '''\n\n    args = params['args']\n    file = get_field(args, 'excelFile')\n    excel_sheet = get_excel_sheet(args)\n    header = get_header(get_field(args, 'excelHeader'))\n    parsed_excel = pd.read_excel(io=file, sheet_name=excel_sheet, header=header)\n\n    with dmgr.overwrite:\n        dmgr[params['data']['write_to_dmgr']] = parsed_excel\n```\n\n### `checkpoint`\nCheckpoints save the current state and content of the data manger to disk in HDF5 format. The workflow can be resumed at any time from previously created checkpoints.\n\n```python\n@action('general')\ndef checkpoint(dmgr, config, params):\n    '''\n    creates a checkpoint file in the current working directory with name\n    Cache_TAG while TAG is supplied by the action config.\n\n    :param tag: the tag for this checkpoint, this can never be \"start\"\n    '''\n    assert params['tag'] != 'start', 'checkpoints can not be named start'\n    dmgr.toCache(params['tag'])\n    mainlogger.info('set checkpoint \"{}\"'.format(params['tag']))\n```\n\n### `printData`\nThis action prints all data stored in the data manager to the console. It can therefore be used for conveniently debugging a workflow.\n\n```python\n@action('general')\ndef printData(dmgr, config, params):\n    '''\n    simple debugging printing function. Prints all data in the data manager.\n\n    Does not have any parameters.\n    '''\n    for k, v in dmgr.items():\n        mainlogger.info(k+' = \\n'+str(v))\n```\n\n",
                "dependencies": "[tool.poetry]\nname = \"ioproc\"\nversion = \"2.2.0\"\ndescription =   \"Workflow framework for data pre- and postprocessing.\"\n\nauthors = [\"Felix Nitsch, Benjamin Fuchs, Judith Riehm, Jan Buschmann <felix.nitsch@dlr.de, benjamin.fuchs@dlr.de, judith.riehm@dlr.de, jan.buschmann@dlr.de>\"]\nlicense = \"MIT\"\nmaintainers = [\"Felix Nitsch, Benjamin Fuchs, Jan Buschmann <ioproc@dlr.de>\"]\nhomepage = 'https://gitlab.com/dlr-ve/ioproc'\nreadme = 'README.md'\nkeywords = [\"workflow management\", \"data pipeline\", \"data science\"]\n\n[tool.poetry.scripts]\nioproc = 'ioproc:run'\n\n[tool.poetry.dependencies]\npython = \">=3.8\"\nCerberus = \">=1.3.4\"\npandas = \">=1.4.2\"\nPyYAML = \">=6.0\"\ntables = \">=3.7.0\"\nfrozendict = \">=2.3.2\"\narrow = \">=1.2.2\"\nclick = \">=8.1.3\"\nJinja2 = \">=3.1.2\"\nattrs = \">=21.4.0\"\ncattrs = \">=22.1.0\"\ntoml = \">=0.10.2\"\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n\n# This file may be used to create an environment using:\n# $ conda create --name <env> --file <this file>\n# platform: win-64\narrow=0.13.1\nblas=1.0\nca-certificates=2020.1.1\ncerberus=1.3.2\ncertifi=2019.11.28\nclick=7.0\nfrozendict=1.2\nfuture=0.18.2\ngdxcc=7.28.20\ngdxpds=1.1.0\nicc_rt=2019.0.0\nintel-openmp=2020.0\nmkl=2020.0\nmkl-service=2.3.0\nmkl_fft=1.0.15\nmkl_random=1.1.0\nnumexpr=2.7.1\nnumpy=1.18.1\nnumpy-base=1.18.1\nopenssl=1.1.1d\npandas=1.0.1\npip=20.0.2\npython=3.7.6\npython-dateutil=2.8.1\npytz=2019.3\npyyaml=5.3\nsetuptools=45.2.0\nsix=1.14.0\nsqlite=3.31.1\ntables=3.6.1\nvc=14.1\nvs2015_runtime=14.16.27012\nwheel=0.34.2\nwincertstore=0.2\nyaml=0.1.7\npoetry=1.1.6\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/iqtools",
            "repo_link": "https://github.com/xaratustrah/iqtools",
            "content": {
                "codemeta": "",
                "readme": "# Welcome to `iqtools`\n[![documentation](https://img.shields.io/badge/docs-mkdocs%20material-blue.svg?style=flat)](https://xaratustrah.github.io/iqtools)[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7615693.svg)](https://doi.org/10.5281/zenodo.7615693)\n\n<div style=\"margin-left:auto;margin-right:auto;text-align:center\">\n<img src=\"https://raw.githubusercontent.com/xaratustrah/iqtools/main/docs/img/icon.png\" width=\"128\">\n</div>\n\nCollection of code for working with offline complex valued time series data ([inphase and quadrature](https://en.wikipedia.org/wiki/In-phase_and_quadrature_components) or IQ Data) with numpy written for Python3.\n\n## Installation and usage\n\n### Quick instructions\n\nThere are many ways to install `iqtools` either fully or partly. One way to do a complete install is this:\n\n#### TL;DR (for Linux and Mac):\n\nQuick but full installation if you have `mamba` installed. Tested on Linux and Mac. First clone the repo, then go to the directory and run these commands.\n\n```\nmamba create -n my_env\nmamba activate my_env\nmamba install -y root pyqt pyfftw\npip install -r requirements.txt\npip install .\n```\n\nWhere `my_env` can be any name you like.\n\n\n#### Test your installation\n\nYou can test your installation by typing:\n\n```bash\npython3 -c 'import ROOT;import PyQt5;from iqtools import*'\n```\n\nIf the command returns without any error, then you are good, i.e. you should be able to use the library, or run one of the user interfaces.\n\n### Detailed installation instructions\n\n#### Preparation\n\nIf you do not need to use `iqtools` with ROOT features, you can skip to the next section. If you like to use `iqtools` with ROOT features within PyROOT, please make sure you have a proper installation of ROOT and PyROOT in your python environment. There are several alternatives of how to install ROOT:\n\n* System wide installation on Linux (Please refer to the web site of [PyROOT](https://root.cern/manual/python/) ). This approach is not recommended\n* An easier way is to install ROOT using `conda-forge` as described [here](https://anaconda.org/conda-forge/root/) or [here](https://iscinumpy.gitlab.io/post/root-conda/).\n* Most recommended is to use `mamba`. For that just install [mamba](https://mamba.readthedocs.io/en/latest/installation.html). Before installing, it is recommended to create a new mamba env and do your work there:\n\n```\nmamba create -n my_env\nmamba activate my_env\nmamba install root pyqt\n```\n\nSame goes with the installation of `pyqt`. If you are not interested in the GUI script, you can just ignore the installation of `pyqt` in the previous step. You will not be able to use the GUI, but you can still use the CLI and of course the library itself.\n\n#### Installing packages\n\nClone the repository or download the source from [GitHUB](https://github.com/xaratustrah/iqtools). Then use `pip` for installing and uninstalling `iqtools`.\n\n    pip install -r requirements.txt\n    pip install .\n\n\n### Windows\n\nUnder windows, ROOT / PyROOT needs to be installed in a different manner. Please refer to the installation instructions on the corresponding [web page](https://root.cern/) of the ROOT project. If you do not need the ROOT functions, you can still run the library, CLI and GUI under Windows. Specifically, it is recommended to download the latest version of [WinPython](https://winpython.github.io/) which contains the `PyQt` library. Just unpacking WinPython is enough, no installation is needed. After this, you can just follow the instructions above to install the requirements and packages via `pip`.\n\nSome stand alone static versions of the `iqgui` for windows may be made available in the future in the release section for the corresponding tags.\n\n### Quick usage\n\n`iqtools` is a library that can be embedded in data analysis projects. You can use its full functionality in your own codes by importing it:\n\n```python\nfrom iqtools import *\n```\n\nand use it accordingly.\n\n`iqtools` offers user interface which do not implement the full functionality of the library, but can be useful for quick access or conversions, so it can be run as a command line program for processing data file as well. For example:\n\n    iqtools --help\n\nThe `iqgui` script is a graphical user interface (GUI) written in Qt with limited functionality, but nevertheless interesting features. You can run it by simply typing:\n\n```bash\niqgui\n```\n\nA simple window will appear, where you can accesss some quick feartures. For more information on the GUI frontend please refer to the [documentation page](https://xaratustrah.github.io/iqtools).\n\n<img src=\"https://raw.githubusercontent.com/xaratustrah/iqtools/main/docs/img/iqgui.png\" width=\"512\">\n\n## Documentation\n\nFor more information please refer to the [documentation page](https://xaratustrah.github.io/iqtools).\n\n## Citation for publications\n\nIf you are using this code in your publications, please refer to [DOI:10.5281/zenodo.7615693](https://doi.org/10.5281/zenodo.7615693) for citation, or cite as:\n\n<small>\nShahab Sanjari. (2023). <i>iqtools: Collection of code for working with offline complex valued time series data in Python.</i> Zenodo. <a href=\"https://doi.org/10.5281/zenodo.7615693\">https://doi.org/10.5281/zenodo.7615693</a>\n</small>\n\n\n## Licensing\n\nPlease see the file [LICENSE.md](./LICENSE.md) for further information about how the content is licensed.\n\n\n## Acknowledgements\n\nMany thanks to @tfoerst3r for providing help with the project structure and licensing issues and to @carlkl for helping with creating a static Windows version of the GUI.\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\nnumpy\nscipy\nmatplotlib\nbeautifulsoup4\nnibabel\nnpTDMS\npyTDMS\nuproot3\nuproot3_methods\nmkdocs\nmkdocs-material\nmkdocstrings\nmkdocstrings-python\n\nfrom setuptools import setup\n\nif __name__ == '__main__':\n    setup()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/isaac",
            "repo_link": "https://github.com/ComputationalRadiationPhysics/isaac",
            "content": {
                "codemeta": "",
                "readme": "![ISAAC](/isaac.png?raw=true \"ISAAC\")\n\nIn Situ Animation of Accelerated Computations\n=====================================================\n\n![Wakefield visualization from PIConGPU](/example_renderings/picongpu_wakefield_1.png?raw=true \"Wakefield visualization from PIConGPU\")\n\nAbout ISAAC\n-----------\n\nMany computations like physics or biologists simulations these days\nrun on accelerated hardware like CUDA GPUs or Intel Xeon Phi, which are\nitself distributed in a big compute cluster communicating over MPI. The\ngoal of ISAAC is to visualize this data without the need to download it\nto the host while using the high computation speed of the accelerator.\n\nISAAC insists of two parts: The server and the insitu library.\nFurthermore there needs to be a client, which is able to show the\ntransfered stream and meta data. A reference HTML5 client is provided,\nbut needs to be adapted to specific simulations and is not part of ISAAC\nitself (but still in the repository).\n\nSimulation code has just to add some calls and settings to the insitu\ntemplate library. After that the server will notice when a simulation\nis running and give the user some options to observe the computations\n_on the fly_. It is also possible to send meta data back to the\nsimulation, e.g. to restart it with improved settings.\n\nInstalling requirements, building and using in own application\n--------------------------------------------------------------\n\nPlease see in [INSTALL.md](./INSTALL.md) for installing, building and\nusing ISAAC.\nIf you need to install ISAAC on a server not accessible from the outside\nyou need to [tunnel the connections](./TUNNEL.md) of the clients.\nA more detailed __documentation__ about using ISAAC __can be\n[found here](http://computationalradiationphysics.github.io/isaac)__.\n\nClient\n------\n\nInside the client directory lay three files:\n* interface.htm\n* interface_vlc.htm\n* interface_presentation.htm\n\nThe very first uses direct JSON injection, but no real streaming protocol\nlike RTP. The second can use RTP, but needs the (free) vlc browser plugin\ninstalled to work. Furthermore the server most be able to directly send\nUDP packages to you, which are blocked by most firewalls. Last but not\nleast this adds some latency for the h264 encoding. However the stream itself\nwill use way less bandwidth. The last client version is for presentation\npurposes, the table with meta data values is not shown, but some are shown\ndirectly in the stream box. E.g. if you visualize\n__[PIConGPU](https://github.com/ComputationalRadiationPhysics/picongpu)__\nwith ISAAC enabled this will show you the count of particles and cells\nused.\n\nKnown issues\n------------\n\n* If streaming over twitch or another rtmp compatible service is used,\n  but the rtmp port (1935) ist blocked or a wrong url passed, the server\n  will crash because of the underlying gStreamer rtmp implementation.\n\nLicensing\n---------\n\nISAAC is licensed under the LGPLv3.\n\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jards",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/jards-user-forum/jards",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/jcuber",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/perftools/jcuber",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/jemris",
            "repo_link": "https://github.com/JEMRIS/jemris",
            "content": {
                "codemeta": "",
                "readme": "General Information\n===================\n\nJEMRIS is a general MRI simulation framework.\n\nThe general process of simulation consists of preparation by choice or\nimplementation of sequence, sample and coil setup and the invocation\nof the simulation run itself.  \n\n\nDocumentation\n=============\n\nIt is _highly_ recommended to read the provided documentation online.\nPlease find the build, install, developer and user documentation under\nhttp://www.jemris.org.\n\n\nLicensing\n=========\n\nThis program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation; either version 2 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program; if not, write to the Free Software\nFoundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA \n02110-1301  USA\n\nFor an explicit declaration of licensing refer to the COPYING file in\nthe root directory of this package.\n\n\nContact\n=======\n\nThis package is maintained by Tony Stoecker <tony.stoecker@dzne.de>.\nPlease find problem specific contact addresses on http://www.jemris.org.\n\n\nInstallation\n============\n\nPlease visit http://www.jemris.org for detailed installation instructions.\n\n\nHow to report bugs\n==================\n\nIf you have identified a bug in JEMRIS you are welcome to send a detailed\nbug report to <tony.stoecker@dzne.de>. Please include`\n\n* Information about your system\n\n   - Which operating system and version (uname -a)\n   - Which C compiler and version (gcc --version)\n   \n  And anything else you think is relevant.\n\n* Information about your version of JEMRIS\n\n   - Version and release number\n   \n* How to reproduce the bug\n\n   - If it is a systematical bug in JEMRIS please provide the\n     sequence, the sample, the coils and the outputs from sequence or\n     simulation GUI to help us to reproduce the bug.\n\nPatches are most welcome.  If possible please provide a pull request on github.\n",
                "dependencies": "cmake_minimum_required (VERSION 2.8) \n\nproject (jemris) \nset (jemris_VERSION_MAJOR 2)\nset (jemris_VERSION_MINOR 9)\nset (jemris_VERSION_PATCH 1)\nset (jemris_VERSION \"${jemris_VERSION_MAJOR}.${jemris_VERSION_MINOR}.${jemris_VERSION_PATCH}\")\n\nset(CMAKE_BUILD_TYPE \"Release\")\n\nset (CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ${CMAKE_SOURCE_DIR}/cmake)\n\nmacro(set_config_option VARNAME STRING)\n  set(${VARNAME} TRUE)\n  list(APPEND CONFIG_OPTIONS ${STRING})\n  message(STATUS \"Found \" ${STRING})\nendmacro(set_config_option)\n\ninclude(GetGitRevisionDescription)\nget_git_head_revision(GIT_REFSPEC GIT_SHA1)\n\ninclude(ConfigureChecks.cmake)\nadd_definitions(-DHAVE_CONFIG_H)\n\n# OS type -----------------------------------------------------------------------\nif (${CMAKE_SYSTEM_NAME} MATCHES \"Windows\")\n  set(WINDOWS TRUE)\nelseif (${CMAKE_SYSTEM_NAME} MATCHES \"Linux\")\n  set(LINUX TRUE)\nelseif (${CMAKE_SYSTEM_NAME} MATCHES \"Darwin\")\n  set(MACOSX TRUE)\nendif()\n\n# Architecture ------------------------------------------------------------------\n# include (VcMacros) DB: not needed, right?\ninclude (OptimizeForArchitecture)\n#find_package (OpenMP)\n#if(OPENMP_FOUND)\n  #message(\"-- Found OpenMP\")\n  #set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${OpenMP_C_FLAGS}\")\n  #set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}\")\n  #set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${OpenMP_EXE_LINKER_FLAGS}\")\n#endif()\n\n# C++ flags ---------------------------------------------------------------------\n# THis should be highly discouraged as it directly sets the compiler flags - this should be handled by cmake to allow\n# proper RELEASE, DEBUG, ... builds\nset(CMAKE_CXX_COMPILER \"mpicxx\")\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-psabi -DTIXML_USE_STL -fPIC -O3\") \nelseif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DTIXML_USE_STL -O3 -Itr1 -std=c++0x -stdlib=libc++\")\nelseif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /DTIXML_USE_STL /EHsc /Ox\n  /nologo /MT /wd4267 /wd4244 /wd4190 /wd4996 /LD /MD /DEXP_STL\") \n  set (CMAKE_LINKER_FLAGS \"${CMAKE_LINKER_FLAGS} /NODEFAULTLIB:LIBCMT\")\n  set (CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} /NODEFAULTLIB:LIBCMT\")\n  set (CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} /NODEFAULTLIB:LIBCMT\")\nendif()\n\ninclude(CheckTypeSize)\ninclude(CheckFunctionExists)\ninclude(CheckIncludeFile)\n\nenable_testing()\n\ninclude (VcMacros)\ninclude (OptimizeForArchitecture)\n\ncheck_type_size(\"void*\" SIZEOF_VOID_P)\nif(SIZEOF_VOID_P EQUAL 8)\n  set_config_option(HAVE_64BIT_SIZE_T \"Have64BitSizeT\")\nendif(SIZEOF_VOID_P EQUAL 8)\n\nfind_package(Boost)\nif(Boost_FOUND)\n  include_directories(${Boost_INCLUDE_DIRS})\n  set (HAVE_BOOST ON CACHE BOOL \"Found boost\")\nendif()\n\nfind_package(Xerces 1.3.0 REQUIRED)\ninclude_directories (${Xerces_INCLUDE_DIRS})\n\nfind_package(GiNaC REQUIRED)\ninclude_directories (${GINAC_INCLUDE_DIRS})\n\n# Hmm does not support components...\nfind_package(Sundials REQUIRED)\ninclude_directories (${SUNDIALS_INCLUDE_DIR})\n# Handle Nvector stuff... here we need to deal with the actual version\nif (SUNDIALS_FOUND)\n   add_definitions(-DHAVE_CVODE_CVODE_H)\n   add_definitions(-DHAVE_NVECTOR_NVECTOR_SERIAL_H)\n   add_definitions(-DHAVE_CVODE_CVODE_DIAG_H)\nendif(SUNDIALS_FOUND)\n\nfind_package(MPI)\nif(MPI_C_FOUND)\n  include_directories (${MPI_INCLUDE_PATH})\n  set (HAVE_MPI_THREADS ON CACHE BOOL \"Found MPI with threads\")\nelseif(MPI_C_FOUND)\n    set(MPIRUN \"MPIRUN-UNAVAILABLE\")\nendif()\n\n\nfind_package(HDF5 COMPONENTS C CXX REQUIRED)\n\nif (NOT DEFINED HDF5_CXX_LIBRARIES)\n   MESSAGE(STATUS \"HDF5 CXX Libraries not set but library present... using alternative! Probably everything is fine!\")\n   LIST(APPEND HDF5_CXX_LIBRARIES ${HDF5_hdf5_cpp_LIBRARY})\n   LIST(APPEND HDF5_CXX_LIBRARIES ${HDF5_hdf5_LIBRARY})\n   LIST(APPEND HDF5_CXX_LIBRARIES ${HDF5_z_LIBRARY})\n   LIST(APPEND HDF5_CXX_LIBRARIES ${HDF5_sz_LIBRARY})\n   LIST(APPEND HDF5_CXX_LIBRARIES ${HDF5_m_LIBRARY})\n   LIST(APPEND HDF5_CXX_LIBRARIES ${HDF5_hdf5_hl_LIBRARY})\nendif(NOT DEFINED HDF5_CXX_LIBRARIES)\n\ninclude_directories (${HDF5_INCLUDE_DIR})\n\n# ISMRMRD\nfind_package(ISMRMRD REQUIRED)\nlink_directories(${ISMRMRD_LIBRARY_DIRS})\ninclude_directories(${ISMRMRD_INCLUDE_DIRS})\n\n# Docker image\nmessage( NOTICE \"pulling docker image for reconstruction server...\")\nexecute_process(COMMAND \"docker\" \"pull\" \"mavel101/bart-reco-server\" RESULT_VARIABLE ret OUTPUT_FILE CMD_OUTPUT)\nif(ret EQUAL \"1\")\n    message( WARNING \"Docker image for reconstruction server cannot be pulled. Docker may not be installed.\")\nelse()\n    message( NOTICE \"Succesfully pulled Docker image.\")\nendif()\n\n# Client conda environment\nmessage( NOTICE \"creating conda environment for reconstruction client.\")\nconfigure_file(${CMAKE_SOURCE_DIR}/ismrmrd_client.yml ${CMAKE_CURRENT_BINARY_DIR}/ismrmrd_client.yml)\nexecute_process(COMMAND \"conda\" \"list\" \"-n\" \"ismrmrd_client\" RESULT_VARIABLE ret OUTPUT_QUIET ERROR_QUIET)\nif(ret EQUAL \"1\")\n  execute_process(COMMAND \"conda\" \"env\" \"create\" \"-f\" \"ismrmrd_client.yml\" RESULT_VARIABLE ret)\n  if(ret EQUAL \"1\")\n      message( WARNING \"Conda environment for reconstruction client cannot be installed. Python may not be installed.\")\n  else()\n      message( NOTICE \"Succesfully installed conda environment.\")\n  endif()\nelse()\n      message( NOTICE \"Conda environment already installed. Environment will be updated if necessary.\")\n      execute_process(COMMAND \"conda\" \"run\" \"-n\" \"ismrmrd_client\" \"conda\" \"env\" \"update\" \"--file\" \"ismrmrd_client.yml\" RESULT_VARIABLE ret)\nendif()\n\n# Is this here really necessary?\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")\n  set(CMAKE_LINK_FLAGS \"${CMAKE_LINK_FLAGS} hdf5.lib hdf5_cpp.lib\")\n  set(ZLIB_LIBRARY \"${HDF5_DIR}/../../lib/zlib.lib\")\n  set(ZLIB_INCLUDE_DIR \"${HDF5_DIR}/../../include\")\n  set(ZLIB_FOUND \"1\")\nendif()\n\nSET(prefix \"${CMAKE_INSTALL_PREFIX}\")\n\nconfigure_file (\n        \"cmake/config.h.in\"\n  \"${PROJECT_SOURCE_DIR}/src/config.h\"\n        @ONLY\n  )\n\n\n\nadd_subdirectory (src) \nadd_subdirectory (share) \n\n# Packaging ---------------------------------------------------------------------\n\n# All\ninclude (InstallRequiredSystemLibraries)\nset (CPACK_RESOURCE_FILE_LICENSE \"${CMAKE_CURRENT_SOURCE_DIR}/COPYING\")\nset (CPACK_GENERATOR \"TBZ2\")\nset (CPACK_PACKAGE_VERSION \"${jemris_VERSION_MAJOR}.${jemris_VERSION_MINOR}.${jemris_VERSION_PATCH}\")\nset (CPACK_PACKAGE_NAME \"jemris\")\nset (CPACK_PACKAGE_CONTACT \"Tony Stoecker <tony.stoecker@dzne.de>\")\nset (CPACK_PACKAGE_DESCRIPTION_SUMMARY \"JEMRIS is a general purpose MRI simulator. Visit http://www.jemris.org for more details.\")\n\n# DEB\nset (CPACK_DEBIAN_PACKAGE_ARCHITECTURE \"amd64\")\nset (CPACK_DEBIAN_PACKAGE_DEPENDS \"ginac-tools, openmpi-bin, libsundials-cvode2, libxerces-c3.2, libhdf5-cpp-103\")\n\ninclude (CPack)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jplag",
            "repo_link": "https://github.com/jplag/JPlag/",
            "content": {
                "codemeta": "",
                "readme": "<p align=\"center\"> \n\t<img alt=\"JPlag logo\" src=\"core/src/main/resources/de/jplag/logo-dark.png\" width=\"350\">\n</p>\n\n# JPlag - Detecting Software Plagiarism\n[![CI Build](https://github.com/jplag/jplag/actions/workflows/maven.yml/badge.svg)](https://github.com/jplag/jplag/actions/workflows/maven.yml)\n[![Latest Release](https://img.shields.io/github/release/jplag/jplag.svg)](https://github.com/jplag/jplag/releases/latest)\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/de.jplag/jplag/badge.svg)](https://maven-badges.herokuapp.com/maven-central/de.jplag/jplag)\n[![License](https://img.shields.io/github/license/jplag/jplag.svg)](https://github.com/jplag/jplag/blob/main/LICENSE)\n[![GitHub commit activity](https://img.shields.io/github/commit-activity/y/jplag/JPlag)](https://github.com/jplag/JPlag/pulse)\n[![SonarCloud Coverage](https://sonarcloud.io/api/project_badges/measure?project=jplag_JPlag&metric=coverage)](https://sonarcloud.io/component_measures?metric=Coverage&view=list&id=jplag_JPlag)\n[![Report Viewer](https://img.shields.io/badge/report%20viewer-online-b80025)](https://jplag.github.io/JPlag/)\n[![Java Version](https://img.shields.io/badge/java-SE%2021-yellowgreen)](#download-and-installation)\n\n\nJPlag finds pairwise similarities among a set of multiple programs. It can reliably detect software plagiarism and collusion in software development, even when obfuscated. All similarities are calculated locally, and no source code or plagiarism results are ever uploaded to the internet. JPlag supports a large number of programming and modeling languages.\n\n* 📈 [JPlag Demo](https://jplag.github.io/Demo/)\n\n* 🏛️ [JPlag on Helmholtz RSD](https://helmholtz.software/software/jplag)\n\n* 🤩 [Give us Feedback in a **short (<5 min) survey**](https://docs.google.com/forms/d/e/1FAIpQLSckqUlXhIlJ-H2jtu2VmGf_mJt4hcnHXaDlwhpUL3XG1I8UYw/viewform?usp=sf_link)\n\n\n## Supported Languages\n\nAll supported languages and their supported versions are listed below.\n\n| Language                                               |                                                                                Version | CLI Argument Name | [state](https://github.com/jplag/JPlag/wiki/2.-Supported-Languages) |  parser   |\n|--------------------------------------------------------|---------------------------------------------------------------------------------------:|-------------------|:-------------------------------------------------------------------:|:---------:|\n| [Java](https://www.java.com)                           |                                                                                     21 | java              |                               mature                                |   JavaC   |\n| [C](https://isocpp.org)                                |                                                                                     11 | c                 |                               legacy                                |  JavaCC   |\n| [C++](https://isocpp.org)                              |                                                                                     14 | cpp               |                                beta                                 |  ANTLR 4  |\n| [C#](https://docs.microsoft.com/en-us/dotnet/csharp/)  |                                                                                      6 | csharp            |                               mature                                |  ANTLR 4  |\n| [Python](https://www.python.org)                       |                                                                                    3.6 | python3           |                                beta                                 |  ANTLR 4  |\n| [JavaScript](https://www.javascript.com/)              |                                                                                    ES6 | javascript        |                                beta                                 |  ANTLR 4  |\n| [TypeScript](https://www.typescriptlang.org/)          | [~5](https://github.com/antlr/grammars-v4/tree/master/javascript/typescript/README.md) | typescript        |                                beta                                 |  ANTLR 4  |\n| [Go](https://go.dev)                                   |                                                                                   1.17 | golang            |                                beta                                 |  ANTLR 4  |\n| [Kotlin](https://kotlinlang.org)                       |                                                                                    1.3 | kotlin            |                                beta                                 |  ANTLR 4  |\n| [R](https://www.r-project.org/)                        |                                                                                  3.5.0 | rlang             |                                beta                                 |  ANTLR 4  |\n| [Rust](https://www.rust-lang.org/)                     |                                                                                 1.60.0 | rust              |                                beta                                 |  ANTLR 4  |\n| [Swift](https://www.swift.org)                         |                                                                                    5.4 | swift             |                                beta                                 |  ANTLR 4  |\n| [Scala](https://www.scala-lang.org)                    |                                                                                 2.13.8 | scala             |                                beta                                 | Scalameta |\n| [LLVM IR](https://llvm.org)                            |                                                                                     15 | llvmir            |                                beta                                 |  ANTLR 4  |\n| [Scheme](http://www.scheme-reports.org)                |                                                                                      ? | scheme            |                               legacy                                |  JavaCC   |\n| [EMF Metamodel](https://www.eclipse.org/modeling/emf/) |                                                                                 2.25.0 | emf               |                                beta                                 |    EMF    |\n| [EMF Model](https://www.eclipse.org/modeling/emf/)     |                                                                                 2.25.0 | emf-model         |                                alpha                                |    EMF    |\n| [SCXML](https://www.w3.org/TR/scxml/)                  |                                                                                    1.0 | scxml             |                                alpha                                |    XML    |\n| Text (naive)                                           |                                                                                      - | text              |                               legacy                                |  CoreNLP  |\n\n## Download and Installation\nYou need Java SE 21 to run or build JPlag.\n\n### Downloading a release\n* Download a [released version](https://github.com/jplag/jplag/releases).\n* In case you depend on the legacy version of JPlag we refer to the [legacy release v2.12.1](https://github.com/jplag/jplag/releases/tag/v2.12.1-SNAPSHOT) and the [legacy branch](https://github.com/jplag/jplag/tree/legacy).\n\n### Via Maven\nJPlag is released on [Maven Central](https://search.maven.org/search?q=de.jplag), it can be included as follows:\n```xml\n<dependency>\n  <groupId>de.jplag</groupId>\n  <artifactId>jplag</artifactId>\n  <version><!--desired version--></version>\n</dependency>\n```\n\n### Building from sources \n1. Download or clone the code from this repository.\n2. Run `mvn clean package` from the root of the repository to compile and build all submodules.\n   Run `mvn clean package assembly:single` instead if you need the full jar which includes all dependencies.\n   Run `mvn -P with-report-viewer clean package assembly:single` to build the full jar with the report viewer. In this case, you'll need [Node.js](https://nodejs.org/en/download) installed.\n3. You will find the generated JARs in the subdirectory `cli/target`.\n\n## Usage\nJPlag can either be used via the CLI or directly via its Java API. For more information, see the [usage information in the wiki](https://github.com/jplag/JPlag/wiki/1.-How-to-Use-JPlag). If you are using the CLI, you can display your results via [jplag.github.io](https://jplag.github.io/JPlag/). No data will leave your computer!\n\n### CLI\n*Note that the [legacy CLI](https://github.com/jplag/jplag/blob/legacy/README.md) is varying slightly.*\nThe language can either be set with the -l parameter or as a subcommand (`jplag [jplag options] <language name> [language options]`). A subcommand takes priority over the -l option.\nWhen using the subcommand, language-specific arguments can be set. A list of language-specific options can be obtained by requesting the help page of a subcommand (e.g. `jplag java -h`).\n\n```\nParameter descriptions: \n      [root-dirs[,root-dirs...]...]\n                        Root-directory with submissions to check for plagiarism.\n      -bc, --bc, --base-code=<baseCode>\n                        Path to the base code directory (common framework used in all submissions).\n  -l, --language=<language>\n                        Select the language of the submissions (default: java). See subcommands below.\n  -M, --mode=<{RUN, VIEW, RUN_AND_VIEW}>\n                        The mode of JPlag: either only run analysis, only open the viewer, or do both (default: null)\n  -n, --shown-comparisons=<shownComparisons>\n                        The maximum number of comparisons that will be shown in the generated report, if set to -1 all comparisons will be shown (default: 500)\n      -new, --new=<newDirectories>[,<newDirectories>...]\n                        Root-directories with submissions to check for plagiarism (same as root).\n      --normalize       Activate the normalization of tokens. Supported for languages: Java, C++.\n      -old, --old=<oldDirectories>[,<oldDirectories>...]\n                        Root-directories with prior submissions to compare against.\n  -r, --result-file=<resultFile>\n                        Name of the file in which the comparison results will be stored (default: results). Missing .zip endings will be automatically added.\n  -t, --min-tokens=<minTokenMatch>\n                        Tunes the comparison sensitivity by adjusting the minimum token required to be counted as a matching section. A smaller value increases the sensitivity but might lead to more\n                          false-positives.\n\nAdvanced\n      --csv-export      Export pairwise similarity values as a CSV file.\n  -d, --debug           Store on-parsable files in error folder.\n  -m, --similarity-threshold=<similarityThreshold>\n                        Comparison similarity threshold [0.0-1.0]: All comparisons above this threshold will be saved (default: 0.0).\n  -p, --suffixes=<suffixes>[,<suffixes>...]\n                        comma-separated list of all filename suffixes that are included.\n  -P, --port=<port>     The port used for the internal report viewer (default: 1996).\n  -s, --subdirectory=<subdirectory>\n                        Look in directories <root-dir>/*/<dir> for programs.\n  -x, --exclusion-file=<exclusionFileName>\n                        All files named in this file will be ignored in the comparison (line-separated list).\n\nClustering\n      --cluster-alg, --cluster-algorithm=<{AGGLOMERATIVE, SPECTRAL}>\n                        Specifies the clustering algorithm (default: spectral).\n      --cluster-metric=<{AVG, MIN, MAX, INTERSECTION}>\n                        The similarity metric used for clustering (default: average similarity).\n      --cluster-skip    Skips the cluster calculation.\n\nSubsequence Match Merging\n      --gap-size=<maximumGapSize>\n                        Maximal gap between neighboring matches to be merged (between 1 and minTokenMatch, default: 6).\n      --match-merging   Enables merging of neighboring matches to counteract obfuscation attempts.\n      --neighbor-length=<minimumNeighborLength>\n                        Minimal length of neighboring matches to be merged (between 1 and minTokenMatch, default: 2).\n\nSubcommands (supported languages):\n  c\n  cpp\n  csharp\n  emf\n  emf-model\n  go\n  java\n  javascript\n  kotlin\n  llvmir\n  python3\n  rlang\n  rust\n  scala\n  scheme\n  scxml\n  swift\n  text\n  typescript\n```\n\n### Java API\n\nThe new API makes it easy to integrate JPlag's plagiarism detection into external Java projects:\n\n<!-- To assure that the code example is always correct, it must be kept in sync\nwith [`ReadmeCodeExampleTest#testReadmeCodeExample`](core/src/test/java/de/jplag/special/ReadmeCodeExampleTest.java). -->\n```java\nLanguage language = new JavaLanguage();\nSet<File> submissionDirectories = Set.of(new File(\"/path/to/rootDir\"));\nFile baseCode = new File(\"/path/to/baseCode\");\nJPlagOptions options = new JPlagOptions(language, submissionDirectories, Set.of()).withBaseCodeSubmissionDirectory(baseCode);\n\ntry {\n    JPlagResult result = JPlag.run(options);\n\n    // Optional\n    ReportObjectFactory reportObjectFactory = new ReportObjectFactory(new File(\"/path/to/output\"));\n    reportObjectFactory.createAndSaveReport(result);\n} catch (ExitException e) {\n    // error handling here\n} catch (FileNotFoundException e) {\n    // handle IO exception here\n}\n```\n\n## Contributing\nWe're happy to incorporate all improvements to JPlag into this codebase. Feel free to fork the project and send pull requests.\nPlease consider our [guidelines for contributions](https://github.com/jplag/JPlag/wiki/3.-Contributing-to-JPlag).\n\n## Contact\nIf you encounter bugs or other issues, please report them [here](https://github.com/jplag/jplag/issues).\nFor other purposes, you can contact us at jplag@ipd.kit.edu .\nIf you are doing research related to JPlag, we would love to know what you are doing. Feel free to contact us!\n\n### More information can be found in our [Wiki](https://github.com/jplag/JPlag/wiki)!\n\n",
                "dependencies": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>de.jplag</groupId>\n    <artifactId>aggregator</artifactId>\n    <version>${revision}</version>\n    <packaging>pom</packaging>\n\n    <name>JPlag Plagiarism Detector</name>\n    <description>JPlag is a system that finds similarities among multiple sets of source code files. This way it can\n        detect software plagiarism and collusion in software development.</description>\n    <url>http://jplag.de</url>\n    <organization>\n        <name>Karlsruhe Institute of Technology (KIT), KASTEL – Institute of Information Security and Dependability</name>\n        <url>https://sdq.kastel.kit.edu</url>\n    </organization>\n    <licenses>\n        <license>\n            <name>GNU General Public License v3.0</name>\n            <url>https://www.gnu.org/licenses/gpl-3.0.txt</url>\n        </license>\n    </licenses>\n\n    <developers>\n        <developer>\n            <id>tsaglam</id>\n            <name>Timur Sağlam</name>\n            <email>saglam@kit.edu</email>\n            <url>https://dsis.kastel.kit.edu/staff_saglam.php</url>\n            <organization>KASTEL</organization>\n            <organizationUrl>https://dsis.kastel.kit.edu/</organizationUrl>\n            <timezone>GMT+1</timezone>\n        </developer>\n        <developer>\n            <id>sebinside</id>\n            <name>Sebastian Hahner</name>\n            <email>hahner@kit.edu</email>\n            <url>https://dsis.kastel.kit.edu/staff_sebastian_hahner.php</url>\n            <organization>KASTEL</organization>\n            <organizationUrl>https://dsis.kastel.kit.edu/</organizationUrl>\n            <timezone>GMT+1</timezone>\n        </developer>\n    </developers>\n\n    <scm>\n        <connection>scm:git:git://github.com/jplag/JPlag.git</connection>\n        <developerConnection>scm:git:ssh://github.com:jplag/JPlag.git</developerConnection>\n        <url>https://github.com/jplag/JPlag</url>\n    </scm>\n\n    <issueManagement>\n        <system>GitHub</system>\n        <url>https://github.com/jplag/jplag</url>\n    </issueManagement>\n    <ciManagement>\n        <system>GitHub Actions</system>\n        <url>https://github.com/jplag/jplag/actions</url>\n    </ciManagement>\n    <distributionManagement>\n        <snapshotRepository>\n            <id>ossrh</id>\n            <url>https://s01.oss.sonatype.org/content/repositories/snapshots</url>\n        </snapshotRepository>\n    </distributionManagement>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <sonar.projectKey>jplag_JPlag</sonar.projectKey>\n        <sonar.moduleKey>${project.groupId}:${project.artifactId}</sonar.moduleKey>\n        <sonar.organization>jplag</sonar.organization>\n        <sonar.host.url>https://sonarcloud.io</sonar.host.url>\n        <!--suppress UnresolvedMavenProperty -->\n        <sonar.coverage.jacoco.xmlReportPaths>${maven.multiModuleProjectDirectory}/coverage-report/target/site/jacoco-aggregate/jacoco.xml</sonar.coverage.jacoco.xmlReportPaths>\n\n        <maven.compiler.source>21</maven.compiler.source>\n        <maven.compiler.target>21</maven.compiler.target>\n        <spotless.version>2.43.0</spotless.version>\n        <slf4j.version>2.0.13</slf4j.version>\n        <junit.version>5.10.2</junit.version>\n\n        <antlr2.version>2.7.7</antlr2.version>\n        <antlr4.version>4.13.1</antlr4.version>\n        <emf.version>2.36.0</emf.version>\n        <emf.ecore.version>2.30.0</emf.ecore.version>\n        <emf.ecore.xmi.version>2.37.0</emf.ecore.xmi.version>\n        <emfatic.version>1.0.0</emfatic.version>\n\n        <!-- The Revision of JPlag -->\n        <revision>5.1.0</revision>\n    </properties>\n\n    <dependencyManagement>\n        <dependencies>\n            <!-- ANTLR -->\n            <dependency>\n                <groupId>antlr</groupId>\n                <artifactId>antlr</artifactId>\n                <version>${antlr2.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.antlr</groupId>\n                <artifactId>antlr4-runtime</artifactId>\n                <version>${antlr4.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.antlr</groupId>\n                <artifactId>antlr4</artifactId>\n                <version>${antlr4.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>net.sourceforge.argparse4j</groupId>\n                <artifactId>argparse4j</artifactId>\n                <version>0.9.0</version>\n            </dependency>\n\n            <!-- CoreNLP -->\n            <dependency>\n                <groupId>edu.stanford.nlp</groupId>\n                <artifactId>stanford-corenlp</artifactId>\n                <version>4.5.7</version>\n            </dependency>\n\n            <!-- LOGGER -->\n            <dependency>\n                <groupId>org.slf4j</groupId>\n                <artifactId>slf4j-api</artifactId>\n                <version>${slf4j.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.slf4j</groupId>\n                <artifactId>slf4j-simple</artifactId>\n                <version>${slf4j.version}</version>\n            </dependency>\n            <dependency>\n                <groupId>org.kohsuke.metainf-services</groupId>\n                <artifactId>metainf-services</artifactId>\n                <version>1.11</version>\n            </dependency>\n\n            <dependency>\n                <groupId>com.fasterxml.jackson.core</groupId>\n                <artifactId>jackson-databind</artifactId>\n                <version>2.17.1</version>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.junit.jupiter</groupId>\n            <artifactId>junit-jupiter-engine</artifactId>\n            <version>${junit.version}</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.junit.jupiter</groupId>\n            <artifactId>junit-jupiter-params</artifactId>\n            <version>${junit.version}</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>com.github.stefanbirkner</groupId>\n            <artifactId>system-lambda</artifactId>\n            <version>1.2.1</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.mockito</groupId>\n            <artifactId>mockito-core</artifactId>\n            <version>5.12.0</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-api</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-simple</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <pluginManagement>\n            <plugins>\n                <plugin>\n                    <groupId>com.helger.maven</groupId>\n                    <artifactId>ph-javacc-maven-plugin</artifactId>\n                    <version>4.1.5</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.codehaus.mojo</groupId>\n                    <artifactId>antlr-maven-plugin</artifactId>\n                    <version>2.1</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.antlr</groupId>\n                    <artifactId>antlr4-maven-plugin</artifactId>\n                    <version>${antlr4.version}</version>\n                </plugin>\n\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-jar-plugin</artifactId>\n                    <version>3.4.1</version>\n                    <configuration>\n                        <archive>\n                            <manifest>\n                                <addDefaultImplementationEntries>true</addDefaultImplementationEntries>\n                                <addDefaultSpecificationEntries>true</addDefaultSpecificationEntries>\n                            </manifest>\n                        </archive>\n                    </configuration>\n                    <executions>\n                        <execution>\n                            <goals>\n                                <goal>test-jar</goal>\n                            </goals>\n                        </execution>\n                    </executions>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-assembly-plugin</artifactId>\n                    <version>3.7.1</version>\n                    <configuration>\n                        <descriptorRefs>\n                            <descriptorRef>jar-with-dependencies</descriptorRef>\n                        </descriptorRefs>\n                        <archive>\n                            <manifest>\n                                <addDefaultImplementationEntries>true</addDefaultImplementationEntries>\n                                <addDefaultSpecificationEntries>true</addDefaultSpecificationEntries>\n                            </manifest>\n                        </archive>\n                    </configuration>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-surefire-plugin</artifactId>\n                    <version>3.2.5</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.jacoco</groupId>\n                    <artifactId>jacoco-maven-plugin</artifactId>\n                    <version>0.8.12</version>\n                    <executions>\n                        <execution>\n                            <id>prepare-agent</id>\n                            <goals>\n                                <goal>prepare-agent</goal>\n                            </goals>\n                        </execution>\n                        <execution>\n                            <id>report</id>\n                            <goals>\n                                <goal>report</goal>\n                            </goals>\n                            <configuration>\n                                <formats>\n                                    <format>XML</format>\n                                </formats>\n                            </configuration>\n                        </execution>\n                    </executions>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-gpg-plugin</artifactId>\n                    <version>3.2.4</version>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-deploy-plugin</artifactId>\n                    <version>3.1.2</version>\n                </plugin>\n            </plugins>\n        </pluginManagement>\n        <plugins>\n            <plugin>\n                <groupId>org.codehaus.mojo</groupId>\n                <artifactId>flatten-maven-plugin</artifactId>\n                <version>1.6.0</version>\n                <configuration>\n                    <updatePomFile>true</updatePomFile>\n                    <flattenMode>resolveCiFriendliesOnly</flattenMode>\n                    <pomElements>\n                        <dependencyManagement>expand</dependencyManagement>\n                        <dependencies>expand</dependencies>\n                    </pomElements>\n                </configuration>\n                <executions>\n                    <execution>\n                        <id>flatten</id>\n                        <goals>\n                            <goal>flatten</goal>\n                        </goals>\n                        <phase>process-resources</phase>\n                    </execution>\n                    <execution>\n                        <id>flatten.clean</id>\n                        <goals>\n                            <goal>clean</goal>\n                        </goals>\n                        <phase>clean</phase>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.codehaus.mojo</groupId>\n                <artifactId>build-helper-maven-plugin</artifactId>\n                <version>3.6.0</version>\n                <executions>\n                    <execution>\n                        <id>add-source</id>\n                        <goals>\n                            <goal>add-source</goal>\n                        </goals>\n                        <phase>generate-sources</phase>\n                        <configuration>\n                            <sources>\n                                <source>${project.build.directory}/generated-sources/javacc/</source>\n                                <source>${project.build.directory}/generated-sources/antlr/</source>\n                            </sources>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-javadoc-plugin</artifactId>\n                <version>3.6.3</version>\n                <executions>\n                    <execution>\n                        <id>attach-javadocs</id>\n                        <goals>\n                            <goal>jar</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-source-plugin</artifactId>\n                <version>3.3.1</version>\n                <executions>\n                    <execution>\n                        <id>attach-sources</id>\n                        <goals>\n                            <goal>jar</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.sonatype.plugins</groupId>\n                <artifactId>nexus-staging-maven-plugin</artifactId>\n                <version>1.6.13</version>\n                <extensions>true</extensions>\n                <configuration>\n                    <serverId>ossrh</serverId>\n                    <nexusUrl>https://s01.oss.sonatype.org/</nexusUrl>\n                    <autoReleaseAfterClose>true</autoReleaseAfterClose>\n                </configuration>\n            </plugin>\n            <plugin>\n                <groupId>com.diffplug.spotless</groupId>\n                <artifactId>spotless-maven-plugin</artifactId>\n                <version>${spotless.version}</version>\n                <configuration>\n                    <java>\n                        <eclipse>\n                            <!--suppress UnresolvedMavenProperty -->\n                            <file>${maven.multiModuleProjectDirectory}/formatter.xml</file>\n                        </eclipse>\n\n                        <importOrder>\n                            <file>spotless.importorder</file>\n                        </importOrder>\n                        <removeUnusedImports></removeUnusedImports>\n                    </java>\n                    <pom>\n                        <sortPom>\n                            <encoding>UTF-8</encoding>\n                            <keepBlankLines>true</keepBlankLines>\n                            <indentBlankLines>false</indentBlankLines>\n                            <nrOfIndentSpace>4</nrOfIndentSpace>\n                        </sortPom>\n                    </pom>\n                </configuration>\n            </plugin>\n            <plugin>\n                <groupId>org.jacoco</groupId>\n                <artifactId>jacoco-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n    <profiles>\n        <profile>\n            <id>module-defaults</id>\n            <activation>\n                <activeByDefault>true</activeByDefault>\n            </activation>\n            <modules>\n                <module>cli</module>\n                <module>core</module>\n                <module>coverage-report</module>\n                <module>endtoend-testing</module>\n                <module>languages</module>\n                <module>language-api</module>\n                <module>language-antlr-utils</module>\n                <module>language-testutils</module>\n            </modules>\n        </profile>\n        <profile>\n            <id>deployment</id>\n            <activation>\n                <activeByDefault>false</activeByDefault>\n            </activation>\n            <modules>\n                <module>core</module>\n                <module>languages</module>\n                <module>language-api</module>\n                <module>language-testutils</module>\n                <module>language-antlr-utils</module>\n            </modules>\n            <build>\n                <plugins>\n                    <plugin>\n                        <groupId>org.apache.maven.plugins</groupId>\n                        <artifactId>maven-gpg-plugin</artifactId>\n                        <executions>\n                            <execution>\n                                <id>sign-artifacts</id>\n                                <goals>\n                                    <goal>sign</goal>\n                                </goals>\n                                <phase>verify</phase>\n                                <configuration>\n                                    <keyname>0xC6E9DAC2</keyname>\n                                    <!--suppress UnresolvedMavenProperty -->\n                                    <passphrase>${env.GPG_PASSPHRASE}</passphrase>\n                                </configuration>\n                            </execution>\n                        </executions>\n                    </plugin>\n                </plugins>\n            </build>\n        </profile>\n    </profiles>\n</project>\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jtrack",
            "repo_link": "https://github.com/Biomarker-Development-at-INM7",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/jube",
            "repo_link": "https://github.com/FZJ-JSC/JUBE",
            "content": {
                "codemeta": "",
                "readme": "<div align=\"center\">\n<img src=\"docs/logo/JUBE-Logo.svg\" alt=\"JUBE\" height=\"170em\"/>\n</div>\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7534372.svg)](https://doi.org/10.5281/zenodo.7534372)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n\n# What is JUBE?\n\nThe JUBE benchmarking environment provides a script-based framework for easily\ncreating benchmark and workflow sets, running those sets on different computer\nsystems, and evaluating the results.\nIt is actively developed by the [Juelich Supercomputing Centre](https://www.fz-juelich.de/en/ias/jsc).\nIt focuses on managing the complexity of combinatorial benchmarks and ensuring reproducibility of the benchmarks.\nJUBE provides support for different workflows and the ability to use vendor-supplied platform configurations.\nThe benchmark configuration and scripts can be specified in either YAML or XML format.\nJUBE is primarily designed for use on supercomputers with *scheduding* systems\nlike Slurm or PBS, but also works on laptops running Linux or MacOS operating systems.\n\n## Documentation\n\nJUBE is not (yet) available on `pypi` (it is work in progress).\nThe source code can be downloaded from any of the following places:\n- [GitHub](https://github.com/FZJ-JSC/JUBE)\n- [JSC JUBE Webpage](https://www.fz-juelich.de/en/ias/jsc/services/user-support/software-tools/jube/download)\n\nJUBE can be installed using `pip` or `setup.py` and needs *python 3.2* or higher.\nYou will also need *SQLite* version 3.35.0 (or higher) to use the database as a result output.\nInstallation instructions can be found [here](https://apps.fz-juelich.de/jsc/jube/docu/tutorial.html#installation).\n\nThe documentation for JUBE is split into Beginner Tutorial, Advanced Tutorial, \nFAQ, CLI, and Glossary and can be found in the \n**[User Guide](https://apps.fz-juelich.de/jsc/jube/docu/index.html)**.\n\nIn addition to the documentation, there are also \n[tutorial examples](examples)\nwhich are described in the tutorials of the user guide and \n[benchmark examples](https://github.com/FZJ-JSC/jube-configs), which are curated\nexamples of JUBE benchmarks (the latter will be either replaced or \nupdated/extended soon).\n\nFor more information on the design and architecture of JUBE, please refer to\nthis [paper](https://ebooks.iospress.nl/DOI/10.3233/978-1-61499-621-7-431).\n\n\n## Community and Contributing\n\nJUBE is an open-source project and we welcome your questions, discussions and contributions.\nQuestions can be asked directly to the JSC JUBE developers via mail to\n[jube.jsc@fz-juelich.de](mailto:jube.jsc@fz-juelich.de) and issues can be\nreported in the issue tracker.\nWe also welcome contributions in the form of pull requests.\nContributions can include anything from bug fixes and documentation to new features.\n\nJUBE development is currently still taking place on an internal GitLab instance.\nHowever, we are in a transition phase to move development to GitHub. The complete\nmove will take some time. In the meantime, we will decide individually how to\nproceed with Pull Requests opened on GitHub. Before you start implementing new\nfeatures, we would recommended to contact us, as we still have several open\nbranches in GitLab.\n\n- **[GitHub Issue Tracker](https://github.com/FZJ-JSC/JUBE/issues)**\n- **[Github Discussions](https://github.com/FZJ-JSC/JUBE/discussions)**\n- **[GitHub Pull Requests](https://github.com/FZJ-JSC/JUBE/pulls)**\n\nPlease ensure that your contributions to JUBE are compliant with the \n[contribution](CONTRIBUTING.md), \n[developer](https://apps.fz-juelich.de/jsc/jube/docu/devel.html) and\n[community](CODE_OF_CONDUCT.md) guidelines.\n\n# Citing JUBE\n\nIf you use JUBE in your work, please cite the\n[software release](https://zenodo.org/records/7534372)\nand the [paper](https://ebooks.iospress.nl/DOI/10.3233/978-1-61499-621-7-431).\n\n# Acknowledgments\n\nWe gratefully acknowledge the support of the following research projects and \ninstitutions in the development of JUBE and for granting compute time to develop JUBE. \n\n- UNSEEN (BMWi project, ID: 03EI1004A-F)\n- Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) and the John von\nNeumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS at\nJülich Supercomputing Centre (JSC)\n\n",
                "dependencies": "# JUBE Benchmarking Environment\n# Copyright (C) 2008-2024\n# Forschungszentrum Juelich GmbH, Juelich Supercomputing Centre\n# http://www.fz-juelich.de/jsc/jube\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n# For installation you can use:\n#\n# python setup.py install --user\n#\n# to install it into your .local folder. .local/bin must be inside your $PATH.\n# You can also change the folder by using --prefix instead of --user\n\nimport os\nadd_opt = dict()\ntry:\n    from setuptools import setup\n    import sys\n    add_opt[\"install_requires\"] = ['pyyaml']\n    if sys.hexversion < 0x02070000:\n        add_opt[\"install_requires\"].append(\"argparse\")\nexcept ImportError:\n    from distutils.core import setup\n\nSHARE_PATH = \"share/jube\"\n\n\ndef rel_path(directory, new_root=\"\"):\n    \"\"\"Return list of tuples (directory, list of files)\n    recursively from directory\"\"\"\n    setup_dir = os.path.join(os.path.dirname(__file__))\n    cwd = os.getcwd()\n    result = list()\n    if setup_dir != \"\":\n        os.chdir(setup_dir)\n    for path_info in os.walk(directory):\n        root = path_info[0]\n        filenames = path_info[2]\n        files = list()\n        for filename in filenames:\n            path = os.path.join(root, filename)\n            if (os.path.isfile(path)) and (filename[0] != \".\"):\n                files.append(path)\n        if len(files) > 0:\n            result.append((os.path.join(new_root, root), files))\n    if setup_dir != \"\":\n        os.chdir(cwd)\n    return result\n\n\nconfig = {'name': 'JUBE',\n          'description': 'JUBE Benchmarking Environment',\n          'author': 'Forschungszentrum Juelich GmbH',\n          'url': 'www.fz-juelich.de/ias/jsc/jube',\n          'download_url': 'www.fz-juelich.de/ias/jsc/jube',\n          'author_email': 'jube.jsc@fz-juelich.de',\n          'version': '2.7.1',\n          'packages': ['jube','jube.result_types','jube.util'],\n          'package_data': {'jube': ['help.txt']},\n          'data_files': ([(os.path.join(SHARE_PATH, 'docu'),\n                           ['docs/JUBE.pdf']),\n                          (SHARE_PATH,\n                          ['AUTHORS','LICENSE','RELEASE_NOTES','CITATION.cff',\n                           'CODE_OF_CONDUCT.md', 'CONTRIBUTING.md'])] +\n                         rel_path(\"examples\", SHARE_PATH) +\n                         rel_path(\"contrib\", SHARE_PATH) +\n                         rel_path(\"platform\", SHARE_PATH)),\n          'scripts': ['bin/jube', 'bin/jube-autorun'],\n          'long_description': (\n              \"Automating benchmarks is important for reproducibility and \"\n              \"hence comparability which is the major intent when \"\n              \"performing benchmarks. Furthermore managing different \"\n              \"combinations of parameters is error-prone and often \"\n              \"results in significant amounts work especially if the \"\n              \"parameter space gets large.\\n\"\n              \"In order to alleviate these problems JUBE helps performing \"\n              \"and analyzing benchmarks in a systematic way. It allows \"\n              \"custom work flows to be able to adapt to new architectures.\\n\"\n              \"For each benchmark application the benchmark data is written \"\n              \"out in a certain format that enables JUBE to deduct the \"\n              \"desired information. This data can be parsed by automatic \"\n              \"pre- and post-processing scripts that draw information, \"\n              \"and store it more densely for manual interpretation.\\n\"\n              \"The JUBE benchmarking environment provides a script based \"\n              \"framework to easily create benchmark sets, run those sets \"\n              \"on different computer systems and evaluate the results. It \"\n              \"is actively developed by the Juelich Supercomputing Centre \"\n              \"of Forschungszentrum Juelich, Germany.\"),\n          'license': 'GPLv3',\n          'platforms': 'Linux',\n          'classifiers': [\n              \"Development Status :: 5 - Production/Stable\",\n              \"Environment :: Console\",\n              \"Intended Audience :: End Users/Desktop\",\n              \"Intended Audience :: Developers\",\n              \"Intended Audience :: System Administrators\",\n              \"License :: OSI Approved :: GNU General Public License v3 \" +\n              \"(GPLv3)\",\n              \"Operating System :: POSIX :: Linux\",\n              \"Programming Language :: Python :: 3.2\",\n              \"Topic :: System :: Monitoring\",\n              \"Topic :: System :: Benchmark\",\n              \"Topic :: Software Development :: Testing\"],\n          'keywords': 'JUBE Benchmarking Environment'}\nconfig.update(add_opt)\n\nsetup(**config)\n\ntry:\n    import ruamel.yaml\nexcept ImportError:\n    print(\"Warning: The python package 'ruamel.yaml' is not installed. The validity of yaml files cannot be checked properly and silent errors can occur. Nevertheless, the installation is complete.\")\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jukkr",
            "repo_link": "https://iffgit.fz-juelich.de/kkr/jukkr",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/julearn",
            "repo_link": "https://github.com/juaml/julearn",
            "content": {
                "codemeta": "",
                "readme": "# julearn\n\n![PyPI](https://img.shields.io/pypi/v/julearn?style=flat-square)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/julearn?style=flat-square)\n![PyPI - Wheel](https://img.shields.io/pypi/wheel/julearn?style=flat-square)\n[![Anaconda-Server Badge](https://anaconda.org/conda-forge/julearn/badges/version.svg)](https://anaconda.org/conda-forge/julearn)\n![GitHub](https://img.shields.io/github/license/juaml/julearn?style=flat-square)\n![Codecov](https://img.shields.io/codecov/c/github/juaml/julearn?style=flat-square)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json)](https://github.com/charliermarsh/ruff)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)\n\n## About\n\nThe Forschungszentrum Jülich Machine Learning Library\n\nCheck our full documentation here: https://juaml.github.io/julearn/index.html\n\nIt is currently being developed and maintained at the [Applied Machine Learning](https://www.fz-juelich.de/en/inm/inm-7/research-groups/applied-machine-learning-aml) group at [Forschungszentrum Juelich](https://www.fz-juelich.de/en), Germany.\n\n## Installation\n\nUse `pip` to install from PyPI like so:\n\n```\npip install julearn\n```\n\nYou can also install via `conda`, like so:\n\n```\nconda install -c conda-forge julearn\n```\n\n## Licensing\n\njulearn is released under the AGPL v3 license:\n\njulearn, FZJuelich AML machine learning library.\nCopyright (C) 2020, authors of julearn.\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n## Citing\n\nIf you use julearn in a scientific publication, please use the following reference\n\n> Hamdan, Sami, Shammi More, Leonard Sasse, Vera Komeyer, Kaustubh R. Patil, and Federico Raimondo. ‘Julearn: An Easy-to-Use Library for Leakage-Free Evaluation and Inspection of ML Models’. arXiv, 19 October 2023. https://doi.org/10.48550/arXiv.2310.12568.\n\nSince julearn is also heavily reliant on scikit-learn, please also cite them: https://scikit-learn.org/stable/about.html#citing-scikit-learn\n\n",
                "dependencies": "[build-system]\nrequires = [\n  \"setuptools >= 61.0.0\",\n  \"wheel\",\n  \"setuptools_scm[toml] >= 6.2\"\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"julearn\"\ndescription = \"Juelich Machine Learning Library\"\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\nlicense = {text = \"AGPL-3.0-only\"}\nauthors = [\n    { name = \"Fede Raimondo\", email = \"f.raimondo@fz-juelich.de\" },\n    { name = \"Sami Hamdan\", email = \"s.hamdan@fz-juelich.de\" },\n]\nmaintainers = [\n    {name = \"Sami Hamdan\", email = \"s.hamdan@fz-juelich.de\"},\n]\nkeywords = [\n    \"machine-learning\",\n]\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Intended Audience :: Science/Research\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved\",\n    \"Natural Language :: English\",\n    \"Topic :: Software Development\",\n    \"Topic :: Scientific/Engineering\",\n    \"Operating System :: OS Independent\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n]\ndependencies = [\n    \"numpy>=1.24,<1.27\",\n    \"pandas>=1.5.0,<2.3\",\n    \"statsmodels>=0.13,<0.15\",\n    \"scikit-learn>=1.2.0,<1.6.0\",\n]\ndynamic = [\"version\"]\n\n[project.urls]\nhomepage = \"https://juaml.github.io/julearn\"\ndocumentation = \"https://juaml.github.io/julearn\"\nrepository = \"https://github.com/juaml/julearn\"\n\n[project.optional-dependencies]\ndev = [\"tox\", \"pre-commit\"]\ndocs = [\n    \"seaborn>=0.12.2,<0.13\",\n    \"sphinx>=5.3.0,<7.3\",\n    \"sphinx-gallery>=0.13.0,<0.15\",\n    \"furo>=2022.9.29,<2024.0.0\",\n    \"sphinx_copybutton>=0.5.0,<0.6\",\n    \"numpydoc>=1.5.0,<1.6\",\n    \"towncrier<24\",\n    \"scikit-optimize>=0.10.0,<0.11\",\n    \"optuna>=3.6.0,<3.7\",\n    \"optuna_integration>=3.6.0,<4.1\",\n]\ndeslib = [\"deslib>=0.3.5,<0.4\"]\nviz = [\n    \"panel>=1.3.0\",\n    \"bokeh>=3.0.0\",\n    \"param>=2.0.0\",\n]\nskopt = [\"scikit-optimize>=0.10.0,<0.11\"]\noptuna = [\n    \"optuna>=3.6.0,<3.7\",\n    \"optuna_integration>=3.6.0,<4.1\",\n]\n# Add all optional functional dependencies (skip deslib until its fixed)\n# This does not include dev/docs building dependencies\nall = [\"julearn[viz,skopt,optuna]\"]\n\n################\n# Tool configs #\n################\n\n[tool.setuptools]\npackages = [\"julearn\"]\n\n[tool.setuptools_scm]\nversion_scheme = \"guess-next-dev\"\nlocal_scheme = \"no-local-version\"\nwrite_to = \"julearn/_version.py\"\n\n[tool.black]\nline-length = 79\ntarget-version = [\"py38\", \"py39\", \"py310\", \"py311\"]\n\n[tool.codespell]\nskip = \"*/auto_examples/*,*.html,.git/,*.pyc,*/_build/*,*/api/generated/*.examples,julearn/external/*\"\ncount = \"\"\nquiet-level = 3\nignore-words = \"ignore_words.txt\"\ninteractive = 0\nbuiltin = \"clear,rare,informal,names,usage,code\"\n\n[tool.ruff]\nline-length = 79\nextend-exclude = [\n    \"__init__.py\",\n    \"docs\",\n    \"examples\",\n    \"external\",\n]\n\n[tool.ruff.lint]\nselect = [\n    # flake8-bugbear\n    \"B\",\n    # flake8-blind-except\n    \"BLE\",\n    # flake8-comprehensions\n    \"C4\",\n    # mccabe\n    \"C90\",\n    # pydocstyle\n    \"D\",\n    # pycodestyle errors\n    \"E\",\n    # pyflakes\n    \"F\",\n    # isort\n    \"I\",\n    # pep8-naming\n    \"N\",\n    # pygrep-hooks\n    \"PGH\",\n    # ruff\n    \"RUF\",\n    # flake8-type-checking\n    \"TCH\",\n    # pyupgrade\n    \"UP\",\n    # pycodestyle warnings\n    \"W\",\n    # flake8-2020\n    \"YTT\",\n]\nextend-ignore = [\n    # Use of `functools.lru_cache` or `functools.cache` on methods can lead to\n    # memory leaks. The cache may retain instance references, preventing garbage\n    # collection.\n    \"B019\",\n    # abstract class with no abstract methods\n    \"B024\",\n    \"D202\",\n    # missing docstring in __init__, incompatible with numpydoc\n    \"D107\",\n    # use r\"\"\" if any backslashes in a docstring\n    \"D301\",\n    # class names should use CapWords convention\n    \"N801\",\n    # function name should be lowercase\n    \"N802\",\n    # variable in function should be lowercase\n    \"N806\",\n    # use specific rule codes when ignoring type issues\n    \"PGH003\",\n]\n\n[tool.ruff.lint.isort]\nlines-after-imports = 2\nknown-first-party = [\"julearn\"]\nknown-third-party =[\n    \"numpy\",\n    \"pandas\",\n    \"sklearn\",\n    \"statsmodels\",\n    \"bokeh\",\n    \"panel\",\n    \"param\",\n    \"deslib\",\n    \"pytest\",\n]\n\n[tool.ruff.lint.mccabe]\nmax-complexity = 20\n\n[tool.towncrier]\ndirectory = \"docs/changes/newsfragments\"\nfilename = \"docs/whats_new.rst\"\npackage = \"julearn\"\n# to use gh_substitutions\nissue_format = \":gh:`{issue}`\"\n# modify to have proper toctree\nunderlines = \"-^~\"\n# set line length to 79\nwrap = true\n\n# Need to put default explicitly as custom is not combined with default\n\n[tool.towncrier.fragment.bugfix]\nname = \"Bugfixes\"\nshowcontent = true\n\n[tool.towncrier.fragment.doc]\nname = \"Improved Documentation\"\nshowcontent = true\n\n[tool.towncrier.fragment.feature]\nname = \"Features\"\nshowcontent = true\n\n[tool.towncrier.fragment.misc]\nname = \"Misc\"\nshowcontent = true\n\n[tool.towncrier.fragment.removal]\nname = \"Deprecations and Removals\"\nshowcontent = true\n\n# Add custom towncrier fragment for enhancements\n[tool.towncrier.fragment.enh]\nname = \"Enhancements\"\nshowcontent = true\n\n# Add custom towncrier fragment for API changes\n[tool.towncrier.fragment.change]\nname = \"API Changes\"\nshowcontent = true\n\n## Configure pyright to ignore assignment types until scikit-learn stubs are updated\n[tool.pyright]\nreportAssignmentType = \"none\"\nexclude = [\n    \"docs/auto_examples/\",\n    \"*.html\",\n    \".git/\",\n    \"*.pyc,\",\n    \"*/_build/*\",\n    \"*/api/generated/*.examples\",\n    \"build/\",\n    \"examples/XX_disabled/\",\n    \".tox\",\n    \".eggs\",\n    \"examples/\",  # Lots of problems due to bad stubs, avoid filling the example with # type:ignore\n    \"julearn/external\",  # External code, not to be checked\n    \"scratch/\",  # place to prototype, not to be checked\n]\n\n\"\"\"Set up julearn package.\"\"\"\n\n# Authors: Federico Raimondo <f.raimondo@fz-juelich.de>\n#          Sami Hamdan <s.hamdan@fz-juelich.de>\n#          Synchon Mandal <s.mandal@fz-juelich.de>\n# License: AGPL\n\nfrom setuptools import setup\n\n\nif __name__ == \"__main__\":\n    setup()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jumpdiff",
            "repo_link": "https://github.com/LRydin/jumpdiff",
            "content": {
                "codemeta": "",
                "readme": "![PyPI - License](https://img.shields.io/pypi/l/jumpdiff)\n![PyPI](https://img.shields.io/pypi/v/jumpdiff)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/jumpdiff)\n[![Build Status](https://github.com/LRydin/jumpdiff/actions/workflows/CI.yml/badge.svg)](https://github.com/LRydin/jumpdiff/actions/workflows/CI.yml)\n[![codecov](https://codecov.io/gh/LRydin/jumpdiff/branch/master/graph/badge.svg)](https://codecov.io/gh/LRydin/jumpdiff)\n[![Documentation Status](https://readthedocs.org/projects/jumpdiff/badge/?version=latest)](https://jumpdiff.readthedocs.io/en/latest/?badge=latest)\n\n# jumpdiff\n`jumpdiff` is a `python` library with non-parametric Nadaraya─Watson estimators to extract the parameters of jump-diffusion processes.\nWith `jumpdiff` one can extract the parameters of a jump-diffusion process from one-dimensional timeseries, employing both a kernel-density estimation method combined with a set on second-order corrections for a precise retrieval of the parameters for short timeseries.\n\n## Installation\nTo install `jumpdiff`, run\n\n```\n   pip install jumpdiff\n```\n\nThen on your favourite editor just use\n\n```python\n   import jumpdiff as jd\n```\n\n## Dependencies\nThe library parameter estimation depends on `numpy` and `scipy` solely. The mathematical formulae depend on `sympy`. It stems from [`kramersmoyal`](https://github.com/LRydin/KramersMoyal) project, but functions independently from it<sup>3</sup>.\n\n## Documentation\nYou can find the documentation [here](https://jumpdiff.readthedocs.io/).\n\n# Jump-diffusion processes\n## The theory\nJump-diffusion processes<sup>1</sup>, as the name suggest, are a mixed type of stochastic processes with a diffusive and a jump term.\nOne form of these processes which is mathematically traceable is given by the [Stochastic Differential Equation](https://en.wikipedia.org/wiki/Stochastic_differential_equation)\n\n<img src=\"/Others/SDE_1.png\" title=\"A jump diffusion process\" height=\"25\"/>\n\nwhich has 4 main elements: a drift term <img src=\"/Others/a_xt.png\" title=\"drift term\" height=\"18\"/>, a diffusion term <img src=\"/Others/b_xt.png\" title=\"diffusion term\" height=\"18\"/>, and jump amplitude term <img src=\"/Others/xi.png\" title=\"jump amplitude term\" height=\"18\"/>, which is given by a Gaussian distribution, and finally a jump rate <img src=\"/Others/lambda.png\" title=\"jump rate term\" height=\"14\"/>.\nYou can find a good review on this topic in Ref. 2.\n\n## Integrating a jump-diffusion process\nLet us use the functions in `jumpdiff` to generate a jump-difussion process, and subsequently retrieve the parameters. This is a good way to understand the usage of the integrator and the non-parametric retrieval of the parameters.\n\nFirst we need to load our library. We will call it `jd`\n```python\nimport jumpdiff as jd\n```\nLet us thus define a jump-diffusion process and use `jd_process` to integrate it. Do notice here that we need the drift <img src=\"/Others/a_xt.png\" title=\"drift term\" height=\"18\"/> and diffusion <img src=\"/Others/b_xt.png\" title=\"diffusion term\" height=\"18\"/> as functions.\n\n```python\n# integration time and time sampling\nt_final = 10000\ndelta_t = 0.001\n\n# A drift function\ndef a(x):\n    return -0.5*x\n\n# and a (constant) diffusion term\ndef b(x):\n    return 0.75\n\n# Now define a jump amplitude and rate\nxi = 2.5\nlamb = 1.75\n\n# and simply call the integration function\nX = jd.jd_process(t_final, delta_t, a=a, b=b, xi=xi, lamb=lamb)\n```\n\nThis will generate a jump diffusion process `X` of length `int(10000/0.001)` with the given parameters.\n\n<img src=\"/Others/X_trajectory.png\" title=\"A jump-difussion process\" height=\"200\"/>\n\n## Using `jumpdiff` to retrieve the parameters\n### Moments and Kramers─Moyal coefficients\nTake the timeseries `X` and use the function `moments` to retrieve the conditional moments of the process.\nFor now let us focus on the shortest time lag, so we can best approximate the Kramers─Moyal coefficients.\nFor this case we can simply employ\n\n```python\nedges, moments = jd.moments(timeseries = X)\n```\nIn the array `edges` are the limits of our space, and in our array `moments` are recorded all 6 powers/order of our conditional moments.\nLet us take a look at these before we proceed, to get acquainted with them.\n\nWe can plot the first moment with any conventional plotter, so lets use here `plotly` from `matplotlib`\n\n```python\nimport matplotlib.plotly as plt\n\n# we want the first power, so we need 'moments[1,...]'\nplt.plot(edges, moments[1,...])\n```\nThe first moment here (i.e., the first Kramers─Moyal coefficient) is given solely by the drift term that we have selected `-0.5*x`\n\n<img src=\"/Others/1_moment.png\" title=\"The 1st Kramers─Moyal coefficient\" height=\"200\"/>\n\nAnd the second moment (i.e., the second Kramers─Moyal coefficient) is a mixture of both the contributions of the diffusive term <img src=\"/Others/b_xt.png\" title=\"diffusion term\" height=\"18\"/> and the jump terms <img src=\"/Others/xi.png\" title=\"jump amplitude term\" height=\"18\"/> and <img src=\"/Others/lambda.png\" title=\"jump rate term\" height=\"14\"/>.\n\n<img src=\"/Others/2_moment.png\" title=\"The 2nd Kramers─Moyal coefficient\" height=\"200\"/>\n\nYou have this stored in `moments[2,...]`.\n\n### Retrieving the jump-related terms\nNaturally one of the most pertinent questions when addressing jump-diffusion processes is the possibility of recovering these same parameters from data. For the given jump-diffusion process we can use the `jump_amplitude` and `jump_rate` functions to non-parametrically estimate the jump amplitude <img src=\"/Others/xi.png\" title=\"jump amplitude term\" height=\"18\"/> and jump rate <img src=\"/Others/lambda.png\" title=\"jump rate term\" height=\"18\"/> terms.\n\nAfter having the `moments` in hand, all we need is\n\n```python\n# first estimate the jump amplitude\nxi_est = jd.jump_amplitude(moments = moments)\n\n# and now estimated the jump rate\nlamb_est = jd.jump_rate(moments = moments)\n```\nwhich resulted in our case in `(xi_est) ξ = 2.43 ± 0.17` and `(lamb_est) λ = 1.744 * delta_t` (don't forget to divide `lamb_est` by `delta_t`)!\n\n### Other functions and options\nInclude in this package is also the [Milstein scheme](https://en.wikipedia.org/wiki/Milstein_method) of integration, particularly important when the diffusion term has some spacial `x` dependence. `moments` can actually calculate the conditional moments for different lags, using the parameter `lag`.\n\nIn `formulae` the set of formulas needed to calculate the second order corrections are given (in `sympy`).\n\n# Contributions\nWe welcome reviews and ideas from everyone. If you want to share your ideas, upgrades, doubts, or simply report a bug, open an [issue](https://github.com/LRydin/jumpdiff/issues) here on GitHub, or contact us directly.\nIf you need help with the code, the theory, or the implementation, drop us an email.\nWe abide to a [Conduct of Fairness](contributions.md).\n\n# Changelog\n- Version 0.4 - Designing a set of self-consistency checks, the documentation, examples, and a trial code. Code at PyPi.\n- Version 0.3 - Designing a straightforward procedure to retrieve the jump amplitude and jump rate functions, alongside with a easy `sympy` displaying the correction.\n- Version 0.2 - Introducing the second-order corrections to the moments\n- Version 0.1 - Design an implementation of the `moments` functions, generalising `kramersmoyal` `km`.\n\n# Literature and Support\n\n### History\nThis project was started in 2017 at the [neurophysik](https://www.researchgate.net/lab/Klaus-Lehnertz-Lab-2) by Leonardo Rydin Gorjão, Jan Heysel, Klaus Lehnertz, and M. Reza Rahimi Tabar, and separately by Pedro G. Lind, at the Department of Computer Science, Oslo Metropolitan University. From 2019 to 2021, Pedro G. Lind, Leonardo Rydin Gorjão, and Dirk Witthaut developed a set of corrections and an implementation for python, presented here.\n\n### Funding\nHelmholtz Association Initiative _Energy System 2050 - A Contribution of the Research Field Energy_ and the grant No. VH-NG-1025 and *STORM - Stochastics for Time-Space Risk Models* project of the Research Council of Norway (RCN) No. 274410.\n\n---\n##### Bibliography\n\n<sup>1</sup> Tabar, M. R. R. *Analysis and Data-Based Reconstruction of Complex Nonlinear Dynamical Systems.* Springer, International Publishing (2019), Chapter [*Stochastic Processes with Jumps and Non-vanishing Higher-Order Kramers–Moyal Coefficients*](https://doi.org/10.1007/978-3-030-18472-8_11).\n\n<sup>2</sup> Friedrich, R., Peinke, J., Sahimi, M., Tabar, M. R. R. *Approaching complexity by stochastic methods: From biological systems to turbulence,* [Physics Reports 506, 87–162 (2011)](https://doi.org/10.1016/j.physrep.2011.05.003).\n\n<sup>3</sup> Rydin Gorjão, L., Meirinhos, F. *kramersmoyal: Kramers–Moyal coefficients for stochastic processes.* [Journal of Open Source Software, **4**(44) (2019)](https://doi.org/10.21105/joss.01693).\n\n##### Extended Literature\nYou can find further reading on SDE, non-parametric estimatons, and the general principles of the Fokker–Planck equation, Kramers–Moyal expansion, and related topics in the classic (physics) books\n\n- Risken, H. *The Fokker–Planck equation.* Springer, Berlin, Heidelberg (1989).\n- Gardiner, C.W. *Handbook of Stochastic Methods.* Springer, Berlin (1985).\n\nAnd an extensive review on the subject [here](http://sharif.edu/~rahimitabar/pdfs/80.pdf)\n\n",
                "dependencies": "numpy\nscipy\nsympy\n\nimport setuptools\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=\"jumpdiff\",\n    version=\"0.4.2\",\n    author=\"Leonardo Rydin Gorjão\",\n    author_email=\"leonardo.rydin@gmail.com\",\n    description=\"jumpdiff: Non-parametric estimators for jump-diffusion processes for Python.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/LRydin/jumpdiff\",\n    packages=setuptools.find_packages(),\n    install_requires=[\n        \"numpy\",\n        \"scipy\",\n        \"sympy\",\n    ],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    license=\"MIT License\",\n    python_requires='>=3.5',\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jumper",
            "repo_link": "https://go.fzj.de/jumper",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jupedsim",
            "repo_link": "https://github.com/PedestrianDynamics/jupedsim",
            "content": {
                "codemeta": "",
                "readme": "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1293771.svg)](https://doi.org/10.5281/zenodo.1293771)\n[![GitHub license](https://img.shields.io/badge/license-LGPL-blue.svg)](https://raw.githubusercontent.com/PedestrianDynamics/jupedsim/master/LICENSE)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/jupedsim)\n![PyPI - Version](https://img.shields.io/pypi/v/jupedsim)\n\n# Jülich Pedestrian Simulator - JuPedSim\n\nJuPedSim is a library to simulate pedestrian dynamics. This software is mainly\ndeveloped at the Institute for Civil Safety\n[IAS-7](https://www.fz-juelich.de/en/ias/ias-7) of the Jülich Research Center\n(Forschungszentrum Jülich) in Germany.\n\n## Installation\n\nIt is easiest to install directly with pip from\n[PyPi.org](https://pypi.org/project/jupedsim/)\n\n```\npip install jupedsim\n```\n\n## Usage\n\nPlease consult our [documentation.](http://jupedsim.org)\n\n## Contributing\n\nJuPedSim is licensed under [GNU LGPLv3](LICENSE) hence we are looking forward\nto your contributions and would be happy to see questions, issues and pull\nrequests.\n\n### Questions\n\nIf you have a question or a problem please open a new topic in [GitHub\ndiscussions](https://github.com/PedestrianDynamics/jupedsim/discussions).\n\n### Issues\n\nIf you found a bug and want to give us a chance to fix it we would be very\nhappy to hear from you. To make it easy for us to help you please include the\nfollowing information when you open a [new\nissue](https://github.com/PedestrianDynamics/jupedsim/issues):\n\n* What did JuPedSim do?\n* What did you expect JuPedSim to do?\n* How can we reproduce the issue?\n\n### Pull Requests\n\nIf you encounter a bug and are would like to submit a fix feel free to open a\nPR, we will look into it.\n\nBefore embarking on larger work it is a good idea to\n[discuss](https://github.com/PedestrianDynamics/jupedsim/discussions) what you\nplan.\n\nWhile we are very happy if you contribute we reserve us the right to\ndecline your PR because it may not fit into our vision of JuPedSim.\n\n## License\n\n[GNU LGPLv3](LICENSE)\n\n## Building from source\n\nHere you have two options.\n\n### With setuptools\n\nYou will need a C++20 capable compiler and CMake >= 3.19 installed on your\nsystem. Then install our python dependencies via pip. Our python package\ndependencies are listed in `requirements.txt` in the root of this repository.\nNow you can call `pip install .`\n\nE.g.:\n\n```bash\ncd jupedsim\npip install -r requirements.txt\npip install .\n```\n\n### Compile yourself\n\nYou will need a C++20 capable compiler and CMake >= 3.19 installed on your\nsystem. Then install our python dependencies via pip. Our python package\ndependencies are listed in `requirements.txt` in the root of this repository.\nNow you can generate makefiles with CMake, then compile and run the python\nlibrary.\n\n```bash\npip install -r jupedsim/requirements.txt\nmkdir jupedsim-build\ncd jupedsim-build\ncmake ../jupedsim\nmake -j\nsource ./environment\n```\n\nThe last line in the above description will populate the python path with the\nlocation of our python code and the native library.\n\n> [!WARNING]\n>\n> When sourcing `./environment` from the build folder you need to ensure JuPedSim\n> is not installed in the current python environment. Otherwise there will be\n> erroneous calls to the wrong python code, resulting in crashes and/or\n> exceptions.\n\n\n",
                "dependencies": "################################################################################\n# Project setup\n################################################################################\ncmake_minimum_required(VERSION 3.22 FATAL_ERROR)\nproject(JuPedSim VERSION 1.3.0 LANGUAGES CXX)\nset(CMAKE_CXX_STANDARD 20)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\nset(CMAKE_MODULE_PATH \"${CMAKE_SOURCE_DIR}/cmake_modules\")\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\ninclude(helper_functions)\n\nprint_var(PROJECT_VERSION)\n\n# Set default build type to release\nset(default_build_type \"Release\")\nif(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n  message(STATUS \"Setting build type to '${default_build_type}' as none was specified.\")\n  set(CMAKE_BUILD_TYPE \"${default_build_type}\" CACHE\n      STRING \"Choose the type of build.\" FORCE)\nendif()\n\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\nset(USE_IPO ON)\n\ncheck_prefix_path()\n\n################################################################################\n# Optional features\n################################################################################\nset(WERROR OFF CACHE BOOL \"Build wth -Werror enabled. Not supported on Windows\")\nprint_var(WERROR)\n\nset(BUILD_TESTS OFF CACHE BOOL \"Build tests\")\nprint_var(BUILD_TESTS)\n\nset(BUILD_WITH_ASAN OFF CACHE BOOL\n  \"Build with address sanitizer support. Linux / macOS only.\")\nprint_var(BUILD_WITH_ASAN)\nif(BUILD_WITH_ASAN AND ${CMAKE_SYSTEM} MATCHES \"Windows\")\n    message(FATAL_ERROR \"Address sanitizer builds are not supported on Windows\")\nendif()\n\nset(BUILD_BENCHMARKS OFF CACHE BOOL \"Build micro benchmark\")\nprint_var(BUILD_BENCHMARKS)\n\nset(WITH_FORMAT OFF CACHE BOOL \"Create format tools\")\nprint_var(WITH_FORMAT)\nif(WITH_FORMAT AND ${CMAKE_SYSTEM} MATCHES \"Windows\")\n    message(FATAL_ERROR \"Format target not supported on Windows\")\nendif()\n\n################################################################################\n# Compilation flags\n################################################################################\n# Note: Setting global compile flags via CMAKE_CXX_FLAGS has the drawback that\n#       generator expressions cannot be used. This leads to all kind of\n#       conditional adding of flags. It is generally preferable to use generator\n#       expresssions.\n#\n# WARNING: Do not break the lines, each option has to be on its own line or\n#          CMake will enclose multiple flags in '' which the compiler then\n#          treats as a single flag and does not understand.\nlist(APPEND COMMON_COMPILE_OPTIONS\n    $<$<AND:$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>,$<BOOL:${WERROR}>>:-Werror>\n    $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-Wall>\n    $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-Wextra>\n    $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fdiagnostics-color=always>\n    $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fPIC>\n    $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fno-omit-frame-pointer>\n    $<$<CXX_COMPILER_ID:MSVC>:/W2>\n    $<$<CXX_COMPILER_ID:MSVC>:/EHsc>\n    $<$<CXX_COMPILER_ID:MSVC>:/MP>\n    $<$<AND:$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>>,$<BOOL:${BUILD_WITH_ASAN}>>:-fno-optimize-sibling-calls>\n    $<$<AND:$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>>,$<BOOL:${BUILD_WITH_ASAN}>>:-fsanitize=address>\n    $<$<AND:$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>>,$<BOOL:${BUILD_WITH_ASAN}>>:-shared-libasan>\n)\n\n################################################################################\n# Dependencies\n################################################################################\nadd_subdirectory(third-party)\n\n################################################################################\n# VCS info\n################################################################################\nfind_package(Git QUIET)\nfind_program(GIT_SCM git DOC \"Git version control\")\nmark_as_advanced(GIT_SCM)\nfind_file(GITDIR NAMES .git PATHS ${CMAKE_SOURCE_DIR} NO_DEFAULT_PATH)\nif (GIT_SCM AND GITDIR)\n    # the commit's SHA1, and whether the building workspace was dirty or not\n    # describe --match=NeVeRmAtCh --always --tags --abbrev=40 --dirty\n    execute_process(COMMAND\n        \"${GIT_EXECUTABLE}\" --no-pager describe --match=Nevermatch --tags --always --dirty\n    WORKING_DIRECTORY \"${CMAKE_SOURCE_DIR}\"\n    OUTPUT_VARIABLE GIT_SHA1\n    ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE)\n    # branch\n    execute_process(\n    COMMAND \"${GIT_EXECUTABLE}\" rev-parse --abbrev-ref HEAD\n    WORKING_DIRECTORY \"${CMAKE_SOURCE_DIR}\"\n    OUTPUT_VARIABLE GIT_BRANCH\n    OUTPUT_STRIP_TRAILING_WHITESPACE\n    )\n\n    # the date of the commit\n    execute_process(COMMAND\n    \"${GIT_EXECUTABLE}\" log -1 --format=%ad --date=local\n    WORKING_DIRECTORY \"${CMAKE_SOURCE_DIR}\"\n    OUTPUT_VARIABLE GIT_DATE\n    ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE)\n\n  execute_process(COMMAND\n    \"${GIT_EXECUTABLE}\" describe --tags --abbrev=0\n    WORKING_DIRECTORY \"${CMAKE_SOURCE_DIR}\"\n    OUTPUT_VARIABLE GIT_TAG\n    ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE)\n\n    # the subject of the commit\n    execute_process(COMMAND\n    \"${GIT_EXECUTABLE}\" log -1 --format=%s\n    WORKING_DIRECTORY \"${CMAKE_SOURCE_DIR}\"\n    OUTPUT_VARIABLE GIT_COMMIT_SUBJECT\n    ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE)\n    # remove # from subject\n    string(REGEX REPLACE \"[\\#\\\"]+\"\n       \"\" GIT_COMMIT_SUBJECT\n       ${GIT_COMMIT_SUBJECT})\nelse()\n    message(STATUS \"Not in a git repo\")\n    set(GIT_SHA1 \"UNKNOWN\")\n    set(GIT_DATE \"UNKNOWN\")\n    set(GIT_COMMIT_SUBJECT \"UNKNOWN\")\n    set(GIT_BRANCH \"UNKNOWN\")\n    set(GIT_TAG \"UNKNOWN\")\nendif()\n\nadd_library(git-info INTERFACE)\ntarget_compile_definitions(git-info INTERFACE\n    GIT_COMMIT_HASH=\"${GIT_SHA1}\"\n    GIT_COMMIT_DATE=\"${GIT_DATE}\"\n    GIT_TAG=\"${GIT_TAG}\"\n    GIT_COMMIT_SUBJECT=\"${GIT_COMMIT_SUBJECT}\"\n    GIT_BRANCH=\"${GIT_BRANCH}\"\n)\nconfigure_file(cmake_templates/BuildInfo.hpp.in ${CMAKE_BINARY_DIR}/generated/build_info/BuildInfo.hpp @ONLY)\nadd_library(build_info INTERFACE)\ntarget_include_directories(build_info INTERFACE\n    ${CMAKE_BINARY_DIR}/generated/build_info\n)\nif(UNIX)\n    configure_file(cmake_templates/unix_env.in ${CMAKE_BINARY_DIR}/environment @ONLY)\nendif()\n\n################################################################################\n# Testing\n################################################################################\nif(BUILD_TESTS)\n    if(UNIX)\n        set(pytest-wrapper-in ${CMAKE_SOURCE_DIR}/cmake_templates/run-systemtests.unix.in)\n        set(pytest-wrapper-out ${CMAKE_BINARY_DIR}/run-systemtests)\n    else()\n        set(pytest-wrapper-in ${CMAKE_SOURCE_DIR}/cmake_templates/run-systemtests.windows.in)\n        set(pytest-wrapper-out ${CMAKE_BINARY_DIR}/run-systemtests.cmd)\n    endif()\n    configure_file(\n        ${pytest-wrapper-in}\n        ${pytest-wrapper-out}\n        @ONLY\n    )\n    add_custom_target(tests\n        DEPENDS systemtests unittests\n    )\n    add_custom_target(systemtests\n        COMMENT \"Running system tests\"\n        COMMAND ${pytest-wrapper-out} -vv -s\n                --basetemp=${CMAKE_BINARY_DIR}/systemtest-out\n                --junit-xml=result-systemtests.xml\n        DEPENDS py_jupedsim\n    )\n    set(unittests_dependency_list libjupedsim-unittests libsimulator-unittests)\n    add_custom_target(unittests\n        DEPENDS ${unittests_dependency_list}\n    )\n\n    add_custom_target(libjupedsim-unittests\n        COMMENT \"Running libjupedsim unittests\"\n        COMMAND $<TARGET_FILE:libjupedsim-tests>\n                --gtest_output=xml:result-libjupedsim-unittests.xml\n        DEPENDS libjupedsim-tests\n    )\n    add_custom_target(libsimulator-unittests\n        COMMENT \"Running unit tests\"\n        COMMAND $<TARGET_FILE:libsimulator-tests>\n                --gtest_output=xml:result-libsimulator-unittests.xml\n        DEPENDS libsimulator-tests\n    )\nendif()\n\nif(UNIX)\n    set(performancetests-wrapper-in ${CMAKE_SOURCE_DIR}/cmake_templates/run-performancetests.unix.in)\n    set(performancetests-wrapper-out ${CMAKE_BINARY_DIR}/run-performancetests)\n    configure_file(\n        ${performancetests-wrapper-in}\n        ${performancetests-wrapper-out}\n        @ONLY\n    )\nendif()\n\n################################################################################\n# Add libraries / executables\n################################################################################\nadd_subdirectory(libjupedsim)\nadd_subdirectory(libcommon)\nadd_subdirectory(libsimulator)\nadd_subdirectory(python_bindings_jupedsim)\n\n################################################################################\n# Code formatting\n################################################################################\nif(UNIX AND WITH_FORMAT)\n    set(clang-format-version 15)\n    find_program(CLANG_FORMAT\n        NAMES\n            clang-format-${clang-format-version}\n            clang-format\n        REQUIRED\n    )\n    if(CLANG_FORMAT)\n        execute_process(\n            COMMAND ${CLANG_FORMAT} --version\n            OUTPUT_VARIABLE version_string\n            ERROR_QUIET\n            OUTPUT_STRIP_TRAILING_WHITESPACE\n        )\n        if(version_string MATCHES \"^.*clang-format version .*\")\n            string(REGEX REPLACE\n                \"^.*clang-format version ([.0-9]+).*\"\n                \"\\\\1\"\n                version\n                \"${version_string}\"\n            )\n            if(version MATCHES \"^${clang-format-version}.*\")\n                message(STATUS \"Found clang-format ${version}, add format-check and reformat targets\")\n                set(folders libcommon libjupedsim libsimulator)\n                add_custom_target(check-format\n                    COMMENT \"Checking format with clang-format\"\n                    COMMAND find ${folders}\n                        -name '*.cpp'\n                        -o -name '*.c'\n                        -o -name '*.h'\n                        -o -name '*.hpp' | xargs ${CLANG_FORMAT}\n                        -n -Werror --style=file\n                    WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\n                )\n                add_custom_target(reformat\n                    COMMENT \"Reformatting code with clang-format\"\n                    COMMAND find ${folders}\n                        -name '*.cpp'\n                        -o -name '*.c'\n                        -o -name '*.h'\n                        -o -name '*.hpp' | xargs ${CLANG_FORMAT}\n                        -i --style=file\n                    WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\n                )\n            else ()\n                message(FATAL_ERROR \"Could not create formatting target, clang-format version ${version} found, but ${clang-format-version} required\")\n            endif()\n        else ()\n            message(FATAL_ERROR \"Could not create formatting target, clang-format ${version_string} does not yield a version number\")\n        endif()\n    endif()\nendif()\n\n################################################################################\n# Integration tests\n################################################################################\nif (BUILD_TESTS)\n  # Find libraries needed by python tests\n  find_python_library(pytest)\nendif (BUILD_TESTS)\n\n[tool.ruff]\nextend-exclude = [\"third-party\", \"docs\"]\nline-length = 80\nindent-width = 4\ntarget-version = \"py312\"\nextend-include = [\"*.ipynb\"]\n\n[tool.ruff.lint]\n# Enable Pyflakes (`F`) and a subset of the pycodestyle (`E`)  codes by default.\n# Unlike Flake8, Ruff doesn't enable pycodestyle warnings (`W`) or\n# McCabe complexity (`C901`) by default.\n# select = [\"E4\", \"E7\", \"E9\", \"F\"]\nselect = [\n    # pycodestyle\n    \"E\",\n    # Pyflakes\n    \"F\",\n    # isort\n    \"I\",\n]\nignore = [\"E501\"]\n\n# Allow fix for all enabled rules (when `--fix`) is provided.\nfixable = [\"ALL\"]\nunfixable = []\n\n# Allow unused variables when underscore-prefixed.\ndummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"\n\n[tool.ruff.format]\n# Like Black, use double quotes for strings.\nquote-style = \"double\"\n\n# Like Black, indent with spaces, rather than tabs.\nindent-style = \"space\"\n\n# Like Black, respect magic trailing commas.\nskip-magic-trailing-comma = false\n\n# Like Black, automatically detect the appropriate line ending.\nline-ending = \"auto\"\n\n# Enable auto-formatting of code examples in docstrings. Markdown,\n# reStructuredText code/literal blocks and doctests are all supported.\n#\n# This is currently disabled by default, but it is planned for this\n# to be opt-out in the future.\ndocstring-code-format = false\n\n# Set the line length limit used when formatting code snippets in\n# docstrings.\n#\n# This only has an effect when the `docstring-code-format` setting is\n# enabled.\ndocstring-code-line-length = \"dynamic\"\n\n\nnumpy~=1.25\nshapely~=2.0\npyside6~=6.5\nvtk~=9.3\n\n# build deps\nwheel\nbuild\nsetuptools\n\n# test deps\npytest~=7.2\npandas~=2.1\n\n# ci deps\njinja2\nruff\n\n# Copyright © 2012-2024 Forschungszentrum Jülich GmbH\n# SPDX-License-Identifier: LGPL-3.0-or-later\nimport glob\nimport os\nimport pathlib\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport tempfile\nimport textwrap\nfrom pathlib import Path\n\nfrom setuptools import Extension, setup\nfrom setuptools.command.build_ext import build_ext\n\nmin_cpp_standard = 20\nmin_cmake_version = \"3.19\"\n\n# Read version number from CMakeLists.txt\nwith open(\"CMakeLists.txt\", \"r\", encoding=\"utf-8\") as cmakelist:\n    cmake_input = cmakelist.read()\n    version_line = re.findall(r\"project\\(JuPedSim.*\", cmake_input)[0]\n    start_index = version_line.rfind(\"VERSION\") + len(\"VERSION\")\n    end_index = version_line.find(\"LANGUAGES\")\n    version = version_line[start_index:end_index].strip()\n\nwith open(\"pypi-readme.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\n# Convert distutils Windows platform specifiers to CMake -A arguments\nPLAT_TO_CMAKE = {\n    \"win32\": \"Win32\",\n    \"win-amd64\": \"x64\",\n    \"win-arm32\": \"ARM\",\n    \"win-arm64\": \"ARM64\",\n}\n\n\ndef check_cmake():\n    try:\n        result = subprocess.run(\n            [\"cmake\", \"--version\"], check=True, capture_output=True, text=True\n        )\n        # Check CMake version\n        found_cmake_version = re.search(\n            r\"(\\d+).(\\d+).(\\d+)\", str(result.stdout)\n        )\n        for min_version, found_version in zip(\n            min_cmake_version.split(\".\"), found_cmake_version.groups()\n        ):\n            if found_version < min_version:\n                return False\n    except Exception as _:\n        return False\n    return True\n\n\ndef check_cpp_compiler():\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        simple_main = textwrap.dedent(\n            \"\"\"\n            int main(){ \n                return 0;\n            }\n            \"\"\"\n        )\n        cpp_file = tmp_dir + \"/main.cpp\"\n        with open(cpp_file, \"w\") as cp_file:\n            cp_file.write(simple_main)\n\n        simple_cmake = textwrap.dedent(\n            f\"\"\"\n            cmake_minimum_required(VERSION 3.19)\n            project(SimpleTest)\n            set(CMAKE_CXX_STANDARD {min_cpp_standard})\n            set(CMAKE_CXX_STANDARD_REQUIRED ON)\n            add_executable(simple_test main.cpp)\n            \"\"\"\n        )\n        cmake_file = tmp_dir + \"/CMakeLists.txt\"\n        with open(cmake_file, \"w\") as cm_file:\n            cm_file.write(simple_cmake)\n\n        tmp_dir_build = pathlib.Path(tmp_dir) / \"build\"\n        tmp_dir_build.mkdir()\n\n        try:\n            subprocess.run(\n                [\"cmake\", \"-S\", str(tmp_dir), \"-B\", str(tmp_dir_build)],\n                check=True,\n            )\n        except Exception as _:\n            return False\n    return True\n\n\n# A CMakeExtension needs a sourcedir instead of a file list.\n# The name must be the _single_ output extension from the CMake build.\n# If you need multiple extensions, see scikit-build.\nclass CMakeExtension(Extension):\n    def __init__(self, name: str, sourcedir: str = \"\") -> None:\n        super().__init__(name, sources=[])\n        self.sourcedir = os.fspath(Path(sourcedir).resolve())\n\n\nclass CMakeBuild(build_ext):\n    def build_extension(self, ext: CMakeExtension) -> None:\n        if not check_cmake():\n            raise ModuleNotFoundError(\n                f\"No CMake or no CMake >= {min_cmake_version} installation \"\n                f\"found on the system, please install \"\n                f\"CMake >= {min_cmake_version} to install JuPedSim.\"\n            )\n\n        if not check_cpp_compiler():\n            raise ModuleNotFoundError(\n                \"Could not compile a simple C++ program, \"\n                f\"please install a C++ compiler with C++{min_cpp_standard} \"\n                f\"support to install JuPedSim.\"\n            )\n\n        # Must be in this form due to bug in .resolve() only fixed in Python 3.10+\n        ext_fullpath = Path.cwd() / self.get_ext_fullpath(ext.name)\n        extdir = ext_fullpath.parent.resolve()\n\n        # Using this requires trailing slash for auto-detection & inclusion of\n        # auxiliary \"native\" libs\n        debug = (\n            int(os.environ.get(\"DEBUG\", 0))\n            if self.debug is None\n            else self.debug\n        )\n        cfg = \"Debug\" if debug else \"Release\"\n\n        # CMake lets you override the generator - we need to check this.\n        # Can be set with Conda-Build, for example.\n        cmake_generator = os.environ.get(\"CMAKE_GENERATOR\", \"\")\n\n        # Set Python_EXECUTABLE instead if you use PYBIND11_FINDPYTHON\n        # EXAMPLE_VERSION_INFO shows you how to pass a value into the C++ code\n        # from Python.\n        cmake_args = [\n            f\"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY={extdir}{os.sep}\",\n            f\"-DCMAKE_BUILD_TYPE={cfg}\",  # not used on MSVC, but no harm\n            \"-DCMAKE_UNITY_BUILD=ON\",\n            f\"-DPython_EXECUTABLE={sys.executable}\",\n        ]\n\n        # Pile all .so in one place and use $ORIGIN as RPATH\n        cmake_args += [\"-DCMAKE_BUILD_WITH_INSTALL_RPATH=TRUE\"]\n        cmake_args += [\"-DCMAKE_INSTALL_RPATH={}\".format(\"$ORIGIN\")]\n\n        build_args = []\n        # Adding CMake arguments set as environment variable\n        # (needed e.g. to build for ARM OSx on conda-forge)\n        if \"CMAKE_ARGS\" in os.environ:\n            cmake_args += [\n                item for item in os.environ[\"CMAKE_ARGS\"].split(\" \") if item\n            ]\n\n        if self.compiler.compiler_type != \"msvc\":\n            # Using Ninja-build since it a) is available as a wheel and b)\n            # multithreads automatically. MSVC would require all variables be\n            # exported for Ninja to pick it up, which is a little tricky to do.\n            # Users can override the generator with CMAKE_GENERATOR in CMake\n            # 3.15+.\n            if not cmake_generator or cmake_generator == \"Ninja\":\n                try:\n                    import ninja\n\n                    ninja_executable_path = Path(ninja.BIN_DIR) / \"ninja\"\n                    cmake_args += [\n                        \"-GNinja\",\n                        f\"-DCMAKE_MAKE_PROGRAM:FILEPATH={ninja_executable_path}\",\n                    ]\n                except ImportError:\n                    pass\n\n        else:\n            # Single config generators are handled \"normally\"\n            single_config = any(\n                x in cmake_generator for x in {\"NMake\", \"Ninja\"}\n            )\n\n            # CMake allows an arch-in-generator style for backward compatibility\n            contains_arch = any(x in cmake_generator for x in {\"ARM\", \"Win64\"})\n\n            # Specify the arch if using MSVC generator, but only if it doesn't\n            # contain a backward-compatibility arch spec already in the\n            # generator name.\n            if not single_config and not contains_arch:\n                cmake_args += [\"-A\", PLAT_TO_CMAKE[self.plat_name]]\n\n            # Multi-config generators have a different way to specify configs\n            if not single_config:\n                cmake_args += [\n                    f\"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_{cfg.upper()}={extdir}\"\n                ]\n                build_args += [\"--config\", cfg]\n\n        if sys.platform.startswith(\"darwin\"):\n            # Cross-compile support for macOS - respect ARCHFLAGS if set\n            archs = re.findall(r\"-arch (\\S+)\", os.environ.get(\"ARCHFLAGS\", \"\"))\n            if archs:\n                cmake_args += [\n                    \"-DCMAKE_OSX_ARCHITECTURES={}\".format(\";\".join(archs))\n                ]\n\n        # Set CMAKE_BUILD_PARALLEL_LEVEL to control the parallel build level\n        # across all generators.\n        if \"CMAKE_BUILD_PARALLEL_LEVEL\" not in os.environ:\n            # self.parallel is a Python 3 only way to set parallel jobs by hand\n            # using -j in the build_ext call, not supported by pip or PyPA-build.\n            if hasattr(self, \"parallel\") and self.parallel:\n                # CMake 3.12+ only.\n                build_args += [f\"-j{self.parallel}\"]\n\n        build_temp = Path(self.build_temp) / ext.name\n        if not build_temp.exists():\n            build_temp.mkdir(parents=True)\n\n        build_args += [\"-j\"]\n\n        subprocess.run(\n            [\"cmake\", ext.sourcedir, *cmake_args], cwd=build_temp, check=True\n        )\n        subprocess.run(\n            [\"cmake\", \"--build\", \".\", *build_args], cwd=build_temp, check=True\n        )\n\n        # Copy library files to root build folder\n        files = glob.glob(str(build_temp) + \"/lib/py_jupedsim*.so\")\n        files.extend(glob.glob(str(build_temp) + \"/lib/py_jupedsim*.dylib\"))\n        files.extend(glob.glob(str(build_temp) + \"/lib/py_jupedsim*.pyd\"))\n\n        for lib_file in files:\n            shutil.copy(dst=extdir / \"jupedsim\", src=lib_file)\n\n\n# The information here can also be placed in setup.cfg - better separation of\n# logic and declaration, and simpler if you include description/version in a file.\nsetup(\n    name=\"jupedsim\",\n    version=version,\n    maintainer=\"JuPedSim Development Core Team\",\n    maintainer_email=\"dev@jupedsim.org\",\n    description=\"JuPedSim is an open source pedestrian dynamics simulator\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    license_files=(\"LICENSE\",),\n    ext_modules=[CMakeExtension(\"python_bindings_jupedsim\")],\n    cmdclass={\"build_ext\": CMakeBuild},\n    zip_safe=False,\n    python_requires=\">=3.10,<3.13\",\n    packages=[\n        \"jupedsim\",\n        \"jupedsim.models\",\n        \"jupedsim.internal\",\n        \"jupedsim.native\",\n        \"jupedsim_visualizer\",\n    ],\n    package_dir={\n        \"jupedsim\": \"python_modules/jupedsim/jupedsim\",\n        \"jupedsim.models\": \"python_modules/jupedsim/jupedsim/models\",\n        \"jupedsim.internal\": \"python_modules/jupedsim/jupedsim/internal\",\n        \"jupedsim.native\": \"python_modules/jupedsim/jupedsim/native\",\n        \"jupedsim_visualizer\": \"python_modules/jupedsim_visualizer/jupedsim_visualizer\",\n    },\n    install_requires=[\n        \"numpy~=1.25\",\n        \"shapely~=2.0\",\n        \"pyside6~=6.5\",\n        \"vtk~=9.3\",\n    ],\n    scripts=[\"python_modules/jupedsim_visualizer/bin/jupedsim-visualizer\"],\n    url=\"https://www.jupedsim.org\",\n    project_urls={\n        \"Documentation\": \"https://www.jupedsim.org\",\n        \"Source\": \"https://github.com/PedestrianDynamics/jupedsim\",\n        \"Tracker\": \"https://github.com/PedestrianDynamics/jupedsim/issues\",\n    },\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"License :: OSI Approved :: GNU Lesser General Public License v3 or later (LGPLv3+)\",\n        \"Operating System :: Microsoft :: Windows\",\n        \"Operating System :: Unix\",\n        \"Operating System :: MacOS\",\n        \"Natural Language :: English\",\n        \"Programming Language :: C++\",\n        \"Programming Language :: C\",\n        \"Programming Language :: Python :: 3 :: Only\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Science/Research\",\n    ],\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jupyterhub-outpost",
            "repo_link": "https://github.com/kreuzert/jupyterhub-outpost",
            "content": {
                "codemeta": "",
                "readme": "[![Documentation Status](https://readthedocs.org/projects/jupyterhub-outpost/badge/?version=latest)](https://jupyterhub-outpost.readthedocs.io/en/latest/?badge=latest)\n[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/jupyterhub-outpost)](https://artifacthub.io/packages/search?repo=jupyterhub-outpost)\n\n# JupyterHub Outpost\n\nJupyterHub Outpost can be used as an additional, external source to start and manage single-user servers. Like in JupyterHub itself, different Spawners can be configured at the Outpost. It's best used together with the [jupyterhub-outpostspawner](https://pypi.org/project/jupyterhub-outpostspawner/) configured at JupyterHub.\n\n## Documentation\n\nLink to [documentation](https://jupyterhub-outpost.readthedocs.io).  \n\n## Overview  \n  \nThe JupyterHub community created many useful [JupyterHub Spawner](https://jupyterhub.readthedocs.io/en/latest/reference/spawners.html#examples) over the past years, to allow JupyterHub to use the specific resources of different systems. For most of these Spawners JupyterHub has to run at the system itself. The JupyterHub Outpost service allows the use of these Spawners on remote systems, if JupyterHub uses the [OutpostSpawner](https://github.com/kreuzert/jupyterhub-outpostspawner/)..\n\nOther Spawners like [SSHSpawner](https://github.com/NERSC/sshspawner) can spawn single-user servers on remote systems, but are not able to use system-specific features like [KubeSpawner](https://github.com/jupyterhub/kubespawner) or [BatchSpawner](https://github.com/jupyterhub/batchspawner).\n\nThe JupyterHub Outpost service in combination with the OutpostSpawner enables a single JupyterHub to offer multiple remote systems of different types.  \n  \n- Use one JupyterHub to offer single-user servers on multiple systems.\n- Each system may use a different JupyterHub Spawner.\n- Integrated SSH port forwarding solution to reach remote single-user server.\n- supports the JupyterHub `internal_ssl` feature.\n- shows events gathered by the remote Spawner to the user.\n- Users can override the configuration of the remote Spawner at runtime (e.g. to select a different Docker Image).\n- One JupyterHub Outpost can be connected to multiple JupyterHubs, without interfering with each other.\n  \n## Requirements  \n  \nJupyterHub must run on a Kubernetes Cluster (recommended is the use of Zero2JupyterHub).  \nThe JupyterHub Outpost must fulfill the requirements of the configured Spawner class. \n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jupyterhub-outpostspawner",
            "repo_link": "https://github.com/kreuzert/jupyterhub-outpostspawner",
            "content": {
                "codemeta": "",
                "readme": "\n[![Documentation Status](https://readthedocs.org/projects/jupyterhub-outpostspawner/badge/?version=latest)](https://jupyterhub-outpostspawner.readthedocs.io/en/latest/?badge=latest)\n\n# OutpostSpawner\n\nThe OutpostSpawner in combination with the [JupyterHub Outpost service](https://github.com/kreuzert/jupyterhub-outpost) enables JupyterHub to spawn single-user notebook servers on multiple remote resources.\n\n## Documentation\n\nLink to [documentation](https://jupyterhub-outpostspawner.readthedocs.io).\n\n## Overview  \n  \nThe JupyterHub community created many useful [JupyterHub Spawner](https://jupyterhub.readthedocs.io/en/latest/reference/spawners.html#examples) over the past years, to allow JupyterHub to use the specific resources of different systems. For most of these Spawners JupyterHub has to run at the system itself. The OutpostSpawner enables the use of these Spawners on remote systems.\n\nOther Spawners like [SSHSpawner](https://github.com/NERSC/sshspawner) can spawn single-user servers on remote systems, but are not able to use system-specific features like [KubeSpawner](https://github.com/jupyterhub/kubespawner) or [BatchSpawner](https://github.com/jupyterhub/batchspawner).\n\nThe JupyterHub Outpost service in combination with the OutpostSpawner enables a single JupyterHub to offer multiple remote systems of different types.  \n  \n- Use one JupyterHub to offer single-user servers on multiple systems.\n- Each system may use a different JupyterHub Spawner.\n- Integrated SSH port forwarding solution to reach remote single-user server.\n- supports the JupyterHub `internal_ssl` feature.\n- shows events gathered by the remote Spawner to the user.\n- Users can override the configuration of the remote Spawner at runtime (e.g. to select a different Docker Image).\n- One JupyterHub Outpost can be connected to multiple JupyterHubs, without interfering with each other.\n  \n## Requirements  \n  \nJupyterHub must run on a Kubernetes Cluster (recommended is the use of Zero2JupyterHub).  \nThe JupyterHub Outpost must fulfill the requirements of the configured Spawner class. \n\n",
                "dependencies": "\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"outpostspawner\"]\ninclude = [\"*.py\"]\nexclude = [\"test*\"]\n\n[project]\nname = \"jupyterhub-outpostspawner\"\ndescription = \"JupyterHub OutpostSpawner enables start on multiple, remote system\"\nreadme = \"README.md\"\nrequires-python = \">=3.9\"\nlicense = {file = \"LICENSE\"}\nkeywords = [\"jupyterhub\", \"spawner\", \"kubernetes\"]\nauthors = [\n    {name = \"Tim Kreuzer\", email = \"t.kreuzer@fz-juelich.de\"},\n    {name = \"Alice Grosch\", email = \"a.grosch@fz-juelich.de\"}\n]\n\ndependencies = [\n    \"escapism\",\n    \"jinja2\",\n    \"jupyterhub>=4.0.0\",\n    \"traitlets\",\n    \"urllib3\",\n    \"jupyterhub-forwardbasespawner\",\n    \"kubernetes\"\n]\ndynamic = [\"version\"]\n\n[tool.black]\ntarget_version = [\n  \"py39\",\n  \"py310\",\n  \"py311\",\n  \"py312\",\n]\n\n[tool.hatch.version]\npath = \"outpostspawner/_version.py\"\n\n[tool.isort]\nprofile = \"black\"\n\n[tool.tbump]\n# Uncomment this if your project is hosted on GitHub:\ngithub_url = \"https://github.com/kreuzert/jupyterhub-outpostspawner\"\n\n[tool.tbump.version]\ncurrent = \"1.0.2\"\nregex = '''\n  (?P<major>\\d+)\n  \\.\n  (?P<minor>\\d+)\n  \\.\n  (?P<patch>\\d+)\n  (?P<pre>((a|b|rc)\\d+)|)\n  \\.?\n  (?P<post>((post)\\d+)|)\n  (?P<dev>(?<=\\.)dev\\d*|)\n'''\n\n[tool.tbump.git]\nmessage_template = \"Bump to {new_version}\"\ntag_template = \"{new_version}\"\n\n# For each file to patch, add a [[tool.tbump.file]] config\n# section containing the path of the file, relative to the\n# pyproject.toml location.\n\n#[[tool.tbump.file]]\n#src = \"pyproject.toml\"\n#search = 'version = \"{current_version}\"'\n\n[[tool.tbump.file]]\nsrc = \"outpostspawner/_version.py\"\n#version_template = '({major}, {minor}, {patch}, \"{pre}\", \"{dev}\")'\n#search = \"version_info = {current_version}\"\n\n#[[tool.tbump.file]]\n#src = \"docs/source/_static/rest-api.yml\"\n#search = \"version: {current_version}\"\n\n[tool.djlint]\nindent = 2\nprofile = \"jinja\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jurassic",
            "repo_link": "https://github.com/slcs-jsc/jurassic",
            "content": {
                "codemeta": "",
                "readme": "# Juelich Rapid Spectral Simulation Code\n\nThe Juelich Rapid Spectral Simulation Code (JURASSIC) is a fast infrared radiative transfer model for the analysis of atmospheric remote sensing measurements.\n\n![logo](https://github.com/slcs-jsc/jurassic/blob/master/docs/logo/JURASSIC_320px.png)\n\n[![release (latest by date)](https://img.shields.io/github/v/release/slcs-jsc/jurassic)](https://github.com/slcs-jsc/jurassic/releases)\n[![commits since latest release (by SemVer)](https://img.shields.io/github/commits-since/slcs-jsc/jurassic/latest)](https://github.com/slcs-jsc/jurassic/commits/master)\n[![last commit](https://img.shields.io/github/last-commit/slcs-jsc/jurassic.svg)](https://github.com/slcs-jsc/jurassic/commits/master)\n[![top language](https://img.shields.io/github/languages/top/slcs-jsc/jurassic.svg)](https://github.com/slcs-jsc/jurassic/tree/master/src)\n[![code size in bytes](https://img.shields.io/github/languages/code-size/slcs-jsc/jurassic.svg)](https://github.com/slcs-jsc/jurassic/tree/master/src)\n[![codacy](https://api.codacy.com/project/badge/Grade/aaba414eaf9e4e6784f13458a285ec2f)](https://app.codacy.com/gh/slcs-jsc/jurassic?utm_source=github.com&utm_medium=referral&utm_content=slcs-jsc/jurassic&utm_campaign=Badge_Grade_Settings)\n[![codecov](https://codecov.io/gh/slcs-jsc/jurassic/branch/master/graph/badge.svg?token=TYGWEJMOLI)](https://codecov.io/gh/slcs-jsc/jurassic)\n[![tests](https://img.shields.io/github/actions/workflow/status/slcs-jsc/jurassic/tests.yml?branch=master&label=tests)](https://github.com/slcs-jsc/jurassic/actions)\n[![docs](https://img.shields.io/github/actions/workflow/status/slcs-jsc/jurassic/docs.yml?branch=master&label=docs)](https://slcs-jsc.github.io/jurassic)\n[![license](https://img.shields.io/github/license/slcs-jsc/jurassic.svg)](https://github.com/slcs-jsc/jurassic/blob/master/COPYING)\n[![doi](https://zenodo.org/badge/DOI/10.5281/zenodo.4572889.svg)](https://doi.org/10.5281/zenodo.4572889)\n\n## Features\n\n* JURASSIC uses the emissivity growth approximation (EGA) or the Curtis-Godson approximation (CGA) to conduct infrared radiative transfer calculations. Band transmittances are obtained from pre-calculated look-up tables from line-by-line calculations.\n* The model was carefully tested in intercomparisons with the Karlsruhe Optimized and Precise Radiative Transfer Algorithm (KOPRA), the Reference Forward Model (RFM), and the Stand-alone AIRS Radiative Transfer Algorithm (SARTA).\n* JURASSIC features an MPI-OpenMP hybrid parallelization for efficient use on HPC systems.\n* Distributed open source under the terms and conditions of the GNU GPL.\n\n## Getting started\n\n### Prerequisites\n\nThis documentation describes the installation of JURASSIC on a Linux system. A number of standard tools (gcc, git, make) and software libraries are needed to install JURASSIC. The [GNU Scientific Library](https://www.gnu.org/software/gsl) is required for numerical calculations. A copy of this library can be found in the git repository.\n\nStart by downloading the source code from the git repository:\n\n    git clone https://github.com/slcs-jsc/jurassic.git\n\nTo update an existing installation use:\n\n    git pull https://github.com/slcs-jsc/jurassic.git\n\n### Installation\n\nFirst, compile the GSL library needed for JURASSIC by using the build script:\n\n    cd jurassic/lib\n    ./build.sh\n\nNext, change to the source directory, edit the Makefile according to your needs, and try to compile the code:\n\n    cd jurassic/src\n    emacs Makefile\n    make\n\nThe binaries will be linked statically, i.e., they can be copied and run on other machines. Sometimes this causes problems. In this case remove the '-static' flag from the CFLAGS in the Makefile and compile again.\n\nBy default we use rather strict compiler warnings. All warning messages will be turned into errors and no binaries will be produced. This behavior is enforced by the flag '-Werror'.\n\nThe binaries will remain in the jurassic/src/ directory.\n\nTo run the test cases to check the installation, please use:\n\n    make check\n\nThis will run sequentially through a set of tests. The execution of the tests will stop if any of the tests fails. Please inspect the log messages.\n\n### Run the examples\n\nJURASSIC provides a project directory for testing the examples and also to store other experiments:\n\n    cd jurassic/projects\n\nThis shows how to run the example for the nadir sounder:\n\n    cd nadir\n    ./run.sh\n\nThis shows how to run the example for the limb sounder:\n\n    cd ../limb\n    ./run.sh\n\nIn both examples, we generate an observation geometry file,\n\n    cat obs.tab\n\na standard atmosphere for mid-latitudes,\n\n    cat atm.tab\n\nand conduct radiative transfer calculations for two or three detector channels:\n\n    cat rad.tab\n\nThe output of the simulation is verified by comparing it to reference data.\nAdditionally, gnuplot is used to create plots of the radiance data:\n\n<p align=\"center\"><img src=\"projects/limb/plot_rad.png\" width=\"45%\"/> &emsp; <img src=\"projects/nadir/plot_rad.png\" width=\"45%\"/></p>\n\nKernel functions are calculated using a finite difference method:\n\n<p align=\"center\"><img src=\"projects/limb/plot_kernel_temperature_792.png\" width=\"45%\"/> &emsp; <img src=\"projects/nadir/plot_kernel_temperature_668.5410.png\" width=\"45%\"/></p>\n\n<p align=\"center\"><img src=\"projects/limb/plot_kernel_H2O_792.png\" width=\"45%\"/> &emsp; <img src=\"projects/nadir/plot_kernel_CO2_668.5410.png\" width=\"45%\"/></p>\n\n## Further information\n\nMore detailed information for new users and developers of JURASSIC is collected in the [GitHub wiki](https://github.com/slcs-jsc/jurassic/wiki).\n\nThese are the main references for citing the JURASSIC model in scientific publications:\n\n* Baumeister, P. F. and Hoffmann, L.: Fast infrared radiative transfer calculations using graphics processing units: JURASSIC-GPU v2.0, Geosci. Model Dev., 15, 1855–1874, https://doi.org/10.5194/gmd-15-1855-2022, 2022.\n\n* Hoffmann, L., and M. J. Alexander, Retrieval of stratospheric temperatures from Atmospheric Infrared Sounder radiance measurements for gravity wave studies, J. Geophys. Res., 114, D07105, https://doi.org/10.1029/2008JD011241, 2009.\n\n* Hoffmann, L., Kaufmann, M., Spang, R., Müller, R., Remedios, J. J., Moore, D. P., Volk, C. M., von Clarmann, T., and Riese, M.: Envisat MIPAS measurements of CFC-11: retrieval, validation, and climatology, Atmos. Chem. Phys., 8, 3671-3688, https://doi.org/10.5194/acp-8-3671-2008, 2008.\n\n* You can cite the source code of JURASSIC by using the DOI https://doi.org/10.5281/zenodo.4572889. This DOI represents all versions, and will always resolve to the latest one. Specific DOIs for each release of JURASSIC can be found on the zenodo web site.\n\nPlease see the [citation file](https://github.com/slcs-jsc/jurassic/blob/master/CITATION.cff) for further information.\n\n## Contributing\n\nWe are interested in sharing JURASSIC for operational or research applications. Please do not hesitate to contact us, if you have any further questions or need support.\n\n## License\n\nJURASSIC is distributed under the [GNU General Public License v3.0](https://github.com/slcs-jsc/jurassic/blob/master/COPYING).\n\n## Contact\n\nDr. Lars Hoffmann\n\nJülich Supercomputing Centre, Forschungszentrum Jülich\n\ne-mail: l.hoffmann@fz-juelich.de\n\nwebsite: https://www.fz-juelich.de/ias/jsc/slcs\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/juri",
            "repo_link": "https://github.com/FZJ-JSC/JURI",
            "content": {
                "codemeta": "",
                "readme": "# JURI - Jülich Reporting Interface\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10232352.svg)](https://doi.org/10.5281/zenodo.10232352)\n\nJURI provides a template driven fully client based web framework to visualize data lists\nand associated data graphs.\n\nJURI is currently used for the [LLview Job Reporting](https://github.com/FZJ-JSC/LLview) and Kontview @JSC. \n\n## Installation\n\nInstallation instructions can be currently found on [LLview's documentation page](https://apps.fz-juelich.de/jsc/llview/docu/install/).\n\n## Further Information\n\nFor further information please see: https://www.fz-juelich.de/jsc/llview\n\nContact: [llview.jsc@fz-juelich.de](mailto:llview.jsc@fz-juelich.de)\n\n## Copyright, License and CLA\n\nCopyright (c) 2023 Forschungszentrum Juelich GmbH, Juelich Supercomputing Centre  \nhttps://www.fz-juelich.de/jsc/llview  \n\nThis is an open source software distributed under the GPLv3 license. More information see the LICENSE file at the top level.\n\nContributions must follow the Contributor License Agreement. More information see the CONTRIBUTING.md file at the top level.\n\n\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kaapana",
            "repo_link": "https://github.com/kaapana/kaapana",
            "content": {
                "codemeta": "",
                "readme": "\n<p align=\"center\">\n <img src=\"https://www.kaapana.ai/kaapana-downloads/kaapana-docs/stable/img/kaapana_logo_2.png\" height=170 alt=\"kaapana\" border=\"0\" />\n</p>\n\n[![Documentation Status](https://readthedocs.org/projects/kaapana/badge/?version=latest)](https://kaapana.readthedocs.io/en/latest/?badge=latest)\n<a href=\"https://join.slack.com/t/kaapana/shared_invite/zt-hilvek0w-ucabihas~jn9PDAM0O3gVQ/\"><img src=\"https://img.shields.io/badge/chat-slack-blueviolet\" /></a>\n\n## What is Kaapana?\n\n<p>\n  <a href=\"https://www.kaapana.ai/kaapana-downloads/kaapana-docs/stable/gif/kaapana-v0.2.1-showcase.mp4\" target=\"_blank\">\n    <img src=\"https://www.kaapana.ai/kaapana-downloads/kaapana-docs/stable/img/thumbnail_kaapana_vid.png\" />\n  </a>\n</p>\n\nKaapana (from the hawaiian word kaʻāpana, meaning \"distributor\" or \"part\") is an open-source toolkit for state-of-the-art platform provisioning in the field of medical data analysis. The applications comprise  AI-based workflows and federated learning scenarios with a focus on radiological and radiotherapeutic imaging. \n\nObtaining large amounts of medical data necessary for developing and training modern machine learning methods is an extremely challenging effort that often fails in a multi-center setting, e.g. due to technical, organizational and legal hurdles. A federated approach where the data remains under the authority of the individual institutions and is only processed on-site is, in contrast, a promising approach ideally suited to overcome these difficulties.\n\nFollowing this federated concept, the goal of Kaapana is to provide a framework and a set of tools for sharing data processing algorithms, for standardized workflow design and execution as well as for performing distributed method development. This will facilitate data analysis in a compliant way enabling researchers and clinicians to perform large-scale multi-center studies.\n\nBy adhering to established standards and by adopting widely used open technologies for private cloud development and containerized data processing, Kaapana integrates seamlessly with the existing clinical IT infrastructure, such as the Picture Archiving and Communication System (PACS), and ensures modularity and easy extensibility.\n\nCore components of Kaapana:\n* **Workflow management:** Large-scale image processing with SOTA deep learning algorithms, such as [nnU-Net](https://github.com/MIC-DKFZ/nnunet) image segmentation and [TotalSegmentator](https://github.com/wasserth/TotalSegmentator)\n* **Datasets:** Exploration, visualization and curation of medical images\n* **Extensions:** Simple integration of new, customized algorithms and applications into the framework\n* **Storage:** An integrated PACS system and Minio for other types of data\n* **System monitoring:** Extensive resource and system monitoring for administrators\n* **User management** Simple user management via [Keycloak](https://www.keycloak.org/)\n\nCore technologies used in Kaapana:\n* [Kubernetes](https://kubernetes.io/): Container orchestration system\n* [Helm](https://helm.sh/): The package manager for Kubernetes\n* [Airflow](https://airflow.apache.org/): Workflow management system enabling complex and flexible data processing workflows\n* [OpenSearch](https://opensearch.org/): Search engine for DICOM metadata-based searches\n* [dcm4chee](https://www.dcm4che.org/): Open source PACS system serving as a central DICOM data storage\n* [Prometheus](https://github.com/prometheus/prometheus): Collecting metrics for system monitoring\n* [Grafana](https://github.com/grafana/grafana): Visualization for monitoring metrics\n* [Keycloak](https://www.keycloak.org/): User authentication\n\n\nCurrently, Kaapana is used in multiple projects in which a Kaapana-based platform is deployed at multiple clinical sites with the objective of distributed radiological image analysis and quantification. The projects include [RACOON](https://racoon.network/) initiated by [NUM](https://www.netzwerk-universitaetsmedizin.de) with all 38 German university clinics participating, the Joint Imaging Platform ([JIP](https://jip.dktk.dkfz.de/jiphomepage/)) initiated by the German Cancer Consortium ([DKTK](https://dktk.dkfz.de/)) with 11 university clinics participating as well as [DART](https://cce-dart.com) initiated by the [Cancer Core Europe](https://cancercoreeurope.eu/) with 7 cancer research centers participating.\n\nFor more information, please also take a look at our publication of the Kaapana-based [Joint Imaging Platform in JCO Clinical Cancer Informatics](https://ascopubs.org/doi/full/10.1200/CCI.20.00045).\n\n## Documentation\n\nCheck out the [documentation](https://kaapana.readthedocs.io/en/latest/) for further information about how Kaapana works, for instructions on how to build, deploy, use and further develop the platform.\n\n## Where to find us\n* [GitLab](https://gitlab.hzdr.de/kaapana/kaapana/): The main Kaapana repository, mirrored on GitHub.\n* [Slack](https://kaapana.slack.com/): Join the community for discussions and updates.\n* [YouTube](https://www.youtube.com/@KaapanaAI): Tutorials, demos and more in-depth presentations.\n* [Website](https://kaapana.ai/)\n\n## Versioning\n\nAs of Kaapana 0.2.0 we follow strict [SemVer](https://semver.org/) approach to versioning.\n\n## Citations\nPlease [cite](https://ascopubs.org/action/showCitFormats?doi=10.1200/CCI.20.00045) the [following paper](https://ascopubs.org/doi/full/10.1200/CCI.20.00045) when using Kaapana:\n\n    Jonas Scherer, Marco Nolden, Jens Kleesiek, Jasmin Metzger, Klaus Kades, Verena Schneider, Michael Bach, Oliver Sedlaczek, Andreas M. Bucher, Thomas J. Vogl, ...Klaus Maier-Hein. Joint Imaging Platform for Federated Clinical Data Analytics. JCO Clinical Cancer Informatics, 4:10271038, November 2020. doi: 10.1200/CCI.20.00045. URL https://ascopubs.org/doi/full/10.1200/CCI.20.00045.\n\nWhen using Kapaana for federated learning please also [cite](https://link.springer.com/chapter/10.1007/978-3-031-18523-6_13#citeas) the [following paper](https://link.springer.com/book/10.1007/978-3-031-18523-6):\n\n    Klaus Kades, Jonas Scherer, Maximilian Zenk, Marius Kempf, and Klaus MaierHein. Towards Real-World Federated Learning in Medical Image Analysis Using Kaapana. In Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health, pages 130140, Cham, 2022b. Springer Nature Switzerland. ISBN 978-3-031-18523-6. doi: 10.1007/978-3-031-18523-6_13. URL https://doi.org/10.1007/978-3-031-18523-6_13.\n\n## Licence\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program (see file LICENCE).  \nIf not, see <https://www.gnu.org/licenses/>.\n\n## Considerations on our license choice\n\nYou can use Kaapana to build any product you like, including commercial closed-source ones since it is a highly modular system. Kaapana is licensed under the [GNU Affero General Public License](https://www.gnu.org/licenses/agpl-3.0.en.html) for now since we want to ensure that we can integrate all developments and contributions to its core system for maximum benefit to the community and give everything back. We consider switching to a more liberal license in the future. This decision will depend on how our project develops and what the feedback from the community is regarding the license. \n\nKaapana is built upon the great work of many other open-source projects, see the documentation for details. For now, we only release source code we created ourselves since providing pre-built docker containers and licensing for highly modular container-based systems is [a complex task](https://www.linuxfoundation.org/blog/2020/04/docker-containers-what-are-the-open-source-licensing-considerations/). We have done our very best to fulfill all requirements, and the choice of AGPL was motivated mainly to make sure we can improve and advance Kaapana in the best way for the whole community. If you have thoughts about this or if you disagree with our way of using a particular third-party toolkit or miss something please let us know and get in touch. We are open to any feedback and advice on this challenging topic.\n\n## Acknowledgments\n\n### Supporting projects\n\n**Building Data Rich Clinical Trials - CCE_DART**: This project has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement No 965397. Website: <https://cce-dart.com/>\n\n**Capturing Tumor Heterogeneity in Hepatocellular Carcinoma - A Radiomics Approach Systematically Tested in Transgenic Mice** This project is partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – 410981386. Website: <https://gepris.dfg.de/gepris/projekt/410981386>\n\n**Data Science Driven Surgical Oncology Project**: This work was partially supported by the Data Science Driven Surgical Oncology Project (DSdSO), funded by the Surgical Oncology Program at the National Center for Tumor Diseases (NCT), Heidelberg, a partnership by DKFZ, UKHD, Heidelberg University. Website: <https://www.nct-heidelberg.de/forschung/precision-local-therapy-and-image-guidance/surgical-oncology.html>\n\n**Joint Imaging Platform**: This work was partially supported by Joint Imaging Platform, funded by the German Cancer Consortium. Website: <https://jip.dktk.dkfz.de/jiphomepage/>\n\n**HiGHmed**: This work was partially supported by the HiGHmed Consortium, funded by the German Federal Ministry of Education and Research (BMBF, funding code 01ZZ1802A). Website: <https://highmed.org/>\n\n**RACOON**: This work was partially supported by RACOON, funded by the German Federal Ministry of Education and ResearchDieses in the Netzwerk Universitätsmedizin (NUM; funding code 01KX2021). Website: <https://www.netzwerk-universitaetsmedizin.de/projekte/racoon>\n\n**Trustworthy Federated Data Analysis - TFDA**: This work is partially funded by the Helmholtz Association within the project \"Trustworthy Federated Data Analytics” (TFDA) (funding number\nZT-I-OO1 4). Website: <https://tfda.hmsp.center/>\n\nCopyright (C) 2024  German Cancer Research Center (DKFZ)\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kadi4mat",
            "repo_link": "https://gitlab.com/iam-cms/kadi",
            "content": {
                "codemeta": "",
                "readme": "# Kadi4Mat\n\n**Kadi4Mat**, or **Kadi** for short, is a generic and open source virtual\nresearch environment. Originally developed in the context of materials science,\nKadi4Mat can be used for the management of any type of research data within\ndifferent research disciplines and use cases. For more information about the\nproject, please see its [website](https://kadi.iam.kit.edu) and\n[documentation](https://kadi.readthedocs.io/en/stable).\n\n## Installation\n\nWhile the packaged code of Kadi4Mat can easily be installed as a Python package\nvia [pip](https://pypi.org/project/kadi), a complete installation requires a\nfew additional dependencies and considerations. Please refer to the [stable\ndocumentation](https://kadi.readthedocs.io/en/stable) for full installation\ninstructions.\n\n## Development\n\nContributions to the code are always welcome. However, please consider creating\nan issue first, as described below, if you are planning to make larger changes.\nPlease refer to the [latest\ndocumentation](https://kadi.readthedocs.io/en/latest) for instructions on how\nto set up a development environment of Kadi4Mat as well as other useful\ninformation, such as how to set up a separate fork of the [main\nrepository](https://gitlab.com/iam-cms/kadi).\n\nIn order to merge any contributions back into the main repository, please open\na corresponding [merge\nrequest](https://gitlab.com/iam-cms/kadi/-/merge_requests). Typically, the\nsource branch of the merge request would be a separate (feature) branch of your\nforked repository containing the changes to merge, while the target branch\nshould correspond to the `master` branch of the main repository. Depending on\nthe changes, please make sure to add appropriate tests, documentation,\ntranslations, etc. and also add a corresponding entry to the changelog in\n[`HISTORY.md`](https://gitlab.com/iam-cms/kadi/-/blob/master/HISTORY.md), if\nnecessary. Furthermore, you can add yourself as a contributor to\n[`AUTHORS.md`](https://gitlab.com/iam-cms/kadi/-/blob/master/AUTHORS.md).\n\n## Issues\n\nFor any issues regarding Kadi4Mat (bugs, suggestions, discussions, etc.) please\nuse the [issue tracker](https://gitlab.com/iam-cms/kadi/-/issues) of this\nproject. Make sure to add one or more fitting labels to each issue in order to\nkeep them organized. Before creating a new issue, please also check whether a\nsimilar issue is already open. Note that creating or interacting with issues\nrequires a GitLab account.\n\nFor **bugs** in particular, please use the provided [`Bug`\ntemplate](https://gitlab.com/iam-cms/kadi/-/issues/new?issuable_template=Bug)\nwhen creating a new issue, which also adds the `Bug` label to the issue\nautomatically. For **security-related** issues or concerns, please see\n[`SECURITY.md`](https://gitlab.com/iam-cms/kadi/-/blob/master/SECURITY.md).\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kagen-communication-free-massively-distributed-graph-generators",
            "repo_link": "https://github.com/KarlsruheGraphGeneration/KaGen",
            "content": {
                "codemeta": "",
                "readme": "# Communication-free Graph Generators (+ others)\n\nThis is the code to accompany our eponymous paper: *Funke, D., Lamm, S., Sanders, P., Schulz, C., Strash, D. and von Looz, M., 2017. Communication-free Massively Distributed Graph Generation. arXiv preprint arXiv:1710.07565.*\nYou can find a freely accessible online version [in the arXiv](https://arxiv.org/abs/1710.07565).\n\nIf you use this library in the context of an academic publication, we ask that you cite our paper:\n\n```bibtex\n@inproceedings{funke2017communication,\n  title={Communication-free Massively Distributed Graph Generation},\n  author={Funke, Daniel and Lamm, Sebastian and Sanders, Peter and Schulz, Christian and Strash, Darren and von Looz, Moritz},\n  booktitle={2018 {IEEE} International Parallel and Distributed Processing Symposium, {IPDPS} 2018, Vancouver, BC, Canada, May 21 -- May 25, 2018},\n  year={2018},\n}\n```\n\nAdditionally, if you use the Barabassi-Albert generator, we ask that you cite the [paper](https://arxiv.org/abs/1602.07106):\n\n```bibtex\n@article{sanders2016generators,\n  title={Scalable generation of scale-free graphs},\n  journal={Information Processing Letters},\n  volume={116},\n  number={7},\n  pages={489 -- 491},\n  year={2016},\n  author={Sanders, Peter and Schulz, Christian},\n}\n```\n\nIf you use the R-MAT generator, we ask that you cite the [paper](https://www.cambridge.org/core/journals/network-science/article/linear-work-generation-of-rmat-graphs/68A0DDA58A7B84E9B3ACA2DBB123A16C):\n\n```bibtex\n@article{HubSan2020RMAT, \n  title={Linear Work Generation of {R-MAT} Graphs}, \n  volume={8}, \n  number={4}, \n  journal={Network Science}, \n  publisher={Cambridge University Press}, \n  author={H{\\\"u}bschle-Schneider, Lorenz and Sanders, Peter}, \n  year={2020}, \n  pages={543 -- 550},\n}\n```\n\n## Introduction \n\nNetwork generators serve as a tool to alleviate the need for synthethic instances with controllable parameters by algorithm developers and researchers. \nHowever, many generators fail to provide instances on a massive scale due to their sequential nature or resource constraints.\n\nIn our work, we present novel generators for a variety of network models commonly found in practice.\nBy making use of pseudorandomization and divide-and-conquer schemes, our generators follow a communication-free paradigm.\nThe resulting generators are often embarrassingly parallel and have a near optimal scaling behavior.\nThis allows us to generate instances of up to $2^{43}$ vertices and $2^{47}$ edges in less than 22 minutes on 32,768 cores.\nTherefore, our generators allow new graph families to be used on an unprecedented scale.\n\n## Requirements \n\nIn order to compile the generators, you require: \n\n* A modern, C++17-ready compiler such as `g++` version 9 or higher or `clang` version 11 or higher. \n  * Note: Apple Clang is **not** supported. \n* OpenMPI\n* [Google Sparsehash](https://github.com/sparsehash/sparsehash)\n* CGAL (optional, only required for the Delaunay generators)\n\nYou can install these dependencies via your package manager:\n\n```shell\n# Ubuntu, Debian \napt-get install gcc-12 g++-12 libopenmpi-dev libcgal-dev libsparsehash-dev \n\n# Arch Linux, Manjaro\npacman -S gcc sparsehash openmpi cgal\n\n# Fedora \ndnf install gcc openmpi sparsehash-devel CGAL-devel\n\n# macOS using Homebrew \nbrew install gcc open-mpi google-sparsehash cgal\n\n# macOS using MacPorts \nport install gcc12 openmpi sparsehash cgal5\n```\n\n## Building KaGen \n\nTo compile the code either run `compile.sh` or use the following instructions:\n\n```shell\ngit submodule update --init --recursive\ncmake -B build -DCMAKE_BUILD_TYPE=Release\ncmake --build build --parallel\n```\n\n## Running KaGen \n\nAfter building KaGen, the standalone application is located at `build/app/KaGen`. \nA list of all command line options is available using the `./KaGen --help` option. \nTo view the options of a specific graph generator, use:\n\n```shell \n./KaGen <gnm-undirected|gnm-directed|gnp-undirected|gnp-directed|rgg2d|rgg3d|grid2d|grid3d|rdg2d|rdg3d|rhg|ba|kronecker|rmat> --help\n```\n\nBy default, the generated graph is written to a single file `out` (`-o` option) in DIMACS edge list format (`-f` option).\nOther output formats include:\n\n- `-f edgelist`: DIMACS edge list format (default)\n- `-f binary-edgelist`: DIMACS binary edge list format, use `--32` to write the file with 32 bit data types \n- `-f metis`: Metis graph format\n- `-f hmetis`: hMetis hypergraph format; **note:** KaGen still generates a graph, i.e., every hyperedge will contain two pins\n- `-f dot`: GraphViz dot file (add `-C` to include vertex coordinates for 2D graph generators)\n- `-f coordinates`: Text file containing vertex coordinates \n- `-f parhip`: Binary graph format used by [ParHIP](https://github.com/KaHIP/KaHIP)\n- `-f xtrapulp`: Binary graph format used by [XtraPuLP](https://github.com/HPCGraphAnalysis/PuLP), use `--32` to write the file with 32 bit data types\n\nExperimental output formats include:\n\n- `-f experimental/hmetis-ep`: hMetis hypergraph format, but the graph is transformed s.t. a partition of the hypergraph is an edge partition of the generated graph\n- `-f experimental/freight-netl`: hypergraph format used by FREIGHT; **note:** KaGen still generates a graph, i.e., every hyperedge will contain two pins\n- `-f experimental/freight-netl-ep`: hypergraph format used by FREIGHT, but the graph is transformed s.t. a partition of the hypergraph is an edge partition of the generated graph\n\nOne graph can be stored in multiple formats by passing the `-f` repeatedly, e.g., `-o out -f metis -f coordinates` will write two files `out.metis` and `out.xyz`.\nIf you want each PE to write its edges to a seperate file, use the `--distributed-output` flag.\n\n## Using the KaGen Library\n\nThe KaGen library is located at `build/library/libkagen.a` (use `-DBUILD_SHARED_LIBS=On` to build a shared library instead) and can be used in C++ and C projects.\nIf you are using CMake, you can use KaGen by adding this repository as a Git submodule to your project and including it in your CMake configuration:\n\n```cmake \nadd_subdirectory(external/KaGen)\ntarget_link_libraries(<your-target> PUBLIC KaGen::KaGen)\n```\n\nAlternatively, you can use `FetchContent`: \n\n```cmake \ninclude(FetchContent)\nFetchContent_Declare(KaGen \n  GIT_REPOSITORY https://github.com/sebalamm/KaGen.git \n  GIT_TAG master)\nFetchContent_MakeAvailable(KaGen)\nset_property(DIRECTORY \"${KaGen_SOURCE_DIR}\" PROPERTY EXCLUDE_FROM_ALL YES) # optional\n\ntarget_link_libraries(<your-target> PUBLIC KaGen::KaGen)\n```\n\nExamples on how to use the C and C++ interfaces are available in the `examples/` directory.\nThe examples given below only show the C++ interface.\n\n**Note**: Instead of calling the library functions listed below, you can also use `KaGen::GenerateFromOptionString()` \nto pass the generator options as a string (documentation is available in `kagen/kagen.h`).\n\nThe library functions return the generated graph as an instance of type `kagen::Graph`. \nBy default, the graph is represented as an edge list, i.e., a vector `kagen::Graph::edges[]` containing pairs of vertices.\nTo generate a graph in compressed sparse row (CSR) format, call `kagen::KaGen::UseCSRRepresentation()` before generating the graph. \nThen, access the graph via `kagen::Graph::xadj[]` and `kagen::Graph::adjncy[]`.\n\n## General Graph Format\n\nUnless noted otherwise, KaGen generates **simple**, **undirected** graphs, i.e.,\ngraphs without self-loops, without multi-edges and where for every edge (u, v),\nthere is also a reverse edge (v, u).\n\nWhen using KaGen in a distributed setting, each PE owns an equally sized range of consecutive vertices.\nAn edge is owned by the PE that owns its tail vertex.\nThus, an edge (u, v) is in the edge list of the PE that owns vertex u, while the reverse edge \n(v, u) is in the edge list of the PE owning vertex v.\n\n## Communication-free Graph Generators\n\n### Erdos-Renyi Graphs with Fixed Number of Edges\nGenerate a random Erdos-Renyi graph with a fixed number of edges.\nThe graph can either be directed or undirected and can contain self-loops.\n\n#### Application\n```\nmpirun -n <nproc> ./KaGen <gnm-directed|gnm-undirected> \n  -n <number of vertices>\n  [-N <number of vertices as a power of two>]\n  -m <number of edges>\n  [-M <number of edges as a power of two>]\n  [--self-loops]\n  [-k <number of chunks>]\n  [-s <seed>]\n```\n\n#### Library\n```c++\nKaGen gen(MPI_COMM_WORLD);\n\nGraph graph_directed = gen.GenerateDirectedGNM(n, m, self_loops = false);\nGraph graph_undirected = gen.GenerateUndirectedGNM(n, m, self_loops = false);\n```\n\n---\n\n### Erdos-Renyi graphs with Fixed Edge Probability\nGenerate a random Erdos-Renyi graph with a fixed edge probability.\nThe graph can either be directed or undirected and can contain self-loops.\n\n#### Application\n```\nmpirun -n <nproc> ./KaGen <gnp_directed|gnp_undirected> \n  -n <number of vertices>\n  [-N <number of vertices as a power of two>]\n  -p <edge probability>\n  [--self-loops]\n  [-k <number of chunks>]\n  [-s <seed>]\n```\n\n#### Library\n```c++\nKaGen gen(MPI_COMM_WORLD);\n\nGraph graph_directed = gen.GenerateDirectedGNP(n, p, self_loops = false);\nGraph graph_undirected = gen.GenerateUndirectedGNP(n, p, self_loops = false);\n```\n\n---\n\n### Random Geometric Graphs\nGenerate an undirected random geometric graph.\n\n**Note:** This generator is parameterized by the number of vertices in the graph and its edge radius. \nEither parameter can be omitted in favor of the desired number of edges, in which case the omitted \nparameter is approximated such that the expected number of edges matches the desired number of edges.\n\n#### Application\n```\nmpirun -n <nproc> ./KaGen <rgg2d|rgg3d> \n  -n <number of vertices>\n  [-N <number of vertices as a power of two>]\n  -r <edge radius>\n  -m <number of edges>                     # only if -n or -r are omitted\n  [-M <number of edges as a power of two>] # only if -n or -r are omitted\n  [-k <number of chunks>] \n  [-s <seed>]\n```\n\n#### Library\n```c++\nKaGen gen(MPI_COMM_WORLD);\n\nGraph graph = gen.GenerateRGG2D(n, r, coordinates = false);\nGraph graph = gen.GenerateRGG2D_NM(n, m, coordinates = false); // deduce r s.t. E[# edges] = m\nGraph graph = gen.GenerateRGG2D_MR(m, r, coordinates = false); // deduce n s.t. E[# edges] = m\n\nGraph graph = gen.GenerateRGG3D(n, r, coordinates = false);\nGraph graph = gen.GenerateRGG3D_NM(n, m, coordinates = false); // deduce r s.t. E[# edges] = m\nGraph graph = gen.GenerateRGG3D_MR(m, r, coordinates = false); // deduce n s.t. E[# edges] = m\n```\n\n--- \n\n### Random Delaunay Graphs\nGenerate an undirected random delaunay graph.\n\n**Note:** The graph can be generated with periodic boundary conditions to avoid long edges at the border using the `-p` flag. \nHowever, this can yield unexpected results when using less than 9 PEs (2D) / 27 PEs (3D) to generate the graph.\n\n#### Application\n```\nmpirun -n <nproc> ./KaGen <rdg2d|rdg3d>\n  -n <number of vertices>\n  [-N <number of vertices as a power of two>]\n  [--periodic]\n  [-s <seed>]\n```\n\n#### Library\n```c++\nKaGen gen(MPI_COMM_WORLD);\n\nGraph graph = gen.GenerateRDG2D(n, periodic, coordinates = false);\nGraph graph = gen.GenerateRDG2D_M(m, periodic, coordinates = false);\n\nGraph graph = gen.GenerateRDG3D(n, coordinates = false);\nGraph graph = gen.GenerateRDG3D_M(m, coordinates = false);\n```\n\n---\n\n### Random Grid Graphs\nGenerate an undirected random grid graph. \n\n#### Application \n```\nmpirun -n <nproc> ./KaGen <grid2d|grid3d>\n  -x <width of grid>\n  [-X <width of grid as a power of two>]\n  -y <height of grid>\n  [-Y <height of grid as a power of two>]\n  -z <depth of grid (grid3d only)>\n  [-Z <depth of grid as a power of two (grid3d only)>]\n  -p <edge probability>\n  -m <number of edges>                     # only if -p is omitted\n  [-M <number of edges as a power of two>] # only if -p is omitted\n  [--periodic]\n  [-k <number of cunks>]\n  [-s <seed>]\n```\n\n#### Library \n```c++\nKaGen gen(MPI_COMM_WORLD);\n\nGraph graph = gen.GenerateGrid2D(x, y, p, periodic, coordinates = false);\nGraph graph = gen.GenerateGrid2D_N(n, p, periodic, coordinates = false); // x, y = sqrt(n)\nGraph graph = gen.GenerateGrid2D_NM(n, m, periodic, coordinates = false); // x, y = sqrt(n)\n\nGraph graph = gen.GenerateGrid3D(x, y, z, p, periodic, coordinates = false);\nGraph graph = gen.GenerateGrid3D_N(n, p, periodic, coordinates = false); // x, y, z = cbrt(n) \nGraph graph = gen.GenerateGrid3D_NM(n, m, periodic, coordinates = false); // x, y, z = cbrt(n) \n```\n\n--- \n\n### Random Hyperbolic Graphs \nGenerate a two dimensional undirected random hyperbolic graph.\n\n**Note:** On x86 systems, the generator can use 64 bit or 80 bit floating point numbers.\nThis can be controlled explicitly by using the `--hp-floats` or `--no-hp-floats` flags. \nIf neither flag is set, KaGen switches to 80 bit precision automatically if the generated graph has more than 2^29 vertices.\n\n**Note:** Due to floating point inaccuracies, this generator performs communication in a post-processing step.\n\n#### Application\n```\nmpirun -n <nproc> ./KaGen rhg\n  -n <number of vertices>\n  [-N <number of vertices as a power of two>]\n  -g <power-law exponent>\n  -d <average vertex degree>\n  [-k <number of chunks>]\n  [--hp-floats]\n  [--no-hp-floats]\n  [-s <seed>]\n```\n\n#### Library\n```c++\nKaGen gen(MPI_COMM_WORLD);\n\nGraph graph = gen.GenerateRHG(gamma, n, d, coordinates = false);\nGraph graph = gen.GenerateRHG_NM(gamma, n, m, coordinates = false); // deduce d s.t. E[# edges] = m\nGraph graph = gen.GenerateRHG_MD(gamma, m, d, coordinates = false); // deduce n s.t. E[# edges] = m\n```\n\n## Non-communication-free Graph Generators \n\nSince the original publication, several other graph generators have been integrated into the KaGen framework. \n\n### Barabassi-Albert Graphs \n\nGenerate a random Barabassi-Albert graph.\nThe graph may contain self-loops and multi edges.\n\n#### Application\n```\nmpirun -n <nproc> ./KaGen ba \n  -n <number of vertices>\n  [-N <number of vertices as a power of two>]\n  -d <minimum degree for each vertex>\n  [--directed]\n  [--self-loops]\n  [-k <number of chunks>]\n  [-s <seed>]\n```\n\n#### Library\n```c++\nKaGen gen(MPI_COMM_WORLD);\n\nGraph graph = gen.GenerateBA(n, d, directed = false, self_loops = false);\nGraph graph = gen.GenerateBA_NM(n, m, directed = false, self_loops = false);\nGraph graph = gen.GenerateBA_MD(m, d, directed = false, self_loops = false);\n```\n\n---\n\n### R-MAT Graphs\nGenerate a random RMAT graph.\n\nEach PE generates a random R-MAT graph with n vertices and m/\\<nproc\\> edges.\nAfterwards, the vertices are assigned to PEs round-robin style and edges are distributed accordingly.\n\n#### Application\n```\nmpirun -n <nproc> ./KaGen rmat \n  -n <number of vertices> # should be a power of two\n  [-N <number of vertices as a power of two>]\n  -m <number of edges>\n  [-M <number of edges as a power of two>]\n  -a <probability for an edge to land in block a>\n  -b <probability for an edge to land in block b>\n  -c <probability for an edge to land in block c>\n  [--directed]\n  [--self-loops]\n  [-s <seed>]\n```\n\n#### Library \n```c++\nKaGen gen(MPI_COMM_WORLD);\n\nGraph graph = gen.GenerateRMAT(n, m, a, b, c, directed = false, self_loops = false);\n```\n\n---\n\n### Kronecker Graphs \nGenerate a random Kronecker graph.\n\nEach PE generates a random Kronecker graph with n vertices and m/\\<nproc\\> edges.\nAfterwards, the vertices are assigned to PEs round-robin style and edges are distributed accordingly.\n\n#### Application \n```\nmpirun -n <nproc> ./KaGen kronecker \n  -n <number of vertices> # should be a power of two \n  [-N <number of vertices as a power of  two>]\n  -m <number of edges> \n  [-M <number of edges as a power of two>]\n  [--directed]\n  [--self-loops]\n  [-s <seed>]\n```\n\n#### Library \n\n```c++\nKaGen gen(MPI_COMM_WORLD);\n\nGraph graph = gen.GenerateKronecker(n, m, directed = false, self_loops = false);\n```\n\n## Static Graph Generators\n\n### Image Graph Generator\nGenerates a graph based on an input image.\nEach pixel is represented by a vertex with edges to its neighboring vertices.\nThe image has to be converted to KARGB format first (a simple binary file containing the uncompressed R, G, B channels of the image) by \nusing the `img2kargb` or `upsb2kargb` tool shipped with KaGen.\n\n#### Application\n```\nmpirun -n <nproc> ./KaGen image\n  --filename=<path to kargb file>\n  [--weight-model=<l2, inv-l2, inv-ratio>]\n  [--weight-multiplier=1]\n  [--weight-offset=0]\n  [--min-weight-threshold=1]\n  [--max-weight-threshold=inf]\n  [--neighborhood=<4, 8, 24>]\n  [--max-grid-x=<...>]\n  [--max-grid-y=<...>]\n  [--grid-x=<...>]\n  [--grid-y=<...>]\n  [--cols-per-pe=<...>]\n  [--rows-per-pe=<...>]\n```\n\n#### Library \n\n```c++\nKaGen gen(MPI_COMM_WORLD);\n\nGraph graph = gen.GenerateFromOptionString(\"image;filename=<...>;...\");\n```\n\n--- \n\n### File Graph Generator\nPseudo-generator that loads a static graph from disk.\nCan be used to convert input formats to output format, or to load static graphs when using KaGen as a library.\n\n#### Application \n```\nmpirun -n <nproc> ./KaGen file\n  --filename=<path to graph>\n  --input-format=<metis|parhip>\n  [--distribution=<balance-vertices|balance-edges>]\n```\n\n#### Library \n\n```c++\nKaGen gen(MPI_COMM_WORLD);\n\nGraph graph = gen.GenerateFromOptionString(\"file;filename=<...>;input_format=<...>;distribution=<...>\");\n```\n\n## Tools\n\nTools can be installed via `cmake --install build --component tools`. The following tools are included: \n\n```shell\n# graphstats: compute some basic statistics for the given graphs\nmpirun ./app/tools/graphstats <path to graph(s), ...>\n  [-f <format, e.g., metis, parhip, plain-edgelist>]\n\n# chkgraph: validate a graph file in any supported input format\nmpirun -n <nproc> ./app/tools/chkgraph <path to graph>\n  [-f <format, e.g., metis, parhip, plain-edgelist>] \n  [--64bits]                  # allow 64 bit weights and IDs\n  [--self-loops]              # allow self loops\n  [--directed]                # allow directed graphs (i.e., not all reverse edges are present)\n  [--multi-edges]             # allow multi edges\n  [--negative-edge-weights]   # allow negative edge weights\n  [--negative-vertex-weights] # allow negative vertex weights\n  \n# pangraph: convert a graph file between supported formats in external memory\n./app/tools/pangraph --input-format=<...> --input-filename=<...> --output-format=<...> --output-filename=<...>\n  [-C <num chunks = 1>]       # split the graph into <num chunks> chunks; only one chunk has to fit into internal memory at a time\n  [-T <tmp directory = /tmp>] # directory to be used for temporary files (requires free space roughly the size of the input graph)\n  [--remove-self-loops]       # remove any self-loops during convertion\n  [--add-reverse-edges]       # make all edges undirected by adding potentially missing reverse edges\n  [--sort-edges]              # sort the outgoing edges by destination vertex ID\n  [-n <num vertices>]         # provide the number of vertices in the graph -- currently only used for the plain-edgelist input format\n```\n\n---\n\n**[License](/LICENSE):** 2-clause BS\n\n",
                "dependencies": "################################################################################\n# CMakeLists.txt\n#\n# Root CMake build script for generator\n#\n# Copyright (C) 2016-2017 Sebastian Lamm <lamm@ira.uka.de>\n#\n# All rights reserved. Published under the BSD-2 license in the LICENSE file.\n################################################################################\n\ncmake_minimum_required(VERSION 3.16)\nproject(kagen LANGUAGES C CXX)\nset(CMAKE_CXX_STANDARD 20)\nlist(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\")\n\n################################################################################\n\noption(KAGEN_NODEPS \"Build KaGen without any dependencies.\" OFF)\n\noption(KAGEN_USE_MARCH_NATIVE \"Compile with -march=native.\" OFF)\noption(KAGEN_USE_CGAL \"If available, link against CGal to enable RDG generators.\" ON)\noption(KAGEN_USE_SPARSEHASH \"Build with Google Sparsehash. If turned off, fall back to std::unordered_map<>.\" ON)\noption(KAGEN_USE_FAST_MATH \"Use -ffast-math.\" OFF)\noption(KAGEN_USE_MKL \"Build with Intel MKL random generator.\" OFF)\noption(KAGEN_USE_XXHASH \"Build with xxHash. If turned off, path permutation will not be available.\" ON)\n\noption(KAGEN_WARNINGS_ARE_ERRORS \"Make compiler warnings compiler errors.\" OFF)\n\noption(KAGEN_BUILD_TESTS \"Build unit tests.\" OFF)\noption(KAGEN_BUILD_APPS \"Build binaries.\" ON)\noption(KAGEN_BUILD_EXAMPLES \"Build examples.\" ON)\n\n################################################################################\n\ninclude(FetchContent) \n\nFetchContent_Declare(googletest\n    SYSTEM \n    GIT_REPOSITORY https://github.com/google/googletest.git \n    GIT_TAG release-1.12.1)\n\n################################################################################\n\n# If KAGEN_NODEPS is set, disable all dependency flags\nif (KAGEN_NODEPS)\n    message(STATUS \"Building without any dependencies.\")\n\n    set(KAGEN_USE_CGAL OFF)\n    set(KAGEN_USE_SPARSEHASH OFF)\n    set(KAGEN_USE_MKL OFF)\n    set(KAGEN_USE_XXHASH OFF)\n    set(KAGEN_BUILD_TESTS OFF) # requires GoogleTest\nendif ()\n\n# Prohibit in-source builds\nif (\"${PROJECT_SOURCE_DIR}\" STREQUAL \"${PROJECT_BINARY_DIR}\")\n    message(SEND_ERROR \"In-source builds are not allowed.\")\nendif ()\n\n# Default to Release building for single-config generators\nif (NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n    message(STATUS \"Defaulting CMAKE_BUILD_TYPE to Release\")\n    set(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"Build type\")\nendif ()\n\n# Warning flags\nlist(APPEND KAGEN_WARNING_FLAGS\n    \"-W\"\n    \"-Wall\"\n    \"-Wextra\"\n    \"-Wpedantic\"\n    \"-Wno-unused-local-typedefs\"\n    )\n\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n    list(APPEND KAGEN_WARNING_FLAGS\n        \"-Wextra-semi\"\n        \"-fcolor-diagnostics\"\n        \"-Wdeprecated\"\n        )\nendif ()\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n    list(APPEND KAGEN_WARNING_FLAGS\n        \"-Wsuggest-override\"\n        \"-fdiagnostics-color=always\"\n        \"-Wcast-qual\"\n        \"-Winit-self\"\n        \"-Woverloaded-virtual\"\n        \"-Wredundant-decls\"\n        )\nendif ()\n\nif (KAGEN_WARNINGS_ARE_ERRORS)\n    list(APPEND KAGEN_WARNING_FLAGS \"-Werror\")\nendif ()\n\n# Enable -march=native on Debug and Release builds\nif (KAGEN_USE_MARCH_NATIVE)\n    include(CheckCXXCompilerFlag)\n    check_cxx_compiler_flag(\"-march=native\" KAGEN_HAS_MARCH_NATIVE)\n    if(KAGEN_HAS_MARCH_NATIVE)\n        list(APPEND CMAKE_CXX_FLAGS \"-march=native\")\n    endif ()\nendif ()\n\n# Remove -rdynamic from linker flags (smaller binaries which cannot be loaded\n# with dlopen() -- something no one needs)\nstring(REGEX REPLACE \"-rdynamic\" \"\" CMAKE_SHARED_LIBRARY_LINK_C_FLAGS \"${CMAKE_SHARED_LIBRARY_LINK_C_FLAGS}\")\nstring(REGEX REPLACE \"-rdynamic\" \"\" CMAKE_SHARED_LIBRARY_LINK_CXX_FLAGS \"${CMAKE_SHARED_LIBRARY_LINK_CXX_FLAGS}\")\n\n# Enable UndefinedBehaviorSanitizer\nif (OFF)\n    list(APPEND CMAKE_CXX_FLAGS \"-fsanitize=undefined\")\nendif ()\n\n# Use -ffast-math\nif (KAGEN_USE_FAST_MATH)\n    list(APPEND CMAKE_CXX_FLAGS \"-ffast-math\")\nendif ()\n\n###############################################################################\n# libmorton\n###############################################################################\nadd_library(morton INTERFACE)\ntarget_include_directories(morton SYSTEM INTERFACE extlib/libmorton/include)\nlist(APPEND KAGEN_LINK_LIBRARIES morton)\n\n###############################################################################\n# xxHash \n###############################################################################\nif (KAGEN_USE_XXHASH)\n    message(STATUS \"Building with xxHash\")\n\n    set(${PROJECT_NAME}_XXHASH_DIR \"${CMAKE_CURRENT_LIST_DIR}/extlib/xxHash\")\n    set(BUILD_SHARED_LIBS OFF)\n    set(XXHASH_BUILD_ENABLE_INLINE_API ON)\n    set(XXHASH_BUILD_XXHSUM OFF)\n    add_subdirectory(\"${${PROJECT_NAME}_XXHASH_DIR}/cmake_unofficial\" EXCLUDE_FROM_ALL)\n    list(APPEND KAGEN_LINK_LIBRARIES xxHash::xxhash)\n    add_definitions(-DKAGEN_XXHASH_FOUND)\nendif ()\n\n###############################################################################\n# MPI\n###############################################################################\nset(MPI_DETERMINE_LIBRARY_VERSION TRUE) # needed for KaTestrophe\nfind_package(MPI REQUIRED)\nlist(APPEND KAGEN_LINK_LIBRARIES MPI::MPI_CXX)\n\n###############################################################################\n# pthread\n###############################################################################\nfind_package(Threads REQUIRED)\nlist(APPEND KAGEN_LINK_LIBRARIES Threads::Threads)\n\n###############################################################################\n# Google Sparsehash\n###############################################################################\nif (KAGEN_USE_SPARSEHASH)\n    message(STATUS \"Building with Sparsehash\")\n    find_package(Sparsehash REQUIRED)\n    list(APPEND KAGEN_LINK_LIBRARIES Sparsehash::Sparsehash)\n    add_definitions(-DKAGEN_SPARSEHASH_FOUND)\nendif ()\n\n###############################################################################\n# CGAL\n###############################################################################\nif (KAGEN_USE_CGAL)\n    set(CGAL_DO_NOT_WARN_ABOUT_CMAKE_BUILD_TYPE TRUE CACHE BOOL \"Do not warn about Debug mode\")\n    set(CGAL_DONT_OVERRIDE_CMAKE_FLAGS TRUE CACHE BOOL \"Force CGAL to maintain CMAKE flags\")\n    find_package(CGAL QUIET)\n    if (CGAL_FOUND)\n        add_definitions(-DKAGEN_CGAL_FOUND) \n        include(${CGAL_USE_FILE})\n    else ()\n        message(STATUS \"Could not find the CGAL library: Random Delaunay Graphs will not be available\")\n    endif ()\nendif ()\n\n###############################################################################\n# Sampling library -> MKL\n###############################################################################\nif (KAGEN_USE_MKL)\n    find_package(MKL)\n    if (MKL_FOUND)\n        message(STATUS \"Building with MKL\")\n        list(APPEND KAGEN_INCLUDE_DIRS ${MKL_INCLUDE_DIR})\n        list(APPEND KAGEN_LINK_LIBRARIES ${MKL_LP_LIBRARY} ${MKL_CORE_LIBRARY} ${MKL_SEQUENTIAL_LIBRARY})\n        add_definitions(-DSAMPLING_HAVE_MKL)\n        add_definitions(-DRMAT_HAVE_MKL)\n        add_definitions(-DKAGEN_MKL_FOUND)\n    else ()\n        message(STATUS \"MKL requested but not found, building without MKL\")\n    endif ()\nendif ()\n\n################################################################################\n\nadd_subdirectory(kagen)\n\nif (KAGEN_BUILD_APPS)\n    add_subdirectory(app)\nelse ()\n    message(STATUS \"Apps disabled.\")\nendif ()\n\nif (KAGEN_BUILD_EXAMPLES) \n    add_subdirectory(examples)\nelse ()\n    message(STATUS \"Examples disabled.\")\nendif ()\n\n################################################################################\n\nadd_library(KaGen::KaGen ALIAS kagen)\nadd_library(KaGen::cKaGen ALIAS kagen) # @deprecated, use KaGen::KaGen\n\n################################################################################\n\nif (KAGEN_BUILD_TESTS)\n    enable_testing()\n    add_subdirectory(tests)\nendif ()\n\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kahypar",
            "repo_link": "https://github.com/kahypar/",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kaminpar",
            "repo_link": "https://github.com/KaHIP/KaMinPar",
            "content": {
                "codemeta": "",
                "readme": "# KaMinPar\n\nKaMinPar is a shared-memory parallel tool to heuristically solve the graph partitioning problem: divide a graph into k disjoint blocks of roughly equal weight while\nminimizing the number of edges between blocks.\nCompeting algorithms are mostly evaluated for small values of k. If k is large, they often compute highly imbalance solutions, solutions of low quality or suffer excessive running time.\nKaMinPar substantially mitigates these problems.\nIt computes partitions of comparable quality to other high-quality graph partitioning tools while guaranteeing the balance constraint for unweighted input graphs.\nMoreover, for large values of k, it is an order of magnitude faster than competing algorithms.\n\n## Installation Notes\n\n### Requirements\n\n* **Compiler:** C++20-ready GCC or Clang compiler\n* **Dependencies:** CMake, Intel TBB, MPI (optional)\n* **System:** Linux (x86, ARM) or macOS (ARM)\n\n### Quickstart\n\nAfter cloning the repository, make sure to initialize the submodules:\n\n```shell\ngit submodule update --init --recursive\n```\n\nThen, follow the standard CMake build procedure:\n\n```shell\ncmake -B build --preset=default\ncmake --build build --parallel\n```\n\nTo partition a graph in METIS format (see, e.g., the [KaHIP manual](https://github.com/KaHIP/KaHIP/raw/master/manual/kahip.pdf)), run:\n\n```shell\n# KaMinPar: shared-memory partitioning\n./build/apps/KaMinPar [-P default|terapart|strong|largek] -G <graph filename> -k <number of blocks> -t <nproc> [-o <output partition>]\n\n# dKaMinPar: distributed partitioning\nmpirun -n <nproc> ./build/apps/dKaMinPar [-P default|strong|xterapart] -G <graph filename> -k <number of blocks> [-o <output partition>]\n```\n\nThe computed partition is written to a text file, where the n-th line contains the block ID (0-based) of the n-th vertex.\n\nThere are multiple configuration presets that tune the algorithm for different scenarios:\n\n* `-P default`: fast partitioning with quality comparable to Metis\n* `-P terapart`: same partition quality as `default`, but with reduced memory consumption (slightly slower)\n* `-P strong`: better quality than `default` at the cost of increased runtime\n* `-P largek`: faster for large values of k (e.g., k > 1024); may reduce partition quality for smaller k\n\nConfiguration presets can be inspected using the `--dump-config` flag.\nTo build a custom configuration, dump one of the presets to a file, modify it and load it using `-C <filename>`:\n\n```shell\n./build/KaMinPar -P terapart --dump-config > custom.ini\n# ... modify custom.ini ...\n./build/KaMinPar -C custom.ini <...>\n```\n\n## Using the Library Interface\n\nIf you are using CMake, you can use the partitioners as libraries by adding this repository as a Git submodule to your project and including it in your CMake configuration:\n\n```cmake\nadd_subdirectory(external/KaMinPar)\n\ntarget_link_libraries(<your-target> PUBLIC KaMinPar::KaMinPar)  # Shared-memory partitioning\ntarget_link_libraries(<your-target> PUBLIC KaMinPar::dKaMinPar) # Distributed partitioning\n```\n\nAlternatively, you can use `FetchContent`:\n\n```cmake\ninclude(FetchContent)\nFetchContent_Declare(KaMinPar\n  GIT_REPOSITORY https://github.com/KaHIP/KaMinPar.git\n  GIT_TAG main)\nFetchContent_MakeAvailable(KaMinPar)\nset_property(DIRECTORY \"${KaMinPar_SOURCE_DIR}\" PROPERTY EXCLUDE_FROM_ALL YES) # optional\n\ntarget_link_libraries(<your-target> PUBLIC KaMinPar::KaMinPar)  # Shared-memory partitioning\ntarget_link_libraries(<your-target> PUBLIC KaMinPar::dKaMinPar) # Distributed partitioning\n```\n\nThen, call the libraries as follows:\n\n```c++\n#include <kaminpar-shm/kaminpar.h>\n#include <kaminpar-dist/dkaminpar.h>\n\nusing namespace kaminpar;\n\n// Call the shared-memory partitioner:\nKaMinPar shm(int num_threads, shm::create_default_context());\n// KaMinPar::reseed(int seed);\nshm.borrow_and_mutate_graph(NodeID n, EdgeID *xadj, NodeID *adjncy, NodeWeight *vwgt = nullptr, EdgeWeight *adjwgt = nullptr);\n// alternatively: shm.copy_graph(n, xadj, adjncy, vwgt, adjwgt); will work on a copy of the graph\nshm.compute_partition(BlockID number_of_blocks, double epsilon, std::span<BlockID> out_partition);\n// alternatively: shm.compute_partition(std::vector<BlockWeight> max_block_weights, std::span<BlockID> out_partition);\n// Note: you must ensure that the total max block weight is larger than the total node weight of the graph\n\n// Call the distributed partitioner:\ndKaMinPar dist(MPI_Comm comm, int num_threads, dist::create_default_context());\n// dKaMinPar::reseed(int seed); \ndist.import_graph(GlobalNodeID *vtxdist, GlobalEdgeID *xadj, GlobalNodeID *adjncy, GlobalNodeWeight *vwvgt = nullptr, GlobalEdgeWeight *adjwgt = nullptr);\ndist.compute_partition(BlockID number_of_blocks, BlockID *out_partition);\n```\n\nMore examples can be found in the `examples/` directory. \n\n## Licensing\n\nKaMinPar is free software provided under the MIT license.\nIf you use KaMinPar in an academic setting, please cite the appropriate publication(s) listed below.\n\n```\n// KaMinPar\n@InProceedings{DeepMultilevelGraphPartitioning,\n  author    = {Lars Gottesb{\\\"{u}}ren and\n               Tobias Heuer and\n               Peter Sanders and\n               Christian Schulz and\n               Daniel Seemaier},\n  title     = {Deep Multilevel Graph Partitioning},\n  booktitle = {29th Annual European Symposium on Algorithms, {ESA} 2021},\n  series    = {LIPIcs},\n  volume    = {204},\n  pages     = {48:1--48:17},\n  publisher = {Schloss Dagstuhl - Leibniz-Zentrum f{\\\"{u}}r Informatik},\n  year      = {2021},\n  url       = {https://doi.org/10.4230/LIPIcs.ESA.2021.48},\n  doi       = {10.4230/LIPIcs.ESA.2021.48}\n}\n\n// dKaMinPar (distributed KaMinPar)\n@InProceedings{DistributedDeepMultilevelGraphPartitioning,\n  author    = {Sanders, Peter and Seemaier, Daniel},\n  title     = {Distributed Deep Multilevel Graph Partitioning},\n  booktitle = {Euro-Par 2023: Parallel Processing},\n  year      = {2023},\n  publisher = {Springer Nature Switzerland},\n  pages     = {443--457},\n  isbn      = {978-3-031-39698-4}\n}\n\n// [x]TeraPart (memory-efficient [d]KaMinPar)\n@misc{TeraPart,\n      title={Tera-Scale Multilevel Graph Partitioning}, \n      author={Daniel Salwasser and Daniel Seemaier and Lars Gottesbüren and Peter Sanders},\n      year={2024},\n      eprint={2410.19119},\n      archivePrefix={arXiv},\n      primaryClass={cs.DS},\n      url={https://arxiv.org/abs/2410.19119}, \n}\n```\n\n\n",
                "dependencies": "cmake_minimum_required(VERSION 3.21)\nlist(APPEND CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake/modules)\n\ninclude(FetchContent)\ninclude(CheckCXXCompilerFlag)\n\nproject(KaMinPar\n        DESCRIPTION \"Shared-memory and distributed-memory Graph Partitioner\"\n        LANGUAGES C CXX)\n\nset(PROJECT_VENDOR \"Daniel Seemaier\")\nset(PROJECT_CONTACT \"daniel.seemaier@kit.edu\")\nset(CMAKE_CXX_STANDARD 20)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\n################################################################################\n## Options                                                                    ##\n################################################################################\n\n# Control what to build\n#######################\noption(KAMINPAR_BUILD_TESTS \"Build unit tests\" ON)\noption(KAMINPAR_BUILD_DISTRIBUTED \"Build distributed partitioner.\" OFF)\noption(KAMINPAR_BUILD_APPS \"Build binaries.\" ON)\noption(KAMINPAR_BUILD_BENCHMARKS \"Build benchmark binaries.\" OFF)\noption(KAMINPAR_BUILD_TOOLS \"Build tool binaries.\" OFF)\noption(KAMINPAR_BUILD_EXAMPLES \"Build examples.\" OFF)\n\noption(KAMINPAR_BUILD_EXPERIMENTAL_FEATURES \"Include experimental features in the build. This might increase compile times drastically.\" OFF)\n\n# Control how to build\n######################\noption(KAMINPAR_ENABLE_HEAP_PROFILING \"Profile and output heap memory usage.\" OFF)\noption(KAMINPAR_ENABLE_PAGE_PROFILING \"Profile pages allocated via mmap.\" OFF)\noption(KAMINPAR_ENABLE_STATISTICS \"Generate and output detailed statistics.\" OFF)\noption(KAMINPAR_ENABLE_TIMERS \"Measure running times. Must be set to 'OFF' if the library interface is used from multiple threads simulatinously.\" ON)\noption(KAMINPAR_ENABLE_TIMER_BARRIERS \"Add additional MPI_Barrier() instructions for more accurate time measurements.\" ON)\n\noption(KAMINPAR_ENABLE_THP \"Use transparent huge pages for large memory allocations (Linux only).\" ON)\n\noption(KAMINPAR_BUILD_WITH_ASAN \"Enable address sanitizer.\" OFF)\noption(KAMINPAR_BUILD_WITH_UBSAN \"Enable undefined behaviour sanitizer.\" OFF)\noption(KAMINPAR_BUILD_WITH_MTUNE_NATIVE \"Build with -mtune=native.\" ON)\noption(KAMINPAR_BUILD_WITH_CCACHE \"Use ccache to build.\" ON)\noption(KAMINPAR_BUILD_WITH_DEBUG_SYMBOLS \"Always build with debug symbols, even in Release mode.\" ON)\noption(KAMINPAR_BUILD_WITH_MTKAHYPAR \"If Mt-KaHyPar can be found, build the Mt-KaHyPar initial partitioner.\" OFF)\noption(KAMINPAR_BUILD_WITH_GROWT \"Build the shared-memory partitioner with Growt.\" ON)\noption(KAMINPAR_BUILD_WITH_SPARSEHASH \"Build with Google Sparsehash.\" ON)\noption(KAMINPAR_BUILD_WITH_PG \"Build with the -pg option for profiling.\" OFF)\noption(KAMINPAR_BUILD_WITH_BACKWARD \"Build with backward-cpp for stack traces (distributed partitioner only).\" OFF)\n\n# Control data type sizes\n#########################\n# These IDs refer to the shared-memory partitioner + local IDs of the distributed partitioner\noption(KAMINPAR_64BIT_IDS \"Use 64 bits for node and edge IDs.\" OFF)\noption(KAMINPAR_64BIT_EDGE_IDS \"Use 64 bits for edge IDs.\" OFF)\noption(KAMINPAR_64BIT_NODE_IDS \"Use 64 bits for node IDs.\" OFF)\n\n# Node and edge weights for the shared-memory partitioner (+ used as initial partitioner of the distributed partitioner)\noption(KAMINPAR_64BIT_WEIGHTS \"Use 64 bit for node and edge weights.\" OFF)\n\n# Local node and edge weights for the distributed partitioner; should be 64 bit when using DMGP\noption(KAMINPAR_64BIT_LOCAL_WEIGHTS \"Use 64 bit for local node and edge weights.\" OFF)\n\n# The distributed partitioner requires 64 bit node and edge weights for the coarsest graph, \n# which is copied to each PE and build with data types of the shared-memory partitioner.\n# Thus, force 64 bit weights for the shared-memory partitioner in this case.\nif (KAMINPAR_BUILD_DISTRIBUTED)\n    message(STATUS \"Distributed build: enabling 64 bit weights.\")\n    set(KAMINPAR_64BIT_WEIGHTS ON)\nendif ()\n\n# Control graph compression options\n###################################\noption(KAMINPAR_COMPRESSION_HIGH_DEGREE_ENCODING \"Use high-degree encoding for the compressed graph.\" ON)\noption(KAMINPAR_COMPRESSION_INTERVAL_ENCODING \"Use interval encoding for the compressed graph.\" ON)\noption(KAMINPAR_COMPRESSION_STREAMVBYTE_ENCODING \"Use StreamVByte encoding for the compressed graph.\" OFF)\noption(KAMINPAR_COMPRESSION_FAST_DECODING \"Use a fast PEXT-based decoding routine for the compressed graph.\" OFF)\n\nif (KAMINPAR_64BIT_NODE_IDS AND KAMINPAR_COMPRESSION_STREAM_ENCODING)\n    message(FATAL_ERROR \"StreamVByte encoding cannot be used with 64-bit NodeIDs.\")\nendif ()\n\n################################################################################\n## Declare dependencies                                                       ##\n################################################################################\n\nset(KAMINPAR_ASSERTION_LEVEL \"light\" CACHE STRING \"Assertion level.\")\nset_property(CACHE KAMINPAR_ASSERTION_LEVEL PROPERTY STRINGS none light normal heavy)\nmessage(STATUS \"KAssertion level: ${KAMINPAR_ASSERTION_LEVEL}\")\n\n# Export compile commands\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\n# Set warning flags\nlist(APPEND KAMINPAR_WARNING_FLAGS\n    \"-W\"\n    \"-Wall\"\n    \"-Wextra\"\n    \"-Wpedantic\"\n    \"-Wno-unused-local-typedefs\"\n    )\n\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n    list(APPEND KAMINPAR_WARNING_FLAGS\n        \"-Wextra-semi\"\n        \"-fcolor-diagnostics\"\n        \"-Wdeprecated\"\n        )\nendif ()\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n    list(APPEND KAMINPAR_WARNING_FLAGS\n        \"-Wsuggest-override\"\n        \"-fdiagnostics-color=always\"\n        \"-Wcast-qual\"\n        \"-Winit-self\"\n        \"-Woverloaded-virtual\"\n        \"-Wredundant-decls\"\n        )\nendif ()\n\n# Build experimental features that increase compile times\nif (KAMINPAR_BUILD_EXPERIMENTAL_FEATURES)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_EXPERIMENTAL\")\nendif ()\n\n# Always enable Debug symbols (including in Release mode)\nif (KAMINPAR_BUILD_WITH_DEBUG_SYMBOLS)\n    add_compile_options(-g)\nendif ()\n\n# Set compile flags\nset(CMAKE_REQUIRED_FLAGS -Werror) # otherwise the check fails for Apple Clang \ncheck_cxx_compiler_flag(-msse4.1 COMPILER_SUPPORTS_MSSE41)\nif (COMPILER_SUPPORTS_MSSE41)\n    add_compile_options(-msse4.1)\nendif ()\ncheck_cxx_compiler_flag(-mcx16 COMPILER_SUPPORTS_MCX16)\nif (COMPILER_SUPPORTS_MCX16)\n    add_compile_options(-mcx16)\nendif ()\n\nif (KAMINPAR_BUILD_WITH_MTUNE_NATIVE) \n    add_compile_options(-mtune=native -march=native)\nendif ()\n\nif (KAMINPAR_BUILD_WITH_ASAN) \n    add_compile_options(-fsanitize=address)\n    add_link_options(-fsanitize=address)\nendif ()\n\nif (KAMINPAR_BUILD_WITH_UBSAN) \n    add_compile_options(-fsanitize=undefined)\n    add_link_options(-fsanitize=undefined)\nendif ()\n\nif (KAMINPAR_BUILD_WITH_PG)\n    add_compile_options(-pg)\nendif ()\n\n# Pass CMake options to code\nif (KAMINPAR_ENABLE_STATISTICS)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_ENABLE_STATISTICS\")\n    message(STATUS \"Statistics: enabled\")\nelse ()\n    message(STATUS \"Statistics: disabled\")\nendif ()\n\nif (KAMINPAR_ENABLE_HEAP_PROFILING)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_ENABLE_HEAP_PROFILING\")\n    message(STATUS \"Heap Profiling: enabled\")\nelse ()\n    message(STATUS \"Heap Profiling: disabled\")\nendif ()\n\nif (KAMINPAR_ENABLE_PAGE_PROFILING)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_ENABLE_PAGE_PROFILING\")\n    message(STATUS \"Page Profiling: enabled\")\nelse ()\n    message(STATUS \"Page Profiling: disabled\")\nendif ()\n\nif (KAMINPAR_ENABLE_TIMERS)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_ENABLE_TIMERS\")\n    message(STATUS \"Timers: enabled\")\nelse ()\n    message(STATUS \"Timers: disabled\")\nendif ()\n\nif (KAMINPAR_ENABLE_TIMER_BARRIERS)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_ENABLE_TIMER_BARRIERS\")\n    message(STATUS \"Timer barriers: enabled\")\nelse ()\n    message(STATUS \"Timer barriers: disabled\")\nendif ()\n\nif (KAMINPAR_ENABLE_THP)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_ENABLE_THP\")\n    message(STATUS \"Huge pages: enabled\")\nelse ()\n    message(STATUS \"Huge pages: disabled\")\nendif ()\n\nmessage(STATUS \"Graph compression summary:\")\n\nif (KAMINPAR_COMPRESSION_HIGH_DEGREE_ENCODING)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_COMPRESSION_HIGH_DEGREE_ENCODING\")\n    message(\"  High-degree encoding: enabled\")\nelse ()\n    message(\"  High-degree encoding: disabled\")\nendif ()\n\nif (KAMINPAR_COMPRESSION_INTERVAL_ENCODING)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_COMPRESSION_INTERVAL_ENCODING\")\n    message(\"  Interval encoding: enabled\")\nelse ()\n    message(\"  Interval encoding: disabled\")\nendif ()\n\nif (KAMINPAR_COMPRESSION_RUN_LENGTH_ENCODING)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_COMPRESSION_RUN_LENGTH_ENCODING\")\n    message(\"  Run-length encoding: enabled\")\nelse ()\n    message(\"  Run-length encoding: disabled\")\nendif ()\n\nif (KAMINPAR_COMPRESSION_STREAMVBYTE_ENCODING)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_COMPRESSION_STREAMVBYTE_ENCODING\")\n    message(\"  StreamVByte encoding: enabled\")\nelse ()\n    message(\"  StreamVByte encoding: disabled\")\nendif ()\n\nif (KAMINPAR_COMPRESSION_FAST_DECODING)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_COMPRESSION_FAST_DECODING\")\n    add_compile_options(-mbmi2)\n    message(\"  Fast decoding: enabled\")\nelse ()\n    message(\"  Fast decoding: disabled\")\nendif ()\n\nif (KAMINPAR_64BIT_NODE_IDS OR KAMINPAR_64BIT_IDS)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_64BIT_NODE_IDS\")\n    set(KAMINPAR_SHM_NODE_ID_STR \"std::uint64_t\")\nelse ()\n    set(KAMINPAR_SHM_NODE_ID_STR \"std::uint32_t\")\nendif ()\n\nif (KAMINPAR_64BIT_EDGE_IDS OR KAMINPAR_64BIT_IDS)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_64BIT_EDGE_IDS\")\n    set(KAMINPAR_SHM_EDGE_ID_STR \"std::uint64_t\")\nelse ()\n    set(KAMINPAR_SHM_EDGE_ID_STR \"std::uint32_t\")\nendif ()\n\nif (KAMINPAR_64BIT_WEIGHTS)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_64BIT_WEIGHTS\")\n    set(KAMINPAR_SHM_WEIGHT_STR \"std::int64_t\")\nelse () \n    set(KAMINPAR_SHM_WEIGHT_STR \"std::int32_t\")\nendif ()\n\nif (KAMINPAR_64BIT_LOCAL_WEIGHTS)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_64BIT_LOCAL_WEIGHTS\")\n    set(KAMINPAR_DIST_WEIGHT_STR \"std::int64_t\")\nelse ()\n    set(KAMINPAR_DIST_WEIGHT_STR \"std::int32_t\")\nendif ()\n\nmessage(STATUS \"Data type summary:\")\nmessage(\"  {shm, dist}::NodeID: ${KAMINPAR_SHM_NODE_ID_STR}\")\nmessage(\"  {shm, dist}::EdgeID: ${KAMINPAR_SHM_EDGE_ID_STR}\")\nmessage(\"  shm::{Node, Edge}Weight: ${KAMINPAR_SHM_WEIGHT_STR}\")\nmessage(\"  {dist::Global{Node, Edge}ID: std::uint64_t\")\nmessage(\"  dist::Global{Node, Edge}Weight: std::int64_t\")\nmessage(\"  dist::{Node, Edge}Weight: ${KAMINPAR_DIST_WEIGHT_STR}\")\n\n################################################################################\n## Search and fetch dependencies                                              ##\n################################################################################\n\n# Google Sparsehash \nif (KAMINPAR_BUILD_WITH_SPARSEHASH)\n    find_package(Sparsehash REQUIRED)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_SPARSEHASH_FOUND\")\nendif ()\n\nif (KAMINPAR_BUILD_WITH_CCACHE)\n    find_program(CCACHE_PROGRAM ccache)\n    if (CCACHE_PROGRAM)\n        set(CMAKE_CXX_COMPILER_LAUNCHER \"${CCACHE_PROGRAM}\")\n    endif ()\nendif ()\n\nif (KAMINPAR_BUILD_WITH_GROWT)\n    list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_USES_GROWT\")\nendif ()\n\nif (TRUE) \n    add_subdirectory(external/growt EXCLUDE_FROM_ALL)\n    add_library(growt INTERFACE)\n    target_include_directories(growt SYSTEM INTERFACE \"external/growt\")\nendif ()\n\nif (KAMINPAR_BUILD_DISTRIBUTED)\n    # MPI\n    set(MPI_DETERMINE_LIBRARY_VERSION TRUE)\n    find_package(MPI)\n    if (NOT MPI_FOUND) \n        message(WARNING \"MPI not available: cannot build the distributed partitioner\")\n        set(KAMINPAR_BUILD_DISTRIBUTED OFF)\n    endif ()\n\n    if (KAMINPAR_BUILD_WITH_BACKWARD)\n        add_subdirectory(external/bakward-mpi EXCLUDE_FROM_ALL)\n    endif ()\nendif ()\n\nif (KAMINPAR_ASSERTION_LEVEL STREQUAL \"none\")\n    set(KASSERT_ASSERTION_LEVEL 0)\nelseif (KAMINPAR_ASSERTION_LEVEL STREQUAL \"light\")\n    set(KASSERT_ASSERTION_LEVEL 10)\nelseif (KAMINPAR_ASSERTION_LEVEL STREQUAL \"normal\")\n    set(KASSERT_ASSERTION_LEVEL 30)\nelseif (KAMINPAR_ASSERTION_LEVEL STREQUAL \"heavy\")\n    set(KASSERT_ASSERTION_LEVEL 40)\nelse ()\n    message(WARNING \"Invalid assertion level: ${KAMINPAR_ASSERTION_LEVEL}\")\nendif ()\n\n# Add KAssert\nadd_subdirectory(external/kassert EXCLUDE_FROM_ALL)\n\n# If we can find Mt-KaHyPar, make it available for initial partitioning and refinement\nif (KAMINPAR_BUILD_WITH_MTKAHYPAR)\n    find_library(LIB_MTKAHYPAR_GRAPH mtkahypar)\n    if (NOT LIB_MTKAHYPAR_GRAPH)\n        message(STATUS \"Mt-KaHyPar initial partitioning not available: library could not be found on this system\")\n        set(KAMINPAR_BUILD_WITH_MTKAHYPAR OFF)\n    else ()\n        message(STATUS \"Found Mt-KaHyPar at ${LIB_MTKAHYPAR_GRAPH}\")\n    endif ()\nendif ()\n\n# Fetch minimal KaGen for graph IO\nif ((KAMINPAR_BUILD_DISTRIBUTED AND KAMINPAR_BUILD_APPS) OR KAMINPAR_BUILD_BENCHMARKS)\n    set(KAGEN_NODEPS ON CACHE BOOL \"\")\n    set(KAGEN_BUILD_APPS OFF CACHE BOOL \"\")\n    set(KAGEN_BUILD_EXAMPLES OFF CACHE BOOL \"\")\n    set(KAGEN_BUILD_TESTS OFF CACHE BOOL \"\")\n    add_subdirectory(external/KaGen EXCLUDE_FROM_ALL)\nendif ()\n\n################################################################################\n## Add targets in subdirectories                                              ##\n################################################################################\n\n# Start include paths on project root\ninclude_directories(${PROJECT_SOURCE_DIR})\n\n# Shared memory components\nadd_subdirectory(kaminpar-common)\nadd_subdirectory(kaminpar-shm)\n\n# Distributed components\nif (KAMINPAR_BUILD_DISTRIBUTED)\n    add_subdirectory(kaminpar-mpi)\n    add_subdirectory(kaminpar-dist)\nendif ()\n\n# Binaries\nadd_subdirectory(kaminpar-cli)\n\nif (KAMINPAR_BUILD_APPS)\n    add_subdirectory(apps)\nendif ()\n\n# Unit tests\nif (KAMINPAR_BUILD_TESTS)\n    add_subdirectory(external/googletest EXCLUDE_FROM_ALL SYSTEM)\n\n    enable_testing()\n    add_subdirectory(tests)\nendif ()\n\n# Examples\nif (KAMINPAR_BUILD_EXAMPLES)\n    add_subdirectory(examples)\nendif ()\n\n################################################################################\n\nadd_library(KaMinPar::KaMinPar ALIAS kaminpar_shm)\nadd_library(KaMinPar::KaMinParCLI ALIAS kaminpar_cli)\n\nif (KAMINPAR_BUILD_DISTRIBUTED)\n    add_library(KaMinPar::dKaMinPar ALIAS kaminpar_dist)\n    add_library(KaMinPar::dKaMinParCLI ALIAS kaminpar_dist_cli)\nendif ()\n\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kamping-karlsruhe-mpi-next-generation",
            "repo_link": "https://github.com/kamping-site/kamping",
            "content": {
                "codemeta": "",
                "readme": "[![C/C++ CI](https://github.com/kamping-site/kamping/actions/workflows/build.yml/badge.svg)](https://github.com/kamping-site/kamping/actions/workflows/build.yml)\n![GitHub](https://img.shields.io/github/license/kamping-site/kamping)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10949647.svg)](https://doi.org/10.5281/zenodo.10949647)\n\n# KaMPIng: Karlsruhe MPI next generation :rocket:\n\n![KaMPIng logo](./docs/images/logo.svg)\n\nThis is KaMPIng [kampɪŋ], a (near) zero-overhead MPI wrapper for modern C++.\n\nIt covers the whole range of abstraction levels from low-level MPI calls to\nconvenient STL-style bindings, where most parameters are inferred from a small\nsubset of the full parameter set. This allows for both rapid prototyping and\nfine-tuning of distributed code with predictable runtime behavior and memory\nmanagement.\n\nUsing template-metaprogramming, only code paths required for computing\nparameters not provided by the user are generated at compile time, which results in (near) zero-overhead\nbindings.\n\n**:running: Quick Start:**\nKaMPIng is header-only, compatible with all major MPI implementations and requires a C++17-ready compiler.\nThe easiest way to get started is to include KaMPIng using CMake's FetchContent module.\n```cmake\ninclude(FetchContent)\nFetchContent_Declare(\n  kamping\n  GIT_REPOSITORY https://github.com/kamping-site/kamping.git\n  GIT_TAG v0.1.1\n)\n\nFetchContent_MakeAvailable(kamping)\ntarget_link_libraries(myapp PRIVATE kamping::kamping)\n```\nIt is fully compatible with your existing MPI code and you can start using it right away. Just include the headers for the main communicator class and the MPI call that you want to use.\n\n``` c++\n\n#include <kamping/communicator.hpp>\n#include <kamping/collectives/allgather.hpp>\n \nkamping::Communicator comm;\n \nstd::vector<int> input(comm.rank(), comm.rank_signed());\nauto const result = comm.allgatherv(kamping::send_buf(input));\n```\n\nWe provide a wide range of [usage](./examples/usage) and [simple applications](./examples/applications) examples (start with [`allgatherv`](./examples/usage/allgatherv_example.cpp)). Or checkout the [documentation](https://kamping-site.github.io/kamping/) for a description of KaMPIng's core concepts and a full reference.\n\nKaMPIng is developed at the [Algorithm Engineering\nGroup](https://ae.iti.kit.edu/english/index.php) at Karlsruhe Institute of\nTechnology.\n\nIf you use KaMPIng in the context of an academic publication, we kindly ask you to cite [our technical report](https://arxiv.org/abs/2404.05610):\n\n``` bibtex\n@misc{kamping2024,\n  title={KaMPIng: Flexible and (Near) Zero-overhead C++ Bindings for MPI},\n  author={Demian Hespe and Lukas Hübner and Florian Kurpicz and Peter Sanders and Matthias Schimek and Daniel Seemaier and Christoph Stelz and Tim Niklas Uhl},\n  year={2024},\n  eprint={2404.05610},\n  archivePrefix={arXiv},\n  primaryClass={cs.DC}\n}\n```\n\n## Features :sparkles:\n### Named Parameters :speech_balloon:\nUsing plain MPI, operations like `MPI_Allgatherv` often lead to verbose and error-prone boilerplate code:\n\n``` c++\nstd::vector<T> v = ...; // Fill with data\nint size;\nMPI_Comm_size(comm, &size);\nint n = static_cast<int>(v.size());\nstd::vector<int> rc(size), rd(size);\nMPI_Allgather(&n, 1, MPI_INT, rc.data(), 1, MPI_INT, comm);\nstd::exclusive_scan(rc.begin(), rc.end(), rd.begin(), 0);\nint n_glob = rc.back() + rd.back();\nstd::vector<T> v_glob(v_global_size);\nMPI_Allgatherv(v.data(), v_size, MPI_TYPE, v_glob.data(), rc.data(), rd.data(), MPI_TYPE, comm);\n\n```\n\nIn contrast, KaMPIng introduces a streamlined syntax inspired by Python's named parameters. For example, the `allgatherv` operation becomes more intuitive and concise:\n\n```c++\nstd::vector<T> v = ...; // Fill with data\nstd::vector<T> v_glob = comm.allgatherv(send_buf(v));\n```\n\nEmpowered by named parameters, KaMPIng allows users to name and pass parameters in arbitrary order, computing default values only for the missing ones. This not only improves readability but also streamlines the code, providing a user-friendly and efficient way of writing MPI applications.\n\n### Controlling memory allocation :floppy_disk:\nKaMPIng's *resize policies* allow for fine-grained control over when allocation happens:\n\n| resize policy            |                                                                         |\n|--------------------------|-------------------------------------------------------------------------|\n| `kamping::resize_to_fit` | resize the container to exactly accommodate the data                    |\n| `kamping::no_resize`     | assume that the container has enough memory available to store the data |\n| `kamping::grow_only`     | only resize the container if it not large enough                        |\n\n\n``` c++\n// easy to use with sane defaults\nstd::vector<int> v = comm.recv<int>(source(kamping::rank::any));\n\n// flexible memory control\nstd::vector<int> v_out;\nv_out.resize(enough_memory_to_fit);\n// already_known_counts are the recv_counts that may have been computed already earlier and thus do not need to be computed again\ncomm.recv<int>(recv_buf<kamping::no_resize>(v_out), recv_count(i_know_already_know_that), source(kamping::rank::any));\n```\n\n### STL support :books:\n- KaMPIng works with everything that is a `std::contiguous_range`, everywhere.\n- Builtin C++ types are automatically mapped to their corresponding MPI types. \n- All internally used containers can be altered via template parameters.\n### Expandability :jigsaw:\n- Don't like the performance of your MPI implementation's reduce algorithm? Just override it using our plugin architecture.\n- Add additional functionality to communicator objects, without altering any application code.\n- Easy to integrate with existing MPI code.\n- Flexible core library for a new toolbox :toolbox: of distributed datastructures and algorithms\n\n### And much more ... :arrow_upper_right:\n- Safety guarantees for non-blocking communication and easy handling of multiple requests via request pools\n- Compile time and runtime error checking (which can be completely deactivated).\n- Collective hierarchical timers to speed up your evaluation workflow.\n- ...\n\nDive into the [documentation](https://kamping-site.github.io/kamping/) or [tests](https://github.com/kamping-site/kamping/tree/main/tests) to find out more ...\n\n### (Near) zero overhead - for development and performance :chart_with_upwards_trend:\nUsing template-metaprogramming, KaMPIng only generates the code paths required for computing parameters not provided by the user. \nThe following shows a complete implementation of distributed sample sort with KaMPIng. \n\n```c++\nvoid sort(MPI_Comm comm_, std::vector<T>& data, size_t seed) {\n    Communicator<> comm(comm_);\n    size_t const   oversampling_ratio = 16 * static_cast<size_t>(std::log2(comm.size())) + 1;\n    std::vector<T> local_samples(oversampling_ratio);\n    std::sample(data.begin(), data.end(), local_samples.begin(), oversampling_ratio, std::mt19937{seed});\n    auto global_samples = comm.allgather(send_buf(local_samples)).extract_recv_buffer();\n    std::sort(global_samples.begin(), global_samples.end());\n    for (size_t i = 0; i < comm.size() - 1; i++) {\n        global_samples[i] = global_samples[oversampling_ratio * (i + 1)];\n    }\n    global_samples.resize(num_splitters);\n    std::vector<std::vector<T>> buckets(global_samples.size() + 1);\n    for (auto& element: data) {\n        auto const bound = std::upper_bound(global_samples.begin(), global_samples.end(), element);\n        buckets[static_cast<size_t>(bound - global_samples.begin())].push_back(element);\n    }\n    data.clear();\n    std::vector<int> scounts;\n    for (auto& bucket: buckets) {\n        data.insert(data.end(), bucket.begin(), bucket.end());\n        scounts.push_back(static_cast<int>(bucket.size()));\n    }\n    data = comm.alltoallv(send_buf(data), send_counts(scounts)).extract_recv_buffer();\n    std::sort(data.begin(), data.end());\n}\n```\nIt is a lot more concise than the [(verbose) plain MPI implementation](./examples/applications/sample-sort/mpi.hpp), but also introduces no additional overhead to achieve this, as can be seen the following experiment. There we compare the sorting implementation in KaMPIng to other MPI bindings.\n\n![](./plot.svg)\n## Platform :desktop_computer:\n- intensively tested with GCC and Clang and OpenMPI\n- requires a C++17 ready compiler\n- easy integration into other projects using modern CMake\n   \n## Other MPI bindings\n|                                                      | [MPI](https://www.mpi-forum.org/) | [Boost.MPI](https://www.boost.org/doc/libs/1_84_0/doc/html/mpi.html) | [RWTH MPI](https://github.com/VRGroupRWTH/mpi) | [MPL](https://github.com/rabauke/mpl) | ![KaMPIng](./docs/images/icon.svg) |\n|------------------------------------------------------|:---------------------------------:|:--------------------------------------------------------------------:|:----------------------------------------------:|:-------------------------------------:|:-----------------------------------------------:|\n| STL support                                          | :x:                               | :heavy_check_mark:[^2]                                               | :heavy_check_mark:[^3]                         | :heavy_check_mark:[^2]                | :white_check_mark:                              |\n| computation of defaults via additional communication | :x:                               | :x:                                                                  | :white_check_mark:                             | :x:                                   | :white_check_mark:                              |\n| custom reduce operations via lambdas                 | :x:                               | :white_check_mark:                                                   | :x:                                            | :heavy_check_mark:[^4]                | :white_check_mark:                              |\n| containers can be resized automatically              | :x:                               | :heavy_check_mark:[^1]                                               | :heavy_check_mark:[^3]                         | :x:                                   | :white_check_mark:                              |\n| error handling                                       | :white_check_mark:                | :white_check_mark:                                                   | :white_check_mark:                             | :x:                                   | :white_check_mark:                              |\n| actively maintained                                  | :white_check_mark:                | :x:                                                                  | :heavy_check_mark:                             | :white_check_mark:                    | :white_check_mark:                              |\n\n[^1]: partial \n\n[^2]: only `std::vector` \n\n[^3]: only for send and receive buffers\n\n[^4]: not mapped to builtin operations\n\n## LICENSE\nKaMPIng is released under the GNU Lesser General Public License. See [COPYING](COPYING) and [COPYING.LESSER](COPYING.LESSER) for details\n\n",
                "dependencies": "cmake_minimum_required(VERSION 3.25)\n\nlist(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\")\n\n# Project setup\nproject(\n    KaMPIng\n    DESCRIPTION \"Flexible and (near) zero-overhead C++ bindings for MPI\"\n    LANGUAGES CXX\n    VERSION 0.1.1\n)\ninclude(FetchContent)\n\nif (PROJECT_IS_TOP_LEVEL)\n    # folder support for IDEs\n    set_property(GLOBAL PROPERTY USE_FOLDERS ON)\n\n    # this has to be enabled in the main CMakeLists file\n    include(CTest)\n\n    add_subdirectory(docs)\n\n    FetchContent_Declare(\n        Format.cmake\n        GIT_REPOSITORY https://github.com/TheLartians/Format.cmake\n        GIT_TAG v1.8.1\n    )\n    FetchContent_MakeAvailable(Format.cmake)\nendif ()\n\n# Require out-of-source builds\nfile(TO_CMAKE_PATH \"${PROJECT_BINARY_DIR}/CMakeLists.txt\" LOC_PATH)\nif (EXISTS \"${LOC_PATH}\")\n    message(\n        FATAL_ERROR\n            \"You cannot build in a source directory (or any directory with a CMakeLists.txt file). Please make a build \"\n            \"subdirectory. Feel free to remove CMakeCache.txt and CMakeFiles.\"\n    )\nendif ()\n\noption(KAMPING_WARNINGS_ARE_ERRORS OFF)\noption(KAMPING_BUILD_EXAMPLES_AND_TESTS OFF)\noption(KAMPING_TESTS_DISCOVER OFF)\noption(KAMPING_ENABLE_ULFM \"Enable User-Level Failure-Mitigation (ULFM)\" OFF)\noption(KAMPING_ENABLE_SERIALIZATION \"Enable support for serialization (requires Cereal)\" ON)\noption(KAMPING_ENABLE_REFLECTION \"Enable support for reflecting struct members (requires Boost.PFR)\" ON)\noption(\n    KAMPING_REFLECTION_USE_SYSTEM_BOOST_FOR_PFR\n    \"Use Boost.PFR from system installed Boost, instead of using a standalone PFR install or building PFR from source.\"\n    OFF\n)\n\n# Enable compilation with ccache. Defaults to ON if this is the main project.\nif (PROJECT_IS_TOP_LEVEL)\n    option(KAMPING_USE_CCACHE \"Globally enable ccache.\" ON)\nelse ()\n    option(KAMPING_USE_CCACHE \"Globally enable ccache.\" OFF)\nendif ()\n\nif (KAMPING_USE_CCACHE)\n    include(CCache)\nendif ()\n\nset(MPI_DETERMINE_LIBRARY_VERSION TRUE)\nfind_package(MPI REQUIRED)\n\nadd_subdirectory(extern)\n\nadd_library(kamping_base INTERFACE)\ntarget_include_directories(kamping_base INTERFACE include)\n\n# set C++ standard to C++17\ntarget_compile_features(kamping_base INTERFACE cxx_std_17)\ntarget_link_libraries(kamping_base INTERFACE MPI::MPI_CXX)\n\nlist(\n    APPEND\n    KAMPING_WARNING_FLAGS\n    \"-Wall\"\n    \"-Wextra\"\n    \"-Wconversion\"\n    \"-Wnon-virtual-dtor\"\n    \"-Woverloaded-virtual\"\n    \"-Wshadow\"\n    \"-Wsign-conversion\"\n    \"-Wundef\"\n    \"-Wunreachable-code\"\n    \"-Wunused\"\n)\n\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n    list(\n        APPEND\n        KAMPING_WARNING_FLAGS\n        \"-Wcast-align\"\n        \"-Wnull-dereference\"\n        \"-Wpedantic\"\n        \"-Wextra-semi\"\n        \"-Wno-gnu-zero-variadic-macro-arguments\"\n    )\nendif ()\n\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n    list(\n        APPEND\n        KAMPING_WARNING_FLAGS\n        \"-Wcast-align\"\n        \"-Wnull-dereference\"\n        \"-Wpedantic\"\n        \"-Wnoexcept\"\n        \"-Wsuggest-attribute=const\"\n        \"-Wsuggest-attribute=noreturn\"\n        \"-Wsuggest-override\"\n    )\nendif ()\n\n# OFF by default.\nif (KAMPING_WARNINGS_ARE_ERRORS)\n    list(APPEND KAMPING_WARNING_FLAGS \"-Werror\")\nendif ()\n\n# Target for user-code\nadd_library(kamping INTERFACE)\ntarget_link_libraries(kamping INTERFACE kamping_base)\n\n# If enabled, use exceptions, otherwise use KASSERT()\noption(KAMPING_EXCEPTION_MODE \"Use exceptions to report recoverable errors.\" ON)\nif (KAMPING_EXCEPTION_MODE)\n    set(KASSERT_EXCEPTION_MODE 1)\n    target_compile_definitions(kamping INTERFACE -DKASSERT_EXCEPTION_MODE)\n    message(STATUS \"Build with exceptions enabled.\")\nelse ()\n    set(KASSERT_EXCEPTION_MODE 0)\n    message(STATUS \"Build with exceptions disabled. Assertions are used instead.\")\nendif ()\n\n# The assertion level controls which assertions are enabled during runtime:\n#\n# * Level 0: Disable all assertions\n# * Level 10: Exception assertions = only enable exceptions (if not in exception mode)\n# * Level 20: Light assertions = assertions that do not affect the running time of library operations significantly.\n# * Level 30: Normal assertions = assertions that might slow down some operations of the library by a constant factor.\n#   Should only be used in debug mode.\n# * Level 40: Light communication assertions = assertions that perform additional communication causing small running\n#   time overheads.\n# * Level 50: Heavy communication assertions = assertions that perform additional communication causing significant\n#   running time overheads.\n# * Level 60: Heavy assertions = assertions that introduce overhead which renders some library operations infeasible\n#   when invoked with any significant work load.\n#\n# Assertion levels can be set explicitly using the -DKAMPING_ASSERTION_LEVEL=... flag. If no level is set explicitly, we\n# set it to 10 (exceptions only) in Release mode and 30 (up to normal assertions) in Debug mode.\nset(KAMPING_ASSERTION_LEVEL\n    $<IF:$<CONFIG:Debug>,\"normal\",\"exceptions\">\n    CACHE STRING \"Assertion level\"\n)\nset_property(\n    CACHE KAMPING_ASSERTION_LEVEL\n    PROPERTY STRINGS\n             none\n             exceptions\n             light\n             normal\n             light_communication\n             heavy_communication\n             heavy\n)\nmessage(STATUS \"Assertion level: ${KAMPING_ASSERTION_LEVEL}\")\n\n# If KAMPING_ASSERTION_LEVEL defaults to the generator expression, ${KAMPING_ASSERTION_LEVEL} may not be quoted However,\n# if it is explicitly set to some constant string, it must be quoted Thus, all levels are listed twice, once with and\n# without quotes @todo find a better solution for this problem\nstring(\n    CONCAT KASSERT_ASSERTION_LEVEL\n           $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"none\">:0>\n           $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"none\">:0>\n           $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"exceptions\">:10>\n           $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"exceptions\">:10>\n           $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"light\">:20>\n           $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"light\">:20>\n           $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"normal\">:30>\n           $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"normal\">:30>\n           $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"light_communication\">:40>\n           $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"light_communication\">:40>\n           $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"heavy_communication\">:50>\n           $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"heavy_communication\">:50>\n           $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"heavy\">:60>\n           $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"heavy\">:60>\n)\nFetchContent_Declare(\n    kassert\n    GIT_REPOSITORY https://github.com/kamping-site/kassert\n    GIT_TAG v0.1.0\n)\nFetchContent_MakeAvailable(kassert)\n\ntarget_link_libraries(kamping_base INTERFACE kassert::kassert)\n\nFetchContent_Declare(\n    pfr\n    GIT_REPOSITORY https://github.com/boostorg/pfr\n    GIT_TAG 2.2.0\n    SYSTEM FIND_PACKAGE_ARGS 2.2.0\n)\n\nif (KAMPING_ENABLE_REFLECTION)\n    if (KAMPING_REFLECTION_USE_SYSTEM_BOOST_FOR_PFR)\n        find_package(Boost 1.75 COMPONENTS headers CONFIG)\n        if (NOT Boost_FOUND)\n            message(\n                FATAL_ERROR\n                    \"Boost.PFR: No compatible Boost version found. Use KAMPING_REFLECTION_USE_SYSTEM_BOOST_FOR_PFR=OFF to use standalone Boost.PFR.\"\n            )\n        else ()\n            message(STATUS \"Found Boost ${Boost_VERSION}: ${Boost_DIR}\")\n            message(STATUS \"Using system Boost for Boost.PFR\")\n            add_library(kamping_pfr INTERFACE)\n            # when using system installed Boost, it does not provide a PFR target, so we have to link to the headers\n            # target\n            target_link_libraries(kamping_pfr INTERFACE Boost::headers)\n            add_library(Boost::pfr ALIAS kamping_pfr)\n        endif ()\n    else ()\n        FetchContent_MakeAvailable(pfr)\n        if (pr_FOUND)\n            message(STATUS \"Found Boost.PFR: ${pfr_DIR}\")\n        else ()\n            message(STATUS \"Boost.PFR: building from source.\")\n        endif ()\n    endif ()\n    target_link_libraries(kamping_base INTERFACE Boost::pfr)\n    target_compile_definitions(kamping_base INTERFACE KAMPING_ENABLE_REFLECTION)\n    message(STATUS \"Reflection: enabled\")\nelse ()\n    message(STATUS \"Reflection: disabled\")\nendif ()\n\nif (KAMPING_ENABLE_SERIALIZATION)\n    FetchContent_Declare(\n        cereal\n        GIT_REPOSITORY https://github.com/USCiLab/cereal\n        GIT_TAG v1.3.2\n        SYSTEM FIND_PACKAGE_ARGS 1.3.2\n    )\n    set(JUST_INSTALL_CEREAL ON)\n    FetchContent_MakeAvailable(cereal)\n    if (cereal_FOUND)\n        message(STATUS \"Found cereal: ${cereal_DIR}\")\n    else ()\n        message(STATUS \"Cereal: building from source.\")\n    endif ()\n    target_link_libraries(kamping_base INTERFACE cereal::cereal)\n    target_compile_definitions(kamping_base INTERFACE KAMPING_ENABLE_SERIALIZATION)\n    message(STATUS \"Serialization: enabled\")\nelse ()\n    message(STATUS \"Serialization: disabled\")\nendif ()\n\nadd_library(kamping::kamping ALIAS kamping)\n\n# Testing and examples are only built if this is the main project or if KAMPING_BUILD_EXAMPLES_AND_TESTS is set (OFF by\n# default)\nif (PROJECT_IS_TOP_LEVEL OR KAMPING_BUILD_EXAMPLES_AND_TESTS)\n    add_subdirectory(examples)\n    add_subdirectory(tests)\nendif ()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/karri",
            "repo_link": "https://github.com/molaupi/karri",
            "content": {
                "codemeta": "",
                "readme": "# KaRRi\n\nThis repository contains the C++17 implementation of KaRRi, a state-of-the-art dispatcher for the dynamic \ntaxi sharing problem with meeting points. \nKaRRi uses engineered on-the-fly shortest path queries based on bucket contraction hierarchies (BCHs) \nto allow for fast query times with maximum flexibility. \nFor more information on KaRRi's novel techniques, we refer to the related publication:\n\n* Moritz Laupichler, and Peter Sanders. Fast Many-to-Many Routing for Dynamic Taxi Sharing with\n  Meeting Points. 2024 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX),\n  2024\\. https://doi.org/10.1137/1.9781611977929.6\n\nIf you use KaRRi in your scientific publication, we ask that you cite the paper above.\n\n## License\n\nAll files in this repository except the files in the directory `External` are licensed under the MIT\nlicense. External libraries are licensed under their respective licenses.\n\nThis source code is based on a fork of https://github.com/vbuchhold/routing-framework.\nLarge parts of the project structure as well as basic data structures and shortest path algorithms\nare directly taken or adapted from the original framework.\nThe copyright statements in each file state the respective author or authors of the file.\n\n## Prerequisites\n\nTo build KaRRi, you need to have some tools and libraries installed. On Debian and its derivatives\n(such as Ubuntu) the `apt-get` tool can be used:\n\n```\n$ sudo apt-get install build-essential\n$ sudo apt-get install cmake\n$ sudo apt-get install python3 python3-pip; pip3 install -r python_requirements.txt\n$ sudo apt-get install sqlite3 libsqlite3-dev\n$ sudo apt-get install zlib1g-dev\n```\n\nNext, you need to clone the libraries in the `External` subdirectory and build the `RoutingKit` library. To do so,\ntype the following commands at the top-level directory of the framework:\n\n```\n$ git submodule update --init\n$ make -C External/RoutingKit lib/libroutingkit.so\n```\n\n\n## Constructing KaRRi Input\nWe provide bash scripts to generate the input data for the `Berlin-1pct`, `Berlin-10pct`,\n`Ruhr-1pct`, and `Ruhr-10pct` problem instances for the KaRRi algorithm.\nInputs are generated based on OpenStreetMap (OSM) data, which requires the [osmium tool](https://osmcode.org/osmium-tool/). \nOn Debian and its derivatives osmium can be installed using\n```\nsudo apt-get install osmium-tool\n```\n\nAs an example, you can generate the input data for the `Berlin-1pct` instance by typing the following commands\nat the top-level directory: (Downloads multiple GiB of raw OSM data and requires at least 10 GiB of RAM.)\n\n```\n$ cd Publications/KaRRi\n$ bash DownloadGermanyOSMData.sh .\n$ bash FilterGermanyOSMData.sh .\n$ bash PreprocessOSMData.sh . Germany Berlin BoundaryPolygons\n$ bash GenerateKnownInstanceInputData.sh . Berlin-1pct pedestrian\n```\n\nTo generate the input data for the other instances, simply replace `Berlin-1pct` with the instance name\n(`Berlin-10pct`, `Ruhr-1pct`, `Ruhr-10pct`) and replace `Berlin` with `Ruhr` for the\nRuhr instances.\n\n\n## Running KaRRi\nTo run KaRRi in its default configuration (using collective last stop searches, sorted buckets, and\nSIMD instructions), use the provided bash script by typing the following commands at the top-level directory:\n\n```\n$ cd Publications/KaRRi\n$ bash RunKaRRiDefault.sh . <instance-name> <output-dir>\n```\n\nwhere `<instance-name>` can be any of `Berlin-1pct`, `Berlin-10pct`, `Ruhr-1pct`,\nand `Ruhr-10pct`,  and `<output-dir>` is the path to the directory where the output files\nwill be stored.\n\nWe provide functions for a basic evaluation of results in `Publications/KaRRi/eval.R`.\n\n",
                "dependencies": "# ******************************************************************************\n# MIT License\n#\n# Copyright (c) 2020 Valentin Buchhold\n# Copyright (c) 2023 Moritz Laupichler\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n# ******************************************************************************\n\n\ncmake_minimum_required(VERSION 3.24 FATAL_ERROR)\nproject(RoutingFramework CXX)\n\n# Determine the language standard.\nset(CMAKE_CXX_STANDARD 20)\nset(CMAKE_CXX_EXTENSIONS OFF)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\n# Flags when building for the Devel configuration.\nif (CMAKE_CXX_COMPILER_ID MATCHES GNU|Clang)\n    set(CMAKE_CXX_FLAGS_DEVEL -O3)\nendif ()\nif (NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE Devel)\nendif ()\n\n# Enable the compiler to use extended instructions in generated code.\nif (CMAKE_CXX_COMPILER_ID MATCHES GNU|Clang)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -march=native\")\nendif ()\noption(DISABLE_AVX \"Disable use of instructions in the AVX extended instruction set.\" OFF)\nif (DISABLE_AVX)\n    if (CMAKE_CXX_COMPILER_ID MATCHES GNU|Clang)\n        message(\"Disabling use of AVX instructions.\")\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mno-avx\")\n    endif ()\nendif ()\n\n# Request warnings.\nif (CMAKE_CXX_COMPILER_ID MATCHES GNU)\n    set(FULL_WARNINGS \"-Werror\" \"-Wfatal-errors\" \"-Wpedantic\" \"-pedantic-errors\" \"-Wall\" \"-Wextra\" \"-ftemplate-backtrace-limit=1\" \"-Wno-unknown-pragmas\" \"-Wno-sign-compare\")\nelseif (CMAKE_CXX_COMPILER_ID MATCHES Clang)\n    set(FULL_WARNINGS \"-Werror\" \"-Wall\" \"-Wextra\" \"-pedantic-errors\" \"-ferror-limit=1\" \"-ftemplate-backtrace-limit=1\" \"-Wno-sign-compare\")\nendif ()\n\ninclude_directories(${CMAKE_SOURCE_DIR})\n\n# Dependencies installed via package manager\nfind_package(OpenMP)\n\n# Dependencies from git submodules and bundled libraries\nadd_subdirectory(External)\n\n# Dependencies fetched at configure time:\ninclude(FetchContentDependencies.cmake)\n\n# Targets for this project are defined in directories Launchers and RawData\nset(TARGETS_DIRECTORIES Launchers RawData)\nforeach (TAR_DIR ${TARGETS_DIRECTORIES})\n    add_subdirectory(${TAR_DIR})\nendforeach ()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kd-tree-python",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/KD-Tree-Python",
            "content": {
                "codemeta": "",
                "readme": "![Python](https://img.shields.io/badge/Python-3670A0?style=plastic&logo=python&logoColor=ffdd54) ![NumPy](https://img.shields.io/badge/Numpy-777BB4.svg?style=plastic&logo=numpy&logoColor=white) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/KD-Tree-Python?style=plastic&cached)\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13384095.svg)](https://doi.org/10.5281/zenodo.13384095)\n\n# KD-Tree Implementation in Python\n\nThis repository contains Python implementation of the kd-tree data structure and performing k-nearest neighbour search.\n\nIts Matlab implementation is located here: [KD-Tree-Matlab](https://github.com/Ramy-Badr-Ahmed/KD-Tree-Matlab)\n\n### About\nThe kd-tree is a space-partitioning data structure for organizing points in a k-dimensional space.\n\n> [Mathematica Link](https://reference.wolfram.com/language/ref/datastructure/KDTree.html)\n\n### Scripts\n\n1. `build_kdtree.py`\n\n   > Builds a kd-tree from a set of points.\n\n2. `nearest_neighbour_search.py`\n\n   > Performs nearest neighbour search using the built kd-tree.\n\n3. `hypercube_points.py`\n\n   > Generates n-Dimensional Points Uniformly in an n-Dimensional Hypercube.\n\n### Example Usage\n\n```python\nfrom kdtree.build_kdtree import build_kdtree\nfrom kdtree.nearest_neighbor_search import nearest_neighbor_search\nfrom examples.hypercube_points import hypercube_points\n\nnum_points = 5000\ncube_size = 10\nnum_dimensions = 10\n\npoints = hypercube_points(num_points, cube_size, num_dimensions)\nhypercube_kdtree = build_kdtree(points.tolist())\n\nquery_point = np.random.rand(num_dimensions).tolist()\n\nnearest_point, nearest_dist, nodes_visited = nearest_neighbor_search(hypercube_kdtree, query_point)\n\nprint(f\"Query point: {query_point}\")\nprint(f\"Nearest point: {nearest_point}\")\nprint(f\"Distance: {nearest_dist:.4f}\")\nprint(f\"Nodes visited: {nodes_visited}\")\n\n",
                "dependencies": "numpy\npytest\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/key",
            "repo_link": "https://github.com/keyproject/key",
            "content": {
                "codemeta": "",
                "readme": "# KeY -- Deductive Java Program Verifier\n\n[![Tests](https://github.com/KeYProject/key/actions/workflows/tests.yml/badge.svg)](https://github.com/KeYProject/key/actions/workflows/tests.yml) [![CodeQL](https://github.com/KeYProject/key/actions/workflows/codeql.yml/badge.svg)](https://github.com/KeYProject/key/actions/workflows/codeql.yml) [![CodeQuality](https://github.com/KeYProject/key/actions/workflows/code_quality.yml/badge.svg)](https://github.com/KeYProject/key/actions/workflows/code_quality.yml) \n\nThis repository is the home of the interactive theorem prover KeY for formal verification and analysis of Java programs. KeY comes as a standalone GUI application, which allows you to verify the functional correctness of Java programs with respect to formal specifications formulated in the Java Modeling Language JML. Moreover, KeY can also be used as a library e.g. for symbolic program execution, first order reasoning, or test case generation.\n\nFor more information, refer to\n\n* [The KeY homepage](https://key-project.org) \n* [The KeY book](https://www.key-project.org/thebook2/)\n* [The KeY developer documentation](https://keyproject.github.io/key-docs/)\n* KeY's success stories:\n  * [Severe bug discovered in JDK sorting routine (TimSort)](http://www.envisage-project.eu/proving-android-java-and-python-sorting-algorithm-is-broken-and-how-to-fix-it/),  \n  * [Verification of `java.util.IdentityHashMap`](https://doi.org/10.1007/978-3-031-07727-2_4),\n  * [Google Award for analysing a bug in `LinkedList`](https://www.key-project.org/2023/07/23/cwi-researchers-win-google-award-for-finding-a-bug-in-javas-linkedlist-using-key/)\n\nThe current version of KeY is 2.12.2, licensed under GPL v2.\n\n\nFeel free to use the project templates to get started using KeY:\n* [For Verification Projects](https://github.com/KeYProject/verification-project-template)\n* [Using as a Library](https://github.com/KeYProject/key-java-example)\n* [Using as a Symbolic Execution Backend](https://github.com/KeYProject/symbex-java-example)\n\n## Requirements\n\n* Hardware: >=2 GB RAM\n* Operating System: Linux/Unix, MacOSX, Windows\n* Java 17 or newer\n* Optionally, KeY can make use of the following binaries:\n  * SMT Solvers:\n    * [Z3](https://github.com/Z3Prover/z3#z3)\n    * [cvc5](https://cvc5.github.io/)\n    * [CVC4](https://cvc4.github.io/)\n    * [Princess](http://www.philipp.ruemmer.org/princess.shtml)\n\n## Content of the KeY folder\n\nThis folder provides a [gradle](https://gradle.org)-managed project following\n[Maven's standard folder layout](https://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html).\nThere are several subprojects in this folder. In general, every `key.*/` subproject contains a core component of KeY.\nAdditional and optional components are in `keyext.*/` folders. The file `build.gradle` is the root build script\ndescribing the dependencies and common build tasks for all subprojects.\n\n`key.util`, `key.core` and `key.ui` are the base for the product \"KeY Prover\". Special care is needed\nif you plan to make changes here.\n\n\n## Compile and Run KeY\n\nAssuming you are in the directory of this README file, you can create a runnable and deployable version with one of these commands:\n\n1. With `./gradlew key.ui:run` you can run the user interface of KeY directly from the repository. \n   Use `./gradlew key.ui:run --args='--experimental'` to enable experimental features.\n\n2. Use `./gradlew classes` to compile KeY, which includes running JavaCC and Antlr.\n   Likewise, use `./gradlew testClasses` if you also want to compile the JUnit test classes.\n\n3. Test your installation with `./gradlew test`. Be aware that this will usually take multiple hours to complete.\n   With `./gradlew testFast`, you can run a more lightweight test suite that should complete in a few minutes.\n\n   You can select a specific test case with the `--tests` argument. Wildcards are allowed.\n   ```sh\n   ./gradlew :key.<subproject>:test --tests \"<class>.<method>\"\n   ```\n\n   You can debug KeY by adding the `--debug-jvm` option, then attaching a debugger at `localhost:5005`.\n\n4. You can create a single jar-version, aka *fat jar*, of KeY with\n   ```sh\n   ./gradlew :key.ui:shadowJar\n   ```\n   The file is generated in `key.ui/build/libs/key-*-exe.jar`.\n\n5. A distribution is build with\n   ```sh\n   ./gradlew :key.ui:installDist :key.ui:distZip\n   ```\n   The distribution can be tested by calling `key.ui/install/key/bin/key.ui`\n   and is zipped in `key.ui/build/distributions`.\n\n   The distribution gives you potential of using single jar files.\n\n# Developing KeY\n\n* Quality is automatically assessed using [SonarQube](https://sonarqube.org) on each pull request.\n  The results of the assessments (pass/fail) can be inspected in the checks section of the PR.\n\n  The rules and quality gate are maintained by Alexander Weigl\n  <weigl@kit.edu> currently.\n\n* More guideline and documentation for the KeY development can be found under\n[key-docs](https://keyproject.github.io/key-docs/devel/).\n\n\n\n# Issues and Bug Reports\n\n* For bug reports, please use the [issue tracker](https://github.com/KeYProject/key/issues) or send a mail to support@key-project.org. \n\n* For discussions, you may want to subscribe and use the mailing list <key-all@lists.informatik.kit.edu> or use [GitHub discussions](https://github.com/KeYProject/key/discussions).\n\n# Contributing to KeY\n\nFeel free to submit [pull requests](https://github.com/KeYProject/key/pulls) via GitHub. Pull requests are assessed using automatic tests, formatting and static source checkers, as well as a manual review by one of the developers. More guidelines and documentation for the KeY development can be found under [key-docs](https://keyproject.github.io/key-docs/devel/).\n\n\n\n# License Remark\n\n```\nThis is the KeY project - Integrated Deductive Software Design\nCopyright (C) 2001-2011 Universität Karlsruhe, Germany\n\t\t\t\t\t\tUniversität Koblenz-Landau, Germany\n\t\t\t\t\t\tand Chalmers University of Technology, Sweden\nCopyright (C) 2011-2024 Karlsruhe Institute of Technology, Germany\n\t\t\t\t\t\tTechnical University Darmstadt, Germany\n\t\t\t\t\t\tChalmers University of Technology, Sweden\n\nThe KeY system is protected by the GNU General Public License.\nSee LICENSE.TXT for details.\n```\n\n",
                "dependencies": "import groovy.transform.Memoized\n\nplugins {\n    //Support for IntelliJ IDEA\n    //https://docs.gradle.org/current/userguide/idea_plugin.html\n    id(\"idea\")\n\n    //Support for Eclipse\n    //https://docs.gradle.org/current/userguide/eclipse_plugin.html\n    id(\"eclipse\")  //support for Eclipse\n\n    //Checks and adds the license header of the source files:\n    // Task: `licenseMain' and `licenseFormatMain'\n    //https://github.com/hierynomus/license-gradle-plugin\n    id \"com.github.hierynomus.license-base\" version \"0.16.1\"\n    //Generates reports on the license of used packages: Task `downloadLicenses'\n    //Some Licenses requires an entry in the credits (MIT, BSD)\n    id \"com.github.hierynomus.license-report\" version \"0.16.1\"\n\n    // Gives `gradle dependencyUpdate` to show which dependency has a newer version\n    // id \"com.github.ben-manes.versions\" version \"0.39.0\"\n\n    // Code formatting\n    id \"com.diffplug.spotless\" version \"6.25.0\"\n\n    // EISOP Checker Framework\n    id \"org.checkerframework\" version \"0.6.45\"\n\n    id(\"org.sonarqube\") version \"5.0.0.4638\"\n}\n\nsonar {\n  properties {\n    property \"sonar.projectKey\", \"KeYProject_key\"\n    property \"sonar.organization\", \"keyproject\"\n    property \"sonar.host.url\", \"https://sonarcloud.io\"\n  }\n}\n\n\n// Configure this project for use inside IntelliJ:\nidea {\n    module {\n        downloadJavadoc = false\n        downloadSources = true\n    }\n}\n\nstatic def getDate() {\n    return new Date().format('yyyyMMdd')\n}\n\n// The $BUILD_NUMBER is an environment variable set by Jenkins.\ndef build = System.env.BUILD_NUMBER == null ? \"-dev\" : \"-${System.env.BUILD_NUMBER}\"\n\ngroup = \"org.key-project\"\nversion = \"2.12.4$build\"\n\nsubprojects {\n    apply plugin: \"java\"\n    apply plugin: \"java-library\"\n    apply plugin: \"maven-publish\"\n    apply plugin: \"signing\"       // GPG signing of artifacts, required by maven central\n    apply plugin: \"idea\"\n    apply plugin: \"eclipse\"\n\n    apply plugin: \"com.diffplug.spotless\"\n    apply plugin: \"checkstyle\"\n    apply plugin: \"pmd\"\n    apply plugin: \"org.checkerframework\"\n\n    group = rootProject.group\n    version = rootProject.version\n\n    java {\n        sourceCompatibility = 21\n        targetCompatibility = 21\n    }\n\n    repositories {\n        mavenCentral()\n        maven {\n            url 'https://git.key-project.org/api/v4/projects/35/packages/maven'\n        }\n    }\n\n    dependencies {\n        implementation(\"org.slf4j:slf4j-api:2.0.16\")\n        implementation(\"org.slf4j:slf4j-api:2.0.16\")\n        testImplementation(\"ch.qos.logback:logback-classic:1.5.12\")\n\n        //compile group: 'org.apache.logging.log4j', name: 'log4j-api', version: '2.12.0'\n        //compile group: 'org.apache.logging.log4j', name: 'log4j-core', version: '2.12.0'\n\n        compileOnly(\"org.jspecify:jspecify:1.0.0\")\n        testCompileOnly(\"org.jspecify:jspecify:1.0.0\")\n        def eisop_version = \"3.42.0-eisop4\"\n        compileOnly \"io.github.eisop:checker-qual:$eisop_version\"\n        compileOnly \"io.github.eisop:checker-util:$eisop_version\"\n        testCompileOnly \"io.github.eisop:checker-qual:$eisop_version\"\n        checkerFramework \"io.github.eisop:checker:$eisop_version\"\n\n        testImplementation(\"ch.qos.logback:logback-classic:1.5.12\")\n        testImplementation 'org.junit.jupiter:junit-jupiter-api:5.11.3'\n        testImplementation 'org.junit.jupiter:junit-jupiter-params:5.11.3'\n        testImplementation project(':key.util')\n\n        testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.11.3'\n    }\n\n    tasks.withType(JavaCompile) {\n        // Setting UTF-8 as the java source encoding.\n        options.encoding = \"UTF-8\"\n        // Setting the release to Java 21\n        options.release = 21\n    }\n\n    tasks.withType(Javadoc) {\n        failOnError = false\n        options.addBooleanOption 'Xdoclint:none', true\n        //options.verbose()\n        options.encoding = 'UTF-8'\n        options.addBooleanOption('html5', true)\n    }\n\n    tasks.withType(Test) {//Configure all tests\n        systemProperty \"test-resources\", \"src/test/resources\"\n        systemProperty \"testcases\", \"src/test/resources/testcase\"\n        systemProperty \"TACLET_PROOFS\", \"tacletProofs\"\n        systemProperty \"EXAMPLES_DIR\", file(\"$rootProject/key.ui/examples\")\n        systemProperty \"RUNALLPROOFS_DIR\", \"$buildDir/report/runallproves\"\n\n        systemProperty \"key.disregardSettings\", \"true\"\n        maxHeapSize = \"4g\"\n\n        forkEvery = 0 //default\n        maxParallelForks = 1 // weigl: test on master\n    }\n\n    tasks.withType(Test) {\n        useJUnitPlatform {\n            includeEngines 'junit-vintage'\n            includeEngines 'junit-jupiter'\n        }\n    }\n\n\n    test {\n        // Before we switched to JUnit 5, we used JUnit 4 with a customized discovery of test class.\n        // This discovery called AutoSuite and searched in the compiled classes. AutoSuite was\n        // necessary due to bugs caused in some execution order.\n        // AutoSuites made the test order deterministic. A last known commit to find AutoSuite (for the case),\n        // is 980294d04008f6b3798986bce218bac2753b4783.\n\n        useJUnitPlatform {\n            excludeTags \"owntest\", \"interactive\", \"performance\"\n        }\n\n        afterTest { desc, result -> logger.error \"${result.resultType}: ${desc.className}#${desc.name}\" }\n        beforeTest { desc -> logger.error \"> ${desc.className}#${desc.name}\" }\n\n        testLogging {\n            outputs.upToDateWhen { false }\n            showStandardStreams = true\n        }\n    }\n\n    task testFast(type: Test) {\n        group \"verification\"\n        useJUnitPlatform {\n            excludeTags \"slow\", \"performance\", \"interactive\"\n        }\n\n        testLogging {\n            // set options for log level LIFECYCLE\n            events \"failed\"\n            exceptionFormat \"short\"\n\n            // set options for log level DEBUG\n            debug {\n                events \"started\", \"skipped\", \"failed\"\n                exceptionFormat \"full\"\n            }\n\n            // remove standard output/error logging from --info builds\n            // by assigning only 'failed' and 'skipped' events\n            info.events = [\"failed\", \"skipped\"]\n        }\n    }\n\n    // The following two tasks can be used to execute main methods from the project\n    // The main class is set via \"gradle -DmainClass=... execute --args ...\"\n    // see https://stackoverflow.com/questions/21358466/gradle-to-execute-java-class-without-modifying-build-gradle\n    task execute(type: JavaExec) {\n        description 'Execute main method from the project. Set main class via \"gradle -DmainClass=... execute --args ...\"'\n        group \"application\"\n        mainClass.set(System.getProperty('mainClass'))\n        classpath = sourceSets.main.runtimeClasspath\n    }\n\n    task executeInTests(type: JavaExec) {\n        description 'Execute main method from the project (tests loaded). Set main class via \"gradle -DmainClass=... execute --args ...\"'\n        group \"application\"\n        mainClass.set(System.getProperty('mainClass'))\n        classpath = sourceSets.test.runtimeClasspath\n    }\n\n    // findbugs { findbugsTest.enabled = false; ignoreFailures = true }\n    pmd {\n        pmdTest.enabled = false\n        ignoreFailures = true\n        toolVersion = \"6.53.0\"\n        consoleOutput = false\n        rulesMinimumPriority = 5\n        ruleSets = [\"category/java/errorprone.xml\", \"category/java/bestpractices.xml\"]\n    }\n\n    task pmdMainChanged(type: Pmd) {\n        // Specify all files that should be checked\n        def changedFiles = getChangedFiles()\n        source pmdMain.source.filter { f -> f.getAbsoluteFile().toString() in changedFiles }\n        classpath = checkstyleMain.classpath\n        reports {\n            html {\n                enabled true\n                outputLocation = file(\"build/reports/pmd/main_diff.html\")\n            }\n            xml {\n                enabled true\n                outputLocation = file(\"build/reports/pmd/main_diff.xml\")\n            }\n        }\n    }\n\n\n    checkstyle {\n        toolVersion = \"10.6.0\"\n        ignoreFailures = true\n        configFile file(\"$rootDir/gradle/key_checks.xml\")\n        showViolations = false // disable console output\n    }\n\n\n    task checkstyleMainChanged(type: Checkstyle) {\n        // Specify all files that should be checked\n        def changedFiles = getChangedFiles()\n        source checkstyleMain.source.filter { f -> f.getAbsoluteFile().toString() in changedFiles }\n        classpath = checkstyleMain.classpath\n\n        //println(source.getFiles())\n\n        // Define the output folder of the generated reports\n        reports {\n            html {\n                enabled true\n                outputLocation = file(\"build/reports/checkstyle/main_diff.html\")\n            }\n            xml {\n                enabled true\n                outputLocation = file(\"build/reports/checkstyle/main_diff.xml\")\n            }\n        }\n    }\n\n\n    // tasks.withType(FindBugs) {\n    //     reports {\n    //         xml.enabled = false\n    //         html.enabled = true\n    //     }\n    // }\n    tasks.withType(Pmd) {\n        reports {\n            xml.getRequired().set(true)\n            html.getRequired().set(true)\n        }\n    }\n\n    task sourcesJar(type: Jar) {\n        description = 'Create a jar file with the sources from this project'\n        from sourceSets.main.allJava\n        archiveClassifier = 'sources'\n    }\n\n    task javadocJar(type: Jar) {\n        description = 'Create a jar file with the javadocs from this project'\n        from javadoc\n        archiveClassifier = 'javadoc'\n    }\n\n    license {//configures the license file header\n        header = file(\"$rootDir/gradle/header\")\n\n        mapping {\n            //find styles here:\n            // http://code.mycila.com/license-maven-plugin/#supported-comment-types\n            java = \"SLASHSTAR_STYLE\" // DOUBLESLASH_STYLE\n            javascript = \"SLASHSTAR_STYLE\"\n        }\n        mapping(\"key\", \"SLASHSTAR_STYLE\")\n    }\n\n    eclipse { //configures the generated .project and .classpath files.\n        classpath {\n            file {\n                whenMerged { // This adds the exclude entry for every resource and antlr folder.\n                    //As eclipse is so stupid, that it does not distuinguish between resource and java folder correctly.\n                    entries.findAll { it.path.endsWith('src/test/antlr') }.each { it.excludes = [\"**/*.java\"] }\n                    entries.findAll { it.path.endsWith('/resources') }.each { it.excludes = [\"**/*.java\"] }\n                }\n            }\n        }\n    }\n\n    spotless {\n        // see https://github.com/diffplug/spotless/tree/main/plugin-gradle\n\n        // optional: limit format enforcement to just the files changed by this feature branch\n        // ratchetFrom 'origin/master'\n\n        format('Key') {\n            // define the files to apply `misc` to\n            //target '*.gradle', '*.md', '.gitignore'\n            target 'src/main/resources/**/*.key'\n            trimTrailingWhitespace()\n            //indentWithSpaces(4)       // this does not really work\n            endWithNewline()\n            // TODO: license headers are problematic at the moment,\n            //  see https://git.key-project.org/key/key/-/wikis/KaKeY%202022-09-30\n            //licenseHeaderFile(\"$rootDir/gradle/header\", '\\\\s*\\\\\\\\\\\\w+')\n        }\n\n        antlr4 {\n            target 'src/*/antlr4/**/*.g4' // default value, you can change if you want\n            //licenseHeaderFile \"$rootDir/gradle/header\"\n        }\n\n        java {\n            //target(\"*.java\")\n            // don't need to set target, it is inferred from java\n\n            // We ignore the build folder to avoid double checks and checks of generated code.\n            targetExclude 'build/**'\n\n            // allows us to use spotless:off / spotless:on to keep pre-formatted sections\n            // MU: Only ... because of the eclipse(...) below, it is \"@formatter:off\" and \"@formatter:on\"\n            // that must be used instead.\n            toggleOffOn()\n\n            removeUnusedImports()\n\n            /* When new options are added in new versions of the Eclipse formatter, the easiest way is to export the new\n             * style file from the Eclipse GUI and then use the CodeStyleMerger tool in\n             * \"$rootDir/scripts/tools/checkstyle/CodeStyleMerger.java\" to merge the old and the new style files,\n             * i.e. \"java CodeStyleMerger.java <oldStyleFile> <newStyleFile> keyCodeStyle.xml\". The tool adds all\n             * entries with keys that were not present in the old file and optionally overwrites the old entries. The\n             * file is output with ordered keys, such that the file can easily be diffed using git.\n             */\n            eclipse().configFile(\"$rootDir/scripts/tools/checkstyle/keyCodeStyle.xml\")\n            trimTrailingWhitespace()        // not sure how to set this in the xml file ...\n            //googleJavaFormat().aosp().reflowLongStrings()\n\n            // note: you can use an empty string for all the imports you didn't specify explicitly,\n            // '|' to join group without blank line, and '\\\\#` prefix for static imports\n            importOrder('java|javax', 'de.uka', 'org.key_project', '', '\\\\#')\n\n            // specific delimiter: normally just 'package', but spotless crashes for files in default package\n            // (see https://github.com/diffplug/spotless/issues/30), therefore 'import' is needed. '//' is for files\n            // with completely commented out code (which would probably better just be removed in future).\n            if(project.name == 'recoder') {\n                licenseHeaderFile(\"$rootDir/gradle/header-recoder\", '(package|import|//)')\n            }else {\n                licenseHeaderFile(\"$rootDir/gradle/header\", '(package|import|//)')\n            }\n        }\n    }\n\n//    checkerFramework {\n//        checkers = [\n//            \"org.checkerframework.checker.nullness.NullnessChecker\",\n//        ]\n//        extraJavacArgs = [\n//            \"-AonlyDefs=^org\\\\.key_project\\\\.util\",\n//            \"-Xmaxerrs\", \"10000\",\n//            \"-Astubs=$projectDir/src/main/checkerframework\",\n//            \"-Werror\",\n//            \"-Aversion\",\n//        ]\n//    }\n\n    afterEvaluate { // required so project.description is non-null as set by sub build.gradle\n        publishing {\n            publications {\n                mavenJava(MavenPublication) {\n                    from components.java\n                    artifact sourcesJar\n                    artifact javadocJar\n                    pom {\n                        name = projects.name\n                        description = project.description\n                        url = 'https://key-project.org/'\n\n                        licenses {\n                            license {\n                                name = \"GNU General Public License (GPL), Version 2\"\n                                url = \"https://www.gnu.org/licenses/old-licenses/gpl-2.0.html\"\n                            }\n                        }\n\n                        developers {\n                            developer {\n                                id = 'key'\n                                name = 'KeY Developers'\n                                email = 'support@key-project.org'\n                                url = \"https://www.key-project.org/about/people/\"\n                            }\n                        }\n                        scm {\n                            connection = 'scm:git:git://github.com/keyproject/key.git'\n                            developerConnection = 'scm:git:git://github.com/keyproject/key.git'\n                            url = 'https://github.com/keyproject/key/'\n                        }\n                    }\n                }\n            }\n            repositories {\n                maven {\n                    /**\n                     * To be able to publish things on Maven Central, you need two things:\n                     *\n                     * (1) a JIRA account with permission on group-id `org.key-project`\n                     * (2) a keyserver-published GPG (w/o sub-keys)\n                     *\n                     * Your `$HOME/.gradle/gradle.properties` should like this:\n                     * ```\n                     * signing.keyId=YourKeyId\n                     * signing.password=YourPublicKeyPassword\n                     * ossrhUsername=your-jira-id\n                     * ossrhPassword=your-jira-password\n                     * ```\n                     *\n                     * You can test signing with `gradle sign`, and publish with `gradle publish`.\n                     * https://central.sonatype.org/publish/publish-guide/\n                     */\n                    if (project.version.endsWith(\"-SNAPSHOT\")) {\n                        name = \"mavenSnapshot\"\n                        url = \"https://s01.oss.sonatype.org/content/repositories/snapshots/\"\n                        credentials(PasswordCredentials) {\n                            username = project.properties.getOrDefault(\"ossrhUsername\", \"\")\n                            password = project.properties.getOrDefault(\"ossrhPassword\", \"\")\n                        }\n                    } else {\n                        name = \"mavenStaging\"\n                        url = \"https://s01.oss.sonatype.org/service/local/staging/deploy/maven2/\"\n                        credentials(PasswordCredentials) {\n                            username = project.properties.getOrDefault(\"ossrhUsername\", \"\")\n                            password = project.properties.getOrDefault(\"ossrhPassword\", \"\")\n                        }\n                    }\n                }\n\n                /*\n            maven { // deployment to git.key-project.org\n                name = \"GitlabPackages\"\n                url \"https://git.key-project.org/api/v4/projects/35/packages/maven\"\n                credentials(HttpHeaderCredentials) {\n                    if (System.getenv(\"TOKEN\") != null) {\n                        name = 'Private-Token'\n                        value = System.getenv(\"TOKEN\")\n                    } else {\n                        name = 'Job-Token'\n                        value = System.getenv(\"CI_JOB_TOKEN\")\n                    }\n                }\n                authentication {\n                    header(HttpHeaderAuthentication)\n                }\n            }\n            */\n            }\n        }\n\n        signing {\n            useGpgCmd() // works better than the Java implementation, which requires PGP keyrings.\n            sign publishing.publications.mavenJava\n        }\n    }\n}\n\ntask start {\n    description \"Use :key.ui:run instead\"\n    doFirst {\n        println \"Use :key.ui:run instead\"\n    }\n}\n\n// Generation of a JavaDoc across sub projects.\ntask alldoc(type: Javadoc) {\n    group \"documentation\"\n    description \"Generate a JavaDoc across sub projects\"\n    def projects = subprojects\n    //key.ui javadoc is broken\n    source projects.collect { it.sourceSets.main.allJava }\n    classpath = files(projects.collect { it.sourceSets.main.compileClasspath })\n    destinationDir = file(\"${buildDir}/docs/javadoc\")\n\n    if (JavaVersion.current().isJava9Compatible()) {\n        //notworking on jenkins\n        //options.addBooleanOption('html5', true)\n    }\n\n    configure(options) {\n        //showFromPrivate()\n        encoding = 'UTF-8'\n        addBooleanOption 'Xdoclint:none', true\n        // overview = new File( projectDir, 'src/javadoc/package.html' )\n        //stylesheetFile = new File( projectDir, 'src/javadoc/stylesheet.css' )\n        windowTitle = 'KeY API Documentation'\n        docTitle = \"KeY JavaDoc ($project.version) -- ${getDate()}\"\n        bottom = \"Copyright &copy; 2003-2023 <a href=\\\"http://key-project.org\\\">The KeY-Project</a>.\"\n        use = true\n        links += \"https://docs.oracle.com/en/java/javase/17/docs/api/\"\n        links += \"http://www.antlr2.org/javadoc/\"\n        links += \"http://www.antlr3.org/api/Java/\"\n        links += \"https://www.antlr.org/api/Java/\"\n    }\n}\n\n// Creates a jar file with the javadoc over all sub projects.\ntask alldocJar(type: Zip) {\n    dependsOn alldoc\n    description 'Create a jar file with the javadoc over all sub projects'\n    from alldoc\n    archiveFileName = \"key-api-doc-${project.version}.zip\"\n    destinationDirectory = file(\"$buildDir/distribution\")\n}\n\n//conditionally enable jacoco coverage when `-DjacocoEnabled=true` is given on CLI.\ndef jacocoEnabled = System.properties.getProperty(\"jacocoEnabled\") ?: \"false\"\nif (jacocoEnabled.toBoolean()) {\n    project.logger.lifecycle(\"Jacoco enabled. Test performance will be slower.\")\n    apply from: rootProject.file(\"scripts/jacocokey.gradle\")\n}\n\n\n@Memoized\ndef getChangedFiles() {\n    // Get the target and source branch\n    def anchor = \"git merge-base HEAD origin/main\".execute().getText()\n\n    // Get list of all changed files including status\n    def allFiles = \"git diff --name-status --diff-filter=dr $anchor\".execute().getText().split(\"\\n\")\n    //println(\"Found ${allFiles.length} changed files\")\n\n    // Remove the status prefix\n    def files = new TreeSet<String>()\n    for (file in allFiles) {\n        if (file.length() > 1) {\n            def a = file.substring(1).trim()\n            if (!a.isBlank()) {\n                files.add((\"$rootDir/\" + a).toString())\n            }\n        }\n    }\n    // Return the list of touched files\n    files\n}\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/keymaera-x",
            "repo_link": "https://github.com/LS-Lab/KeYmaeraX-release",
            "content": {
                "codemeta": "",
                "readme": "# KeYmaera X Theorem Prover for Hybrid Systems\n\nSelf-driving cars, autonomous robots, modern airplanes, or robotic surgery:\nwe increasingly entrust our lives to computers and therefore should strive\nfor nothing but the highest safety standards - mathematical correctness proof.\nProofs for such cyber-physical systems can be constructed with the KeYmaera X prover.\nAs a _hybrid systems_ theorem prover,\nKeYmaera X analyzes the control program and the physical behavior\nof the controlled system together in _differential dynamic logic_.\n\nKeYmaera X features a minimal core of just about 2000 lines of code\nthat isolates all soundness-critical reasoning.\nSuch a small and simple prover core makes it much easier to trust verification results.\nPre-defined and custom tactics built on top of the core drive automated proof search.\nKeYmaera X comes with a web-based front-end that provides a clean interface\nfor both interactive and automated proving,\nhighlighting the most crucial parts of a verification activity.\nBesides hybrid systems,\nKeYmaera X also supports the verification of _hybrid games_ in _differential game logic_.\n\n**More information** and precompiled binaries are available at [keymaerax.org](https://keymaerax.org/):\n\n* The [KeYmaera X Tutorial](https://keymaeraX.org/Xtutorial.html)\n* The [Logical Foundations of Cyber-Physical Systems](http://lfcps.org/lfcps/) textbook\n* The [KeYmaera X API Documentation](https://keymaerax.org/scaladoc)\n\n## Installation\n\n1. Install Java Runtime Environment version 11 or later, for example from [OpenJDK](https://openjdk.org/)\n\n2. Optionally, install and set up\n  [Wolfram Mathematica](https://www.wolfram.com/mathematica/) (version 10 or later)\n  or [Wolfram Engine](http://www.wolfram.com/engine/).\n  See below for more details on the different arithmetic solvers.\n\n3. Download the file [keymaerax.jar](https://keymaerax.org/keymaerax.jar)\n\n4. Configure KeYmaera X according to the **Configuration** section below.\n\n5. Launch KeYmaera X by opening the console in the same directory as `keymaerax.jar`\n  and running `java -jar keymaerax.jar`.\n  You might need to specify additional arguments as explained in the **Configuration** section.\n\nFor more details on installation, usage, and for troubleshooting steps,\nsee the [Install section of the website](https://keymaerax.org/download.html).\n\n## Configuration\n\nKeYmaera X requires a decision procedure for real arithmetic to finalize proofs.\nIt is compatible with these arithmetic solvers:\n- Wolfram Mathematica\n- Wolfram Engine, a free alternative to Mathematica that requires an active internet connection.\n- The Z3 Theorem Prover, for which built-in binaries are included.\n  This is the fallback when no other solver is configured.\n\nKeYmaera X is extensively tested with Mathematica and some features are only available when using Mathematica.\nAfter starting KeYmaera X you can configure arithmetic tools in the _KeYmaera X->Preferences_ menu.\n\nDepending on the operating system, Mathematica is installed in different locations.\nIf KeYmaera X can't find your Mathematica installation,\nyou need to manually specify the kernel and jlink paths when starting KeYmaera X\nusing the `-mathkernel` and `-jlink` parameters.\n\nIf you installed Mathematica at the\n[default path](https://reference.wolfram.com/language/tutorial/WolframSystemFileOrganization.html),\nthe required values on the different platforms are:\n\n- Linux\n  - `-mathkernel /usr/local/Wolfram/Mathematica/13.0/Executables/MathKernel`\n  - `-jlink /usr/local/Wolfram/Mathematica/13.0/SystemFiles/Links/JLink/SystemFiles/Libraries/Linux-x86-64`\n- macOS\n  - `-mathkernel /Applications/Mathematica.app/Contents/MacOS/MathKernel`\n  - `-jlink /Applications/Mathematica.app/Contents/SystemFiles/Links/JLink/SystemFiles/Libraries/MacOSX-x86-64`\n- Windows\n  - `-mathkernel \"C:\\Program Files\\Wolfram Research\\Mathematica\\13.0\\MathKernel.exe\"`\n  - `-jlink \"C:\\Program Files\\Wolfram Research\\Mathematica\\13.0\\SystemFiles\\Links\\JLink\\SystemFiles\\Libraries\\Windows-x86-64\"`\n\n## Building\n\nTo compile KeYmaera X from source or set up a development environment, see [procedures.md](doc/procedures.md).\nMore detailed but outdated instructions are available\n[in the wiki on GitHub](https://github.com/LS-Lab/KeYmaeraX-release/wiki/Building-Instructions).\n\n## Publications\n\nKeYmaera X implements the uniform substitution calculus for differential dynamic logic\nin order to enable soundness assurance by way of a small trusted LCF-style kernel\nwhile still being amenable to automatic theorem proving.\n\nhttps://www.ls.cs.cmu.edu/publications.html\n\n1. André Platzer.\n  [A complete uniform substitution calculus for differential dynamic logic](https://doi.org/10.1007/s10817-016-9385-1).\n  Journal of Automated Reasoning 59(2), pp. 219-266, 2017.\n  Extended version of [CADE-25](https://doi.org/10.1007/978-3-319-21401-6_32).\n\n2. André Platzer.\n  [Logics of dynamical systems](https://doi.org/10.1109/LICS.2012.13).\n  ACM/IEEE Symposium on Logic in Computer Science, LICS 2012, June 25–28, 2012, Dubrovnik, Croatia, pages 13-24. IEEE 2012.\n\n3. Nathan Fulton, Stefan Mitsch, Jan-David Quesel, Marcus Völp and André Platzer.\n  [KeYmaera X: An axiomatic tactical theorem prover for hybrid systems](https://doi.org/10.1007/978-3-319-21401-6_36).\n  In Amy P. Felty and Aart Middeldorp, editors, International Conference on Automated Deduction, CADE-25, Berlin, Germany, Proceedings, LNCS. Springer, 2015.\n\n4. Nathan Fulton, Stefan Mitsch, Brandon Bohrer and André Platzer.\n  [Bellerophon: Tactical theorem proving for hybrid systems](https://doi.org/10.1007/978-3-319-66107-0_14).\n  In Mauricio Ayala-Rincón and César Muñoz, editors, Interactive Theorem Proving, International Conference, ITP 2017, volume 10499 of LNCS, pp. 207-224. Springer, 2017.\n\n5. André Platzer.\n  [Logical Foundations of Cyber-Physical Systems](http://lfcps.org/lfcps/).\n  Springer, Cham, 2018.\n  [DOI](https://doi.org/10.1007/978-3-319-63588-0), [Videos](http://video.lfcps.org/)\n\nThe soundness assurances provided by a small LCF-style kernel are further strengthened\nby a cross-verification of the soundness theorem for the uniform substitution calculus.\n\n6. Brandon Bohrer, Vincent Rahli, Ivana Vukotic, Marcus Völp and André Platzer.\n  [Formally verified differential dynamic logic](https://doi.org/10.1145/3018610.3018616).\n  ACM SIGPLAN Conference on Certified Programs and Proofs, CPP 2017, Jan 16-17, 2017, Paris, France, pages 208-221, ACM, 2017.\n  [Isabelle/HOL](https://github.com/LS-Lab/Isabelle-dL) and [Coq](https://github.com/LS-Lab/Coq-dL)\n\nA secondary goal of KeYmaera X is to also make it possible to implement extensions of differential dynamic logic,\nsuch as differential game logic for hybrid games\nas well as quantified differential dynamic logic for distributed hybrid systems:\n\n7. André Platzer.\n  [Differential game logic](https://doi.org/10.1145/2817824).\n  ACM Trans. Comput. Log. 17(1), 2015.\n\n8. André Platzer.\n  [Differential hybrid games](https://doi.org/10.1145/3091123).\n  ACM Trans. Comput. Log. 18(3), 2017.\n\n9. André Platzer.\n  [A complete axiomatization of quantified differential dynamic logic for distributed hybrid systems](https://doi.org/10.2168/LMCS-8(4:17)2012).\n  Logical Methods in Computer Science 8(4), pages 1-44, 2012.\n\nKeYmaera X implements fast generalized uniform substitution algorithms, also cross-verified:\n\n10. André Platzer.\n  [Uniform substitution for differential game logic](https://doi.org/10.1007/978-3-319-94205-6_15).\n  In Didier Galmiche, Stephan Schulz and Roberto Sebastiani, editors, Automated Reasoning, 9th International Joint Conference, IJCAR 2018, volume 10900 of LNCS, pp. 211-227. Springer 2018.\n\n11. André Platzer.\n  [Uniform substitution at one fell swoop](https://doi.org/10.1007/978-3-030-29436-6_25).\n  In Pascal Fontaine, editor, International Conference on Automated Deduction, CADE-27, volume 11716 of LNCS, pp. 425-441. Springer, 2019.\n  [Isabelle/HOL](http://isa-afp.org/entries/Differential_Game_Logic.html)\n\nAutomatic proofs for differential equation invariants are based on:\n\n12. André Platzer and Yong Kiam Tan.\n  [Differential equation invariance axiomatization](https://doi.org/10.1145/3380825).\n  J. ACM 67(1), 6:1-6:66, 2020.\n  Extended version of [LICS'18](https://doi.org/10.1145/3209108.3209147).\n\nLiveness proofs for differential equations are based on:\n\n13. Yong Kiam Tan and André Platzer.\n  Yong Kiam Tan and André Platzer.\n  [An axiomatic approach to existence and liveness for differential equations](https://doi.org/10.1007/s00165-020-00525-0).\n  Formal Aspects of Computing 33(4), pp 461-518, 2021.\n  Special issue for selected papers from [FM'19](https://doi.org/10.1007/978-3-030-30942-8_23).\n\nKeYmaera X uses the [Pegasus](http://pegasus.keymaeraX.org/) tool\nfor invariant generation (which gets better when additional software is installed):\n\n14. Andrew Sogokon, Stefan Mitsch, Yong Kiam Tan, Katherine Cordwell and André Platzer.\n  [Pegasus: Sound continuous invariant generation](https://doi.org/10.1007/s10703-020-00355-z).\n  Formal Methods in System Design, 58(1), pp. 5-41, 2022.\n  Special issue for selected papers from [FM'19](https://doi.org/10.1007/978-3-030-30942-8_10).\n\nKeYmaera X implements the [ModelPlex](http://modelplex.net) method\nto ensure that verification results about models apply to cyber-physical system implementations.\nModelPlex generates provably correct monitor conditions that, if checked to hold at runtime,\nare provably guaranteed to imply that the offline safety verification results about the CPS model\napply to the present run of the actual CPS implementation.\n\n15. Stefan Mitsch and André Platzer.\n  [ModelPlex: Verified runtime validation of verified cyber-physical system models](https://doi.org/10.1007/s10703-016-0241-z).\n  Formal Methods in System Design 49(1), pp. 33-74. 2016.\n  Special issue for selected papers from [RV'14](https://doi.org/10.1007/978-3-319-11164-3_17).\n\n16. Yong Kiam Tan, Stefan Mitsch and André Platzer.\n  [Verifying switched system stability with logic](https://doi.org/10.1145/3501710.3519541)\n  In Ezio Bartocci and Sylvie Putot, editors, Hybrid Systems: Computation and Control (part of CPS Week 2022), HSCC'22. Article No. 2, pp. 1-11. ACM, 2022.\n\nThe design principles for the user interface of KeYmaera X are described in:\n\n17. Stefan Mitsch and André Platzer.\n  [The KeYmaera X proof IDE: Concepts on usability in hybrid systems theorem proving](https://doi.org/10.4204/EPTCS.240.5).\n  In Catherine Dubois, Paolo Masci and Dominique Méry, editors, 3rd Workshop on Formal Integrated Development Environment F-IDE 2016, volume 240 of EPTCS, pp. 67-81, 2017.\n\nModel and proof management techniques are described in:\n\n18. Stefan Mitsch.\n  [Implicit and Explicit Proof Management in KeYmaera X](https://doi.org/10.4204/EPTCS.338.8)\n  In José Proença and Andrei Paskevich, editors, 6th Workshop on Formal Integrated Development Environment F-IDE 2021, volume 338 of EPTCS 338, pp. 53-67, 2021.\n\nA comparison of KeYmaera X with its predecessor provers is described in:\n\n19. Stefan Mitsch and André Platzer.\n  [A Retrospective on Developing Hybrid System Provers in the KeYmaera Family: A Tale of Three Provers](https://doi.org/10.1007/978-3-030-64354-6_2).\n  In Wolfgang Ahrendt et al., editors, Deductive Software Verification: Future Perspectives, volume 12345 of LNCS, pp. 21-64. Springer, 2020.\n\n## Copyright and Licenses\n\nCopyright (C) 2014-2024 Carnegie Mellon University, Karlsruhe Institute of Technology\n\nDeveloped by Andre Platzer, Stefan Mitsch, Nathan Fulton, Brandon Bohrer,\nYong Kiam Tan, Andrew Sogokon, Fabian Immler, Katherine Cordwell,\nEnguerrand Prebet, Joscha Mennicken, Tobias Erthal.\nWith previous contributions by Nathan Fulton, Jan-David Quesel,\nMarcus Voelp, Ran Ji.\nSee [COPYRIGHT.txt](COPYRIGHT.txt) for details.\n\nSee [LICENSE.txt](LICENSE.txt) for the conditions of using this software.\n\nThe KeYmaera X distribution contains external tools.\nA list of tools and their licenses can be found in [LICENSES_THIRD_PARTY.txt](LICENSES_THIRD_PARTY.txt).\n\n## Contact\n\nKeYmaera X developers: keymaerax@keymaerax.org\n\n",
                "dependencies": "import java.io.{FileInputStream, InputStreamReader}\nimport java.nio.charset.StandardCharsets\nimport java.nio.file.{Files, Paths}\nimport java.util.Properties\n\nThisBuild / scalaVersion := \"2.13.13\"\nThisBuild / version := \"5.1.1\"\n\nThisBuild / scalacOptions ++= Seq(\n  // Always show all non-suppressed warnings. See `scalac -Wconf:help` for more info.\n  // https://www.scala-lang.org/2021/01/12/configuring-and-suppressing-warnings.html\n  \"-Wconf:any:w\",\n  \"-Ymacro-annotations\",\n)\n\nThisBuild / assemblyMergeStrategy := {\n  // Multiple dependency jars have a module-info.class file in the same location.\n  // Without custom rules, they cause merge conflicts with sbt-assembly.\n  // Since we're building an uberjar, it should be safe to discard them (according to stackoverflow).\n  // https://stackoverflow.com/a/55557287\n  case PathList(elements @ _*) if elements.last == \"module-info.class\" => MergeStrategy.discard\n\n  // https://github.com/sbt/sbt-assembly#merge-strategy\n  case path =>\n    val oldStrategy = (ThisBuild / assemblyMergeStrategy).value\n    oldStrategy(path)\n}\n\n// Never execute tests in parallel across all sub-projects\nGlobal / concurrentRestrictions += Tags.limit(Tags.Test, 1)\n\nlazy val macros = project\n  .in(file(\"keymaerax-macros\"))\n  .disablePlugins(AssemblyPlugin)\n  .settings(\n    name := \"KeYmaeraX Macros\",\n\n    libraryDependencies += \"org.scala-lang\" % \"scala-reflect\" % scalaVersion.value,\n  )\n\nlazy val core = project\n  .in(file(\"keymaerax-core\"))\n  .enablePlugins(BuildInfoPlugin)\n  .dependsOn(macros)\n  .settings(\n    name := \"KeYmaeraX Core\",\n    mainClass := Some(\"org.keymaerax.cli.KeymaeraxCore\"),\n\n    libraryDependencies += \"org.scala-lang\" % \"scala-compiler\" % scalaVersion.value,\n\n    libraryDependencies += \"cc.redberry\" %% \"rings.scaladsl\" % \"2.5.8\",\n    libraryDependencies += \"com.github.scopt\" %% \"scopt\" % \"4.1.0\",\n    libraryDependencies += \"com.lihaoyi\" %% \"fastparse\" % \"3.1.0\",\n    libraryDependencies += \"io.github.classgraph\" % \"classgraph\" % \"4.8.174\",\n    libraryDependencies += \"io.spray\" %% \"spray-json\" % \"1.3.6\",\n    libraryDependencies += \"org.apache.commons\" % \"commons-configuration2\" % \"2.10.1\",\n    libraryDependencies += \"org.apache.commons\" % \"commons-lang3\" % \"3.14.0\",\n    libraryDependencies += \"org.reflections\" % \"reflections\" % \"0.10.2\",\n    libraryDependencies += \"org.typelevel\" %% \"paiges-core\" % \"0.4.3\",\n    libraryDependencies += \"org.typelevel\" %% \"spire\" % \"0.18.0\",\n\n    // Logging\n    //\n    // KeYmaera X and some of its dependencies use slf4j for logging.\n    // Slf4j is a facade for various logging frameworks called \"providers\".\n    // Forcing slf4j 2 usually works fine for dependencies that create logs,\n    // but forcing an slf4j 1 provider to use slf4j 2 will break things.\n    // Since some dependencies updated to slf4j 2 already, we have to use an slf4j 2 provider\n    // and force all other dependencies to use slf4j 2 as well via sbt's default version conflict resolution.\n    //\n    // https://github.com/jokade/slogging?tab=readme-ov-file#getting-started\n    // https://www.baeldung.com/slf4j-with-log4j2-logback#Log4j2\n    libraryDependencies += \"biz.enef\" %% \"slogging-slf4j\" % \"0.6.2\",\n    libraryDependencies += \"org.apache.logging.log4j\" % \"log4j-api\" % \"2.23.1\",\n    libraryDependencies += \"org.apache.logging.log4j\" % \"log4j-core\" % \"2.23.1\",\n    libraryDependencies += \"org.apache.logging.log4j\" % \"log4j-slf4j2-impl\" % \"2.23.1\",\n\n    // A published version of scala-smtlib that works with Scala 2.13\n    // https://github.com/regb/scala-smtlib/issues/46#issuecomment-955691728\n    // https://mvnrepository.com/artifact/com.regblanc/scala-smtlib_2.13/0.2.1-42-gc68dbaa\n    libraryDependencies += \"com.regblanc\" %% \"scala-smtlib\" % \"0.2.1-42-gc68dbaa\",\n\n    Compile / run / mainClass := mainClass.value,\n    assembly / mainClass := mainClass.value,\n    assembly / assemblyJarName := s\"${normalizedName.value}-${version.value}.jar\",\n\n    // Include version number as constant in source code\n    buildInfoKeys := Seq[BuildInfoKey](\n      version,\n      \"copyright\" -> Files.readString(Paths.get(\"COPYRIGHT.txt\")),\n      \"license\" -> Files.readString(Paths.get(\"LICENSE.txt\")),\n      \"licensesThirdParty\" -> Files.readString(Paths.get(\"LICENSES_THIRD_PARTY.txt\")),\n    ),\n    buildInfoPackage := \"org.keymaerax.info\",\n    buildInfoOptions += BuildInfoOption.PackagePrivate,\n\n    // Use Mathematica's JLink.jar as unmanaged dependency\n    // The path is read from the property mathematica.jlink.path in the file local.properties\n    Compile / unmanagedJars += {\n      val properties = new Properties()\n      try {\n        val stream = new FileInputStream(\"local.properties\")\n        val reader = new InputStreamReader(stream, StandardCharsets.UTF_8)\n        properties.load(reader)\n      } catch {\n        case e: Throwable => throw new Exception(\"Failed to load file local.properties\", e)\n      }\n      val jlinkPath: String = properties.getProperty(\"mathematica.jlink.path\")\n      if (jlinkPath == null) {\n        throw new Exception(\"Property mathematica.jlink.path not found in file local.properties\")\n      }\n      file(jlinkPath)\n    },\n  )\n\nlazy val webui = project\n  .in(file(\"keymaerax-webui\"))\n  .dependsOn(macros, core)\n  .settings(\n    name := \"KeYmaeraX WebUI\",\n    mainClass := Some(\"org.keymaerax.cli.KeymaeraxWebui\"),\n\n    /// sqlite driver\n    libraryDependencies += \"com.typesafe.slick\" %% \"slick\" % \"3.5.1\",\n    libraryDependencies += \"com.typesafe.slick\" %% \"slick-codegen\" % \"3.5.1\",\n    libraryDependencies += \"org.xerial\" % \"sqlite-jdbc\" % \"3.45.3.0\",\n\n    // Akka\n    libraryDependencies += \"com.typesafe.akka\" %% \"akka-http\" % \"10.5.3\",\n    libraryDependencies += \"com.typesafe.akka\" %% \"akka-http-spray-json\" % \"10.5.3\",\n    libraryDependencies += \"com.typesafe.akka\" %% \"akka-http-xml\" % \"10.5.3\",\n    libraryDependencies += \"com.typesafe.akka\" %% \"akka-slf4j\" % \"2.8.5\",\n    libraryDependencies += \"com.typesafe.akka\" %% \"akka-stream\" % \"2.8.5\",\n    libraryDependencies += \"io.spray\" %% \"spray-json\" % \"1.3.6\",\n\n    Compile / run / mainClass := mainClass.value,\n    assembly / mainClass := mainClass.value,\n    assembly / assemblyJarName := s\"${normalizedName.value}-${version.value}.jar\",\n\n    // Include some resources as triggers for triggered execution (~)\n    watchTriggers += baseDirectory.value.toGlob / \"src\" / \"main\" / \"resources\" / \"partials\" / \"*.html\",\n    watchTriggers += baseDirectory.value.toGlob / \"src\" / \"main\" / \"resources\" / \"js\" / \"*.{js,map}\",\n    watchTriggers += baseDirectory.value.toGlob / \"src\" / \"main\" / \"resources\" / \"*.html\",\n\n    /////////////\n    // Testing //\n    /////////////\n\n    libraryDependencies += \"org.scalatest\" %% \"scalatest\" % \"3.2.18\" % Test,\n    libraryDependencies += \"org.scalamock\" %% \"scalamock\" % \"5.2.0\" % Test,\n\n    // For generating HTML scalatest reports using the `-h <directory>` flag\n    // See \"Using Reporters\" in https://www.scalatest.org/user_guide/using_scalatest_with_sbt\n    // https://stackoverflow.com/a/59059383\n    // https://github.com/scalatest/scalatest/issues/1736\n    libraryDependencies += \"com.vladsch.flexmark\" % \"flexmark-all\" % \"0.64.8\" % Test,\n\n    Test / parallelExecution := false,\n\n    // set fork to true in order to run tests in their own Java process.\n    // not forking avoids broken pipe exceptions in test reporter, but forking might become necessary in certain\n    // multithreaded setups (see ScalaTest documentation)\n    Test / fork := false,\n\n    // record and report test durations\n    Test / testOptions += Tests.Argument(TestFrameworks.ScalaTest, \"-oD\"),\n\n    // report long-running tests (report every hour for tests that run longer than 1hr)\n    Test / testOptions += Tests.Argument(TestFrameworks.ScalaTest, \"-W\", \"3600\", \"3600\"),\n\n    resolvers ++= Resolver.sonatypeOssRepos(\"snapshots\"), // ScalaMeter\n    testFrameworks += new TestFramework(\"org.scalameter.ScalaMeterFramework\"),\n\n    logBuffered := false,\n  )\n\n// build KeYmaera X full jar with sbt clean assembly\nlazy val root = project\n  .in(file(\".\"))\n  .aggregate(macros, core, webui)\n  .enablePlugins(ScalaUnidocPlugin)\n  .disablePlugins(AssemblyPlugin)\n  .settings(\n    name := \"KeYmaeraX\",\n\n    Compile / doc / scalacOptions ++= Seq(\"-doc-root-content\", \"rootdoc.txt\"),\n    ScalaUnidoc / unidoc / scalacOptions += \"-Ymacro-expand:none\",\n    ScalaUnidoc / unidoc / unidocProjectFilter := inAnyProject -- inProjects(macros),\n  )\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kinfit",
            "repo_link": "https://github.com/KinFit/KinFit.git",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kramersmoyal",
            "repo_link": "https://github.com/LRydin/KramersMoyal",
            "content": {
                "codemeta": "",
                "readme": "[![DOI](https://joss.theoj.org/papers/10.21105/joss.01693/status.svg)](https://doi.org/10.21105/joss.01693)\n![PyPI - License](https://img.shields.io/pypi/l/kramersmoyal) ![PyPI](https://img.shields.io/pypi/v/kramersmoyal) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/kramersmoyal)\n[![Build Status](https://github.com/LRydin/KramersMoyal/actions/workflows/CI.yml/badge.svg)](https://github.com/LRydin/KramersMoyal/actions/workflows/CI.yml)\n[![codecov](https://codecov.io/gh/LRydin/KramersMoyal/branch/master/graph/badge.svg)](https://codecov.io/gh/LRydin/KramersMoyal) [![Documentation Status](https://readthedocs.org/projects/kramersmoyal/badge/?version=latest)](https://kramersmoyal.readthedocs.io/en/latest/?badge=latest)\n\n# KramersMoyal\n`kramersmoyal` is a python package designed to obtain the Kramers–Moyal coefficients, or conditional moments, from stochastic data of any dimension. It employs kernel density estimations, instead of a histogram approach, to ensure better results for low number of points as well as allowing better fitting of the results.\n\nThe [paper](https://doi.org/10.21105/joss.01693) is now officially published on [JOSS](https://joss.theoj.org/). The paper is also available [here](/paper/paper.pdf), or you can find it in the [ArXiv](https://arxiv.org/abs/1912.09737).\n\n# Installation\nTo install `kramersmoyal`, just use `pip`\n\n```\npip install kramersmoyal\n```\nThen on your favourite editor just use\n```python\nfrom kramersmoyal import km\n```\n\n## Dependencies\nThe library depends on `numpy` and `scipy`.\n\n# A one-dimensional stochastic process\n\nA Jupyter notebook with this example can be found [here](/examples/kmc.ipynb)\n\n## The theory\nTake, for example, the well-documented one-dimension Ornstein–Uhlenbeck process, also known as Va&#353;&#237;&#269;ek process, see [here](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process). This process is governed by two main parameters: the mean-reverting parameter &theta; and the diffusion parameter &sigma;\n\n<img src=\"/other/OU_eq.png\" title=\"Ornstein–Uhlenbeck process\" height=\"25\"/>\n\nwhich can be solved in various ways. For our purposes, recall that the drift coefficient, i.e., the first-order Kramers–Moyal coefficient, is given by ![](/other/inline_KM_1.png) and the second-order Kramers–Moyal coefficient is ![](/other/inline_KM_2.png), i.e., the diffusion.\n\nGenerate an exemplary Ornstein–Uhlenbeck process with your favourite integrator, e.g., the [Euler–Maruyama](https://en.wikipedia.org/wiki/Euler%E2%80%93Maruyama_method) or with a more powerful tool from [`JiTCSDE`](https://github.com/neurophysik/jitcsde) found on GitHub.\nFor this example let's take &theta;=.3 and &sigma;=.1, over a total time of 500 units, with a sampling of 1000 Hertz, and from the generated data series retrieve the two parameters, the drift -&theta;y(t) and diffusion &sigma;.\n\n## Integrating an Ornstein–Uhlenbeck process\nHere is a short code on generating a Ornstein–Uhlenbeck stochastic trajectory with a simple Euler–Maruyama integration method\n\n```python\n# integration time and time sampling\nt_final = 500\ndelta_t = 0.001\n\n# The parameters theta and sigma\ntheta = 0.3\nsigma = 0.1\n\n# The time array of the trajectory\ntime = np.arange(0, t_final, delta_t)\n\n# Initialise the array y\ny = np.zeros(time.size)\n\n# Generate a Wiener process\ndw = np.random.normal(loc=0, scale=np.sqrt(delta_t), size=time.size)\n\n# Integrate the process\nfor i in range(1,time.size):\n    y[i] = y[i-1] - theta*y[i-1]*delta_t + sigma*dw[i]\n```\n\nFrom here we have a plain example of an Ornstein–Uhlenbeck process, always drifting back to zero, due to the mean-reverting drift &theta;. The effect of the noise can be seen across the whole trajectory.\n\n<img src=\"/other/fig1.png\" title=\"Ornstein–Uhlenbeck process\" height=\"200\"/>\n\n## Using `kramersmoyal`\nTake the timeseries `y` and let's study the Kramers–Moyal coefficients. For this let's look at the drift and diffusion coefficients of the process, i.e., the first and second Kramers–Moyal coefficients, with an `epanechnikov` kernel\n```python\n\n# The kmc holds the results, where edges holds the binning space\nkmc, edges = km(y, powers=2)\n```\n\nThis results in\n\n<img src=\"/other/fig2.png\" title=\"Drift and diffusion terms of an Ornstein–Uhlenbeck process\" height=\"200\"/>\n\nNotice here that to obtain the Kramers–Moyal coefficients you need to divide `kmc` by the timestep `delta_t`. This normalisation stems from the Taylor-like approximation, i.e., the Kramers–Moyal expansion (`delta t` &rarr; 0).\n\n# A two-dimensional diffusion process\n\nA Jupyter notebook with this example can be found [here](/examples/kmc.ipynb)\n\n## Theory\n\nA two-dimensional diffusion process is a stochastic process that comprises two ![](/other/inline_W.png) and allows for a mixing of these noise terms across its two dimensions.\n\n<img src=\"/other/2D-diffusion.png\" alt=\"2D-diffusion\" title=\"A 2-dimensional diffusion process\" height=\"60\" />\n\nwhere we will select a set of state-dependent parameters obeying\n\n<img src=\"/other/parameters_2D-diffusion.png\" alt=\"2D-diffusion\" title=\"Specific parameters for the diffusion process\" height=\"70\" />\n\nwith ![](/other/inline_parameters_2D-diffusion_1.png) and ![](/other/inline_parameters_2D-diffusion_2.png).\n\n## Choice of parameters\nAs an example, let's take the following set of parameters for the drift vector and diffusion matrix\n\n```python\n# integration time and time sampling\nt_final = 2000\ndelta_t = 0.001\n\n# Define the drift vector N\nN = np.array([2.0, 1.0])\n\n# Define the diffusion matrix g\ng = np.array([[0.5, 0.0], [0.0, 0.5]])\n\n# The time array of the trajectory\ntime = np.arange(0, t_final, delta_t)\n```\n\n## Integrating a 2-dimensional process\nIntegrating the previous stochastic trajectory with a simple Euler–Maruyama integration method\n\n```python\n# Initialise the array y\ny = np.zeros([time.size, 2])\n\n# Generate two Wiener processes with a scale of np.sqrt(delta_t)\ndW = np.random.normal(loc=0, scale=np.sqrt(delta_t), size=[time.size, 2])\n\n# Integrate the process (takes about 20 secs)\nfor i in range(1, time.size):\n    y[i,0] = y[i-1,0]  -  N[0] * y[i-1,0] * delta_t + g[0,0]/(1 + np.exp(y[i-1,0]**2)) * dW[i,0]  +  g[0,1] * dW[i,1]\n    y[i,1] = y[i-1,1]  -  N[1] * y[i-1,1] * delta_t + g[1,0] * dW[i,0]  +  g[1,1]/(1 + np.exp(y[i-1,1]**2)) * dW[i,1]\n```\n\nThe stochastic trajectory in 2 dimensions for `10` time units (`10000` data points)\n\n<img src=\"/other/fig3.png\" alt=\"2D-diffusion\" title=\"2-dimensional trajectory\" height=\"280\" />\n\n## Back to `kramersmoyal` and the Kramers–Moyal coefficients\nFirst notice that all the results now will be two-dimensional surfaces, so we will need to plot them as such\n\n```python\n# Choose the size of your target space in two dimensions\nbins = [100, 100]\n\n# Introduce the desired orders to calculate, but in 2 dimensions\npowers = np.array([[0,0], [1,0], [0,1], [1,1], [2,0], [0,2], [2,2]])\n# insert into kmc:   0      1      2      3      4      5      6\n\n# Notice that the first entry in [,] is for the first dimension, the\n# second for the second dimension...\n\n# Choose a desired bandwidth bw\nbw = 0.1\n\n# Calculate the Kramers−Moyal coefficients\nkmc, edges = km(y, bw=bw, bins=bins, powers=powers)\n\n# The K−M coefficients are stacked along the first dim of the\n# kmc array, so kmc[1,...] is the first K−M coefficient, kmc[2,...]\n# is the second. These will be 2-dimensional matrices\n```\n\nNow one can visualise the Kramers–Moyal coefficients (surfaces) in green and the respective theoretical surfaces in black. (Don't forget to normalise: `kmc / delta_t`).\n\n<img src=\"/other/fig4.png\" alt=\"2D-diffusion\" title=\"2-dimensional Kramers–Moyal surfaces (green) and the theoretical surfaces (black)\" height=\"480\" />\n\n# Contributions\nWe welcome reviews and ideas from everyone. If you want to share your ideas or report a bug, open an [issue](https://github.com/LRydin/KramersMoyal/issues) here on GitHub, or contact us directly.\nIf you need help with the code, the theory, or the implementation, do not hesitate to contact us, we are here to help.\nWe abide to a [Conduct of Fairness](contributions.md).\n\n# TODOs\nNext on the list is\n- Include more kernels\n- Work through the documentation carefully\n\n# Changelog\n- Version 0.4.1 - Changing CI. Correcting `kmc[0,:]` normalisation. Various Simplifications. Bins as ints, powers as ints.\n- Version 0.4.0 - Added the documentation, first testers, and the Conduct of Fairness\n- Version 0.3.2 - Adding 2 kernels: `triagular` and `quartic` and extending the documentation and examples.\n- Version 0.3.1 - Corrections to the fft triming after convolution.\n- Version 0.3.0 - The major breakthrough: Calculates the Kramers–Moyal coefficients for data of any dimension.\n- Version 0.2.0 - Introducing convolutions and `gaussian` and `uniform` kernels. Major speed up in the calculations.\n- Version 0.1.0 - One and two dimensional Kramers–Moyal coefficients with an `epanechnikov` kernel.\n\n# Literature and Support\n\n### Literature\nThe study of stochastic processes from a data-driven approach is grounded in extensive mathematical work. From the applied perspective there are several references to understand stochastic processes, the Fokker–Planck equations, and the Kramers–Moyal expansion\n\n- Tabar, M. R. R. (2019). *Analysis and Data-Based Reconstruction of Complex Nonlinear Dynamical Systems.* Springer, International Publishing\n- Risken, H. (1989). *The Fokker–Planck equation.* Springer, Berlin, Heidelberg.\n- Gardiner, C.W. (1985). *Handbook of Stochastic Methods.* Springer, Berlin.\n\nYou can find and extensive review on the subject [here](http://sharif.edu/~rahimitabar/pdfs/80.pdf)<sup>1</sup>\n\n### History\nThis project was started in 2017 at the [neurophysik](https://www.researchgate.net/lab/Klaus-Lehnertz-Lab-2) by Leonardo Rydin Gorjão, Jan Heysel, Klaus Lehnertz, and M. Reza Rahimi Tabar. Francisco Meirinhos later devised the hard coding to python. The project is now supported by Dirk Witthaut and the [Institute of Energy and Climate Research Systems Analysis and Technology Evaluation](https://www.fz-juelich.de/iek/iek-ste/EN/Home/home_node.html).\n\n### Funding\nHelmholtz Association Initiative _Energy System 2050 - A Contribution of the Research Field Energy_ and the grant No. VH-NG-1025 and *STORM - Stochastics for Time-Space Risk Models* project of the Research Council of Norway (RCN) No. 274410.\n\n---\n\n<sup>1</sup> Friedrich, R., Peinke, J., Sahimi, M., Tabar, M. R. R. *Approaching complexity by stochastic methods: From biological systems to turbulence,* [Phys. Rep. 506, 87–162 (2011)](https://doi.org/10.1016/j.physrep.2011.05.003).\n\n",
                "dependencies": "numpy\nscipy\n\nimport setuptools\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=\"kramersmoyal\",\n    version=\"0.4.1\",\n    author=\"Leonardo Rydin Gorjão and Francisco Meirinhos\",\n    author_email=\"leonardo.rydin@gmail.com\",\n    description=\"Calculate Kramers-Moyal coefficients for stochastic process of any dimension, up to any order.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/LRydin/KramersMoyal\",\n    packages=setuptools.find_packages(),\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires='>=3.7',\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/lapy",
            "repo_link": "https://github.com/Deep-MI/lapy",
            "content": {
                "codemeta": "",
                "readme": "[![PyPI version](https://badge.fury.io/py/lapy.svg)](https://pypi.org/project/lapy/)\r\n# LaPy\r\n\r\nLaPy is an open-source Python package for differential geometry on triangle\r\nand tetrahedra meshes. It includes an FEM solver to estimate the Laplace,\r\nPoisson or Heat equations. Further functionality includes the computations\r\nof gradients, divergence, mean-curvature flow, conformal mappings, \r\ngeodesics, ShapeDNA (Laplace spectra), and IO and plotting methods. \r\n\r\nLaPy is written purely in Python 3 without sacrificing speed as almost all\r\nloops are vectorized, drawing upon efficient and sparse mesh data structures.\r\n\r\n## Contents:\r\n\r\n- **TriaMesh**: a class for triangle meshes offering various operations, such as\r\n  fixing orientation, smoothing, curvature, boundary, quality, normals, and\r\n  various efficient mesh datastructures (edges, adjacency matrices). IO from\r\n  OFF, VTK and other formats.\r\n- **TetMesh**: a class for tetrahedral meshes (orientation, boundary, IO ...)\r\n- **Solver**: a class for linear FEM computation (Laplace stiffness and mass\r\n  matrix, fast and sparse eigenvalue solver, anisotropic Laplace, Poisson)\r\n- **io**: module for IO of vertex functions and eigenvector files\r\n- **diffgeo**: module for gradients, divergence, mean curvature flow, etc.\r\n- **heat**: module for heat kernel and diffusion\r\n- **shapedna**: module for the ShapeDNA descriptor of surfaces and solids\r\n- **plot**: module for interactive visualizations (wrapping plotly)\r\n\r\n## Usage:\r\n\r\nThe LaPy package is a comprehensive collection of scripts, so we refer to the\r\n'help' function and docstring of each module / function / class for usage info.\r\nFor example:\r\n\r\n```\r\nimport lapy as lp\r\nhelp(lp.TriaMesh)\r\nhelp(lp.Solver)\r\n```\r\n\r\nIn the `examples` subdirectory, we provide several Jupyter notebooks that\r\nillustrate prototypical use cases of the toolbox.\r\n\r\n## Installation:\r\n\r\nUse the following code to install the latest release of LaPy into your local\r\nPython package directory:\r\n\r\n`python3 -m pip install lapy`\r\n\r\nUse the following code to install the dev package in editable mode to a location of\r\nyour choice:\r\n\r\n`python3 -m pip install --user --src /my/preferred/location --editable git+https://github.com/Deep-MI/Lapy.git#egg=lapy`\r\n\r\nSeveral functions, e.g. the Solver, require a sparse matrix decomposition, for which either the LU decomposition (from scipy sparse, default) or the faster Cholesky decomposition (from scikit-sparse cholmod, recommended) can be used. If the parameter flag use_cholmod is True, the code will try to import cholmod from the scikit-sparse package. If this fails, an error will be thrown. If you would like to use cholmod, you need to install scikit-sparse separately, as pip currently cannot install it (conda can). scikit-sparse requires numpy and scipy to be installed separately beforehand.\r\n\r\n## API Documentation\r\n\r\nThe API Documentation can be found at https://deep-mi.org/LaPy .\r\n\r\n## References:\r\n\r\nIf you use this software for a publication please cite both these papers:\r\n\r\n**[1]** Laplace-Beltrami spectra as 'Shape-DNA' of surfaces and solids. Reuter M, Wolter F-E, Peinecke N. Computer-Aided Design. 2006;38(4):342-366. http://dx.doi.org/10.1016/j.cad.2005.10.011\r\n\r\n**[2]** BrainPrint: a discriminative characterization of brain morphology. Wachinger C, Golland P, Kremen W, Fischl B, Reuter M. Neuroimage. 2015;109:232-48. http://dx.doi.org/10.1016/j.neuroimage.2015.01.032 http://www.ncbi.nlm.nih.gov/pubmed/25613439\r\n\r\nShape-DNA [1] introduces the FEM methods and the Laplace spectra for shape analysis, while BrainPrint [2] focusses on medical applications.\r\n\r\nFor Geodesics please also cite:\r\n\r\n[3] Crane K, Weischedel C, Wardetzky M. Geodesics in heat: A new approach to computing distance based on heat flow. ACM Transactions on Graphics. https://doi.org/10.1145/2516971.2516977\r\n\r\nFor non-singular mean curvature flow please cite:\r\n\r\n[4] Kazhdan M, Solomon J, Ben-Chen M. 2012. Can Mean-Curvature Flow be Modified to be Non-singular? Comput. Graph. Forum 31, 5, 1745–1754.\r\nhttps://doi.org/10.1111/j.1467-8659.2012.03179.x\r\n\r\nFor conformal mapping please cite:\r\n\r\n[5] Choi PT, Lam KC, Lui LM. FLASH: Fast Landmark Aligned Spherical Harmonic Parameterization for Genus-0 Closed Brain Surfaces. SIAM Journal on Imaging Sciences, vol. 8, no. 1, pp. 67-94, 2015. https://doi.org/10.1137/130950008\r\n\r\nWe invite you to check out our lab webpage at https://deep-mi.org\r\n\n",
                "dependencies": "[build-system]\nrequires = [\n    'setuptools >= 61.0.0',\n    'numpy>=2',\n]\nbuild-backend = 'setuptools.build_meta'\n\n[project]\nname = 'lapy'\nversion = '1.2.0'\ndescription = 'A package for differential geometry on meshes (Laplace, FEM)'\nreadme = 'README.md'\nlicense = {file = 'LICENSE'}\nrequires-python = '>=3.9'\nauthors = [\n    {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'},\n]\nmaintainers = [\n    {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'},\n]\nkeywords = [\n    'python',\n    'Laplace',\n    'FEM',\n    'ShapeDNA',\n    'BrainPrint',\n    'Triangle Mesh',\n    'Tetrahedra Mesh',\n    'Geodesics in Heat',\n    'Mean Curvature Flow',\n]\nclassifiers = [\n    'Operating System :: Microsoft :: Windows',\n    'Operating System :: Unix',\n    'Operating System :: MacOS',\n    'Programming Language :: Python :: 3 :: Only',\n    'Programming Language :: Python :: 3.9',\n    'Programming Language :: Python :: 3.10',\n    'Programming Language :: Python :: 3.11',\n    'Programming Language :: Python :: 3.12',\n    'Natural Language :: English',\n    'License :: OSI Approved :: MIT License',\n    'Intended Audience :: Science/Research',\n]\ndependencies = [\n    'nibabel',\n    'numpy>=1.21',\n    'plotly',\n    'psutil',\n    'scipy!=1.13.0',\n]\n\n[project.optional-dependencies]\nbuild = [\n    'build',\n    'twine',\n]\nchol = [\n    'scikit-sparse',\n]\ndoc = [\n    'furo!=2023.8.17',\n    'matplotlib',\n    'memory-profiler',\n    'numpydoc',\n    'sphinx!=7.2.*',\n    'sphinxcontrib-bibtex',\n    'sphinx-copybutton',\n    'sphinx-design',\n    'sphinx-gallery',\n    'sphinx-issues',\n    'pypandoc',\n    'nbsphinx',\n    'IPython', # For syntax highlighting in notebooks\n    'ipykernel',\n]\nstyle = [\n    'bibclean',\n    'codespell',\n    'pydocstyle[toml]',\n    'ruff',\n]\ntest = [\n    'pytest',\n    'pytest-cov',\n    'pytest-timeout',\n]\nall = [\n    'lapy[build]',\n    'lapy[chol]',\n    'lapy[doc]',\n    'lapy[style]',\n    'lapy[test]',\n]\nfull = [\n    'lapy[all]',\n]\n\n[project.urls]\nhomepage = 'https://Deep-MI.github.io/LaPy/dev/index.html'\ndocumentation = 'https://Deep-MI.github.io/LaPy/dev/index.html'\nsource = 'https://github.com/Deep-MI/LaPy'\ntracker = 'https://github.com/Deep-MI/LaPy/issues'\n\n[project.scripts]\nlapy-sys_info = 'lapy.commands.sys_info:run'\n\n[tool.setuptools]\ninclude-package-data = false\n\n[tool.setuptools.packages.find]\ninclude = ['lapy*']\nexclude = ['lapy*tests']\n\n[tool.pydocstyle]\nconvention = 'numpy'\nignore-decorators = '(copy_doc|property|.*setter|.*getter|pyqtSlot|Slot)'\nmatch = '^(?!setup|__init__|test_).*\\.py'\nmatch-dir = '^lapy.*'\nadd_ignore = 'D100,D104,D107'\n\n[tool.ruff]\nline-length = 88\nextend-exclude = [\n    \"doc\",\n    \".github\",\n    \"data\",\n]\n\n[tool.ruff.lint]\n# https://docs.astral.sh/ruff/linter/#rule-selection\nselect = [\n    \"E\",   # pycodestyle\n    \"F\",   # Pyflakes\n    \"UP\",  # pyupgrade\n    \"B\",   # flake8-bugbear\n    \"I\",   # isort\n    # \"SIM\", # flake8-simplify\n]\n\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\"F401\"]\n\"examples/*\" = [\"E501\"]   # ignore too long lines in example ipynb\n\n[tool.pytest.ini_options]\nminversion = '6.0'\naddopts = '--durations 20 --junit-xml=junit-results.xml --verbose'\nfilterwarnings = []\n\n[tool.coverage.run]\nbranch = true\ncover_pylib = false\nomit = [\n    '**/__init__.py',\n    '**/lapy/_version.py',\n    '**/lapy/commands/*',\n    '**/tests/**',\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    'pragma: no cover',\n    'if __name__ == .__main__.:',\n]\nprecision = 2\n\nfrom setuptools import setup\n\nsetup()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/libertem",
            "repo_link": "https://github.com/LiberTEM/LiberTEM/",
            "content": {
                "codemeta": "",
                "readme": "|docs|_ |gitter|_ |azure|_ |github|_ |codeclimate|_ |precommit|_ |joss|_ |zenodo|_ |pypi|_ |condaforge|_\n\n.. |docs| image:: https://img.shields.io/badge/%F0%9F%95%AE-docs-green.svg\n.. _docs: https://libertem.github.io/LiberTEM/\n\n.. |gitter| image:: https://badges.gitter.im/join_chat.svg\n.. _gitter: https://gitter.im/LiberTEM/Lobby\n\n.. |azure| image:: https://dev.azure.com/LiberTEM/LiberTEM/_apis/build/status/LiberTEM.LiberTEM-data?branchName=master\n.. _azure: https://dev.azure.com/LiberTEM/LiberTEM/_build/latest?definitionId=4&branchName=master\n\n.. |zenodo| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.1477847.svg\n.. _zenodo: https://doi.org/10.5281/zenodo.1477847\n\n.. |github| image:: https://img.shields.io/badge/GitHub-MIT-informational\n.. _github: https://github.com/LiberTEM/LiberTEM/\n\n.. |codeclimate| image:: https://api.codeclimate.com/v1/badges/dee042f64380f64737e5/maintainability\n.. _codeclimate: https://codeclimate.com/github/LiberTEM/LiberTEM\n\n.. |joss| image:: https://joss.theoj.org/papers/10.21105/joss.02006/status.svg\n.. _joss: https://doi.org/10.21105/joss.02006\n\n.. |precommit| image:: https://results.pre-commit.ci/badge/github/LiberTEM/LiberTEM/master.svg\n.. _precommit: https://results.pre-commit.ci/latest/github/LiberTEM/LiberTEM/master\n\n.. |pypi| image:: https://badge.fury.io/py/libertem.svg\n.. _pypi: https://pypi.org/project/libertem/\n\n.. |condaforge| image:: https://anaconda.org/conda-forge/libertem/badges/version.svg\n.. _condaforge: https://anaconda.org/conda-forge/libertem\n\nLiberTEM is an open source platform for high-throughput distributed processing\nof large-scale binary data sets and live data streams using a modified\n`MapReduce programming model <https://en.wikipedia.org/wiki/MapReduce>`_. The\ncurrent focus is `pixelated\n<https://en.wikipedia.org/wiki/Scanning_transmission_electron_microscopy#Universal_detectors>`_\nscanning transmission electron microscopy (`STEM\n<https://en.wikipedia.org/wiki/Scanning_transmission_electron_microscopy>`_)\n\\[`MacLaren et al. (2016) <https://doi.org/10.1002/9783527808465.EMC2016.6284>`_,\n`Ophus (2019) <https://doi.org/10.1017/s1431927619000497>`_\\] and scanning electron\nbeam diffraction data.\n\nMapReduce-like processing allows to specify an algorithm through two functions:\nOne function that is mapped on portions of the input data, and another function\nthat merges (reduces) a partial result from this mapping step into the complete\nresult. A wide range of TEM and 4D STEM processing tasks can be expressed in\nthis fashion, see `Applications`_.\n\nThe UDF interface of LiberTEM offers a standardized, versatile API to decouple\nthe mathematical core of an algorithm from details of data source, parallelism,\nand use of results. Mapping and merging can be performed in any order and with\ndifferent subdivisions of the input data, including running parts of the\ncalculation concurrently. That means the same implementation can be used in a\nwide range of modalities, including massive scaling on clusters. Since each\nmerge step produces an intermediate result, this style of processing is suitable\nfor displaying live results from a running calculation in a GUI application and\nfor `processing live data streams <https://github.com/LiberTEM/LiberTEM-live>`_.\nA closed-loop feedback between processing and instrument control can be realized\nas well. See `User-defined functions\n<https://libertem.github.io/LiberTEM/udf.html>`_ for more details on the\nLiberTEM UDF interface.\n\nThe LiberTEM back-end offers `high throughput and scalability\n<https://libertem.github.io/LiberTEM/architecture.html>`_ on PCs, single server\nnodes, clusters and cloud services. On clusters it can use fast distributed\nlocal storage on high-performance SSDs. That way it achieves `very high\naggregate IO performance\n<https://libertem.github.io/LiberTEM/performance.html>`_ on a compact and\ncost-efficient system built from stock components. All CPU cores and CUDA\ndevices in a system can be used in parallel.\n\nLiberTEM is supported on Linux, Mac OS X and Windows. Other platforms that allow\ninstallation of Python 3.7+ and the required packages will likely work as well. The\nGUI is running in a web browser.\n\nInstallation\n------------\n\nThe short version:\n\n.. code-block:: shell\n\n    $ virtualenv -p python3 ~/libertem-venv/\n    $ source ~/libertem-venv/bin/activate\n    (libertem-venv) $ python -m pip install \"libertem[torch]\"\n\n    # optional for GPU support\n    # See also https://docs.cupy.dev/en/stable/install.html\n    (libertem-venv) $ python -m pip install cupy\n\nPlease see `our documentation\n<https://libertem.github.io/LiberTEM/install.html>`_ for details!\n\nAlternatively, to run the `LiberTEM Docker image\n<https://libertem.github.io/LiberTEM/deployment/clustercontainer.html>`_:\n\n.. code-block:: shell\n\n    $ docker run -p localhost:9000:9000 --mount type=bind,source=/path/to/your/data/,dst=/data/,ro ghcr.io/libertem/libertem\n\nor\n\n.. code-block:: shell\n\n    $ singularity exec docker://ghcr.io/libertem/libertem /venv/bin/libertem-server\n\nDeployment for offline data processing on a single-node system for a local user\nis thoroughly tested and can be considered stable. Deployment on a cluster is\nexperimental and still requires some additional work, see `Issue #105\n<https://github.com/LiberTEM/LiberTEM/issues/105>`_. Back-end support for live data processing\nis still experimental as well, see https://github.com/LiberTEM/LiberTEM-live.\n\nApplications\n------------\n\nSince LiberTEM is programmable through `user-defined functions (UDFs)\n<https://libertem.github.io/LiberTEM/udf.html>`_, it can be used for a wide\nrange of processing tasks on array-like data and data streams. The following\napplications have been implemented already:\n\n- Virtual detectors (virtual bright field, virtual HAADF, center of mass\n  \\[`Krajnak et al. (2016) <https://doi.org/10.1016/j.ultramic.2016.03.006>`_\\],\n  custom shapes via masks)\n- `Analysis of amorphous materials <https://libertem.github.io/LiberTEM/app/amorphous.html>`_\n- `Strain mapping <https://libertem.github.io/LiberTEM-blobfinder/>`_\n- `Off-axis electron holography reconstruction <https://libertem.github.io/LiberTEM-holo/>`_\n- `Single Side Band ptychography <https://ptychography-4-0.github.io/ptychography/>`_\n\nSome of these applications are available through an `interactive web GUI\n<https://libertem.github.io/LiberTEM/usage.html#gui-usage>`_. Please see `the\napplications section <https://libertem.github.io/LiberTEM/applications.html>`_\nof our documentation for details!\n\nThe Python API and user-defined functions (UDFs) can be used for complex\noperations such as arbitrary linear operations and other features like data\nexport. Example Jupyter notebooks are available in the `examples directory\n<https://github.com/LiberTEM/LiberTEM/tree/master/examples>`_. If you are having\ntrouble running the examples, please let us know by filing an issue or\nby `joining our Gitter chat <https://gitter.im/LiberTEM/Lobby>`_.\n\nLiberTEM is suitable as a high-performance processing backend for other\napplications, including live data streams. `Contact us\n<https://gitter.im/LiberTEM/Lobby>`_ if you are interested!\n\nLiberTEM is evolving rapidly and prioritizes features following user demand and\ncontributions. Currently we are working on `live data processing\n<https://github.com/LiberTEM/LiberTEM-live>`_, improving application support for sparse\ndata and event-based detectors, performance improvements for GPU processing, and implementing\nanalysis methods for various applications of pixelated\nSTEM and other large-scale detector data. If you like to influence the direction\nthis project is taking, or if you'd like to `contribute\n<https://libertem.github.io/LiberTEM/contributing.html>`_, please join our\n`gitter chat <https://gitter.im/LiberTEM/Lobby>`_ and our `general mailing list\n<https://groups.google.com/forum/#!forum/libertem>`_.\n\nFile formats\n------------\n\nLiberTEM currently opens most file formats used for pixelated STEM. See `our\ngeneral information on loading data\n<https://libertem.github.io/LiberTEM/formats.html>`_ and `format-specific\ndocumentation\n<https://libertem.github.io/LiberTEM/reference/dataset.html#formats>`_ for more\ninformation!\n\n- Raw binary files\n- NumPy .npy binary files\n- Thermo Fisher EMPAD detector \\[`Tate et al. (2016) <https://doi.org/10.1017/S1431927615015664>`_\\] files\n- `Quantum Detectors MIB format <https://quantumdetectors.com/products/merlinem/>`_\n- Nanomegas .blo block files\n- Direct Electron DE5 files (HDF5-based) and Norpix SEQ files for `DE-Series <https://directelectron.com/de-series-cameras/>`_ detectors\n- `Gatan K2 IS <https://web.archive.org/web/20180809021832/http://www.gatan.com/products/tem-imaging-spectroscopy/k2-camera>`_ raw format\n- Stacks of Gatan DM3 and DM4 files (via `openNCEM <https://github.com/ercius/openNCEM>`_)\n- Single-file Gatan DM4 scans when saved using C-ordering\n- FRMS6 from PNDetector pnCCD cameras \\[`Simson et al. (2015) <https://doi.org/10.1017/s1431927615011836>`_\\]\n  (currently alpha, gain correction still needs UI changes)\n- FEI SER files (via `openNCEM <https://github.com/ercius/openNCEM>`_)\n- MRC (via `openNCEM <https://github.com/ercius/openNCEM>`_)\n- HDF5-based formats such as HyperSpy files, NeXus and EMD\n- TVIPS binary files\n- Sparse data in Raw CSR (compressed sparse row) format, as is possible\n  to generate from event-based detectors\n- Please contact us if you are interested in support for an additional format!\n\nLive processing and detectors (experimental)\n--------------------------------------------\n\nSee `LiberTEM-live <https://libertem.github.io/LiberTEM-live/>`_!\n\nLicense\n-------\n\nLiberTEM is licensed under the MIT license.\n\nAcknowledgements\n----------------\n\nWe are very grateful for your continuing support for LiberTEM!\n\nSee `the acknowledgement page\n<https://libertem.github.io/acknowledgements.html>`_ for a list of authors and\ncontributors to LiberTEM and its subprojects. See also our info on `funding\n<https://libertem.github.io/#funding>`_ and `industry partners\n<https://libertem.github.io/#industry-partners>`_.\n\n",
                "dependencies": "[build-system]\nrequires = [\n    \"hatchling\",\n    \"hatch-fancy-pypi-readme\",\n]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"libertem\"\ndynamic = [\"version\", \"readme\"]\ndescription = \"Open pixelated STEM framework\"\nlicense = { file = \"LICENSE\" }\nrequires-python = \">=3.9.3\"\nauthors = [\n    { name = \"the LiberTEM team\", email = \"libertem-dev@googlegroups.com\" },\n]\nkeywords = [\n    \"electron\",\n    \"microscopy\",\n]\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Environment :: Console\",\n    \"Environment :: Web Environment\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: End Users/Desktop\",\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Natural Language :: English\",\n    \"Operating System :: MacOS :: MacOS X\",\n    \"Operating System :: Microsoft :: Windows\",\n    \"Operating System :: POSIX :: Linux\",\n    \"Programming Language :: JavaScript\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering\",\n    \"Topic :: Scientific/Engineering :: Physics\",\n    \"Topic :: Scientific/Engineering :: Visualization\",\n]\ndependencies = [\n    \"autopep8\",\n    \"click\",\n    \"cloudpickle\",\n    \"colorcet\",\n    \"dask!=2023.6.1\",\n    \"defusedxml\",\n    \"distributed>=2.19.0\",\n    \"h5py\",\n    \"ipympl\",\n    \"jsonschema\",\n    \"jupyter_ui_poll\",\n    \"matplotlib\",\n    \"nbconvert\",\n    \"nbformat\",\n    \"ncempy>=1.10\",\n    # Minimum constraints of numba for all Python versions we support\n    # See https://numba.readthedocs.io/en/stable/release-notes-overview.html\n    \"numba>=0.53;python_version < '3.10'\",\n    \"numba>=0.55;python_version < '3.11'\",\n    \"numba>=0.57;python_version < '3.12'\",\n    \"numba>=0.59;python_version < '3.13'\",\n    # for any future Python release, constrain numba to a recent version,\n    # otherwise, version resolution might try to install an ancient version\n    # that isn't constrained properly:\n    \"numba>=0.61;python_version >= '3.13'\",  \n    \"numexpr!=2.8.6\",\n    \"numpy\",\n    \"opentelemetry-api\",\n    \"pillow\",\n    \"psutil\",\n    \"pywin32!=226;platform_system==\\\"Windows\\\"\",\n    \"scikit-image\",\n    \"scikit-learn\",\n    \"scipy>=1.4.1\",\n    \"sparse\",\n    \"sparseconverter>=0.4.0\",\n    \"tblib\",\n    \"threadpoolctl>=3.0\",\n    \"tomli\",\n    \"tornado>=5\",\n    \"tqdm\",\n    \"typing-extensions\",\n]\n\n[project.optional-dependencies]\nbqplot = [\n    \"bqplot\",\n    \"bqplot-image-gl\",\n    \"ipython\",\n]\ncupy = [\n    \"cupy\",\n]\nhdbscan = [\n    \"hdbscan;( python_version!='3.11' or platform_system!='Windows')\",\n    \"hdbscan<=0.8.30;( python_version=='3.11' and platform_system=='Windows')\",\n]\nhdf5plugin = [\n    \"hdf5plugin\",\n]\ntorch = [\n    \"torch<1.12\",\n]\ntracing = [\n    \"opentelemetry-distro\",\n    \"opentelemetry-exporter-otlp\",\n]\n\n[project.scripts]\nlibertem-server = \"libertem.web.cli:main\"\nlibertem-worker = \"libertem.executor.cli:main\"\n\n[project.urls]\nRepository = \"https://github.com/LiberTEM/LiberTEM\"\nHomepage = \"https://libertem.github.io/LiberTEM/\"\n\n[tool.hatch.version]\npath = \"src/libertem/__version__.py\"\n\n[tool.hatch.build.targets.wheel]\nartifacts = [\n    \"/src/libertem/_baked_revision.py\",\n]\n\n[tool.hatch.build.targets.sdist]\nartifacts = [\n    \"/src/libertem/_baked_revision.py\",\n]\ninclude = [\n    \"/src\",\n    \"/tests\",\n    \"/client/src\",\n    \"/client/public\",\n    \"/client/types\",\n    \"/client/README.md\",\n    \"/client/*.*js*\",\n    \"/LICENSE\",\n    \"/README.rst\",\n    \"/pytest.ini\",\n    \"/conftest.py\",\n    \"/test_requirements.txt\",\n    \"/override_requirements.txt\",\n    \"/tox.ini\",\n    \"/.flake8\",\n]\nexclude = [\n    \"*.pyc\",\n    \"*.nbi\",\n    \"*.nbc\",\n    \"__pycache__\",\n    \".mypy_cache\",\n]\n\n[tool.hatch.build.hooks.custom]\n# this enables hatch_build.py\n\n[tool.hatch.metadata.hooks.fancy-pypi-readme]\n\"content-type\" = \"text/x-rst\"\n\n[[tool.hatch.metadata.hooks.fancy-pypi-readme.fragments]]\npath = \"README.rst\"\n\n[[tool.hatch.metadata.hooks.fancy-pypi-readme.substitutions]]\npattern = \":(cite|doc):`[^`]+` ?\"\nreplacement = \"\"\n\n[tool.coverage.run]\nbranch = true\ninclude = [\n    \"src/\"\n]\n\n[tool.coverage.report]\n# Regexes for lines to exclude from consideration\nexclude_lines = [\n    # Have to re-enable the standard pragma\n    \"pragma: no cover\",\n\n    # Don't complain about missing debug-only code:\n    \"def __repr__\",\n    \"if self.debug\",\n\n    # Don't complain about typing branches:\n    \"if TYPE_CHECKING\",\n    \"if typing.TYPE_CHECKING\",\n\n    # Don't complain if tests don't hit defensive assertion code:\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n\n    # Don't complain if non-runnable code isn't run:\n    \"if 0:\",\n    \"if False:\",\n    \"if __name__ == .__main__.:\",\n]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/lightning-uq-box",
            "repo_link": "https://github.com/lightning-uq-box/lightning-uq-box",
            "content": {
                "codemeta": "",
                "readme": "<p align=\"center\">\n<img src=\"https://github.com/lightning-uq-box/lightning-uq-box/blob/main/docs/_static/lettering.jpeg?raw=true\" alt=\"Lightning-UQ-Box logo\" width=\"600\" height=\"auto\" />\n</p>\n\n[![docs](https://readthedocs.org/projects/lightning-uq-box/badge/?version=latest)](https://lightning-uq-box.readthedocs.io/en/latest/)\n[![style](https://github.com/lightning-uq-box/lightning-uq-box/actions/workflows/style.yaml/badge.svg)](https://github.com/lightning-uq-box/lightning-uq-box/actions/workflows/style.yaml)\n[![tests](https://github.com/lightning-uq-box/lightning-uq-box/actions/workflows/tests.yaml/badge.svg)](https://github.com/lightning-uq-box/lightning-uq-box/actions/workflows/tests.yaml)\n[![codecov](https://codecov.io/gh/lightning-uq-box/lightning-uq-box/branch/main/graph/badge.svg?token=oa3Z3PMVOg)](https://app.codecov.io/gh/lightning-uq-box/lightning-uq-box)\n[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/lightning-uq-box/lightning-uq-box/blob/main/LICENSE)\n<a href=\"https://pytorch.org/get-started/locally/\"><img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white\"></a>\n<a href=\"https://pytorchlightning.ai/\"><img alt=\"Lightning\" src=\"https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white\"></a> &emsp;\n\n# lightning-uq-box\n\nThe lightning-uq-box is a PyTorch library that provides various Uncertainty Quantification (UQ) techniques for modern neural network architectures.\n\nWe hope to provide the starting point for a collaborative open source effort to make it easier for practitioners to include UQ in their workflows and\nremove possible barriers of entry. Additionally, we hope this can be a pathway to more easily compare methods across UQ frameworks and potentially enhance the development of new UQ methods for neural networks.\n\n*The project is currently under active development, but we nevertheless hope for early feedback, feature requests, or contributions. Please check the [Contribution Guide](https://lightning-uq-box.readthedocs.io/en/latest/contribute.html) for further information.*\n\nThe goal of this library is threefold:\n\n1. Provide implementations for a variety of Uncertainty Quantification methods for Modern Deep Neural Networks that work with a range of neural network architectures and have different theoretical underpinnings\n2. Make it easy to compare UQ methods on a given dataset\n3. Focus on reproducibility of experiments with minimum boiler plate code and standardized evaluation protocols\n\nTo this end, each UQ-Method is essentially just a [Lightning Module](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) which can be used with a [Lightning Data Module](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) and a [Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html) to execute training, evaluation and inference for your desired task. The library also utilizes the [Lightning Command Line Interface (CLI)](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.cli.LightningCLI.html) for better reproducibility of experiments and setting up experiments at scale.\n\n# Theory Guide\n\nFor a comprehensive document that provides more mathematical details for each method and generally forms the basis of our implementations, please see the [Theory Guide](./docs/api/Lightning_UQ_Box_Theory_Guide.pdf). As a living document, we plan to update it as the library encompasses more methods. If you have any questions, or find typos or errors, feel free to open an issue.\n\n# Installation\n\nThe recommended way to install the latest released version is via pip,\n\n```console\npip install lightning-uq-box\n```\n\nFor the latest development version you can run,\n\n```console\npip install git+https://github.com/lightning-uq-box/lightning-uq-box.git\n```\n\nThe package is also available for installation via conda or spack. You can find instructions in the [documention](https://lightning-uq-box.readthedocs.io/en/latest/installation.html)\n\n# UQ-Methods\n\nIn the tables that follow below, you can see what UQ-Method/Task combination is currently supported by the Lightning-UQ-Box via these indicators:\n\n- ✅ supported\n- ❌ not designed for this task\n- ⏳ in progress\n\nThe implemented methods are of course not exhaustive, as the number of new methods keeps increasing. For an overview of methods that we are tracking or are planning to support, take a look at [this issue](https://github.com/lightning-uq-box/lightning-uq-box/issues/43).\n\n## Classification of UQ-Methods\n\nThe following sections aims to give an overview of different UQ-Methods by grouping them according to some commonalities. We agree that there could be other groupings as well and welcome suggestions to improve this overview. We also follow this grouping for the API documentation in the hopes to make navigation easier.\n\n### Single Forward Pass Methods\n\n| UQ-Method                                     | Regression | Classification | Segmentation | Pixel Wise Regression |\n|-----------------------------------------------|:----------:|:--------------:|:------------:|:---------------------:|\n| Quantile Regression (QR)                      |     ✅     |       ❌       |      ❌      |          ✅           |\n| Deep Evidential (DE)                          |     ✅     |       ⏳       |      ⏳      |          ✅           |\n| Mean Variance Estimation (MVE)                |     ✅     |       ❌       |      ❌      |          ✅           |\n| ZigZag                                        |     ✅     |       ✅       |      ❌      |          ❌           |\n| Mixture Density Networks                      |     ✅     |       ❌       |      ❌      |          ⏳           |\n\n### Approximate Bayesian Methods\n\n| UQ-Method                                     | Regression | Classification | Segmentation | Pixel Wise Regression |\n|-----------------------------------------------|:----------:|:--------------:|:------------:|:---------------------:|\n| Bayesian Neural Network VI ELBO (BNN_VI_ELBO) |     ✅     |       ✅       |      ✅      |          ⏳           |\n| Bayesian Neural Network VI (BNN_VI)           |     ✅     |       ⏳       |      ⏳      |          ⏳           |\n| Deep Kernel Learning (DKL)                    |     ✅     |       ✅       |      ❌      |          ❌           |\n| Deterministic Uncertainty Estimation (DUE)    |     ✅     |       ✅       |      ❌      |          ❌           |\n| Laplace Approximation (Laplace)               |     ✅     |       ✅       |      ❌      |          ❌           |\n| Monte Carlo Dropout (MC-Dropout)              |     ✅     |       ✅       |      ✅      |          ✅           |\n| Stochastic Gradient Langevin Dynamics (SGLD)  |     ✅     |       ✅       |      ⏳      |          ⏳           |\n| Spectral Normalized Gaussian Process (SNGP)   |     ✅     |       ✅       |      ❌      |          ❌           |\n| Stochastic Weight Averaging Gaussian (SWAG)   |     ✅     |       ✅       |      ✅      |          ✅           |\n| Variational Bayesian Last Layer (VBLL)        |     ✅     |       ✅       |      ❌      |          ❌           |\n| Deep Ensemble                                 |     ✅     |       ✅       |      ✅      |          ✅           |\n| Masked Ensemble                               |     ✅     |       ✅       |      ⏳      |          ⏳           |\n| Density Uncertainty Layer                     |     ✅     |       ✅       |      ❌      |          ❌           |\n\n### Generative Models\n\n| UQ-Method                                     | Regression | Classification | Segmentation | Pixel Wise Regression |\n|-----------------------------------------------|:----------:|:--------------:|:------------:|:---------------------:|\n| Classification And Regression Diffusion (CARD)|     ✅     |       ✅       |      ❌      |          ❌           |\n| Probabilistic UNet                            |     ❌     |       ❌       |      ✅      |          ❌           |\n| Hierarchical Probabilistic UNet               |     ❌     |       ❌       |      ✅      |          ❌           |\n| Variational Auto-Encoder (VAE)                |     ❌     |       ❌       |      ❌      |          ✅           |\n\n### Post-Hoc methods\n\n| UQ-Method                                     | Regression | Classification | Segmentation | Pixel Wise Regression |\n|-----------------------------------------------|:----------:|:--------------:|:------------:|:---------------------:|\n| Test Time Augmentation (TTA)                  |     ✅     |       ✅       |      ⏳      |          ⏳           |\n| Temperature Scaling                           |     ❌     |       ✅       |      ⏳      |          ❌           |\n| Conformal Quantile Regression (Conformal QR)  |     ✅     |       ❌       |      ❌      |          ⏳           |\n| Regularized Adaptive Prediction Sets (RAPS)   |     ❌     |       ✅       |      ❌      |          ❌           |\n| Image to Image Conformal                      |     ❌     |       ❌       |      ❌      |          ✅           |\n\n# Tutorials\n\nWe try to provide many different tutorials so that users can get a better understanding of implemented methods and get a feel for how they apply to different problems.\nHead over to the [tutorials](https://lightning-uq-box.readthedocs.io/en/latest/tutorial_overview.html) page to get started. These tutorials can also be launched in google colab if you navigate to the rocket icon at the top of a tutorial page.\n\n# Documentation\nWe aim to provide an extensive documentation on all included UQ-methods that provide some theoretical background, as well as tutorials that illustrate these methods on toy datasets.\n\n# Citation\n\nIf you use this software in your work, please cite our [paper](https://arxiv.org/abs/2410.03390):\n\n```bibtex\n@article{Lehmann_Lightning_UQ_Box_2024,\n  author = {Lehmann, Nils and Gawlikowski, Jakob and Stewart, Adam J. and Jancauskas, Vytautas and Depeweg, Stefan and Nalisnick, Eric and Gottschling, Nina M.},\n  journal = {arXiv preprint arXiv:2410.03390},\n  title = {{Lightning UQ Box}: A Comprehensive Framework for Uncertainty Quantification in Deep Learning},\n  year = {2024}\n}\n```\n\n",
                "dependencies": "[build-system]\nrequires = [\n    # setuptools 61+ required for pyproject.toml support\n    \"setuptools>=61\",\n]\nbuild-backend = \"setuptools.build_meta\"\n\n# https://packaging.python.org/en/latest/specifications/declaring-project-metadata/\n[project]\nname = \"lightning-uq-box\"\ndescription = \"Lightning-UQ-Box: A toolbox for uncertainty quantification in deep learning\"\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\nlicense = {file = \"LICENSE\"}\nauthors = [\n    {name = \"Nils Lehmann\", email = \"n.lehmann@tum.de\"},\n]\nmaintainers = [\n    {name = \"Nils Lehmann\", email = \"n.lehmann@tum.de\"},\n]\nkeywords = [\"pytorch\", \"lightning\", \"uncertainty quantification\", \"conformal prediction\", \"bayesian deep learning\"]\n\nclassifiers = [\n    \"Development Status :: 3 - Alpha\",\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: Apache Software License\",\n    \"Operating System :: OS Independent\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n]\ndependencies = [\n    # einops 0.3+ required for einops.repeat\n    \"einops>=0.3\",\n    # lightning 2+ required for LightningCLI args + sys.argv support\n    \"lightning>=2.4.0\",\n    # matplotlib 3.5 required for Python 3.10 wheels\n    \"matplotlib>=3.5\",\n    # numpy 1.21.1+ required by Python 3.10 wheels\n    \"numpy>=1.21.1\",\n    # pandas 1.1.3+ required for Python 3.10 wheels\n    \"pandas>=1.1.3\",\n    # torch 1.12+ required by torchvision\n    \"torch>=2.0\",\n    # torchmetrics 0.10+ required for binary/multiclass/multilabel classification metrics\n    \"torchmetrics>=1.2\",\n    # torchvision 0.13+ required for torchvision.models._api.WeightsEnum\n    \"torchvision>=0.16.1\",\n    # scikit-learn\n    \"scikit-learn>=1.3\",\n    # for deep kernel learning and other GP models\n    \"gpytorch>=1.11\",\n    # Laplace Approximation\n    \"laplace-torch>=0.2.1\",\n    # Uncertainty toolbox metrics\n    \"uncertainty-toolbox>=0.1.1\",\n    # Kornia for Test Time Augmentations and other image processing\n    \"kornia>=0.6.9\",\n    # timm models for image classification and regression\n    \"timm>=0.9.2\",\n    # torchseg for segmentation, pixelwise regression\n    \"torchseg>=0.0.1a1\",\n    # saving pixelwise and segmentation predictions\n    \"h5py>=3.12.1\",\n    # exponential moving average for pytorch models\n    \"ema-pytorch>=0.7.0\"\n]\ndynamic = [\"version\"]\n\n[project.optional-dependencies]\ntests = [\n    ### Tests ###\n    # pytest 7.3+ required for tmp_path_retention_policy\n    \"pytest>=7.3\",\n    # pytest-cov 4+ required for pytest 7.2+ compatibility\n    \"pytest-cov>=4\",\n    # pytest lazy fixture\n    \"pytest-lazy-fixture>=0.6\",\n    # hydra-core\n    \"hydra-core>=1.3.2\",\n    # omegaconf\n    \"omegaconf>=2.3.0\",\n    # jsonargparse\n    \"jsonargparse[signatures]>=4.28.0\",\n    ## Style\n    # ruff 0.2+ required for [ruff.lint]\n    \"ruff>=0.2\",\n]\n\ndocs = [\n    ### Docs ###\n    # sphinx\n    \"sphinx>=4,<7\",\n    # ipywidgets 7+ required by nbsphinx\n    \"ipywidgets>=7\",\n    # notebooks with sphinx\n    \"nbsphinx>=0.8.5\",\n    # reat the docs theme\n    \"sphinx-book-theme>=1.0.1\",\n    # Extension for markdown\n    \"myst-parser>=2.0\",\n    # toggle dropdowns\n    \"sphinx-togglebutton>=0.3.2\",\n    # jupytext notebook creation from .py files\n    \"jupytext>=1.15.2\",\n    # ipykernel to run notebooks\n    \"ipykernel>=6.29.3\",\n]\n\n[project.scripts]\nuq-box = \"lightning_uq_box.main:main\"\n\n[tool.pytest.ini_options]\n# Skip slow tests by default\naddopts = \"-m 'not slow'\"\n# https://docs.pytest.org/en/latest/how-to/capture-warnings.html\nfilterwarnings = [\n    # Warnings raised by dependencies of dependencies, out of our control\n\n    # Expected warnings\n    # Lightning warns us about using num_workers=0, but it's faster on macOS\n    \"ignore:The dataloader, .*, does not have many workers which may be a bottleneck:UserWarning\",\n    # Lightning warns us about using the CPU when a GPU is available\n    \"ignore:GPU available but not used.:UserWarning\",\n\n    # https://github.com/Lightning-AI/lightning/issues/16756\n    \"ignore:Deprecated call to `pkg_resources.declare_namespace:DeprecationWarning\",\n    \"ignore:pkg_resources is deprecated as an API.:DeprecationWarning:lightning_utilities.core.imports\",\n    \"ignore:distutils Version classes are deprecated. Use packaging.version instead.\",\n\n    # testing is run with num_workers=0\n    \"ignore:The .*dataloader.* does not have many workers which may be a bottleneck:UserWarning:lightning\",\n    \"ignore:The .*dataloader.* does not have many workers which may be a bottleneck:lightning.fabric.utilities.warnings.PossibleUserWarning:lightning\",\n\n    # Lightning CLI\n    \"ignor:LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments\",\n\n    # Lightning warns us if TensorBoard is not installed\n    \"ignore:Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package:UserWarning\",\n\n    # https://github.com/Lightning-AI/lightning/issues/18545\n    \"ignore:LightningCLI's args parameter is intended to run from within Python like if it were from the command line.:UserWarning\",\n\n    # MLP has activation_fn as a module that can be changed, we use it in tests\n    \"ignore:Unable to serialize instance ReLU()\",\n\n    # Post Hoc methods TODO maybe more finegrained control in tests\n    \"ignore:`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\",\n    \"ignore:`training_step` returned `None`. If this was on purpose, ignore this warning...\",\n\n    # MC-Dropout timm libraries implement dropout as functional an not module\n    \"ignore:No dropout layers found in model, maybe dropout is implemented through nn.fucntional?\",\n\n    # MacOS runner issues\n    \"ignore:Skipping device Apple Paravirtual device that does not support Metal 2.0:UserWarning\",\n    \"ignore:Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded:RuntimeWarning\",\n\n    # Run tests on GPU\n    \"ignore:The `srun` command is available on your system but is not used:lightning.fabric.utilities.warnings.PossibleUserWarning\",\n]\n\n[tool.env]\nUSE_IOMP = \"0\"\n\n[tool.ruff]\nextend-include = [\"*.ipynb\"]\nfix = true\n\n[tool.ruff.format]\nskip-magic-trailing-comma = true\n\n[tool.ruff.lint]\nextend-select = [\"D\", \"I\", \"UP\"]\n\n[tool.ruff.lint.per-file-ignores]\n\"docs/**\" = [\"D\"]\n\"tests/**\" = [\"D\"]\n\n[tool.ruff.lint.isort]\nsplit-on-trailing-comma = false\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n[tool.setuptools.dynamic]\nversion = {attr = \"lightning_uq_box.__version__\"}\n\n[tool.setuptools.package-data]\nlightning_uq_box = [\"py.typed\"]\n\n[tool.setuptools.packages.find]\ninclude = [\"lightning_uq_box*\"]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/linmog",
            "repo_link": "https://jugit.fz-juelich.de/iek-10/public/optimization/linmog",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/llama",
            "repo_link": "https://github.com/alpaka-group/llama",
            "content": {
                "codemeta": "",
                "readme": "LLAMA – Low-Level Abstraction of Memory Access\n==============================================\n\n[![ReadTheDocs](https://img.shields.io/badge/Docs-Read%20the%20Docs-blue.svg)](https://llama-doc.readthedocs.io)\n[![Doxygen](https://img.shields.io/badge/API-Doxygen-blue.svg)](https://alpaka-group.github.io/llama)\n[![Language](https://img.shields.io/badge/Language-C%2B%2B17-blue.svg)](https://isocpp.org/)\n[![Paper](https://img.shields.io/badge/Paper-Wiley%20Online%20Library-blue.svg)](https://doi.org/10.1002/spe.3077)\n[![Preprint](https://img.shields.io/badge/Preprint-arXiv-blue.svg)](https://arxiv.org/abs/2106.04284)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5901241.svg)](https://doi.org/10.5281/zenodo.5901241)\n[![codecov](https://codecov.io/gh/alpaka-group/llama/branch/develop/graph/badge.svg?token=B94D9G96FA)](https://codecov.io/gh/alpaka-group/llama)\n\n![LLAMA](docs/images/logo_400x169.png)\n\nLLAMA is a cross-platform C\\++17/C\\++20 header-only template library for the abstraction of data layout and memory access.\nIt separtes the view of the algorithm on the memory and the real data layout in the background.\nThis allows for performance portability in applications running on heterogeneous hardware with the very same code.\n\nDocumentation\n-------------\n\nOur extensive user documentation is available on [Read the Docs](https://llama-doc.rtfd.io).\nIt includes:\n\n* Installation instructions\n* Motivation and goals\n* Overview of concepts and ideas\n* Descriptions of LLAMA's constructs\n\nAn API documentation is generated by [Doxygen](https://alpaka-group.github.io/llama/) from the C++ source.\nPlease read the documentation on Read the Docs first!\n\nSupported compilers\n-------------------\n\nLLAMA tries to stay close to recent developments in C++ and so requires fairly up-to-date compilers.\nThe following compilers are supported by LLAMA and tested as part of our CI:\n\n\n| Linux                                                                                         | Windows                                             | MacOS                            |\n|-----------------------------------------------------------------------------------------------|-----------------------------------------------------|----------------------------------|\n| g++ 10 - 13 </br> clang++ 12 - 17 </br> icpx (latest) </br> nvc++ 23.5 </br> nvcc 11.6 - 12.3 | Visual Studio 2022 </br> (latest on GitHub actions) | clang++ </br> (latest from brew) |\n\n\nSingle header\n-------------\n\nWe create a single-header version of LLAMA on each commit,\nwhich you can find on the [single-header branch](https://github.com/alpaka-group/llama/tree/single-header).\n\nThis also useful, if you would like to play with LLAMA on Compiler explorer:\n```c++\n#include <https://raw.githubusercontent.com/alpaka-group/llama/single-header/llama.hpp>\n```\n\nContributing\n------------\n\nWe greatly welcome contributions to LLAMA.\nRules for contributions can be found in [CONTRIBUTING.md](CONTRIBUTING.md).\n\nScientific publications\n-----------------------\n\nWe published an [article](https://doi.org/10.1002/spe.3077) on LLAMA in the journal of Software: Practice and Experience.\nWe gave a talk on LLAMA at CERN's Compute Accelerator Forum on 2021-05-12.\nThe video recording (starting at 40:00) and slides are available here on [CERN's Indico](https://indico.cern.ch/event/975010/).\nMind that some of the presented LLAMA APIs have been renamed or redesigned in the meantime.\n\nWe presented recently added features to LLAMA at the ACAT22 workshop as a [poster](https://indico.cern.ch/event/1106990/contributions/5096939/)\nand a contribution to the [proceedings](https://arxiv.org/abs/2302.08251).\nAdditionally, we gave a [talk](https://indico.cern.ch/event/1106990/contributions/4991259/) at ACAT22 on LLAMA's instrumentation capabilities during a case study on [AdePT](https://github.com/apt-sim/AdePT),\nagain, with a contribution to the [proceedings](https://arxiv.org/abs/2302.08252).\n\nAttribution\n-----------\n\nIf you use LLAMA for scientific work, please consider citing this project.\nWe upload all releases to [Zenodo](https://zenodo.org/record/4911494),\nwhere you can export a citation in your preferred format.\nWe provide a DOI for each release of LLAMA.\nAdditionally, consider citing the [LLAMA paper](https://doi.org/10.1002/spe.3077).\n\nLicense\n-------\n\nLLAMA is licensed under the [MPL-2.0](LICENSE).\n\n",
                "dependencies": "cmake_minimum_required (VERSION 3.18.3)\nproject (llama CXX)\n\n# llama\nfind_package(Boost 1.74.0 REQUIRED)\nfind_package(fmt CONFIG QUIET)\nadd_library(${PROJECT_NAME} INTERFACE)\ntarget_include_directories(${PROJECT_NAME} INTERFACE $<BUILD_INTERFACE:${PROJECT_SOURCE_DIR}/include> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>)\ntarget_compile_features(${PROJECT_NAME} INTERFACE cxx_std_17)\ntarget_link_libraries(${PROJECT_NAME} INTERFACE Boost::headers)\nadd_compile_definitions(BOOST_ATOMIC_NO_LIB) # we don't need the compiled part in LLAMA or its examples\nif (fmt_FOUND)\n\ttarget_link_libraries(${PROJECT_NAME} INTERFACE fmt::fmt)\n\tif (CMAKE_CXX_COMPILER_ID STREQUAL \"NVHPC\")\n\t\ttarget_compile_definitions(${PROJECT_NAME} INTERFACE -DFMT_USE_NONTYPE_TEMPLATE_PARAMETERS=0 -DFMT_USE_NONTYPE_TEMPLATE_ARGS=0)\n\tendif()\nelse()\n\tmessage(WARNING \"The fmt library was not found. You cannot use llama's dumping facilities.\")\nendif()\n\n# llama::llama to make subdirectory projects work\nadd_library(${PROJECT_NAME}::${PROJECT_NAME} ALIAS ${PROJECT_NAME})\n\n# llama IDE target to make source browsable/editable in IDEs\nfile(GLOB_RECURSE llamaSources \"${CMAKE_CURRENT_SOURCE_DIR}/include/**\")\nadd_custom_target(\"llamaIde\" SOURCES ${llamaSources})\nsource_group(TREE \"${CMAKE_CURRENT_LIST_DIR}/include/llama\" FILES ${llamaSources})\n\n# default build type, see: https://www.kitware.com/cmake-and-the-default-build-type/\nif(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n\tmessage(STATUS \"Setting build type to 'Release' as none was specified.\")\n\tset_property(CACHE CMAKE_BUILD_TYPE PROPERTY VALUE \"Release\")\nendif()\n\nif (MSVC)\n\t# FIXME(bgruber): alpaka uses M_PI, so we need to make it available on MSVC. This may be fixed in alpaka 1.0.0.\n\ttarget_compile_definitions(llama INTERFACE _USE_MATH_DEFINES)\n\ttarget_compile_options(${PROJECT_NAME} INTERFACE /Zc:lambda) # needed in C++17 mode, remove when upgrading to C++20\nendif()\n\n# CUDA\ninclude(CheckLanguage)\ncheck_language(CUDA)\nif (CMAKE_CUDA_COMPILER)\n\tenable_language(CUDA)\n\tset(CMAKE_CUDA_ARCHITECTURES \"35\" CACHE STRING \"CUDA architectures to compile for\")\n\n\tif (CMAKE_CUDA_COMPILER_ID STREQUAL \"Clang\")\n\t\ttarget_compile_definitions(${PROJECT_NAME} INTERFACE -DFMT_USE_FLOAT128=0)\n\n\t\t# Workaround for clang as CUDA compiler with libstdc++ 12\n\t\tfile(WRITE \"${CMAKE_CURRENT_BINARY_DIR}/clang_cuda_libstdc++12_workaround.hpp\"\n\t\t\t\t\"#include <__clang_cuda_runtime_wrapper.h>\\n\"\n\t\t\t\t\"#if defined(__clang__) && defined(__CUDA__) && defined(_GLIBCXX_RELEASE) && _GLIBCXX_RELEASE >= 12 && defined(__noinline__)\\n\"\n\t\t\t\t\"#    undef __noinline__\\n\"\n\t\t\t\t\"#endif\\n\")\n\t\ttarget_compile_options(${PROJECT_NAME} INTERFACE -include \"${CMAKE_CURRENT_BINARY_DIR}/clang_cuda_libstdc++12_workaround.hpp\")\n\tendif()\nelse()\n\tmessage(WARNING \"Could not find CUDA. Try setting CMAKE_CUDA_COMPILER. CUDA tests and examples are disabled.\")\nendif()\n\n# tests\ninclude(CMakeDependentOption)\ncmake_dependent_option(LLAMA_COMPILE_TESTS_AS_CUDA \"Sets the language of all test code to CUDA.\" OFF \"BUILD_TESTING;CMAKE_CUDA_COMPILER\" OFF)\n\noption(BUILD_TESTING \"\" OFF)\ninclude(CTest)\nif (BUILD_TESTING)\n\toption(LLAMA_SYSTEM_CATCH2 \"Use the system provided Catch2. This may result in a build failure, if Catch2 was compiled with a different C++ version as the LLAMA tests.\" ON)\n\tif (LLAMA_SYSTEM_CATCH2)\n\t\tfind_package(Catch2 3.0.1 REQUIRED)\n\t\tinclude(Catch)\n\telse()\n\t\t# get Catch2 v3 and build it from source with the same C++ standard as the tests\n\t\tInclude(FetchContent)\n\t\tFetchContent_Declare(Catch2 GIT_REPOSITORY https://github.com/catchorg/Catch2.git GIT_TAG v3.0.1)\n\t\tFetchContent_MakeAvailable(Catch2)\n\t\ttarget_compile_features(Catch2 PUBLIC cxx_std_20)\n\t\tinclude(Catch)\n\n\t\t# hide Catch2 cmake variables by default in cmake gui\n\t\tget_cmake_property(variables VARIABLES)\n\t\tforeach (var ${variables})\n\t\t\tif (var MATCHES \"^CATCH_\")\n\t\t\t\tmark_as_advanced(${var})\n\t\t\tendif()\n\t\tendforeach()\n\tendif()\n\n\tfile(GLOB_RECURSE testSources \"${CMAKE_CURRENT_SOURCE_DIR}/tests/**\")\n\tadd_executable(tests ${testSources})\n\tcatch_discover_tests(tests)\n\tsource_group(TREE \"${CMAKE_CURRENT_LIST_DIR}/tests\" FILES ${testSources})\n\ttarget_compile_features(tests PRIVATE cxx_std_20)\n\n\tif (LLAMA_COMPILE_TESTS_AS_CUDA)\n\t\tforeach(f ${testSources})\n\t\t\tset_source_files_properties(${f} PROPERTIES LANGUAGE CUDA)\n\t\tendforeach()\n\t\ttarget_compile_options(tests PRIVATE --extended-lambda --expt-relaxed-constexpr)\n\tendif()\n\n\tif (MSVC)\n\t\ttarget_compile_options(tests PRIVATE /permissive- /constexpr:steps10000000 /diagnostics:caret)\n\telse()\n\t\ttarget_compile_options(tests PRIVATE -Wall -Wextra $<$<NOT:$<COMPILE_LANGUAGE:CUDA>>:-Werror=narrowing> -march=native)\n\t\tif (NOT CMAKE_CXX_COMPILER_ID STREQUAL \"NVHPC\")\n\t\t\ttarget_compile_options(tests PRIVATE -Wno-missing-braces)\n\t\tendif()\n\tendif()\n\tif (CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"AppleClang\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"IntelLLVM\")\n\t\ttarget_compile_options(tests PRIVATE -fconstexpr-steps=10000000)\n\tendif()\n\tif (CMAKE_CXX_COMPILER_ID STREQUAL \"IntelLLVM\")\n\t\ttarget_compile_options(tests PRIVATE -fp-model=precise)\n\tendif()\n\tif (CMAKE_CXX_COMPILER_ID STREQUAL \"NVHPC\")\n\t\ttarget_compile_options(tests PRIVATE --display_error_number -Wc,--pending_instantiations=0)\n\t\ttarget_compile_options(tests PRIVATE --diag_suppress=177) # disable: #177-D: variable \"<unnamed>::autoRegistrar72\" was declared but never referenced\n\tendif()\n\tif (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n\t\tif (LLAMA_ENABLE_ASAN_FOR_TESTS AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 12.0 AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 14.0)\n\t\t\ttarget_compile_options(tests PRIVATE -Wno-maybe-uninitialized) # triggered inside std::function by std::regex\n\t\tendif()\n\t\tif (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 13.0 AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 14.0)\n\t\t\ttarget_compile_options(tests PRIVATE -Wno-dangling-reference) # triggered by an access involving RecordRef, so basically everywhere\n\t\tendif()\n\tendif()\n\ttarget_link_libraries(tests PRIVATE Catch2::Catch2WithMain llama::llama)\n\n\toption(LLAMA_ENABLE_ASAN_FOR_TESTS \"Enables address sanitizer for tests\" OFF)\n\tif (LLAMA_ENABLE_ASAN_FOR_TESTS)\n\t\tif (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n\t\t\ttarget_compile_options(tests PRIVATE -fsanitize=address -fno-omit-frame-pointer)\n\t\t\ttarget_link_options   (tests PRIVATE -fsanitize=address -fno-omit-frame-pointer)\n\t\telseif(MSVC)\n\t\t\ttarget_compile_options(tests PRIVATE /fsanitize=address)\n\t\t\ttarget_link_options   (tests PRIVATE /fsanitize=address)\n\t\tendif()\n\tendif()\n\n\toption(LLAMA_ENABLE_COVERAGE_FOR_TESTS \"Enables code coverage for tests\" OFF)\n\tif (LLAMA_ENABLE_COVERAGE_FOR_TESTS)\n\t\tif (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n\t\t\ttarget_compile_options(tests PRIVATE --coverage)\n\t\t\ttarget_link_options(tests PRIVATE --coverage)\n\t\tendif()\n\tendif()\nendif()\n\n# examples\noption(LLAMA_BUILD_EXAMPLES \"Building (and installing) the examples\" OFF)\nif (LLAMA_BUILD_EXAMPLES)\n\tset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR})\n\n\t# general examples\n\tadd_subdirectory(\"examples/vectoradd\")\n\tadd_subdirectory(\"examples/nbody\")\n\tadd_subdirectory(\"examples/nbody_code_comp\")\n\tadd_subdirectory(\"examples/heatequation\")\n\tadd_subdirectory(\"examples/viewcopy\")\n\tadd_subdirectory(\"examples/bufferguard\")\n\tadd_subdirectory(\"examples/raycast\")\n\tadd_subdirectory(\"examples/bytesplit\")\n\tadd_subdirectory(\"examples/bitpackint\")\n\tadd_subdirectory(\"examples/bitpackfloat\")\n\tadd_subdirectory(\"examples/memmap\")\n\tadd_subdirectory(\"examples/stream\")\n\tadd_subdirectory(\"examples/falsesharing\")\n\tadd_subdirectory(\"examples/comptime\")\n\n\t# alpaka examples\n\tfind_package(alpaka 1.0)\n\tif (_alpaka_FOUND)\n\t\tadd_subdirectory(\"examples/alpaka/nbody\")\n\t\tadd_subdirectory(\"examples/alpaka/vectoradd\")\n\t\tadd_subdirectory(\"examples/alpaka/asyncblur\")\n\t\tadd_subdirectory(\"examples/alpaka/pic\")\n\t\tadd_subdirectory(\"examples/alpaka/daxpy\")\n\t\tadd_subdirectory(\"examples/alpaka/babelstream\")\n\telse()\n\t\tmessage(WARNING \"Could not find alpaka. Alpaka examples are disabled.\")\n\tendif()\n\n\t# ROOT examples\n\tfind_package(ROOT QUIET)\n\tif (ROOT_FOUND)\n\t\tadd_subdirectory(\"examples/root/lhcb_analysis\")\n\tendif()\n\n\t# CUDA examples\n\tif (CMAKE_CUDA_COMPILER)\n\t\tadd_subdirectory(\"examples/cuda/nbody\")\n\t\tadd_subdirectory(\"examples/cuda/pitch\")\n\t\tadd_subdirectory(\"examples/cuda/viewcopy\")\n\tendif()\n\n\t# SYCL examples\n\tfind_package(IntelSYCL)\n\tif (IntelSYCL_FOUND)\n\t\tadd_subdirectory(\"examples/sycl/nbody\")\n\tendif()\nendif()\n\n# install\ninclude(CMakePackageConfigHelpers)\ninclude(GNUInstallDirs)\n\nset(_llama_INSTALL_CMAKEDIR \"${CMAKE_INSTALL_LIBDIR}/cmake/llama\")\n\nconfigure_package_config_file (\n\t\"${PROJECT_SOURCE_DIR}/cmake/llama-config.cmake.in\"\n\t\"${PROJECT_BINARY_DIR}/cmake/llama-config.cmake\"\n\tINSTALL_DESTINATION \"${_llama_INSTALL_CMAKEDIR}\")\n\nconfigure_file (\n\t\"${PROJECT_SOURCE_DIR}/cmake/llama-config-version.cmake.in\"\n\t\"${PROJECT_BINARY_DIR}/cmake/llama-config-version.cmake\"\n\t@ONLY\n)\n\ninstall( DIRECTORY \"${CMAKE_CURRENT_SOURCE_DIR}/include/llama\" DESTINATION \"include\" )\ninstall(\n\tFILES\n\t\t\"${PROJECT_BINARY_DIR}/cmake/llama-config.cmake\"\n\t\t\"${PROJECT_BINARY_DIR}/cmake/llama-config-version.cmake\"\n\tDESTINATION\n\t\t\"${_llama_INSTALL_CMAKEDIR}\"\n)\n\n{\n  \"$schema\": \"https://raw.githubusercontent.com/microsoft/vcpkg/master/scripts/vcpkg.schema.json\",\n  \"name\": \"llama\",\n  \"version\": \"0.6.0\",\n  \"dependencies\": [\n    \"alpaka\",\n    \"boost-atomic\",\n    \"boost-container\",\n    \"boost-core\",\n    \"boost-functional\",\n    \"boost-mp11\",\n    \"boost-smart-ptr\",\n    {\n      \"name\": \"boost-iostreams\",\n      \"default-features\": false\n    },\n    \"catch2\",\n    \"fmt\",\n    \"tinyobjloader\",\n    \"xsimd\"\n  ]\n}\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/llview",
            "repo_link": "https://github.com/FZJ-JSC/LLview",
            "content": {
                "codemeta": "",
                "readme": "<div align=\"left\">\n  <img src=\"docs/docs/images/LLview_logo.svg\" alt=\"LLview\" height=\"150em\"/>\n</div>\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10221407.svg)](https://doi.org/10.5281/zenodo.10221407)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n\n# LLview\n\n<div align=\"center\">\n  <img src=\"docs/docs/images/LLview_thumbnail.png\" alt=\"LLview thumbnail\" width=\"100%\"/>\n</div>\n\nLLview is a set of software components to monitor clusters that are controlled by a resource manager and a scheduler system. Within its Job Reporting module, it provides detailed information of all the individual jobs running on the system. To achieve this, LLview connects to different sources in the system and collects data to present to the user via a web portal. For example, the resource manager provides information about the jobs, while additional daemons may be used to acquire extra information from the compute nodes, keeping the overhead at a minimum, as the metrics are obtained in the range of minutes apart. The LLview portal establishes a link between performance metrics and individual jobs to provide a comprehensive job reporting interface.\n\n## Installation\n\n[Installation instructions](https://apps.fz-juelich.de/jsc/llview/docu/install/) can be found on LLview's [documentation page](https://llview.fz-juelich.de/docs).\n\nLLview presents its gathered data in a Web Portal created by [JURI](https://github.com/FZJ-JSC/JURI).\n\n## Further Information\n\nFor further information please see: http://llview.fz-juelich.de/docs\n\nContact: [llview.jsc@fz-juelich.de](mailto:llview.jsc@fz-juelich.de)\n\n## Copyright, License and CLA\n\nCopyright (c) 2023 Forschungszentrum Juelich GmbH, Juelich Supercomputing Centre  \nhttp://llview.fz-juelich.de/\n\nThis is an open source software distributed under the GPLv3 license. More information see the LICENSE file at the top level.\n\nContributions must follow the Contributor License Agreement. More information see the CONTRIBUTING.md file at the top level.\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/lynx",
            "repo_link": "https://github.com/ajacquey/lynx",
            "content": {
                "codemeta": "",
                "readme": "<h1 align=\"center\">\n  <br>\n  <a href=\"https://gitext.gfz-potsdam.de/ajacquey/lynx\">LYNX</a>\n  <br>\n  Lithosphere dYnamic Numerical toolboX\n  <br>\n  A MOOSE-based application\n  <br>\n</h1>\n\n<h4 align=\"center\">A numerical simulator for modelling deformation of the lithosphere, based on <a href=\"http://mooseframework.org/\" target=\"blank\">MOOSE</a>.</h4>\n\n<p align=\"center\">\n  <a href=\"LICENSE\">\n    <img src=\"https://img.shields.io/badge/license-GPLv3-blue.svg\"\n         alt=\"GPL License\">\n  </a>\n  <a href=\"https://zenodo.org/record/3355376#.XUA2qi2Q1PU\">\n    <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3355376.svg\"\n         alt=\"DOI\">\n  </a>\n</p>\n\n## About\nLYNX (Lithosphere dYnamic Numerical toolboX) is a numerical simulator for modelling coupled Thermo-Hydro-Mechanical processes in the porous rocks of the lithosphere.\nThe simulator is developed by [Antoine Jacquey](http://www.gfz-potsdam.de/en/staff/antoine-jacquey/) <a href=\"https://orcid.org/0000-0002-6259-4305\" target=\"orcid.widget\" rel=\"noopener noreferrer\" style=\"vertical-align:top;\"><img src=\"https://orcid.org/sites/default/files/images/orcid_16x16.png\" style=\"width:1em;margin-right:.5em;\" alt=\"ORCID iD icon\"></a> and [Mauro Cacace](http://www.gfz-potsdam.de/en/section/basin-modeling/staff/profil/mauro-cacace/) <a href=\"https://orcid.org/0000-0001-6101-9918\" target=\"orcid.widget\" rel=\"noopener noreferrer\" style=\"vertical-align:top;\"><img src=\"https://orcid.org/sites/default/files/images/orcid_16x16.png\" style=\"width:1em;margin-right:.5em;\" alt=\"ORCID iD icon\"></a> at the [GFZ Potsdam, German Research Centre for Geosciences](http://www.gfz-potsdam.de/en/home/) from the section [Basin Modelling](http://www.gfz-potsdam.de/en/section/basin-modeling/).\n\n\nLYNX is a MOOSE-based application. Visit the [MOOSE framework](http://mooseframework.org) page for more information.\n\n## Licence\nLYNX is distributed under the [GNU GENERAL PUBLIC LICENSE v3](https://gitext.gfz-potsdam.de/ajacquey/lynx/blob/master/LICENSE).\n\n\n## Getting Started\n\n#### Minimum System Requirements\nThe following system requirements are from the MOOSE framework (see [Getting Started](http://mooseframework.org/getting-started/) for more information):\n* Compiler: C++11 Compliant GCC 4.8.4, Clang 3.4.0, Intel20130607\n* Python 2.7+\n* Memory: 16 GBs (debug builds)\n* Processor: 64-bit x86\n* Disk: 30 GBs\n* OS: UNIX compatible (OS X, most flavors of Linux)\n\n#### 1. Setting Up a MOOSE Installation\nTo install LYNX, you need first to have a working and up-to-date installation of the MOOSE framework.  \nTo do so, please visit the [Getting Started](http://mooseframework.org/getting-started/) page of the MOOSE framework and follow the instructions. If you encounter difficulties at this step, you can ask for help on the [MOOSE-users Google group](https://groups.google.com/forum/#!forum/moose-users).\n\n#### 2. Clone LYNX\nLYNX can be cloned directly from [GitLab](https://gitext.gfz-potsdam.de/ajacquey/lynx) using [Git](https://git-scm.com/). In the following, we refer to the directory `projects` which you created during the MOOSE installation (by default `~/projects`):  \n\n    cd ~/projects\n    git clone https://gitext.gfz-potsdam.de/ajacquey/lynx.git\n    cd ~/projects/lynx\n    git checkout master\n\n*Note: the \"master\" branch of LYNX is the \"stable\" branch which is updated only if all tests are passing.*\n\n#### 3. Compile LYNX\nYou can compile LYNX by following these instructions:\n\n    cd ~/projects/lynx\n    make -j4\n\n#### 4. Test LYNX\nTo make sure that everything was installed properly, you can run the tests suite of LYNX:\n\n    cd ~/projects/lynx\n    ./run_tests -j2\n\nIf all the tests passed, then your installation is working properly. You can now use the LYNX simulator!\n\n## Usage\nTo run LYNX from the command line with multiple processors, use the following command:\n\n    mpiexec -n <nprocs> ~/projects/lynx/lynx-opt -i <input-file>\n\nWhere `<nprocs>` is the number of processors you want to use and `<input-file>` is the path to your input file (extension `.i`).  \n\nInformation about the structure of the LYNX input files can be found in the documentation (link to follow).\n\n## Cite\n\nIf you use LYNX for your work please cite:\n* This repository:  \nJacquey, Antoine B., & Cacace, Mauro. (2019, July 30). LYNX: Lithosphere dYnamic Numerical toolboX, a MOOSE-based application (Version 1.0). Zenodo. http://doi.org/10.5281/zenodo.3355376\n\n* The following research articles:\nJacquey, Antoine B., & Cacace, Mauro. (2020). Multiphysics Modeling of a Brittle‐Ductile Lithosphere: 1. Explicit Visco‐Elasto‐Plastic Formulation and Its Numerical Implementation. Journal of Geophysical Research: Solid Earth. http://doi.org/10.1029/2019jb018474\nJacquey, Antoine B., & Cacace, Mauro. (2020). Multiphysics Modeling of a Brittle‐Ductile Lithosphere: 2. Semi‐brittle, Semi‐ductile Deformation and Damage Rheology. Journal of Geophysical Research: Solid Earth. http://doi.org/10.1029/2019jb018475\n\n\nPlease read the [CITATION](https://gitext.gfz-potsdam.de/ajacquey/lynx//blob/master/CITATION) file for more information.\n\n## Publications using LYNX\n\n* Jacquey, Antoine B., & Cacace, Mauro. (2020). Multiphysics Modeling of a Brittle‐Ductile Lithosphere: 1. Explicit Visco‐Elasto‐Plastic Formulation and Its Numerical Implementation. Journal of Geophysical Research: Solid Earth. http://doi.org/10.1029/2019jb018474\n\n* Jacquey, Antoine B., & Cacace, Mauro. (2020). Multiphysics Modeling of a Brittle‐Ductile Lithosphere: 2. Semi‐brittle, Semi‐ductile Deformation and Damage Rheology. Journal of Geophysical Research: Solid Earth. http://doi.org/10.1029/2019jb018475\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/maftools",
            "repo_link": "https://github.com/PoisonAlien/maftools",
            "content": {
                "codemeta": "",
                "readme": "<img src=\"vignettes/maftools_hex.svg\" align=\"left\" height=\"140\" /></a>\n\n## maftools - An R package to summarize, analyze and visualize MAF files\n\n[![GitHub closed issues](https://img.shields.io/github/issues-closed-raw/poisonalien/maftools.svg)](https://github.com/poisonalien/maftools/issues)\n[![R-CMD-check](https://github.com/PoisonAlien/maftools/workflows/R-CMD-check/badge.svg)](https://github.com/PoisonAlien/maftools/actions)\n\n## Introduction\n\nmaftools is a comprehensive toolkit for processing somatic variants from cohort-based cancer genomic studies. maftools offers over 80 functions to perform the most commonly required tasks in cancer genomics, using [MAF](https://docs.gdc.cancer.gov/Data/File_Formats/MAF_Format/) as the only input file type.\n\n## Installation\n\n```{r}\n#Install from Bioconductor repository\nBiocManager::install(\"maftools\")\n\n#Install from GitHub repository\nBiocManager::install(\"PoisonAlien/maftools\")\n```\n\n## Getting started: Vignette and a case study\n\nA complete documentation of maftools using [TCGA LAML](https://www.nejm.org/doi/full/10.1056/nejmoa1301689) as a case study can be found [here](http://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html).\n\n<p align=\"left\">\n<img src=\"https://user-images.githubusercontent.com/8164062/97981605-d8a59500-1dd2-11eb-9f5e-cc808f7b3f91.gif\" height=\"320\" height=\"400\">\n</p>\n\n## Primary applications \n\nmaftools is extremely easy to use, starting with importing an [MAF](https://docs.gdc.cancer.gov/Data/File_Formats/MAF_Format/) file along with the associated clinical data. Once the data is successfully imported, the resulting MAF object can be passed to various functions. Key applications include:\n\n- [Cohort summarization using oncoplots](https://bioconductor.org/packages/devel/bioc/vignettes/maftools/inst/doc/oncoplots.html#08_Combining_everything)\n- [Identify co-occurring and mutually exclusive events](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#91_Somatic_Interactions)\n- [Clinical enrichment analysis](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#96_Clinical_enrichment_analysis)\n- [Detect cancer driver genes](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#92_Detecting_cancer_driver_genes_based_on_positional_clustering)\n- [Infer tumor heterogeneity](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#99_Tumor_heterogeneity_and_MATH_scores)\n- [Analyze known cancer signaling pathways](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#98_Oncogenic_Signaling_Pathways)\n- [De-novo somatic signature analysis with NMF](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#9103_Signature_analysis)\n- [Compare two cohorts to identify differentially mutated genes](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#95_Comparing_two_cohorts_(MAFs))\n- [Perform survival analysis and predict genesets associated with survival](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#942_Predict_genesets_associated_with_survival)\n- [Drug-gene interactions](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#97_Drug-Gene_Interactions)\n\nBesides the MAF files, maftools can handle sequencing alignment BAM files, copy number output from GISTIC and mosdepth. Please refer to the package documentation sections below to learn more.\n\n- [Generate personalized cancer report](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/cancer_hotspots.html) for known somatic [hotspots](https://www.cancerhotspots.org/)\n- [Sample mismatch and relatedness analysis](https://bioconductor.org/packages/devel/bioc/vignettes/maftools/inst/doc/maftools.html#12_Sample_swap_identification)\n- [Copy number analysis](https://bioconductor.org/packages/devel/bioc/vignettes/maftools/inst/doc/cnv_analysis.html) with [ASCAT](https://github.com/VanLoo-lab/ascat) and [mosdepth](https://github.com/brentp/mosdepth)\n\nMoreover, analyzing all 33 TCGA cohorts along with the harmonized clinical data is a breeze. \n\n- A single command [tcgaLoad](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#13_TCGA_cohorts) will import the desired TCGA cohort thereby avoiding costly time spent on data mining from public databases. \n- Please refer to an associated software package [TCGAmutations](https://github.com/PoisonAlien/TCGAmutations) that provides ready to use `MAF` objects for 33 TCGA cohorts and 2427 cell line profiles from CCLE - along with relevant clinical information for all sequenced samples.\n\n## Citation\n\n**_Mayakonda A, Lin DC, Assenov Y, Plass C, Koeffler HP. 2018. Maftools: efficient and comprehensive analysis of somatic variants in cancer. [Genome Research](https://doi.org/10.1101/gr.239244.118). PMID: [30341162](https://www.ncbi.nlm.nih.gov/pubmed/?term=30341162)_**\n\n\n## Useful links\n\n| File Fomats                                                                                                        | Data portals                                                                                    | Annotation tools                                                                                                                       |\n|--------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n| [Mutation Annotation Format](https://docs.gdc.cancer.gov/Data/File_Formats/MAF_Format/)                            | [TCGA](http://cancergenome.nih.gov)                                                             | [vcf2maf](https://github.com/mskcc/vcf2maf) - for converting your VCF files to MAF                                                     |\n| [Variant Call Format](https://en.wikipedia.org/wiki/Variant_Call_Format)                                           | [ICGC](https://docs.icgc.org/)                                                                  | [annovar2maf](https://github.com/PoisonAlien/annovar2maf) - for converting annovar output files to MAF                                 |\n| ICGC [Simple Somatic Mutation Format](https://docs.icgc.org/submission/guide/icgc-simple-somatic-mutation-format/) | [Broad Firehose](https://gdac.broadinstitute.org/)                                              | [bcftools csq](https://samtools.github.io/bcftools/howtos/csq-calling.html) - Rapid annotations of VCF files with variant consequences |\n|                                                                                                                    | [cBioPortal](https://www.cbioportal.org/)                                                       | [Annovar](https://annovar.openbioinformatics.org/en/latest/)                                                              |\n|                                                                                                                    | [PeCan](https://pecan.stjude.cloud/)                                                            | [Funcotator](https://gatk.broadinstitute.org/hc/en-us/articles/360037224432-Funcotator)                                                |\n|                                                                                                                    | [CIViC](https://civicdb.org/home) - Clinical interpretation of variants in cancer               |                                                                                                                                        |\n|                                                                                                                    | [DGIdb](http://www.dgidb.org/) - Information on drug-gene interactions and the druggable genome |                                                                                                                                        |\n\n\n## Useful packages/tools\n\nBelow are some more useful software packages for somatic variant analysis\n\n* [TRONCO](https://github.com/BIMIB-DISCo/TRONCO) - Repository of the TRanslational ONCOlogy library (R)\n* [dndscv](https://github.com/im3sanger/dndscv) - dN/dS methods to quantify selection in cancer and somatic evolution (R)\n* [cloneevol](https://github.com/hdng/clonevol) - Inferring and visualizing clonal evolution in multi-sample cancer sequencing (R)\n* [sigminer](https://github.com/ShixiangWang/sigminer) - Primarily for signature analysis and visualization in R. Supports `maftools` output (R)\n* [GenVisR](https://github.com/griffithlab/GenVisR) - Primarily for visualization (R)\n* [comut](https://github.com/vanallenlab/comut) - Primarily for visualization (Python)\n* [TCGAmutations](https://github.com/PoisonAlien/TCGAmutations) - pre-compiled curated somatic mutations from TCGA cohorts (from Broad Firehose and TCGA MC3 Project) that can be loaded into `maftools` (R)\n* [somaticfreq](<https://github.com/PoisonAlien/somaticfreq>) - rapid genotyping of known somatic hotspot variants from the tumor BAM files. Generates a browsable/sharable HTML report. (C)\n\n***\n\n#### Powered By\n\n* [data.table](https://github.com/Rdatatable/data.table/wiki) at [warp speed](https://en.wikipedia.org/wiki/Warp_drive)\n\n",
                "dependencies": "Type: Package\nPackage: maftools\nTitle: Summarize, Analyze and Visualize MAF Files\nVersion: 2.22.10\nDate: 2021-04-30\nAuthors@R: \n    person(given = \"Anand\",\n           family = \"Mayakonda\",\n           role = c(\"aut\", \"cre\"),\n           email = \"anand_mt@hotmail.com\",\n           comment = c(ORCID = \"0000-0003-1162-687X\"))\nMaintainer: Anand Mayakonda <anand_mt@hotmail.com>\nDescription: Analyze and visualize Mutation Annotation Format (MAF) files\n    from large scale sequencing studies. This package provides various\n    functions to perform most commonly used analyses in cancer genomics\n    and to create feature rich customizable visualzations with minimal\n    effort.\nLicense: MIT + file LICENSE\nURL: https://github.com/PoisonAlien/maftools\nBugReports: https://github.com/PoisonAlien/maftools/issues\nDepends: \n    R (>= 3.3)\nImports: \n    data.table,\n    grDevices,\n    methods,\n    RColorBrewer,\n    Rhtslib,\n    survival,\n    DNAcopy,\n    pheatmap\nSuggests: \n    berryFunctions,\n    Biostrings,\n    BSgenome,\n    BSgenome.Hsapiens.UCSC.hg19,\n    GenomicRanges,\n    IRanges,\n    knitr,\n    mclust,\n    MultiAssayExperiment,\n    NMF,\n    R.utils,\n    RaggedExperiment,\n    rmarkdown,\n    S4Vectors\nLinkingTo: \n    Rhtslib,\n    zlibbioc\nVignetteBuilder: \n    knitr\nbiocViews: DataRepresentation, DNASeq, Visualization, DriverMutation,\n    VariantAnnotation, FeatureExtraction, Classification, SomaticMutation,\n    Sequencing, FunctionalGenomics, Survival\nEncoding: UTF-8\nLazyData: TRUE\nNeedsCompilation: no\nPackaged: 2016-04-08 02:06:05 UTC; anand\nRoxygenNote: 7.3.1\nSystemRequirements: GNU make, curl\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mainzelliste",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/mallob",
            "repo_link": "https://github.com/domschrei/mallob",
            "content": {
                "codemeta": "",
                "readme": "[![status](https://joss.theoj.org/papers/700e9010c4080ffe8ae4df21cf1cc899/status.svg)](https://joss.theoj.org/papers/700e9010c4080ffe8ae4df21cf1cc899)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6890239.svg)](https://doi.org/10.5281/zenodo.6890239)\n\n# Mallob\n\nThe platform **Mallob** (**Mal**leable **Lo**ad **B**alancer, or **Ma**ssively P**a**ra**ll**el **Lo**gic **B**ackend) is a distributed platform for processing automated reasoning tasks in modern large-scale HPC and cloud environments. Mallob primarily solves instances of the NP-complete _propositional satisfiability_ (SAT) problem – an essential building block at the core of automated reasoning and Symbolic AI. Mallob's flexible and decentralized approach to job scheduling allows to concurrently process many tasks of varying priority by different users. As such, Mallob can drastically improve your academic or industrial workflows tied to automated reasoning.\n\nMallob's tightly integrated distributed general-purpose SAT solving engine, which we refer to as MallobSat, has received a large amount of attention, five gold medals of the International SAT Competition's Cloud Track in a row, and Amazon's proposition that our system is, \"by a _wide_ margin, the most powerful SAT solver on the planet\" (Byron Cook, [Amazon Science blog post](https://www.amazon.science/blog/automated-reasonings-scientific-frontiers)).\nMallob is also the first distributed system that supports _incremental SAT solving_, i.e., interactive solving procedures over evolving formulas. Most recently, Mallob spearheaded the adoption of unsatisfiability proof checking in parallel and distributed SAT solving. Since proofs can serve as crucial witnesses for a result’s correctness, Mallob is suitable even for the most critical use cases.\n\n<hr/>\n\n# Setup\n\n## Prerequisites\n\nNote that we only support Linux as an operating system.\n(Some people have been developing and experimenting with Mallob within the WSL, but there seem to be issues related to inotify.)\n\n* CMake ≥ 3.11.4\n* Open MPI (or another MPI implementation)\n* GDB\n* [jemalloc](https://github.com/jemalloc/jemalloc)\n\n## Building\n\n```bash\n# Only needed if building with MALLOB_APP_SAT (enabled by default).\n# For non-x86-64 architectures (ARM, POWER9, etc.), prepend `DISABLE_FPU=1` to \"bash\".\n( cd lib && bash fetch_and_build_sat_solvers.sh )\n\n# Build Mallob\nmkdir -p build\ncd build\nCC=$(which mpicc) CXX=$(which mpicxx) cmake -DCMAKE_BUILD_TYPE=RELEASE -DMALLOB_APP_SAT=1 -DMALLOB_USE_JEMALLOC=1 -DMALLOB_LOG_VERBOSITY=4 -DMALLOB_ASSERT=1 -DMALLOB_SUBPROC_DISPATCH_PATH=\\\"build/\\\" ..\nmake; cd ..\n\n# Optional - only needed for on-the-fly LRAT checking\n( cd lib && bash fetch_and_build_impcheck.sh && cp impcheck/build/impcheck_* ../build/ )\n```\n\nSpecify `-DCMAKE_BUILD_TYPE=RELEASE` for a release build or `-DCMAKE_BUILD_TYPE=DEBUG` for a debug build.\nYou can use the following Mallob-specific build options:\n\n| Usage                                       | Description                                                                                                |\n| ------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n| -DMALLOB_ASSERT=<0/1>                       | Turn on assertions (even on release builds). Setting to 0 limits assertions to debug builds.               |\n| -DMALLOB_JEMALLOC_DIR=path                  | If necessary, provide a path to a local installation of `jemalloc` where `libjemalloc.*` is located.       |\n| -DMALLOB_LOG_VERBOSITY=<0..6>               | Only compile logging messages of the provided maximum verbosity and discard more verbose log calls.        |\n| -DMALLOB_SUBPROC_DISPATCH_PATH=\\\\\"path\\\\\"   | Subprocess executables must be located under <path> for Mallob to find. (Use `\\\"build/\\\"` by default.)     |\n| -DMALLOB_USE_ASAN=<0/1>                     | Compile with Address Sanitizer for debugging purposes.                                                     |\n| -DMALLOB_USE_GLUCOSE=<0/1>                  | Compile with support for Glucose SAT solver (disabled by default for licensing reasons, see below).        |\n| -DMALLOB_USE_JEMALLOC=<0/1>                 | Compile with Scalable Memory Allocator `jemalloc` instead of default `malloc`.                             |\n| -DMALLOB_APP_KMEANS=<0/1>                   | Compile with K-Means clustering engine.                                                                    |\n| -DMALLOB_APP_SAT=<0/1>                      | Compile with SAT solving engine.                                                                           |\n| -DMALLOB_MAX_N_APPTHREADS_PER_PROCESS=<N>   | Max. number of application threads (solver threads for SAT) per process to support. (max: 128)             |\n| -DMALLOB_BUILD_LRAT_MODULES=<0/1>           | Also build standalone LRAT checker                                                                         |\n\n## Docker\n\nWe also provide a setup based on Docker containerization. Please consult the documentation in the `docker/` directory.\n\n## Bash Autocompletion\n\nMallob features bash auto-completion by pressing TAB. To enable this, execute this command from Mallob's base directory:\n\n  source scripts/run/autocomplete.sh\n\nFrom this directory you can now autocomplete program options by pressing TAB once or twice.\n\n<hr/>\n\n# Testing\n\n**Note:** In its current state, the test suite expects that Mallob is built and run with OpenMPI, i.e., that `mpicc` and `mpicxx` (for building) and `mpirun` (for execution) link to OpenMPI executables on your system. For other MPI implementations, you may still be able to run the tests by removing or replacing the option `--oversubscribe` from the function `run()` in `scripts/run/systest_commons.sh`.\n\nIn order to test that the system has been built and set up correctly, run the following command.\n```\nbash scripts/run/systest.sh mono drysched sched osc\n```\nThis will locally run a suite of automated tests which cover the basic functionality of Mallob as a scheduler and as a SAT solving engine. \nTo include Glucose in the tests, prepend the above command with \"GLUCOSE=1\".\nRunning the tests takes a few minutes and in the end \"All tests done.\" should be output.\n\n<hr/>\n\n# Usage\n\nWe first explain how to execute Mallob in general, then detail how to solve an isolated problem and then turn to solving many instances in a row or at the same time.\n\n## General\n\nGiven a single machine with two hardware threads per core, the following command executed in Mallob's base directory assigns one MPI process to each set of four physical cores (eight hardware threads) and then runs four solver threads on each MPI process.\n\n```\nRDMAV_FORK_SAFE=1; NPROCS=\"$(($(nproc)/8))\"; mpirun -np $NPROCS --bind-to core --map-by ppr:${NPROCS}:node:pe=4 build/mallob -t=4 $MALLOB_OPTIONS\n```\n\nGiven a machine with `$nthreads` cores (and twice the number of hardware threads), the following command spawns a single process with one solver thread per core (per hardware thread):\n\n```\nRDMAV_FORK_SAFE=1; mpirun -np 1 --bind-to core --map-by ppr:1:node:pe=$nthreads build/mallob -t=$nthreads $MALLOB_OPTIONS\nRDMAV_FORK_SAFE=1; mpirun -np 1 --bind-to hwthread --map-by ppr:1:node:pe=$((2*$nthreads)) build/mallob -t=$((2*$nthreads)) $MALLOB_OPTIONS\n```\n\nAlternatively, only executing `build/mallob -t=$nthreads $MALLOB_OPTIONS` works as well in this case but does not pin threads to cores.\n\nYou can always stop Mallob via Ctrl+C (interrupt signal) or by executing `killall mpirun` (or `killall build/mallob`). \nYou can also specify the number of jobs to process (with `-J=$NUM_JOBS`) and/or the time to pass (with `-T=$TIME_LIMIT_SECS`) before Mallob terminates on its own.\n\nFor exact and clean logging, you should not rely on a textfile in which you piped Mallob's output.\nInstead, specify a logging directory with `-log=<log-dir>` where separate sub-directories and files will be created for each worker / thread. \nThis can be combined with the `-q` option to suppress Mallob's output to STDOUT. \nVerbosity of logging can be set with the `-v` option (as long as Mallob was compiled with the respective verbosity or higher, see `-DMALLOB_LOG_VERBOSITY` above).\nAll further options of Mallob can be seen by executing Mallob with the `-h` option. (This also works without the `mpirun` prefix.)\n\nFor running Mallob on distributed clusters, please also consult [our quickstart guide for clusters](docs/clusters.md) and/or the user documentation of your particular cluster.\n\n## SAT Solving\n\nIn general, in order to let Mallob process only a single instance, use option `-mono=$PROBLEM_FILE` where `$PROBLEM_FILE` is the path and file name of the problem to solve (DIMACS CNF format, possibly with .xz or .lzma compression, for SAT; whitespace-separated plain text file for K-Means). Specify the application of this instance with `-mono-app=sat` or `-mono-app=kmeans`.\n\nIn this mode, all processes participate in solving, overhead is minimal, and Mallob terminates immediately after the job has been processed.\nUse option `-s2f=path/to/output.txt` (\"solution to file\") to write the result and (if applicable) the found satisfying assignment to a text file.\n\n### Producing Proofs of Unsatisfiability\n\nTo enable proof production, just set the option `-proof=path/to/final/compressed/prooffile.lrat` together with `-mono=path/to/input.cnf`. You also need to set a log directory with `-proof-dir=path/to/dir` where intermediate files will be written to on each machine. The final proof is output in compressed LRAT format.\n\nCaDiCaL is currently the only supported solver backend for parallel/distributed proof production. However, it is possible to employ other solvers as long as their only purpose is to find a satisfying assignment (i.e., exported clauses and unsatisfiability results from these solvers are discarded). See *Portfolio Tweaking* below.\n\nFor instance, you can check the output proof with the standalone LRAT checker that comes with Mallob if you set `-DMALLOB_BUILD_LRAT_MODULES=1` at build time:\n```bash\nbuild/standalone_lrat_checker path/to/input.cnf path/to/final/compressed/prooffile.lrat\n```\nFurther synergies are possible; you can set `-uninvert=0` for Mallob and `--reversed` for the checker to avoid one entire I/O pass over the proof that \"uninverts\" its lines.\nYou can use the [`drat-trim`](https://github.com/marijnheule/drat-trim) tool suite to decompress a proof; note that you need to `#define MODE 2` (1=DRAT, 2=LRAT) in `decompress.c` before building.\n\n### Trusted solving *without* explicit proof production\n\nProof production can be costly and bottlenecked by the I/O bandwidth of the single process which needs to write the entire proof. A more scalable approach is to check all proof information on-the-fly, without writing it to disk, and to transfer clause soundness guarantees across machines via cryptographic signatures. This is explained in detail in our [2024 SAT publication](https://dominikschreiber.de/papers/2024-sat-trusted-pre.pdf).\n\nExecute the command chain fetching and building [ImpCheck](https://github.com/domschrei/impcheck) in the above *Building* section. Then just use Mallob's option `-otfc=1` (without any `-proof*` options) to enable on-the-fly checking. Again, only CaDiCaL is supported for UNSAT whereas any solver can be employed for boosting satisfying assignments. By default, found satisfying assignments are also validated, which can be disabled via `-otfcm=0`.\n\nLog lines of the following shape are reporting a trusted result from a proof checking process:\n```\nc 0.851 0 <#11606> S0.0 TRUSTED checker reported UNSAT - sig c6c0a823f35ce38cdb31c9483dc98143\n```\n```\nc 0.242 0 <#11607> S0.0 TRUSTED checker reported SAT - sig a43e47d81715035d79290d1a6acf05e8\n```\nTo be extra safe (e.g., if you are suspecting garbled or tampered-with logging output), execute the following command to validate the output signature:\n```bash\nbuild/impcheck_confirm -formula-input=path/to/input.cnf -result=X -result-sig=SIG\n```\nwhere `X` is either 10 (for SAT) or 20 (for UNSAT), and `SIG` is the reported signature.\n\n**Note:** On-the-fly checking can also be used in Mallob's scheduled mode of operation. Globally unique clause IDs are ensured by adding a large offset times $x$ to a new solver thread's clause ID counter if the job has already experienced $x$ _balancing epochs_, i.e., received $x$ volume updates, since its initialization. The offset is chosen in such a way that 10,000 solvers each producing 10,000 clauses per second can run for 10,000 seconds before they may begin overlapping with clause IDs from the next balancing epoch. `ImpCheck` notices and reports any errors that would result from such a corner case.\n\n### Portfolio Tweaking\n\nMallob allows to customize the employed SAT solver backends and some of their flavors. This is done with the `-satsolver` option, which expects a string representing the solver backends to cycle over. The option also allows for a limited set of regular expression symbols. Here are some examples:\n```bash\n... -satsolver='c' # CaDiCaL only.\n... -satsolver='kcl' # Kissat, CaDiCaL, Lingeling, Kissat, CaDiCaL, Lingeling, Kissat, ...\n... -satsolver='k(c)*' # One Kissat, then only CaDiCaL (always put brackets around the argument of '*')\n... -satsolver='kCLCLcl' # Capital letters indicate using truly incremental SAT solving for incremental jobs\n... -satsolver='l+(c!){37}' # One Lingeling configured for satisfiable instances (+), then 37 LRAT-producing (!) CaDiCaLs, repeat\n... -satsolver='(c!){37}k+((c!){37}l+)*' # As above, but replacing the 1st Lingeling with Kissat\n```\n\n## Solve multiple instances in an orchestrated manner\n\nIf you want to solve a fixed set of $n$ formulae or wish to evaluate Mallob's scheduling behavior with simulated jobs, follow these steps:\n\n* Write the set of formulae into a text file `$INSTANCE_FILE` (one line per path).\n* Configure the base properties of a job with a JSON file `$JOB_TEMPLATE`. For a plain job with default properties you can use `templates/job-template.json`.\n* Configure the behavior of each job-introducing process (\"client\") with a JSON file `$CLIENT_TEMPLATE`. You can find the simplest possible configuration in `templates/client-template.json` and a more complex randomized configuration in `templates/client-template-random.json`. Both files contain all necessary documentation to adjust them as desired.\n\nThen use these Mallob options:\n```\n-c=1 -ajpc=$MAX_PAR_JOBS -ljpc=$((2*$MAX_PAR_JOBS)) -J=$NUM_JOBS -job-desc-template=$INSTANCE_FILE -job-template=$JOB_TEMPLATE -client-template=$CLIENT_TEMPLATE -pls=0\n```\nwhere `$NUM_JOBS` is set to $n$ (if it is larger than $n$, a client cycles through the provided job descriptions indefinitely). You can set `-sjd=1` to shuffle the provided job descriptions. You can also increase the number of client processes introducing jobs by increasing the value of `-c`. However, note that the provided configuration for active jobs in the system is applied to each of the clients independently, hence the formulae provided in the instance file are not split up among the clients but rather duplicated.\n\n## Process jobs on demand\n\nThis is the default and most general configuration of Mallob, i.e., without `-mono` or `-job-template` options.\nYou can manually set the number of worker processes (`-w`) and the number of client processes introducing jobs (`-c`). By default, all processes are workers (`-w=-1`) and a single process is additionally a client (`-c=1`). The $k$ client processes are always the $k$ processes of the highest ranks, and they open up file system interfaces for introducing jobs and retrieving results at the directories `.api/jobs.0/` through `.api/jobs.`$k-1$`/`.\n\n### Introducing a Job\n\nTo introduce a job to the system, drop a JSON file in `.api/jobs.`$i$`/in/` (e.g., `.api/jobs.0/in/`) on the filesystem of the according PE structured like this:  \n```\n{\n    \"application\": \"SAT\",\n    \"user\": \"admin\", \n    \"name\": \"test-job-1\", \n    \"files\": [\"/path/to/difficult/formula.cnf\"], \n    \"priority\": 0.7, \n    \"wallclock-limit\": \"5m\", \n    \"cpu-limit\": \"10h\",\n    \"arrival\": 10.3,\n    \"dependencies\": [\"admin.prereq-job1\", \"admin.prereq-job2\"],\n    \"incremental\": false\n}\n```    \n\nHere is a brief overview of all required and optional fields in the JSON API:\n\n| Field name        | Required? | Value type   | Description                                                                                                    |\n| ----------------- | :-------: | -----------: | -------------------------------------------------------------------------------------------------------------- |\n| user              | **yes**   | String       | A string specifying the user who is submitting the job                                                         |\n| name              | **yes**   | String       | A user-unique name for this job (increment)                                                                    |\n| files             | **yes***  | String array | File paths of the input to solve. For SAT, this must be a single (text file or compressed file or named pipe). |\n| priority          | **yes***  | Float > 0    | Priority of the job (higher is more important)                                                                 |\n| application       | **yes**   | String       | Which kind of problem is being solved; currently either of \"SAT\" or \"DUMMY\" (default: DUMMY)                   |\n| wallclock-limit   | no        | String       | Job wallclock limit: combination of a number and a unit (ms/s/m/h/d)                                           |\n| cpu-limit         | no        | String       | Job CPU time limit: combination of a number and a unit (ms/s/m/h/d)                                            |\n| arrival           | no        | Float >= 0   | Job's arrival time (seconds) since program start; ignore job until then                                        |\n| max-demand        | no        | Int >= 0     | Override the max. number of MPI processes this job should receive at any point in time (0: no limit)           |\n| dependencies      | no        | String array | User-qualified job names (using \".\" as a separator) which must exit **before** this job is introduced          |\n| interrupt         | no        | Bool         | If `true`, the job given by \"user\" and \"name\" is interrupted (for incremental jobs, just the current revision).|\n| incremental       | no        | Bool         | Whether this job has multiple _increments_ / _revisions_ and should be treated as such                         |\n| literals          | no        | Int array    | You can specify the set of SAT literals (for this increment) directly in the JSON.                             |\n| precursor         | no        | String       | _(Only for incremental jobs)_ User-qualified job name (`<user>.<jobname>`) of this job's previous increment    |\n| assumptions       | no        | Int array    | _(Only for incremental jobs)_ You can specify the set of assumptions for this increment directly in the JSON.  |\n| done              | no        | Bool         | _(Only for incremental jobs)_ If `true`, the incremental job given by \"precursor\" is finalized and cleaned up. |\n\n*) Not needed if `done` is set to `true`.\n\nIn the above example, a job is introduced with priority 0.7, with a wallclock limit of five minutes and a CPU limit of 10 CPUh.\n\nFor SAT solving, the input can be provided (a) as a plain file, (b) as a compressed (.lzma / .xz) file, or (c) as a named (UNIX) pipe.\nIn each case, you have the option of providing the payload (i) in text form (i.e., a valid CNF description), or, with field `content-mode: \"raw\"`, in binary form (i.e., a sequence of bytes representing integers).  \nFor text files, Mallob uses the common iCNF extension for incremental formulae: The file may contain a single line of the form `a <lit1> <lit2> ... 0` where `<lit1>`, `<lit2>` etc. are assumption literals.   \nFor binary files, Mallob reads clauses as integer sequences with separation zeroes in between.\nTwo zeroes in a row (i.e., an \"empty clause\") signal the end of clause literals, after which a number of assumption integers may be specified. Another zero signals that the description is complete.  \nIf providing a named pipe, make sure that (a) the named pipe is already created when submitting the job and (b) your application pipes the formula _after_ submitting the job (else it will hang indefinitely except if this is done in a separate thread).\n\nAssumptions can also be specified directly in the JSON describing the job via the `assumptions` field (without any trailing zero). This way, an incremental application could maintain a single text file with a monotonically growing set of clauses.\n\nThe \"arrival\" and \"dependencies\" fields are useful to test a particular preset scenario of jobs: The \"arrival\" field ensures that the job will be scheduled only after Mallob ran for the specified amount of seconds. The \"dependencies\" field ensures that the job is scheduled only if all specified other jobs are already processed.\n\nMallob is notified by the kernel as soon as a valid file is placed in `.api/jobs.0/in/` and will immediately remove the file and schedule the job.\n\n### Retrieving a Job Result\n\nUpon completion of a job, Mallob writes a result JSON file under `.api/jobs.0/out/<user-name>.<job-name>.json` (you can repeatedly query the directory contents or employ a kernel-level mechanism like `inotify`).\nSuch a file may look like this:\n```\n{\n    \"application\": \"SAT\",\n    \"cpu-limit\": \"10h\",\n    \"file\": \"/path/to/difficult/formula.cnf\",\n    \"name\": \"test-job-1\",\n    \"priority\": 0.7,\n    \"result\": {\n        \"resultcode\": 10,\n        \"resultstring\": \"SAT\",\n        \"solution\": [0, 1, 2, 3, 4, 5]\n    },\n    \"stats\": {\n        \"time\": {\n            \"parsing\": 0.03756427764892578,\n            \"processing\": 0.07197785377502441,\n            \"scheduling\": 0.0002980232238769531,\n            \"total\": 0.11040472984313965\n        },\n        \"used_cpu_seconds\": 0.2633516788482666,\n        \"used_wallclock_seconds\": 0.06638360023498535\n    },\n    \"user\": \"admin\",\n    \"wallclock-limit\": \"5m\"\n}\n```\nThe result code is 0 is unknown, 10 if SAT (solved successfully), and 20 if UNSAT (no solution exists).\nThe `solution` field is application-dependent.\nFor SAT solving, in case of SATISFIABLE, the solution field contains the found satisfying assignment; in case of UNSAT, the result for an incremental job contains the set of failed assumptions.\nInstead of the \"solution\" field, the response may also contain the fields \"solution-size\" and \"solution-file\" if the solution is large and if option `-pls` is set. In that case, your application has to read `solution-size` integers (as bytes) representing the solution from the named pipe located at `solution-file`.\n\n<hr/>\n\n# Debugging\n\nDebugging of distributed applications can be difficult, especially in Mallob's case where message passing goes hand in hand with multithreading and inter-process communication. Please take a look at [docs/debugging.md](docs/debugging.md) for some notes on how Mallob runs can be diagnosed and debugged appropriately.\n\n<hr/>\n\n# Programming Interfaces\n\nMallob can be extended in the following ways:\n\n* New options for Mallob can be added in `src/optionslist.hpp`.\n    - Options which are specific to a certain application can be found and edited in `src/app/$APPKEY/options.hpp`.\n* To add a new SAT solver to be used in a SAT solver engine, do the following:\n    - Add a subclass of `PortfolioSolverInterface`. (You can use the existing implementation for any of the existing solvers and adapt it to your solver.)\n    - Add your solver to the portfolio initialization in `src/app/sat/execution/engine.cpp`.\n* To extend Mallob by adding another kind of application (like combinatorial search, planning, SMT, ...), please read [docs/application_engines.md](docs/application_engines.md).\n* To add a unit test, create a class `test_*.cpp` in `src/test` and then add the test case to the bottom of `CMakeLists.txt`.\n* To add a system test, consult the files `scripts/systest_commons.sh` and/or `scripts/systest.sh`.\n\n<hr/>\n\n# Licensing and remarks\n\nThe source code of Mallob can be used, changed and redistributed under the terms of the **Lesser General Public License (LGPLv3)**, one exception being the Glucose interface which is excluded from compilation by default (see below).\n**Please approach us if you require a deviating license.**\n\nThe used versions of Lingeling, YalSAT, CaDiCaL, and Kissat are MIT-licensed, as is HordeSat (the massively parallel solver system our SAT engine was based on) and the proof-related tools which are included and/or fetched in the `tools/` directory.\n\nThe Glucose interface of Mallob, unfortunately, is non-free software due to the [non-free license of (parallel-ready) Glucose](https://github.com/mi-ki/glucose-syrup/blob/master/LICENCE). Notably, its usage in competitive events is restricted. So when compiling Mallob with `-DMALLOB_USE_GLUCOSE=1` make sure that you have read and understood these restrictions.\n\nWithin our codebase we make thankful use of the following liberally licensed projects:\n\n* [robin-map](https://github.com/Tessil/robin-map) by Thibaut Goetghebuer-Planchon, for efficient unordered maps and sets\n* [libcuckoo](https://github.com/efficient/libcuckoo) by Manu Goyal et al., for concurrent hash tables\n* [JSON for Modern C++](https://github.com/nlohmann/json) by Niels Lohmann, for reading and writing JSON files\n* [Compile Time Regular Expressions](https://github.com/hanickadot/compile-time-regular-expressions) by Hana Dusíková, for matching particular user inputs\n* [robin_hood hashing](https://github.com/martinus/robin-hood-hashing) by Martin Ankerl, for efficient unordered maps and sets\n\n## Bibliography\n\nIf you make use of Mallob in an academic setting or in a competitive event, please cite the most relevant among the following publications.\n\n#### SAT'21: Focus on SAT solving\n```bibtex\n@inproceedings{schreiber2021scalable,\n  title={Scalable SAT Solving in the Cloud},\n  author={Schreiber, Dominik and Sanders, Peter},\n  booktitle={International Conference on Theory and Applications of Satisfiability Testing},\n  pages={518--534},\n  year={2021},\n  organization={Springer},\n  doi={10.1007/978-3-030-80223-3_35}\n}\n```\n#### Euro-Par'22: Focus on decentralized scheduling\n```bibtex\n@inproceedings{sanders2022decentralized,\n  title={Decentralized Online Scheduling of Malleable {NP}-hard Jobs},\n  author={Sanders, Peter and Schreiber, Dominik},\n  booktitle={International European Conference on Parallel and Distributed Computing},\n  pages={119--135},\n  year={2022},\n  organization={Springer},\n  doi={10.1007/978-3-031-12597-3_8}\n}\n```\n#### TACAS'23: Proofs of unsatisfiability\n```bibtex\n@InProceedings{michaelson2023unsatisfiability,\n  author={Michaelson, Dawn and Schreiber, Dominik and Heule, Marijn J. H. and Kiesl-Reiter, Benjamin and Whalen, Michael W.},\n  title={Unsatisfiability proofs for distributed clause-sharing SAT solvers},\n  booktitle={Tools and Algorithms for the Construction and Analysis of Systems (TACAS)},\n  year={2023},\n  organization={Springer},\n  pages={348--366},\n  doi={10.1007/978-3-031-30823-9_18},\n}\n```\n#### SAT Competition TRs\n```bibtex\n@article{schreiber2020engineering,\n  title={Engineering HordeSat Towards Malleability: mallob-mono in the {SAT} 2020 Cloud Track},\n  author={Schreiber, Dominik},\n  journal={SAT Competition 2020},\n  pages={45--46}\n}\n@article{schreiber2021mallob,\n  title={Mallob in the {SAT} Competition 2021},\n  author={Schreiber, Dominik},\n  journal={SAT Competition 2021},\n  pages={38--39}\n}\n@article{schreiber2022mallob,\n  title={Mallob in the {SAT} Competition 2022},\n  author={Schreiber, Dominik},\n  journal={SAT Competition 2022},\n  pages={46--47}\n}\n@article{schreiber2023mallob,\n  title={Mallob\\{32,64,1600\\} in the {SAT} Competition 2023},\n  author={Schreiber, Dominik},\n  journal={SAT Competition 2023},\n  pages={46--47}\n}\n```\n#### Doctoral thesis (featuring all of the above + new content)\n```bibtex\n@phdthesis{schreiber2023scalable,\n  author={Dominik Schreiber},\n  title={Scalable {SAT} Solving and its Application},\n  year={2023},\n  school={Karlsruhe Institute of Technology},\n  doi={10.5445/IR/1000165224}\n}\n```\n\nFurther references:\n\n* **[Mallob IPASIR Bridge for incremental SAT solving](https://github.com/domschrei/mallob-ipasir-bridge)**\n* **[ImpCheck - Immediate Massively Parallel Propositional Proof Checking](https://github.com/domschrei/impcheck)**\n* **[Experimental data at Zenodo](https://zenodo.org/doi/10.5281/zenodo.10184679)**\n* **[Mallob at Helmholtz Research Software Directory (RSD)](https://helmholtz.software/software/mallob)**\n\n",
                "dependencies": "cmake_minimum_required(VERSION 3.11.4)\nproject (mallob)\n\nfind_package(MPI REQUIRED)\n\nset(THREADS_PREFER_PTHREAD_FLAG ON)\nfind_package(Threads REQUIRED)\n\nset(CMAKE_CXX_STANDARD 17)\n\n\n# Build-specific compile options\n\nset(BASE_LIBS Threads::Threads)\nset(MY_DEBUG_OPTIONS \"-rdynamic -g -ggdb\")\n\nif(CMAKE_BUILD_TYPE MATCHES DEBUG)\n    add_definitions(-DMALLOB_VERSION=\\\"dbg\\\")\n    if(MALLOB_ASSERT_HEAVY)\n        add_definitions(-DMALLOB_ASSERT=2)\n    else()\n        add_definitions(-DMALLOB_ASSERT=1)\n    endif()\nelse()\n    add_definitions(-DMALLOB_VERSION=\\\"rls\\\")\n    if(MALLOB_ASSERT_HEAVY)\n        add_definitions(-DMALLOB_ASSERT=2)\n    elseif(MALLOB_ASSERT)\n        add_definitions(-DMALLOB_ASSERT=1)\n    endif() \nendif()\n\nif(MALLOB_LOG_VERBOSITY)\n    add_definitions(-DLOGGER_STATIC_VERBOSITY=${MALLOB_LOG_VERBOSITY})\nendif()\n\nif(MALLOB_MAX_N_APPTHREADS_PER_PROCESS)\n    add_definitions(-DMALLOB_MAX_N_APPTHREADS_PER_PROCESS=${MALLOB_MAX_N_APPTHREADS_PER_PROCESS})\nendif()\n\nif(MALLOB_SUBPROC_DISPATCH_PATH)\n    add_definitions(-DMALLOB_SUBPROC_DISPATCH_PATH=${MALLOB_SUBPROC_DISPATCH_PATH})\nendif()\n\nif(DEFINED MALLOB_TRUSTED_SUBPROCESSING)\n    add_definitions(-DMALLOB_TRUSTED_SUBPROCESSING=${MALLOB_TRUSTED_SUBPROCESSING})\nendif()\n\nif(MALLOB_USE_ASAN)\n    set(MY_DEBUG_OPTIONS \"${MY_DEBUG_OPTIONS} -fno-omit-frame-pointer -fsanitize=address -static-libasan\") \nendif()\n\nif(MALLOB_USE_TBBMALLOC)\n    set(BASE_LIBS tbbmalloc_proxy ${BASE_LIBS})\nendif()\n\nif(MALLOB_USE_JEMALLOC)\n    if(MALLOB_JEMALLOC_DIR)\n        link_directories(${MALLOB_JEMALLOC_DIR})\n    endif()\n    set(BASE_LIBS jemalloc ${BASE_LIBS})\nendif()\n\n# Default application\nif(NOT DEFINED MALLOB_APP_SAT)\n    set(MALLOB_APP_SAT 1)\nendif()\n\n\n# Libraries and includes\n\nset(BASE_LIBS ${BASE_LIBS} ${MPI_CXX_LIBRARIES} ${MPI_CXX_LINK_FLAGS} m z rt dl CACHE INTERNAL \"\")\nset(BASE_INCLUDES ${MPI_CXX_INCLUDE_PATH} src src/util/tsl lib CACHE INTERNAL \"\")\nset(BASE_COMPILEFLAGS CACHE INTERNAL \"\")\n\n\n# Base source files\n\nset(BASE_SOURCES ${BASE_SOURCES} src/app/job.cpp src/app/app_registry.cpp src/app/app_message_subscription.cpp src/balancing/event_driven_balancer.cpp src/balancing/request_matcher.cpp src/balancing/routing_tree_request_matcher.cpp src/comm/msg_queue/message_queue.cpp src/comm/mpi_base.cpp src/comm/mympi.cpp src/comm/sysstate_unresponsive_crash.cpp src/core/scheduling_manager.cpp src/data/job_description.cpp src/data/job_result.cpp src/data/job_transfer.cpp src/interface/json_interface.cpp src/interface/api/api_connector.cpp src/scheduling/job_scheduling_update.cpp src/util/logger.cpp src/util/option.cpp src/util/params.cpp src/util/permutation.cpp src/util/random.cpp src/util/sys/atomics.cpp src/util/sys/fileutils.cpp src/util/sys/process.cpp src/util/sys/proc.cpp src/util/sys/process_dispatcher.cpp src/util/sys/shared_memory.cpp src/util/sys/tmpdir.cpp src/util/sys/terminator.cpp src/util/sys/threading.cpp src/util/sys/thread_pool.cpp src/util/sys/timer.cpp src/util/sys/watchdog.cpp src/util/ringbuf/ringbuf.c CACHE INTERNAL \"\")\n\n# Use to debug\n#message(\"mallob_commons sources pre application registration: ${BASE_SOURCES}\")\n\n\n# Define test function\n\nfunction(new_test testname)\n    message(\"Adding test: ${testname}\")\n    add_executable(test_${testname} src/test/test_${testname}.cpp)\n    target_include_directories(test_${testname} PRIVATE ${BASE_INCLUDES})\n    target_compile_options(test_${testname} PRIVATE ${BASE_COMPILEFLAGS})\n    target_link_libraries(test_${testname} mallob_commons)\n    add_test(NAME test_${testname} COMMAND test_${testname})\nendfunction()\n\n\n# Add application-specific build configuration\n\n# Setup (do not change)\nfile(WRITE \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_includes.h~\" \"\")\nfile(WRITE \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_options.h~\" \"\")\nfile(WRITE \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_commands.h~\" \"\")\nfunction(register_mallob_app appkey)\n    message(\"Registering application: ${appkey}\")\n    file(APPEND \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_includes.h~\" \"#include \\\"app/${appkey}/register.hpp\\\"\\n\")\n    file(APPEND \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_options.h~\" \"#include \\\"app/${appkey}/options.hpp\\\"\\n\")\n    file(APPEND \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_commands.h~\" \"register_mallob_app_${appkey}();\\n\")\n    include(${CMAKE_CURRENT_SOURCE_DIR}/src/app/${appkey}/setup.cmake)\nendfunction()\nfunction(publish_app_registry_changes)\n    execute_process(COMMAND ${CMAKE_COMMAND} -E compare_files \n        \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_options.h\" \n        \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_options.h~\" \n        RESULT_VARIABLE compare_result)\n    if(compare_result EQUAL 0)\n        message(\"No changes in register options.\")\n    else()\n        message(\"Moving changed app registry.\")\n        file(RENAME \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_includes.h~\" \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_includes.h\")\n        file(RENAME \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_options.h~\" \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_options.h\")\n        file(RENAME \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_commands.h~\" \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_commands.h\")\n    endif() \nendfunction()\n\n# Include applications\nregister_mallob_app(\"dummy\") # always register the \"no-op\" application\nif(MALLOB_APP_SAT) \n    register_mallob_app(\"sat\")\nendif()\nif(MALLOB_APP_KMEANS)\n    register_mallob_app(\"kmeans\")\nendif()\n# Include further applications here:\n#if(MALLOB_APP_YOURAPP)\n#    register_mallob_app(\"yourapp\")\n#endif()\n# ...\npublish_app_registry_changes()\n\n\n# Library with Mallob's common source files\n\n#message(\"mallob_commons sources post application registration: ${BASE_SOURCES}\") # Use to debug\nadd_library(mallob_commons\n    STATIC ${BASE_SOURCES}\n)\ntarget_include_directories(mallob_commons PRIVATE ${BASE_INCLUDES})\ntarget_compile_options(mallob_commons PRIVATE ${BASE_COMPILEFLAGS})\ntarget_link_libraries(mallob_commons ${BASE_LIBS})\n\n\n# Executables\n\nadd_executable(mallob src/core/client.cpp src/core/worker.cpp src/main.cpp)\ntarget_include_directories(mallob PRIVATE ${BASE_INCLUDES})\ntarget_compile_options(mallob PRIVATE ${BASE_COMPILEFLAGS})\ntarget_link_libraries(mallob mallob_commons)\n\nadd_executable(mallob_process_dispatcher src/app/sat/main_dispatch.cpp)\ntarget_include_directories(mallob_process_dispatcher PRIVATE ${BASE_INCLUDES})\ntarget_compile_options(mallob_process_dispatcher PRIVATE ${BASE_COMPILEFLAGS})\ntarget_link_libraries(mallob_process_dispatcher mallob_commons)\n\n\n# Debug flags to find line numbers in stack traces etc.\n\nadd_definitions(\"${MY_DEBUG_OPTIONS}\")\nSET(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${MY_DEBUG_OPTIONS}\")\n\n\n# Tests\n\nenable_testing()\n\nnew_test(bitsets)\nnew_test(permutation)\nnew_test(message_queue)\nnew_test(volume_calculator)\nnew_test(concurrent_malloc)\nnew_test(hashing)\nnew_test(async_collective)\nnew_test(random)\nnew_test(reverse_file_reader)\nnew_test(categorized_external_memory)\nnew_test(bidirectional_pipe)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mallocmc",
            "repo_link": "https://github.com/alpaka-group/mallocMC",
            "content": {
                "codemeta": "",
                "readme": "mallocMC\n=============\n\nmallocMC: *Memory Allocator for Many Core Architectures*\n\nThis project provides a framework for **fast memory managers** on **many core\naccelerators**. It is based on [alpaka](https://github.com/alpaka-group/alpaka)\nto run on many different accelerators and comes with multiple allocation\nalgorithms out-of-the-box. Custom ones can be added easily due to the\npolicy-based design.\n\nUsage\n-------\n\nFollow the step-by-step instructions in [Usage.md](Usage.md) to replace your\n`new`/`malloc` calls with a *blacingly fast* mallocMC heap! :rocket:\n\nInstall\n-------\n\nmallocMC is header-only, but requires a few other C++ libraries to be\navailable. Our installation notes can be found in [INSTALL.md](INSTALL.md).\n\nContributing\n------------\n\nRules for contributions are found in [CONTRIBUTING.md](./CONTRIBUTING.md).\n\nOn the Algorithms\n-----------------------------\n\nThis library was originally inspired by the *ScatterAlloc* algorithm,\n[forked](https://en.wikipedia.org/wiki/Fork_%28software_development%29)\nfrom the **ScatterAlloc** project, developed by the\n[Managed Volume Processing](http://www.icg.tugraz.at/project/mvp)\ngroup at [Institute for Computer Graphics and Vision](http://www.icg.tugraz.at),\nTU Graz (kudos!). The currently shipped algorithms are using similar ideas but\ndiffer from the original one significantly.\n\nFrom the original project page (which is no longer existent to the best of our\nknowledge):\n\n```quote\nScatterAlloc is a dynamic memory allocator for the GPU. It is\ndesigned concerning the requirements of massively parallel\nexecution.\n\nScatterAlloc greatly reduces collisions and congestion by\nscattering memory requests based on hashing. It can deal with\nthousands of GPU-threads concurrently allocating memory and its\nexecution time is almost independent of the thread count.\n\nScatterAlloc is open source and easy to use in your CUDA projects.\n```\n\nOur Homepage: <https://www.hzdr.de/crp>\n\nVersions and Releases\n---------------------\n\nOfficial releases can be found in the\n[Github releases](https://github.com/alpaka-group/mallocMC/releases).\nWe try to stick to [semantic versioning](https://semver.org/) but we'll bump\nthe major version number for major features.\nDevelopment happens on the `dev` branch.\nChanges there have passed the CI and a code review but we make no guarantees\nabout API or feature stability in this branch.\n\nLiterature\n----------\n\nJust an incomplete link collection for now:\n\n- [Paper](https://doi.org/10.1109/InPar.2012.6339604) by\n  Markus Steinberger, Michael Kenzel, Bernhard Kainz and Dieter Schmalstieg\n\n- 2012, May 5th: [Presentation](http://innovativeparallel.org/Presentations/inPar_kainz.pdf)\n        at *Innovative Parallel Computing 2012* by *Bernhard Kainz*\n\n- Junior Thesis [![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.34461.svg)](http://dx.doi.org/10.5281/zenodo.34461) by\n  Carlchristian Eckert (2014)\n\nLicense\n-------\n\nWe distribute the modified software under the same license as the\noriginal software from TU Graz (by using the\n[MIT License](https://en.wikipedia.org/wiki/MIT_License)).\nPlease refer to the [LICENSE](LICENSE) file.\n\n",
                "dependencies": "project(mallocMC LANGUAGES CXX)\ncmake_minimum_required(VERSION 3.18)\n\nset(CMAKE_CXX_STANDARD 20)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\nif(POLICY CMP0074)\n  cmake_policy(SET CMP0074 NEW)\nendif()\n\n# find alpaka\nset(mallocMC_ALPAKA_PROVIDER \"intern\" CACHE STRING \"Select which alpaka is used\")\nset_property(CACHE mallocMC_ALPAKA_PROVIDER PROPERTY STRINGS \"intern;extern\")\nmark_as_advanced(mallocMC_ALPAKA_PROVIDER)\nif(${mallocMC_ALPAKA_PROVIDER} STREQUAL \"intern\")\n  set(alpaka_BUILD_EXAMPLES OFF)\n  set(BUILD_TESTING OFF)\n  add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/alpaka ${CMAKE_BINARY_DIR}/alpaka)\nelse()\n  find_package(alpaka HINTS $ENV{ALPAKA_ROOT})\nendif()\n\nif(NOT TARGET alpaka::alpaka)\n  message(FATAL \"Required mallocMC dependency alpaka could not be found!\")\nendif()\n\n# Catch2\nset(mallocMC_CATCH2_PROVIDER \"intern\" CACHE STRING \"Select which Catch2 is used\")\nset_property(CACHE mallocMC_CATCH2_PROVIDER PROPERTY STRINGS \"intern;extern\")\nmark_as_advanced(mallocMC_CATCH2_PROVIDER)\n\n# for installation, just copy include folder to install folder\ninstall(\n    DIRECTORY \"${CMAKE_CURRENT_SOURCE_DIR}/src/include/.\"\n    DESTINATION include\n)\n\n# warnings\nadd_library(warnings INTERFACE)\nif(CMAKE_COMPILER_IS_GNUCXX)\n  target_compile_options(warnings INTERFACE -Wall -Wshadow -Wno-unknown-pragmas -Wextra -Wno-unused-parameter -Wno-unused-local-typedefs)\nelseif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Intel\")\n  target_compile_options(warnings INTERFACE -Wall -Wshadow)\nelseif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"PGI\")\n  target_compile_options(warnings INTERFACE -Minform=inform)\nendif()\n\n# Executables\nfile(GLOB_RECURSE headers src/include/**)\nadd_custom_target(mallocMCIde SOURCES ${headers}) # create a target with the header files for IDE projects\nsource_group(TREE ${CMAKE_CURRENT_LIST_DIR}/src/include FILES ${headers})\n\nalpaka_add_executable(mallocMC_Example01 EXCLUDE_FROM_ALL examples/mallocMC_example01.cpp)\ntarget_include_directories(mallocMC_Example01 PUBLIC ${CMAKE_CURRENT_LIST_DIR}/src/include)\ntarget_link_libraries(mallocMC_Example01 PUBLIC alpaka::alpaka warnings)\n\nalpaka_add_executable(mallocMC_Example03 EXCLUDE_FROM_ALL examples/mallocMC_example03.cpp)\ntarget_include_directories(mallocMC_Example03 PUBLIC ${CMAKE_CURRENT_LIST_DIR}/src/include)\ntarget_link_libraries(mallocMC_Example03 PUBLIC alpaka::alpaka warnings)\n\nadd_custom_target(examples DEPENDS mallocMC_Example01 mallocMC_Example03)\n\nif(${mallocMC_CATCH2_PROVIDER} STREQUAL \"intern\")\n  add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/thirdParty/catch2 ${CMAKE_BINARY_DIR}/catch2)\n  include(Catch)\nelse()\n  # get Catch2 v3 and build it from source with the same C++ standard as the tests\n  Include(FetchContent)\n  FetchContent_Declare(Catch2 GIT_REPOSITORY https://github.com/catchorg/Catch2.git GIT_TAG v3.7.1)\n  FetchContent_MakeAvailable(Catch2)\n  target_compile_features(Catch2 PUBLIC cxx_std_20)\n  include(Catch)\n\n  # hide Catch2 cmake variables by default in cmake gui\n  get_cmake_property(variables VARIABLES)\n  foreach (var ${variables})\n    if (var MATCHES \"^CATCH_\")\n      mark_as_advanced(${var})\n    endif()\n  endforeach()\nendif()\n\nfile(GLOB_RECURSE testSources \"${CMAKE_CURRENT_SOURCE_DIR}/tests/*/*.cpp\")\nalpaka_add_executable(tests EXCLUDE_FROM_ALL ${testSources})\ncatch_discover_tests(tests)\nsource_group(TREE \"${CMAKE_CURRENT_LIST_DIR}/tests\" FILES ${testSources})\ntarget_compile_features(tests PRIVATE cxx_std_20)\ntarget_include_directories(tests PUBLIC ${CMAKE_CURRENT_LIST_DIR}/src/include)\ntarget_link_libraries(tests PRIVATE alpaka::alpaka Catch2::Catch2WithMain)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mapman",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/massbank",
            "repo_link": "https://github.com/MassBank",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/materials-learning-algorithms",
            "repo_link": "https://github.com/mala-project/mala",
            "content": {
                "codemeta": "",
                "readme": "![image](./docs/source/img/logos/mala_horizontal.png)\n\n# MALA\n\n[![CPU](https://github.com/mala-project/mala/actions/workflows/cpu-tests.yml/badge.svg)](https://github.com/mala-project/mala/actions/workflows/cpu-tests.yml)\n[![image](https://github.com/mala-project/mala/actions/workflows/gh-pages.yml/badge.svg)](https://mala-project.github.io/mala/)\n[![image](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5557255.svg)](https://doi.org/10.5281/zenodo.5557255)\n\n\nMALA (Materials Learning Algorithms) is a data-driven framework to generate surrogate models of density functional theory calculations based on machine learning. Its purpose is to enable multiscale modeling by bypassing computationally expensive steps in state-of-the-art density functional simulations.\n\nMALA is designed as a modular and open-source python package. It enables users to perform the entire modeling toolchain using only a few lines of code. MALA is jointly developed by the Sandia National Laboratories (SNL) and the Center for Advanced Systems Understanding (CASUS). See [Contributing](docs/source/CONTRIBUTE.md) for contributing code to the repository.\n\nThis repository is structured as follows:\n```\n├── examples : contains useful examples to get you started with the package\n├── install : contains scripts for setting up this package on your machine\n├── mala : the source code itself\n├── test : test scripts used during development, will hold tests for CI in the future\n└── docs : Sphinx documentation folder\n```\n\n## Installation\n\n> **WARNING**: Even if you install MALA via PyPI, please consult the full installation instructions afterwards. External modules (like the QuantumESPRESSO bindings) are not distributed via PyPI!\n\nPlease refer to [Installation of MALA](docs/source/install/installing_mala.rst).\n\n## Running\n\nYou can familiarize yourself with the usage of this package by running\nthe examples in the `example/` folder.\n\n## Contributors\n\nMALA is jointly maintained by \n\n- [Sandia National Laboratories](https://www.sandia.gov/) (SNL), USA.\n    - Scientific supervisor: Sivasankaran Rajamanickam, code maintenance: \nJon Vogel\n- [Center for Advanced Systems Understanding](https://www.casus.science/) (CASUS), Germany.\n    - Scientific supervisor: Attila Cangi, code maintenance: Lenz Fiedler\n\nA full list of contributors can be found [here](docs/source/CONTRIBUTE.md).\n\n## Citing MALA\n\nIf you publish work which uses or mentions MALA, please cite the following paper:\n\nJ. A. Ellis, L. Fiedler, G. A. Popoola, N. A. Modine, J. A. Stephens, A. P. Thompson,\nA. Cangi, S. Rajamanickam (2021). Accelerating Finite-temperature\nKohn-Sham Density Functional Theory with Deep Neural Networks.\n[Phys. Rev. B 104, 035120 (2021)](https://doi.org/10.1103/PhysRevB.104.035120)\n\nalongside this repository.\n\n",
                "dependencies": "[tool.black]\nline-length = 79\n\n[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\n\n# See install/README.md\n##torch\n\nase\nmpmath\nnumpy\noptuna\nscipy\npandas\ntensorboard\nopenpmd-api\nscikit-spatial\ntqdm\n\nfrom setuptools import setup, find_packages\n\n# Doing it as suggested here:\n# https://packaging.python.org/guides/single-sourcing-package-version/\n# (number 3)\n\nversion = {}\nwith open(\"mala/version.py\") as fp:\n    exec(fp.read(), version)\n\nwith open(\"README.md\") as f:\n    readme = f.read()\n\nwith open(\"LICENSE\") as f:\n    license = f.read()\n\nextras = {\n    \"dev\": [\"bump2version\"],\n    \"opt\": [\"oapackage\", \"scikit-learn\"],\n    \"test\": [\"pytest\", \"pytest-cov\"],\n    \"doc\": open(\"docs/requirements.txt\").read().splitlines(),\n    \"experimental\": [\"asap3\", \"dftpy\", \"minterpy\"],\n}\n\nsetup(\n    name=\"materials-learning-algorithms\",\n    version=version[\"__version__\"],\n    description=(\n        \"Materials Learning Algorithms. \"\n        \"A framework for machine learning materials properties from \"\n        \"first-principles data.\"\n    ),\n    long_description=readme,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/mala-project/mala\",\n    author=\"MALA developers\",\n    license=license,\n    packages=find_packages(\n        exclude=(\"test\", \"docs\", \"examples\", \"install\", \"ml-dft-sandia\")\n    ),\n    zip_safe=False,\n    install_requires=open(\"requirements.txt\").read().splitlines(),\n    extras_require=extras,\n    python_requires=\">=3.10.4\",\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/matrad",
            "repo_link": "https://github.com/e0404/matRad",
            "content": {
                "codemeta": "",
                "readme": "[![Current Release](https://img.shields.io/github/v/release/e0404/matRad)](https://github.com/e0404/matRad/releases) \n[![Downloads](https://img.shields.io/github/downloads/e0404/matRad/total)](https://github.com/e0404/matRad/releases) \n[![Contributors](https://img.shields.io/github/contributors/e0404/matRad)](https://github.com/e0404/matRad/graphs/contributors)\n\n[![GitHub Build Status](https://github.com/e0404/matRad/actions/workflows/tests.yml/badge.svg)](https://github.com/e0404/matRad/actions/workflows/tests.yml)\n[![codecov](https://codecov.io/gh/e0404/matRad/graph/badge.svg?token=xQhUQLu4FK)](https://codecov.io/gh/e0404/matRad)\n\nCitable DOIs:\n- General DOI: [![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.3879615.svg)](https://doi.org/10.5281/zenodo.3879615)\n- Latest Release: [![DOI](https://zenodo.org/badge/29671667.svg)](https://zenodo.org/badge/latestdoi/29671667)\n\n# General information\n\n---\n\nmatRad is an open source treatment planning system for radiation therapy written in Matlab. It supports planning of intensity-modulated radiation therapy for mutliple modalities and is meant **for educational and research purposes**. **IT IS NOT SUITABLE FOR CLINICAL USE** (also see the no-warranty clause in the GPL license). The source code is maintained by a development team at the German Cancer Reserach Center - DKFZ in Heidelberg, Germany, and other contributors around the world. We are always looking for more people willing to help improve matRad. Do not hesitate and [get in touch](mailto:contact@matRad.org).\n\nMore information can be found on the project page  at <http://e0404.github.io/matRad/>; a wiki documentation is under constant development at <https://github.com/e0404/matRad/wiki>.\n\n# Getting Started\nIf you want to quickly run matRad, start with the Quick Start below. Some information on the structure of matRad for more sustainable use is given afterwards.\n\n## Quick Start\nIt’s the first time you want to use matRad?\n\nFirst, get a local copy of matRad by download or git cloning. Having done that, we recommend you navigate into the folder in Matlab and execute \n```\nmatRad_rc\n```\nwhich will setup the path & configuration and tell you the current version.\n\nThen there’re three options for a pleasant start with matRad. Choose one or try out each of them.\n\n### Option 1: Using the GUI\n\nFor an intuitive workflow with the graphical user interface, type \n```\nmatRadGUI\n```\nin your command window. An empty GUI should be opened. Click the _*Load.mat_ data-Button in the Workflow-section to load a patient. Set the plan and optimization parameters, calculate the dose influence matrix and execute the fluence optimization in the GUI.\n\n### Option 2: Using the main script\n\nIf you prefer scripting, open the default script *matRad.m* from the main matRad folder:\n```\nedit matRad.m\n```\nUse it to learn something about the code structure and execute it section by section.\n\nYou can also run the full script for an example photon plan by just typing\n```\nmatRad\n``` \nin your command window.\n\n### Option 3: Using the examples\n\nThe most time consuming but also most educational approach to matRad. \n\nWhen in the main matRad folder, navigate to the folder *examples*. Open one of the examples given there. Execute it section by section. Move on to the next example afterwards.\n\n## Advanced information for new users\n### Folder Structure\n#### Core Source Code\nMost of the source code of matRad is located in the \"matRad\" subfolder. Within the first level of matRad, you find the functions handling the basic workflow steps. These functions have simple interfaces relying on matRad's main data structures ct, cst, stf, dij, resultGUI, and pln.\nAdditionally, it contains MatRad_Config.m which is a singleton class implementation to handle global configuration of matRad. Check out the infos further below. \n\nWe try to keep the main workflow functions as consistent as possible, while the fine-grained implementation in the subfolders within matRad/* may undergo larger changes.\n#### User Directory\nBy default, matRad adds the \"userdata\" folder to the path. It is the place to put your custom scripts, machine data, imported patients etc. Just follow the README files in the folders. Contents of this folder are added to the .gitignore and will thus be ignored during your development efforts, keeping your repository clean.\n#### Third-Party & Submodules\nOur ThirdParty-Tools used in matRad are stored in the thirdParty folder including licenses. Submodules contains references to used git repositories, and you might recognize that some dependencies appear both in submodules and thirdParty. This is mainly to maintain operation if the code is downloaded (and not cloned), and also helps us to maintain the build process of mex files built from source in the submodules (and then added to ThirdParty). \n#### Tests\nThe \"test\" folder contains xUnit-Style tests based on the MOxUnit framework. You can run those tests by running matRad_runTests from the root directory. Check the README file within the test folder for more information.\n\n### MatRad_Config / matRad_cfg\nmatRad maintains its global configuration, including some default parameters, as well as a logging mechanism with different levels, in the MatRad_Config.m class serving as a \"singleton\" throughout matRad. You will see many functions using a call like `matRad_cfg = MatRad_Config.instance();`, which will get you the global configuration anywhere in the code or in the command window. Alternatively, `matRad_rc` will return matRad_cfg as well.\n\n# Need help?\nIf you encounter problems with matRad, please consider the following guidelines **before** submitting issues on our github page. \n\n* Check you are using the newest version of matRad.\n* Please check the description of how to set up matRad and its technical documentation in the [wiki](https://github.com/e0404/matRad/wiki).\n* Go through the relevant examples and see if they answer your question (see *Option 3* above!)\n* Check open and closed issues for your question.\n\nStill having problems? Then create an issue, provide a **minimum example** of your attempted workflow / what causes the problems and be patient!\n\n# Citing matRad\n\n### Scientific papers\n\nIf you use matRad in a scientific publication, consider citing the following paper:\n\nWieser, Hans-Peter, et al. \"Development of the open-source dose calculation and optimization toolkit matRad.\" Medical Physics 44.6 (2017): 2556-2568. \n\n[![DOI](https://img.shields.io/badge/DOI-10.1002%2Fmp.12251-blue)](https://doi.org/10.1002/mp.12251) \n\nBibTex entry:\n```\n@article{wieser2017development,\n  title={Development of the open-source dose calculation and optimization toolkit matRad},\n  author={Wieser, Hans-Peter and Cisternas, Eduardo and Wahl, Niklas and Ulrich, Silke and Stadler, Alexander and Mescher, Henning and M{\\\"u}ller, Lucas-Raphael and Klinge, Thomas and Gabrys, Hubert and Burigo, Lucas and others},\n  journal={Medical Physics},\n  volume={44},\n  number={6},\n  pages={2556--2568},\n  year={2017},\n  publisher={Wiley Online Library},\n  doi={10.1002/mp.12251}\n}\n```\n\n### Citing as Software\n\nmatRad's code also has its own general DOI with Zenodo: \n\n[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.3879615.svg)](https://doi.org/10.5281/zenodo.3879615)\n\nYou can cite specific versions of matRad in your work! For example, Here is the badge that lead's to the latest release of matRad:\n\n[![DOI](https://zenodo.org/badge/29671667.svg)](https://zenodo.org/badge/latestdoi/29671667)\n\n# Funding Sources\nmatRad developments (on this branch) were (in parts) funded by:\n- The German Research Foundation (DFG), Project No. 265744405 & 443188743\n- The German Cancer Aid, Project No. 70113094\n- The German Federal Ministry of Education and Research (BMBF), Project No. 01DN17048\n- Mathworks Academic Research Support\n\n---\n\nCopyright 2022 the matRad development team. \n\nmatrad@dkfz.de\n\nAll the elements of the compilation of matRad and Ipopt are free software. You can redistribute and/or modify matRad's source code version provided as files with .m and .mat extension under the terms of the GNU GENERAL PUBLIC LICENSE Version 3 (GPL v3). You can also add to matRad the Ipopt functionality by using the precompiled mex files of the Ipopt optimizer in object code version which are licensed under the Eclipse Public License Version 1.0 (EPL v1.0), also made available for download via https://projects.coin-or.org/Ipopt.\nmatRad also contains interfaces to an open-source photon Monte Carlo dose calculation engine developed by Edgardo Dörner hosted on GitHub (http://github.com/edoerner/ompMC) and to the open-source proton Monte Carlo project MCsquare (www.openmcsquare.org) from UCLouvain, Louvain-la-Neuve, Belgium. Both interfaces are integrated into matRad as submodules.\n\nIn addition, we provide a matlab standalone version of the compilation of matRad and Ipopt, where the files of matRad and Ipopt are licensed under GPL v3 and EPL v1.0 respectively. The matlab standalone version is meant to be used by students for learning and practicing scientific programming and does not yet contain the interfaces to the aforementioned Monte Carlo dose calculation engines.\n\nmatRad is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n\nPlease note that we treat the compilation of matRad and Ipopt as separate and independent works (or modules, components, programs). Therefore, to the best of our understanding, the compilation of matRad and Ipopt is subject to the \"Mere Aggregation\" exception in section 5 of the GNU v3 and the exemption from \"Contributions\" in section 1. b) ii) of the EPL v1.0. Should this interpretation turn out to be not in compliance with the applicable laws in force, we have provided you with an additional permission under GNU GPL version 3 section 7 to allow you to use the work resulting from combining matRad with Ipopt.\n\nYou will receive a copy of the GPL v3  and a copy of the EPL v1.0 in the file LICENSE.md along with the compilation. If not, see http://www.gnu.org/licenses/ and/or http://opensource.org/licenses/EPL-1.0/.\n\n---\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mcodac",
            "repo_link": "https://gitlab.com/dlr-sy/mcodac",
            "content": {
                "codemeta": "",
                "readme": "[![PyPi](https://img.shields.io/static/v1?label=PyPi&message=1.2.0&color=informational&logo=pypi)](https://pypi.org/project/mcodac/)\n[![doi](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.13383097-red.svg)](https://zenodo.org/records/13383097)\n[![pipeline status](https://gitlab.com/dlr-sy/mcodac/badges/mcd_development/pipeline.svg)]()\n\n# MCODAC\nMCODAC (Modular COmposite Damage Analysis Code) is a Fortran library for the evaluation of pristine and damaged composite structures. \nIn addition to basic mathematical tools for tensor manipulation, it contains multidimensional interpolation methods, numerical optimization routines and common utility algorithms used in continuum mechanics. \nFurthermore, the library contains analysis methods specifically tailored to composites, from micromechanical homogenization approaches to macroscopic fatigue models of orthotropic multilayer composites. \nThis project is compiled for Python using [f2py](https://numpy.org/doc/stable/f2py).\n> Installation from source requires an active Fortran compiler (ifort, gfortran). \n## Downloading\nUse GIT to get the latest code base. From the command line, use\n```\ngit clone https://gitlab.com/dlr-sy/mcodac mcodac\n```\nIf you check out the repository for the first time, you have to initialize all submodule dependencies first. Execute the following from within the repository. \n```\ngit submodule update --init --recursive\n```\nTo update all refererenced submodules to the latest production level, use\n```\ngit submodule foreach --recursive 'git pull origin $(git config -f $toplevel/.gitmodules submodule.$name.branch || echo master)'\n```\n## Installation\nMCODAC can be installed from source using [poetry](https://python-poetry.org). If you don't have [poetry](https://python-poetry.org) installed, run\n```\npip install poetry --pre --upgrade\n```\nto install the latest version of [poetry](https://python-poetry.org) within your python environment. Use\n```\npoetry update\n```\nto update all dependencies in the lock file or directly execute\n```\npoetry install\n```\nto install all dependencies from the lock file. Last, you should be able to import MCODAC as a python package.\n```python\nimport mcodac\n```\n## Example\nPlease refer to the linked [repository](https://gitlab.com/dlr-sy/mcodac) for specific application examples.\n## Contact\n* [Marc Garbade](mailto:marc.garbade@dlr.de)\n## Support\n* [List of Contributors](CONTRIBUTING.md)\n",
                "dependencies": "# TOML file to create MCODAC\r\n#  \r\n# @note: TOML file            \r\n# Created on 29.12.2022    \r\n# \r\n# @version:  1.0    \r\n# ----------------------------------------------------------------------------------------------\r\n# @requires:\r\n#        - \r\n# \r\n# @change: \r\n#        -    \r\n#    \r\n# @author: garb_ma                                                     [DLR-SY,STM Braunschweig]\r\n# ----------------------------------------------------------------------------------------------\r\n[build-system]\r\nrequires = [\"poetry-core>=1.0\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n\r\n[tool.poetry]\r\nname = \"mcodac\"\r\nversion = \"1.2.0\"\r\ndescription = \"Calculation of pristine and damaged composite structures\"\r\nauthors = [\"Garbade, Marc <marc.garbade@dlr.de>\"]\r\nmaintainers = [\"Garbade, Marc <marc.garbade@dlr.de>\",\r\n               \"Lüders, Caroline <caroline.lüders@dlr.de>\",\r\n               \"Bogenfeld, Raffael <raffael.bogenfeld@dlr.de>\",\r\n               \"Heinecke, Falk <falk.heinecke@volkswagen.de>\",\r\n               \"Hein, Robert <robert.hein@dlr.de>\"]\r\nlicense = \"GPL-3.0-or-later\"\r\npackages = [{include=\"**/*\", from=\"bin\"}]\r\nexclude = [\"bin/_*\",\r\n           \"bin/**/.*\",\r\n           \"bin/linux/*\",\r\n           \"bin/windows/*\"]\r\nrepository = \"https://gitlab.com/dlr-sy/mcodac\"\r\ndocumentation = \"https://gitlab.com/dlr-sy/mcodac/-/blob/mcd_development/README.md\"\r\nkeywords = [\"analysis\",\"damage\",\"composite\"]\r\nreadme = \"README.md\"\r\nclassifiers = [\r\n    \"Development Status :: 4 - Beta\",\r\n    \"Topic :: Scientific/Engineering\",\r\n    \"Programming Language :: Python :: 2\",\r\n    \"Programming Language :: Python :: 3\",\r\n    \"License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)\",\r\n    \"Operating System :: OS Independent\"\r\n]\r\n\r\n[tool.poetry.urls]\r\nChangelog = \"https://gitlab.com/dlr-sy/mcodac/-/blob/mcd_development/CHANGELOG.md\"\r\n\r\n[[tool.poetry.source]]\r\nname = \"dlr-pypi\"\r\nurl = \"https://pypi.python.org/simple\"\r\npriority = \"primary\"\r\n\r\n[[tool.poetry.source]]\r\nname = \"dlr-sy\"\r\nurl = \"https://gitlab.dlr.de/api/v4/groups/541/-/packages/pypi/simple\"\r\npriority = \"supplemental\"\r\n\r\n[tool.poetry.build]\r\nscript = \"config/build.py\"\r\ngenerate-setup-file = false\r\n\r\n[tool.poetry.dependencies]\r\npython = \"~2.7 || ^3.5\"\r\nnumpy = [{version = \"^1.16\", python = \"~2.7 || ~3.5\"},\r\n         {version = \"^1.18\", python = \"~3.6\"},\r\n         {version = \"^1.21\", python = \"~3.7\"},\r\n         {version = \"^1.22\", python = \"~3.8\"},\r\n         {version = \">=1.22,<2\", python = \"^3.9\"}]\r\n\r\n# All mandatory development dependencies\r\n[tool.poetry.group.dev.dependencies]\r\nmkl = [{version = \"~2022.2\", platform = \"win32\"}]\r\nmkl-devel = [{version = \"~2022.2\", platform = \"win32\"}]\r\ntbb = [{version = \"~2021.7\", platform = \"win32\"}]\r\npyx-core = [{version = \"^1.17\", python = \"~2.7 || ^3.5,<3.7\"},\r\n            {git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake.git\", develop=true, python=\"^3.7\"}]\r\n\r\n[tool.pyxmake.abaqus]\r\nname = \"standardU\"\r\nsource = \"solver\"\r\nfiles = \"mcd_astandard\"\r\nlibs = [\"mcd_core{arch}\",\"bbeam{arch}\",\r\n        \"muesli{arch}\",\"dispmodule{arch}\",\r\n        \"interp{arch}\",\"pchip{arch}\",\"toms{arch}\"]\r\ninclude = [\"include/{platform}/{arch}\",\r\n           \"include/{platform}/{arch}/boxbeam\",\r\n           \"include/{platform}/{arch}/dispmodule\",\r\n           \"include/{platform}/{arch}/toms\"]\r\ndependency = \"lib/{platform}/{arch}\"\r\noutput = \"bin/{platform}/{arch}\"\r\n\r\n[tool.pyxmake.cmake]\r\nname = \"mcodac\"\r\nsource = \"config\"\r\n\r\n[tool.pyxmake.doxygen]\r\nname = \"MCODAC\"\r\ntitle = [\"MCODAC\", \"MCODAC Developer Guide\"]\r\nsource = \"src\"\r\noutput = \"doc/mcd_core\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/me-compute",
            "repo_link": "https://github.com/rizac/me-compute",
            "content": {
                "codemeta": "",
                "readme": "# <img align=\"left\" height=\"30\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/medien_kommunikation/Infothek/Mediathek/Bilder/GFZ/GFZ_Logo/GFZ-Logo_eng_RGB.svg\"> Me-compute <img align=\"right\" height=\"50\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/GFZ_Wortmarke_SVG_klein_en_edit.svg\">\n\n|Jump to: | [Installation](#installation) | [Usage](#usage) | [Citation](#citation) |\n| - | - | - | - |\n\n\n\nProgram to compute energy Magnitude (Me) from downloaded seismic events. \n\nThe download is performed via [stream2segment](https://github.com/rizac/stream2segment)\n(included in this package) into a custom SQLite or Postgres database (in this case, \nthe database has to be setup beforehand).\n\nOnce downloaded, events and data within a customizable time window can be \nfetched from the database in order to compute each event Me (Me = mean \nof all stations energy magnitudes in the 5th-95th percentiles). The computed Me are available\nin several formats: CSV, HDF, HTML and QuakeMl (see Usage below for details).\n\nThe download + processing routines can be chained and scheduled on a server to compute\nthe energy magnitude in semi-realtime (e.g. daily or weekly. See instructions below)\n\n\n## Installation\nMake virtualenv `python3 -m venv [PYPATH]` and activate it:\n`source [PYPATH]/bin/activate`. \n\n**Remember that any command of the program must be done with the virtual env activated**\n\nUpdate required packages for installing Python stuff:\n```console\npip install --upgrade pip setuptools\n```\n\nInstall the program: From the directory where you cloned `mecompute`: \n\n1. [Optional] If you want to be safer and install **exactly** the dependencies \n   with the tested versions, and you don't have conflicts with \n   existing dependencies (e.g., your virtualenv is empty and not supposed to \n   have other packages installed), \n   then you can run: `pip install -r ./requirements.txt` or \n   `pip install -r ./requirements.dev.txt` (the latter if you want to run tests)\n \n2. Install the program:\n   ```bash\n   pip install .\n   ```\n   or (if you want to run tests):\n   ```bash\n   pip install \".[dev]\"\n   ```\n   (add the `-e` option if you want to install in [editable mode](https://stackoverflow.com/a/35064498))\n   **The installation creates a new terminal command `me-compute` within your virtualenv,\n   that you can inspect via**: \n   ```bash\n   me-compute --help\n   ```\n\n## Usage\n\nFirst of all, you should configure your download routine. The repository contains \na `config` directory (git-ignored), with several configuration files that you can copy and modify.\nMost of them are for experienced users and are already filled with default values: \nthe only routine that has to be customized is the download routine\n(file `download.yaml`, see below)\n\n\n### Events and data Download:\n\nThe download routine downloads data and metadata from the configured FDSN\nevent and dataselect web services into a custom database (Sqlite or Postgres using\n[stream2segment](https://github.com/rizac/stream2segment). With Postgres,\nthe db has to be setup beforehand) . Open `download.yaml`\n(or a copy of it) and configure `dburl` (ideally, you might want to setup also\n`start`, `end`, `events_url` and `data_url`). Then run stream2segment with the `s2s`\ncommand:\n\n```commandline\ns2s download -c download.yaml\n```\n\n\n### Me computation\n\nTo compute the energy magnitude of the events saved on the db, you run the\n`me-compute` command with customized options, e.g.:\n\n```bash\nme-compute -s [START] -e [END] -d download.yaml ... [OUTPUT_DIR]\n```\n\n\nSTART and END are the start and end time of the \nevents to consider, in ISO format (e.g. \"2016-03-31\"). If omitted, they will be\ninferred (Type `me-compute --help` for more details)\n\nOUTPUT_DIR is the destination directory. You can use the special characters \n`%S%` and `%E%` that will be replaced with the start and end time strings (see above). \nThe output directory and its parents will be created if they do not exist. \n\nIn the output directory, the following files will be saved:\n\n- **station-energy-magnitude.hdf** A tabular file where each row represents a\n  station(^) and each column the station computed data and metadata,\n  including the station energy magnitude.\n  \n  (^) Note: technically speaking, a single HDF row represents a waveform. \n  We talk about station because by default we download a single channel \n  per station (the vertical component `BHZ`, see `download.yaml` \n  for details)\n  \n\n- **energy-magnitude.csv** A tabular file where each row represents a seismic \n  event, aggregating the result of the previous file into the final event energy \n  magnitude. The event Me is the mean of all station energy magnitudes within \n  the 5-95 percentiles. Empty or non-numeric Me values indicate that the energy \n  magnitude could not be computed or resulted in invalid values (NaN, null, \n  +-inf)\n\n\n- **energy-magnitude.html** A report that can be opened in the user browser to\n  visualize the computed energy magnitudes on maps and HTML tables\n\n\n- **[eventid1].xml, ..., [eventid1].xml** All processed events saved in QuakeML\n  format, updated with the information of their energy magnitude. Only events \n  with valid Me will be saved\n\n\n- **energy-magnitude.log** the log file where the info, errors and warnings\n  of the routine are stored. The core energy magnitude computation at station\n  level (performed via `stream2segment` utilities) has a separated and more\n  detailed log file (see below)\n\n\n- **station-energy-magnitude.log** the log file where the info, errors and \n  warnings of the station energy magnitude computation have been stored\n\n\n### Cron job (schedule downloads+ Me computation)\n\nAssuming your Python virtualenv is at `[VEN_PATH]`, with your python \nvirtualenv activated (`source [VENV_PATH]/bin/activate`),\ntype `which me-compute`. You should see something like\n`[VENV_PATH]/bin/me-compute` (same for `which s2s`). \n\nWith the paths above, you can set up cron jobs to schedule all above routines.  \nFor instance, below an example file that can be edited via\n`crontab -e` (https://linux.die.net/man/1/crontab):\n\nIt downloads every day shortly after midnight (00:05) events and data of the \nprevious day (the download time window must be configured in \nthe download.yaml file). Afterwards, it computes the energy magnitude at 5:00 AM\n(5 hours are a more than sufficient time to complete the download of all data):\n\n```bash \n# For more information see the manual pages of crontab(5) and cron(8)\n# \n# m h  dom mon dow   command\n5 0 * * * [VENV_PATH]/bin/python [VENV_PATH]/bin/s2s download -c /home/download.private.yaml\n0 5 * * * [VENV_PATH]/bin/python [VENV_PATH]/bin/me-compute -d [DOWNLOAD_YAML] -s [START] -e [END] \"[ROOT_DIR]/me-result_%S%_%E%\"\n```\n\n\n### Misc\n\n#### Run tests and generate test data\n\nRun: \n```commandline\npytest ./me-compute/test\n```\n\nNote that there is only one test routine generating files in a `test/tmp` directory\n(git-ignored). The directory is **not** deleted automatically in order to leave \ndevelopers the ability to perform an additional visual test on the generated output \n(e.g. HTML report)\n\n\n## Citation\n\n> Zaccarelli, Riccardo (2023): me-compute: a Python software to download events and data from FDSN web services and compute their energy magnitude (Me). GFZ Data Services. https://doi.org/10.5880/GFZ.2.6.2023.008\n\n",
                "dependencies": "blinker==1.6.2\nblosc2==2.0.0\ncertifi==2023.5.7\ncharset-normalizer==3.1.0\nclick==8.1.3\ncontourpy==1.1.0\ncycler==0.11.0\nCython==0.29.35\ndecorator==5.1.1\nFlask==2.3.2\nfonttools==4.40.0\ngreenlet==2.0.2\nidna==3.4\nitsdangerous==2.1.2\nJinja2==3.1.2\nkiwisolver==1.4.4\nlxml==4.9.2\nMarkupSafe==2.1.3\nmatplotlib==3.7.1\n# -e git+https://github.com/rizac/me-compute.git@5b34509404d4de9be0c1656f865c0d900da1b105#egg=me_compute\nmsgpack==1.0.5\nnumexpr==2.8.4\nnumpy==1.25.0\nobspy==1.4.0\npackaging==23.1\npandas==2.0.2\nPillow==9.5.0\npsutil==5.9.5\npsycopg2==2.9.6\npy-cpuinfo==9.0.0\npyparsing==3.1.0\npython-dateutil==2.8.2\npytz==2023.3\nPyYAML==6.0\nrequests==2.31.0\nscipy==1.10.1\nsdaas @ git+https://git@github.com/rizac/sdaas.git@6647d5dd3e802019a5fcdaf9758f912242a105c9\nsix==1.16.0\nSQLAlchemy==2.0.16\nstream2segment @ git+https://git@github.com/rizac/stream2segment.git@a7a9a24af9b29eb71f3a592bfc76b9ef5a947dc0\ntables==3.8.0\ntyping_extensions==4.6.3\ntzdata==2023.3\nurllib3==2.0.3\nWerkzeug==2.3.6\n\n\"\"\"A setuptools based setup module.\nTaken from:\nhttps://github.com/pypa/sampleproject/blob/master/setup.py\n\nSee also:\nhttp://python-packaging-user-guide.readthedocs.org/en/latest/distributing/\n\nAdditional links:\nhttps://packaging.python.org/en/latest/distributing.html\nhttps://github.com/pypa/sampleproject\n\"\"\"\nfrom __future__ import print_function\n\n# Always prefer setuptools over distutils\nfrom setuptools import setup, find_packages\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, 'README.md'), encoding='utf-8') as f:\n    long_description = f.read()\n\n# http://stackoverflow.com/questions/2058802/how-can-i-get-the-version-defined-in-setup-py-setuptools-in-my-package\nversion = \"1.0\"\n# with open(path.join(here, 'program_version')) as version_file:\n#    version = version_file.read().strip()\n\n\n# copy config directory:\nconfig_src = path.abspath(path.join(here, 'mecompute', 'base-config'))\nconfig_dest = path.abspath(path.join(here, 'config'))\nif not path.isdir(config_dest):\n    import shutil\n    shutil.copytree(config_src, config_dest)\n\n\nsetup(\n    name='me-compute',\n\n    # Versions should comply with PEP440.  For a discussion on single-sourcing\n    # the version across setup.py and the project code, see\n    # https://packaging.python.org/en/latest/single_source_version.html\n    version=version,\n\n    description='A Python project to compute energy agnitude (Me) from downloaded seismic events',\n    long_description=long_description,\n\n    # The project's main homepage.\n    url='https://github.com/rizac/me-compute',\n\n    # Author details\n    author='riccardo zaccarelli',\n    author_email='rizac@gfz-potsdam.de',  # FIXME: what to provide?\n\n    # Choose your license\n    license='GPL',\n    python_requires='>=3.8',\n\n    # See https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        'Development Status :: 3 - Alpha',\n\n        # Indicate who your project is intended for\n        'Intended Audience :: Science/Research',\n        'Topic :: Scientific/Engineering',\n\n        # Pick your license as you wish (should match \"license\" above)\n        'License :: OSI Approved :: GNU License',\n\n\n        # Specify the Python versions you support here.\n        # 'Programming Language :: Python :: 3.5',\n        # 'Programming Language :: Python :: 3.6',\n        # 'Programming Language :: Python :: 3.7',\n        # 'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n    ],\n\n    # What does your project relate to?\n    keywords='compute Magnitude Energy (Me)',\n\n    # You can just specify the packages manually here if your project is\n    # simple. Or you can use find_packages().\n    packages=find_packages(exclude=['contrib', 'docs', 'tests', 'htmlcov']),\n\n    # Alternatively, if you want to distribute just a my_module.py, uncomment\n    # this:\n    #   py_modules=[\"my_module\"],\n\n    # List run-time dependencies here.  These will be installed by pip when\n    # your project is installed. For info see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=[\n        'stream2segment @ git+https://git@github.com/rizac/stream2segment.git',\n        'sdaas @ git+https://git@github.com/rizac/sdaas.git'\n        # 'git+https://github.com/rizac/stream2segment.git#egg=stream2segment',\n        # 'git+https://github.com/rizac/sdaas.git#egg=sdaas'\n    ],\n\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). You can install these using the following syntax,\n    # for example:\n    # $ pip install -e .[dev,test]  (pip install -e \".[dev,test]\" in zsh)\n    extras_require={\n        # use latest versions. Without boundaries\n        'dev': [# 'pep8>=1.7.0',\n                'pylint',\n                'pytest',\n                # 'pytest-cov>=2.5.1',\n                # 'pytest-mock>=1.6.2'\n               ],\n        # 'jupyter': ['jupyter>=1.0.0']\n    },\n\n    # If there are data files included in your packages that need to be\n    # installed, specify them here.  If using Python 2.6 or less, then these\n    # have to be included in MANIFEST.in as well.\n    #\n    # package_data={\n    #    'sample': ['package_data.dat'],\n    # },\n\n    # make the installation process copy also the package data (see MANIFEST.in)\n    # for info see https://python-packaging.readthedocs.io/en/latest/non-code-files.html\n    include_package_data=True,\n    # zip_safe=False,\n\n    # Although 'package_data' is the preferred approach, in some case you may\n    # need to place data files outside of your packages. See:\n    # http://docs.python.org/3/distutils/setupscript.html#installing-additional-files # noqa\n    # In this case, 'data_file' will be installed into '<sys.prefix>/my_data'\n    #\n    # data_files=[('my_data', ['data/data_file'])],\n\n    # To provide executable scripts, use entry points in preference to the\n    # \"scripts\" keyword. Entry points provide cross-platform support and allow\n    # pip to create the appropriate form of executable for the target platform.\n    entry_points={\n        'console_scripts': [\n            'me-compute=mecompute.cli:cli',\n        ],\n    },\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mitk",
            "repo_link": "https://github.com/MITK/MITK",
            "content": {
                "codemeta": "",
                "readme": "![MITK Logo][logo]\n\nThe [Medical Imaging Interaction Toolkit][mitk] (MITK) is a free open-source software\nsystem for development of interactive medical image processing software. MITK\ncombines the [Insight Toolkit][itk] (ITK) and the [Visualization Toolkit][vtk] (VTK) with an application framework.\n\nThe links below provide high-level and reference documentation targeting different\nusage scenarios:\n\n - Get a [high-level overview][mitk-overview] about MITK with pointers to further\n   documentation\n - End-users looking for help with MITK applications should read the\n   [MITK User Manual][mitk-usermanual]\n - Developers contributing to or using MITK, please see the [MITK Developer Manual][mitk-devmanual]\n   as well as the [MITK API Reference][mitk-apiref]\n\nSee the [MITK homepage][mitk] for details.\n\nSupported platforms\n-------------------\n\nMITK is a cross-platform C++ toolkit and officially supports:\n\n - Windows\n - Linux\n - macOS\n\nFor details, please read the [Supported Platforms][platforms] page.\n\nLicense\n-------\n\nCopyright (c) [German Cancer Research Center (DKFZ)][dkfz]. All rights reserved.\n\nMITK is available as free open-source software under a [3-clause BSD license][license].\n\nDownload\n--------\n\nThe *MitkWorkbench* application and a bunch of command-line apps are released twice per year on our [Download][download] page and the [GitHub Releases][releases] page.\n\nThe official MITK source code is available in the [MITK Git repository][git_repo]. The Git clone command is\n\n    git clone https://github.com/MITK/MITK.git\n\nActive development takes place in the MITK develop branch and its usage is advised for advanced users only.\n\nHow to contribute\n-----------------\n\nContributions are encouraged. To make the contribution process as smooth as possible, please read [Contributing to MITK][contribute] before.\n\nBuild instructions\n------------------\n\nMITK uses [CMake][cmake] to configure a build tree. The following is a crash course about cloning, configuring, and building MITK with Ninja on Linux or macOS when all [prerequisites][prerequisites] are met:\n\n    git clone https://github.com/MITK/MITK.git\n    mkdir MITK-superbuild\n    cmake -S MITK -B MITK-superbuild -G \"Ninja\" -D CMAKE_BUILD_TYPE=Release\n    cmake --build MITK-superbuild\n\nOn Windows, configuring and building with Visual Studio/MSBuild would look something like this:\n\n    cmake -S MITK -B MITK-superbuild -G \"Visual Studio 17 2022\"\n    cmake --build MITK-superbuild --config Release -- -m\n\nRead the comprehensive [build instructions][build] page for details.\n\nUseful links\n------------\n\n - [Homepage][mitk]\n - [Download][download]\n - [Create an issue/ask for help][issues]\n\n[logo]: https://github.com/MITK/MITK/raw/master/mitk.png\n[mitk]: https://www.mitk.org\n[itk]: https://itk.org\n[vtk]: https://vtk.org\n[mitk-overview]: https://docs.mitk.org/2024.12/\n[mitk-usermanual]: https://docs.mitk.org/2024.12/UserManualPortal.html\n[mitk-devmanual]: https://docs.mitk.org/2024.12/DeveloperManualPortal.html\n[mitk-apiref]: https://docs.mitk.org/2024.12/usergroup0.html\n[platforms]: https://docs.mitk.org/2024.12/SupportedPlatformsPage.html\n[prerequisites]: https://docs.mitk.org/2024.12/BuildInstructionsPage.html#BuildInstructions_Prerequisites\n[build]: https://docs.mitk.org/2024.12/BuildInstructionsPage.html\n[dkfz]: https://www.dkfz.de\n[license]: https://github.com/MITK/MITK/blob/master/LICENSE\n[download]: https://www.mitk.org/Download\n[releases]: https://github.com/MITK/MITK/releases\n[git_repo]: https://github.com/MITK/MITK\n[contribute]: https://github.com/MITK/MITK/blob/master/CONTRIBUTING.md\n[cmake]: https://www.cmake.org\n[issues]: https://github.com/MITK/MITK/issues\n\n",
                "dependencies": "#[[ When increasing the minimum required version, check if Boost_ADDITIONAL_VERSIONS\n    in CMake/PackageDepends/MITK_Boost_Config.cmake can be removed. See the first\n    long comment in CMakeExternals/Boost.cmake for details. ]]\n\nset(MITK_CMAKE_MINIMUM_REQUIRED_VERSION 3.18)\ncmake_minimum_required(VERSION ${MITK_CMAKE_MINIMUM_REQUIRED_VERSION})\n\nif(CMAKE_VERSION VERSION_GREATER_EQUAL 3.19 AND CMAKE_VERSION VERSION_LESS 3.19.2)\n  message(FATAL_ERROR \"\\\nCMake v${CMAKE_VERSION} is defective [1]. \\\nPlease either downgrade to v3.18 or upgrade to at least v3.19.2.\\n\\\n[1] https://gitlab.kitware.com/cmake/cmake/-/issues/21529\")\nendif()\n\n#-----------------------------------------------------------------------------\n# Policies\n#-----------------------------------------------------------------------------\n\n#[[ T28060\n\n    https://cmake.org/cmake/help/v3.18/policy/CMP0091.html\n    https://cmake.org/cmake/help/v3.18/variable/CMAKE_MSVC_RUNTIME_LIBRARY.html\n\n    We pass CMP0091 to all external projects as command-line argument:\n      -DCMAKE_POLICY_DEFAULT_CMP0091:STRING=OLD\n ]]\ncmake_policy(SET CMP0091 OLD)\n\nif(POLICY CMP0135)\n  # https://cmake.org/cmake/help/v3.24/policy/CMP0135.html\n  # Set timestamps of extracted ExternalProject_Add() downloads to time of extraction.\n  cmake_policy(SET CMP0135 NEW)\nendif()\n\nif(POLICY CMP0167)\n  # https://cmake.org/cmake/help/v3.30/policy/CMP0167.html\n  # The FindBoost module is removed.\n  cmake_policy(SET CMP0167 OLD)\nendif()\n\n#-----------------------------------------------------------------------------\n# Superbuild Option - Enabled by default\n#-----------------------------------------------------------------------------\n\noption(MITK_USE_SUPERBUILD \"Build MITK and the projects it depends on via SuperBuild.cmake.\" ON)\n\nif(MITK_USE_SUPERBUILD)\n  project(MITK-superbuild)\n  set(MITK_SOURCE_DIR ${PROJECT_SOURCE_DIR})\n  set(MITK_BINARY_DIR ${PROJECT_BINARY_DIR})\nelse()\n  project(MITK VERSION 2024.12.00)\n  include_directories(SYSTEM ${MITK_SUPERBUILD_BINARY_DIR})\nendif()\n\n#-----------------------------------------------------------------------------\n# MITK Extension Feature\n#-----------------------------------------------------------------------------\n\nset(MITK_EXTENSION_DIRS \"\" CACHE STRING \"\")\n\nunset(MITK_ABSOLUTE_EXTENSION_DIRS)\n\nforeach(MITK_EXTENSION_DIR ${MITK_EXTENSION_DIRS})\n  get_filename_component(MITK_ABSOLUTE_EXTENSION_DIR \"${MITK_EXTENSION_DIR}\" ABSOLUTE)\n  list(APPEND MITK_ABSOLUTE_EXTENSION_DIRS \"${MITK_ABSOLUTE_EXTENSION_DIR}\")\nendforeach()\n\nset(MITK_DIR_PLUS_EXTENSION_DIRS \"${MITK_SOURCE_DIR}\" ${MITK_ABSOLUTE_EXTENSION_DIRS})\n\n#-----------------------------------------------------------------------------\n# Update CMake module path\n#-----------------------------------------------------------------------------\n\nset(MITK_CMAKE_DIR ${MITK_SOURCE_DIR}/CMake)\n\nset(CMAKE_MODULE_PATH ${MITK_CMAKE_DIR})\n\nforeach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS})\n  set(MITK_CMAKE_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/CMake\")\n  if(EXISTS \"${MITK_CMAKE_EXTENSION_DIR}\")\n    list(APPEND CMAKE_MODULE_PATH \"${MITK_CMAKE_EXTENSION_DIR}\")\n  endif()\nendforeach()\n\n#-----------------------------------------------------------------------------\n# CMake function(s) and macro(s)\n#-----------------------------------------------------------------------------\n\n# Standard CMake macros\ninclude(FeatureSummary)\ninclude(CTest)\ninclude(CMakeParseArguments)\ninclude(FindPackageHandleStandardArgs)\n\n# MITK macros\ninclude(mitkFunctionGetGccVersion)\ninclude(mitkFunctionCheckCompilerFlags)\ninclude(mitkFunctionSuppressWarnings) # includes several functions\ninclude(mitkMacroEmptyExternalProject)\ninclude(mitkFunctionEnableBuildConfiguration)\ninclude(mitkFunctionWhitelists)\ninclude(mitkFunctionAddExternalProject)\ninclude(mitkFunctionAddLibrarySearchPaths)\n\nSUPPRESS_VC_DEPRECATED_WARNINGS()\n\n#-----------------------------------------------------------------------------\n# Set a default build type if none was specified\n#-----------------------------------------------------------------------------\n\nif(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n  message(STATUS \"Setting build type to 'Debug' as none was specified.\")\n  set(CMAKE_BUILD_TYPE Debug CACHE STRING \"Choose the type of build.\" FORCE)\n\n  # Set the possible values of build type for cmake-gui\n  set_property(CACHE CMAKE_BUILD_TYPE PROPERTY\n               STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\")\nendif()\n\nif(CMAKE_COMPILER_IS_GNUCXX)\n  mitkFunctionGetGccVersion(${CMAKE_CXX_COMPILER} GCC_VERSION)\nelse()\n  set(GCC_VERSION 0)\nendif()\n\nset(MITK_CXX_STANDARD 17)\n\nset(CMAKE_CXX_EXTENSIONS 0)\nset(CMAKE_CXX_STANDARD ${MITK_CXX_STANDARD})\nset(CMAKE_CXX_STANDARD_REQUIRED 1)\n\n# This is necessary to avoid problems with compile feature checks.\n# CMAKE_CXX_STANDARD seems to only set the -std=c++<std> flag for targets.\n# However, compile flag checks also need to be done with -std=c++<std>.\n# The MITK_CXX<std>_FLAG variable is also used for external projects\n# build during the MITK super-build.\nmitkFunctionCheckCompilerFlags(\"-std=c++${MITK_CXX_STANDARD}\" MITK_CXX${MITK_CXX_STANDARD}_FLAG)\n\n#-----------------------------------------------------------------------------\n# Warn if source or build path is too long\n#-----------------------------------------------------------------------------\n\nif(WIN32)\n  set(_src_dir_length_max 50)\n  set(_bin_dir_length_max 50)\n  if(MITK_USE_SUPERBUILD)\n    set(_src_dir_length_max 34) # _src_dir_length_max - strlen(ep/src/ITK-build)\n    set(_bin_dir_length_max 40) # _bin_dir_length_max - strlen(MITK-build)\n  endif()\n\n  string(LENGTH \"${MITK_SOURCE_DIR}\" _src_n)\n  string(LENGTH \"${MITK_BINARY_DIR}\" _bin_n)\n\n  # The warnings should be converted to errors\n  if(_src_n GREATER _src_dir_length_max)\n    message(WARNING \"MITK source code directory path length is too long (${_src_n} > ${_src_dir_length_max}).\"\n                    \"Please move the MITK source code directory to a directory with a shorter path.\" )\n  endif()\n  if(_bin_n GREATER _bin_dir_length_max)\n    message(WARNING \"MITK build directory path length is too long (${_bin_n} > ${_bin_dir_length_max}).\"\n                    \"Please move the MITK build directory to a directory with a shorter path.\" )\n  endif()\nendif()\n\n#-----------------------------------------------------------------------------\n# Switch on parallel builds in Visual Studio\n#-----------------------------------------------------------------------------\n\n# Since Visual Studio 2019 v16.9 a new Multi-ToolTask scheduler is available,\n# which schedules and limits tasks both within (was /MP) and across targets (-m).\n# Make sure to add the -m option in build scripts when using MSBuild or CMake\n# to build everything:\n#\n#   msbuild -m\n#\n#   resp.\n#\n#   cmake --build <path> -- -m\n\nif(MSVC_VERSION VERSION_GREATER_EQUAL 1928)\n  if(NOT CMAKE_VS_GLOBALS MATCHES \"(^|;)UseMultiToolTask=\")\n    list(APPEND CMAKE_VS_GLOBALS UseMultiToolTask=true)\n  endif()\n\n  if(NOT CMAKE_VS_GLOBALS MATCHES \"(^|;)EnforceProcessCountAcrossBuilds=\")\n    list(APPEND CMAKE_VS_GLOBALS EnforceProcessCountAcrossBuilds=true)\n  endif()\nendif()\n\n#-----------------------------------------------------------------------------\n# Additional MITK Options (also shown during superbuild)\n#-----------------------------------------------------------------------------\n\n# -----------------------------------------\n# General build options\noption(BUILD_SHARED_LIBS \"Build MITK with shared libraries\" ON)\noption(WITH_COVERAGE \"Enable/Disable coverage\" OFF)\noption(BUILD_TESTING \"Test the project\" ON)\noption(MITK_FAST_TESTING \"Disable long-running tests like packaging\" OFF)\noption(MITK_XVFB_TESTING \"Execute test drivers through xvfb-run\" OFF)\noption(MITK_PCH \"Enable precompiled headers\" ON)\n\noption(MITK_BUILD_ALL_APPS \"Build all MITK applications\" OFF)\noption(MITK_BUILD_EXAMPLES \"Build the MITK Examples\" OFF)\n\nmark_as_advanced(\n  MITK_XVFB_TESTING\n  MITK_FAST_TESTING\n  MITK_BUILD_ALL_APPS\n  MITK_PCH\n)\n\n#-----------------------------------------------------------------------------\n# Set UI testing flags\n#-----------------------------------------------------------------------------\nif(MITK_XVFB_TESTING)\n  set(MITK_XVFB_TESTING_COMMAND \"xvfb-run\" \"--auto-servernum\" CACHE STRING \"Command and options to test through Xvfb\")\n  mark_as_advanced(MITK_XVFB_TESTING_COMMAND)\nendif(MITK_XVFB_TESTING)\n\n# -----------------------------------------\n# Other options\nset(MITK_CUSTOM_REVISION_DESC \"\" CACHE STRING \"Override MITK revision description\")\nmark_as_advanced(MITK_CUSTOM_REVISION_DESC)\n\nset_property(GLOBAL PROPERTY MITK_EXTERNAL_PROJECTS \"\")\n\ninclude(CMakeExternals/ExternalProjectList.cmake)\n\nforeach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS})\n  set(MITK_CMAKE_EXTERNALS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/CMakeExternals\")\n  if(EXISTS \"${MITK_CMAKE_EXTERNALS_EXTENSION_DIR}/ExternalProjectList.cmake\")\n    include(\"${MITK_CMAKE_EXTERNALS_EXTENSION_DIR}/ExternalProjectList.cmake\")\n  endif()\nendforeach()\n\n# -----------------------------------------\n# Other MITK_USE_* options not related to\n# external projects build via the\n# MITK superbuild\n\noption(MITK_USE_BLUEBERRY \"Build the BlueBerry platform\" ON)\noption(MITK_USE_OpenMP \"Use OpenMP\" OFF)\noption(MITK_USE_Python3 \"Use Python 3\" OFF)\n\n#-----------------------------------------------------------------------------\n# Build configurations\n#-----------------------------------------------------------------------------\n\nset(_buildConfigs \"Custom\")\n\nfile(GLOB _buildConfigFiles CMake/BuildConfigurations/*.cmake)\n\nforeach(_buildConfigFile ${_buildConfigFiles})\n  get_filename_component(_buildConfigFile ${_buildConfigFile} NAME_WE)\n  list(APPEND _buildConfigs ${_buildConfigFile})\nendforeach()\n\nforeach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS})\n  file(GLOB _extBuildConfigFiles \"${MITK_EXTENSION_DIR}/CMake/BuildConfigurations/*.cmake\")\n\n  foreach(_extBuildConfigFile ${_extBuildConfigFiles})\n    get_filename_component(_extBuildConfigFile \"${_extBuildConfigFile}\" NAME_WE)\n    list(APPEND _buildConfigs \"${_extBuildConfigFile}\")\n  endforeach()\n\n  list(REMOVE_DUPLICATES _buildConfigs)\nendforeach()\n\nset(MITK_BUILD_CONFIGURATION \"WorkbenchRelease\" CACHE STRING \"Use pre-defined MITK configurations\")\nset_property(CACHE MITK_BUILD_CONFIGURATION PROPERTY STRINGS ${_buildConfigs})\n\nmitkFunctionEnableBuildConfiguration()\n\nmitkFunctionCreateWhitelistPaths(MITK)\nmitkFunctionFindWhitelists(MITK)\n\n# -----------------------------------------\n# Qt version related variables\n\noption(MITK_USE_Qt6 \"Use Qt 6 library\" ON)\n\nif(MITK_USE_Qt6)\n  set(MITK_QT6_MINIMUM_VERSION 6.6)\n  set(MITK_QT6_COMPONENTS\n    Concurrent\n    Core\n    Core5Compat\n    CoreTools\n    Designer\n    DesignerComponentsPrivate\n    Gui\n    Help\n    LinguistTools\n    Network\n    OpenGL\n    OpenGLWidgets\n    Qml\n    Sql\n    StateMachine\n    Svg\n    ToolsTools\n    UiTools\n    WebEngineCore\n    WebEngineWidgets\n    Widgets\n    Xml\n  )\n  if(APPLE)\n    list(APPEND MITK_QT6_COMPONENTS DBus)\n  endif()\n\n  # Hint at default install locations of Qt\n  if(NOT Qt6_DIR)\n    if(MSVC)\n      set(_dir_candidates \"C:/Qt\")\n\n      if(CMAKE_GENERATOR MATCHES \"^Visual Studio [0-9]+ ([0-9]+)\")\n        set(_compilers \"msvc${CMAKE_MATCH_1}\")\n      elseif(CMAKE_GENERATOR MATCHES \"Ninja\")\n        include(mitkFunctionGetMSVCVersion)\n        mitkFunctionGetMSVCVersion()\n        if(VISUAL_STUDIO_PRODUCT_NAME MATCHES \"^Visual Studio ([0-9]+)\")\n          set(_compilers \"msvc${CMAKE_MATCH_1}\")\n        endif()\n      endif()\n\n      if(_compilers MATCHES \"[0-9]+\")\n        if (CMAKE_MATCH_0 EQUAL 2022)\n          list(APPEND _compilers \"msvc2019\") # Binary compatible\n        endif()\n      endif()\n    else()\n      set(_dir_candidates ~/Qt)\n\n      if(APPLE)\n        set(_compilers clang)\n      else()\n        list(APPEND _dir_candidates /opt/Qt)\n        set(_compilers gcc)\n      endif()\n    endif()\n\n    if(CMAKE_SIZEOF_VOID_P EQUAL 8)\n      foreach(_compiler ${_compilers})\n        list(APPEND _compilers64 \"${_compiler}_64\")\n      endforeach()\n      set(_compilers ${_compilers64})\n    endif()\n\n    if(APPLE)\n      list(APPEND _compilers macos)\n    endif()\n\n    foreach(_dir_candidate ${_dir_candidates})\n      get_filename_component(_dir_candidate ${_dir_candidate} REALPATH)\n      foreach(_compiler ${_compilers})\n        set(_glob_expression \"${_dir_candidate}/6.*/${_compiler}\")\n        file(GLOB _hints ${_glob_expression})\n        list(SORT _hints)\n        list(APPEND MITK_QT6_HINTS ${_hints})\n      endforeach()\n    endforeach()\n  endif()\n\n  find_package(Qt6 ${MITK_QT6_MINIMUM_VERSION} COMPONENTS ${MITK_QT6_COMPONENTS} REQUIRED HINTS ${MITK_QT6_HINTS})\n\n  get_target_property(QT_QMAKE_EXECUTABLE Qt6::qmake LOCATION)\n  get_target_property(QT_HELPGENERATOR_EXECUTABLE Qt6::qhelpgenerator LOCATION)\nendif()\n\nif(Qt6_DIR)\n  list(APPEND CMAKE_PREFIX_PATH \"${Qt6_DIR}/../../..\")\n  list(REMOVE_DUPLICATES CMAKE_PREFIX_PATH)\nendif()\n\n# -----------------------------------------\n# Custom dependency logic\n\nif(WIN32 AND Qt6_DIR)\n  set(_dir_candidate \"${Qt6_DIR}/../../../../../Tools/OpenSSLv3/Win_x64\")\n  get_filename_component(_dir_candidate ${_dir_candidate} ABSOLUTE)\n  if(EXISTS \"${_dir_candidate}\")\n    set(OPENSSL_ROOT_DIR \"${_dir_candidate}\")\n  endif()\nendif()\n\nfind_package(OpenSSL 3)\n\nif(NOT OpenSSL_FOUND)\n  find_package(OpenSSL 1.1.1)\nendif()\n\noption(MITK_USE_SYSTEM_Boost \"Use the system Boost\" OFF)\n\nset(MITK_USE_Boost_LIBRARIES \"\" CACHE STRING \"A semi-colon separated list of required Boost libraries\")\n\nif(MITK_USE_httplib AND NOT OpenSSL_FOUND)\n  set(openssl_message \"Could not find OpenSSL (dependency of cpp-httplib).\\n\")\n  if(UNIX)\n    if(APPLE)\n      set(openssl_message \"${openssl_message}Please install it using your favorite package management \"\n                          \"system (i.e. Homebrew or MacPorts).\\n\")\n    else()\n      set(openssl_message \"${openssl_message}Please install the dev package of OpenSSL (i.e. libssl-dev).\\n\")\n    endif()\n  else()\n    set(openssl_message \"${openssl_message}Please either install Win32 OpenSSL:\\n\"\n                        \"  https://slproweb.com/products/Win32OpenSSL.html\\n\"\n                        \"Or use the Qt Maintenance tool to install (recommended):\\n\"\n                        \"  Developer and Designer Tools > OpenSSL Toolkit > OpenSSL 64-bit binaries\\n\")\n  endif()\n  set(openssl_message \"${openssl_message}If it still cannot be found, you can hint CMake to find OpenSSL by \"\n                      \"adding/setting the OPENSSL_ROOT_DIR variable to the root directory of an \"\n                      \"OpenSSL installation. Make sure to clear variables of partly found \"\n                      \"versions of OpenSSL before, or they will be mixed up.\")\n  message(FATAL_ERROR ${openssl_message})\nendif()\n\nif(MITK_USE_Python3)\n  set(MITK_USE_ZLIB ON CACHE BOOL \"\" FORCE)\n\n  if(APPLE)\n    set(python3_mininum_version 3.11)\n  else()\n    set(python3_mininum_version 3.8)\n  endif()\n\n  find_package(Python3 ${python3_mininum_version} REQUIRED COMPONENTS Interpreter Development NumPy)\n\n  if(WIN32)\n    string(REPLACE \"\\\\\" \"/\" Python3_STDARCH \"${Python3_STDARCH}\")\n    string(REPLACE \"\\\\\" \"/\" Python3_STDLIB \"${Python3_STDLIB}\")\n    string(REPLACE \"\\\\\" \"/\" Python3_SITELIB \"${Python3_SITELIB}\")\n  endif()\nendif()\n\nif(BUILD_TESTING AND NOT MITK_USE_CppUnit)\n  message(\"> Forcing MITK_USE_CppUnit to ON because BUILD_TESTING=ON\")\n  set(MITK_USE_CppUnit ON CACHE BOOL \"Use CppUnit for unit tests\" FORCE)\nendif()\n\nif(MITK_USE_BLUEBERRY)\n  option(MITK_BUILD_ALL_PLUGINS \"Build all MITK plugins\" OFF)\n  mark_as_advanced(MITK_BUILD_ALL_PLUGINS)\n\n  if(NOT MITK_USE_CTK)\n    message(\"> Forcing MITK_USE_CTK to ON because of MITK_USE_BLUEBERRY\")\n    set(MITK_USE_CTK ON CACHE BOOL \"Use CTK in MITK\" FORCE)\n  endif()\nendif()\n\n#-----------------------------------------------------------------------------\n# Pixel type multiplexing\n#-----------------------------------------------------------------------------\n\n# Customize the default pixel types for multiplex macros\n\nset(MITK_ACCESSBYITK_INTEGRAL_PIXEL_TYPES\n    \"int, unsigned int, short, unsigned short, char, unsigned char\"\n    CACHE STRING \"List of integral pixel types used in AccessByItk and InstantiateAccessFunction macros\")\n\nset(MITK_ACCESSBYITK_FLOATING_PIXEL_TYPES\n    \"double, float\"\n    CACHE STRING \"List of floating pixel types used in AccessByItk and InstantiateAccessFunction macros\")\n\nset(MITK_ACCESSBYITK_COMPOSITE_PIXEL_TYPES\n    \"itk::RGBPixel<unsigned char>, itk::RGBAPixel<unsigned char>\"\n    CACHE STRING \"List of composite pixel types used in AccessByItk and InstantiateAccessFunction macros\")\n\nset(MITK_ACCESSBYITK_DIMENSIONS\n    \"2,3\"\n    CACHE STRING \"List of dimensions used in AccessByItk and InstantiateAccessFunction macros\")\n\nmark_as_advanced(MITK_ACCESSBYITK_INTEGRAL_PIXEL_TYPES\n                 MITK_ACCESSBYITK_FLOATING_PIXEL_TYPES\n                 MITK_ACCESSBYITK_COMPOSITE_PIXEL_TYPES\n                 MITK_ACCESSBYITK_DIMENSIONS\n                )\n\n# consistency checks\n\nif(NOT MITK_ACCESSBYITK_INTEGRAL_PIXEL_TYPES)\n  set(MITK_ACCESSBYITK_INTEGRAL_PIXEL_TYPES\n      \"int, unsigned int, short, unsigned short, char, unsigned char\"\n      CACHE STRING \"List of integral pixel types used in AccessByItk and InstantiateAccessFunction macros\" FORCE)\nendif()\n\nif(NOT MITK_ACCESSBYITK_FLOATING_PIXEL_TYPES)\n  set(MITK_ACCESSBYITK_FLOATING_PIXEL_TYPES\n      \"double, float\"\n      CACHE STRING \"List of floating pixel types used in AccessByItk and InstantiateAccessFunction macros\" FORCE)\nendif()\n\nif(NOT MITK_ACCESSBYITK_COMPOSITE_PIXEL_TYPES)\n  set(MITK_ACCESSBYITK_COMPOSITE_PIXEL_TYPES\n    \"itk::RGBPixel<unsigned char>, itk::RGBAPixel<unsigned char>\"\n    CACHE STRING \"List of composite pixel types used in AccessByItk and InstantiateAccessFunction macros\" FORCE)\nendif()\n\nif(NOT MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES)\n  string(REPLACE \",\" \";\" _integral_types ${MITK_ACCESSBYITK_INTEGRAL_PIXEL_TYPES})\n  string(REPLACE \",\" \";\" _floating_types ${MITK_ACCESSBYITK_FLOATING_PIXEL_TYPES})\n  foreach(_scalar_type ${_integral_types} ${_floating_types})\n    set(MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES\n        \"${MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES}itk::VariableLengthVector<${_scalar_type}>,\")\n  endforeach()\n  string(LENGTH \"${MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES}\" _length)\n  math(EXPR _length \"${_length} - 1\")\n  string(SUBSTRING \"${MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES}\" 0 ${_length} MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES)\n  set(MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES ${MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES}\n      CACHE STRING \"List of vector pixel types used in AccessByItk and InstantiateAccessFunction macros for itk::VectorImage types\" FORCE)\nendif()\n\nif(NOT MITK_ACCESSBYITK_DIMENSIONS)\n  set(MITK_ACCESSBYITK_DIMENSIONS\n      \"2,3\"\n      CACHE STRING \"List of dimensions used in AccessByItk and InstantiateAccessFunction macros\")\nendif()\n\nfind_package(Git REQUIRED)\n\n#-----------------------------------------------------------------------------\n# Superbuild script\n#-----------------------------------------------------------------------------\n\nif(MITK_USE_SUPERBUILD)\n  include(\"${CMAKE_CURRENT_SOURCE_DIR}/SuperBuild.cmake\")\n\n  # Print configuration summary\n  message(\"\\n\\n\")\n  feature_summary(\n    DESCRIPTION \"------- FEATURE SUMMARY FOR ${PROJECT_NAME} -------\"\n    WHAT ALL)\n  return()\nendif()\n\n#*****************************************************************************\n#****************************  END OF SUPERBUILD  ****************************\n#*****************************************************************************\n\n#-----------------------------------------------------------------------------\n# Organize MITK targets in folders\n#-----------------------------------------------------------------------------\n\nset_property(GLOBAL PROPERTY USE_FOLDERS ON)\nset(MITK_ROOT_FOLDER \"MITK\" CACHE STRING \"\")\nmark_as_advanced(MITK_ROOT_FOLDER)\n\n#-----------------------------------------------------------------------------\n# CMake function(s) and macro(s)\n#-----------------------------------------------------------------------------\n\ninclude(WriteBasicConfigVersionFile)\ninclude(CheckCXXSourceCompiles)\ninclude(GenerateExportHeader)\n\ninclude(mitkFunctionAddManifest)\ninclude(mitkFunctionAddCustomModuleTest)\ninclude(mitkFunctionCheckModuleDependencies)\ninclude(mitkFunctionCompileSnippets)\ninclude(mitkFunctionConfigureVisualStudioUserProjectFile)\ninclude(mitkFunctionCreateBlueBerryApplication)\ninclude(mitkFunctionCreateCommandLineApp)\ninclude(mitkFunctionCreateModule)\ninclude(mitkFunctionCreatePlugin)\ninclude(mitkFunctionCreateProvisioningFile)\ninclude(mitkFunctionGetLibrarySearchPaths)\ninclude(mitkFunctionGetVersion)\ninclude(mitkFunctionGetVersionDescription)\ninclude(mitkFunctionInstallAutoLoadModules)\ninclude(mitkFunctionInstallCTKPlugin)\ninclude(mitkFunctionInstallProvisioningFiles)\ninclude(mitkFunctionInstallThirdPartyCTKPlugins)\ninclude(mitkFunctionOrganizeSources)\ninclude(mitkFunctionUseModules)\nif( ${MITK_USE_MatchPoint} )\n  include(mitkFunctionCreateMatchPointDeployedAlgorithm)\nendif()\ninclude(mitkMacroConfigureItkPixelTypes)\ninclude(mitkMacroCreateExecutable)\ninclude(mitkMacroCreateModuleTests)\ninclude(mitkMacroGenerateToolsLibrary)\ninclude(mitkMacroGetLinuxDistribution)\ninclude(mitkMacroGetPMDPlatformString)\ninclude(mitkMacroInstall)\ninclude(mitkMacroInstallHelperApp)\ninclude(mitkMacroInstallTargets)\ninclude(mitkMacroMultiplexPicType)\n\n#-----------------------------------------------------------------------------\n# Global CMake variables\n#-----------------------------------------------------------------------------\n\nif(NOT DEFINED CMAKE_DEBUG_POSTFIX)\n  # We can't do this yet because the CTK Plugin Framework\n  # cannot cope with a postfix yet.\n  #set(CMAKE_DEBUG_POSTFIX d)\nendif()\n\n#-----------------------------------------------------------------------------\n# Output directories.\n#-----------------------------------------------------------------------------\n\nset(_default_LIBRARY_output_dir lib)\nset(_default_RUNTIME_output_dir bin)\nset(_default_ARCHIVE_output_dir lib)\n\nforeach(type LIBRARY RUNTIME ARCHIVE)\n  # Make sure the directory exists\n  if(MITK_CMAKE_${type}_OUTPUT_DIRECTORY\n     AND NOT EXISTS ${MITK_CMAKE_${type}_OUTPUT_DIRECTORY})\n    message(\"Creating directory MITK_CMAKE_${type}_OUTPUT_DIRECTORY: ${MITK_CMAKE_${type}_OUTPUT_DIRECTORY}\")\n    file(MAKE_DIRECTORY \"${MITK_CMAKE_${type}_OUTPUT_DIRECTORY}\")\n  endif()\n\n  if(MITK_CMAKE_${type}_OUTPUT_DIRECTORY)\n    set(CMAKE_${type}_OUTPUT_DIRECTORY ${MITK_CMAKE_${type}_OUTPUT_DIRECTORY})\n  else()\n    set(CMAKE_${type}_OUTPUT_DIRECTORY ${PROJECT_BINARY_DIR}/${_default_${type}_output_dir})\n    set(MITK_CMAKE_${type}_OUTPUT_DIRECTORY ${CMAKE_${type}_OUTPUT_DIRECTORY})\n  endif()\n\n  set(CMAKE_${type}_OUTPUT_DIRECTORY ${CMAKE_${type}_OUTPUT_DIRECTORY} CACHE INTERNAL \"Output directory for ${type} files.\")\n  mark_as_advanced(CMAKE_${type}_OUTPUT_DIRECTORY)\nendforeach()\n\n#-----------------------------------------------------------------------------\n# Set MITK specific options and variables (NOT available during superbuild)\n#-----------------------------------------------------------------------------\n\nif(OpenSSL_FOUND AND WIN32)\n  #[[ On Windows, CMake is able to locate the link libraries for OpenSSL but it\n      does not look for the corresponding DLLs that we need to copy to our\n      binary directories and include in packaging.\n\n      Setting these paths manually is cumbersome so we try to use a simple\n      heuristic to automatically set them:\n\n        - Based on the link libraries (usually located in a lib folder),\n          try to find the \"../bin\" binary directory.\n        - Use the base file names of the link libraries to find corresponding\n          DLLs like \"<base name>*.dll\", that usually are named like\n          \"<base name>-1_1-x64.dll\" or similar.\n   ]]\n  set(openssl_ssl_dll \"\")\n  set(openssl_crypto_dll \"\")\n\n  if(OPENSSL_SSL_LIBRARY)\n    set(openssl_ssl_lib \"\")\n\n    list(LENGTH OPENSSL_SSL_LIBRARY num_items)\n    if(num_items GREATER 1)\n      list(FIND OPENSSL_SSL_LIBRARY \"optimized\" optimized)\n      if(NOT optimized EQUAL -1)\n        math(EXPR optimized \"${optimized}+1\")\n        if(optimized LESS num_items)\n          list(GET OPENSSL_SSL_LIBRARY ${optimized} openssl_ssl_lib)\n        endif()\n      endif()\n    else()\n      set(openssl_ssl_lib \"${OPENSSL_SSL_LIBRARY}\")\n    endif()\n\n    if(EXISTS \"${openssl_ssl_lib}\")\n      get_filename_component(openssl_bin_dir \"${openssl_ssl_lib}\" DIRECTORY)\n      get_filename_component(openssl_bin_dir \"${openssl_bin_dir}\" DIRECTORY)\n      set(openssl_bin_dir \"${openssl_bin_dir}/bin\")\n\n      if(EXISTS \"${openssl_bin_dir}\")\n        get_filename_component(openssl_ssl_basename \"${openssl_ssl_lib}\" NAME_WE)\n        file(GLOB openssl_ssl_dll \"${openssl_bin_dir}/${openssl_ssl_basename}*.dll\")\n        list(LENGTH openssl_ssl_dll num_findings)\n        if(num_findings GREATER 1)\n          set(openssl_ssl_dll \"\")\n        endif()\n\n        set(openssl_crypto_lib \"\")\n\n        list(LENGTH OPENSSL_CRYPTO_LIBRARY num_items)\n        if(num_items GREATER 1)\n          list(FIND OPENSSL_CRYPTO_LIBRARY \"optimized\" optimized)\n          if(NOT optimized EQUAL -1)\n            math(EXPR optimized \"${optimized}+1\")\n            if(optimized LESS num_items)\n              list(GET OPENSSL_CRYPTO_LIBRARY ${optimized} openssl_crypto_lib)\n            endif()\n          endif()\n        else()\n          set(openssl_crypto_lib \"${OPENSSL_CRYPTO_LIBRARY}\")\n        endif()\n\n        get_filename_component(openssl_crypto_basename \"${openssl_crypto_lib}\" NAME_WE)\n        file(GLOB openssl_crypto_dll \"${openssl_bin_dir}/${openssl_crypto_basename}*.dll\")\n        list(LENGTH openssl_crypto_dll num_findings)\n        if(num_findings GREATER 1)\n          set(openssl_crypto_dll \"\")\n        endif()\n      endif()\n    endif()\n  endif()\n\n  set(MITK_OPENSSL_SSL_DLL \"${openssl_ssl_dll}\" CACHE FILEPATH \"\")\n\n  if(DEFINED CACHE{MITK_OPENSSL_SSL_DLL} AND NOT MITK_OPENSSL_SSL_DLL AND openssl_ssl_dll)\n    set(MITK_OPENSSL_SSL_DLL \"${openssl_ssl_dll}\" CACHE FILEPATH \"\" FORCE)\n  endif()\n\n  set(MITK_OPENSSL_CRYPTO_DLL \"${openssl_crypto_dll}\" CACHE FILEPATH \"\")\n\n  if(DEFINED CACHE{MITK_OPENSSL_CRYPTO_DLL} AND NOT MITK_OPENSSL_CRYPTO_DLL AND openssl_crypto_dll)\n    set(MITK_OPENSSL_CRYPTO_DLL \"${openssl_crypto_dll}\" CACHE FILEPATH \"\" FORCE)\n  endif()\n\n  if(MITK_OPENSSL_SSL_DLL AND EXISTS \"${MITK_OPENSSL_SSL_DLL}\" AND MITK_OPENSSL_CRYPTO_DLL AND EXISTS \"${MITK_OPENSSL_CRYPTO_DLL}\")\n    foreach(config_type ${CMAKE_CONFIGURATION_TYPES})\n      execute_process(COMMAND \"${CMAKE_COMMAND}\" -E make_directory \"${MITK_BINARY_DIR}/bin/${config_type}\")\n      configure_file(\"${MITK_OPENSSL_SSL_DLL}\" \"${MITK_BINARY_DIR}/bin/${config_type}/\" COPYONLY)\n      configure_file(\"${MITK_OPENSSL_CRYPTO_DLL}\" \"${MITK_BINARY_DIR}/bin/${config_type}/\" COPYONLY)\n    endforeach()\n\n    MITK_INSTALL(FILES\n      \"${MITK_OPENSSL_SSL_DLL}\"\n      \"${MITK_OPENSSL_CRYPTO_DLL}\"\n    )\n  endif()\nendif()\n\n# Look for optional Doxygen package\nfind_package(Doxygen)\n\noption(BLUEBERRY_DEBUG_SMARTPOINTER \"Enable code for debugging smart pointers\" OFF)\nmark_as_advanced(BLUEBERRY_DEBUG_SMARTPOINTER)\n\n# Ask the user to show the console window for applications\noption(MITK_SHOW_CONSOLE_WINDOW \"Use this to enable or disable the console window when starting MITK GUI Applications\" ON)\nmark_as_advanced(MITK_SHOW_CONSOLE_WINDOW)\n\nif(NOT MITK_FAST_TESTING)\n  if(MITK_CTEST_SCRIPT_MODE STREQUAL \"Continuous\" OR MITK_CTEST_SCRIPT_MODE STREQUAL \"Experimental\")\n    set(MITK_FAST_TESTING ON)\n  endif()\nendif()\n\nif(NOT UNIX)\n  set(MITK_WIN32_FORCE_STATIC \"STATIC\" CACHE INTERNAL \"Use this variable to always build static libraries on non-unix platforms\")\nendif()\n\nif(MITK_BUILD_ALL_PLUGINS)\n  set(MITK_BUILD_ALL_PLUGINS_OPTION \"FORCE_BUILD_ALL\")\nendif()\n\n# Configure pixel types used for ITK image access multiplexing\nmitkMacroConfigureItkPixelTypes()\n\n# Configure module naming conventions\nset(MITK_MODULE_NAME_REGEX_MATCH \"^[A-Z].*$\")\nset(MITK_MODULE_NAME_REGEX_NOT_MATCH \"^[Mm][Ii][Tt][Kk].*$\")\nset(MITK_DEFAULT_MODULE_NAME_PREFIX \"Mitk\")\nset(MITK_MODULE_NAME_PREFIX ${MITK_DEFAULT_MODULE_NAME_PREFIX})\nset(MITK_MODULE_NAME_DEFAULTS_TO_DIRECTORY_NAME 1)\n\n#-----------------------------------------------------------------------------\n# Get MITK version info\n#-----------------------------------------------------------------------------\n\nmitkFunctionGetVersion(${MITK_SOURCE_DIR} MITK)\nmitkFunctionGetVersionDescription(${MITK_SOURCE_DIR} MITK)\n\n# MITK_VERSION\nset(MITK_VERSION_STRING \"${MITK_VERSION_MAJOR}.${MITK_VERSION_MINOR}.${MITK_VERSION_PATCH}\")\nif(MITK_VERSION_PATCH STREQUAL \"99\")\n  set(MITK_VERSION_STRING \"${MITK_VERSION_STRING}-${MITK_REVISION_SHORTID}\")\nendif()\n\n#-----------------------------------------------------------------------------\n# Installation preparation\n#\n# These should be set before any MITK install macros are used\n#-----------------------------------------------------------------------------\n\n# on macOS all BlueBerry plugins get copied into every\n# application bundle (.app directory) specified here\nif(MITK_USE_BLUEBERRY AND APPLE)\n\n  foreach(MITK_EXTENSION_DIR ${MITK_DIR_PLUS_EXTENSION_DIRS})\n    set(MITK_APPLICATIONS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Applications\")\n    if(EXISTS \"${MITK_APPLICATIONS_EXTENSION_DIR}/AppList.cmake\")\n      set(MITK_APPS \"\")\n      include(\"${MITK_APPLICATIONS_EXTENSION_DIR}/AppList.cmake\")\n      foreach(mitk_app ${MITK_APPS})\n        # extract option_name\n        string(REPLACE \"^^\" \"\\\\;\" target_info ${mitk_app})\n        set(target_info_list ${target_info})\n        list(GET target_info_list 1 option_name)\n        list(GET target_info_list 0 app_name)\n        # check if the application is enabled\n        if(${option_name} OR MITK_BUILD_ALL_APPS)\n          set(MACOSX_BUNDLE_NAMES ${MACOSX_BUNDLE_NAMES} Mitk${app_name})\n        endif()\n      endforeach()\n    endif()\n  endforeach()\n\nendif()\n\n#-----------------------------------------------------------------------------\n# Set coverage Flags\n#-----------------------------------------------------------------------------\n\nif(WITH_COVERAGE)\n  if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n    set(coverage_flags \"-g -fprofile-arcs -ftest-coverage -O0 -DNDEBUG\")\n    set(COVERAGE_CXX_FLAGS ${coverage_flags})\n    set(COVERAGE_C_FLAGS ${coverage_flags})\n  endif()\nendif()\n\n#-----------------------------------------------------------------------------\n# MITK C/CXX Flags\n#-----------------------------------------------------------------------------\n\nset(MITK_C_FLAGS \"${COVERAGE_C_FLAGS}\")\nset(MITK_C_FLAGS_DEBUG )\nset(MITK_C_FLAGS_RELEASE )\nset(MITK_CXX_FLAGS \"${COVERAGE_CXX_FLAGS} ${MITK_CXX${MITK_CXX_STANDARD}_FLAG}\")\nset(MITK_CXX_FLAGS_DEBUG )\nset(MITK_CXX_FLAGS_RELEASE )\n\nset(MITK_EXE_LINKER_FLAGS )\nset(MITK_SHARED_LINKER_FLAGS )\n\nif(WIN32)\n  set(MITK_CXX_FLAGS \"${MITK_CXX_FLAGS} -DWIN32_LEAN_AND_MEAN -DNOMINMAX\")\n  mitkFunctionCheckCompilerFlags(\"/wd4005\" MITK_CXX_FLAGS) # warning C4005: macro redefinition\n  mitkFunctionCheckCompilerFlags(\"/wd4231\" MITK_CXX_FLAGS) # warning C4231: nonstandard extension used : 'extern' before template explicit instantiation\n  # the following line should be removed after fixing bug 17637\n  mitkFunctionCheckCompilerFlags(\"/wd4316\" MITK_CXX_FLAGS) # warning C4316: object alignment on heap\n  mitkFunctionCheckCompilerFlags(\"/wd4180\" MITK_CXX_FLAGS) # warning C4180: qualifier applied to function type has no meaning\n  mitkFunctionCheckCompilerFlags(\"/wd4251\" MITK_CXX_FLAGS) # warning C4251: 'identifier' : class 'type' needs to have dll-interface to be used by clients of class 'type2'\nendif()\n\nif(APPLE)\n  set(MITK_CXX_FLAGS \"${MITK_CXX_FLAGS} -DGL_SILENCE_DEPRECATION\") # Apple deprecated OpenGL in macOS 10.14\nendif()\n\nif(NOT MSVC_VERSION)\n  foreach(_flag\n    -Wall\n    -Wextra\n    -Wpointer-arith\n    -Winvalid-pch\n    -Wcast-align\n    -Wwrite-strings\n    -Wno-error=gnu\n    -Wno-error=unknown-pragmas\n    # The strict-overflow warning is generated by ITK template code\n    -Wno-error=strict-overflow\n    -Woverloaded-virtual\n    -Wstrict-null-sentinel\n    #-Wold-style-cast\n    #-Wsign-promo\n    -Wno-deprecated-copy\n    -Wno-array-bounds\n    -Wno-cast-function-type\n    -Wno-maybe-uninitialized\n    -Wno-error=stringop-overread\n    -fdiagnostics-show-option\n    )\n    mitkFunctionCheckCAndCXXCompilerFlags(${_flag} MITK_C_FLAGS MITK_CXX_FLAGS)\n  endforeach()\nendif()\n\nif(CMAKE_COMPILER_IS_GNUCXX AND NOT APPLE)\n  mitkFunctionCheckCompilerFlags(\"-Wl,--no-undefined\" MITK_SHARED_LINKER_FLAGS)\n  mitkFunctionCheckCompilerFlags(\"-Wl,--as-needed\" MITK_SHARED_LINKER_FLAGS)\nendif()\n\nif(CMAKE_COMPILER_IS_GNUCXX)\n  mitkFunctionCheckCAndCXXCompilerFlags(\"-fstack-protector-all\" MITK_C_FLAGS MITK_CXX_FLAGS)\n  set(MITK_CXX_FLAGS_RELEASE \"-U_FORTIFY_SOURCES -D_FORTIFY_SOURCE=2 ${MITK_CXX_FLAGS_RELEASE}\")\nendif()\n\nset(MITK_MODULE_LINKER_FLAGS ${MITK_SHARED_LINKER_FLAGS})\nset(MITK_EXE_LINKER_FLAGS ${MITK_SHARED_LINKER_FLAGS})\n\n#-----------------------------------------------------------------------------\n# MITK Packages\n#-----------------------------------------------------------------------------\n\nset(MITK_MODULES_PACKAGE_DEPENDS_DIR ${MITK_SOURCE_DIR}/CMake/PackageDepends)\nset(MODULES_PACKAGE_DEPENDS_DIRS ${MITK_MODULES_PACKAGE_DEPENDS_DIR})\n\nforeach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS})\n  set(MITK_PACKAGE_DEPENDS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/CMake/PackageDepends\")\n  if(EXISTS \"${MITK_PACKAGE_DEPENDS_EXTENSION_DIR}\")\n    list(APPEND MODULES_PACKAGE_DEPENDS_DIRS \"${MITK_PACKAGE_DEPENDS_EXTENSION_DIR}\")\n  endif()\nendforeach()\n\nif(NOT MITK_USE_SYSTEM_Boost)\n  set(Boost_NO_SYSTEM_PATHS 1)\nendif()\n\nset(Boost_USE_MULTITHREADED 1)\nset(Boost_USE_STATIC_LIBS 0)\nset(Boost_USE_STATIC_RUNTIME 0)\nset(Boost_ADDITIONAL_VERSIONS 1.74 1.74.0)\n\n# We need this later for a DCMTK workaround\nset(_dcmtk_dir_orig ${DCMTK_DIR})\n\n# This property is populated at the top half of this file\nget_property(MITK_EXTERNAL_PROJECTS GLOBAL PROPERTY MITK_EXTERNAL_PROJECTS)\nforeach(ep ${MITK_EXTERNAL_PROJECTS})\n  get_property(_package GLOBAL PROPERTY MITK_${ep}_PACKAGE)\n  get_property(_components GLOBAL PROPERTY MITK_${ep}_COMPONENTS)\n  if(MITK_USE_${ep} AND _package)\n    if(_components)\n      find_package(${_package} COMPONENTS ${_components} REQUIRED CONFIG)\n    else()\n      # Prefer config mode first because it finds external\n      # <proj>Config.cmake files pointed at by <proj>_DIR variables.\n      # Otherwise, existing Find<proj>.cmake files could fail.\n\n      if(DEFINED ${_package}_DIR)\n        #we store the information because it will be overwritten by find_package\n        #and would get lost for all EPs that use on Find<proj>.cmake instead of config\n        #files.\n        set(_temp_EP_${_package}_dir ${${_package}_DIR})\n      endif(DEFINED ${_package}_DIR)\n\n      find_package(${_package} QUIET CONFIG)\n      string(TOUPPER \"${_package}\" _package_uc)\n      if(NOT (${_package}_FOUND OR ${_package_uc}_FOUND))\n        if(DEFINED _temp_EP_${_package}_dir)\n            set(${_package}_DIR ${_temp_EP_${_package}_dir} CACHE PATH \"externally set dir of the package ${_package}\" FORCE)\n        endif(DEFINED _temp_EP_${_package}_dir)\n\n        find_package(${_package} REQUIRED)\n      endif()\n    endif()\n  endif()\nendforeach()\n\n# Ensure that the MITK CMake module path comes first\nset(CMAKE_MODULE_PATH\n  ${MITK_CMAKE_DIR}\n  ${CMAKE_MODULE_PATH}\n  )\n\nif(MITK_USE_DCMTK)\n  if(${_dcmtk_dir_orig} MATCHES \"${MITK_EXTERNAL_PROJECT_PREFIX}.*\")\n    # Help our FindDCMTK.cmake script find our super-build DCMTK\n    set(DCMTK_DIR ${MITK_EXTERNAL_PROJECT_PREFIX})\n  else()\n    # Use the original value\n    set(DCMTK_DIR ${_dcmtk_dir_orig})\n  endif()\nendif()\n\nif(MITK_USE_DCMQI)\n  # Due to the preferred CONFIG mode in find_package calls above,\n  # the DCMQIConfig.cmake file is read, which does not provide useful\n  # package information. We explicitly need MODULE mode to find DCMQI.\n    # Help our FindDCMQI.cmake script find our super-build DCMQI\n  set(DCMQI_DIR ${MITK_EXTERNAL_PROJECT_PREFIX})\n  find_package(DCMQI REQUIRED)\nendif()\n\nif(MITK_USE_OpenMP)\n  find_package(OpenMP REQUIRED COMPONENTS CXX)\nelse()\n  find_package(OpenMP QUIET COMPONENTS CXX)\n\n  if(OpenMP_FOUND)\n    set(MITK_USE_OpenMP ON CACHE BOOL \"\" FORCE)\n  elseif(APPLE AND OpenMP_libomp_LIBRARY AND NOT OpenMP_CXX_LIB_NAMES)\n    set(OpenMP_CXX_LIB_NAMES libomp CACHE STRING \"\" FORCE)\n    get_filename_component(openmp_lib_dir \"${OpenMP_libomp_LIBRARY}\" DIRECTORY)\n    set(openmp_include_dir \"${openmp_lib_dir}/../include\")\n    if(EXISTS \"${openmp_include_dir}\")\n      get_filename_component(openmp_include_dir \"${openmp_include_dir}\" REALPATH)\n      set(OpenMP_CXX_FLAGS \"-Xpreprocessor -fopenmp -I${openmp_include_dir}\" CACHE STRING \"\" FORCE)\n      find_package(OpenMP QUIET COMPONENTS CXX)\n      if(OpenMP_FOUND)\n        set(MITK_USE_OpenMP ON CACHE BOOL \"\" FORCE)\n      endif()\n    endif()\n  endif()\nendif()\n\n# Qt support\nif(MITK_USE_Qt6)\n  if(MITK_USE_BLUEBERRY)\n    option(BLUEBERRY_USE_QT_HELP \"Enable support for integrating plugin documentation into Qt Help\" ${DOXYGEN_FOUND})\n    mark_as_advanced(BLUEBERRY_USE_QT_HELP)\n\n    # Sanity checks for in-application BlueBerry plug-in help generation\n    if(BLUEBERRY_USE_QT_HELP)\n      set(_force_blueberry_use_qt_help_to_off 0)\n      if(NOT DOXYGEN_FOUND)\n        message(\"> Forcing BLUEBERRY_USE_QT_HELP to OFF because Doxygen was not found.\")\n        set(_force_blueberry_use_qt_help_to_off 1)\n      endif()\n      if(DOXYGEN_FOUND AND DOXYGEN_VERSION VERSION_LESS 1.8.7)\n        message(\"> Forcing BLUEBERRY_USE_QT_HELP to OFF because Doxygen version 1.8.7 or newer not found.\")\n        set(_force_blueberry_use_qt_help_to_off 1)\n      endif()\n\n      if(NOT QT_HELPGENERATOR_EXECUTABLE)\n        message(\"> Forcing BLUEBERRY_USE_QT_HELP to OFF because QT_HELPGENERATOR_EXECUTABLE is empty.\")\n        set(_force_blueberry_use_qt_help_to_off 1)\n      endif()\n\n      if(NOT MITK_USE_Qt6)\n        message(\"> Forcing BLUEBERRY_USE_QT_HELP to OFF because MITK_USE_Qt6 is OFF.\")\n        set(_force_blueberry_use_qt_help_to_off 1)\n      endif()\n\n      if(_force_blueberry_use_qt_help_to_off)\n        set(BLUEBERRY_USE_QT_HELP OFF CACHE BOOL \"Enable support for integrating plugin documentation into Qt Help\" FORCE)\n      endif()\n    endif()\n\n    if(BLUEBERRY_QT_HELP_REQUIRED AND NOT BLUEBERRY_USE_QT_HELP)\n      message(FATAL_ERROR \"BLUEBERRY_USE_QT_HELP is required to be set to ON\")\n    endif()\n  endif()\n\nendif()\n\n#-----------------------------------------------------------------------------\n# Testing\n#-----------------------------------------------------------------------------\n\nif(BUILD_TESTING)\n  # Configuration for the CMake-generated test driver\n  set(CMAKE_TESTDRIVER_EXTRA_INCLUDES \"#include <stdexcept>\")\n  set(CMAKE_TESTDRIVER_BEFORE_TESTMAIN \"\n    try\n      {\")\n  set(CMAKE_TESTDRIVER_AFTER_TESTMAIN \"\n      }\n      catch (const std::exception& e)\n      {\n        fprintf(stderr, \\\"%s\\\\n\\\", e.what());\n        return EXIT_FAILURE;\n      }\n      catch (...)\n      {\n        printf(\\\"Exception caught in the test driver\\\\n\\\");\n        return EXIT_FAILURE;\n      }\")\n\n  set(MITK_TEST_OUTPUT_DIR \"${MITK_BINARY_DIR}/test_output\")\n  if(NOT EXISTS ${MITK_TEST_OUTPUT_DIR})\n    file(MAKE_DIRECTORY ${MITK_TEST_OUTPUT_DIR})\n  endif()\n\n  # Test the package target\n  include(mitkPackageTest)\nendif()\n\nconfigure_file(mitkTestingConfig.h.in ${MITK_BINARY_DIR}/mitkTestingConfig.h)\n\n#-----------------------------------------------------------------------------\n# MITK_SUPERBUILD_BINARY_DIR\n#-----------------------------------------------------------------------------\n\n# If MITK_SUPERBUILD_BINARY_DIR isn't defined, it means MITK is *NOT* build using Superbuild.\n# In that specific case, MITK_SUPERBUILD_BINARY_DIR should default to MITK_BINARY_DIR\nif(NOT DEFINED MITK_SUPERBUILD_BINARY_DIR)\n  set(MITK_SUPERBUILD_BINARY_DIR ${MITK_BINARY_DIR})\nendif()\n\n#-----------------------------------------------------------------------------\n# Set C/CXX and linker flags for MITK code\n#-----------------------------------------------------------------------------\n\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${MITK_CXX_FLAGS}\")\nset(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} ${MITK_CXX_FLAGS_DEBUG}\")\nset(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} ${MITK_CXX_FLAGS_RELEASE}\")\nset(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${MITK_C_FLAGS}\")\nset(CMAKE_C_FLAGS_DEBUG \"${CMAKE_C_FLAGS_DEBUG} ${MITK_C_FLAGS_DEBUG}\")\nset(CMAKE_C_FLAGS_RELEASE \"${CMAKE_C_FLAGS_RELEASE} ${MITK_C_FLAGS_RELEASE}\")\n\nset(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${MITK_EXE_LINKER_FLAGS}\")\nset(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} ${MITK_SHARED_LINKER_FLAGS}\")\nset(CMAKE_MODULE_LINKER_FLAGS \"${CMAKE_MODULE_LINKER_FLAGS} ${MITK_MODULE_LINKER_FLAGS}\")\n\n#-----------------------------------------------------------------------------\n# Add subdirectories\n#-----------------------------------------------------------------------------\n\nadd_subdirectory(Utilities)\n\nadd_subdirectory(Modules)\n\ninclude(\"${CMAKE_CURRENT_SOURCE_DIR}/Modules/ModuleList.cmake\")\nmitkFunctionWhitelistModules(MITK MITK_MODULES)\n\nset(MITK_ROOT_FOLDER_BACKUP \"${MITK_ROOT_FOLDER}\")\nforeach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS})\n  get_filename_component(MITK_ROOT_FOLDER \"${MITK_EXTENSION_DIR}\" NAME)\n  set(MITK_MODULES_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Modules\")\n  if(EXISTS \"${MITK_MODULES_EXTENSION_DIR}/ModuleList.cmake\")\n    set(MITK_MODULES \"\")\n    include(\"${MITK_MODULES_EXTENSION_DIR}/ModuleList.cmake\")\n    foreach(mitk_module ${MITK_MODULES})\n      add_subdirectory(\"${MITK_MODULES_EXTENSION_DIR}/${mitk_module}\" \"Modules/${mitk_module}\")\n    endforeach()\n  endif()\n  set(MITK_MODULE_NAME_PREFIX ${MITK_DEFAULT_MODULE_NAME_PREFIX})\nendforeach()\nset(MITK_ROOT_FOLDER \"${MITK_ROOT_FOLDER_BACKUP}\")\n\nadd_subdirectory(Wrapping)\n\nset(MITK_DOXYGEN_OUTPUT_DIR \"${PROJECT_BINARY_DIR}/Documentation/Doxygen\" CACHE PATH\n  \"Output directory for doxygen generated documentation.\")\n\nif(MITK_USE_BLUEBERRY)\n  include(\"${CMAKE_CURRENT_SOURCE_DIR}/Plugins/PluginList.cmake\")\n  mitkFunctionWhitelistPlugins(MITK MITK_PLUGINS)\n\n  set(mitk_plugins_fullpath \"\")\n  foreach(mitk_plugin ${MITK_PLUGINS})\n    list(APPEND mitk_plugins_fullpath Plugins/${mitk_plugin})\n  endforeach()\n\n  set(MITK_PLUGIN_REGEX_LIST \"\")\n  foreach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS})\n    set(MITK_PLUGINS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Plugins\")\n    if(EXISTS \"${MITK_PLUGINS_EXTENSION_DIR}/PluginList.cmake\")\n      set(MITK_PLUGINS \"\")\n      include(\"${MITK_PLUGINS_EXTENSION_DIR}/PluginList.cmake\")\n      foreach(mitk_plugin ${MITK_PLUGINS})\n        list(APPEND mitk_plugins_fullpath \"${MITK_PLUGINS_EXTENSION_DIR}/${mitk_plugin}\")\n      endforeach()\n    endif()\n  endforeach()\n\n  if(EXISTS ${MITK_PRIVATE_MODULES}/PluginList.cmake)\n    include(${MITK_PRIVATE_MODULES}/PluginList.cmake)\n\n    foreach(mitk_plugin ${MITK_PRIVATE_PLUGINS})\n      list(APPEND mitk_plugins_fullpath ${MITK_PRIVATE_MODULES}/${mitk_plugin})\n    endforeach()\n  endif()\n\n  if(MITK_BUILD_EXAMPLES)\n    include(\"${CMAKE_CURRENT_SOURCE_DIR}/Examples/Plugins/PluginList.cmake\")\n    set(mitk_example_plugins_fullpath )\n    foreach(mitk_example_plugin ${MITK_EXAMPLE_PLUGINS})\n      list(APPEND mitk_example_plugins_fullpath Examples/Plugins/${mitk_example_plugin})\n      list(APPEND mitk_plugins_fullpath Examples/Plugins/${mitk_example_plugin})\n    endforeach()\n  endif()\n\n  # Specify which plug-ins belong to this project\n  macro(GetMyTargetLibraries all_target_libraries varname)\n    set(re_ctkplugin_mitk \"^org_mitk_[a-zA-Z0-9_]+$\")\n    set(re_ctkplugin_bb \"^org_blueberry_[a-zA-Z0-9_]+$\")\n    set(_tmp_list)\n    list(APPEND _tmp_list ${all_target_libraries})\n    ctkMacroListFilter(_tmp_list re_ctkplugin_mitk re_ctkplugin_bb MITK_PLUGIN_REGEX_LIST OUTPUT_VARIABLE ${varname})\n  endmacro()\n\n  # Get infos about application directories and build options\n  set(mitk_apps_fullpath \"\")\n  foreach(MITK_EXTENSION_DIR ${MITK_DIR_PLUS_EXTENSION_DIRS})\n    set(MITK_APPLICATIONS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Applications\")\n    if(EXISTS \"${MITK_APPLICATIONS_EXTENSION_DIR}/AppList.cmake\")\n      set(MITK_APPS \"\")\n      include(\"${MITK_APPLICATIONS_EXTENSION_DIR}/AppList.cmake\")\n      foreach(mitk_app ${MITK_APPS})\n        # extract option_name\n        string(REPLACE \"^^\" \"\\\\;\" target_info ${mitk_app})\n        set(target_info_list ${target_info})\n        list(GET target_info_list 0 directory_name)\n        list(GET target_info_list 1 option_name)\n        if(${option_name})\n          list(APPEND mitk_apps_fullpath \"${MITK_APPLICATIONS_EXTENSION_DIR}/${directory_name}^^${option_name}\")\n        endif()\n      endforeach()\n    endif()\n  endforeach()\n\n  if (mitk_plugins_fullpath)\n    ctkMacroSetupPlugins(${mitk_plugins_fullpath}\n                         BUILD_OPTION_PREFIX MITK_BUILD_\n                         APPS ${mitk_apps_fullpath}\n                         BUILD_ALL ${MITK_BUILD_ALL_PLUGINS}\n                         COMPACT_OPTIONS)\n  endif()\n\n  set(MITK_PLUGIN_USE_FILE \"${MITK_BINARY_DIR}/MitkPluginUseFile.cmake\")\n  if(${PROJECT_NAME}_PLUGIN_LIBRARIES)\n    ctkFunctionGeneratePluginUseFile(${MITK_PLUGIN_USE_FILE})\n  else()\n    file(REMOVE ${MITK_PLUGIN_USE_FILE})\n    set(MITK_PLUGIN_USE_FILE )\n  endif()\nendif()\n\n#-----------------------------------------------------------------------------\n# Documentation\n#-----------------------------------------------------------------------------\n\nset(MITK_DOXYGEN_ADDITIONAL_INPUT_DIRS)\nset(MITK_DOXYGEN_ADDITIONAL_IMAGE_PATHS)\nset(MITK_DOXYGEN_ADDITIONAL_EXAMPLE_PATHS)\n\nforeach(MITK_EXTENSION_DIR ${MITK_DIR_PLUS_EXTENSION_DIRS})\n  set(MITK_DOXYGEN_ADDITIONAL_INPUT_DIRS \"${MITK_DOXYGEN_ADDITIONAL_INPUT_DIRS} \\\"${MITK_EXTENSION_DIR}\\\"\")\n  set(MITK_DOXYGEN_ADDITIONAL_IMAGE_PATHS \"${MITK_DOXYGEN_ADDITIONAL_IMAGE_PATHS} \\\"${MITK_EXTENSION_DIR}\\\"\")\n  # MITK_DOXYGEN_ADDITIONAL_EXAMPLE_PATHS should be modified by MITK extensions as needed\nendforeach()\n\nif(DOXYGEN_FOUND)\n  foreach(MITK_EXTENSION_DIR ${MITK_DIR_PLUS_EXTENSION_DIRS})\n    set(MITK_DOCUMENTATION_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Documentation\")\n    file(GLOB MITK_DOCUMENTATION_EXTENSION_FILES CONFIGURE_DEPENDS \"${MITK_DOCUMENTATION_EXTENSION_DIR}/*.cmake\")\n    foreach(doc_file ${MITK_DOCUMENTATION_EXTENSION_FILES})\n      include(\"${doc_file}\")\n    endforeach()\n  endforeach()\n\n  # Transform list of example paths into space-separated string of quoted paths\n  unset(example_paths)\n  foreach(example_path ${MITK_DOXYGEN_ADDITIONAL_EXAMPLE_PATHS})\n    set(example_paths \"${example_paths} \\\"${example_path}\\\"\")\n  endforeach()\n  set(MITK_DOXYGEN_ADDITIONAL_EXAMPLE_PATHS \"${example_paths}\")\n\n  add_subdirectory(Documentation)\nendif()\n\n#-----------------------------------------------------------------------------\n# Installation\n#-----------------------------------------------------------------------------\n\n\n# set MITK cpack variables\n# These are the default variables, which can be overwritten ( see below )\ninclude(mitkSetupCPack)\n\nset(use_default_config ON)\n\nset(ALL_MITK_APPS \"\")\nset(activated_apps_no 0)\n\nforeach(MITK_EXTENSION_DIR ${MITK_DIR_PLUS_EXTENSION_DIRS})\n  set(MITK_APPLICATIONS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Applications\")\n  if(EXISTS \"${MITK_APPLICATIONS_EXTENSION_DIR}/AppList.cmake\")\n    set(MITK_APPS \"\")\n    include(\"${MITK_APPLICATIONS_EXTENSION_DIR}/AppList.cmake\")\n    foreach(mitk_app ${MITK_APPS})\n      string(REPLACE \"^^\" \"\\\\;\" target_info ${mitk_app})\n      set(target_info_list ${target_info})\n      list(GET target_info_list 0 directory_name)\n      list(GET target_info_list 1 option_name)\n      list(GET target_info_list 2 executable_name)\n      list(APPEND ALL_MITK_APPS \"${MITK_EXTENSION_DIR}/Applications/${directory_name}^^${option_name}^^${executable_name}\")\n      if(${option_name} OR MITK_BUILD_ALL_APPS)\n        MATH(EXPR activated_apps_no \"${activated_apps_no} + 1\")\n      endif()\n    endforeach()\n  endif()\nendforeach()\n\nlist(LENGTH ALL_MITK_APPS app_count)\n\nif(app_count EQUAL 1 AND (activated_apps_no EQUAL 1 OR MITK_BUILD_ALL_APPS))\n  # Corner case if there is only one app in total\n  set(use_project_cpack ON)\nelseif(activated_apps_no EQUAL 1 AND NOT MITK_BUILD_ALL_APPS)\n  # Only one app is enabled (no \"build all\" flag set)\n  set(use_project_cpack ON)\nelse()\n  # Less or more then one app is enabled\n  set(use_project_cpack OFF)\nendif()\n\nforeach(mitk_app ${ALL_MITK_APPS})\n  # extract target_dir and option_name\n  string(REPLACE \"^^\" \"\\\\;\" target_info ${mitk_app})\n  set(target_info_list ${target_info})\n  list(GET target_info_list 0 target_dir)\n  list(GET target_info_list 1 option_name)\n  list(GET target_info_list 2 executable_name)\n  # check if the application is enabled\n  if(${option_name} OR MITK_BUILD_ALL_APPS)\n    # check whether application specific configuration files will be used\n    if(use_project_cpack)\n      # use files if they exist\n      if(EXISTS \"${target_dir}/CPackOptions.cmake\")\n        include(\"${target_dir}/CPackOptions.cmake\")\n      endif()\n\n      if(EXISTS \"${target_dir}/CPackConfig.cmake.in\")\n        set(CPACK_PROJECT_CONFIG_FILE \"${target_dir}/CPackConfig.cmake\")\n        configure_file(${target_dir}/CPackConfig.cmake.in\n                       ${CPACK_PROJECT_CONFIG_FILE} @ONLY)\n        set(use_default_config OFF)\n      endif()\n    endif()\n  # add link to the list\n  list(APPEND CPACK_CREATE_DESKTOP_LINKS \"${executable_name}\")\n  endif()\nendforeach()\n\n# if no application specific configuration file was used, use default\nif(use_default_config)\n  configure_file(${MITK_SOURCE_DIR}/MITKCPackOptions.cmake.in\n                 ${MITK_BINARY_DIR}/MITKCPackOptions.cmake @ONLY)\n  set(CPACK_PROJECT_CONFIG_FILE \"${MITK_BINARY_DIR}/MITKCPackOptions.cmake\")\nendif()\n\n# include CPack model once all variables are set\ninclude(CPack)\n\n# Additional installation rules\ninclude(mitkInstallRules)\n\n#-----------------------------------------------------------------------------\n# Last configuration steps\n#-----------------------------------------------------------------------------\n\n# ---------------- Export targets -----------------\n\nset(MITK_EXPORTS_FILE \"${MITK_BINARY_DIR}/MitkExports.cmake\")\nfile(REMOVE ${MITK_EXPORTS_FILE})\n\nset(targets_to_export)\nget_property(module_targets GLOBAL PROPERTY MITK_MODULE_TARGETS)\nif(module_targets)\n  list(APPEND targets_to_export ${module_targets})\nendif()\n\nif(MITK_USE_BLUEBERRY)\n  if(MITK_PLUGIN_LIBRARIES)\n    list(APPEND targets_to_export ${MITK_PLUGIN_LIBRARIES})\n  endif()\nendif()\n\nexport(TARGETS ${targets_to_export} APPEND\n       FILE ${MITK_EXPORTS_FILE})\n\nset(MITK_EXPORTED_TARGET_PROPERTIES )\nforeach(target_to_export ${targets_to_export})\n  get_target_property(autoload_targets ${target_to_export} MITK_AUTOLOAD_TARGETS)\n  if(autoload_targets)\n    set(MITK_EXPORTED_TARGET_PROPERTIES \"${MITK_EXPORTED_TARGET_PROPERTIES}\nset_target_properties(${target_to_export} PROPERTIES MITK_AUTOLOAD_TARGETS \\\"${autoload_targets}\\\")\")\n  endif()\n  get_target_property(autoload_dir ${target_to_export} MITK_AUTOLOAD_DIRECTORY)\n  if(autoload_dir)\n    set(MITK_EXPORTED_TARGET_PROPERTIES \"${MITK_EXPORTED_TARGET_PROPERTIES}\nset_target_properties(${target_to_export} PROPERTIES MITK_AUTOLOAD_DIRECTORY \\\"${autoload_dir}\\\")\")\n  endif()\n\n  get_target_property(deprecated_module ${target_to_export} MITK_MODULE_DEPRECATED_SINCE)\n  if(deprecated_module)\n    set(MITK_EXPORTED_TARGET_PROPERTIES \"${MITK_EXPORTED_TARGET_PROPERTIES}\nset_target_properties(${target_to_export} PROPERTIES MITK_MODULE_DEPRECATED_SINCE \\\"${deprecated_module}\\\")\")\n  endif()\nendforeach()\n\n# ---------------- External projects -----------------\n\nget_property(MITK_ADDITIONAL_LIBRARY_SEARCH_PATHS_CONFIG GLOBAL PROPERTY MITK_ADDITIONAL_LIBRARY_SEARCH_PATHS)\n\nset(MITK_CONFIG_EXTERNAL_PROJECTS )\n#string(REPLACE \"^^\" \";\" _mitk_external_projects ${MITK_EXTERNAL_PROJECTS})\n\nforeach(ep ${MITK_EXTERNAL_PROJECTS})\n  get_property(_components GLOBAL PROPERTY MITK_${ep}_COMPONENTS)\n  set(MITK_CONFIG_EXTERNAL_PROJECTS \"${MITK_CONFIG_EXTERNAL_PROJECTS}\nset(MITK_USE_${ep} ${MITK_USE_${ep}})\nset(MITK_${ep}_DIR \\\"${${ep}_DIR}\\\")\nset(MITK_${ep}_COMPONENTS ${_components})\n\")\nendforeach()\n\nforeach(ep ${MITK_EXTERNAL_PROJECTS})\n  get_property(_package GLOBAL PROPERTY MITK_${ep}_PACKAGE)\n  get_property(_components GLOBAL PROPERTY MITK_${ep}_COMPONENTS)\n  if(_components)\n      set(_components_arg COMPONENTS \\${_components})\n  else()\n    set(_components_arg)\n  endif()\n\n  if(_package)\n    set(MITK_CONFIG_EXTERNAL_PROJECTS \"${MITK_CONFIG_EXTERNAL_PROJECTS}\nif(MITK_USE_${ep})\n  set(${ep}_DIR \\${MITK_${ep}_DIR})\n  if(MITK_${ep}_COMPONENTS)\n    mitkMacroFindDependency(${_package} COMPONENTS \\${MITK_${ep}_COMPONENTS})\n  else()\n    mitkMacroFindDependency(${_package})\n  endif()\nendif()\")\n  endif()\nendforeach()\n\n\n# ---------------- Tools -----------------\n\nconfigure_file(${MITK_SOURCE_DIR}/CMake/ToolExtensionITKFactory.cpp.in\n               ${MITK_BINARY_DIR}/ToolExtensionITKFactory.cpp.in COPYONLY)\nconfigure_file(${MITK_SOURCE_DIR}/CMake/ToolExtensionITKFactoryLoader.cpp.in\n               ${MITK_BINARY_DIR}/ToolExtensionITKFactoryLoader.cpp.in COPYONLY)\nconfigure_file(${MITK_SOURCE_DIR}/CMake/ToolGUIExtensionITKFactory.cpp.in\n               ${MITK_BINARY_DIR}/ToolGUIExtensionITKFactory.cpp.in COPYONLY)\n\n# ---------------- Configure files -----------------\n\nconfigure_file(mitkVersion.h.in ${MITK_BINARY_DIR}/mitkVersion.h)\nconfigure_file(mitkConfig.h.in ${MITK_BINARY_DIR}/mitkConfig.h)\n\nset(IPFUNC_INCLUDE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/Utilities/ipFunc)\nset(UTILITIES_DIR ${CMAKE_CURRENT_SOURCE_DIR}/Utilities)\n\nconfigure_file(mitkConfig.h.in ${MITK_BINARY_DIR}/mitkConfig.h)\nconfigure_file(MITKConfig.cmake.in ${MITK_BINARY_DIR}/MITKConfig.cmake @ONLY)\n\nwrite_basic_config_version_file(${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}ConfigVersion.cmake\n  VERSION ${MITK_VERSION_STRING} COMPATIBILITY AnyNewerVersion)\n\n#-----------------------------------------------------------------------------\n# MITK Applications\n#-----------------------------------------------------------------------------\n\n# This must come after MITKConfig.h was generated, since applications\n# might do a find_package(MITK REQUIRED).\nadd_subdirectory(Applications)\n\nif(MSVC AND TARGET MitkWorkbench)\n  set_directory_properties(PROPERTIES VS_STARTUP_PROJECT MitkWorkbench)\nendif()\n\nforeach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS})\n  set(MITK_APPLICATIONS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Applications\")\n  if(EXISTS \"${MITK_APPLICATIONS_EXTENSION_DIR}/CMakeLists.txt\")\n    add_subdirectory(\"${MITK_APPLICATIONS_EXTENSION_DIR}\" \"Applications\")\n  endif()\nendforeach()\n\n#-----------------------------------------------------------------------------\n# MITK Examples\n#-----------------------------------------------------------------------------\n\nif(MITK_BUILD_EXAMPLES)\n  # This must come after MITKConfig.h was generated, since applications\n  # might do a find_package(MITK REQUIRED).\n  add_subdirectory(Examples)\nendif()\n\n#-----------------------------------------------------------------------------\n# Print configuration summary\n#-----------------------------------------------------------------------------\n\nmessage(\"\\n\\n\")\nfeature_summary(\n  DESCRIPTION \"------- FEATURE SUMMARY FOR ${PROJECT_NAME} -------\"\n  WHAT ALL\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/membrain-v2",
            "repo_link": "https://github.com/teamtomo/membrain-seg",
            "content": {
                "codemeta": "",
                "readme": "# MemBrain-Seg\n\n[![License](https://img.shields.io/pypi/l/membrain-seg.svg?color=green)](https://github.com/teamtomo/membrain-seg/raw/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/membrain-seg.svg?color=green)](https://pypi.org/project/membrain-seg)\n[![Python Version](https://img.shields.io/pypi/pyversions/membrain-seg.svg?color=green)](https://python.org)\n[![CI](https://github.com/teamtomo/membrain-seg/actions/workflows/ci.yml/badge.svg)](https://github.com/teamtomo/membrain-seg/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/gh/teamtomo/membrain-seg/branch/main/graph/badge.svg)](https://codecov.io/gh/teamtomo/membrain-seg)\n\n\nMembrain-Seg<sup>1</sup> is a Python project developed by [teamtomo](https://github.com/teamtomo) for membrane segmentation in 3D for cryo-electron tomography (cryo-ET). This tool aims to provide researchers with an efficient and reliable method for segmenting membranes in 3D microscopic images. Membrain-Seg is currently under early development, so we may make breaking changes between releases.\n\n## Publication: \nMembrain-seg's current functionalities are described on more detail in our [preprint](https://www.biorxiv.org/content/10.1101/2024.01.05.574336v1).\n\n\n<p align=\"center\" width=\"100%\">\n    <img width=\"100%\" src=\"https://user-images.githubusercontent.com/34575029/248259282-ee622267-77fa-4c88-ad38-ad0cfd76b810.png\">\n</p>\n\nMembrain-Seg is currently under early development, so we may make breaking changes between releases.\n\n# Version Updates\nFor a detailed history of changes and updates, please refer to our [CHANGELOG.md](./CHANGELOG.md).\n\n\n# Overview\nMemBrain-seg is a practical tool for membrane segmentation in cryo-electron tomograms. It's built on the U-Net architecture and makes use of a pre-trained model for efficient performance.\nThe U-Net architecture and training parameters are largely inspired by nnUNet<sup>2</sup>.\n\n\nOur current best model is available for download [here](https://drive.google.com/file/d/1tSQIz_UCsQZNfyHg0RxD-4meFgolszo8/view?usp=sharing). Please let us know how it works for you.\nIf the given model does not work properly, you may want to try one of our previous versions:\n\nOther (older) model versions:\n- [v9 -- best model until 10th Aug 2023](https://drive.google.com/file/d/15ZL5Ao7EnPwMHa8yq5CIkanuNyENrDeK/view?usp=sharing)\n- [v9b -- model for non-denoised data until 10th Aug 2023](https://drive.google.com/file/d/1TGpQ1WyLHgXQIdZ8w4KFZo_Kkoj0vIt7/view?usp=sharing)\n\nIf you wish, you can also train a new model using your own data, or combine it with our (soon to come!) publicly-available dataset. \n\nTo enhance segmentation, MemBrain-seg includes preprocessing functions. These help to adjust your tomograms so they're similar to the data our network was trained on, making the process smoother and more efficient.\n\nExplore MemBrain-seg, use it for your needs, and let us know how it works for you!\n\n\nPreliminary [documentation](https://teamtomo.github.io/membrain-seg/) is available, but far from perfect. Please let us know if you encounter any issues, and we are more than happy to help (and get feedback what does not work yet).\n\n```\n[1] Lamm, L., Zufferey, S., Righetto, R.D., Wietrzynski, W., Yamauchi, K.A., Burt, A., Liu, Y., Zhang, H., Martinez-Sanchez, A., Ziegler, S., Isensee, F., Schnabel, J.A., Engel, B.D., and Peng, T, 2024. MemBrain v2: an end-to-end tool for the analysis of membranes in cryo-electron tomography. bioRxiv, https://doi.org/10.1101/2024.01.05.574336\n\n[2] Isensee, F., Jaeger, P.F., Kohl, S.A.A., Petersen, J., Maier-Hein, K.H., 2021. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods 18, 203-211. https://doi.org/10.1038/s41592-020-01008-z\n```\n\n# Installation\nFor detailed installation instructions, please look [here](https://teamtomo.github.io/membrain-seg/installation/).\n\n# Features\n## Segmentation\nSegmenting the membranes in your tomograms is the main feature of this repository. \nPlease find more detailed instructions [here](https://teamtomo.github.io/membrain-seg/Usage/Segmentation/).\n\n## Preprocessing\nCurrently, we provide the following two [preprocessing](https://github.com/teamtomo/membrain-seg/tree/main/src/membrain_seg/tomo_preprocessing) options:\n- Pixel size matching: Rescale your tomogram to match the training pixel sizes\n- Fourier amplitude matching: Scale Fourier components to match the \"style\" of different tomograms\n- Deconvolution: denoises the tomogram by applying the deconvolution filter from Warp\n\nFor more information, see the [Preprocessing](https://teamtomo.github.io/membrain-seg/Usage/Preprocessing/) subsection.\n\n## Model training\nIt is also possible to use this package to train your own model. Instructions can be found [here](https://teamtomo.github.io/membrain-seg/Usage/Training/).\n\n## Patch annotations\nIn case you would like to train a model that works better for your tomograms, it may be beneficial to add some more patches from your tomograms to the training dataset. \nRecommendations on how to to this can be found [here](https://teamtomo.github.io/membrain-seg/Usage/Annotations/).\n\n\n",
                "dependencies": "# https://peps.python.org/pep-0517/\n[build-system]\nrequires = [\"hatchling\", \"hatch-vcs\"]\nbuild-backend = \"hatchling.build\"\n\n# https://peps.python.org/pep-0621/\n[project]\nname = \"membrain-seg\"\ndescription = \"membrane segmentation in 3D for cryo-ET\"\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\nlicense = { text = \"BSD 3-Clause License\" }\nauthors = [\n    { email = \"lorenz.lamm@helmholtz-munich.de\", name = \"Lorenz Lamm\" },\n]\nclassifiers = [\n    \"Development Status :: 3 - Alpha\",\n    \"License :: OSI Approved :: BSD License\",\n    \"Natural Language :: English\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Typing :: Typed\",\n]\ndynamic = [\"version\"]\ndependencies = [\n    \"imageio\",\n    \"mrcfile\",\n    \"monai\",\n    \"numpy<2.0.0\",\n    \"pandas\",\n    \"pytorch-lightning\",\n    \"scikit-image\",\n    \"scipy\",\n    \"simpleitk\",\n    \"torch\",\n    \"typer[all]\",\n]\n\n\n# extras\n# https://peps.python.org/pep-0621/#dependencies-optional-dependencies\n[project.optional-dependencies]\ntest = [\"pytest>=6.0\", \"pytest-cov\"]\ndev = [\n    \"black\",\n    \"ipython\",\n    \"pdbpp\",\n    \"pre-commit\",\n    \"pytest-cov\",\n    \"pytest\",\n    \"rich\",\n    \"ruff\",\n    \"mkdocs-material\",\n]\n\n[project.urls]\nhomepage = \"https://github.com/teamtomo/membrain-seg\"\nrepository = \"https://github.com/teamtomo/membrain-seg\"\n\n# same as console_scripts entry point\n# [project.scripts]\n# spam-cli = \"spam:main_cli\"\n[project.scripts]\ntomo_preprocessing = \"membrain_seg.tomo_preprocessing:cli\"\nmembrain = \"membrain_seg.segmentation.cli:cli\"\npatch_corrections = \"membrain_seg.annotations:cli\"\n\n\n# Entry points\n# https://peps.python.org/pep-0621/#entry-points\n# [project.entry-points.\"spam.magical\"]\n# tomatoes = \"spam:main_tomatoes\"\n\n# https://hatch.pypa.io/latest/config/metadata/\n[tool.hatch.version]\nsource = \"vcs\"\n\n# https://hatch.pypa.io/latest/config/build/#file-selection\n# [tool.hatch.build.targets.sdist]\n# include = [\"/src\", \"/tests\"]\n\n# [tool.hatch.build.targets.wheel]\n# only-include = [\"src\"]\n# sources = [\"src\"]\n\n# https://github.com/charliermarsh/ruff\n[tool.ruff]\nline-length = 88\ntarget-version = \"py38\"\n# https://beta.ruff.rs/docs/rules/\nextend-select = [\n    \"E\",    # style errors\n    \"W\",    # style warnings\n    \"F\",    # flakes\n    \"D\",    # pydocstyle\n    \"I\",    # isort\n    \"U\",    # pyupgrade\n    # \"S\",    # bandit\n    \"C\",    # flake8-comprehensions\n    \"B\",    # flake8-bugbear\n    \"A001\", # flake8-builtins\n    \"RUF\",  # ruff-specific rules\n]\n# I do this to get numpy-style docstrings AND retain\n# D417 (Missing argument descriptions in the docstring)\n# otherwise, see:\n# https://beta.ruff.rs/docs/faq/#does-ruff-support-numpy-or-google-style-docstrings\n# https://github.com/charliermarsh/ruff/issues/2606\nextend-ignore = [\n    \"D100\", # Missing docstring in public module\n    \"D107\", # Missing docstring in __init__\n    \"D203\", # 1 blank line required before class docstring\n    \"D212\", # Multi-line docstring summary should start at the first line\n    \"D213\", # Multi-line docstring summary should start at the second line\n    \"D401\", # First line should be in imperative mood\n    \"D413\", # Missing blank line after last section\n    \"D416\", # Section name should end with a colon\n]\n\n[tool.ruff.per-file-ignores]\n\"tests/*.py\" = [\"D\", \"S\"]\n\"setup.py\" = [\"D\"]\n\n# https://docs.pytest.org/en/6.2.x/customize.html\n[tool.pytest.ini_options]\nminversion = \"6.0\"\ntestpaths = [\"tests\"]\nfilterwarnings = [\"error\"]\n\n# https://mypy.readthedocs.io/en/stable/config_file.html\n[tool.mypy]\nfiles = \"src/**/\"\nstrict = true\ndisallow_any_generics = false\ndisallow_subclassing_any = false\nshow_error_codes = true\npretty = true\n\n# # module specific overrides\n# [[tool.mypy.overrides]]\n# module = [\"numpy.*\",]\n# ignore_errors = true\n\n\n# https://coverage.readthedocs.io/en/6.4/config.html\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"if TYPE_CHECKING:\",\n    \"@overload\",\n    \"except ImportError\",\n    \"\\\\.\\\\.\\\\.\",\n    \"raise NotImplementedError()\",\n]\n[tool.coverage.run]\nsource = [\"src\"]\n\n# https://github.com/mgedmin/check-manifest#configuration\n[tool.check-manifest]\nignore = [\n    \".github_changelog_generator\",\n    \".pre-commit-config.yaml\",\n    \".ruff_cache/**/*\",\n    \"setup.py\",\n    \"tests/**/*\",\n]\n\n# https://python-semantic-release.readthedocs.io/en/latest/configuration.html\n[tool.semantic_release]\nversion_source = \"tag_only\"\nbranch = \"main\"\nchangelog_sections = \"feature,fix,breaking,documentation,performance,chore,:boom:,:sparkles:,:children_crossing:,:lipstick:,:iphone:,:egg:,:chart_with_upwards_trend:,:ambulance:,:lock:,:bug:,:zap:,:goal_net:,:alien:,:wheelchair:,:speech_balloon:,:mag:,:apple:,:penguin:,:checkered_flag:,:robot:,:green_apple:,Other\"\n# commit_parser=semantic_release.history.angular_parser\nbuild_command = \"pip install build && python -m build\"\n\n# # for things that require compilation\n# # https://cibuildwheel.readthedocs.io/en/stable/options/\n# [tool.cibuildwheel]\n# # Skip 32-bit builds & PyPy wheels on all platforms\n# skip = [\"*-manylinux_i686\", \"*-musllinux_i686\", \"*-win32\", \"pp*\"]\n# test-extras = [\"test\"]\n# test-command = \"pytest {project}/tests -v\"\n# test-skip = \"*-musllinux*\"\n\n# [tool.cibuildwheel.environment]\n# HATCH_BUILD_HOOKS_ENABLE = \"1\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mercy",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/merkle-dag-matlab",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Merkle-DAG-Matlab",
            "content": {
                "codemeta": "",
                "readme": "![MATLAB](https://img.shields.io/badge/MATLAB-%23D00000.svg?style=plastic&logo=mathworks&logoColor=white) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/Merkle-DAG-Matlab?style=plastic)\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808889.svg)](https://doi.org/10.5281/zenodo.12808889)\n\n# Merkle-DAG Implementation in MATLAB\n\nThis repository contains MATLAB scripts for implementing and using a Merkle Directed Acyclic Graph (DAG) data structure.\n\nThe Merkle-DAG is a cryptographic data structure used to efficiently verify the integrity and consistency of data blocks.\n\n### About\n\n> [IPFS Link](https://docs.ipfs.tech/concepts/merkle-dag/)\n\n### Overview\n\n- Construct a Merkle-DAG manually or from data blocks.\n- Traverse the graph structure and verify the integrity of data blocks.\n- Multiple hash algorithms for computing node hashes.\n   \n    Supported algorithms from Java Security (via MATLAB)  \n    ```matlab\n    import java.security.MessageDigest;\n    java.security.Security.getAlgorithms('MessageDigest')\n    ```\n\n### Scripts\n\n1. `MerkleDAGNode.m`\n\n   > Represents a node in the Merkle-DAG, holds data, compute hashes, and manage child nodes.\n\n2. `MerkleDAG.m`\n\n   > Constructs the Merkle-DAG from data blocks, performs integrity verification, and provides traversal methods (DFS & BFS).\n\n### Example Usages\n\n#### Manually Create the Merkle-DAG (Adding Nodes)\n\nUse case: for scenarios where the DAG structure is not strictly determined by the data itself (no specific relationships/dependencies).\n\n```matlab\nnode1 = MerkleDAGNode([1 2 3]);     % default: SHA-256, if no hash algorithm specified\nnode2 = MerkleDAGNode([4 5 6]);\nnode3 = MerkleDAGNode([7 8 9]);\n\n% Add children to node1 (hash recursively updated)\nnode1.addChild(node2);\nnode1.addChild(node3);\n\n% Add another child to node2    (hash recursively updated)\nnode4 = MerkleDAGNode([10 11 12]);\nnode2.addChild(node4);\n\n% Display the Merkle-DAG structure\nDAGGraph = MerkleDAG();\nDAGGraph.setRoot(node1)\nDAGGraph.traverseDFS();     % Depth-First (DFS) traversal\nDAGGraph.traverseBFS();     % Breadth-First (BFS) traversal\n\n```\n\n#### Build DAG from data blocks\n\nUse case: automated, allowing constructing a Merkle-DAG from a matrix of data blocks.\n\nEach row of the matrix represents a data block. The DAG is built by hashing these blocks into the graph. \nFor scenarios where the relationships between data blocks are determined by their positions in the matrix.\n\n```matlab\ndataBlocks = [      // compose data of the MerkleDAG as a matrix\n    1 2 3;\n    4 5 6;\n    7 8 9;\n    10 11 12;\n    13 14 15\n];\n\nmerkleDAG = MerkleDAG(dataBlocks, 'SHA-384');\n\n% Display the Merkle-DAG structure\nmerkleDAG.traverseDFS();     % Depth-First (DFS) traversal\nmerkleDAG.traverseBFS();     % Breadth-First (BFS) traversal\n```\n\n#### Integrity Verification\n\nThe verifyBlock method checks whether a specific data block is part of the Merkle-DAG. \nComputes the hash of the data block and verifies it against the hashes in the DAG.\n\n```matlab\n% Verify a specific data block \ndataBlockToVerify = [4 5 6];\nmerkleDAG.verifyBlock(dataBlockToVerify);\n```\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/meshit",
            "repo_link": "https://github.com/bloech/MeshIt",
            "content": {
                "codemeta": "",
                "readme": "MeshIt 2010-2020\n================\n\nThe tool MeshIT uses TRIANGLE <http://www.cs.cmu.edu/~quake/triangle.html> and\nTETGEN <http://wias-berlin.de/software/tetgen> to generate a quality\ntetrahedral mesh based on structural geological information.\n\nThis procedure is fully automatized and needs at least scattered data points\nas input!\n\nMain developers: Mauro Cacace (<mailto:cacace@gfz-potsdam.de>) and\n                 Guido Blöcher (<mailto:bloech@gfz-potsdam.de>).\n\nSome extensions were added by PERFACCT (www.perfacct.eu) by the following developers:\n\t\t\t\tJohannes Spazier\n\t\t\t\tNihed Boussaidi\n\t\t\t\tDanny Puhan\n\nThe source can be compiled as it comes on Windows, Linux and MacOS by running the building options below.\nFor exporting the exodus file format used by Moose (https://github.com/idaholab/moose) the user has to specify some internal flags in the project file meshit.pro:\n \n*\tFor Linux and Mac we suggest to link the static exodus library which comes along with libMesh (https://libmesh.github.io/) provided by Moose framework installation:\n\t*\tSet the EXODUS_LIBMESH variable to `true`\n\t*\tDefine the path to the root directory of the `libmesh` installation using variable LIBMESH\n\n*\tFor Windows you have to link the dynamic exodus library which will be provided by the package mingw-w64-ucrt-x86_64-libexodus provided by the MSYS2 (https://www.msys2.org/) installation:\n\t*\tSet the EXODUS_LIBRARY variable to `true`\n\t*\tDefine the path the rootdirectory of 'exodusII' installation\n\t\nOn all platforms the following requirements are suggested and tested:\n\nWindows:\n\n    Qt 5.15.2 for Windows 64-bit\n\nLinux:\n\n    Qt 5.9.9 for Linux 64-bit\n\nMac:\n\n    Qt 5.14.1 for OS X\n\nPreparations:\n*\tFor Windows users, please follow the following steps for a proper `exodusII` dynamic library installation:\n\t*\tDownload the `msys2-x86_64` installer (https://www.msys2.org/)\n\t*\tRun the installer. Installing MSYS2 requires 64 bit Windows 10 or newer.\n\t*\tEnter your desired Installation Folder (short ASCII-only path on a NTFS volume, no accents, no spaces, no symlinks, no subst or network drives, no FAT).\n\t*\tWhen done, click Finish.\n\t*\tNow MSYS2 is ready for you and a terminal for the UCRT64 environment will launch.\n\t*\tInstall `mingw-w64-ucrt-x86_64-libexodus`. Run the following command:\n\n\t\t\tpacman -S mingw-w64-ucrt-x86_64-libexodus\n\n\t*\tTo enable the dynamic dependencies of the `exodusII` library add the root folder to your environment variable path:\n\t\t*\tOpen the Start Search, type in `env`, and choose `Edit the system environment variables`.\n\t\t*\tClick the `Environment Variables…` button.\n\t\t*\tUnder the `User Variables` section, find the row with “Path” in the first column, and click edit.\n\t\t*\tThe `Edit environment variable` UI will appear. Here, you can click “New” and type in the new path (default installation path C:\\msys64\\ucrt64) you want to add.\n\t\t*\tDismiss all of the dialogs by choosing “OK”. Your changes are saved!\n\t\t*\tYou will probably need to restart apps for them to pick up the change. Restarting the machine would ensure all apps are run with the PATH change.\n\n*\tFor macOS users, please check the version of macOS specified in the\n\t`meshit.pro` file (line `QMAKE_MACOSX_DEPLOYMENT_TARGET = 10.15`). By default, the version of macOS specified is 10.15 (macOS Catalina)\n\nBuilding:\n\n\tqmake meshit.pro\n\tmake/nmake/mingw32-make\n\nCite:\n*\thttps://doi.org/10.5281/zenodo.4327281\n*\tCacace, M., Blöcher, G. MeshIt - a software for three dimensional volumetric meshing of complex faulted reservoirs. Environ Earth Sci 74, 5191–5209 (2015). https://doi.org/10.1007/s12665-015-4537-x\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/messy",
            "repo_link": "https://gitlab.dkrz.de/MESSy/MESSy",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/metabolator",
            "repo_link": "https://codebase.helmholtz.cloud/metabolator/metabolator",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/methylkit",
            "repo_link": "https://github.com/al2na/methylKit",
            "content": {
                "codemeta": "",
                "readme": "\n\n<a name=\"logo\"/>\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/al2na/methylKit/master/inst/methylKit_logo.png\" alt=\"methylKit Logo\"  ></img>\n</a>\n</div>\n\nmethylKit \n========\n\nBuild Status \n\n|  |  | \n| - | - |\n| Github | [![Build Status](https://github.com/al2na/methylKit/actions/workflows/check-standard.yaml/badge.svg)](https://github.com/al2na/methylKit/actions/workflows/check-standard.yaml) |\n| Bioc Release | [![Bioc release status](https://www.bioconductor.org/shields/build/release/bioc/methylKit.svg)](https://bioconductor.org/checkResults/release/bioc-LATEST/methylKit) | \nBioc Devel | [![Bioc devel status](https://www.bioconductor.org/shields/build/devel/bioc/methylKit.svg?text=esfes&)](https://bioconductor.org/checkResults/devel/bioc-LATEST/methylKit) | \n\n\n[![GitHub R package version](https://img.shields.io/github/r-package/v/al2na/methylKit?label=version&)](https://github.com/al2na/methylKit/blob/master/NEWS)\n[![codecov](https://codecov.io/github/al2na/methylKit/branch/master/graphs/badge.svg)](https://codecov.io/github/al2na/methylKit) \n\n# Introduction\n\n*methylKit* is an [R](http://en.wikipedia.org/wiki/R_%28programming_language%29) package \nfor DNA methylation analysis and annotation from high-throughput bisulfite sequencing. The \npackage is designed to deal with sequencing data from \n[RRBS](http://www.nature.com/nprot/journal/v6/n4/abs/nprot.2010.190.html) and its variants,\nbut also target-capture methods such as [Agilent SureSelect \nmethyl-seq](http://www.halogenomics.com/sureselect/methyl-seq). \nIn addition, methylKit can \ndeal with base-pair resolution data for 5hmC obtained from Tab-seq or oxBS-seq. It can also \nhandle whole-genome bisulfite sequencing data if proper input format is provided.\n\n## Current Features\n\n * Coverage statistics\n * Methylation statistics\n * Sample correlation and clustering\n * Differential methylation analysis \n * Feature annotation and accessor/coercion functions \n * Multiple visualization options  \n * Regional and tiling windows analysis\n * (Almost) proper [documentation](https://bioconductor.org/packages/release/bioc/vignettes/methylKit/inst/doc/methylKit.html)\n * Reading methylation calls directly from [Bismark(Bowtie/Bowtie2](http://www.bioinformatics.bbsrc.ac.uk/projects/bismark/) alignment files\n * Batch effect control\n * Multithreading support (for faster differential methylation calculations) \n * Coercion to objects from Bioconductor package GenomicRanges\n * Reading methylation percentage data from generic text files\n\n\n\n\n\n## Staying up-to-date\n\nYou can subscribe to our googlegroups page to get the latest information about new releases and features (low-frequency, only updates are posted)\n\n- https://groups.google.com/forum/#!forum/methylkit\n\nTo ask questions please use methylKit_discussion forum\n\n- https://groups.google.com/forum/#!forum/methylkit_discussion\n\nYou can also check out the blogposts we make on using methylKit\n\n- http://zvfak.blogspot.de/search/label/methylKit\n\n-------\n\n## Installation\n\nin R console,\n```r\nlibrary(devtools)\ninstall_github(\"al2na/methylKit\", build_vignettes=FALSE, \n  repos=BiocManager::repositories(),\n  dependencies=TRUE)\n```\nif this doesn't work, you might need to add `type=\"source\"` argument.\n\n### Install the development version\n```r\nlibrary(devtools)\ninstall_github(\"al2na/methylKit\", build_vignettes=FALSE, \n  repos=BiocManager::repositories(),ref=\"development\",\n  dependencies=TRUE)\n```\nif this doesn't work, you might need to add `type=\"source\"` argument.\n\n\n-------\n\n# How to Use\n\nTypically, bisulfite converted reads are aligned to the genome and % methylation value per base is calculated by processing alignments. *`methylKit`* takes that  % methylation value per base information as input. Such input file may be obtained from [AMP pipeline](http://code.google.com/p/amp-errbs/) for aligning RRBS reads. A typical input file looks like this:\n\n```\nchrBase\tchr\tbase\tstrand\tcoverage\tfreqC\tfreqT\nchr21.9764539\tchr21\t9764539\tR\t12\t25.00\t75.00\nchr21.9764513\tchr21\t9764513\tR\t12\t0.00\t100.00\nchr21.9820622\tchr21\t9820622\tF\t13\t0.00\t100.00\nchr21.9837545\tchr21\t9837545\tF\t11\t0.00\t100.00\nchr21.9849022\tchr21\t9849022\tF\t124\t72.58\t27.42\nchr21.9853326\tchr21\t9853326\tF\t17\t70.59\t29.41\n\n```\n\n\n*`methylKit`* reads in those files and performs basic statistical analysis and annotation for differentially methylated regions/bases. Also a tab separated text file with a generic format can be read in, such as methylation ratio files from [BSMAP](http://code.google.com/p/bsmap/), see [here](http://zvfak.blogspot.com/2012/10/how-to-read-bsmap-methylation-ratio.html) for an example. Alternatively, `read.bismark` function can read SAM file(s) output by [Bismark](http://www.bioinformatics.bbsrc.ac.uk/projects/bismark/)(using bowtie/bowtie2) aligner (the SAM file must be sorted based on chromosome and read start). The sorting must be done by unix sort or samtools, sorting using other tools may change the column order of the SAM file and that will cause an error. \n\nBelow, there are several options showing how to do basic analysis with *`methylKit`*.\n\n## Documentation ##\n * You can look at the vignette [here](https://bioconductor.org/packages/release/bioc/vignettes/methylKit/inst/doc/methylKit.html). This is the primary source of documentation. It includes detailed examples.\n * You can check out the [slides](https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/methylkit/methylKitTutorialSlides_2013.pdf ) for a tutorial at EpiWorkshop 2013. This works with older versions of methylKit, you may need to update the function names.\n * You can check out the [tutorial](https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/methylkit/methylKitTutorial_feb2012.pdf) prepared for  EpiWorkshop 2012. This works with older versions of methylKit, you may need to update the function names.\n* You can check out the [slides](https://www.slideshare.net/AlexanderGosdschan/eurobioc-2018-metyhlkit-overview) prepared for EuroBioc 2018. This also includes more recent features of methylKit  and is meant to give you a quick overview about what you can do with the package. \n\n\n\n\n\n\n## Downloading Annotation Files\nAnnotation files in BED format are needed for annotating your differentially methylated regions. You can download annotation files from UCSC table browser for your genome of interest. Go to  [http://genome.ucsc.edu/cgi-bin/hgGateway]. On the top menu click on \"tools\" then \"table browser\". Select your \"genome\" of interest and \"assembly\" of interest from the drop down menus. Make sure you select the correct genome and assembly. Selecting wrong genome and/or assembly will return unintelligible results in downstream analysis. \n\nFrom here on you can either download *gene annotation* or *CpG island annotation*.\n\n1. For gene annotation, select _\"Genes and Gene prediction tracks\"_ from the *\"group\"* drop-down menu. Following that, select _\"Refseq Genes\"_ from the *\"track\"* drop-down menu. Select _\"BED- browser extensible data\"_ for the *\"output format\"*. Click *\"get output\"* and on the following page click *\"get BED\"* without changing any options. save the output as a text file.\n2. For CpG island annotation, select _\"Regulation\"_ from the *\"group\"* drop-down menu. Following that, select _\"CpG islands\"_ from the *\"track\"* drop-down menu. Select _\"BED- browser extensible data\"_  for the *\"output format\"*. Click *\"get output\"* and on the following page click *\"get BED\"* without changing any options. save the output as a text file.\n\n\nIn addition, you can check this tutorial to learn how to download any track from UCSC in BED format (http://www.openhelix.com/cgi/tutorialInfo.cgi?id=28)\n\n\n\n\n-------\n# R script for Genome Biology publication\nThe most recent version of the R script in the Genome Biology manuscript is [here](http://code.google.com/p/methylkit/downloads/list?q=label:AdditionalFile4 ).\n\n-------\n# Citing methylKit\nIf you used methylKit please cite:\n\n\n * Altuna Akalin, Matthias Kormaksson, Sheng Li, Francine E. Garrett-Bakelman, Maria E. Figueroa, Ari Melnick, Christopher E. Mason. _(2012)_. *\"[methylKit: A comprehensive R package for the analysis of genome-wide DNA methylation profiles.](http://genomebiology.com/2012/13/10/R87/)\"* _Genome Biology_ , 13:R87.\n\nIf you used flat-file objects or over-dispersion corrected tests please consider citing:\n\n* Wreczycka K, Gosdschan A, Yusuf D, Grüning B, Assenov Y, Akalin A. *\"[Strategies for analyzing bisulfite sequencing data.](https://linkinghub.elsevier.com/retrieve/pii/S0168-1656(17)31593-6)\"* J Biotechnol., 2017\n\nand also consider citing the following publication as a use-case with specific cutoffs:\n\n * Altuna Akalin, Francine E. Garrett-Bakelman, Matthias Kormaksson, Jennifer Busuttil, Lu Zhang, Irina Khrebtukova, Thomas A. Milne, Yongsheng Huang, Debabrata Biswas, Jay L. Hess, C. David Allis, Robert G. Roeder, Peter J. M. Valk, Bob Löwenberg, Ruud Delwel, Hugo F. Fernandez, Elisabeth Paietta, Martin S. Tallman, Gary P. Schroth, Christopher E. Mason, Ari Melnick, Maria E. Figueroa. _(2012)_. *\"[Base-Pair Resolution DNA Methylation Sequencing Reveals Profoundly Divergent Epigenetic Landscapes in Acute Myeloid Leukemia.](http://www.plosgenetics.org/article/info%3Adoi%2F10.1371%2Fjournal.pgen.1002781)\"* _PLoS Genetics_ 8(6).\n\n-------\n# Contact & Questions\ne-mail to [methylkit_discussion@googlegroups.com](mailto:methylkit_discussion@googlegroups.com ) or post a question using [the web interface](https://groups.google.com/forum/#!forum/methylkit_discussion).\n\nif you are going to submit bug reports or ask questions, please send sessionInfo() output from R console as well.\n\nQuestions are very welcome, although we suggest you read the paper, documentation(function help pages and the vignette) and [ blog entries](http://zvfak.blogspot.com/search/label/methylKit) first. The answer to your question might be there already.\n\n-------\n# Contribute to the development\nSee the [trello board](https://trello.com/b/k2kv1Od7/methylkit) for methylKit development. You can contribute to the methylKit development via github ([http://github.com/al2na/methylKit/]) by opening an issue and discussing what you want to contribute, we will guide you from there. In addition, you should:\n\n * Bump up the version in the DESCRIPTION file on the 3rd number. For example, the master branch has the version numbering as in \"X.Y.1\". If you make a change to master branch you should bump up the version in the DESCRIPTION file to \"X.Y.2\".\n\n * Add your changes to the NEWS file as well under the correct version and appropriate section. Attribute the changes to yourself, such as \"Contributed by X\"\n \nLicense\n---------\nArtistic License/GPL\n\n",
                "dependencies": "Package: methylKit\nType: Package\nTitle: DNA methylation analysis from high-throughput\n    bisulfite sequencing results\nVersion: 1.33.1\nAuthor: Altuna Akalin [aut, cre], Matthias Kormaksson [aut], \n    Sheng Li [aut], Arsene Wabo [ctb], Adrian Bierling [aut], \n    Alexander Blume [aut], Katarzyna Wreczycka [ctb]\nMaintainer: Altuna Akalin <aakalin@gmail.com>, Alexander Blume \n    <alex.gos90@gmail.com>\nDescription: methylKit is an R package for DNA methylation analysis and\n    annotation from high-throughput bisulfite sequencing. The package is\n    designed to deal with sequencing data from RRBS and its variants, but also\n    target-capture methods and whole genome bisulfite sequencing. It also has\n    functions to analyze base-pair resolution 5hmC data from experimental\n    protocols such as oxBS-Seq and TAB-Seq. Methylation calling can be \n    performed directly from Bismark aligned BAM files.\nLicense: Artistic-2.0\nURL: https://github.com/al2na/methylKit\nBugReports: https://github.com/al2na/methylKit/issues\nLazyLoad: yes\nNeedsCompilation: yes\nLinkingTo: Rcpp, Rhtslib (>= 1.13.1), zlibbioc\nSystemRequirements: GNU make\nbiocViews: DNAMethylation, Sequencing, MethylSeq\nDepends:\n    R (>= 3.5.0),\n    GenomicRanges (>= 1.18.1),\n    methods\nImports:\n    IRanges,\n    data.table (>= 1.9.6),\n    parallel,\n    S4Vectors (>= 0.13.13),\n    GenomeInfoDb,\n    KernSmooth,\n    qvalue,\n    emdbook,\n    Rsamtools,\n    gtools,\n    fastseg,\n    rtracklayer,\n    mclust,\n    mgcv,\n    Rcpp,\n    R.utils,\n    limma, \n    grDevices, \n    graphics, \n    stats, \n    utils\nSuggests:\n    testthat (>= 2.1.0),\n    knitr,\n    rmarkdown,\n    genomation,\n    BiocManager\nVignetteBuilder: knitr\nCollate:\n    'methylKit.R'\n    'backbone.R'\n    'diffMeth.R'\n    'clusterSamples.R'\n    'regionalize.R'\n    'processBismarkAln.R'\n    'RcppExports.R'\n    'document_data.R'\n    'bedgraph.R'\n    'reorganize.R'\n    'percMethylation.R'\n    'normalizeCoverage.R'\n    'pool.R'\n    'adjustMethylC.R'\n    'updateMethObject.R'\n    'batchControl.R'\n    'dataSim.R'\n    'methylDBClasses.R'\n    'methylDBFunctions.R'\n    'tabix.functions.R'\n    'methSeg.R'\n    'diffMethDSS.R'\n    'deprecated_defunct.R'\n    'onUnload.R'\nRoxygenNote: 7.1.1\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/metrics-reloaded",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/mfdfa",
            "repo_link": "https://github.com/LRydin/MFDFA",
            "content": {
                "codemeta": "",
                "readme": "[![DOI:10.1016/j.cpc.2021.108254](http://img.shields.io/badge/DOI-10.1016/j.cpc.2021.108254-00ff00.svg)](https://doi.org/10.1016/j.cpc.2021.108254)\n[![arXiv](https://img.shields.io/badge/arXiv-2104.10470-00ff00.svg)](https://arxiv.org/abs/2104.10470)\n[![zenodo](https://zenodo.org/badge/224135077.svg)](https://zenodo.org/badge/latestdoi/224135077)\n![PyPI - License](https://img.shields.io/pypi/l/MFDFA)\n![PyPI](https://img.shields.io/pypi/v/MFDFA)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/MFDFA)\n[![Build Status](https://github.com/LRydin/MFDFA/actions/workflows/CI.yml/badge.svg)](https://github.com/LRydin/MFDFA/actions/workflows/CI.yml)\n[![codecov](https://codecov.io/gh/LRydin/MFDFA/branch/master/graph/badge.svg)](https://codecov.io/gh/LRydin/MFDFA)\n[![Documentation Status](https://readthedocs.org/projects/mfdfa/badge/?version=latest)](https://mfdfa.readthedocs.io/en/latest/?badge=latest)\n\n\n# MFDFA\nMultifractal Detrended Fluctuation Analysis `MFDFA` is a model-independent method to uncover the self-similarity of a stochastic process or auto-regressive model.\n`DFA` was first developed by Peng *et al.*<sup>1</sup> and later extended to study multifractality `MFDFA` by Kandelhardt *et al.*<sup>2</sup>.\n\nIn the latest release there is as well added a moving window system, especially useful for short timeseries, a recent extension to DFA called *extended DFA*, and the extra feature of Empirical Mode Decomposition as detrending method.\n\n# Installation\nTo install MFDFA you can simply use\n\n```\npip install MFDFA\n```\nAnd on your favourite editor simply import `MFDFA` as\n```python\nfrom MFDFA import MFDFA\n```\nThere is an added library `fgn` to generate fractional Gaussian noise.\n\nYou can find the latest published paper of this library in Computer Physics Communications [L. Rydin Gorjão, G. Hassan, J. Kurths, and D. Witthaut, _MFDFA: Efficient multifractal detrended fluctuation analysis in python_, Computer Physics Communications *273*, 108254 2022](https://doi.org/10.1016/j.cpc.2021.108254). You can find the paper [here](https://github.com/LRydin/MFDFA/blob/master/paper/paper.pdf).\n\n# The `MFDFA` library\n`MFDFA` basis is solely dependent on `numpy`, especially `numpy`'s `polynomial`. In version 0.3 a [Empirical Mode Decomposition](https://en.wikipedia.org/wiki/Hilbert%E2%80%93Huang_transform) method was added for an alternative method of detrending timeseries, relying on [Dawid Laszuk's](https://github.com/laszukdawid/PyEMD) `PyEMD`.\n\n# Employing the `MFDFA` library\n\n## An exemplary one-dimensional fractional Ornstein–Uhlenbeck process\nThe rationale here is simple: Numerically integrate a stochastic process in which we know exactly the fractal properties, characterised by the Hurst coefficient, and recover this with MFDFA.\nWe will use a fractional Ornstein–Uhlenbeck, a commonly employ stochastic process with mean-reverting properties.\nFor a more detailed explanation on how to integrate an Ornstein–Uhlenbeck process, see the [kramersmoyal's package](https://github.com/LRydin/KramersMoyal#a-one-dimensional-stochastic-process).\nYou can also follow the [fOU.ipynb](/examples/fOU.ipynb)\n\n### Generating a fractional Ornstein–Uhlenbeck process\nThis is one method of generating a (fractional) Ornstein–Uhlenbeck process with *H=0.7*, employing a simple Euler–Maruyama integration method\n\n```python\n# Imports\nfrom MFDFA import MFDFA\nfrom MFDFA import fgn\n# where this second library is to generate fractional Gaussian noises\n\n# integration time and time sampling\nt_final = 2000\ndelta_t = 0.001\n\n# Some drift theta and diffusion sigma parameters\ntheta = 0.3\nsigma = 0.1\n\n# The time array of the trajectory\ntime = np.arange(0, t_final, delta_t)\n\n# The fractional Gaussian noise\nH = 0.7\ndB = (t_final ** H) * fgn(N = time.size, H = H)\n\n# Initialise the array y\ny = np.zeros([time.size])\n\n# Integrate the process\nfor i in range(1, time.size):\n    y[i] = y[i-1] - theta * y[i-1] * delta_t + sigma * dB[i]\n```\nAnd now you have a fractional process with a self-similarity exponent *H=0.7*\n\n### Using the `MFDFA`\nTo now utilise the `MFDFA`, we take this exemplary process and run the (multifractal) detrended fluctuation analysis. For now lets consider only the monofractal case, so we need only `q=2`.\n```python\n# Select a band of lags, which usually ranges from\n# very small segments of data, to very long ones, as\nlag = np.unique(np.logspace(0.5, 3, 100).astype(int))\n# Notice these must be ints, since these will segment\n# the data into chucks of lag size\n\n# Select the power q\nq = 2\n\n# The order of the polynomial fitting\norder = 1\n\n# Obtain the (MF)DFA as\nlag, dfa = MFDFA(y, lag = lag, q = q, order = order)\n```\n\nNow we need to visualise the results, which can be understood in a log-log scale. To find *H* we need to fit a line to the results in the log-log plot\n```python\n# To uncover the Hurst index, lets get some log-log plots\nplt.loglog(lag, dfa, 'o', label='fOU: MFDFA q=2')\n\n# And now we need to fit the line to find the slope. Don't\n# forget that since you are plotting in a double logarithmic\n# scales, you need to fit the logs of the results\nH_hat = np.polyfit(np.log(lag)[4:20],np.log(dfa[4:20]),1)[0]\n\n# Now what you should obtain is: slope = H + 1\nprint('Estimated H = '+'{:.3f}'.format(H_hat[0]))\n```\n\n<img src=\"docs/_static/fig1.png\" title=\"MFDFA of a fractional Ornstein–Uhlenbeck process\" height=\"250\"/>\n\n\n## Uncovering multifractality in stochastic processes\nYou can find more about multifractality in the [documentation](https://mfdfa.readthedocs.io/en/latest/1dLevy.html).\n\n# Changelog\n- Version 0.4.3 - Reverting negative values in the estimation of the singularity strenght α.\n- Version 0.4.2 - Corrected spectral plots. Added [examples](https://github.com/LRydin/MFDFA/tree/master/examples) from the paper.\n- Version 0.4.1 - Added conventional spectral plots as _h(q)_ vs _q_, _τ(q)_ vs _q_, and _f(α)_ vs _α_.\n- Version 0.4 - EMD is now optional. Restored back compatibility: py3.3 to py3.9. For EMD py3.6 or larger is needed.\n- Version 0.3 - Adding EMD detrending. First release. PyPI code.\n- Version 0.2 - Removed experimental features. Added documentation\n- Version 0.1 - Uploaded initial working code\n\n# Contributions\nI welcome reviews and ideas from everyone. If you want to share your ideas or report a bug, open an [issue](https://github.com/LRydin/KramersMoyal/issues) here on GitHub, or contact me directly.\nIf you need help with the code, the theory, or the implementation, do not hesitate to reach out, I am here to help.\nThis package abides to a [Conduct of Fairness](contributions.md).\n\n# Literature and Support\n### Submission history\nThis library has been submitted for publication at [The Journal of Open Source Software](https://joss.theoj.org/) in December 2019. It was rejected. The review process can be found [here on GitHub](https://github.com/openjournals/joss-reviews/issues/1966). The plan is to extend the library and find another publisher.\n\n### History\nThis project was started in 2019 at the [Faculty of Mathematics, University of Oslo](https://www.mn.uio.no/math/english/research/groups/risk-stochastics/) in the Risk and Stochastics section by Leonardo Rydin Gorjão and is supported by Dirk Witthaut and the [Institute of Energy and Climate Research Systems Analysis and Technology Evaluation](https://www.fz-juelich.de/iek/iek-ste/EN/Home/home_node.html). I'm very thankful to all the folk in Section 3 in the Faculty of Mathematics, University of Oslo, for helping me getting around the world of stochastic processes: Dennis, Anton, Michele, Fabian, Marc, Prof. Benth and Prof. di Nunno. In April 2020 Galib Hassan joined in extending `MFDFA`, particularly the implementation of `EMD`.\n\n\n### Funding\nHelmholtz Association Initiative *Energy System 2050 - A Contribution of the Research Field Energy* and the grant No. VH-NG-1025; *STORM - Stochastics for Time-Space Risk Models* project of the Research Council of Norway (RCN) No. 274410, and the *E-ON Stipendienfonds*.\n\n### References\n<sup>1</sup>Peng, C.-K., Buldyrev, S. V., Havlin, S., Simons, M., Stanley, H. E., & Goldberger, A. L. (1994). *Mosaic organization of DNA nucleotides*. [Physical Review E, 49(2), 1685–1689](https://doi.org/10.1103/PhysRevE.49.1685)\\\n<sup>2</sup>Kantelhardt, J. W., Zschiegner, S. A., Koscielny-Bunde, E., Havlin, S., Bunde, A., & Stanley, H. E. (2002). *Multifractal detrended fluctuation analysis of nonstationary time series*. [Physica A: Statistical Mechanics and Its Applications, 316(1-4), 87–114](https://doi.org/10.1016/S0378-4371(02)01383-3)\n\n",
                "dependencies": "numpy\n\nimport setuptools\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=\"MFDFA\",\n    version=\"0.4.3\",\n    author=\"Leonardo Rydin Gorjao\",\n    author_email=\"leonardo.rydin@gmail.com\",\n    description=\"Multifractal Detrended Fluctuation Analysis in Python\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/LRydin/MFDFA\",\n    packages=setuptools.find_packages(),\n    install_requires = [\"numpy\"],\n    extras_require = {\"EMD-signal\": [\"EMD-signal\"],\n                      \"matplotlib\": [\"matplotlib\"]},\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    license=\"MIT License\",\n    python_requires='>=3.6',\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mhm",
            "repo_link": "https://git.ufz.de/mhm/mhm",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mibianto",
            "repo_link": "https://github.com/ccb-sb/mibianto",
            "content": {
                "codemeta": "",
                "readme": "# MiBiAnTo: An Online Microbiome Analysis Tool\n\n[Mibianto Webpage](https://www.ccb.uni-saarland.de/mibianto)\n\n## Issues, queries and request\n\nWe have created this GitHub repository to handle communication with users. Please feel free to make use of it to get in touch with questions or remarks regarding the database website or content.\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/micromechanics-indentationgui",
            "repo_link": "https://github.com/micromechanics/indentationGUI",
            "content": {
                "codemeta": "",
                "readme": "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.11563149.svg)](https://doi.org/10.5281/zenodo.11563149)\n\n<h2 align = \"center\">\nmicromechanics-indentationGUI\n</h2>\n<p align=\"center\">\n  <img\n  src=\"https://raw.githubusercontent.com/micromechanics/indentationGUI/main/micromechanics_indentationGUI/pic/logo.png\" \n  width=\"314\"\n  title=\"micromechanics-indentationGUI\" >\n</p>\n\n# Install\n- Create a new environment (python >= 3.8) using **anaconda-navigator** (https://www.anaconda.com/). In Anaconda's documentation (https://docs.anaconda.com/free/navigator/) you will learn:\n  - how to **install** anaconda-navigator in the section of \"*Anaconda Navigator* => *Installation*\" (reading takes ~5 min), \n  - how to **create and activate** a new environment in the section of \"*Anaconda Navigator* => *Tutorials* => *How to create a Python 3.5 environment from Anaconda2 or Anaconda3*\" (reading takes ~5 min).\n\n- In the terminal of the environment created by Anaconda Navigator, keyboard type the following command and press Enter\n``` bash\npip install micromechanics-indentationGUI\n```\n# Upgrade\nIn the terminal, keyboard type the following command and press Enter\n``` bash\npip install --upgrade micromechanics-indentationGUI\n```\n# Usage\nUsers need to know:\n- For fast reading using **HDF5** files:\n  - Using the given original file (e.g. the XLSX file for G200X), the HDF5 file will be automatically generated at the first calibration/calculation (or when an HDF5 file with the same name as the XLSX file does not exist).\n  - The automatically generated HDF5 file has the same file name as the original file (e.g. the XLSX file for G200X) except the file extension of \".h5\" and locates in the same folder as the original file.\n  - The original file extension (e.g. '.xlsx' for G200X) should be given in the path instead of the file extension of the HDF5 file (\".h5\").\n  - **[Important]** If you changed the content of the original file, please delete the correspoding HDF5 file.\n\nRunning by keyboard typing the following command and pressing Enter in the terminal\n``` bash\nmicromechanics-indentationGUI\n``` \n# Uninstall\nIn the terminal, keyboard type the following command and press Enter\n``` bash\npip uninstall micromechanics-indentationGUI\n```\n\n# More detailed descriptions for developers\n\n# Prepare and create a new version\n- Delete RecentFiles.txt in /indentationGUI/micromechanics_indentationGUI\n\n- Delete *.hf in /indentationGUI/micromechanics_indentationGUI/Examples\n\n- Set \"# pylint: skip-file\" for all files named \"***_ui.py\"\n\n- Test the code: linting, documentation and then the tests from the project's main directory\n``` bash\npylint micromechanics_indentationGUI/\nmake -C docs html\n# python tests/testVerification.py\n```\n\nThen upload/create-pull-request to GitHub, via\n``` bash\n./commit.py 'my message'\n```\n\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools >= 43.0.0\", \"wheel\"]\n\n#This file is autogenerated by commit.py from setup.cfg. Change content there\npyside6 ==6.4.2\nnumpy\nmatplotlib >= 3.6.3\nmicromechanics == 1.1.14\npandas ==1.4.2\nopenpyxl >= 3.1.1\nscikit-learn\ntables\nxlsxwriter == 3.1.2\nfrom setuptools import setup\nfrom setuptools.command.install import install\nimport os\nimport platform\nimport commit\nimport sys\n\n\nclass PostInstallCommand(install):\n  \"\"\"Post-installation for installation mode.\"\"\"\n  def run(self):\n    install.run(self)\n    if platform.system() == \"Linux\" and ('generic' in platform.platform()):\n        self.create_desktop_file()\n\n  def create_desktop_file(self):\n    \"\"\"create .desktop file for Linux-Ubuntu user\"\"\"\n    pythonVersion = sys.version[:3]\n    installation_path = os.path.join(sys.prefix, 'lib', f\"python{pythonVersion}\", 'site-packages')\n    desktop_file_content = f\"\"\"\n    [Desktop Entry]\n    Type=Application\n    Name=indentationGUI\n    Exec={os.path.join(sys.prefix, 'bin', 'indentationGUI')}\n    Icon={os.path.join(installation_path, 'micromechanics_indentationGUI', 'pic', 'indentationGUI.png')}\n    Terminal=false\n    \"\"\"\n    desktop_file_path = os.path.expanduser(\"~/.local/share/applications/indentationGUI.desktop\")\n    with open(desktop_file_path, \"w\") as desktop_file:\n        desktop_file.write(desktop_file_content)\n\nif __name__ == '__main__':\n  setup(\n        name='micromechanics-indentationGUI',\n        version=commit.get_version()[1:],\n        cmdclass ={'install':PostInstallCommand}\n        )\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/millepede-ii",
            "repo_link": "https://gitlab.desy.de/claus.kleinwort/millepede-ii",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/minterpy",
            "repo_link": "https://github.com/casus/minterpy",
            "content": {
                "codemeta": "",
                "readme": "![](./docs/assets/Wordmark-color.png)\n\n[![Code style: black][black-badge]][black-link]\n# minterpy\n\n<figure class=\"quote\">\n  <blockquote>\n  to minterpy *sth.* (transitive verb) -- to produce a multivariate polynomial representation of *sth.* .\n  </blockquote>\n  <figcaption>\n    &mdash; The minterpy developers in <cite>[\"Lifting the curse of dimensionality\"](https://interpol.pages.hzdr.de/minterpy/fundamentals/introduction.html)</cite>\n  </figcaption>\n</figure>\n\n---\n\n`minterpy` is an open-source Python package for a multivariate generalization\nof the classical Newton and Lagrange interpolation schemes as well as related tasks.\nIt is based on an optimized re-implementation of\nthe multivariate interpolation prototype algorithm (*MIP*) by Hecht et al.[^1]\nand thereby provides software solutions that lift the curse of dimensionality from interpolation tasks.\nWhile interpolation occurs as the bottleneck of most computational challenges,\n`minterpy` aims to free empirical sciences from their computational limitations.\n\n`minterpy` is continuously extended and improved\nby adding further functionality and modules that provide novel digital solutions\nto a broad field of computational challenges, including but not limited to:\n\n- multivariate interpolation\n- non-linear polynomial regression\n- numerical integration\n- global (black-box) optimization\n- surface level-set methods\n- non-periodic spectral partial differential equations (PDE) solvers on\n  flat and complex geometries\n- machine learning regularization\n- data reconstruction\n- computational solutions in algebraic geometry\n\n## Installation\n\nSince this implementation is a prototype,\nwe currently only provide the installation by self-building from source.\nWe recommend to using `git` to get the `minterpy` source:\n\n```bash\ngit clone https://gitlab.hzdr.de/interpol/minterpy.git\n```\n\nWithin the source directory,\nyou may use the following package manager to install ``minterpy``.\n\nA best practice is to create a virtual environment for `minterpy`.\nYou can do this with the help of [conda] and the ``environment.yaml`` by:\n\n```bash\nconda env create -f environment.yaml\n```\n\nA new conda environment called `minterpy` is created.\nActivate the new environment by:\n\n```bash\nconda activate minterpy\n```\n\nFrom within the environment, install the `minterpy` using [pip],\n\n```bash\npip install [-e] .[all,dev,docs]\n```\n\nwhere the flag `-e` means the package is directly linked\ninto the python site-packages of your Python version.\nThe options `[all,dev,docs]` refer to the requirements defined\nin the `options.extras_require` section in `setup.cfg`.\n\nYou **must not** use the command `python setup.py install` to install `minterpy`,\nas you cannot always assume the files `setup.py` will always be present\nin the further development of `minterpy`.\n\nFinally, if you want to deactivate the conda environment, type:\n\n```bash\nconda deactivate\n```\n\nAlternative to conda, you can create a new virtual environment via\n[venv], [virtualenv], or [pyenv-virtualenv].\nSee [CONTRIBUTING.md](./CONTRIBUTING.md) for details.\n\n## Quickstart\n\nWith `minterpy` one can easily interpolate a given function.\nFor instance, take the function `f(x) = x\\sin(10x)` in one dimension:\n\n```python\n    import numpy as np\n\n    def test_function(x):\n        return x * np.sin(10*x)\n```\n\nIn order to `minterpy` the function `test_function`\none can use the top-level function `interpolate`:\n\n```python\n    import minterpy as mp\n\n    interpolant = mp.interpolate(test_function,spatial_dimension=1, poly_degree=64)\n```\n\nHere, interpolant is a callable function,\nwhich can be used as a representation of `test_function`.\n`interpolate` takes as arguments the function to interpolate,\nthe number of dimensions (`spatial_dimension`),\nand the degree of the underlying polynomial (`poly_degree`).\n\nYou may adjust this parameter in order to get higher accuracy.\nFor the example above, a degree of 64 produces an interpolant that reproduces\nthe `test_function` almost up to machine precision:\n\n```python\n    import matplotlib.pylab as plt\n\n    x = np.linspace(-1,1,100)\n\n    plt.plot(x,interpolant(x),label=\"interpolant\")\n    plt.plot(x,test_function(x),\"k.\",label=\"test function\")\n    plt.legend()\n    plt.show()\n```\n<img src=\"./docs/assets/images/test-function1D.png\" alt=\"Compare test function with its interpolant\" width=\"400\"/>\n\n\nFor more comprehensive examples, see the [getting started guides](https://interpol.pages.hzdr.de/minterpy/getting-started/index.html)\nsection of the ``minterpy`` docs.\n\n## Testing\n\nAfter installation, we encourage you to at least run the unit tests of `minterpy`,\nwhere we use [`pytest`](https://docs.pytest.org/en/6.2.x/) to run the tests.\n\nIf you want to run all tests, type:\n\n```bash\npytest [-vvv]\n```\n\nfrom within the `minterpy` source directory.\n\n## Contributing to `minterpy`\n\nContributions to the `minterpy` packages are highly welcome.\nWe recommend you have a look at the [CONTRIBUTING.md](./CONTRIBUTING.md) first.\nFor a more comprehensive contribution guide visit\nthe [Contributors section](link-to-developer-section) of the documentation.\n\n## Credits and contributors\n\nThis work was partly funded by the Center for Advanced Systems Understanding (CASUS)\nthat is financed by Germany’s Federal Ministry of Education and Research (BMBF)\nand by the Saxony Ministry for Science, Culture and Tourism (SMWK)\nwith tax funds on the basis of the budget approved by the Saxony State Parliament.\n\n### The minterpy development team\n\nThe core development of the `minterpy` is currently done\nby a small team at the Center for Advanced Systems Understanding ([CASUS]),\nnamely\n\n- Uwe Hernandez Acosta ([HZDR]/[CASUS]) (u.hernandez@hzdr.de)\n- Sachin Krishnan Thekke Veettil ([HZDR]/[CASUS]) (s.thekke-veettil@hzdr.de)\n- Damar Wicaksono ([HZDR]/[CASUS]) (d.wicaksono@hzdr.de)\n- Janina Schreiber ([HZDR]/[CASUS]) (j.schreiber@hzdr.de)\n\n### Mathematical foundation\n\n- Michael Hecht ([HZDR]/[CASUS]) (m.hecht@hzdr.de)\n\n### Former Members and Contributions\n\n- Jannik Michelfeit\n- Nico Hoffman ([HZDR])\n- Steve Schmerler ([HZDR])\n- Vidya Chandrashekar (TU Dresden)\n\n### Acknowledgement\n\n- Klaus Steiniger ([HZDR])\n- Patrick Stiller ([HZDR])\n- Matthias Werner ([HZDR])\n- Krzysztof Gonciarz ([MPI-CBG],[CSBD])\n- Attila Cangi ([HZDR]/[CASUS])\n- Michael Bussmann ([HZDR]/[CASUS])\n\n### Community\n\nThis package would not be possible without many contributions done\nfrom the community as well.\nFor that, we want to send big thanks to:\n\n  - the guy who will show me how to include a list of contributors on github/gitlab\n\n\n## License\n\n[MIT](LICENSE) © minterpy development team\n\n[^1]: [arXiv:2010.10824](https://arxiv.org/abs/2010.10824)\n\n[conda]: https://docs.conda.io/\n[pip]: https://pip.pypa.io/en/stable/\n[venv]: https://docs.python.org/3/tutorial/venv.html\n[virtualenv]: https://virtualenv.pypa.io/en/latest/\n[pyenv-virtualenv]: https://github.com/pyenv/pyenv-virtualenv\n[pre-commit]: https://pre-commit.com/\n[Jupyter]: https://jupyter.org/\n[nbstripout]: https://github.com/kynan/nbstripout\n[Google style]: http://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings\n[virtualenv]: https://virtualenv.pypa.io/en/latest/index.html\n[pytest]: https://docs.pytest.org/en/6.2.x/\n[CASUS]: https://www.casus.science\n[HZDR]: https://www.hzdr.de\n[MPI-CBG]: https://www.mpi-cbg.de\n[CSBD]: https://www.csbdresden.de\n\n\n\n[black-badge]:              https://img.shields.io/badge/code%20style-black-000000.svg\n[black-link]:               https://github.com/psf/black\n\n\n",
                "dependencies": "[build-system]\nrequires = [\"wheel\", \"setuptools>=42\", \"setuptools_scm[toml]>=3.4\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools_scm]\nwrite_to = \"src/minterpy/version.py\"\n\n[tool.isort]\nprofile = \"black\"\n\n#!/usr/bin/env python\n# Copyright (c) 2021, Uwe Hernandez Acosta\n#\n# Distributed under the 3-clause BSD license, see accompanying file LICENSE\n# or https://gitlab.hzdr.de/interpol/minterpy for details.\n\nfrom setuptools import setup\n\nsetup()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mirp",
            "repo_link": "https://github.com/oncoray/mirp",
            "content": {
                "codemeta": "",
                "readme": "<img src=\"https://raw.githubusercontent.com/oncoray/mirp/master/icon/mirp.svg\" align=\"right\" width=\"120\"/>\r\n\r\n![GitHub License](https://img.shields.io/github/license/oncoray/mirp)\r\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mirp)\r\n[![PyPI - Version](https://img.shields.io/pypi/v/mirp)](https://pypi.org/project/mirp/)\r\n![GitHub Actions Workflow Status](https://img.shields.io/github/actions/workflow/status/oncoray/mirp/auto-test-dependencies_timed.yml)\r\n[![JOSS](https://joss.theoj.org/papers/165c85b1ecad891550a21b12c8b2e577/status.svg)](https://joss.theoj.org/papers/165c85b1ecad891550a21b12c8b2e577)\r\n\r\n# Medical Image Radiomics Processor\r\n\r\nMIRP is a python package for quantitative analysis of medical images. It focuses on processing images for integration\r\nwith radiomics workflows. These workflows either use quantitative features computed using MIRP, or directly use MIRP\r\nto process images as input for neural networks and other deep learning models.\r\n\r\nMIRP offers the following main functionality:\r\n\r\n- [Extract and collect metadata](https://oncoray.github.io/mirp/image_metadata.html) from medical images.\r\n- [Find and collect labels or names](https://oncoray.github.io/mirp/mask_labels.html) of regions of interest from image \r\n  segmentations.\r\n- [Compute quantitative features](https://oncoray.github.io/mirp/quantitative_image_analysis.html) from regions of interest in medical images.\r\n- [Process images for deep learning](https://oncoray.github.io/mirp/deep_learning.html).\r\n\r\n## Tutorials\r\n\r\nWe currently offer the following tutorials:\r\n\r\n- [Computing quantitative features from MR images](https://oncoray.github.io/mirp/tutorial_compute_radiomics_features_mr.html)\r\n- [Applying filters to images](https://oncoray.github.io/mirp/tutorial_apply_image_filter.html)\r\n\r\n## Documentation\r\n\r\nDocumentation can be found here: https://oncoray.github.io/mirp/\r\n\r\n## Supported Python and OS\r\n\r\nMIRP currently supports the following Python versions and operating systems: \r\n\r\n| Python | Linux     | Win       | OSX       |\r\n|--------|-----------|-----------|-----------|\r\n| 3.10   | Supported | Supported | Supported |\r\n| 3.11   | Supported | Supported | Supported |\r\n| 3.12   | Supported | Supported | Supported |\r\n\r\n## Supported imaging and mask modalities\r\n\r\nMIRP currently supports the following image modalities:\r\n\r\n| File format | File type | Supported modality                              |\r\n|-------------|-----------|-------------------------------------------------|\r\n| DICOM       | image     | CT, MR (incl. ADC, DCE), PT, RTDOSE, CR, DX, MG |\r\n| DICOM       | mask      | RTSTRUCT, SEG                                   |\r\n| NIfTI       | any       | any                                             |\r\n| NRRD        | any       | any                                             |\r\n| numpy       | any       | any                                             |\r\n\r\nNIfTI, NRRD, and numpy files support any kind of (single-channel) image. MIRP cannot process RGB or 4D images.\r\n\r\n## Installing MIRP\r\nMIRP is available from PyPI and can be installed using `pip`, or other installer tools:\r\n\r\n```commandline\r\npip install mirp\r\n```\r\n\r\n## Examples - Computing Radiomics Features\r\n\r\nMIRP can be used to compute quantitative features from regions of interest in images in an IBSI-compliant manner \r\nusing a standardized workflow This requires both images and masks. MIRP can process DICOM, NIfTI, NRRD and numpy \r\nimages. Masks are DICOM radiotherapy structure sets (RTSTRUCT), DICOM segmentation (SEG) or volumetric data with \r\ninteger labels (e.g. 1, 2, etc.).\r\n\r\nBelow is a minimal working example for extracting features from a single image file and its mask.\r\n\r\n```python\r\nfrom mirp import extract_features\r\n\r\nfeature_data = extract_features(\r\n    image=\"path to image\",\r\n    mask=\"path to mask\",\r\n    base_discretisation_method=\"fixed_bin_number\",\r\n    base_discretisation_n_bins=32\r\n)\r\n```\r\nInstead of providing the path to the image (`\"path_to_image\"`), a numpy image can be provided, and the same goes for \r\n`\"path to mask\"`. The disadvantage of doing so is that voxel spacing cannot be determined. \r\n\r\nMIRP also supports processing images and masks for multiple samples (e.g., patients). The syntax is much the same, \r\nbut depending on the file type and directory structure, additional arguments need to be specified. For example, \r\nassume that files are organised in subfolders for each sample, i.e. `main_folder / sample_name / subfolder`. The \r\nminimal working example is then:\r\n\r\n```python\r\nfrom mirp import extract_features\r\n\r\nfeature_data = extract_features(\r\n    image=\"path to main image directory\",\r\n    mask=\"path to main mask directory\",\r\n    image_sub_folder=\"image subdirectory structure relative to main image directory\",\r\n    mask_sub_folder=\"mask subdirectory structure relative to main mask directory\",\r\n    base_discretisation_method=\"fixed_bin_number\",\r\n    base_discretisation_n_bins=32\r\n)\r\n```\r\nThe above example will compute features sequentially. MIRP supports parallel processing using the `ray` package. \r\nFeature computation can be parallelized by specifying the `num_cpus` argument, e.g. `num_cpus=2` for two CPU threads.\r\n\r\n## Examples - Image Preprocessing for Deep Learning\r\nDeep learning-based radiomics is an alternative to using predefined quantitative features. MIRP supports \r\npreprocessing of images and masks using the same standardized workflow that is used for computing features.\r\n\r\nBelow is a minimal working example for preprocessing deep learning images. Note that MIRP uses the numpy notation \r\nfor indexing, i.e. indices are ordered [*z*, *y*, *x*].\r\n\r\n```python\r\nfrom mirp import deep_learning_preprocessing\r\n\r\nprocessed_images = deep_learning_preprocessing(\r\n    image=\"path to image\",\r\n    mask=\"path to mask\",\r\n    crop_size=[50, 224, 224]\r\n)\r\n```\r\n\r\n## Examples - Summarising Image Metadata\r\n\r\nMIRP can also summarise image metadata. This is particularly relevant for DICOM files that have considerable \r\nmetadata. Other files, e.g. NIfTI, only have metadata related to position and spacing of the image.\r\n\r\nBelow is a minimal working example for extracting metadata from a single image file.\r\n```python\r\nfrom mirp import extract_image_parameters\r\n\r\nimage_parameters = extract_image_parameters(\r\n    image=\"path to image\"\r\n)\r\n```\r\n\r\nMIRP also supports extracting metadata from multiple files. For example, assume that files are organised in \r\nsubfolders for each sample, i.e. `main_folder / sample_name / subfolder`. The minimal working example is then:\r\n```python\r\nfrom mirp import extract_image_parameters\r\n\r\nimage_parameters = extract_image_parameters(\r\n    image=\"path to main image directory\",\r\n    image_sub_folder=\"image subdirectory structure relative to main image directory\"\r\n)\r\n```\r\n\r\n## Examples - Finding labels\r\n\r\nMIRP can identify which labels are present in masks. For a single mask file, labels can be retrieved as follows:\r\n```python\r\nfrom mirp import extract_mask_labels\r\n\r\nmask_labels = extract_mask_labels(\r\n    mask=\"path to mask\"\r\n)\r\n```\r\n\r\nMIRP supports extracting labels from multiple masks. For example, assume that files are organised in subfolders for \r\neach sample, i.e. `main_folder / sample_name / subfolder`. The minimal working example is then:\r\n```python\r\nfrom mirp import extract_mask_labels\r\nmask_labels = extract_mask_labels(\r\n    mask=\"path to main mask directory\",\r\n    mask_sub_folder=\"mask subdirectory structure relative to main mask directory\"\r\n)\r\n```\r\n\r\n## Transitioning to version 2\r\n\r\nVersion 2 is a major refactoring of the previous code base. For users this brings the following noticeable changes:\r\n\r\n- MIRP was previously configured using two `xml` files: [`config_data.xml`](mirp/config_data.xml) for configuring\r\n  directories, data to be read, etc., and [`config_settings.xml`](mirp/config_settings.xml) for configuring experiments.\r\n  While these two files can still be used, MIRP can now be configured directly, without using these files.\r\n- The main functions of MIRP (`mainFunctions.py`) have all been re-implemented.\r\n  - `mainFunctions.extract_features` is now `extract_features` (functional form) or\r\n    `extract_features_generator` (generator). The replacements allow for both writing\r\n    feature values to a directory and returning them as function output. \r\n  - `mainFunctions.extract_images_to_nifti` is now `extract_images` (functional form) or\r\n     `extract_images_generator` (generator). The replacements allow for both writing \r\n     images to a directory (e.g., in NIfTI or numpy format) and returning them as function output.\r\n  - `mainFunctions.extract_images_for_deep_learning` has been replaced by \r\n    `deep_learning_preprocessing` (functional form) and \r\n    `deep_learning_preprocessing_generator` (generator).\r\n  - `mainFunctions.get_file_structure_parameters` and `mainFunctions.parse_file_structure` are deprecated, as the\r\n    the file import system used in version 2 no longer requires a rigid directory structure.\r\n  - `mainFunctions.get_roi_labels` is now `extract_mask_labels`.\r\n  - `mainFunctions.get_image_acquisition_parameters` is now `extract_image_parameters`.\r\n\r\nFor advanced users and developers, the following changes are relevant:\r\n- MIRP previously relied on `ImageClass` and `RoiClass` objects. These have been completely replaced by `GenericImage`\r\n  (and its subclasses, e.g. `CTImage`) and `BaseMask` objects, respectively. New image modalities can be added as\r\n  subclass of `GenericImage` in the `mirp.images` submodule.\r\n- File import, e.g. from DICOM or NIfTI files, in version 1 was implemented in an ad-hoc manner, and required a rigid\r\n  directory structure. Since version 2, file import is implemented using an object-oriented approach, and directory\r\n  structures are more flexible. File import of new modalities can be implemented as a relevant subclass of `ImageFile`.\r\n- MIRP now uses the `ray` package for parallel processing.\r\n\r\n# Citation info\r\nMIRP has been published in *Journal of Open Source Software*:\r\n```Zwanenburg A, Löck S. MIRP: A Python package for standardised radiomics. J Open Source Softw. 2024;9: 6413. doi:10.21105/joss.06413```\r\n\r\n# Contributing\r\nIf you have ideas for improving MIRP, please read the short [contribution guide](./CONTRIBUTING.md).\r\n\r\n# Developers and contributors\r\n\r\nMIRP is developed by:\r\n* Alex Zwanenburg\r\n\r\nWe would like thank the following contributors:\r\n* Stefan Leger\r\n* Sebastian Starke\r\n\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"mirp\"\nversion = \"2.3.0\"\ndescription = \"A package for standardised processing of medical imaging and computation of quantitative features.\"\nauthors = [\n    {name = \"Alex Zwanenburg\", email = \"alexander.zwanenburg@nct-dresden.de\"}\n]\nlicense = {file = \"LICENSE.txt\"}\nreadme = \"README.md\"\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"License :: OSI Approved :: European Union Public Licence 1.2 (EUPL 1.2)\",\n    \"Operating System :: OS Independent\",\n    \"Topic :: Scientific/Engineering :: Image Processing\",\n    \"Topic :: Scientific/Engineering :: Medical Science Apps.\"\n]\nrequires-python = \">=3.10\"\ndependencies = [\n    \"itk>=5.3.0\",\n    \"matplotlib>=3.7.0\",\n    \"numpy>=1.25\",\n    \"pandas>=2.0.0\",\n    \"pydicom>=2.4.0\",\n    \"pywavelets>=1.4.0\",\n    \"scikit-image>=0.20.0\",\n    \"scipy>=1.11\",\n    \"ray>=2.34.0\",\n    \"typing-extensions>=4.10; python_version<'3.11'\"\n]\n\n[project.urls]\nRepository = \"https://github.com/oncoray/mirp\"\nDocumentation = \"https://oncoray.github.io/mirp/\"\nChangelog = \"https://github.com/oncoray/mirp/blob/master/NEWS.md\"\n\n[project.optional-dependencies]\ntest = [\"pytest>=7.4.0\"]\ndocs = [\"sphinx>=5.0.0\", \"sphinx_rtd_theme>=2.0.0\", \"nbsphinx\"]\n\n[tool.pytest.ini_options]\naddopts = [\"--strict-markers\"]\nmarkers = [\n    \"ci: marks tests for continuous integration\"\n]\n#filterwarnings = [\n#   \"error:Arrays of 2-dimensional vectors are deprecated\"\n#]\n\n[tool.setuptools.packages.find]\nwhere = [\".\"]\ninclude = [\"mirp*\"]\nexclude = [\"docs*\"]\n\n[tool.setuptools.package-data]\nmirp = [\"config_settings.xml\", \"config_data.xml\"]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mlair",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/esde/machine-learning/mlair",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mmpxrt",
            "repo_link": "https://codebase.helmholtz.cloud/smid55/mmpxrt",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mdis",
            "repo_link": "https://git.gfz-potsdam.de/icdp-osg/mdis",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mode-behave",
            "repo_link": "https://github.com/julianreul/mode_behave",
            "content": {
                "codemeta": "",
                "readme": "Model Purpose and General Information\n=====================================\nMO|DE.behave is a Python-based software package for the estimation and \nsimulation of discrete choice models. The purpose of this software is to enable \nthe rapid quantitative analysis of survey data on choice behavior, \nutilizing advanced discrete choice methods. \nTherefore, MO|DE.behave incorporates estimation routines for conventional \nmultinomial logit models, as well as for mixed logit models with nonparametric \ndistributions.\nFurthermore, MO|DE.behave contains a set of post-processing tools for visualizing \nestimation and simulation results. Additionally, pre-estimated \ndiscrete choice simulation methods for transportation research are included to \nenrich the software package for this specific community.\n\nOn mixed logit models:\nIn recent years, a new modeling approach in the field of discrete choice theory \nbecame popular – the mixed logit model (see Train, K. (2009): \"Mixed logit\", \nin Discrete choice methods with simulation (pp. 76–93), Cambridge University Press). \nConventional discrete choice models only have a limited capability to describe \nthe heterogeneity of choice preferences within a base population, i.e., \nthe divergent choice behavior of different individuals or consumer groups can \nonly be studied to a limited degree. Mixed logit models overcome this deficiency and \nallow for the analysis of preference distributions across base populations.\n\nCommunication and contribution:\nWe encourage active participation in the software development process to adapt \nit to user needs. If you would like to contribute to the project or report any bugs, \nplease refer to the contribution-file or simply create an issue in the repository.\nFor any other interests (e.g. potential research collaborations), please \ndirectly contact the project maintainers via email, as indicated and \nupdated on GitHub.\n\nDocumentation on GitHub Pages: https://fzj-iek3-vsa.github.io/mode_behave/\n\nInstallation\n============\n1. Download or clone the repository to a local folder.\n#. Open (Anaconda) Prompt.\n#. Create a new environment from reference_environment.yml file (recommended)::\n\n      conda env create -f reference_environment.yml\n      \n#. Activate the environment with::\n\n      conda activate env_mode_behave\n      \n#. cd to the directory, where you stored the repository and where the setup.py file is located.\n\n#. In this folder run::\n    \n      pip install -e .\n      \n#. Alternatively, run::\n      \n      pip install mode-behave\n\n\nWorkflow\n========\n\nThis section explains an exemplary workflow from model setup to estimation \nand post-processing for a sub-sample of survey data on household decisions\non the type of propulsion technology when purchasing a new car.\nThe propulsion types are differentiated into \"ICEV: Internal combustion engine vehicle\",\n\"PHEV: Plug-in Hybrid Electric Vehicle\", \"BEV: Battery Electric Vehicle\", \nand \"FCEV: Fuel Cell Electric Vehicle\".\nThe data was collected in the year 2021 among German households and \nthe respective sub-sample is provided with the model (./mode_behave_public/InputData/example_data.csv).\nThe complete script accessible as well (./mode_behave_public/Deployments/example_estimation.py)\n\n1. Import model and required modules with::\n\n      import numpy as np\n      import pandas as pd\n      \n      import mode_behave_public as mb\n      \n2. Load data with (PATH_TO_DATA requires individual definition. See below for further documentation on required data formats.)::\n      \n      example_data = pd.read_csv(PATH_TO_DATA + \"example_data.csv\")\n      \n3. Definition of model parameters \"PURCHASE_PRICE\", \"RANGE\", and \"CHARGING_FUELING_TIME\". See section \"Structure of Parameters and Input Data\" for further information::\n      \n      param_fixed = []\n      param_random = ['PURCHASE_PRICE', 'RANGE', 'CHARGING_FUELING_TIME']\n\n      param_temp = {'constant': \n                        {\n                         'fixed':[],\n                         'random':[]\n                         },\n                    'variable':\n                        {\n                         'fixed': [],\n                         'random':[]\n                         }\n                    }\n\n      param_temp['variable']['fixed'] = param_fixed\n      param_temp['variable']['random'] = param_random  \n\n4. Initialize a model with::\n    \n      model = mb.Core(\n          param=param_temp, \n          data_in=example_data, \n          alt=4,\n          equal_alt=1,\n          include_weights=False\n          )\n      \n   The structure of the input data and the parameter-input are given below.\n\n5. Estimate the model with::\n\n    model.estimate_mixed_logit(\n        min_iter=10, \n        max_iter=1000,\n        tol=0.01,\n        space_method = 'std_value',\n        scale_space = 2,\n        max_shares = 1000,\n        bits_64=True,\n        t_stats_out=False\n        )\n      \n    The estimation of the mixed logit model can be modified by definition of keyword-arguments\n    during instantiation and within the estimation-method itself.\n    \n    Arguments for instantiation (ov.Core(...))::\n    \n        dict param:\n            Indicates the names of the model attributes. \n            The attribute-names shall be derived from the column names of the input data.\n        str data_name: \n            Indicates the name of the input data-file. \n        int alt: \n            Indicates the number of considered choice alternatives.\n        int equal_alt: \n            Indicates the maximum number of equal choice alternatives per choice set.\n    \n    Keyword-arguments for instantiation (ov.Core(...))::\n    \n        boolean include_weights: \n            If this is set to True, the model will search for a\n            column in the input-data, called \"weight\", which indicates the weight\n            for each observation. Defaults to True.\n    \n    Keyword-arguments for estimation-method (model.estimate_mixed_logit(...))::\n    \n        int min_inter: \n            Min. iterations for EM-algorithm.\n        int max_iter: \n            Max. iterations for EM-algorithm.\n        float tol: \n            Numerical tolerance of EM-algorithm.\n        bool bit_64: \n            Defaults to False. If set to True, all numbers are calculated\n            in 64-bit format, which increases precision, but also runtime.\n        str space_method: \n            Defines the chosen method to span the parameter space for the mixed logit estimation.\n        int scale_space: \n            Defines the size of the space, relative to the chosen space_method.\n        int max_shares: \n            Defines the maximum number of points to be observed in the parameter space.\n\n      \n6. Visualize the estimated preferences::\n\n    model.visualize_space(\n        k=2, \n        scale_individual=True, \n        cluster_method='kmeans', \n        external_points=np.array([model.initial_point]),\n        bw_adjust=0.03,\n        names_choice_options={0: \"ICEV\", 1: \"PHEV\", 2: \"BEV\", 3: \"FCEV\"}\n        )\n\n    Keyword-arguments::\n           \n        int k:\n            Number of preference clusters to be analyzed.\n        boolean scale_individual:\n            Scales the visualized preferences to fit the bounds (-1, 1),\n            to ease the comparability of preferences between different model attributes.\n        str cluster_method:\n            Defines the clustering algorithm for the identification of\n            diverging preference groups.\n        array external_points:\n            An array of preferences to be visualized in the figure as \n            a reference point. In this case, the mean preferences of the \n            base population are visualized with \"model.initial_point\"\n        float bw_adjust:\n            Smoothing parameter for the displayed preference distribution.\n        dict names_choice_options:\n            This dictionary can be used to define the names of the choice options.\n            \n7. Simulate the choice probabilities for each choice options in diverging scenarios (more exemplary use cases of this method can be found in the script example_estimation.py)::\n    \n    model.forecast(method='MNL', \n                sense_scenarios={\"Cheap_EV\": {\n                    \"PURCHASE_PRICE\": [[1.1], [1.1], [0.5], [0.5]]}\n                    },\n                names_choice_options={0: \"ICEV\", 1: \"PHEV\", 2: \"BEV\", 3: \"FCEV\"},\n                name_scenario='sensitivity'\n                )\n\n    Keyword-arguments::\n           \n        str method:\n            Defines the type of choice model to be used among \n            \"MNL\" (Multinomial logit), \"LC\" (Latent class), and \"MXL\" (Mixed logit) \n        dict sense_scenarios:\n            Can be used to define diverging scenarios from the base scenario,\n            which is defined by the mean values in the base data. The values\n            indicate scaling factors by which the attributes are changed.\n            E.g., a value of 1.1 for the attribute \"PURCHASE_PRICE\" indicates\n            a 10% increase in the purchase price for the respective choice option.\n        dict names_choice_options:\n            This dictionary can be used to define the names of the choice options.\n        str name_scenario:\n            This string can be defined to declare the scenario name. It is \n            used to store the generated visualization under this name \n            in the output folder \"./mode_behave_public/Visualizations/\"\n               \n\nTesting\n=======\n\nThe software includes testing routines, written with the package *unittest*, \nto ensure its functionality throughout the development process. \nThe first test-routine checks the functionality\nof the estimation routines (PATH: *./test/test_estimation.py*), while the second\ntest routine checks the functionality of simulation routines \n(PATH: *./test/test_simulation.py*)\n\nThese testing routines can be activated in two ways:\n\n1. Via GitHub Actions:\n    Whenever a new commit is pushed to the repository, GitHub Actions\n    are automatically triggered, which execute the test routines.\n    The test results are displayed in the GitHub Actions tab in the \n    software's repository online.\n2. Via manual execution:\n    Alternatively, the test routines can be called manually. You might chose\n    this option, if you develop the software locally and want to validate \n    your changes before pushing a new commit. To execute the existing test \n    routines manually, open the (Anaconda) prompt and enter these commands::\n        \n        cd \"PATH_TO_MODULE/test/\"\n        python -m unittest test_estimation.py\n        python -m unittest test_simulation.py\n        \n    These commands execute the two test routines for estimation and simulation.\n    Substitute *PATH_TO_MODULE* with the path to the repository's home\n    directory on your local machine.\n        \n\nIf new features are added to the software, there should also be new test\nroutines added, which check their sustained functionality thoughout the \ndevelopment process (test-driven development).\n    \n\nStructure of Parameters and Input Data\n======================================\n\n1. Input data\n\n   The input dataset contains the observations with which the model is \n   calibrated. The input data is called with the specified string of the\n   keyword-argument *data_in*. The input data must be loaded from .csv- or \n   .pickle-format before model initialization.\n   The data shall follow the structure below::\n   \n       Rows: \n           Observations.\n       \n       Columns:\n           One column per parameter of the utility function AND per alternative AND per equal alternative.\n           Specified as: **'Attribute_name_' + str(no_alternative) + str(no_equal_alternative)**\n           \n           One column for the choice-indication of EACH alternative AND per equal alternative.\n           Specified as: **choice_' + str(no_alternative) + str(no_equal_alternative)**\n           \n           One column per alternative AND per equal alternative, indicating the availability.\n           Specified as: **'av_' + str(no_alternative) + str(no_equal_alternative)**\n           \n           If a parameter is constant across alternatives or equal alternatives, then let the columns be equal.\n           \n           Furthermore, the observations can be given a weight. Therefore, an additional column needs to be provided, named 'weight'. - Without any further suffix.\n       \n       Index: The index shall start from '0'.\n          \n2. Initialization argument 'param':\n    \n   'param' is specified as a dictionary containing the attribute names of the \n   utility function, sorted by type::\n   \n       param['constant']['fixed']: \n           Attributes, which are constant over choice \n           options and fixed within the parameter space. \n       param['constant']['random']: \n           Attributes, which are constant over choice \n           options and randomly distributed over the parameter space. \n       param['variable']['fixed']: \n           Attributes, which vary over choice \n           options and are fixed within the parameter space. \n       param['variable']['random']: \n           Attributes, which vary over choice \n           options and are randomly distributed over the parameter space. \n     \n3. The vector x, containing the initial estimates for the logit coefficients.\n\n   The coefficients in vector x (solution vector of maximum likelihood optimization)\n   follow a certain structure (alternatives=alt)::\n   \n       x[:(alt-1)]: \n           ASC-constants for the alternatives 1-#of alternatives. ASC for choice option 0 defaults to 0.\n       x[(alt-1):(alt-1)+no_constant_fixed]: \n           Coefficients of constant and fixed attributes.\n       x[(alt-1)+no_constant_fixed:(alt-1)+(no_constant_fixed+no_constant_random)]: \n           Coefficients of constant and fixed attributes.   \n       x[(alt-1)+(no_constant_fixed+no_constant_random):(alt-1)+(no_constant_fixed+no_constant_random)+no_variable_fixed*alt]: \n           Coefficients of variable (thus multiplication with alternatives) \n           and fixed attributes.\n       x[(alt-1)+(no_constant_fixed+no_constant_random)+no_variable_fixed*alt:(alt-1)+(no_constant_fixed+no_constant_random)+(no_variable_fixed+no_variable_random)*alt]: \n           Coefficients of variable and random attributes.\n      \nTheoretical Background\n======================\nA mixed logit model is a multinomial logit model (MNL), in which the coefficients \ndo not take a single value, but are distributed over a parameter space. \nWithin this package, the mixed logit models \nare estimated on a discrete parameter space, which is specified by the researcher (nonparametric design).\nThe discrete subsets of the parameter space are called classes, \nanalogously to latent class models (LCM). The goal of the estimation procedure\nis to estimate the optimal share, i.e. weight, of each class within the discrete parameter space.\nThe algorithm roughly follows the procedure below:\n\n1. Estimate initial coefficients of a standard multinomial logit model.\n2. Specify a continuous parameter space for the random coefficients with\n   the mean and the standard deviation of each initially calculated random coefficient. \n   (The standard deviation can be calculated from a k-fold cross-validation.)\n   Alternatively, the parameter space can be defined via the absolute values\n   of the parameters.\n3. Draw points (maximum number of point = -max_shares-) from the parameter space via latin hypercube sampling.\n3. Estimate the optimal share for each drawn point with an expectation-maximization (EM) algorithm. (see Train, 2009)\n\n      \nFurther reading:\n\n* Train, K. (2009): \"Mixed logit\", in Discrete choice methods with simulation (pp. 76–93), Cambridge University Press\n* Train, K. (2008): \"EM algorithms for nonparametric estimation of mixing distributions\", in Journal of Choice Modelling, 1(1), 40–69, https://doi.org/10.1016/S1755-5345(13)70022-8\n* Train, K. (2016): \"Mixed logit with a flexible mixing distribution\", in Journal of Choice Modelling, 19, 40–53, https://doi.org/10.1016/j.jocm.2016.07.004\n* McFadden, D. and Train, K. (2000): \"Mixed MNL models for discrete response\", in Journal of Applied Econometrics, 15(5), 447-470, https://www.jstor.org/stable/2678603 \n\nPost-Analysis\n=============\n\n1. Access of estimated coefficients and summary statistics::\n        \n    model.shares: \n        Estimated shares of discrete classes within parameter space.\n    model.points: \n        Parameter space of random coefficients.\n    model.initial_point: \n        Coefficients of initially estimated logit model.\n     \n2. Visualization of parameter space::\n\n    model.visualize_space(**kwargs)\n      \n    int k:\n        k incidates the number of cluster centers, \n        to which the estimated random parameters \n        of the mixed logit model shall be attributed. \n        \n    The cluster centers indicate different potential choice or consumer groups. \n    This method clusters the estimated random preferences and shows \n    the position of the cluster centers as well as the overall distribution\n    of estimated random parameters across the whole parameter space.\n      \n3. Forecast with cluster centers::\n\n    model.forecast(method, **kwargs)\n                \n    str method:\n        \"method\" indicates the type of the discrete choice model (\"MNL\", \"MXL\", or \"LC\" for latent class).\n    int k:\n        Also \"k\" can be given to indicate the number of cluster centers which shall be analyzed.\n    dict sense_scenarios:\n        Indicates the relative change in the value of selected model attributes.\n        This keyword is useful for conducting sensitivity analyses.\n    list av_external:\n        This parameter is used to externally define the availabilities of certain\n        choice options. E.g., if a choice option shall be excluded from the simulation.\n        \n    This method forecasts the mean choice, based on the estimated parameters \n    of each cluster center and the attribute values of the base data. \n    It is a good reference point to study the diverging choice\n    behavior of each cluster center.\n\n4. Cluster the drawn points from the parameter space to similar preference groups (e.g. consumer groups)::\n\n    model.cluster_space(method, k, **kwargs)\n    \n    str method:\n        Indicates the clustering algorithm, e.g. kmeans. \n    int k:\n        Indicates the number of cluster centers.\n    \n    The output of this method is the classification of the drawn points\n    from the parameter space into clusters. The second output are\n    the calculated cluster centers. The clusters can be interpreted as consumer groups.\n\n5. Assignment of observations to cluster centers::\n    \n    model.assign_to_cluster(**kwargs)\n    \n    This method calculates probabilities for each observation in the base data,\n    which indicate the likelihood with which an observation belongs to a \n    cluster center (the method internally calls self.cluster_space to\n    determine the cluster centers). \n    This method is useful to characterize the consumer groups.\n          \nSimulation\n==========\n\nThe model incorporates a class **Simulation**, which contains customized\nmethods to simulate previously estimated choice models.\nIn order to simulate choice probabilities, the model must be instantiated as follows::\n\n   model = mb.Core(model_type = 'simulation', simulation_type = 'mode_choice')\n   \n   str simulation_type:\n       Specifies which kind of simulation shall be conducted.\n       Currently only MNL-simulations are implemented.\n\nThe following MNL-simulations are currently available:\n\n**MNL-Model for Mode-Choice (simulation_type = 'mode_choice')**::\n\n    model.simulate_mode_choice(agegroup, occupation, regiontype, distance, av)\n    \nThe method simulates the probability of mode choice for ten different modes\n(Walking, Biking, MIV-self, MIV-co, bus_near, train_near, train_city, bus_far, train_far, carsharing).\nInput parameters are the agegroup of the simulated agent (1: <18, 2: 18-65, 3: >65),\nthe occupation (1: full-time work, 2: part-time, 3: education, 4: no occupation),\nthe regiontype of residence (according to RegioStaR7 - BMVI classification),\ndistance (travel cost and time are derived from this variable, based on \ncost-assumptions for the year 2020. Also, the regiontype for the calculation\nof average speeds is assumed to be identical with the specified regiontype\nof the home location of the agent),\nas well as the availability of each mode in numpy-array format.\nFilename of pre-estimated model parameters: 'initial_point_mode'\n\n**MNL-model for the probability of the number of cars per households (simulation_type = 'car_ownership')**::\n\n   model.simulate_hh_cars(regiontype, hh_size,\n                        adults_working, children, htype, quali_opnv, sharing,\n                        relative_cost_per_car, age_adults)\n                         \nThe method simulates the probability, that a household owns 0-3+ cars (4 discrete alternatives).\nInput parameters are the regiontype of residence in I/O-format according to \nRegioStaR7 BMVI classification (e.g.: regiontype = 1 for \"Metropolis\"),\nthe household size (hh_size), the number of working adults (adults_working),\nthe number of children in the household (children), the housing type (htype)\nin I/O-format (e.g.: 1, if individual house, 0, if multi-apartment house),\nthe quality of public transport in the residence area (1: Very Bad, 2: Bad, 3: Good, 4: Very Good),\nwhether the household holds a carsharing-membership (sharing), the\nratio of the average car price divided by net monthly household income (relative_cost_per_car).\nAverage market prices can be derived from Kraus' vehicle cost model.\nLast input parameter is the average age of the adults, living in the household,\nscaled by *0.1!\n",
                "dependencies": "Pillow>=9.5.0\nmatplotlib>=3.7.1\nnumba>=0.56.4\nnumpy>=1.23.5\npandas>=2.0.1\npickleshare>=0.7.5\nscikit-learn>=1.2.2\nscikit-optimize>=0.9.0\nscipy>=1.10.1\nseaborn>=0.12.2\nfrom setuptools import setup\n\nsetup(name='mode_behave',\n      version='1.1.2',\n      description='Estimation and simulation of discrete choice models',\n      author='Julian Reul',\n      author_email='j.reul@fz-juelich.de',\n      url='https://github.com/julianreul/mode_behave',\n      license='MIT',\n      packages=['mode_behave_public'],\n      install_requires=[\n          'Pillow>=9.5.0',\n          'matplotlib>=3.7.1',\n          'numba>=0.56.4',\n          'numpy>=1.23.5',\n          'pandas>=2.0.1',\n          'pickleshare>=0.7.5',\n          'scikit-learn>=1.2.2',\n          'scikit-optimize>=0.9.0',\n          'scipy>=1.10.1',\n          'seaborn>=0.12.2'\n          ],    \n      include_package_data=True,\n      zip_safe=False)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/moovie",
            "repo_link": "https://jugit.fz-juelich.de/IBG-1/ModSim/MooViE",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mptrac",
            "repo_link": "https://github.com/slcs-jsc/mptrac",
            "content": {
                "codemeta": "",
                "readme": "# Massive-Parallel Trajectory Calculations\n\nMassive-Parallel Trajectory Calculations (MPTRAC) is a Lagrangian particle dispersion model for the analysis of atmospheric transport processes in the free troposphere and stratosphere.\n\n![logo](https://github.com/slcs-jsc/mptrac/blob/master/docs/logo/MPTRAC_320px.png)\n\n[![release (latest by date)](https://img.shields.io/github/v/release/slcs-jsc/mptrac)](https://github.com/slcs-jsc/mptrac/releases)\n[![commits since latest release (by SemVer)](https://img.shields.io/github/commits-since/slcs-jsc/mptrac/latest)](https://github.com/slcs-jsc/mptrac/commits/master)\n[![last commit](https://img.shields.io/github/last-commit/slcs-jsc/mptrac.svg)](https://github.com/slcs-jsc/mptrac/commits/master)\n[![top language](https://img.shields.io/github/languages/top/slcs-jsc/mptrac.svg)](https://github.com/slcs-jsc/mptrac/tree/master/src)\n[![code size in bytes](https://img.shields.io/github/languages/code-size/slcs-jsc/mptrac.svg)](https://github.com/slcs-jsc/mptrac/tree/master/src)\n[![codacy](https://api.codacy.com/project/badge/Grade/a9de7b2239f843b884d2a4eb583726c9)](https://app.codacy.com/gh/slcs-jsc/mptrac?utm_source=github.com&utm_medium=referral&utm_content=slcs-jsc/mptrac&utm_campaign=Badge_Grade_Settings)\n[![codecov](https://codecov.io/gh/slcs-jsc/mptrac/branch/master/graph/badge.svg?token=4X6IEHWUBJ)](https://codecov.io/gh/slcs-jsc/mptrac)\n[![tests](https://img.shields.io/github/actions/workflow/status/slcs-jsc/mptrac/tests.yml?branch=master&label=tests)](https://github.com/slcs-jsc/mptrac/actions)\n[![docs](https://img.shields.io/github/actions/workflow/status/slcs-jsc/mptrac/docs.yml?branch=master&label=docs)](https://slcs-jsc.github.io/mptrac)\n[![license](https://img.shields.io/github/license/slcs-jsc/mptrac.svg)](https://github.com/slcs-jsc/mptrac/blob/master/COPYING)\n[![doi](https://zenodo.org/badge/DOI/10.5281/zenodo.4400597.svg)](https://doi.org/10.5281/zenodo.4400597)\n\n## Features\n\n* MPTRAC calculates air parcel trajectories by solving the kinematic equation of motion using given horizontal wind and vertical velocity fields from global reanalyses or forecasts.\n* Mesoscale diffusion and subgrid-scale wind fluctuations are simulated using the Langevin equation to add stochastic perturbations to the trajectories. A new inter-parcel exchange module represents mixing of air.\n* Additional modules are implemented to simulate convection, sedimentation, exponential decay, gas and aqueous phase chemistry, and wet and dry deposition.\n* Meteorological data pre-processing code provides estimates of the boundary layer, convective available potential energy, geopotential heights, potential vorticity, and tropopause data.\n* Various output methods for particle, grid, ensemble, profile, sample, and station data. Gnuplot and ParaView interfaces for visualization.\n* MPI-OpenMP-OpenACC hybrid parallelization and distinct code optimizations for efficient use from single workstations to HPC and GPU systems.\n* Distributed open source under the terms of the GNU GPL.\n\n## Getting started\n\n### Prerequisites\n\nThis README file describes how to install MPTRAC on a Linux system.\n\nThe following software dependencies are required to compile MPTRAC:\n\n* the [GNU make](https://www.gnu.org/software/make) build tool\n* the C compiler of the [GNU Compiler Collection (GCC)](https://gcc.gnu.org)\n* the [GNU Scientific Library (GSL)](https://www.gnu.org/software/gsl) for numerical calculations\n* the [netCDF library](http://www.unidata.ucar.edu/software/netcdf) for file-I/O\n\nThe following optional software is required to enable additional features of MPTRAC:\n\n* the distributed version control system [Git](https://git-scm.com/) to access the code repository\n* the [HDF5 library](https://www.hdfgroup.org/solutions/hdf5) to enable the netCDF4 file format\n* the [Zstandard library](https://facebook.github.io/zstd) and the [zfp library](https://computing.llnl.gov/projects/zfp) for compressed meteo data\n* the [NVIDIA HPC Software Development Kit](https://developer.nvidia.com/hpc-sdk) for GPU support\n* an MPI library such as [OpenMPI](https://www.open-mpi.org) or [ParaStationMPI](https://github.com/ParaStation/psmpi) for HPC support\n* the graphing utility [gnuplot](http://www.gnuplot.info) for visualization\n\nSome of the software is provided along with the MPTRAC repository, please see next section.\n\n### Installation\n\nStart by downloading the latest or one of the previous [MPTRAC releases](https://github.com/slcs-jsc/mptrac/releases). Unzip the release file:\n\n    unzip mptrac-x.y.zip\n\nAlternatively, you can get the development version of the software from the GitHub repository:\n\n    git clone https://github.com/slcs-jsc/mptrac.git\n\nSeveral libraries shipped along with MPTRAC can be compiled and installed by running a build script:\n\n    cd [mptrac_directory]/libs\n    ./build.sh -a\n\nThen change to the source directory and edit the `Makefile` according to your needs:\n\n    cd [mptrac_directory]/src\n    emacs Makefile\n\nIn particular, you may want to check:\n\n* Edit the `LIBDIR` and `INCDIR` paths to point to the directories where the GSL, netCDF, and other libraries are located on your system.\n\n* By default, the MPTRAC binaries are linked statically, i.e., they can be copied and used on other machines. However, sometimes static compilation causes problems, e.g., in combination with dynamically compiled GSL and netCDF libraries or when using MPI or OpenACC. In this case, disable the `STATIC` flag and remember to set the `LD_LIBRARY_PATH` to include the paths to the shared libraries.\n\n* To make use of the MPI parallelization of MPTRAC, the `MPI` flag must be enabled. Further steps will require an MPI library such as OpenMPI to be available on your system. To make use of the OpenACC parallelization, the `GPU` flag must be enabled. The NVIDIA HPC SDK is required to compile the GPU code. MPTRAC's OpenMP parallelization is always enabled.\n\nNext, try compiling the code:\n\n    make [-j]\n\nTo run the test cases to check the installation, use\n\n    make check\n\nThis will run a series of tests sequentially. It will stop if any of the tests fail. Please check the log messages.\n\n### Run the example\n\nAn example is provided to illustrate how to simulate the dispersion of volcanic ash from the eruption of the Puyehue-Cordón Caulle volcano, Chile, in June 2011.\n\nThe example can be found in the `projects/example/` subdirectory. The `projects/` subdirectory can also be used to store the results of your own simulation experiments with MPTRAC.\n\nThe example simulation is controlled by a shell script:\n\n    cd mptrac/projects/example\n    ./run.sh\n\nSee the `run.sh` script for how to invoke MPTRAC programs such as `atm_init` and `atm_split` to initialize the trajectory seeds and `trac` to compute the trajectories.\n\nThe script generates simulation output in the `examples/data` subdirectory. The corresponding reference data can be found in `examples/data.ref`.\n\nA set of plots of the simulation output at different time steps after the eruption, generated by means of the `gnuplot` plotting tool, can be found in `examples/plots`. The plots should look similar to the output provided in `examples/plots.ref`.\n\nThis is an example showing the particle positions and grid output on 6th and 8th of June 2011:\n<p align=\"center\"><img src=\"projects/example/plots.ref/atm_2011_06_06_00_00.tab.png\" width=\"45%\"/> &emsp; <img src=\"projects/example/plots.ref/grid_2011_06_06_00_00.tab.png\" width=\"45%\"/></p>\n<p align=\"center\"><img src=\"projects/example/plots.ref/atm_2011_06_08_00_00.tab.png\" width=\"45%\"/> &emsp; <img src=\"projects/example/plots.ref/grid_2011_06_08_00_00.tab.png\" width=\"45%\"/></p>\n\n## Further information\n\nThese are the main scientific publications that provide information about MPTRAC:\n\n* Hoffmann, L., Baumeister, P. F., Cai, Z., Clemens, J., Griessbach, S., Günther, G., Heng, Y., Liu, M., Haghighi Mood, K., Stein, O., Thomas, N., Vogel, B., Wu, X., and Zou, L.: Massive-Parallel Trajectory Calculations version 2.2 (MPTRAC-2.2): Lagrangian transport simulations on graphics processing units (GPUs), Geosci. Model Dev., 15, 2731–2762, https://doi.org/10.5194/gmd-15-2731-2022, 2022.\n\n* Hoffmann, L., T. Rößler, S. Griessbach, Y. Heng, and O. Stein, Lagrangian transport simulations of volcanic sulfur dioxide emissions: Impact of meteorological data products, J. Geophys. Res. Atmos., 121, 4651-4673, https://doi.org/10.1002/2015JD023749, 2016. \n\nAdditional references are collected on the [references web page](https://slcs-jsc.github.io/mptrac/references/).\n\nMore detailed information for users of MPTRAC is provided in the [user manual](https://slcs-jsc.github.io/mptrac).\n\nInformation for developers of MPTRAC can be found in the [doxygen manual](https://slcs-jsc.github.io/mptrac/doxygen).\n\n## Contributing\n\nWe are interested in supporting operational and research applications with MPTRAC.\n\nYou can submit bug reports or feature requests on the [issue tracker](https://github.com/slcs-jsc/mptrac/issues).\n\nProposed code changes and fixes can be submitted as [pull requests](https://github.com/slcs-jsc/mptrac/pulls).\n\nPlease do not hesitate to contact us if you have any questions or need assistance.\n\n## License\n\nMPTRAC is being developed at the Jülich Supercomputing Centre, Forschungszentrum Jülich, Germany.\n\nMPTRAC is distributed under the terms of the [GNU General Public License v3.0](https://github.com/slcs-jsc/mptrac/blob/master/COPYING).\n\nPlease see the [citation file](https://github.com/slcs-jsc/mptrac/blob/master/CITATION.cff) for more information about citing the MPTRAC model in scientific publications.\n\n## Contact\n\nDr. Lars Hoffmann\n\nJülich Supercomputing Centre, Forschungszentrum Jülich\n\ne-mail: l.hoffmann@fz-juelich.de\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mss",
            "repo_link": "https://github.com/Open-MSS/MSS",
            "content": {
                "codemeta": "",
                "readme": "**Chat:**\n[![IRC: #mss-general on libera.chat](https://img.shields.io/badge/libera.chat-%23MSS_General-blue)](https://web.libera.chat/?channels=#mss-general)\n[![IRC: #mss-gsoc on libera.chat](https://img.shields.io/badge/libera.chat-%23MSS_GSoC-brightgreen)](https://web.libera.chat/?channels=#mss-gsoc)\n\n\nMission Support System Usage Guidelines\n=======================================\n\nWelcome to the Mission Support System software for planning\natmospheric research flights. This document is intended to point you\ninto the right direction in order to get the software working on your\ncomputer.\n\n\nInstalling MSS\n==============\n\nAutomatically\n-------------\n\n- For **Windows**, go [here](https://github.com/Open-MSS/mss-install/blob/main/Windows.bat?raw=1)\n    - Right click on the webpage and select \"Save as...\" to download the file\n    - Double click the downloaded file and follow further instructions\n        - For fully automatic installation, open cmd and execute it with `/Path/To/Windows.bat -a`\n- For **Linux/Mac**, go [here](https://github.com/Open-MSS/mss-install/blob/main/LinuxMac.sh?raw=1)\n    - Right click on the webpage and select \"Save as...\" to download the file\n    - Make it executable via `chmod +x LinuxMac.sh`\n    - Execute it and follow further instructions `./LinuxMac.sh`\n        - For fully automatic installation, run it with the -a parameter `./LinuxMac.sh -a`\n\nManually\n--------\n\nAs **Beginner** start with an installation of Miniforge\nGet [miniforge](https://github.com/conda-forge/miniforge#download) for your Operation System\n\n\nYou must install mss into a new environment to ensure the most recent\nversions for dependencies (On the Anaconda Prompt on Windows, you have\nto leave out the 'source' here and below).\n\n```\n  $ mamba create -n mssenv\n  $ mamba activate mssenv\n  (mssenv) $ mamba install mss python\n```\nFor updating an existing MSS installation to the current version, it is\nbest to install it into a new environment. If an existing environment\nshall be updated, it is important to update all packages in this\nenvironment.\n\n```\n  $ mamba activate mssenv\n  (mssenv) $ msui --update\n```\n\nIt is possible to list all versions of `mss` available on your platform with:\n\n```\n    $ mamba search mss --channel conda-forge\n```\n\nFor a simple test you can setup a demodata wms server and start a msolab server with default settings\n\n```\n  (mssenv) $ mswms_demodata --seed\n  (mssenv) $ export PYTHONPATH=~/mss\n  (mssenv) $ mswms &\n  (mssenv) $ mscolab start &\n  (mssenv) $ msui\n```\n\n\n\n\nCurrent release info\n====================\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/mss.svg)](https://anaconda.org/conda-forge/mss)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6572620.svg)](https://doi.org/10.5281/zenodo.6572620)\n[![JuRSE Code Pick](https://img.shields.io/badge/JuRSE_Code_Pick-July_2024-blue)](https://www.fz-juelich.de/en/rse/jurse-community/jurse-code-of-the-month/july-2024)\n[![Conda Platforms](https://img.shields.io/conda/pn/conda-forge/mss.svg)](https://anaconda.org/conda-forge/mss)\n[![DOCS](https://img.shields.io/badge/%F0%9F%95%AE-docs-green.svg)](https://mss.rtfd.io)\n[![Conda Recipe](https://img.shields.io/badge/recipe-mss-green.svg)](https://anaconda.org/conda-forge/mss)\n[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/mss.svg)](https://anaconda.org/conda-forge/mss)\n[![Coverage Status](https://coveralls.io/repos/github/Open-MSS/MSS/badge.svg?branch=develop)](https://coveralls.io/github/Open-MSS/MSS?branch=develop)\n\n\n\n\n\nPublications\n============\n\nPlease read the reference documentation\n\n   Bauer, R., Grooß, J.-U., Ungermann, J., Bär, M., Geldenhuys, M., and Hoffmann, L.: The Mission Support\n   System (MSS v7.0.4) and its use in planning for the SouthTRAC aircraft campaign, Geosci.\n   Model Dev., 15, 8983–8997, https://doi.org/10.5194/gmd-15-8983-2022, 2022.\n\n   Rautenhaus, M., Bauer, G., and Doernbrack, A.: A web service based\n   tool to plan atmospheric research flights, Geosci. Model Dev., 5,\n   55-71, https://doi.org/10.5194/gmd-5-55-2012, 2012.\n\nand the paper's Supplement (which includes a tutorial) before using the\napplication. The documents are available at:\n\n- http://www.geosci-model-dev.net/5/55/2012/gmd-5-55-2012.pdf\n- http://www.geosci-model-dev.net/5/55/2012/gmd-5-55-2012-supplement.pdf\n\nFor copyright information, please see the files NOTICE and LICENSE, located\nin the same directory as this README file.\n\n\n   When using this software, please be so kind and acknowledge its use by\n   citing the above mentioned reference documentation in publications,\n   presentations, reports, etc. that you create. Thank you very much.\n\n",
                "dependencies": "[tool.codespell]\nexclude-file = \"codespell-ignored-lines.txt\"\nignore-words-list = [\n    \"PRES\",\n    \"degreeE\",\n    \"doubleClick\",\n    \"indexIn\",\n    \"socio-economic\",\n]\n\n# -*- coding: utf-8 -*-\n\"\"\"\n\n    mslib.setup\n    ~~~~~~~~~~~~~~~~\n\n    setuptools script\n\n    This file is part of MSS.\n\n    :copyright: Copyright 2016-2017 Reimar Bauer\n    :copyright: Copyright 2016-2024 by the MSS team, see AUTHORS.\n    :license: APACHE-2.0, see LICENSE for details.\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\"\"\"\n\n# The README.txt file should be written in reST so that PyPI can use\n# it to generate your project's PyPI page.\nimport os\nfrom past.builtins import execfile\nfrom setuptools import setup, find_namespace_packages\nlong_description = open('README.md').read()\nexecfile('mslib/version.py')\n\nconsole_scripts = [\n    \"mscolab = mslib.mscolab.mscolab:main\",\n    \"mss = mslib.msui.mss:main\",\n    \"mssautoplot = mslib.utils.mssautoplot:main\",\n    \"msui = mslib.msui.msui:main\",\n    \"mswms = mslib.mswms.mswms:main\",\n    \"mswms_demodata = mslib.mswms.demodata:main\"]\nif os.name != 'nt':\n    console_scripts.append('msidp = mslib.msidp.idp:main')\n\nsetup(\n    name=\"mss\",\n    version=__version__,  # noqa\n    description=\"MSS - Mission Support System\",\n    long_description=long_description,\n    classifiers=\"Development Status :: 5 - Production/Stable\",\n    keywords=\"mslib\",\n    maintainer=\"Reimar Bauer\",\n    maintainer_email=\"rb.proj@gmail.com\",\n    author=\"Marc Rautenhaus\",\n    author_email=\"wxmetvis@posteo.de\",\n    license=\"Apache-2.0\",\n    url=\"https://github.com/Open-MSS/MSS\",\n    platforms=\"any\",\n    packages=find_namespace_packages(include=[\"mslib\", \"mslib.*\"]),\n    namespace_packages=[],\n    include_package_data=True,\n    zip_safe=False,\n    install_requires=[],  # we use conda build recipe\n    entry_points=dict(\n        console_scripts=console_scripts,\n    ),\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mtress",
            "repo_link": "https://github.com/mtress/mtress",
            "content": {
                "codemeta": "",
                "readme": "# Model Template for Renewable Energy Supply Systems (MTRESS)\n\nThis is a generic model for [oemof.solph](https://github.com/oemof/oemof-solph/)\nthat provides a variety of possible technology combinations for energy supply systems.\nIt is tailored for optimising control strategies fulfilling fixed demand time series\nfor electricity, heat, and domestic hot water using any selected combination\nof the implemented supply technologies.\n\nThe development of Version 2 was funded by the Federal Ministry for Economic Affairs and Energy (BMWi)\nand the Federal Ministry of Education and Research (BMBF) of Germany\nin the project ENaQ (project number 03SBE111).\nThe development of the heat sector formulations in Version 3 was funded by the Federal Ministry of\nEducation and Research (BMBF) of Germany in the project Wärmewende Nordwest (project number 03SF0624).\n\n\n## Installation\n\nMTRESS depends on solph, which is automatically instaled using pip\nif you `pip install mtress`. However, pip will not install a solver,\nto perform the actual optimisation. Please refer to the\n[documentation of solph](https://oemof-solph.readthedocs.io/en/v0.4.4/readme.html#installing-a-solver)\nto learn how to install a solver.\n\n\n## Contributing\n\nYou are welcome to contribute to MTRESS. We use [Black code style](https://black.readthedocs.io/),\nand put our code under [MIT license](LICENSE). When contributing, you need to do the same.\nFor smaller changes, you can just open a merge request. If you plan something bigger,\nplease open an issue first, so that we can discuss beforehand and avoid double work.\n\n\n## Contact\n\nThe software development is administrated by [Patrik Schönfeldt](mailto:patrik.schoenfeldt@dlr.de),\nfor general questions please contact him. Individual authors may leave their contact information\nin the [citation.cff](CITATION.cff).\n\n",
                "dependencies": "graphviz\noemof.solph>=0.5.1,<0.6\noemof.thermal>=0.0.6.dev1\npandas>=1.3.4\nPyYAML>=6.0\nnumpy>=1.21.4\nsetuptools>=59.1.1\npvlib\n\nimport os\nfrom setuptools import find_packages, setup\n\n\ndef read(file_name):\n    return open(os.path.join(os.path.dirname(__file__), file_name)).read()\n\nsetup(\n    name=\"mtress\",\n    version=\"3.0.0a2\",\n    url=\"https://github.com/mtress/mtress\",\n    author=\"Deutsches Zentrum für Luft- und Raumfahrt e.V. (DLR)\",\n    author_email=\"patrik.schoenfeldt@dlr.de\",\n    packages=find_packages(),\n    classifiers=[\n        # complete classifier list:\n        # http://pypi.python.org/pypi?%3Aaction=list_classifiers\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: Unix\",\n        \"Operating System :: POSIX\",\n        \"Operating System :: Microsoft :: Windows\",\n        \"Operating System :: MacOS\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n    ],\n    python_requires=\">=3.10\",\n    long_description=read(\"README.md\"),\n    long_description_content_type=\"text/markdown\",\n    zip_safe=False,\n    install_requires=read(\"requirements.txt\"),\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/multiphase-code-repository-by-hzdr",
            "repo_link": "https://codebase.helmholtz.cloud/fwdc/multiphase/code",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/nest",
            "repo_link": "https://github.com/nest/nest-simulator",
            "content": {
                "codemeta": "",
                "readme": "# The Neural Simulation Tool - NEST\n\n[![Documentation](https://img.shields.io/readthedocs/nest-simulator?logo=readthedocs&logo=Read%20the%20Docs&label=Documentation)](https://nest-simulator.org/documentation)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/2218/badge)](https://bestpractices.coreinfrastructure.org/projects/2218)\n[![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/nest/nest-simulator/badge)](https://scorecard.dev/viewer/?uri=github.com/nest/nest-simulator)\n[![License](http://img.shields.io/:license-GPLv2+-green.svg)](http://www.gnu.org/licenses/gpl-2.0.html)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10834751.svg)](https://doi.org/10.5281/zenodo.10834751)\n\n[![Latest release](https://img.shields.io/github/release/nest/nest-simulator.svg?color=brightgreen&label=latest%20release)](https://github.com/nest/nest-simulator/releases)\n[![GitHub contributors](https://img.shields.io/github/contributors/nest/nest-simulator?logo=github)](https://github.com/nest/nest-simulator)\n[![GitHub commit activity](https://img.shields.io/github/commit-activity/y/nest/nest-simulator?logo=github&color=%23ff6633)](https://github.com/nest/nest-simulator)\n\n[![Ubuntu version](https://img.shields.io/badge/ubuntu-(PPA)-blue?logo=debian)](https://nest-simulator.readthedocs.io/en/latest/installation/)\n[![Fedora package](https://img.shields.io/fedora/v/nest?logo=fedora)](https://src.fedoraproject.org/rpms/nest)\n[![conda-forge version](https://img.shields.io/conda/vn/conda-forge/nest-simulator.svg?logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/nest-simulator)\n[![Homebrew version](https://img.shields.io/homebrew/v/nest.svg?logo=apple)](https://formulae.brew.sh/formula/nest)\n[![Docker Image Version](https://img.shields.io/docker/v/nest/nest-simulator?color=blue&label=docker&logo=docker&logoColor=white&sort=semver)](https://hub.docker.com/r/nest/nest-simulator)\n[![Virtual applicance](https://img.shields.io/badge/VM-v3.7-blue?logo=CodeSandbox)](https://nest-simulator.readthedocs.io/en/latest/installation/livemedia.html#live-media)\n\n[![YouTube Video Views](https://img.shields.io/youtube/views/K7KXmIv6ROY?style=social)](https://www.youtube.com/results?search_query=nest-simulator+neurons)\n[![Twitter Follow](https://img.shields.io/twitter/follow/nestsimulator?style=social)](https://twitter.com/nestsimulator)\n\nNEST is a simulator for spiking neural network models that focuses on the\ndynamics, size and structure of neural systems rather than on the exact\nmorphology of individual neurons.\n\nA NEST simulation tries to follow the logic of an electrophysiological\nexperiment that takes place inside a computer with the difference that the\nneural system to be investigated must be defined by the experimenter.\n\nNEST is ideal for networks of spiking neurons of any size, for example:\n\n- Models of information processing, e.g., in the visual or auditory cortex of\n  mammals,\n- Models of network activity dynamics, e.g., laminar cortical networks or\n  balanced random networks,\n- Models of learning and plasticity.\n\n## Key features of NEST\n\n* NEST provides a Python interface or a stand-alone application\n* NEST provides a large collection of [neurons and synapse models](https://nest-simulator.org/documentation/models/index.html)\n* NEST provides numerous [example network scripts](https://nest-simulator.org/documentation/examples/index.html) along with\n  [tutorials and guides](https://nest-simulator.org/documentation/get-started_index.html) to help you develop your simulation\n* NEST has a large community of experienced developers and users; NEST was first released in 1994 under the name SYNOD, and has been extended and improved ever since\n* NEST is extensible: you can extend NEST by adding your own modules\n* NEST is scalable: Use NEST on your laptop or the largest supercomputers\n* NEST is memory efficient: It makes the best use of your multi-core computer and compute clusters with minimal user intervention\n* NEST is an open source project and is licensed under the GNU General Public License v2 or later\n* NEST employs continuous integration workflows in order to maintain high code quality standards for correct and reproducible simulations\n\n\n## Documentation\n\nPlease visit our [online documentation](https://nest-simulator.org/documentation) for details on installing and using NEST.\n\n\n## Cite NEST\n\nIf you use NEST as part of your research, please cite the *version* of NEST you used.\nThe full citation for each release can be found on [Zenodo](https://zenodo.org/search?q=title%3ANEST%20AND%20-description%3Agraphical%20AND%20simulator&l=list&p=1&s=10&sort=publication-desc)\n\nFor general citations, please use\n\n`Gewaltig M-O & Diesmann M (2007) NEST (Neural Simulation Tool) Scholarpedia 2(4):1430.`\n\n## Contact\n\n\nIf you need help or would like to discuss an idea or issue,\njoin our [maling list](https://nest-simulator.org/documentation/developer_space/guidelines/mailing_list_guidelines.html),\nwhere we encourage active participation from our developers and users to share their knowledge and experience with NEST.\n\n\nYou can find other [ways to get in touch here](https://nest-simulator.org/documentation/community.html).\n\n\n## Contribute\n\nNEST is built on an active community and we welcome contributions to our code and documentation.\n\n\nFor bug reports, feature requests, documentation improvements, or other issues,\nyou can create a [GitHub issue](https://github.com/nest/nest-simulator/issues/new/choose),\n\nFor working with NEST code and documentation, you can find guidelines for contributions\n[in our documentation](https://nest-simulator.org/documentation/developer_space/index.html#contribute-to-nest)\n\n\n## Publications\n\nYou can find a list of NEST [related publications here](https://www.nest-simulator.org/publications/).\n\n## License\n\n\nNEST is open source software and is licensed under the [GNU General Public\nLicense v2](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html) or\nlater.\n\nGeneral information on the NEST Initiative can be found at\nits homepage at https://www.nest-initiative.org.\n\n",
                "dependencies": "# CMakeLists.txt\n#\n# This file is part of NEST.\n#\n# Copyright (C) 2004 The NEST Initiative\n#\n# NEST is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 2 of the License, or\n# (at your option) any later version.\n#\n# NEST is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with NEST.  If not, see <http://www.gnu.org/licenses/>\n\ncmake_minimum_required( VERSION 3.19 )\n\n# add cmake modules: for all `include(...)` first look here\nlist( APPEND CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake )\n\nproject( nest CXX C )\nset( NEST_USER_EMAIL \"users@nest-simulator.org\" )\n\ninclude( ColorMessages )\n\n# check if the given CMAKE_INSTALL_PREFIX is not empty\nif(\"${CMAKE_INSTALL_PREFIX}\" STREQUAL \"\")\n  printError(\"CMAKE_INSTALL_PREFIX cannot be an empty string\")\nendif()\n\n# handle relative installation prefixes\nif( NOT IS_ABSOLUTE ${CMAKE_INSTALL_PREFIX})\n    # convert relative path to absolute path\n    get_filename_component(absPath ${CMAKE_INSTALL_PREFIX} ABSOLUTE BASE_DIR ${CMAKE_BINARY_DIR})\n    set(CMAKE_INSTALL_PREFIX ${absPath})\n    printInfo(\"Relative CMAKE_INSTALL_PREFIX has been converted to absolute path ${CMAKE_INSTALL_PREFIX}\")\nendif()\n\n################################################################################\n##################         All User Defined options           ##################\n################################################################################\n\n# use Python to build PyNEST\nset( with-python ON CACHE STRING \"Build PyNEST [default=ON].\" )\noption( cythonize-pynest \"Use Cython to cythonize pynestkernel.pyx [default=ON]. If OFF, PyNEST has to be build from a pre-cythonized pynestkernel.pyx.\" ON )\n\n# select parallelization scheme\nset( with-mpi OFF CACHE STRING \"Build with MPI parallelization [default=OFF].\" )\nset( with-openmp ON CACHE BOOL \"Build with OpenMP multi-threading [default=ON]. Optionally set OMP compiler flags.\" )\n\n# external libraries\nset( with-libneurosim OFF CACHE STRING \"Build with libneurosim [default=OFF]. Optionally give the directory where libneurosim is installed.\" )\nset( with-music OFF CACHE STRING \"Build with MUSIC [default=OFF]. Optionally give the directory where MUSIC is installed.\" )\nset( with-sionlib OFF CACHE STRING \"Build with SIONlib [default=OFF]. Optionally give the directory where sionlib is installed.\" )\nset( with-boost ON CACHE STRING \"Build with Boost [default=ON]. To set a specific Boost installation, give the install path.\" )\nset( with-hdf5 OFF CACHE STRING \"Find a HDF5 library. To set a specific HDF5 installation, set install path. [default=ON]\" )\nset( with-readline ON CACHE STRING \"Build with GNU Readline library [default=ON]. To set a specific library, give the install path.\" )\nset( with-ltdl ON CACHE STRING \"Build with ltdl library [default=ON]. To set a specific ltdl, give the  install path. NEST uses ltdl for dynamic loading of external user modules.\" )\nset( with-gsl ON CACHE STRING \"Build with the GSL library [default=ON]. To set a specific library, give the install path.\" )\n\n# NEST properties\nset( with-modelset \"full\" CACHE STRING \"The modelset to include. Sample configurations are in the modelsets directory. This option is mutually exclusive with -Dwith-models. [default=full].\" )\nset( with-models OFF CACHE STRING \"The models to include as a semicolon-separated list of model headers (without the .h extension). This option is mutually exclusive with -Dwith-modelset. [default=OFF].\" )\nset( tics_per_ms \"1000.0\" CACHE STRING \"Specify elementary unit of time [default=1000 tics per ms].\" )\nset( tics_per_step \"100\" CACHE STRING \"Specify resolution [default=100 tics per step].\" )\nset( with-detailed-timers OFF CACHE STRING \"Build with detailed internal time measurements [default=OFF]. Detailed timers can affect the performance.\" )\nset( target-bits-split \"standard\" CACHE STRING \"Split of the 64-bit target neuron identifier type [default='standard']. 'standard' is recommended for most users. If running on more than 262144 MPI processes or more than 512 threads, change to 'hpc'.\" )\n\n# generic build configuration\noption( static-libraries \"Build static executable and libraries [default=OFF]\" OFF )\nset( with-optimize ON CACHE STRING \"Enable user defined optimizations [default=ON (uses '-O2')]. When OFF, no '-O' flag is passed to the compiler. Explicit compiler flags can be given; separate multiple flags by ';'.\" )\nset( with-warning ON CACHE STRING \"Enable user defined warnings [default=ON (uses '-Wall')]. Separate  multiple flags by ';'.\" )\nset( with-debug OFF CACHE STRING \"Enable user defined debug flags [default=OFF]. When ON, '-g' is used. Separate  multiple flags by ';'.\" )\nset( with-cpp-std \"c++17\" CACHE STRING \"C++ standard to use for compilation [default='c++17'].\" )\nset( with-intel-compiler-flags OFF CACHE STRING \"User defined flags for the Intel compiler [default='-fp-model strict']. Separate multiple flags by ';'.\" )\nset( with-libraries OFF CACHE STRING \"Link additional libraries [default=OFF]. Give full path. Separate multiple libraries by ';'.\" )\nset( with-includes OFF CACHE STRING \"Add additional include paths [default=OFF]. Give full path without '-I'. Separate multiple include paths by ';'.\" )\nset( with-defines OFF CACHE STRING \"Additional defines, e.g. '-DXYZ=1' [default=OFF]. Separate multiple defines by ';'.\" )\n\n# documentation build configuration\nset( with-userdoc OFF CACHE STRING \"Build user documentation [default=OFF]\")\nset( with-devdoc OFF CACHE STRING \"Build developer documentation [default=OFF]\")\n\nset( with-full-logging OFF CACHE STRING \"Write debug output to 'dump_<num_ranks>_<rank>.log' file [default=OFF]\")\n\n################################################################################\n##################      Project Directory variables           ##################\n################################################################################\n\n# In general use the CMAKE_INSTALL_<dir> and CMAKE_INSTALL_FULL_<dir> vars from\n# GNUInstallDirs (included after calling nest_process_with_python()), but the\n# CMAKE_INSTALL_DATADIR is usually just CMAKE_INSTALL_DATAROOTDIR\n# and we want it to be CMAKE_INSTALL_DATAROOTDIR/PROJECT_NAME\nset( CMAKE_INSTALL_DATADIR \"share/${PROJECT_NAME}\" CACHE STRING \"Relative directory, where NEST installs its data (share/nest)\" )\n\n################################################################################\n##################           Find utility programs            ##################\n################################################################################\n\nfind_program( SED NAMES sed gsed )\n\n################################################################################\n##################                Load includes               ##################\n################################################################################\n\n# This include checks the symbols, etc.\ninclude( CheckIncludesSymbols )\n\n# These includes publish function names.\ninclude( ProcessOptions )\ninclude( CheckExtraCompilerFeatures )\ninclude( ConfigureSummary )\ninclude( GetTriple )\n\n# get triples arch-vendor-os\nget_host_triple( NEST_HOST_TRIPLE NEST_HOST_ARCH NEST_HOST_VENDOR NEST_HOST_OS )\nget_target_triple( NEST_TARGET_TRIPLE NEST_TARGET_ARCH NEST_TARGET_VENDOR NEST_TARGET_OS )\n\n# Process the command line arguments\n# IMPORTANT: Do not change the order of nest_process_with_python() and include( GNUInstallDirs )!\n#            If NEST is built with Python, nest_process_with_python() defaults CMAKE_INSTALL_PREFIX\n#            to the active virtual Python environment. This effects the inclusion\n#            of GNUInstallDirs defining CMAKE_INSTALL_<dir> and CMAKE_INSTALL_FULL_<dir>.\nnest_process_with_python()\ninclude( GNUInstallDirs )\nnest_post_process_with_python()\nnest_process_with_std()\nnest_process_with_intel_compiler_flags()\nnest_process_with_warning()\nnest_process_with_libraries()\nnest_process_with_includes()\nnest_process_with_defines()\nnest_process_static_libraries()\nnest_process_tics_per_ms()\nnest_process_tics_per_step()\nnest_process_with_libltdl()\nnest_process_with_readline()\nnest_process_with_gsl()\nnest_process_with_openmp()\nnest_process_with_mpi()\nnest_process_with_detailed_timers()\nnest_process_with_libneurosim()\nnest_process_with_music()\nnest_process_with_sionlib()\nnest_process_with_mpi4py()\nnest_process_with_boost()\nnest_process_with_hdf5()\nnest_process_target_bits_split()\nnest_process_userdoc()\nnest_process_devdoc()\nnest_process_full_logging()\n\nnest_process_models()\n\n# These two function calls must come last, as to prevent unwanted interactions of the newly set flags\n# with detection/compilation operations carried out in earlier functions. The optimize/debug flags set\n# using these functions should only apply to the compilation of NEST, not to that of test programs\n# generated by CMake when it tries to detect compiler options or such.\nnest_process_with_optimize()\nnest_process_with_debug()\n\nnest_get_color_flags()\nset( CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${NEST_C_COLOR_FLAGS}\" )\nset( CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${NEST_CXX_COLOR_FLAGS}\" )\n\n# check additionals\nnest_check_exitcode_abort()\nnest_check_exitcode_segfault()\nnest_check_have_cmath_makros_ignored()\nnest_check_have_alpha_cxx_std_bug()\nnest_check_have_sigusr_ignored()\nnest_check_have_static_template_declaration_fail()\nnest_check_have_stl_vector_capacity_base_unity()\nnest_check_have_stl_vector_capacity_doubling()\nnest_check_have_xlc_ice_on_using()\nnest_check_have_std_nan()\nnest_check_have_std_isnan()\nnest_check_random123()\n\ninclude( NestVersionInfo )\nget_version_info()\nprintInfo(\"Done configuring NEST version: ${NEST_VERSION}\")\n\nenable_testing()\nset( TEST_OPTS \"\" )\n\nif ( HAVE_PYTHON )\n  set( TEST_OPTS \"${TEST_OPTS};--with-python=${PYTHON}\" )\nendif ()\n\nif ( HAVE_MUSIC )\n  set( TEST_OPTS \"${TEST_OPTS};--with-music=${MUSIC_EXECUTABLE}\" )\nendif ()\n\nadd_custom_target( installcheck\n  COMMAND ${CMAKE_COMMAND} -E env\n    ${CMAKE_INSTALL_FULL_DATADIR}/testsuite/do_tests.sh\n\t--prefix=${CMAKE_INSTALL_PREFIX}\n\t${TEST_OPTS}\n  WORKING_DIRECTORY \"${PROJECT_BINARY_DIR}\"\n  COMMENT \"Executing NEST's testsuite...\"\n)\n\n# N.B. to ensure \"make install\" is always run before \"make installcheck\", we\n# would ideally like to add:\n#   add_dependencies( installcheck install )\n# However, an issue in CMake at time of writing (May 2020, see\n# https://gitlab.kitware.com/cmake/cmake/-/issues/8438) precludes us from doing\n# so.\n\n################################################################################\n##################        Define Subdirectories here          ##################\n################################################################################\n\nadd_subdirectory( doc )\nadd_subdirectory( bin )\nadd_subdirectory( examples )\nadd_subdirectory( build_support )\nadd_subdirectory( lib )\nadd_subdirectory( libnestutil )\nadd_subdirectory( models )\nadd_subdirectory( sli )\nadd_subdirectory( nest )\nadd_subdirectory( nestkernel )\nadd_subdirectory( thirdparty )\nadd_subdirectory( testsuite )\nif ( HAVE_PYTHON )\n  add_subdirectory( pynest )\nendif ()\n\n################################################################################\n##################           Summary of flags                 ##################\n################################################################################\n\n# used in nest-config\n\n# all compiler flags\nif ( NOT CMAKE_BUILD_TYPE OR \"${CMAKE_BUILD_TYPE}\" STREQUAL \"None\" )\n  set( ALL_CFLAGS \"${CMAKE_C_FLAGS}\" )\n  set( ALL_CXXFLAGS \"${CMAKE_CXX_FLAGS}\" )\nelseif ( ${CMAKE_BUILD_TYPE} STREQUAL \"Debug\" )\n  set( ALL_CFLAGS \"${CMAKE_C_FLAGS}   ${CMAKE_C_FLAGS_DEBUG}\" )\n  set( ALL_CXXFLAGS \"${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_DEBUG}\" )\nelseif ( ${CMAKE_BUILD_TYPE} STREQUAL \"Release\" )\n  set( ALL_CFLAGS \"${CMAKE_C_FLAGS}   ${CMAKE_C_FLAGS_RELEASE}\" )\n  set( ALL_CXXFLAGS \"${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_RELEASE}\" )\nelseif ( ${CMAKE_BUILD_TYPE} STREQUAL \"RelWithDebInfo\" )\n  set( ALL_CFLAGS \"${CMAKE_C_FLAGS}   ${CMAKE_C_FLAGS_RELWITHDEBINFO}\" )\n  set( ALL_CXXFLAGS \"${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_RELWITHDEBINFO}\" )\nelseif ( ${CMAKE_BUILD_TYPE} STREQUAL \"MinSizeRel\" )\n  set( ALL_CFLAGS \"${CMAKE_C_FLAGS}   ${CMAKE_C_FLAGS_MINSIZEREL}\" )\n  set( ALL_CXXFLAGS \"${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_MINSIZEREL}\" )\nelse ()\n  printError( \"Unknown build type: '${CMAKE_BUILD_TYPE}'\" )\nendif ()\nif ( with-defines )\n  foreach ( def ${with-defines} )\n    set( ALL_CFLAGS \"${def} ${ALL_CFLAGS}\" )\n    set( ALL_CXXFLAGS \"${def} ${ALL_CXXFLAGS}\" )\n  endforeach ()\nendif ()\n# add sionlib defines\nforeach ( def ${SIONLIB_DEFINES} )\n    set( ALL_CFLAGS \"${ALL_CFLAGS} ${def}\" )\n    set( ALL_CXXFLAGS \"${ALL_CXXFLAGS} ${def}\" )\nendforeach ()\n\n# libraries required to link extension modules\nset( MODULE_LINK_LIBS\n  \"-lnest\"\n  \"-lsli\"\n  \"${LTDL_LIBRARIES}\"\n  \"${READLINE_LIBRARIES}\"\n  \"${GSL_LIBRARIES}\"\n  \"${LIBNEUROSIM_LIBRARIES}\"\n  \"${MUSIC_LIBRARIES}\"\n  \"${MPI_CXX_LIBRARIES}\"\n  \"${OpenMP_CXX_LIBRARIES}\"\n  \"${SIONLIB_LIBRARIES}\"\n  \"${BOOST_LIBRARIES}\" )\n\nif ( with-libraries )\n  set( MODULE_LINK_LIBS \"${MODULE_LINK_LIBS};${with-libraries}\" )\nendif ()\nstring( REPLACE \";\" \" \" MODULE_LINK_LIBS \"${MODULE_LINK_LIBS}\" )\n\n# libraries requied to link NEST\nset( ALL_LIBS\n  \"-lnest\"\n  ${MODULE_LINK_LIBS} )\n\n\n# all includes\nset( ALL_INCLUDES_tmp\n  \"${CMAKE_INSTALL_FULL_INCLUDEDIR}/nest\"\n  \"${LTDL_INCLUDE_DIRS}\"\n  \"${READLINE_INCLUDE_DIRS}\"\n  \"${GSL_INCLUDE_DIRS}\"\n  \"${LIBNEUROSIM_INCLUDE_DIRS}\"\n  \"${MUSIC_INCLUDE_DIRS}\"\n  \"${MPI_CXX_INCLUDE_PATH}\"\n  \"${BOOST_INCLUDE_DIR}\" )\nset( ALL_INCLUDES \"\" )\nforeach ( INC ${ALL_INCLUDES_tmp} ${with-includes} )\n  if ( INC AND NOT INC STREQUAL \"\" )\n    set( ALL_INCLUDES \"${ALL_INCLUDES} -I${INC}\" )\n  endif ()\nendforeach ()\nset( ALL_INCLUDES \"${ALL_INCLUDES} ${SIONLIB_INCLUDE}\" )\n\n################################################################################\n##################           File generation here             ##################\n################################################################################\n\nconfigure_file(\n    \"${PROJECT_SOURCE_DIR}/libnestutil/config.h.in\"\n    \"${PROJECT_BINARY_DIR}/libnestutil/config.h\" @ONLY\n)\n\nconfigure_file(\n    \"${PROJECT_SOURCE_DIR}/pynest/setup.py.in\"\n    \"${PROJECT_BINARY_DIR}/pynest/setup.py\" @ONLY\n)\n\nconfigure_file(\n    \"${PROJECT_SOURCE_DIR}/bin/nest-config.in\"\n    \"${PROJECT_BINARY_DIR}/bin/nest-config\" @ONLY\n)\n\nconfigure_file(\n    \"${PROJECT_SOURCE_DIR}/bin/nest_vars.sh.in\"\n    \"${PROJECT_BINARY_DIR}/bin/nest_vars.sh\" @ONLY\n)\n\nconfigure_file(\n    \"${PROJECT_SOURCE_DIR}/doc/fulldoc.conf.in\"\n    \"${PROJECT_BINARY_DIR}/doc/fulldoc.conf\" @ONLY\n)\n\nconfigure_file(\n    \"${PROJECT_SOURCE_DIR}/pynest/nest/versionchecker.py.in\"\n    \"${PROJECT_BINARY_DIR}/pynest/nest/versionchecker.py\" @ONLY\n)\n\n\n################################################################################\n##################            Install Extra Files             ##################\n################################################################################\n\ninstall( FILES LICENSE README.md\n    DESTINATION ${CMAKE_INSTALL_DOCDIR}\n    )\n\nnest_print_config_summary()\n\n[tool.pytest.ini_options]\r\nmarkers = [\r\n    \"skipif_missing_gsl: skip test if NEST was built without GSL support\",\r\n    \"skipif_missing_hdf5: skip test if NEST was built without HDF5 support\",\r\n    \"skipif_missing_mpi: skip test if NEST was built without MPI support\",\r\n    \"skipif_missing_threads: skip test if NEST was built without multithreading support\",\r\n    \"simulation: the simulation class to use. Always pass a 2nd dummy argument\"\r\n]\r\n\r\n[tool.isort]\r\nprofile = \"black\"\r\nknown_third_party = \"nest\"\r\n\r\n[tool.black]\r\nline-length = 120\r\n\n# Required Python packages for NEST Simulator.\n#\n# This file specifies the required Python packages for those who would\n# like to compile NEST or build NEST documentation themselves.\n# If you just want to execute NEST, you should install NEST\n# directly as described in https://www.nest-simulator.org/installation.\n#\n# Note that this file only specifies the required Python packages and\n# not the entire software stack needed to build and work with NEST Simulator.\n# To create a mamba environment with the entire software stack for NEST\n# Simulator, use the the environment yaml file.\n#\n# The requirements from this file can be installed by\n#\n#     pip install -r requirements.txt\n\n\n# To build and work with PyNEST\n-r requirements_pynest.txt\n\n# To run NEST testsuite\n-r requirements_testing.txt\n\n# To build NEST documentation\n-r requirements_docs.txt\n\n# To run NEST Server\n-r requirements_nest_server.txt\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/nest-ml",
            "repo_link": "https://github.com/nest/nestml",
            "content": {
                "codemeta": "",
                "readme": "[![astropy](http://img.shields.io/badge/powered%20by-AstroPy-orange.svg?style=flat)](http://www.astropy.org/) [![NESTML build](https://github.com/nest/nestml/actions/workflows/nestml-build.yml/badge.svg)](https://github.com/nest/nestml/actions/)\n\n# NESTML: The NEST Modelling Language\n\nNESTML is a domain-specific language that supports the specification of neuron models in a precise and concise syntax, based on the syntax of Python. Model equations can either be given as a simple string of mathematical notation or as an algorithm written in the built-in procedural language. The equations are analyzed by the associated toolchain, written in Python, to compute an exact solution if possible or use an appropriate numeric solver otherwise.\n\n## Documentation\n\nFull documentation can be found at:\n\n<pre><p align=\"center\"><a href=\"https://nestml.readthedocs.io/\">https://nestml.readthedocs.io/</a></p></pre>\n\n## Directory structure\n\n`models` - Example neuron models in NESTML format.\n\n`pynestml` - The source code of the PyNESTML toolchain.\n\n`tests` - A collection of tests for testing of the toolchain's behavior.\n\n`doc` - The documentation of the modeling language NESTML as well as processing toolchain PyNESTML.\n\n`extras` - Miscellaneous development tools, editor syntax highlighting rules, etc.\n\n## License\n\nCopyright (C) 2017 The NEST Initiative\n\nNESTML is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 2 of the License, or (at your option) any later version.\n\nNESTML is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License along with NESTML. If not, see <http://www.gnu.org/licenses/>.\n\n## Acknowledgements\n\nThis software was initially supported by the JARA-HPC Seed Fund *NESTML - A modeling language for spiking neuron and synapse models for NEST* and the Initiative and Networking Fund of the Helmholtz Association and the Helmholtz Portfolio Theme *Simulation and Modeling for the Human Brain*.\n\nThis software was developed in part or in whole in the Human Brain Project, funded from the European Union's Horizon 2020 Framework Programme for Research and Innovation under Specific Grant Agreements No. 720270, No. 785907 and No. 945539 (Human Brain Project SGA1, SGA2 and SGA3).\n\n",
                "dependencies": "# note: copy any changes to doc/requirements.txt\nnumpy >= 1.8.2\nscipy\nsympy >= 1.1.1, <1.11\nantlr4-python3-runtime == 4.13.2\nsetuptools\nJinja2 >= 2.10\ntyping;python_version<\"3.5\"\nastropy\nodetoolbox >= 2.4\n\n# -*- coding: utf-8 -*-\n#\n# setup.py\n#\n# This file is part of NEST.\n#\n# Copyright (C) 2004 The NEST Initiative\n#\n# NEST is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 2 of the License, or\n# (at your option) any later version.\n#\n# NEST is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with NEST.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport sys\nfrom setuptools import setup, find_packages\n\nassert sys.version_info.major >= 3 and sys.version_info.minor >= 9, \"Python 3.9 or higher is required to run NESTML\"\n\nwith open(\"requirements.txt\") as f:\n    requirements = f.read().splitlines()\n\ndata_files = []\nfor dir_to_include in [\"doc\", \"models\", \"extras\"]:\n    for dirname, dirnames, filenames in os.walk(dir_to_include):\n        fileslist = []\n        for filename in filenames:\n            fullname = os.path.join(dirname, filename)\n            fileslist.append(fullname)\n        data_files.append((dirname, fileslist))\n\nsetup(\n    name=\"NESTML\",\n    version=\"8.0.0\",\n    description=\"NESTML is a domain specific language that supports the specification of neuron models in a\"\n                \" precise and concise syntax, based on the syntax of Python. Model equations can either be given\"\n                \" as a simple string of mathematical notation or as an algorithm written in the built-in procedural\"\n                \" language. The equations are analyzed by NESTML to compute an exact solution if possible or use an \"\n                \" appropriate numeric solver otherwise.\",\n    license=\"GNU General Public License v2.0\",\n    url=\"https://github.com/nest/nestml\",\n    packages=find_packages(),\n    package_data={\"pynestml\": [\"codegeneration/resources_autodoc/*.jinja2\",\n                               \"codegeneration/resources_nest/point_neuron/*.jinja2\",\n                               \"codegeneration/resources_nest/point_neuron/common/*.jinja2\",\n                               \"codegeneration/resources_nest/point_neuron/directives_cpp/*.jinja2\",\n                               \"codegeneration/resources_nest/point_neuron/setup/*.jinja2\",\n                               \"codegeneration/resources_nest/point_neuron/setup/common/*.jinja2\",\n                               \"codegeneration/resources_nest_compartmental/cm_neuron/*.jinja2\",\n                               \"codegeneration/resources_nest_compartmental/cm_neuron/directives_cpp/*.jinja2\",\n                               \"codegeneration/resources_nest_compartmental/cm_neuron/setup/*.jinja2\",\n                               \"codegeneration/resources_nest_compartmental/cm_neuron/setup/common/*.jinja2\",\n                               \"codegeneration/resources_python_standalone/point_neuron/*.jinja2\",\n                               \"codegeneration/resources_python_standalone/point_neuron/directives_py/*.jinja2\",\n                               \"codegeneration/resources_spinnaker/*.jinja2\",\n                               \"codegeneration/resources_spinnaker/directives_py/*\",\n                               \"codegeneration/resources_spinnaker/directives_cpp/*\",\n                               \"codegeneration/resources_nest_desktop/*.jinja2\"]},\n    data_files=data_files,\n    entry_points={\n        \"console_scripts\": [\n            \"nestml = pynestml.frontend.pynestml_frontend:main\",\n        ],\n    },\n\n    install_requires=requirements,\n    test_suite=\"tests\",\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/nuclear-nexus",
            "repo_link": "https://gitlab.desy.de/fs-mcp/nuclear-nexus",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/nnu-net",
            "repo_link": "https://github.com/MIC-DKFZ/nnUNet",
            "content": {
                "codemeta": "",
                "readme": "# Welcome to the new nnU-Net!\n\nClick [here](https://github.com/MIC-DKFZ/nnUNet/tree/nnunetv1) if you were looking for the old one instead.\n\nComing from V1? Check out the [TLDR Migration Guide](documentation/tldr_migration_guide_from_v1.md). Reading the rest of the documentation is still strongly recommended ;-)\n\n## **2024-04-18 UPDATE: New residual encoder UNet presets available!**\nResidual encoder UNet presets substantially improve segmentation performance.\nThey ship for a variety of GPU memory targets. It's all awesome stuff, promised! \nRead more :point_right: [here](documentation/resenc_presets.md) :point_left:\n\nAlso check out our [new paper](https://arxiv.org/pdf/2404.09556.pdf) on systematically benchmarking recent developments in medical image segmentation. You might be surprised!\n\n# What is nnU-Net?\nImage datasets are enormously diverse: image dimensionality (2D, 3D), modalities/input channels (RGB image, CT, MRI, microscopy, ...), \nimage sizes, voxel sizes, class ratio, target structure properties and more change substantially between datasets. \nTraditionally, given a new problem, a tailored solution needs to be manually designed and optimized  - a process that \nis prone to errors, not scalable and where success is overwhelmingly determined by the skill of the experimenter. Even \nfor experts, this process is anything but simple: there are not only many design choices and data properties that need to \nbe considered, but they are also tightly interconnected, rendering reliable manual pipeline optimization all but impossible! \n\n![nnU-Net overview](documentation/assets/nnU-Net_overview.png)\n\n**nnU-Net is a semantic segmentation method that automatically adapts to a given dataset. It will analyze the provided \ntraining cases and automatically configure a matching U-Net-based segmentation pipeline. No expertise required on your \nend! You can simply train the models and use them for your application**.\n\nUpon release, nnU-Net was evaluated on 23 datasets belonging to competitions from the biomedical domain. Despite competing \nwith handcrafted solutions for each respective dataset, nnU-Net's fully automated pipeline scored several first places on \nopen leaderboards! Since then nnU-Net has stood the test of time: it continues to be used as a baseline and method \ndevelopment framework ([9 out of 10 challenge winners at MICCAI 2020](https://arxiv.org/abs/2101.00232) and 5 out of 7 \nin MICCAI 2021 built their methods on top of nnU-Net, \n [we won AMOS2022 with nnU-Net](https://amos22.grand-challenge.org/final-ranking/))!\n\nPlease cite the [following paper](https://www.google.com/url?q=https://www.nature.com/articles/s41592-020-01008-z&sa=D&source=docs&ust=1677235958581755&usg=AOvVaw3dWL0SrITLhCJUBiNIHCQO) when using nnU-Net:\n\n    Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring \n    method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n\n\n## What can nnU-Net do for you?\nIf you are a **domain scientist** (biologist, radiologist, ...) looking to analyze your own images, nnU-Net provides \nan out-of-the-box solution that is all but guaranteed to provide excellent results on your individual dataset. Simply \nconvert your dataset into the nnU-Net format and enjoy the power of AI - no expertise required!\n\nIf you are an **AI researcher** developing segmentation methods, nnU-Net:\n- offers a fantastic out-of-the-box applicable baseline algorithm to compete against\n- can act as a method development framework to test your contribution on a large number of datasets without having to \ntune individual pipelines (for example evaluating a new loss function)\n- provides a strong starting point for further dataset-specific optimizations. This is particularly used when competing \nin segmentation challenges\n- provides a new perspective on the design of segmentation methods: maybe you can find better connections between \ndataset properties and best-fitting segmentation pipelines?\n\n## What is the scope of nnU-Net?\nnnU-Net is built for semantic segmentation. It can handle 2D and 3D images with arbitrary \ninput modalities/channels. It can understand voxel spacings, anisotropies and is robust even when classes are highly\nimbalanced.\n\nnnU-Net relies on supervised learning, which means that you need to provide training cases for your application. The number of \nrequired training cases varies heavily depending on the complexity of the segmentation problem. No \none-fits-all number can be provided here! nnU-Net does not require more training cases than other solutions - maybe \neven less due to our extensive use of data augmentation. \n\nnnU-Net expects to be able to process entire images at once during preprocessing and postprocessing, so it cannot \nhandle enormous images. As a reference: we tested images from 40x40x40 pixels all the way up to 1500x1500x1500 in 3D \nand 40x40 up to ~30000x30000 in 2D! If your RAM allows it, larger is always possible.\n\n## How does nnU-Net work?\nGiven a new dataset, nnU-Net will systematically analyze the provided training cases and create a 'dataset fingerprint'. \nnnU-Net then creates several U-Net configurations for each dataset: \n- `2d`: a 2D U-Net (for 2D and 3D datasets)\n- `3d_fullres`: a 3D U-Net that operates on a high image resolution (for 3D datasets only)\n- `3d_lowres` → `3d_cascade_fullres`: a 3D U-Net cascade where first a 3D U-Net operates on low resolution images and \nthen a second high-resolution 3D U-Net refined the predictions of the former (for 3D datasets with large image sizes only)\n\n**Note that not all U-Net configurations are created for all datasets. In datasets with small image sizes, the \nU-Net cascade (and with it the 3d_lowres configuration) is omitted because the patch size of the full \nresolution U-Net already covers a large part of the input images.**\n\nnnU-Net configures its segmentation pipelines based on a three-step recipe:\n- **Fixed parameters** are not adapted. During development of nnU-Net we identified a robust configuration (that is, certain architecture and training properties) that can \nsimply be used all the time. This includes, for example, nnU-Net's loss function, (most of the) data augmentation strategy and learning rate.\n- **Rule-based parameters** use the dataset fingerprint to adapt certain segmentation pipeline properties by following \nhard-coded heuristic rules. For example, the network topology (pooling behavior and depth of the network architecture) \nare adapted to the patch size; the patch size, network topology and batch size are optimized jointly given some GPU \nmemory constraint. \n- **Empirical parameters** are essentially trial-and-error. For example the selection of the best U-net configuration \nfor the given dataset (2D, 3D full resolution, 3D low resolution, 3D cascade) and the optimization of the postprocessing strategy.\n\n## How to get started?\nRead these:\n- [Installation instructions](documentation/installation_instructions.md)\n- [Dataset conversion](documentation/dataset_format.md)\n- [Usage instructions](documentation/how_to_use_nnunet.md)\n\nAdditional information:\n- [Learning from sparse annotations (scribbles, slices)](documentation/ignore_label.md)\n- [Region-based training](documentation/region_based_training.md)\n- [Manual data splits](documentation/manual_data_splits.md)\n- [Pretraining and finetuning](documentation/pretraining_and_finetuning.md)\n- [Intensity Normalization in nnU-Net](documentation/explanation_normalization.md)\n- [Manually editing nnU-Net configurations](documentation/explanation_plans_files.md)\n- [Extending nnU-Net](documentation/extending_nnunet.md)\n- [What is different in V2?](documentation/changelog.md)\n\nCompetitions:\n- [AutoPET II](documentation/competitions/AutoPETII.md)\n\n[//]: # (- [Ignore label]&#40;documentation/ignore_label.md&#41;)\n\n## Where does nnU-Net perform well and where does it not perform?\nnnU-Net excels in segmentation problems that need to be solved by training from scratch, \nfor example: research applications that feature non-standard image modalities and input channels,\nchallenge datasets from the biomedical domain, majority of 3D segmentation problems, etc . We have yet to find a \ndataset for which nnU-Net's working principle fails!\n\nNote: On standard segmentation \nproblems, such as 2D RGB images in ADE20k and Cityscapes, fine-tuning a foundation model (that was pretrained on a large corpus of \nsimilar images, e.g. Imagenet 22k, JFT-300M) will provide better performance than nnU-Net! That is simply because these \nmodels allow much better initialization. Foundation models are not supported by nnU-Net as \nthey 1) are not useful for segmentation problems that deviate from the standard setting (see above mentioned \ndatasets), 2) would typically only support 2D architectures and 3) conflict with our core design principle of carefully adapting \nthe network topology for each dataset (if the topology is changed one can no longer transfer pretrained weights!) \n\n## What happened to the old nnU-Net?\nThe core of the old nnU-Net was hacked together in a short time period while participating in the Medical Segmentation \nDecathlon challenge in 2018. Consequently, code structure and quality were not the best. Many features \nwere added later on and didn't quite fit into the nnU-Net design principles. Overall quite messy, really. And annoying to work with.\n\nnnU-Net V2 is a complete overhaul. The \"delete everything and start again\" kind. So everything is better \n(in the author's opinion haha). While the segmentation performance [remains the same](https://docs.google.com/spreadsheets/d/13gqjIKEMPFPyMMMwA1EML57IyoBjfC3-QCTn4zRN_Mg/edit?usp=sharing), a lot of cool stuff has been added. \nIt is now also much easier to use it as a development framework and to manually fine-tune its configuration to new \ndatasets. A big driver for the reimplementation was also the emergence of [Helmholtz Imaging](http://helmholtz-imaging.de), \nprompting us to extend nnU-Net to more image formats and domains. Take a look [here](documentation/changelog.md) for some highlights.\n\n# Acknowledgements\n<img src=\"documentation/assets/HI_Logo.png\" height=\"100px\" />\n\n<img src=\"documentation/assets/dkfz_logo.png\" height=\"100px\" />\n\nnnU-Net is developed and maintained by the Applied Computer Vision Lab (ACVL) of [Helmholtz Imaging](http://helmholtz-imaging.de) \nand the [Division of Medical Image Computing](https://www.dkfz.de/en/mic/index.php) at the \n[German Cancer Research Center (DKFZ)](https://www.dkfz.de/en/index.html).\n\n",
                "dependencies": "[project]\nname = \"nnunetv2\"\nversion = \"2.5.1\"\nrequires-python = \">=3.9\"\ndescription = \"nnU-Net is a framework for out-of-the box image segmentation.\"\nreadme = \"readme.md\"\nlicense = { file = \"LICENSE\" }\nauthors = [\n    { name = \"Fabian Isensee\", email = \"f.isensee@dkfz-heidelberg.de\"},\n    { name = \"Helmholtz Imaging Applied Computer Vision Lab\" }\n]\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n    \"Intended Audience :: Healthcare Industry\",\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: Apache Software License\",\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    \"Topic :: Scientific/Engineering :: Image Recognition\",\n    \"Topic :: Scientific/Engineering :: Medical Science Apps.\",\n]\nkeywords = [\n    'deep learning',\n    'image segmentation',\n    'semantic segmentation',\n    'medical image analysis',\n    'medical image segmentation',\n    'nnU-Net',\n    'nnunet'\n]\ndependencies = [\n    \"torch>=2.1.2\",\n    \"acvl-utils>=0.2,<0.3\",  # 0.3 may bring breaking changes. Careful!\n    \"dynamic-network-architectures>=0.3.1,<0.4\",  # 0.3.1 and lower are supported, 0.4 may have breaking changes. Let's be careful here\n    \"tqdm\",\n    \"dicom2nifti\",\n    \"scipy\",\n    \"batchgenerators>=0.25\",\n    \"numpy>=1.24\",\n    \"scikit-learn\",\n    \"scikit-image>=0.19.3\",\n    \"SimpleITK>=2.2.1\",\n    \"pandas\",\n    \"graphviz\",\n    'tifffile',\n    'requests',\n    \"nibabel\",\n    \"matplotlib\",\n    \"seaborn\",\n    \"imagecodecs\",\n    \"yacs\",\n    \"batchgeneratorsv2>=0.2\",\n    \"einops\"\n]\n\n[project.urls]\nhomepage = \"https://github.com/MIC-DKFZ/nnUNet\"\nrepository = \"https://github.com/MIC-DKFZ/nnUNet\"\n\n[project.scripts]\nnnUNetv2_plan_and_preprocess = \"nnunetv2.experiment_planning.plan_and_preprocess_entrypoints:plan_and_preprocess_entry\"\nnnUNetv2_extract_fingerprint = \"nnunetv2.experiment_planning.plan_and_preprocess_entrypoints:extract_fingerprint_entry\"\nnnUNetv2_plan_experiment = \"nnunetv2.experiment_planning.plan_and_preprocess_entrypoints:plan_experiment_entry\"\nnnUNetv2_preprocess = \"nnunetv2.experiment_planning.plan_and_preprocess_entrypoints:preprocess_entry\"\nnnUNetv2_train = \"nnunetv2.run.run_training:run_training_entry\"\nnnUNetv2_predict_from_modelfolder = \"nnunetv2.inference.predict_from_raw_data:predict_entry_point_modelfolder\"\nnnUNetv2_predict = \"nnunetv2.inference.predict_from_raw_data:predict_entry_point\"\nnnUNetv2_convert_old_nnUNet_dataset = \"nnunetv2.dataset_conversion.convert_raw_dataset_from_old_nnunet_format:convert_entry_point\"\nnnUNetv2_find_best_configuration = \"nnunetv2.evaluation.find_best_configuration:find_best_configuration_entry_point\"\nnnUNetv2_determine_postprocessing = \"nnunetv2.postprocessing.remove_connected_components:entry_point_determine_postprocessing_folder\"\nnnUNetv2_apply_postprocessing = \"nnunetv2.postprocessing.remove_connected_components:entry_point_apply_postprocessing\"\nnnUNetv2_ensemble = \"nnunetv2.ensembling.ensemble:entry_point_ensemble_folders\"\nnnUNetv2_accumulate_crossval_results = \"nnunetv2.evaluation.find_best_configuration:accumulate_crossval_results_entry_point\"\nnnUNetv2_plot_overlay_pngs = \"nnunetv2.utilities.overlay_plots:entry_point_generate_overlay\"\nnnUNetv2_download_pretrained_model_by_url = \"nnunetv2.model_sharing.entry_points:download_by_url\"\nnnUNetv2_install_pretrained_model_from_zip = \"nnunetv2.model_sharing.entry_points:install_from_zip_entry_point\"\nnnUNetv2_export_model_to_zip = \"nnunetv2.model_sharing.entry_points:export_pretrained_model_entry\"\nnnUNetv2_move_plans_between_datasets = \"nnunetv2.experiment_planning.plans_for_pretraining.move_plans_between_datasets:entry_point_move_plans_between_datasets\"\nnnUNetv2_evaluate_folder = \"nnunetv2.evaluation.evaluate_predictions:evaluate_folder_entry_point\"\nnnUNetv2_evaluate_simple = \"nnunetv2.evaluation.evaluate_predictions:evaluate_simple_entry_point\"\nnnUNetv2_convert_MSD_dataset = \"nnunetv2.dataset_conversion.convert_MSD_dataset:entry_point\"\n\n[project.optional-dependencies]\ndev = [\n    \"black\",\n    \"ruff\",\n    \"pre-commit\"\n]\n\n[build-system]\nrequires = [\"setuptools>=67.8.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.codespell]\nskip = '.git,*.pdf,*.svg'\n#\n# ignore-words-list = ''\n\nimport setuptools\n\nif __name__ == \"__main__\":\n    setuptools.setup()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/nodejs-tcp-server-client",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Nodejs-TCP-Server-Client",
            "content": {
                "codemeta": "",
                "readme": "![NODE.JS](https://img.shields.io/badge/NODE.JS-%2343853D.svg?&style=plastic&logo=node.js&logoColor=white) ![JavaScript](https://img.shields.io/badge/JavaScript-323330?style=plastic&logo=javascript&logoColor=f7df1e) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/nodejs-tcp_server-client?&color=green&style=plastic)\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808924.svg)](https://doi.org/10.5281/zenodo.12808924) [![SWH](https://archive.softwareheritage.org/badge/swh:1:dir:3022219080921ea266808592fa83f3afc5242282/)](https://archive.softwareheritage.org/swh:1:dir:3022219080921ea266808592fa83f3afc5242282;origin=https://github.com/Ramy-Badr-Ahmed/node-tcp;visit=swh:1:snp:e0d42b65bc06365c756247a55b21ca431f17a53a;anchor=swh:1:rev:d9e2239926489996936d18aa5804ce1fc2503181)\n\n\n# TCP Communication with Node.js\n\nA TCP server and client implementation using Node.js's `net` module, operating at the Transport Layer of the OSI Model.\n\nThis implementation focuses on direct communication without additional overhead such as data compression or encryption/decryption, suitable for safe and trusted networks.\n\nThe TLS variant (@Presentation-Layer of the OSI Model) is located here: [Node-TLS](https://github.com/Ramy-Badr-Ahmed/node-tls)\n\n#### Some Use Cases:\n\n- Network Diagnostics \n    > Test network connectivity and latency between network segments.\n\n- Embedded Systems Communication\n  > Integrate TCP client with embedded devices (e.g. Raspberry Pi, Arduino with Ethernet/Wi-Fi shields) to send data to a central server (for monitoring and control).\n\n- Time Synchronization\n  > Use the server to provide a timestamp service for devices on a network (ensure synchronized time across various systems).\n\n- IoT Apps\n    > Use the TCP server as a central hub to collect data from various IoT devices.\n   \n    > Set up the TCP client to send sensor data periodically from remote IoT devices to the server for analysis (centralized data receiver/logger).\n  \n#### Quick Start:\nServer:\n```shell\nnpm install\nnode tcpServer.js   # Runs Server\n```\nClient:\n```shell\nnpm install\nnode tcpClient.js   # Runs Client\n```  \n\nLogs and Outputs:\n\nThe server and client will log various server/client events and actions, such as connection establishment, data transmission, and encountered errors.\n\nReferences\n\n- [Net Module](https://nodejs.org/api/net.html)\n\n",
                "dependencies": "{\n  \"name\": \"tcp-server-client\",\n  \"version\": \"1.0.0\",\n  \"main\": \"tcpServer.js\",\n  \"scripts\": {\n    \"start-server\": \"node tcpServer.js\",\n    \"start-client\": \"node tcpClient.js\"\n  },\n  \"keywords\": [],\n  \"author\": \"Ramy-Badr-Ahmed\",\n  \"description\": \"A simple TCP server and client implementation using Node.js.\",\n  \"license\": \"Apache-2.0\",\n  \"dependencies\": {\n    \"uuid\": \"^10.0.0\"\n  }\n}\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/nodejs-tls-server-client",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Nodejs-TLS-Server-Client",
            "content": {
                "codemeta": "",
                "readme": "![NODE.JS](https://img.shields.io/badge/NODE.JS-%2343853D.svg?&style=plastic&logo=node.js&logoColor=white) ![JavaScript](https://img.shields.io/badge/JavaScript-323330?style=plastic&logo=javascript&logoColor=f7df1e) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/nodejs-tls_server-client?color=green&style=plastic)\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808908.svg)](https://doi.org/10.5281/zenodo.12808908) [![SWH](https://archive.softwareheritage.org/badge/swh:1:dir:8017c373f704257957a1cc9b5044c7347651b899/)](https://archive.softwareheritage.org/swh:1:dir:8017c373f704257957a1cc9b5044c7347651b899;origin=https://github.com/Ramy-Badr-Ahmed/node-tls;visit=swh:1:snp:eec57a10aaa0a231ac22e6c8a476c167a0669b66;anchor=swh:1:rev:0b48c4c274fb30ea4c7913f1d77083f9e2baa888) \n\n# TLS Communication with Node.js\n\nA TLS server and client implementation using Node.js's tls module, operating at the Presentation Layer of the OSI Model. \n\nThis implementation focuses on encrypted and authenticated communication, suitable for secure and trusted networks.\n\nThe TCP variant (@Transport-Layer of the OSI Model) is located here: [Node-TCP](https://github.com/Ramy-Badr-Ahmed/node-tcp)\n\n#### Use Cases:\n\n- Network Security\n\n    > Ensure secure communication between network segments with TLS encryption and certificate-based authentication.\n\n- Secure Embedded Systems Communication\n\n    > Integrate TLS client with embedded devices (e.g., Raspberry Pi, Arduino with Ethernet/Wi-Fi shields) to send data securely to a central server (for monitoring and control).\n\n- Secure Time Synchronization\n\n    > Use the server to provide a timestamp service for devices on a network (ensure synchronized time across various systems with TLS security).\n\n- Secure IoT Applications\n\n    > Use the TLS server as a central hub to collect data from various IoT devices securely.\n    \n    > Set up the TLS client to send sensor data periodically from remote IoT devices to the server for analysis (centralized secure data receiver/logger).\n\n### Quick Start:\n\nPrerequisites:\n\n- Node.js (v14.x or later)\n- OpenSSL (for generating certificates)\n\nServer:\n\nPlace server certificate and key under the `Certs\\server` directory\n\n```shell\nnpm install\nnode tlsServer.js   # Runs Server\n```  \n\nClient:\n\nPlace client certificate and key under the `Certs\\client` directory\n\n```shell\nnpm install\nnode tlsClient.js   # Runs Client\n```  \n\nLogs and Outputs:\n\nThe server and client will log various events and actions, such as connection establishment, data transmission, handshake report, session management, and encountered errors.\n\nReferences\n\n- [TLS (SSL) Module](https://nodejs.org/api/tls.html)\n\n",
                "dependencies": "{\n  \"name\": \"tls-server-client\",\n  \"version\": \"1.0.0\",\n  \"main\": \"tlsServer.js\",\n  \"scripts\": {\n    \"start-server\": \"node tlsServer.js\",\n    \"start-client\": \"node tlsClient.js\"\n  },\n  \"keywords\": [],\n  \"author\": \"Ramy-Badr-Ahmed\",\n  \"description\": \"A TLS server and client implementation using Node.js.\",\n  \"license\": \"Apache-2.0\",\n  \"dependencies\": {\n    \"uuid\": \"^10.0.0\"\n  }\n}\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/novosparc",
            "repo_link": "https://github.com/rajewsky-lab/novosparc",
            "content": {
                "codemeta": "",
                "readme": "|PyPI| |Docs| |PePy|\n\n.. |PyPI| image:: https://img.shields.io/pypi/v/novosparc.svg\n   :target: https://pypi.org/project/novosparc/\n.. |Docs| image:: https://readthedocs.org/projects/novosparc/badge/?version=latest\n   :target: https://novosparc.readthedocs.io/\n.. |PePy| image:: https://static.pepy.tech/badge/novosparc\n   :target: https://pepy.tech/project/novosparc\n\nnovoSpaRc - *de novo* Spatial Reconstruction of Single-Cell Gene Expression\n===========================================================================\n\n.. image:: https://raw.githubusercontent.com/nukappa/nukappa.github.io/master/images/novosparc.png\n   :width: 90px\n   :align: left\n\n``novoSpaRc`` predicts locations of single cells in space by solely using \nsingle-cell RNA sequencing data. An existing reference database of marker genes\nis not required, but significantly enhances performance if available.\n\n``novoSpaRc`` accompanies the following publications:\n\n    | *Gene Expression Cartography*\n    | M Nitzan*, N Karaiskos*, N Friedman†, N Rajewsky†\n    | `Nature (2019) <https://www.nature.com/articles/s41586-019-1773-3>`_\n\nand\n\n    | *novoSpaRc: flexible spatial reconstruction of single-cell gene expression with optimal transport*\n    | N Moriel*, E Senel*, N Friedman, N Rajewsky, N Karaiskos†, M Nitzan†\n    | `Nature Protocols (2021) <https://www.nature.com/articles/s41596-021-00573-7>`_\n\nRead the `documentation <https://novosparc.readthedocs.io>`_ and the \n`tutorial <https://github.com/rajewsky-lab/novosparc/blob/master/reconstruct_drosophila_embryo_tutorial.ipynb>`_ for more information.\n\n",
                "dependencies": "POT>=0.7.0\nnumpy\nanndata>=0.7.5\nmatplotlib>=3.3.2\nscipy\npandas\nrecommonmark\nscanpy>=1.6.0\nscikit_learn\n\nfrom setuptools import setup, find_packages\n\nwith open(\"README.rst\", \"r\") as fh:\n    long_description = fh.read()\n\ntry:\n    from novosparc import __author__, __email__, __version__\nexcept ImportError:  # Deps not yet installed\n    __author__ = __email__ = __version__ = ''\n\nwith open('requirements.txt', 'r') as f:\n    required_packages = f.read().splitlines()\n\nsetup(\n    name=\"novosparc\",\n    version=__version__,\n    author=__author__,\n    author_email=__email__,\n    description=\"De novo spatial reconstruction of single-cell gene expression.\",\n    long_description=long_description,\n    url=\"https://github.com/rajewsky-lab/novosparc\",\n    license='MIT',\n    install_requires=required_packages,\n    packages=find_packages(),\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: POSIX :: Linux\",\n        \"Operating System :: MacOS :: MacOS X\"\n    ]\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/o3as",
            "repo_link": "https://git.scc.kit.edu/synergy.o3as/",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/odm2sms",
            "repo_link": "https://jugit.fz-juelich.de/sms/odm2sms",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/odv",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/oemof-solph",
            "repo_link": "https://github.com/oemof/oemof-solph/",
            "content": {
                "codemeta": "",
                "readme": "\n|tox-pytest| |tox-checks| |appveyor| |coveralls| |codecov|\n\n|scrutinizer| |codacy| |codeclimate|\n\n|wheel| |packaging| |supported-versions|\n\n|docs| |zenodo|\n\n|version| |commits-since| |chat|\n\n\n------------------------------\n\n.. |tox-pytest| image:: https://github.com/oemof/oemof-solph/workflows/tox%20pytests/badge.svg?branch=dev\n     :target: https://github.com/oemof/oemof-solph/actions?query=workflow%3A%22tox+checks%22\n\n.. |tox-checks| image:: https://github.com/oemof/oemof-solph/workflows/tox%20checks/badge.svg?branch=dev\n     :target: https://github.com/oemof/oemof-solph/actions?query=workflow%3A%22tox+checks%22\n\n.. |packaging| image:: https://github.com/oemof/oemof-solph/workflows/packaging/badge.svg?branch=dev\n     :target: https://github.com/oemof/oemof-solph/actions?query=workflow%3Apackaging\n\n.. |docs| image:: https://readthedocs.org/projects/oemof-solph/badge/?style=flat\n    :target: https://readthedocs.org/projects/oemof-solph\n    :alt: Documentation Status\n\n.. |appveyor| image:: https://ci.appveyor.com/api/projects/status/github/oemof/oemof-solph?branch=dev&svg=true\n    :alt: AppVeyor Build Status\n    :target: https://ci.appveyor.com/project/oemof-developer/oemof-solph\n\n.. |coveralls| image:: https://coveralls.io/repos/oemof/oemof-solph/badge.svg?branch=dev&service=github\n    :alt: Coverage Status\n    :target: https://coveralls.io/github/oemof/oemof-solph\n\n.. |codecov| image:: https://codecov.io/gh/oemof/oemof-solph/branch/dev/graphs/badge.svg?branch=dev\n    :alt: Coverage Status\n    :target: https://codecov.io/gh/oemof/oemof-solph\n\n.. |codacy| image:: https://api.codacy.com/project/badge/Grade/a6e5cb2dd2694c73895e142e4cf680d5\n    :target: https://app.codacy.com/gh/oemof/oemof-solph/dashboard\n    :alt: Codacy Code Quality Status\n\n.. |codeclimate| image:: https://codeclimate.com/github/oemof/oemof-solph/badges/gpa.svg\n   :target: https://codeclimate.com/github/oemof/oemof-solph\n   :alt: CodeClimate Quality Status\n\n.. |version| image:: https://img.shields.io/pypi/v/oemof.solph.svg\n    :alt: PyPI Package latest release\n    :target: https://pypi.org/project/oemof.solph\n\n.. |wheel| image:: https://img.shields.io/pypi/wheel/oemof.solph.svg\n    :alt: PyPI Wheel\n    :target: https://pypi.org/project/oemof.solph\n\n.. |supported-versions| image:: https://img.shields.io/pypi/pyversions/oemof.solph.svg\n    :alt: Supported versions\n    :target: https://pypi.org/project/oemof.solph\n\n.. |supported-implementations| image:: https://img.shields.io/pypi/implementation/oemof.solph.svg\n    :alt: Supported implementations\n    :target: https://pypi.org/project/oemof.solph\n\n.. |commits-since| image:: https://img.shields.io/github/commits-since/oemof/oemof-solph/latest/dev\n    :alt: Commits since latest release\n    :target: https://github.com/oemof/oemof-solph/compare/master...dev\n\n.. |zenodo| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.596235.svg\n    :alt: Zenodo DOI\n    :target: https://doi.org/10.5281/zenodo.596235\n\n.. |scrutinizer| image:: https://img.shields.io/scrutinizer/quality/g/oemof/oemof-solph/dev.svg\n    :alt: Scrutinizer Status\n    :target: https://scrutinizer-ci.com/g/oemof/oemof-solph/\n\n.. |chat| image:: https://img.shields.io/badge/chat-oemof:matrix.org-%238ADCF7\n     :alt: matrix-chat\n     :target: https://matrix.to/#/#oemof:matrix.org\n\n\n.. figure:: https://raw.githubusercontent.com/oemof/oemof-solph/492e3f5a0dda7065be30d33a37b0625027847518/docs/_logo/logo_oemof_solph_FULL.svg\n    :align: center\n\n------------------------------\n\n===========\noemof.solph\n===========\n\n**A model generator for energy system modelling and optimisation (LP/MILP)**\n\n.. contents::\n    :depth: 2\n    :local:\n    :backlinks: top\n\n\nIntroduction\n============\n\nThe oemof.solph package is part of the\n`Open energy modelling framework (oemof) <https://github.com/oemof/oemof>`_.\nThis is an organisational framework to bundle tools for energy (system) modelling.\noemof-solph is a model generator for energy system modelling and optimisation.\n\nThe package ``oemof.solph`` is very often called just ``oemof``.\nThis is because installing the ``oemof`` meta package was once the best way to get ``oemof.solph``.\nNotice that you should prefeably install ``oemof.solph`` instead of ``oemof``\nif you want to use ``solph``.\n\n\nEverybody is welcome to use and/or develop oemof.solph.\nRead our `contribution <https://oemof.readthedocs.io/en/latest/contributing.html>`_ section.\n\nContribution is already possible on a low level by simply fixing typos in\noemof's documentation or rephrasing sections which are unclear.\nIf you want to support us that way please fork the oemof-solph repository to your own\nGitHub account and make changes as described in the `github guidelines <https://docs.github.com/en/get-started/quickstart/hello-world>`_\n\nIf you have questions regarding the use of oemof including oemof.solph you can visit the openmod forum (`tag oemof <https://forum.openmod-initiative.org/tags/c/qa/oemof>`_ or `tag oemof-solph <https://forum.openmod-initiative.org/tags/c/qa/oemof-solph>`_) and open a new thread if your questions hasn't been already answered.\n\nKeep in touch! - You can become a watcher at our `github site <https://github.com/oemof/oemof>`_,\nbut this will bring you quite a few mails and might be more interesting for developers.\nIf you just want to get the latest news, like when is the next oemof meeting,\nyou can follow our news-blog at `oemof.org <https://oemof.org/>`_.\n\nDocumentation\n=============\nThe `oemof.solph documentation <https://oemof-solph.readthedocs.io/>`_ is powered by readthedocs. Use the `project site <https://readthedocs.org/projects/oemof>`_ of oemof.solph to choose the version of the documentation. Go to the `download page <https://readthedocs.org/projects/oemof/downloads/>`_ to download different versions and formats (pdf, html, epub) of the documentation.\n\n\n.. _installation_label:\n\nInstallation\n============\n\n\nIf you have a working Python installation, use pypi to install the latest version of oemof.solph.\nPython >= 3.8 is recommended. Lower versions may work but are not tested.\n\nWe highly recommend to use virtual environments.\nPlease refer to the documentation of your Python distribution (e.g. Anaconda,\nMicromamba, or the version of Python that came with your Linux installation)\nto learn how to set up and use virtual environments.\n\n::\n\n    (venv) pip install oemof.solph\n\nIf you want to use the latest features, you might want to install the **developer version**. The developer version is not recommended for productive use::\n\n    (venv) pip install https://github.com/oemof/oemof-solph/archive/dev.zip\n\n\nFor running an oemof-solph optimisation model, you need to install a solver.\nFollowing you will find guidelines for the installation process for different operating systems.\n\n.. _windows_solver_label:\n.. _linux_solver_label:\n\nInstalling a solver\n-------------------\n\nThere are several solvers that can work with oemof, both open source and commercial.\nTwo open source solvers are widely used (CBC and GLPK), but oemof suggests CBC (Coin-or branch and cut).\nIt may be useful to compare results of different solvers to see which performs best.\nOther commercial solvers, like Gurobi or Cplex, are also options.\nHave a look at the `pyomo docs <https://pyomo.readthedocs.io/en/stable/api/pyomo.solvers.plugins.solvers.html>`_\nto learn about which solvers are supported.\n\nCheck the solver installation by executing the test_installation example below (see section Installation Test).\n\n**Linux**\n\nTo install the solvers have a look at the package repository of your Linux distribution or search for precompiled packages. GLPK and CBC ares available at Debian, Feodora, Ubuntu and others.\n\n**Windows**\n\n 1. Download `CBC <https://github.com/coin-or/Cbc/releases>`_\n 2. Download `GLPK (64/32 bit) <https://sourceforge.net/projects/winglpk/>`_\n 3. Unpack CBC/GLPK to any folder (e.g. C:/Users/Somebody/my_programs)\n 4. Add the path of the executable files of both solvers to the PATH variable using `this tutorial <https://www.computerhope.com/issues/ch000549.htm>`_\n 5. Restart Windows\n\nCheck the solver installation by executing the test_installation example (see the `Installation test` section).\n\n\n**Mac OSX**\n\nPlease follow the installation instructions on the respective homepages for details.\n\nCBC-solver: https://github.com/coin-or/Cbc\n\nGLPK-solver: http://arnab-deka.com/posts/2010/02/installing-glpk-on-a-mac/\n\nIf you install the CBC solver via brew (highly recommended), it should work without additional configuration.\n\n\n**conda**\n\nProvided you are using a Linux or MacOS, the CBC-solver can also be installed in a `conda` environment. Please note, that it is highly recommended to `use pip after conda <https://www.anaconda.com/blog/using-pip-in-a-conda-environment>`_, so:\n\n.. code:: console\n\n    (venv) conda install -c conda-forge coincbc\n    (venv) pip install oemof.solph\n\n\n.. _check_installation_label:\n\nInstallation test\n-----------------\n\nTest the installation and the installed solver by running the installation test\nin your virtual environment:\n\n.. code:: console\n\n  (venv) oemof_installation_test\n\nIf the installation was successful, you will receive something like this:\n\n.. code:: console\n\n    *********\n    Solver installed with oemof:\n    glpk: working\n    cplex: not working\n    cbc: working\n    gurobi: not working\n    *********\n    oemof.solph successfully installed.\n\nas an output.\n\nContributing\n============\n\nA warm welcome to all who want to join the developers and contribute to\noemof.solph.\n\nInformation on the details and how to approach us can be found\n`in the oemof documentation <https://oemof.readthedocs.io/en/latest/contributing.html>`_ .\n\nCiting\n======\n\nFor explicitly citing solph, you might want to refer to\n`DOI:10.1016/j.simpa.2020.100028 <https://doi.org/10.1016/j.simpa.2020.100028>`_,\nwhich gives an overview over the capabilities of solph.\nThe core ideas of oemof as a whole are described in\n`DOI:10.1016/j.esr.2018.07.001 <https://doi.org/10.1016/j.esr.2018.07.001>`_\n(preprint at `arXiv:1808.0807 <https://arxiv.org/abs/1808.08070v1>`_).\nTo allow citing specific versions, we use the zenodo project to get a DOI for each version.\n\nExample Applications\n====================\n\nThe combination of specific modules (often including other packages) is called an\napplication (app). For example, it can depict a concrete energy system model.\nYou can find a large variety of helpful examples in the documentation.\nThe examples show the optimisation of different energy systems and are supposed\nto help new users to understand the framework's structure.\n\nYou are welcome to contribute your own examples via a `pull request <https://github.com/oemof/oemof-solph/pulls>`_\nor by e-mailing us (see `here <https://oemof.org/contact/>`_ for contact information).\n\nLicense\n=======\n\nCopyright (c) oemof developer group\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n",
                "dependencies": "[build-system]\nrequires = [\"flit_core >=3.2,<4\"]\nbuild-backend = \"flit_core.buildapi\"\n\n[tool.flit.sdist]\ninclude = [\n    \"AUTHORS.rst\",\n    \"CITATION.cff\",\n    \"CODE_OF_CONDUCT.md\",\n    \"CONTRIBUTING.rst\",\n    \"LICENSE\",\n    \"README.rst\",\n    \"VERSION\",\n    \"tox.ini\",\n    \"ci/\",\n    \"docs/\",\n    \"examples/\",\n    \"src/\",\n    \"tests/\",\n    \".bumpversion.cfg\",\n    \".coveragerc\",\n    \".editorconfig\",\n    \".flake8\",\n    \".pep8speaks.yml\",\n    \".readthedocs.yaml\",\n    \".scrutinizer.yml\",\n]\nexclude = [\"docs/_build\"]\n\n[project]\nname = \"oemof.solph\"\ndynamic = [\"version\"]\ndescription = \"A model generator for energy system modelling and optimisation.\"\nreadme = \"README.rst\"\nauthors = [\n    {name = \"oemof developer group\", email = \"contact@oemof.org\"},\n]\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: Unix\",\n    \"Operating System :: POSIX\",\n    \"Operating System :: Microsoft :: Windows\",\n    \"Operating System :: MacOS\",\n    \"Operating System :: OS Independent\",\n    \"Programming Language :: Python\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Programming Language :: Python :: 3.13\",\n    \"Programming Language :: Python :: Implementation :: CPython\",\n    \"Topic :: Utilities\",\n]\nrequires-python = \">=3.10\"\ndependencies = [\n    \"blinker\",\n    \"dill\",\n    \"numpy >= 2.0.0\",\n    \"pandas >= 2.2.0\",\n    \"pyomo >= 6.8.0\",\n    \"networkx\",\n    \"oemof.tools >= 0.4.3\",\n    \"oemof.network >= 0.5.0\",\n]\nlicense = {text = \"MIT\"}\n\n[project.urls]\nHomepage = \"https://github.com/oemof/oemof-solph\"\nDocumentation = \"https://oemof-solph.readthedocs.io/\"\nChangelog = \"https://oemof-solph.readthedocs.io/en/latest/changelog.html\"\n\"Issue Tracker\" = \"https://github.com/oemof/oemof-solph/issues/\"\n\n[project.optional-dependencies]\ndev = [\n    \"flit\",\n    \"matplotlib\",\n    \"nbformat\",\n    \"pytest\",\n    \"sphinx\",\n    \"sphinx_rtd_theme\",\n    \"sphinx-copybutton\",\n    \"sphinx-design\",\n    \"termcolor\",\n    \"tox\",\n]\n\n[project.scripts]\noemof_installation_test = \"oemof.solph._console_scripts:check_oemof_installation\"\n\n[tool.black]\nline-length = 79\ntarget-version = ['py39', 'py310', 'py311']\ninclude = '\\.pyi?$'\nexclude = '''\n/(\n    \\.eggs\n  | \\.git\n  | \\.hg\n  | \\.mypy_cache\n  | \\.tox\n  | \\.venv\n  | _build\n  | buck-out\n  | build\n  | dist\n  | ci\n)/\n'''\n\n[tool.pytest.ini_options]\nnorecursedirs = [\n    \".git\",\n    \".tox\",\n    \".env\",\n    \"dist\",\n    \"build\",\n    \"docs/_build\",\n    \"migrations\",\n    \"examples\",\n]\npython_files = [\n    \"test_*.py\",\n    \"*_test.py\",\n    \"*_tests.py\",\n    \"tests.py\",\n]\naddopts = \"\"\"\n    -ra\n    --strict-markers\n    --ignore=docs/conf.py\n    --ignore=setup.py\n    --ignore=ci\n    --ignore=.eggs\n    --doctest-modules\n    --doctest-glob=\\\\*.rst\n    --tb=short\n    --pyargs\n\"\"\"\ntestpaths = [\n    \"src/oemof/solph/\",\n    \"tests/\",\n    \"docs/\",\n]\n\n[tool.isort]\nforce_single_line = true\nline_length = 79\nknown_first_party = \"oemof-solph\"\ndefault_section = \"THIRDPARTY\"\nforced_separate = \"test_oemof-solph\"\nskip = \"migrations\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/opencarp",
            "repo_link": "https://git.opencarp.org/openCARP/openCARP",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/openfuelcell2",
            "repo_link": "https://github.com/openFuelCell2/openFuelCell2",
            "content": {
                "codemeta": "",
                "readme": "# openFuelCell2\n\n[openFuelCell2](https://openfuelcell2.github.io/) is a computational fluid dynamics (CFD) toolbox for simulating electrochemical devices such as fuel cells and electrolysis. The solver is based on the open-source library, OpenFOAM®.\n\n## About the code\n\nThe source code was developed from a previous open-source repository called [openFuelCell](http://openfuelcell.sourceforge.net/). It was also inspired by the standard solver \"reactingTwoPhaseEulerFoam\" in OpenFOAM®. The solver can consider coupled problems with multi-region and multi-physics issues, including single and two phase flows, multiple species components, charge transfer, and electrochemical reactions in different regions. More applications and solvers will be available in the future.\n\n## How to use\n\nThe code is compiled with the OpenFOAM libraries, either [ORG](https://openfoam.org/) or [COM](https://www.openfoam.com/) versions. The default branch is compatable with the COM version, while the other branches are also provided for different OpenFOAM environments. The available environments will include: OpenFOAM-v2012, OpenFOAM-v2106, OpenFOAM-v2306, OpenFOAM-v6, OpenFOAM-v8. Note: the main branch is only compatible with OpenFOAM-v2306, while the others are under preparation.\n\n```bash\n# Download the source code\n# Setup the corresponding openfoam environment\n# Switch to the corresponding branch\n\n# Change dictionary to the repository\ncd openFuelCell2/src\n\n# Compile the source code with\n./Allwmake\n\n# Or compile in parallel\n./Allwmake -j n\n```\n\nYou can also clear the libraries and executable files with\n\n```bash\n\ncd openFuelCell2/src\n\n./Allwclean\n\n```\n\n## Computational domains\n\n---\n\nTake the cross-section of a fuel cell as an example. The computational domain gives,\n\n<div align=\"center\">\n  <img src=\"images/computationDomain.jpg\" height=\"70%\" width=\"70%\">\n</div>\n\nIn a PEM fuel cell or other types, there are several domains/regions: air, fuel, electrolyte, and interconnect. This can be found from the repository [openFuelCell](http://openfuelcell.sourceforge.net/). However, additional domains/regions, e.g. phiEA, phiEC, and phiI are also necessary to account for electron/ion and dissolved water transfer.\n\nTo consider the coupling transfer problems in a PEM fuel cell, a global region, also called as parent mesh, and several local regions, also called as child meshes, are used. In the global region, only the energy equation is solved. In the local regions, corresponding partial differential equations will be discretized and solved. During the simulation, material properties, e.g. density, thermal conductivity, etc., are mapped from local regions to the global region, while the temperature field is mapped from the global region to the local regions.\n\nThe local regions can be classified as three different types, namely fluid, solid, and electric regions. See the [code](src/libSrc/fuelCellSystems/regions).\n\n- Fluid region:\n\n  This region represents the space where fluid flows by. In a fuel cell or electrolyzer, it consists with gas channels and/or porous regions. In this region, the following processes are addressed:\n\n  - Fluid flow (single/two phase)\n  - Species transfer\n  - Electrochemical reaction\n  - Heat and mass transfer\n\n  For example, in a fuel cell, the following parts apply to this region,\n\n  - Air flow paths + porous electrodes\n  - Fuel flow paths + porous electrodes\n  - Cooling channels\n\n- Solid region:\n\n  This represents the solid components in fuel cell or electrolyzer. In the current solver, no equations will be solved here. However, stress analysis during assembly and thermal effects may be implemented in future applications.\n\n  For example. in a fuel cell, the following components apply to this region,\n\n  - Electrolyte/membrane\n  - Interconnect/Bipolar-plate\n  - Endplate\n\n- Electric region:\n\n  This region accounts for the electric-conductive components. It is designed to consider electron/ion transfer specifically. However, it is found that the proton transfer region is the same as the region where dissolved water transfer takes place. Therefore, a switcher is enabled in the code to turn on/off the dissolved water transfer model. The following equations will be solved,\n\n  - Potential equations (Poisson equations)\n  - Dissolved water transfer equations (diffusion and electro-osmotic-drag)\n\n  For example, in a fuel cell, the following components belong to this region:\n\n  - Bipolar-plate, GDL, MPL, CL -> electron transfer regions\n  - Catalyst coated membrane, CCM, -> proton transfer and dissolved water transfer region\n\n- Global region:\n\n  The heat source/sinks in local regions will be mapped to this region. And the obtained temperature is mapped back to the local regions. The heat source/sink include:\n\n  - Joule heat from the electron/proton regions.\n  - Condensation/evaporation in the fluid regions.\n  - Electrochemical reactions in the fluid regions.\n\n## Recent updates\n\n---\n\n- [Oct. 2020] A new branch for openFOAM-ESI\n  > The majority part of this update was conducted by Mr. Steffen Hess. The code is able to compile in the OpenFOAM-ESI environment.\n- [Nov. 2020] The new branch for openFOAM-ESI\n  > Some bugs were found and fixed:\n    1. The compiling sequence.\n    2. The locations of gravity fields, g. The files \"g\" move to constant/.\n    3. The functionObjects library is missing.\n    4. Remove some warnings: apply new functions in OpenFOAM-ESI.\n- [Nov. 2021] The new branch for openFOAM-2106\n  > Some bugs were found and fixed:\n    1. The method **heatTransfer(T, cellListIO)** in class \"TwoResistanceHeatTransferPhaseSystem\" is fixed.\n  > Dictionary structure is rearranged:\n    1. src: source files\n    2. appSrc: application source files\n    3. libSrc: libraries source files\n    4. run: test cases\n- [Jun. 2022] Clean the source code for releasing\n  > Bugs are found and fixed:\n    1. The previous solver might predict results with singularities in phiI region. This is fixed.\n  > The source code is updated:\n    1. Change the phase name to none if single-phase flow is simulated. This makes the variable names change from _A.air_ to _A_.\n    2. Moving the correction of diffusivity from MultiComponentPhaseModel to **diffusivityModelList**.\n    3. Moving the definition of phase properties from phaseProperties to regionProperties.\n    4. Avoiding redundant output of diffusivity coefficients.\n    5. Applying a different method to Update the value of phi in phiI region --> use setReference of phiEqn. This seems to make the solution more stable.\n    6. Making the \"porousZone\" flexible to read. If the file doesn't exist, no porous zones are applied.\n    7. Update the test cases: rewrite the scripts.\n    8. Change the header of each file. openFuelCell is included.\n- [Dec. 2022] Update the repository\n    1. Fixed a bug in diffusivityList\n    2. Update the tutorial\n- [Sep. 2023] Update the repository for public release\n    1. A new branch is included for OpenFOAM-v2306\n    2. The prescribed mean current density and voltage are now defined as a function of time. (Assailable functions can be found in OpenFOAM/primitives/functions/Function1).\n    3. Include the radiation model in solid region.\n    4. Copy thermoTools to the repo. (need to remove this in next update.)\n    5. In test cases, when it comes to two-phase flow, a steadyState scheme is now used, specially for ddt term of species transfer.\n    6. Update the preprocessing script for an easier usage.\n\n## Related publications\n\n- Journal\n\n  - Zhang, Shidong, Steffen Hess, Holger Marschall, Uwe Reimer, Steven Beale, and Werner Lehnert. \"openFuelCell2: A New Computational Tool for Fuel Cells, Electrolyzers, and other Electrochemical Devices and Processes.\" Computer Physics Communications, Forthcoming (2023).\n\n  - Zhang, Shidong, Shangzhe Yu, Roland Peters, Steven B. Beale, Holger Marschall, Felix Kunz, and Rüdiger-A. Eichel. \"A new procedure for rapid convergence in numerical performance calculations of electrochemical cells.\" Electrochimica Acta (2023): 143275.\n\n  - Yu, Shangzhe, Shidong Zhang, Dominik Schäfer, Roland Peters, Felix Kunz, and Rüdiger-A. Eichel. \"Numerical Modeling and Simulation of the Solid Oxide Cell Stacks and Metal Interconnect Oxidation with OpenFOAM.\" Energies 16, no. 9 (2023): 3827.\n\n  - Zhang, Shidong, Steven B. Beale, Uwe Reimer, Martine Andersson, and Werner Lehnert. \"Polymer electrolyte fuel cell modeling-A comparison of two models with different levels of complexity.\" International Journal of Hydrogen Energy 45, no. 38 (2020): 19761-19777.\n\n  - Zhang, Shidong. \"Low-Temperature Polymer Electrolyte Fuel Cells.\" In Electrochemical Cell Calculations with OpenFOAM, pp. 59-85. Springer, Cham, 2022.\n\n- Conference\n\n  - Hess, Steffen, Shidong Zhang, Thomas Kadyk, Werner Lehnert, Michael Eikerling, and Steven B. Beale. \"Numerical Two-Phase Simulations of Alkaline Water Electrolyzers.\" ECS Transactions 112, no. 4 (2023): 419.\n\n  - Zhang, Shidong, Kai Wang, Shangzhe Yu, Nicolas Kruse, Roland Peters, Felix Kunz, and Rudiger-A. Eichel. \"Multiscale and Multiphysical Numerical Simulations of Solid Oxide Cell (SOC).\" ECS Transactions 111, no. 6 (2023): 937.\n\n  - Zhang, Shidong, Steven B. Beale, Yan Shi, Holger Janßen, Uwe Reimer, and Werner Lehnert. \"Development of an Open-Source Solver for Polymer Electrolyte Fuel Cells.\" ECS Transactions 98, no. 9 (2020): 317.\n\n- Thesis\n\n  - Zhang, Shidong. Modeling and Simulation of Polymer Electrolyte Fuel Cells. No. FZJ-2020-02318. Elektrochemische Verfahrenstechnik, 2020.\n\n## Developers\n\n---\n\nThe code is firstly developed by [Shidong Zhang](s.zhang@fz-juelich.de) for the PhD thesis, supervised by Prof. [Werner Lehnert](w.lehnert@fz-juelich.de) and Prof. [Steven Beale](s.beale@fz-juelich.de). The detailed model description and simulation results can be found in the thesis, `Modeling and Simulation of Polymer Electrolyte Fuel Cells` by FZJ. The following individuals also contribute to the optimization of the code,\n\n- Steffen Hess (s.hess@fz-juelich.de), Forschungszentrum Juelich, IEK-14\n\n- Prof. Steven B. Beale (s.beale@fz-juelich.de), Forschungszentrum Juelich, IEK-13\n\nTo be continued...\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/opengeosys",
            "repo_link": "https://gitlab.opengeosys.org/ogs/ogs",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/openpmd-api",
            "repo_link": "https://github.com/openPMD/openPMD-api/",
            "content": {
                "codemeta": "",
                "readme": "\nC++ & Python API for Scientific I/O with openPMD\n================================================\n\n[![Supported openPMD Standard](https://img.shields.io/badge/openPMD-1.0.0--1.1.0-blue)](https://github.com/openPMD/openPMD-standard/releases)\n[![Doxygen](https://img.shields.io/badge/API-Doxygen-blue)](https://www.openpmd.org/openPMD-api)\n[![Gitter chat](https://img.shields.io/gitter/room/openPMD/API)](https://gitter.im/openPMD/API)\n![Supported Platforms][api-platforms]\n[![License](https://img.shields.io/badge/license-LGPLv3-blue)](https://www.gnu.org/licenses/lgpl-3.0.html)\n[![DOI](https://rodare.hzdr.de/badge/DOI/10.14278/rodare.27.svg)](https://doi.org/10.14278/rodare.27)\n[![CodeFactor](https://www.codefactor.io/repository/github/openpmd/openpmd-api/badge)](https://www.codefactor.io/repository/github/openpmd/openpmd-api)\n[![Coverage Status](https://coveralls.io/repos/github/openPMD/openPMD-api/badge)](https://coveralls.io/github/openPMD/openPMD-api)\n[![Documentation Status](https://readthedocs.org/projects/openpmd-api/badge/?version=latest)](https://openpmd-api.readthedocs.io/en/latest/?badge=latest)\n[![Linux/OSX Build Status dev](https://travis-ci.com/openPMD/openPMD-api.svg?branch=dev)](https://travis-ci.com/openPMD/openPMD-api)\n[![Windows Build Status dev](https://ci.appveyor.com/api/projects/status/x95q4n620pqk0e0t/branch/dev?svg=true)](https://ci.appveyor.com/project/ax3l/openpmd-api/branch/dev)\n[![PyPI Wheel Release](https://github.com/openPMD/openPMD-api/workflows/wheels/badge.svg?branch=wheels&event=push)](https://github.com/openPMD/openPMD-api/actions?query=workflow%3Awheels)\n[![Nightly Packages Status](https://dev.azure.com/axelhuebl/openPMD-api/_apis/build/status/openPMD.openPMD-api?branchName=azure_install&label=nightly%20packages)](https://dev.azure.com/axelhuebl/openPMD-api/_build/latest?definitionId=1&branchName=azure_install)\n[![Coverity Scan Build Status](https://scan.coverity.com/projects/17602/badge.svg)](https://scan.coverity.com/projects/openpmd-openpmd-api)\n\n[api-platforms]: https://img.shields.io/badge/platforms-linux%20|%20osx%20|%20win-blue \"Supported Platforms\"\n\nopenPMD is an open meta-data schema that provides meaning and self-description for data sets in science and engineering.\nSee [the openPMD standard](https://github.com/openPMD/openPMD-standard) for details of this schema.\n\nThis library provides a reference API for openPMD data handling.\nSince openPMD is a schema (or markup) on top of portable, hierarchical file formats, this library implements various backends such as HDF5, ADIOS2 and JSON.\nWriting & reading through those backends and their associated files are supported for serial and [MPI-parallel](https://www.mpi-forum.org/docs/) workflows.\n\n## Usage\n\n### C++\n\n[![C++17][api-cpp]](https://isocpp.org/) ![C++17 API: Beta][dev-beta]\n\n[api-cpp]: https://img.shields.io/badge/language-C%2B%2B17-yellowgreen \"C++17 API\"\n[dev-beta]: https://img.shields.io/badge/phase-beta-yellowgreen \"Status: Beta\"\n\n```cpp\n#include <openPMD/openPMD.hpp>\n#include <iostream>\n\n// ...\n\nauto s = openPMD::Series(\"samples/git-sample/data%T.h5\", openPMD::Access::READ_ONLY);\n\nfor( auto const & [step, it] : s.iterations ) {\n    std::cout << \"Iteration: \" << step << \"\\n\";\n\n    for( auto const & [name, mesh] : it.meshes ) {\n        std::cout << \"  Mesh '\" << name << \"' attributes:\\n\";\n        for( auto const& val : mesh.attributes() )\n            std::cout << \"    \" << val << '\\n';\n    }\n\n    for( auto const & [name, species] : it.particles ) {\n        std::cout << \"  Particle species '\" << name << \"' attributes:\\n\";\n        for( auto const& val : species.attributes() )\n            std::cout << \"    \" << val << '\\n';\n    }\n}\n```\n\n### Python\n\n[![Python3][api-py3]](https://www.python.org/) ![Python3 API: Beta][dev-beta]\n\n[api-py3]: https://img.shields.io/badge/language-Python3-yellowgreen \"Python3 API\"\n\n\n```py\nimport openpmd_api as io\n\n# ...\n\nseries = io.Series(\"samples/git-sample/data%T.h5\", io.Access.read_only)\n\nfor k_i, i in series.iterations.items():\n    print(\"Iteration: {0}\".format(k_i))\n\n    for k_m, m in i.meshes.items():\n        print(\"  Mesh '{0}' attributes:\".format(k_m))\n        for a in m.attributes:\n            print(\"    {0}\".format(a))\n\n    for k_p, p in i.particles.items():\n        print(\"  Particle species '{0}' attributes:\".format(k_p))\n        for a in p.attributes:\n            print(\"    {0}\".format(a))\n```\n\n### More!\n\nCurious?\nOur manual shows full [read & write examples](https://openpmd-api.readthedocs.io/en/latest/usage/firstwrite.html), both serial and MPI-parallel!\n\n## Dependencies\n\nRequired:\n* CMake 3.22.0+\n* C++17 capable compiler, e.g., g++ 7+, clang 7+, MSVC 19.15+, icpc 19+, icpx\n\nShipped internally (downloaded by CMake unless `openPMD_SUPERBUILD=OFF` is set):\n* [Catch2](https://github.com/catchorg/Catch2) 2.13.10+ ([BSL-1.0](https://github.com/catchorg/Catch2/blob/master/LICENSE.txt))\n* [pybind11](https://github.com/pybind/pybind11) 2.13.0+ ([new BSD](https://github.com/pybind/pybind11/blob/master/LICENSE))\n* [NLohmann-JSON](https://github.com/nlohmann/json) 3.9.1+ ([MIT](https://github.com/nlohmann/json/blob/develop/LICENSE.MIT))\n* [toml11](https://github.com/ToruNiina/toml11) 3.7.1+ ([MIT](https://github.com/ToruNiina/toml11/blob/master/LICENSE))\n\nI/O backends:\n* [JSON](https://en.wikipedia.org/wiki/JSON)\n* [HDF5](https://support.hdfgroup.org/HDF5) 1.8.13+ (optional)\n* [ADIOS2](https://github.com/ornladios/ADIOS2) 2.7.0+ (optional)\n\nwhile those can be built either with or without:\n* MPI 2.1+, e.g. OpenMPI 1.6.5+ or MPICH2\n\nOptional language bindings:\n* Python:\n  * Python 3.8 - 3.13\n  * pybind11 2.13.0+\n  * numpy 1.15+\n  * mpi4py 2.1+ (optional, for MPI)\n  * pandas 1.0+ (optional, for dataframes)\n  * dask 2021+ (optional, for dask dataframes)\n* CUDA C++ (optional, currently used only in tests)\n\n## Installation\n\n[![Spack Package](https://img.shields.io/badge/spack.io-openpmd--api-brightgreen)](https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api)\n[![Conda Package](https://img.shields.io/badge/conda.io-openpmd--api-brightgreen)](https://anaconda.org/conda-forge/openpmd-api)\n[![Brew Package](https://img.shields.io/badge/brew.sh-openpmd--api-brightgreen)](https://github.com/openPMD/homebrew-openPMD)\n[![PyPI Package](https://img.shields.io/badge/pypi.org-openpmd--api-brightgreen)](https://pypi.org/project/openPMD-api)\n[![From Source](https://img.shields.io/badge/from_source-CMake-brightgreen)](https://cmake.org)\n\nOur community loves to help each other.\nPlease [report installation problems](https://github.com/openPMD/openPMD-api/issues/new?labels=install&template=install_problem.md) in case you should get stuck.\n\nChoose *one* of the install methods below to get started:\n\n### [Spack](https://spack.io)\n\n[![Spack Version](https://img.shields.io/spack/v/openpmd-api)](https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api)\n[![Spack Platforms](https://img.shields.io/badge/platforms-linux%20|%20osx%20-blue)](https://spack.io)\n[![Spack Use Case](https://img.shields.io/badge/use_case-desktop_%28C%2B%2B,_py%29,_development,_HPC-brightgreen)](https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api)\n\n```bash\n# optional:               +python -adios2 -hdf5 -mpi\nspack install openpmd-api\nspack load openpmd-api\n```\n\n### [Conda](https://conda.io)\n\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/openpmd-api)](https://anaconda.org/conda-forge/openpmd-api)\n[![Conda Platforms](https://img.shields.io/badge/platforms-linux%20|%20osx%20|%20win-blue)](https://anaconda.org/conda-forge/openpmd-api)\n[![Conda Use Case](https://img.shields.io/badge/use_case-desktop_%28py%29-brightgreen)](https://anaconda.org/conda-forge/openpmd-api)\n[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/openpmd-api)](https://anaconda.org/conda-forge/openpmd-api)\n\n```bash\n# optional:                      OpenMPI support  =*=mpi_openmpi*\n# optional:                        MPICH support  =*=mpi_mpich*\nconda create -n openpmd -c conda-forge openpmd-api\nconda activate openpmd\n```\n\n### [Brew](https://brew.sh)\n\n[![Brew Version](https://img.shields.io/badge/brew-latest_version-orange)](https://github.com/openPMD/homebrew-openPMD)\n[![Brew Platforms](https://img.shields.io/badge/platforms-linux%20|%20osx%20-blue)](https://docs.brew.sh/Homebrew-on-Linux)\n[![Brew Use Case](https://img.shields.io/badge/use_case-desktop_%28C%2B%2B,_py%29-brightgreen)](https://brew.sh)\n\n```bash\nbrew tap openpmd/openpmd\nbrew install openpmd-api\n```\n\n### [PyPI](https://pypi.org)\n\n[![PyPI Version](https://img.shields.io/pypi/v/openPMD-api)](https://pypi.org/project/openPMD-api)\n[![PyPI Platforms](https://img.shields.io/badge/platforms-linux%20|%20osx%20|%20win-blue)](https://pypi.org/project/openPMD-api/#files)\n[![PyPI Use Case](https://img.shields.io/badge/use_case-desktop_%28py%29-brightgreen)](https://pypi.org/project/openPMD-api)\n[![PyPI Format](https://img.shields.io/pypi/format/openPMD-api)](https://pypi.org/project/openPMD-api)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/openPMD-api)](https://pypi.org/project/openPMD-api)\n\nOn very old macOS versions (<10.9) or on exotic processor architectures, this install method *compiles from source* against the found installations of HDF5, ADIOS2, and/or MPI (in system paths, from other package managers, or loaded via a module system, ...).\n\n```bash\n# we need pip 19 or newer\n# optional:                   --user\npython3 -m pip install -U pip\n\n# optional:                        --user\npython3 -m pip install openpmd-api\n```\n\nIf MPI-support shall be enabled, we always have to recompile:\n```bash\n# optional:                                    --user\npython3 -m pip install -U pip packaging setuptools wheel\npython3 -m pip install -U cmake\n\n# optional:                                                                   --user\nopenPMD_USE_MPI=ON python3 -m pip install openpmd-api --no-binary openpmd-api\n```\n\nFor some exotic architectures and compilers, you might need to disable a compiler feature called [link-time/interprocedural optimization](https://en.wikipedia.org/wiki/Interprocedural_optimization) if you encounter linking problems:\n```bash\nexport CMAKE_INTERPROCEDURAL_OPTIMIZATION=OFF\n# optional:                                                --user\npython3 -m pip install openpmd-api --no-binary openpmd-api\n```\n\nAdditional CMake options can be passed via individual environment variables, which need to be prefixed with `openPMD_CMAKE_`.\n\n### From Source\n\n[![Source Use Case](https://img.shields.io/badge/use_case-development-brightgreen)](https://cmake.org)\n\nopenPMD-api can also be built and installed from source using [CMake](https://cmake.org/):\n\n```bash\ngit clone https://github.com/openPMD/openPMD-api.git\n\nmkdir openPMD-api-build\ncd openPMD-api-build\n\n# optional: for full tests, with unzip\n../openPMD-api/share/openPMD/download_samples.sh\n\n# for own install prefix append:\n#   -DCMAKE_INSTALL_PREFIX=$HOME/somepath\n# for options append:\n#   -DopenPMD_USE_...=...\n# e.g. for python support add:\n#   -DopenPMD_USE_PYTHON=ON -DPython_EXECUTABLE=$(which python3)\ncmake ../openPMD-api\n\ncmake --build .\n\n# optional\nctest\n\n# sudo might be required for system paths\ncmake --build . --target install\n```\n\nThe following options can be added to the `cmake` call to control features.\nCMake controls options with prefixed `-D`, e.g. `-DopenPMD_USE_MPI=OFF`:\n\n| CMake Option                 | Values           | Description                                                                  |\n|------------------------------|------------------|------------------------------------------------------------------------------|\n| `openPMD_USE_MPI`            | **AUTO**/ON/OFF  | Parallel, Multi-Node I/O for clusters                                        |\n| `openPMD_USE_HDF5`           | **AUTO**/ON/OFF  | HDF5 backend (`.h5` files)                                                   |\n| `openPMD_USE_ADIOS2`         | **AUTO**/ON/OFF  | ADIOS2 backend (`.bp` files in BP3, BP4 or higher)                           |\n| `openPMD_USE_PYTHON`         | **AUTO**/ON/OFF  | Enable Python bindings                                                       |\n| `openPMD_USE_INVASIVE_TESTS` | ON/**OFF**       | Enable unit tests that modify source code <sup>1</sup>                       |\n| `openPMD_USE_VERIFY`         | **ON**/OFF       | Enable internal VERIFY (assert) macro independent of build type <sup>2</sup> |\n| `openPMD_INSTALL`            | **ON**/OFF       | Add installation targets                                                     |\n| `openPMD_INSTALL_RPATH`      | **ON**/OFF       | Add RPATHs to installed binaries                                             |\n| `Python_EXECUTABLE`          | (newest found)   | Path to Python executable                                                    |\n\n<sup>1</sup> *e.g. changes C++ visibility keywords, breaks MSVC*\n<sup>2</sup> *this includes most pre-/post-condition checks, disabling without specific cause is highly discouraged*\n\n\nAdditionally, the following libraries are downloaded via [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html)\nduring the configuration of the project or, if the corresponding `<PACKAGENAME>_ROOT` variable is provided, can be provided externally:\n* [Catch2](https://github.com/catchorg/Catch2) (2.13.10+)\n* [PyBind11](https://github.com/pybind/pybind11) (2.13.0+)\n* [NLohmann-JSON](https://github.com/nlohmann/json) (3.9.1+)\n* [toml11](https://github.com/ToruNiina/toml11) (3.7.1+)\n\nBy default, this will build as a shared library (`libopenPMD.[so|dylib|dll]`) and installs also its headers.\nIn order to build a static library, append `-DBUILD_SHARED_LIBS=OFF` to the `cmake` command.\nYou can only build a static or a shared library at a time.\n\nBy default, the `Release` version is built.\nIn order to build with debug symbols, pass `-DCMAKE_BUILD_TYPE=Debug` to your `cmake` command.\n\nBy default, tests, examples and command line tools are built.\nIn order to skip building those, pass ``OFF`` to these ``cmake`` options:\n\n| CMake Option                  | Values     | Description              |\n|-------------------------------|------------|--------------------------|\n| `openPMD_BUILD_TESTING`       | **ON**/OFF | Build tests              |\n| `openPMD_BUILD_EXAMPLES`      | **ON**/OFF | Build examples           |\n| `openPMD_BUILD_CLI_TOOLS`     | **ON**/OFF | Build command-line tools |\n| `openPMD_USE_CUDA_EXAMPLES`   | ON/**OFF** | Use CUDA in examples     |\n\n## Linking to your project\n\nThe install will contain header files and libraries in the path set with `-DCMAKE_INSTALL_PREFIX`.\n\n### CMake\n\nIf your project is using CMake for its build, one can conveniently use our provided `openPMDConfig.cmake` package, which is installed alongside the library.\n\nFirst set the following environment hint if openPMD-api was *not* installed in a system path:\n\n```bash\n# optional: only needed if installed outside of system paths\nexport CMAKE_PREFIX_PATH=$HOME/somepath:$CMAKE_PREFIX_PATH\n```\n\nUse the following lines in your project's `CMakeLists.txt`:\n```cmake\n# supports:                       COMPONENTS MPI NOMPI HDF5 ADIOS2\nfind_package(openPMD 0.17.0 CONFIG)\n\nif(openPMD_FOUND)\n    target_link_libraries(YourTarget PRIVATE openPMD::openPMD)\nendif()\n```\n\n*Alternatively*, add the openPMD-api repository source directly to your project and use it via:\n```cmake\nadd_subdirectory(\"path/to/source/of/openPMD-api\")\n\ntarget_link_libraries(YourTarget PRIVATE openPMD::openPMD)\n```\n\nFor development workflows, you can even automatically download and build openPMD-api from within a depending CMake project.\nJust replace the `add_subdirectory` call with:\n```cmake\ninclude(FetchContent)\nset(CMAKE_POLICY_DEFAULT_CMP0077 NEW)\nset(openPMD_BUILD_CLI_TOOLS OFF)\nset(openPMD_BUILD_EXAMPLES OFF)\nset(openPMD_BUILD_TESTING OFF)\nset(openPMD_BUILD_SHARED_LIBS OFF)  # precedence over BUILD_SHARED_LIBS if needed\nset(openPMD_INSTALL OFF)            # or instead use:\n# set(openPMD_INSTALL ${BUILD_SHARED_LIBS})  # only install if used as a shared library\nset(openPMD_USE_PYTHON OFF)\nFetchContent_Declare(openPMD\n  GIT_REPOSITORY \"https://github.com/openPMD/openPMD-api.git\"\n  GIT_TAG        \"0.17.0\")\nFetchContent_MakeAvailable(openPMD)\n```\n\n### Manually\n\nIf your (Linux/OSX) project is build by calling the compiler directly or uses a manually written `Makefile`, consider using our `openPMD.pc` helper file for `pkg-config`, which are installed alongside the library.\n\nFirst set the following environment hint if openPMD-api was *not* installed in a system path:\n\n```bash\n# optional: only needed if installed outside of system paths\nexport PKG_CONFIG_PATH=$HOME/somepath/lib/pkgconfig:$PKG_CONFIG_PATH\n```\n\nAdditional linker and compiler flags for your project are available via:\n```bash\n# switch to check if openPMD-api was build as static library\n# (via BUILD_SHARED_LIBS=OFF) or as shared library (default)\nif [ \"$(pkg-config --variable=static openPMD)\" == \"true\" ]\nthen\n    pkg-config --libs --static openPMD\n    # -L/usr/local/lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lopenPMD -pthread /usr/lib/libmpi.so -pthread /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so /usr/lib/libmpi.so /usr/lib/x86_64-linux-gnu/hdf5/openmpi/libhdf5.so /usr/lib/x86_64-linux-gnu/libsz.so /usr/lib/x86_64-linux-gnu/libz.so /usr/lib/x86_64-linux-gnu/libdl.so /usr/lib/x86_64-linux-gnu/libm.so -pthread /usr/lib/libmpi.so -pthread /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so /usr/lib/libmpi.so\nelse\n    pkg-config --libs openPMD\n    # -L${HOME}/somepath/lib -lopenPMD\nfi\n\npkg-config --cflags openPMD\n# -I${HOME}/somepath/include\n```\n\n## Author Contributions\n\nopenPMD-api is developed by many people.\nIt was initially started by the [Computational Radiation Physics Group](https://hzdr.de/crp) at [HZDR](https://www.hzdr.de/) as successor to [libSplash](https://github.com/ComputationalRadiationPhysics/libSplash/), generalizing the [successful HDF5 & ADIOS1 implementations](https://arxiv.org/abs/1706.00522) in [PIConGPU](https://github.com/ComputationalRadiationPhysics/picongpu).\nThe following people and institutions [contributed](https://github.com/openPMD/openPMD-api/graphs/contributors) to openPMD-api:\n\n* [Axel Huebl (LBNL, previously HZDR)](https://github.com/ax3l):\n  project lead, releases, documentation, automated CI/CD, Python bindings, Dask, installation & packaging, prior reference implementations\n* [Franz Poeschel (CASUS)](https://github.com/franzpoeschel):\n  JSON & ADIOS2 backend, data staging/streaming, reworked class design\n* [Fabian Koller (HZDR)](https://github.com/C0nsultant):\n  initial library design and implementation with HDF5 & ADIOS1 backend\n* [Junmin Gu (LBNL)](https://github.com/guj):\n  non-collective parallel I/O fixes, ADIOS improvements, benchmarks\n\nMaintained by the following research groups:\n\n* [Computational Radiation Physics (CRD)](https://www.casus.science/casus/team/) at CASUS/HZDR, led by [Michael Bussmann](https://github.com/bussmann)\n* [Accelerator Modeling Program (AMP)](https://atap.lbl.gov/accelerator-modeling-program/) at LBNL, led by [Jean-Luc Vay](https://github.com/jlvay)\n* [Scientific Data Management (SDM)](https://crd.lbl.gov/divisions/scidata/sdm/) at LBNL, led by [Kesheng (John) Wu](https://github.com/john18)\n\nFurther thanks go to improvements and contributions from:\n\n* [Carsten Fortmann-Grote (EU XFEL GmbH, now MPI-EvolBio)](https://github.com/CFGrote):\n  draft of our Python unit tests\n* [Dominik Stańczak (Warsaw University of Technology)](https://github.com/StanczakDominik):\n  documentation improvements\n* [Ray Donnelly (Anaconda, Inc.)](https://github.com/mingwandroid):\n  support on conda packaging and libc++ quirks\n* [James Amundson (FNAL)](https://github.com/amundson):\n  compile fix for newer compilers\n* [René Widera (HZDR)](https://github.com/psychocoderHPC):\n  design improvements for initial API design\n* [Erik Zenker (HZDR)](https://github.com/erikzenker):\n  design improvements for initial API design\n* [Sergei Bastrakov (HZDR)](https://github.com/sbastrakov):\n  documentation improvements (windows)\n* [Rémi Lehe (LBNL)](https://github.com/RemiLehe):\n  package integration testing on macOS and Linux\n* [Lígia Diana Amorim (LBNL)](https://github.com/LDAmorim):\n  package integration testing on macOS\n* [Kseniia Bastrakova (HZDR)](https://github.com/KseniaBastrakova):\n  compatibility testing\n* [Richard Pausch (HZDR)](https://github.com/PrometheusPi):\n  compatibility testing, documentation improvements\n* [Paweł Ordyna (HZDR)](https://github.com/pordyna):\n  report on NVCC warnings\n* [Dmitry Ganyushin (ORNL)](https://github.com/dmitry-ganyushin):\n  Dask prototyping & ADIOS2 benchmarking\n* [John Kirkham (NVIDIA)](https://github.com/jakirkham):\n  Dask guidance & reviews\n* [Erik Schnetter (PITP)](https://github.com/eschnett):\n  C++ API bug fixes\n* [Jean Luca Bez (LBNL)](https://github.com/jeanbez):\n  HDF5 performance tuning\n* [Bernhard Manfred Gruber (CERN)](https://github.com/bernhardmgruber):\n  CMake fix for parallel HDF5\n* [Nils Schild (IPP)](https://github.com/DerNils-git):\n  CMake improvements for subprojects\n\n### Grants\n\nThe openPMD-api authors acknowledge support via the following programs.\nSupported by the CAMPA collaboration, a project of the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research and Office of High Energy Physics, Scientific Discovery through Advanced Computing (SciDAC) program.\nPreviously supported by the Consortium for Advanced Modeling of Particles Accelerators (CAMPA), funded by the U.S. DOE Office of Science under Contract No. DE-AC02-05CH11231.\nSupported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort of two U.S. Department of Energy organizations (Office of Science and the National Nuclear Security Administration).\nThis project has received funding from the European Unions Horizon 2020 research and innovation programme under grant agreement No 654220.\nThis work was partially funded by the Center of Advanced Systems Understanding (CASUS), which is financed by Germany's Federal Ministry of Education and Research (BMBF) and by the Saxon Ministry for Science, Culture and Tourism (SMWK) with tax funds on the basis of the budget approved by the Saxon State Parliament.\nSupported by the HElmholtz Laser Plasma Metadata Initiative (HELPMI) project (ZT-I-PF-3-066), funded by the \"Initiative and Networking Fund\" of the Helmholtz Association in the framework of the \"Helmholtz Metadata Collaboration\" project call 2022.\n\n### Transitive Contributions\n\nopenPMD-api stands on the shoulders of giants and we are grateful for the following projects included as direct dependencies:\n\n* [ADIOS2](https://github.com/ornladios/ADIOS2) by [S. Klasky, N. Podhorszki, W.F. Godoy (ORNL), team, collaborators](https://csmd.ornl.gov/adios) and [contributors](https://github.com/ornladios/ADIOS2/graphs/contributors)\n* [Catch2](https://github.com/catchorg/Catch2) by [Phil Nash](https://github.com/philsquared), [Martin Hořeňovský](https://github.com/horenmar) and [contributors](https://github.com/catchorg/Catch2/graphs/contributors)\n* HDF5 by [the HDF group](https://www.hdfgroup.org) and community\n* [json](https://github.com/nlohmann/json) by [Niels Lohmann](https://github.com/nlohmann) and [contributors](https://github.com/nlohmann/json/graphs/contributors)\n* [toml11](https://github.com/ToruNiina/toml11) by [Toru Niina](https://github.com/ToruNiina) and [contributors](https://github.com/ToruNiina/toml11#Contributors)\n* [pybind11](https://github.com/pybind/pybind11) by [Wenzel Jakob (EPFL)](https://github.com/wjakob) and [contributors](https://github.com/pybind/pybind11/graphs/contributors)\n* all contributors to the evolution of modern C++ and early library preview developers, e.g. [Michael Park (Facebook)](https://github.com/mpark)\n* the [CMake build system](https://cmake.org) and [contributors](https://github.com/Kitware/CMake/blob/master/Copyright.txt)\n* packaging support by the [conda-forge](https://conda-forge.org), [PyPI](https://pypi.org) and [Spack](https://spack.io) communities, among others\n* the [openPMD-standard](https://github.com/openPMD/openPMD-standard) by [Axel Huebl (HZDR, now LBNL)](https://github.com/ax3l) and [contributors](https://github.com/openPMD/openPMD-standard/blob/latest/AUTHORS.md)\n\n",
                "dependencies": "# Preamble ####################################################################\n#\ncmake_minimum_required(VERSION 3.22.0)\n\nproject(openPMD VERSION 0.17.0) # LANGUAGES CXX\n\n# the openPMD \"markup\"/\"schema\" standard version\nset(openPMD_STANDARD_VERSION 1.1.0)\n\ninclude(${openPMD_SOURCE_DIR}/cmake/openPMDFunctions.cmake)\n\n\n# CMake policies ##############################################################\n#\n# CMake 3.24+ tarball download robustness\n#   https://cmake.org/cmake/help/latest/module/ExternalProject.html#url\nif(POLICY CMP0135)\n    cmake_policy(SET CMP0135 NEW)\nendif()\n\n\n# No in-Source builds #########################################################\n#\n# In-source builds clutter up the source directory and lead to mistakes with\n# generated includes\nif(openPMD_SOURCE_DIR STREQUAL openPMD_BINARY_DIR)\n  message(FATAL_ERROR \"In-source builds are not possible. \"\n          \"Please remove the CMakeFiles/ directory and CMakeCache.txt file. \"\n          \"Then run CMake in a temporary build directory. \"\n          \"Learn more: https://hsf-training.github.io/hsf-training-cmake-webpage/02-building/index.html\")\nendif()\n\n\n# Project structure ###########################################################\n#\nget_property(isMultiConfig GLOBAL PROPERTY GENERATOR_IS_MULTI_CONFIG)\n\n# temporary build directories\nif(NOT openPMD_ARCHIVE_OUTPUT_DIRECTORY)\n    if(CMAKE_ARCHIVE_OUTPUT_DIRECTORY)\n        set(openPMD_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_ARCHIVE_OUTPUT_DIRECTORY})\n    else()\n        set(openPMD_ARCHIVE_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/lib\")\n    endif()\nendif()\nif(NOT openPMD_LIBRARY_OUTPUT_DIRECTORY)\n    if(CMAKE_LIBRARY_OUTPUT_DIRECTORY)\n        set(openPMD_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_LIBRARY_OUTPUT_DIRECTORY})\n    else()\n        set(openPMD_LIBRARY_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/lib\")\n    endif()\nendif()\nif(NOT openPMD_RUNTIME_OUTPUT_DIRECTORY)\n    if(CMAKE_RUNTIME_OUTPUT_DIRECTORY)\n        set(openPMD_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_RUNTIME_OUTPUT_DIRECTORY})\n    else()\n        set(openPMD_RUNTIME_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/bin\")\n    endif()\nendif()\nif(NOT openPMD_PDB_OUTPUT_DIRECTORY)\n    if(CMAKE_PDB_OUTPUT_DIRECTORY)\n        set(openPMD_PDB_OUTPUT_DIRECTORY ${CMAKE_PDB_OUTPUT_DIRECTORY})\n    else()\n        set(openPMD_PDB_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY})\n    endif()\nendif()\nif(NOT openPMD_COMPILE_PDB_OUTPUT_DIRECTORY)\n    if(CMAKE_COMPILE_PDB_OUTPUT_DIRECTORY)\n        set(openPMD_COMPILE_PDB_OUTPUT_DIRECTORY ${CMAKE_COMPILE_PDB_OUTPUT_DIRECTORY})\n    else()\n        set(openPMD_COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY})\n    endif()\nendif()\n\n# install directories\nif(NOT CMAKE_INSTALL_LIBDIR AND NOT WIN32)\n    include(GNUInstallDirs)\nendif()\n\nif(NOT openPMD_INSTALL_PREFIX)\n    if(CMAKE_INSTALL_PREFIX)\n        set(openPMD_INSTALL_PREFIX \"${CMAKE_INSTALL_PREFIX}\")\n    else()\n        message(FATAL_ERROR \"openPMD_INSTALL_PREFIX / CMAKE_INSTALL_PREFIX not set.\")\n    endif()\nendif()\nif(NOT openPMD_INSTALL_BINDIR)\n    if(CMAKE_INSTALL_BINDIR)\n        set(openPMD_INSTALL_BINDIR \"${CMAKE_INSTALL_BINDIR}\")\n    else()\n        set(openPMD_INSTALL_BINDIR bin)\n    endif()\nendif()\nif(NOT openPMD_INSTALL_INCLUDEDIR)\n    if(CMAKE_INSTALL_INCLUDEDIR)\n        set(openPMD_INSTALL_INCLUDEDIR \"${CMAKE_INSTALL_INCLUDEDIR}\")\n    else()\n        set(openPMD_INSTALL_INCLUDEDIR include)\n    endif()\nendif()\nif(NOT openPMD_INSTALL_LIBDIR)\n    if(CMAKE_INSTALL_LIBDIR)\n        set(openPMD_INSTALL_LIBDIR \"${CMAKE_INSTALL_LIBDIR}\")\n    else()\n        if(WIN32)\n            set(openPMD_INSTALL_LIBDIR Lib)\n        else()\n            set(openPMD_INSTALL_LIBDIR lib)\n        endif()\n    endif()\nendif()\nif(NOT openPMD_INSTALL_CMAKEDIR)\n    if(CMAKE_INSTALL_CMAKEDIR)\n        set(openPMD_INSTALL_CMAKEDIR \"${CMAKE_INSTALL_CMAKEDIR}/openPMD\")\n    else()\n        if(WIN32)\n            set(openPMD_INSTALL_CMAKEDIR \"cmake\")\n        else()\n            set(openPMD_INSTALL_CMAKEDIR \"${CMAKE_INSTALL_LIBDIR}/cmake/openPMD\")\n        endif()\n    endif()\nendif()\n\n\n# Options and Variants ########################################################\n#\nfunction(openpmd_option name description default)\n    set(openPMD_USE_${name} ${default} CACHE STRING \"${description}\")\n    set_property(CACHE openPMD_USE_${name} PROPERTY\n        STRINGS \"ON;TRUE;AUTO;OFF;FALSE\"\n    )\n    # list of all possible options\n    set(openPMD_CONFIG_OPTIONS ${openPMD_CONFIG_OPTIONS} ${name} PARENT_SCOPE)\nendfunction()\n\nopenpmd_option(MPI            \"Parallel, Multi-Node I/O for clusters\"     AUTO)\nopenpmd_option(HDF5           \"HDF5 backend (.h5 files)\"                  AUTO)\nopenpmd_option(ADIOS2         \"ADIOS2 backend (.bp files)\"                AUTO)\nopenpmd_option(PYTHON         \"Enable Python bindings\"                    AUTO)\n\noption(openPMD_INSTALL               \"Add installation targets\"             ON)\noption(openPMD_INSTALL_RPATH         \"Add RPATHs to installed binaries\"     ON)\noption(openPMD_HAVE_PKGCONFIG        \"Generate a .pc file for pkg-config\"   ON)\n\n# superbuild control\noption(openPMD_SUPERBUILD            \"Download & build extra dependencies\"  ON)\noption(openPMD_USE_INTERNAL_CATCH    \"Use internally shipped Catch2\"        ${openPMD_SUPERBUILD})\noption(openPMD_USE_INTERNAL_PYBIND11 \"Use internally shipped pybind11\"      ${openPMD_SUPERBUILD})\noption(openPMD_USE_INTERNAL_JSON     \"Use internally shipped nlohmann-json\" ${openPMD_SUPERBUILD})\noption(openPMD_USE_INTERNAL_TOML11   \"Use internally shipped toml11\"        ${openPMD_SUPERBUILD})\n\noption(openPMD_USE_INVASIVE_TESTS \"Enable unit tests that modify source code\" OFF)\noption(openPMD_USE_VERIFY \"Enable internal VERIFY (assert) macro independent of build type\" ON)\n\nset(CMAKE_CONFIGURATION_TYPES \"Release;Debug;MinSizeRel;RelWithDebInfo\")\nif(NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE \"Release\")\nendif()\n\ninclude(CMakeDependentOption)\n\n# change CMake default (static libs):\n# build shared libs if supported by target platform\nget_property(SHARED_LIBS_SUPPORTED GLOBAL PROPERTY TARGET_SUPPORTS_SHARED_LIBS)\nif(DEFINED BUILD_SHARED_LIBS)\n    set(openPMD_BUILD_SHARED_LIBS_DEFAULT ${BUILD_SHARED_LIBS})\nelse()\n    set(openPMD_BUILD_SHARED_LIBS_DEFAULT ${SHARED_LIBS_SUPPORTED})\nendif()\noption(openPMD_BUILD_SHARED_LIBS \"Build shared libraries (so/dylib/dll).\"\n    ${openPMD_BUILD_SHARED_LIBS_DEFAULT})\nif(openPMD_BUILD_SHARED_LIBS AND NOT SHARED_LIBS_SUPPORTED)\n    message(FATAL_ERROR \"openPMD_BUILD_SHARED_LIBS requested but not supported by platform\")\nendif()\n\n# Testing logic with possibility to overwrite on a project basis in superbuilds\ninclude(CTest)\nmark_as_advanced(BUILD_TESTING) # automatically defined, default: ON\nif(DEFINED BUILD_TESTING)\n    set(openPMD_BUILD_TESTING_DEFAULT ${BUILD_TESTING})\nelse()\n    set(openPMD_BUILD_TESTING_DEFAULT ON)\nendif()\noption(openPMD_BUILD_TESTING \"Build the openPMD tests\"\n    ${openPMD_BUILD_TESTING_DEFAULT})\n\n# deprecated: backwards compatibility to <= 0.13.*\nif(NOT DEFINED BUILD_CLI_TOOLS)\n    set(BUILD_CLI_TOOLS ON)\nendif()\nif(NOT DEFINED BUILD_EXAMPLES)\n    set(BUILD_EXAMPLES ON)\nendif()\noption(openPMD_BUILD_CLI_TOOLS \"Build the command line tools\" ${BUILD_CLI_TOOLS})\noption(openPMD_BUILD_EXAMPLES  \"Build the examples\" ${BUILD_EXAMPLES})\nopenpmd_option(CUDA_EXAMPLES   \"Use CUDA in examples\" OFF)\n\n\n# Helper Functions ############################################################\n#\n# C++ standard: requirements for a target\nfunction(openpmd_cxx_required target)\n    target_compile_features(${target} PUBLIC cxx_std_17)\n    set_target_properties(${target} PROPERTIES\n        CXX_EXTENSIONS OFF\n        CXX_STANDARD_REQUIRED ON\n    )\nendfunction()\n\n# CUDA C++ standard: requirements for a target\nfunction(openpmd_cuda_required target)\n   target_compile_features(${target} PUBLIC cuda_std_17)\n   set_target_properties(${target} PROPERTIES\n       CUDA_SEPARABLE_COMPILATION ON\n       CUDA_EXTENSIONS OFF\n       CUDA_STANDARD_REQUIRED ON)\nendfunction()\n\n\n# Dependencies ################################################################\n#\nmessage(STATUS \"openPMD-api superbuild: ${openPMD_SUPERBUILD}\")\n\n# external library: MPI (optional)\n# Implementation quirks for BullMPI, Clang+MPI and Brew's MPICH\n#   definitely w/o MPI::MPI_C:\n#     brew's MPICH with C-flag work-arounds - errors AppleClang for CXX targets\n#     https://github.com/Homebrew/homebrew-core/issues/80465\n#     https://lists.mpich.org/pipermail/discuss/2020-January/005863.html\n#   sometimes needed MPI::MPI_C in the past:\n#     Clang+MPI: Potentially needed MPI::MPI_C targets in the past\n#                (exact MPI flavor & Clang version lost)\n#     BullMPI: PUBLIC dependency to MPI::MPI_CXX is missing in MPI::MPI_C target\nset(openPMD_MPI_LINK_C_DEFAULT OFF)\noption(openPMD_MPI_LINK_C  \"Also link the MPI C targets\" ${openPMD_MPI_LINK_C_DEFAULT})\nmark_as_advanced(openPMD_MPI_LINK_C)\nset(openPMD_MPI_NEED_COMPONENTS CXX)\nset(openPMD_MPI_TARGETS MPI::MPI_CXX)\nif(openPMD_MPI_LINK_C)\n    set(openPMD_MPI_NEED_COMPONENTS C ${openPMD_MPI_NEED_COMPONENTS})\n    set(openPMD_MPI_TARGETS MPI::MPI_C ${openPMD_MPI_TARGETS})\nendif()\n\nif(openPMD_USE_MPI STREQUAL AUTO)\n    find_package(MPI COMPONENTS ${openPMD_MPI_NEED_COMPONENTS})\n    if(MPI_FOUND)\n        set(openPMD_HAVE_MPI TRUE)\n    else()\n        set(openPMD_HAVE_MPI FALSE)\n    endif()\nelseif(openPMD_USE_MPI)\n    find_package(MPI REQUIRED COMPONENTS ${openPMD_MPI_NEED_COMPONENTS})\n    set(openPMD_HAVE_MPI TRUE)\nelse()\n    set(openPMD_HAVE_MPI FALSE)\nendif()\n\n\n# external library: nlohmann-json (required)\ninclude(${openPMD_SOURCE_DIR}/cmake/dependencies/json.cmake)\nadd_library(openPMD::thirdparty::nlohmann_json INTERFACE IMPORTED)\ntarget_link_libraries(openPMD::thirdparty::nlohmann_json\n    INTERFACE nlohmann_json::nlohmann_json)\n\n# external library: toml11 (required)\ninclude(${openPMD_SOURCE_DIR}/cmake/dependencies/toml11.cmake)\nadd_library(openPMD::thirdparty::toml11 INTERFACE IMPORTED)\ntarget_link_libraries(openPMD::thirdparty::toml11\n    INTERFACE toml11::toml11)\n\n# external: CUDA (optional)\nif(openPMD_BUILD_EXAMPLES)  # currently only used in examples\n    if(openPMD_USE_CUDA_EXAMPLES STREQUAL AUTO)\n       find_package(CUDAToolkit)\n    elseif(openPMD_USE_CUDA_EXAMPLES)\n       find_package(CUDAToolkit REQUIRED)\n    endif()\nendif()\nif(CUDAToolkit_FOUND)\n    enable_language(CUDA)\n    set(openPMD_HAVE_CUDA_EXAMPLES TRUE)\nelse()\n    set(openPMD_HAVE_CUDA_EXAMPLES FALSE)\nendif()\n\n# external library: HDF5 (optional)\n#   note: in the new hdf5-cmake.config files, major releases like\n#         1.8, 1.10 and 1.12 are not marked compatible versions\n#   We could use CMake 3.19.0+ version ranges, but:\n#     - this issues a Wdev warning with FindHDF5.cmake\n#     - does not work at least with HDF5 1.10:\n#       Could not find a configuration file for package \"HDF5\" that is compatible\n#         with requested version range \"1.8.13...1.12\".\n#         The following configuration files were considered but not accepted:\n#         ../share/cmake/hdf5/hdf5-config.cmake, version: 1.10.7\n#     - thus, we do our own HDF5_VERSION check...\nif(openPMD_USE_HDF5 STREQUAL AUTO)\n    set(HDF5_PREFER_PARALLEL ${openPMD_HAVE_MPI})\n    find_package(HDF5 COMPONENTS C)\n    if(HDF5_FOUND)\n        set(openPMD_HAVE_HDF5 TRUE)\n    else()\n        set(openPMD_HAVE_HDF5 FALSE)\n    endif()\nelseif(openPMD_USE_HDF5)\n    set(HDF5_PREFER_PARALLEL ${openPMD_HAVE_MPI})\n    find_package(HDF5 REQUIRED COMPONENTS C)\n    set(openPMD_HAVE_HDF5 TRUE)\nelse()\n    set(openPMD_HAVE_HDF5 FALSE)\nendif()\n\n# HDF5 checks\nstring(CONCAT openPMD_HDF5_STATUS \"\")\n# version: lower limit\nif(openPMD_HAVE_HDF5)\n    if(\"${HDF5_VERSION}\" STREQUAL \"\")\n        message(WARNING \"HDF5_VERSION is empty. Now assuming it is 1.8.13 or newer.\")\n    else()\n        if(HDF5_VERSION VERSION_LESS 1.8.13)\n            string(CONCAT openPMD_HDF5_STATUS\n                \"Found HDF5 version ${HDF5_VERSION} is too old. At least \"\n                \"version 1.8.13 is required.\\n\")\n        endif()\n    endif()\nendif()\n# we imply support for parallel I/O if MPI variant is ON\nif(openPMD_HAVE_MPI AND openPMD_HAVE_HDF5\n    AND NOT HDF5_IS_PARALLEL      # FindHDF5.cmake\n    AND NOT HDF5_ENABLE_PARALLEL  # hdf5-config.cmake\n)\n    string(CONCAT openPMD_HDF5_STATUS\n            \"Found MPI but only serial version of HDF5. Either set \"\n            \"openPMD_USE_MPI=OFF to disable MPI or set openPMD_USE_HDF5=OFF \"\n            \"to disable HDF5 or provide a parallel install of HDF5.\\n\")\nendif()\n# HDF5 includes mpi.h in the public header H5public.h if parallel\nif(openPMD_HAVE_HDF5 AND\n   (HDF5_IS_PARALLEL OR HDF5_ENABLE_PARALLEL)\n   AND NOT openPMD_HAVE_MPI)\n    string(CONCAT openPMD_HDF5_STATUS\n        \"Found only parallel version of HDF5 but no MPI. Either set \"\n        \"openPMD_USE_MPI=ON to force using MPI or set openPMD_USE_HDF5=OFF \"\n        \"to disable HDF5 or provide a serial install of HDF5.\\n\")\nendif()\n\nif(openPMD_HDF5_STATUS)\n    string(CONCAT openPMD_HDF5_STATUS\n        ${openPMD_HDF5_STATUS}\n        \"If you manually installed a version of HDF5 in \"\n        \"a non-default path, add its installation prefix to the \"\n        \"environment variable CMAKE_PREFIX_PATH to find it: \"\n        \"https://cmake.org/cmake/help/latest/envvar/CMAKE_PREFIX_PATH.html\")\n    if(openPMD_USE_HDF5 STREQUAL AUTO)\n        message(WARNING \"${openPMD_HDF5_STATUS}\")\n        set(openPMD_HAVE_HDF5 FALSE)\n    elseif(openPMD_USE_HDF5)\n        message(FATAL_ERROR \"${openPMD_HDF5_STATUS}\")\n    endif()\nendif()\n\n# external library: ADIOS2 (optional)\nset(openPMD_REQUIRED_ADIOS2_COMPONENTS CXX)\nif(openPMD_HAVE_MPI)\n    list(APPEND openPMD_REQUIRED_ADIOS2_COMPONENTS MPI)\nendif()\nif(openPMD_USE_ADIOS2 STREQUAL AUTO)\n    find_package(ADIOS2 2.7.0 CONFIG COMPONENTS ${openPMD_REQUIRED_ADIOS2_COMPONENTS})\n    if(ADIOS2_FOUND)\n        set(openPMD_HAVE_ADIOS2 TRUE)\n    else()\n        set(openPMD_HAVE_ADIOS2 FALSE)\n    endif()\nelseif(openPMD_USE_ADIOS2)\n    find_package(ADIOS2 2.7.0 REQUIRED CONFIG COMPONENTS ${openPMD_REQUIRED_ADIOS2_COMPONENTS})\n    set(openPMD_HAVE_ADIOS2 TRUE)\nelse()\n    set(openPMD_HAVE_ADIOS2 FALSE)\nendif()\nunset(openPMD_REQUIRED_ADIOS2_COMPONENTS)\n\n# external library: pybind11 (optional)\ninclude(${openPMD_SOURCE_DIR}/cmake/dependencies/pybind11.cmake)\n\n\n# Targets #####################################################################\n#\nset(CORE_SOURCE\n        src/config.cpp\n        src/ChunkInfo.cpp\n        src/Dataset.cpp\n        src/Datatype.cpp\n        src/Error.cpp\n        src/Format.cpp\n        src/Iteration.cpp\n        src/IterationEncoding.cpp\n        src/Mesh.cpp\n        src/ParticlePatches.cpp\n        src/ParticleSpecies.cpp\n        src/Record.cpp\n        src/ReadIterations.cpp\n        src/RecordComponent.cpp\n        src/Series.cpp\n        src/UnitDimension.cpp\n        src/version.cpp\n        src/auxiliary/Date.cpp\n        src/auxiliary/Filesystem.cpp\n        src/auxiliary/JSON.cpp\n        src/auxiliary/Mpi.cpp\n        src/backend/Attributable.cpp\n        src/backend/BaseRecordComponent.cpp\n        src/backend/MeshRecordComponent.cpp\n        src/backend/PatchRecord.cpp\n        src/backend/PatchRecordComponent.cpp\n        src/backend/Writable.cpp\n        src/benchmark/mpi/OneDimensionalBlockSlicer.cpp\n        src/helper/list_series.cpp\n        src/snapshots/ContainerImpls.cpp\n        src/snapshots/ContainerTraits.cpp\n        src/snapshots/IteratorHelpers.cpp\n        src/snapshots/IteratorTraits.cpp\n        src/snapshots/RandomAccessIterator.cpp\n        src/snapshots/Snapshots.cpp\n        src/snapshots/StatefulIterator.cpp)\nset(IO_SOURCE\n        src/IO/AbstractIOHandler.cpp\n        src/IO/AbstractIOHandlerImpl.cpp\n        src/IO/AbstractIOHandlerHelper.cpp\n        src/IO/Access.cpp\n        src/IO/DummyIOHandler.cpp\n        src/IO/IOTask.cpp\n        src/IO/FlushParams.cpp\n        src/IO/HDF5/HDF5IOHandler.cpp\n        src/IO/HDF5/ParallelHDF5IOHandler.cpp\n        src/IO/HDF5/HDF5Auxiliary.cpp\n        src/IO/JSON/JSONIOHandler.cpp\n        src/IO/JSON/JSONIOHandlerImpl.cpp\n        src/IO/JSON/JSONFilePosition.cpp\n        src/IO/ADIOS/ADIOS2IOHandler.cpp\n        src/IO/ADIOS/ADIOS2File.cpp\n        src/IO/ADIOS/ADIOS2Auxiliary.cpp\n        src/IO/InvalidatableFile.cpp)\n\n# library\nif(openPMD_BUILD_SHARED_LIBS)\n    set(_openpmd_lib_type SHARED)\nelse()\n    set(_openpmd_lib_type STATIC)\nendif()\nadd_library(openPMD ${_openpmd_lib_type} ${CORE_SOURCE} ${IO_SOURCE})\nadd_library(openPMD::openPMD ALIAS openPMD)\n\n# properties\nopenpmd_cxx_required(openPMD)\nset_target_properties(openPMD PROPERTIES\n    COMPILE_PDB_NAME openPMD\n    ARCHIVE_OUTPUT_DIRECTORY ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}\n    LIBRARY_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY}\n    RUNTIME_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n    PDB_OUTPUT_DIRECTORY ${openPMD_PDB_OUTPUT_DIRECTORY}\n    COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_COMPILE_PDB_OUTPUT_DIRECTORY}\n\n    POSITION_INDEPENDENT_CODE ON\n    WINDOWS_EXPORT_ALL_SYMBOLS ON\n)\n# note: same as above, but for Multi-Config generators\nif(isMultiConfig)\n    # this is a tweak for setup.py to pick up our libs & pybind module properly\n    # this assumes there will only be one config built\n    option(openPMD_BUILD_NO_CFG_SUBPATH\n           \"For multi-config builds, do not appends the config to build dir\" OFF)\n    mark_as_advanced(openPMD_BUILD_NO_CFG_SUBPATH)\n\n    foreach(CFG IN LISTS CMAKE_CONFIGURATION_TYPES)\n        string(TOUPPER \"${CFG}\" CFG_UPPER)\n        if(openPMD_BUILD_NO_CFG_SUBPATH)  # for setup.py\n            set(CFG_PATH \"\")\n        else()\n            set(CFG_PATH \"/${CFG}\")\n        endif()\n        set_target_properties(openPMD PROPERTIES\n            COMPILE_PDB_NAME_${CFG_UPPER} openPMD\n            ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}${CFG_PATH}\n            LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_LIBRARY_OUTPUT_DIRECTORY}${CFG_PATH}\n            RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}${CFG_PATH}\n            PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PDB_OUTPUT_DIRECTORY}${CFG_PATH}\n            COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_COMPILE_PDB_OUTPUT_DIRECTORY}${CFG_PATH}\n        )\n    endforeach()\nendif()\nset(_cxx_msvc   \"$<AND:$<COMPILE_LANGUAGE:CXX>,$<CXX_COMPILER_ID:MSVC>>\")\nset(_msvc_1914  \"$<VERSION_GREATER_EQUAL:$<CXX_COMPILER_VERSION>,19.14>\")\nset(_msvc_options)\nlist(APPEND _msvc_options\n    $<${_cxx_msvc}:/bigobj>\n    $<${_cxx_msvc}:$<${_msvc_1914}:/Zc:__cplusplus>>\n)\ntarget_compile_options(openPMD PUBLIC ${_msvc_options})\n\n# own headers\ntarget_include_directories(openPMD PUBLIC\n    $<BUILD_INTERFACE:${openPMD_SOURCE_DIR}/include>\n    $<BUILD_INTERFACE:${openPMD_BINARY_DIR}/include>\n    $<INSTALL_INTERFACE:include>\n)\n\n# Catch2 for unit tests\nif(openPMD_BUILD_TESTING)\n    include(${openPMD_SOURCE_DIR}/cmake/dependencies/catch.cmake)\n    add_library(openPMD::thirdparty::Catch2 INTERFACE IMPORTED)\n    target_link_libraries(openPMD::thirdparty::Catch2\n            INTERFACE Catch2::Catch2)\nendif()\n\nif(openPMD_HAVE_MPI)\n    target_link_libraries(openPMD PUBLIC ${openPMD_MPI_TARGETS})\nendif()\n\n# JSON Backend and User-Facing Runtime Options\n#target_link_libraries(openPMD PRIVATE openPMD::thirdparty::nlohmann_json)\ntarget_include_directories(openPMD SYSTEM PRIVATE\n    $<TARGET_PROPERTY:openPMD::thirdparty::nlohmann_json,INTERFACE_INCLUDE_DIRECTORIES>\n    $<TARGET_PROPERTY:openPMD::thirdparty::toml11,INTERFACE_INCLUDE_DIRECTORIES>)\n\n# HDF5 Backend\n#   TODO: Once we require CMake 3.20+, simply link hdf5::hdf5 C lib target\nif(openPMD_HAVE_HDF5)\n    target_link_libraries(openPMD PUBLIC ${HDF5_LIBRARIES})\n    target_include_directories(openPMD SYSTEM PRIVATE ${HDF5_INCLUDE_DIRS})\n    target_compile_definitions(openPMD PRIVATE ${HDF5_DEFINITIONS})\nendif()\n\n# ADIOS2 Backend\nif(openPMD_HAVE_ADIOS2)\n    if(openPMD_HAVE_MPI)\n        target_link_libraries(openPMD PUBLIC adios2::cxx11_mpi)\n    else()\n        target_link_libraries(openPMD PUBLIC adios2::cxx11)\n    endif()\nendif()\n\n# Runtime parameter and API status checks (\"asserts\")\nif(openPMD_USE_VERIFY)\n    target_compile_definitions(openPMD PRIVATE openPMD_USE_VERIFY=1)\nelse()\n    target_compile_definitions(openPMD PRIVATE openPMD_USE_VERIFY=0)\nendif()\n\n# python bindings\nif(openPMD_HAVE_PYTHON)\n    add_library(openPMD.py MODULE\n        src/binding/python/openPMD.cpp\n        src/binding/python/Access.cpp\n        src/binding/python/Attributable.cpp\n        src/binding/python/BaseRecordComponent.cpp\n        src/binding/python/ChunkInfo.cpp\n        src/binding/python/Dataset.cpp\n        src/binding/python/Datatype.cpp\n        src/binding/python/Error.cpp\n        src/binding/python/Helper.cpp\n        src/binding/python/Iteration.cpp\n        src/binding/python/IterationEncoding.cpp\n        src/binding/python/Mesh.cpp\n        src/binding/python/ParticlePatches.cpp\n        src/binding/python/ParticleSpecies.cpp\n        src/binding/python/PatchRecord.cpp\n        src/binding/python/PatchRecordComponent.cpp\n        src/binding/python/Record.cpp\n        src/binding/python/RecordComponent.cpp\n        src/binding/python/MeshRecordComponent.cpp\n        src/binding/python/Series.cpp\n        src/binding/python/UnitDimension.cpp\n    )\n    target_link_libraries(openPMD.py PRIVATE openPMD)\n    target_link_libraries(openPMD.py PRIVATE pybind11::module pybind11::windows_extras)\n\n    # LTO/IPO: CMake target properties work well for 3.18+ and are buggy before\n    set(_USE_PY_LTO ON)  # default shall be ON\n    if(DEFINED CMAKE_INTERPROCEDURAL_OPTIMIZATION)  # overwrite default if defined\n        if(NOT CMAKE_INTERPROCEDURAL_OPTIMIZATION)\n            set(_USE_PY_LTO OFF)\n        endif()\n    endif()\n    message(STATUS \"Python LTO/IPO: ${_USE_PY_LTO}\")\n    set_target_properties(openPMD.py PROPERTIES\n        INTERPROCEDURAL_OPTIMIZATION ${_USE_PY_LTO})\n    unset(_USE_PY_LTO)\n\n    if(EMSCRIPTEN)\n        set_target_properties(openPMD.py PROPERTIES\n            PREFIX \"\")\n    else()\n        pybind11_extension(openPMD.py)\n    endif()\n    if(NOT MSVC AND NOT ${CMAKE_BUILD_TYPE} MATCHES Debug|RelWithDebInfo)\n        pybind11_strip(openPMD.py)\n    endif()\n\n    set_target_properties(openPMD.py PROPERTIES CXX_VISIBILITY_PRESET \"hidden\"\n                                                CUDA_VISIBILITY_PRESET \"hidden\")\n\n    # ancient Clang releases\n    #   https://github.com/openPMD/openPMD-api/issues/542\n    #   https://pybind11.readthedocs.io/en/stable/faq.html#recursive-template-instantiation-exceeded-maximum-depth-of-256\n    #   https://bugs.llvm.org/show_bug.cgi?id=18417\n    #   https://github.com/llvm/llvm-project/commit/e55b4737c026ea2e0b44829e4115d208577a67b2\n    if((\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"AppleClang\" AND\n        CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.1) OR\n       (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\" AND\n        CMAKE_CXX_COMPILER_VERSION VERSION_LESS 4.0))\n            message(STATUS \"Clang: Passing -ftemplate-depth=1024\")\n            target_compile_options(openPMD.py\n                PRIVATE -ftemplate-depth=1024)\n    endif()\n\n    if(WIN32)\n        set(openPMD_INSTALL_PYTHONDIR_DEFAULT\n            \"${CMAKE_INSTALL_LIBDIR}/site-packages\")\n    else()\n        set(openPMD_INSTALL_PYTHONDIR_DEFAULT\n            \"${CMAKE_INSTALL_LIBDIR}/python${Python_VERSION_MAJOR}.${Python_VERSION_MINOR}/site-packages\"\n        )\n    endif()\n    set(openPMD_INSTALL_PYTHONDIR \"${openPMD_INSTALL_PYTHONDIR_DEFAULT}\"\n        CACHE STRING \"Location for installed python package\")\n    set(openPMD_PYTHON_OUTPUT_DIRECTORY \"${openPMD_BINARY_DIR}/${openPMD_INSTALL_PYTHONDIR}\"\n        CACHE STRING \"Build directory for python modules\")\n    set_target_properties(openPMD.py PROPERTIES\n        ARCHIVE_OUTPUT_NAME openpmd_api_cxx\n        LIBRARY_OUTPUT_NAME openpmd_api_cxx\n        COMPILE_PDB_NAME openpmd_api_cxx\n        ARCHIVE_OUTPUT_DIRECTORY ${openPMD_PYTHON_OUTPUT_DIRECTORY}/openpmd_api\n        LIBRARY_OUTPUT_DIRECTORY ${openPMD_PYTHON_OUTPUT_DIRECTORY}/openpmd_api\n        RUNTIME_OUTPUT_DIRECTORY ${openPMD_PYTHON_OUTPUT_DIRECTORY}/openpmd_api\n        PDB_OUTPUT_DIRECTORY ${openPMD_PYTHON_OUTPUT_DIRECTORY}/openpmd_api\n        COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_PYTHON_OUTPUT_DIRECTORY}/openpmd_api\n    )\n    # note: same as above, but for Multi-Config generators\n    if(isMultiConfig)\n        foreach(CFG IN LISTS CMAKE_CONFIGURATION_TYPES)\n            string(TOUPPER \"${CFG}\" CFG_UPPER)\n            if(openPMD_BUILD_NO_CFG_SUBPATH)  # for setup.py\n                set(CFG_PATH \"\")\n            else()\n                set(CFG_PATH \"/${CFG}\")\n            endif()\n            set_target_properties(openPMD.py PROPERTIES\n                COMPILE_PDB_NAME_${CFG_UPPER} openpmd_api_cxx\n                ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PYTHON_OUTPUT_DIRECTORY}${CFG_PATH}/openpmd_api\n                LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PYTHON_OUTPUT_DIRECTORY}${CFG_PATH}/openpmd_api\n                RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PYTHON_OUTPUT_DIRECTORY}${CFG_PATH}/openpmd_api\n                PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PYTHON_OUTPUT_DIRECTORY}${CFG_PATH}/openpmd_api\n                COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PYTHON_OUTPUT_DIRECTORY}${CFG_PATH}/openpmd_api\n            )\n        endforeach()\n    endif()\n    function(copy_aux_py)\n        set(AUX_PY_SRC_DIR ${openPMD_SOURCE_DIR}/src/binding/python/openpmd_api/)\n        set(AUX_PY_DSR_DIR ${openPMD_PYTHON_OUTPUT_DIRECTORY}/openpmd_api/)\n        foreach(src_name IN LISTS ARGN)\n            configure_file(${AUX_PY_SRC_DIR}/${src_name} ${AUX_PY_DSR_DIR}/${src_name} COPYONLY)\n        endforeach()\n    endfunction()\n    copy_aux_py(\n        __init__.py DaskArray.py DaskDataFrame.py DataFrame.py\n        ls/__init__.py   ls/__main__.py\n        pipe/__init__.py pipe/__main__.py\n    )\nendif()\n\n# tests\nset(openPMD_TEST_NAMES\n    Core\n    Auxiliary\n    SerialIO\n    ParallelIO\n    JSON\n)\n# command line tools\nset(openPMD_CLI_TOOL_NAMES\n    ls\n)\nset(openPMD_PYTHON_CLI_TOOL_NAMES\n    pipe\n)\nset(openPMD_PYTHON_CLI_MODULE_NAMES ${openPMD_CLI_TOOL_NAMES})\n# examples\nset(openPMD_EXAMPLE_NAMES\n    1_structure\n    2_read_serial\n    2a_read_thetaMode_serial\n    3_write_serial\n    3a_write_thetaMode_serial\n    3b_write_resizable_particles\n    4_read_parallel\n    5_write_parallel\n    6_dump_filebased_series\n    7_extended_write_serial\n    8_benchmark_parallel\n    8a_benchmark_write_parallel\n    8b_benchmark_read_parallel\n    10_streaming_write\n    10_streaming_read\n    12_span_write\n    13_write_dynamic_configuration\n    14_toml_template\n)\nset(openPMD_PYTHON_EXAMPLE_NAMES\n    2_read_serial\n    2a_read_thetaMode_serial\n    3_write_serial\n    3a_write_thetaMode_serial\n    3b_write_resizable_particles\n    4_read_parallel\n    5_write_parallel\n    7_extended_write_serial\n    9_particle_write_serial\n    10_streaming_write\n    10_streaming_read\n    11_particle_dataframe\n    12_span_write\n    13_write_dynamic_configuration\n)\n\nif(openPMD_USE_INVASIVE_TESTS)\n    if(WIN32)\n        message(WARNING \"Invasive tests that redefine class signatures are \"\n                        \"known to fail on Windows!\")\n    endif()\n    target_compile_definitions(openPMD PRIVATE openPMD_USE_INVASIVE_TESTS=1)\nendif()\n\nif(openPMD_BUILD_TESTING)\n    # compile Catch2 implementation part separately\n    add_library(CatchRunner ${_openpmd_lib_type}\n        test/CatchRunner.cpp)  # Always MPI_Init with Serial Fallback\n    add_library(CatchMain   ${_openpmd_lib_type}\n        test/CatchMain.cpp)    # Serial only\n    openpmd_cxx_required(CatchRunner)\n    openpmd_cxx_required(CatchMain)\n    set_target_properties(CatchRunner CatchMain PROPERTIES\n        ARCHIVE_OUTPUT_DIRECTORY ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}\n        LIBRARY_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY}\n        RUNTIME_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n        PDB_OUTPUT_DIRECTORY ${openPMD_PDB_OUTPUT_DIRECTORY}\n        COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_COMPILE_PDB_OUTPUT_DIRECTORY}\n\n        POSITION_INDEPENDENT_CODE ON\n        WINDOWS_EXPORT_ALL_SYMBOLS ON\n    )\n    set_target_properties(CatchRunner PROPERTIES COMPILE_PDB_NAME CatchRunner)\n    set_target_properties(CatchMain PROPERTIES COMPILE_PDB_NAME CatchMain)\n    # note: same as above, but for Multi-Config generators\n    if(isMultiConfig)\n        foreach(CFG IN LISTS CMAKE_CONFIGURATION_TYPES)\n            string(TOUPPER \"${CFG}\" CFG_UPPER)\n            set_target_properties(CatchRunner PROPERTIES\n                COMPILE_PDB_NAME_${CFG_UPPER} CatchRunner\n                ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}/${CFG}\n                LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_LIBRARY_OUTPUT_DIRECTORY}/${CFG}\n                RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG}\n                PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PDB_OUTPUT_DIRECTORY}/${CFG}\n                COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_COMPILE_PDB_OUTPUT_DIRECTORY}/${CFG}\n            )\n            set_target_properties(CatchMain PROPERTIES\n                COMPILE_PDB_NAME_${CFG_UPPER} CatchMain\n                ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}/${CFG}\n                LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_LIBRARY_OUTPUT_DIRECTORY}/${CFG}\n                RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG}\n                PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PDB_OUTPUT_DIRECTORY}/${CFG}\n                COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_COMPILE_PDB_OUTPUT_DIRECTORY}/${CFG}\n            )\n        endforeach()\n    endif()\n    target_compile_options(CatchRunner PUBLIC ${_msvc_options})\n    target_compile_options(CatchMain   PUBLIC ${_msvc_options})\n    target_link_libraries(CatchRunner PUBLIC openPMD::thirdparty::Catch2)\n    target_link_libraries(CatchMain   PUBLIC openPMD::thirdparty::Catch2)\n    if(openPMD_HAVE_MPI)\n        target_link_libraries(CatchRunner PUBLIC ${openPMD_MPI_TARGETS})\n        target_compile_definitions(CatchRunner PUBLIC openPMD_HAVE_MPI=1)\n    endif()\n\n    macro(additional_testing_sources test_name out_list)\n        if(${test_name} STREQUAL \"SerialIO\")\n            list(APPEND ${out_list}\n                test/Files_SerialIO/close_and_reopen_test.cpp\n                test/Files_SerialIO/filebased_write_test.cpp\n            )\n        endif()\n    endmacro()\n\n    foreach(testname ${openPMD_TEST_NAMES})\n        set(ADDITIONAL_SOURCE_FILES \"\")\n        additional_testing_sources(${testname} ADDITIONAL_SOURCE_FILES)\n        add_executable(${testname}Tests test/${testname}Test.cpp ${ADDITIONAL_SOURCE_FILES})\n        target_include_directories(${testname}Tests PRIVATE test/Files_${testname}/)\n        openpmd_cxx_required(${testname}Tests)\n        set_target_properties(${testname}Tests PROPERTIES\n            COMPILE_PDB_NAME ${testname}Tests\n            ARCHIVE_OUTPUT_DIRECTORY ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}\n            LIBRARY_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY}\n            RUNTIME_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n            PDB_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n            COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n        )\n        # note: same as above, but for Multi-Config generators\n        if(isMultiConfig)\n            foreach(CFG IN LISTS CMAKE_CONFIGURATION_TYPES)\n                string(TOUPPER \"${CFG}\" CFG_UPPER)\n                set_target_properties(${testname}Tests PROPERTIES\n                    COMPILE_PDB_NAME_${CFG_UPPER} ${testname}Tests\n                    ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}/${CFG}\n                    LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_LIBRARY_OUTPUT_DIRECTORY}/${CFG}\n                    RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG}\n                    PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG}\n                    COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG}\n                )\n            endforeach()\n        endif()\n\n        if(openPMD_USE_INVASIVE_TESTS)\n            target_compile_definitions(${testname}Tests PRIVATE openPMD_USE_INVASIVE_TESTS=1)\n        endif()\n        target_link_libraries(${testname}Tests PRIVATE openPMD)\n        if(${testname} MATCHES \"Parallel.+$\")\n            target_link_libraries(${testname}Tests PRIVATE CatchRunner)\n        else()\n            target_link_libraries(${testname}Tests PRIVATE CatchMain)\n        endif()\n\n        if(${testname} STREQUAL JSON)\n            target_include_directories(${testname}Tests SYSTEM PRIVATE\n                $<TARGET_PROPERTY:openPMD::thirdparty::nlohmann_json,INTERFACE_INCLUDE_DIRECTORIES>\n                $<TARGET_PROPERTY:openPMD::thirdparty::toml11,INTERFACE_INCLUDE_DIRECTORIES>)\n        endif()\n    endforeach()\nendif()\n\nif(openPMD_BUILD_CLI_TOOLS)\n    foreach(toolname ${openPMD_CLI_TOOL_NAMES})\n        add_executable(openpmd-${toolname} src/cli/${toolname}.cpp)\n        openpmd_cxx_required(openpmd-${toolname})\n        set_target_properties(openpmd-${toolname} PROPERTIES\n            COMPILE_PDB_NAME openpmd-${toolname}\n            ARCHIVE_OUTPUT_DIRECTORY ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}\n            LIBRARY_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY}\n            RUNTIME_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n            PDB_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n            COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n        )\n        # note: same as above, but for Multi-Config generators\n        if(isMultiConfig)\n            foreach(CFG IN LISTS CMAKE_CONFIGURATION_TYPES)\n                string(TOUPPER \"${CFG}\" CFG_UPPER)\n                set_target_properties(openpmd-${toolname} PROPERTIES\n                    COMPILE_PDB_NAME_${CFG_UPPER} openpmd-${toolname}\n                    ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}/${CFG}\n                    LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_LIBRARY_OUTPUT_DIRECTORY}/${CFG}\n                    RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG}\n                    PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG}\n                    COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG}\n                )\n            endforeach()\n        endif()\n\n        target_link_libraries(openpmd-${toolname} PRIVATE openPMD)\n    endforeach()\nendif()\n\nif(openPMD_BUILD_EXAMPLES)\n    foreach(examplename ${openPMD_EXAMPLE_NAMES})\n        if(${examplename} MATCHES \".+parallel$\" AND NOT openPMD_HAVE_MPI)\n            # skip parallel test\n            continue()\n        endif()\n\n        add_executable(${examplename} examples/${examplename}.cpp)\n        if (openPMD_HAVE_CUDA_EXAMPLES)\n           set_source_files_properties(examples/${examplename}.cpp\n               PROPERTIES LANGUAGE CUDA)\n           openpmd_cuda_required(${examplename})\n        else()\n           openpmd_cxx_required(${examplename})\n        endif()\n        set_target_properties(${examplename} PROPERTIES\n            COMPILE_PDB_NAME ${examplename}\n            ARCHIVE_OUTPUT_DIRECTORY ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}\n            LIBRARY_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY}\n            RUNTIME_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n            PDB_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n            COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n        )\n        # note: same as above, but for Multi-Config generators\n        if(isMultiConfig)\n            foreach(CFG IN LISTS CMAKE_CONFIGURATION_TYPES)\n                string(TOUPPER \"${CFG}\" CFG_UPPER)\n                set_target_properties(${examplename} PROPERTIES\n                    COMPILE_PDB_NAME_${CFG_UPPER} ${examplename}\n                    ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}/${CFG}\n                    LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_LIBRARY_OUTPUT_DIRECTORY}/${CFG}\n                    RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG}\n                    PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG}\n                    COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG}\n                )\n            endforeach()\n        endif()\n        target_link_libraries(${examplename} PRIVATE openPMD)\n    endforeach()\nendif()\n\n\n# Warnings ####################################################################\n#\n# TODO: LEGACY! Use CMake TOOLCHAINS instead!\nif(CMAKE_SOURCE_DIR STREQUAL PROJECT_SOURCE_DIR)\n    # On Windows, Clang -Wall aliases -Weverything; default is /W3\n    if (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\" AND NOT WIN32)\n        # list(APPEND CMAKE_CXX_FLAGS \"-fsanitize=address\") # address, memory, undefined\n        # set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -fsanitize=address\")\n        # set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -fsanitize=address\")\n        # set(CMAKE_MODULE_LINKER_FLAGS \"${CMAKE_MODULE_LINKER_FLAGS} -fsanitize=address\")\n\n        # note: might still need a\n        #   export LD_PRELOAD=libclang_rt.asan.so\n        # or on Debian 9 with Clang 6.0\n        #   export LD_PRELOAD=/usr/lib/llvm-6.0/lib/clang/6.0.0/lib/linux/libclang_rt.asan-x86_64.so:\n        #                     /usr/lib/llvm-6.0/lib/clang/6.0.0/lib/linux/libclang_rt.ubsan_minimal-x86_64.so\n        # at runtime when used with symbol-hidden code (e.g. pybind11 module)\n\n        set(CMAKE_CXX_FLAGS \"-Wall -Wextra -Wpedantic -Wshadow -Woverloaded-virtual -Wextra-semi -Wunreachable-code -Wsign-compare ${CMAKE_CXX_FLAGS}\")\n    elseif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"AppleClang\")\n        set(CMAKE_CXX_FLAGS \"-Wall -Wextra -Wpedantic -Wshadow -Woverloaded-virtual -Wextra-semi -Wunreachable-code -Wsign-compare ${CMAKE_CXX_FLAGS}\")\n    elseif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Intel\")\n        set(CMAKE_CXX_FLAGS \"-w3 -wd193,383,1572 ${CMAKE_CXX_FLAGS}\")\n    elseif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n        set(CMAKE_CXX_FLAGS \"-Wall -Wextra -Wpedantic -Wshadow -Woverloaded-virtual -Wunreachable-code -Wsign-compare ${CMAKE_CXX_FLAGS}\")\n    elseif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")\n        # Warning C4503: \"decorated name length exceeded, name was truncated\"\n        # Symbols longer than 4096 chars are truncated (and hashed instead)\n        set(CMAKE_CXX_FLAGS \"-wd4503 ${CMAKE_CXX_FLAGS}\")\n        # Warning C4244: \"conversion from 'X' to 'Y', possible loss of data\"\n        set(CMAKE_CXX_FLAGS \"-wd4244 ${CMAKE_CXX_FLAGS}\")\n        # Yes, you should build against the same C++ runtime and with same\n        # configuration (Debug/Release). MSVC does inconvenient choices for their\n        # developers, so be it. (Our Windows-users use conda-forge builds, which\n        # are consistent.)\n        set(CMAKE_CXX_FLAGS \"-wd4251 ${CMAKE_CXX_FLAGS}\")\n    endif()\nendif()\n\n\n# Generate Files with Configuration Options ###################################\n#\n# TODO configure a version.hpp\nconfigure_file(\n    ${openPMD_SOURCE_DIR}/include/openPMD/config.hpp.in\n    ${openPMD_BINARY_DIR}/include/openPMD/config.hpp\n    @ONLY\n)\n\nconfigure_file(\n    ${openPMD_SOURCE_DIR}/openPMDConfig.cmake.in\n    ${openPMD_BINARY_DIR}/openPMDConfig.cmake\n    @ONLY\n)\n\n# get absolute paths to linked libraries\nfunction(openpmdreclibs tgtname outname)\n    get_target_property(PC_PRIVATE_LIBS_TGT ${tgtname} INTERFACE_LINK_LIBRARIES)\n    foreach(PC_LIB IN LISTS PC_PRIVATE_LIBS_TGT)\n       if(TARGET ${PC_LIB})\n           openpmdreclibs(${PC_LIB} ${outname})\n       else()\n           if(PC_LIB)\n               string(APPEND ${outname} \" ${PC_LIB}\")\n           endif()\n       endif()\n    endforeach()\n    set(${outname} ${${outname}} PARENT_SCOPE)\nendfunction()\n\nif(openPMD_HAVE_PKGCONFIG)\n    openpmdreclibs(openPMD openPMD_PC_PRIVATE_LIBS)\n    if(openPMD_BUILD_SHARED_LIBS)\n        set(openPMD_PC_STATIC false)\n    else()\n        set(openPMD_PC_STATIC true)\n    endif()\n    configure_file(\n        ${openPMD_SOURCE_DIR}/openPMD.pc.in\n        ${openPMD_BINARY_DIR}/openPMD.pc\n        @ONLY\n    )\nendif()\n\ninclude(CMakePackageConfigHelpers)\nwrite_basic_package_version_file(\"openPMDConfigVersion.cmake\"\n    VERSION ${openPMD_VERSION}\n    COMPATIBILITY SameMajorVersion\n)\n\n\n# Installs ####################################################################\n#\n# headers, libraries and executables\nif(openPMD_INSTALL)\n    set(openPMD_INSTALL_TARGET_NAMES openPMD)\n\n    if(openPMD_BUILD_CLI_TOOLS)\n        foreach(toolname ${openPMD_CLI_TOOL_NAMES})\n            list(APPEND openPMD_INSTALL_TARGET_NAMES openpmd-${toolname})\n        endforeach()\n    endif()\n\n    if(openPMD_INSTALL_RPATH)\n        set(openPMD_INSTALL_RPATH_TARGET_NAMES ${openPMD_INSTALL_TARGET_NAMES})\n        if(openPMD_HAVE_PYTHON)\n            list(APPEND openPMD_INSTALL_RPATH_TARGET_NAMES openPMD.py)\n        endif()\n        if(NOT DEFINED CMAKE_INSTALL_RPATH)\n            if(APPLE)\n                set_target_properties(${openPMD_INSTALL_RPATH_TARGET_NAMES} PROPERTIES\n                    INSTALL_RPATH \"@loader_path\"\n                )\n            elseif(CMAKE_SYSTEM_NAME MATCHES \"Linux\")\n                set_target_properties(${openPMD_INSTALL_RPATH_TARGET_NAMES} PROPERTIES\n                    INSTALL_RPATH \"$ORIGIN\"\n                )\n            endif()\n            # Windows: has no RPath concept, all interdependent `.dll`s must be in\n            #          %PATH% or in the same dir as the calling executable\n        endif()\n\n        if(NOT DEFINED CMAKE_INSTALL_RPATH_USE_LINK_PATH)\n            # those are appended AFTER the paths in INSTALL_RPATH\n            set_target_properties(${openPMD_INSTALL_RPATH_TARGET_NAMES} PROPERTIES\n                INSTALL_RPATH_USE_LINK_PATH ON\n            )\n        endif()\n    endif()\n\n    install(TARGETS ${openPMD_INSTALL_TARGET_NAMES}\n        EXPORT openPMDTargets\n        LIBRARY DESTINATION ${openPMD_INSTALL_LIBDIR}\n        ARCHIVE DESTINATION ${openPMD_INSTALL_LIBDIR}\n        RUNTIME DESTINATION ${openPMD_INSTALL_BINDIR}\n        INCLUDES DESTINATION ${openPMD_INSTALL_INCLUDEDIR}\n    )\n    if(openPMD_HAVE_PYTHON)\n        install(\n            DIRECTORY   ${openPMD_SOURCE_DIR}/src/binding/python/openpmd_api\n            DESTINATION ${openPMD_INSTALL_PYTHONDIR}\n            PATTERN \"*pyc\" EXCLUDE\n            PATTERN \"__pycache__\" EXCLUDE\n        )\n        install(TARGETS openPMD.py\n            DESTINATION ${openPMD_INSTALL_PYTHONDIR}/openpmd_api\n        )\n        if(openPMD_BUILD_CLI_TOOLS)\n            foreach(toolname ${openPMD_PYTHON_CLI_TOOL_NAMES})\n                install(\n                    PROGRAMS ${openPMD_SOURCE_DIR}/src/cli/${toolname}.py\n                    DESTINATION ${openPMD_INSTALL_BINDIR}\n                    RENAME openpmd-${toolname}\n                )\n            endforeach()\n        endif()\n    endif()\n    install(DIRECTORY \"${openPMD_SOURCE_DIR}/include/openPMD\"\n        DESTINATION ${openPMD_INSTALL_INCLUDEDIR}\n        FILES_MATCHING\n            PATTERN \"*.hpp\"\n            PATTERN \"*.tpp\"\n    )\n    install(\n        FILES ${openPMD_BINARY_DIR}/include/openPMD/config.hpp\n        DESTINATION ${openPMD_INSTALL_INCLUDEDIR}/openPMD\n    )\n\n    # CMake package file for find_package(openPMD::openPMD) in depending projects\n    install(EXPORT openPMDTargets\n        FILE openPMDTargets.cmake\n        NAMESPACE openPMD::\n        DESTINATION ${openPMD_INSTALL_CMAKEDIR}\n    )\n    install(\n        FILES\n            ${openPMD_BINARY_DIR}/openPMDConfig.cmake\n            ${openPMD_BINARY_DIR}/openPMDConfigVersion.cmake\n        DESTINATION ${openPMD_INSTALL_CMAKEDIR}\n    )\n    # pkg-config .pc file for depending legacy projects\n    #   This is for projects that do not use a build file generator, e.g.\n    #   because they compile manually on the command line or write their\n    #   Makefiles by hand.\n    if(openPMD_HAVE_PKGCONFIG)\n        install(\n            FILES       ${openPMD_BINARY_DIR}/openPMD.pc\n            DESTINATION ${openPMD_INSTALL_LIBDIR}/pkgconfig\n        )\n    endif()\nendif()\n\n\n# Tests #######################################################################\n#\nif(openPMD_BUILD_TESTING)\n    enable_testing()\n\n    # OpenMPI root guard: https://github.com/open-mpi/ompi/issues/4451\n    if(\"$ENV{USER}\" STREQUAL \"root\")\n        # calling even --help as root will abort and warn on stderr\n        execute_process(COMMAND ${MPIEXEC_EXECUTABLE} --help\n            ERROR_VARIABLE MPIEXEC_HELP_TEXT\n            OUTPUT_STRIP_TRAILING_WHITESPACE)\n            if(${MPIEXEC_HELP_TEXT} MATCHES \"^.*allow-run-as-root.*$\")\n                set(MPI_ALLOW_ROOT --allow-run-as-root)\n            endif()\n    endif()\n    set(MPI_TEST_EXE\n        ${MPIEXEC_EXECUTABLE}\n        ${MPI_ALLOW_ROOT}\n        ${MPIEXEC_NUMPROC_FLAG} 2\n    )\n\n    # do we have openPMD-example-datasets?\n    if(EXISTS \"${openPMD_BINARY_DIR}/samples/git-sample/\")\n        set(EXAMPLE_DATA_FOUND ON)\n        message(STATUS \"Found openPMD-example-datasets: TRUE\")\n    else()\n        message(STATUS \"Note: Skipping example and tool runs (missing openPMD-example-datasets)\")\n        if(WIN32)\n            message(STATUS \"Note: run\\n\"\n                           \"    Powershell.exe -File ${openPMD_SOURCE_DIR}/share/openPMD/download_samples.ps1\\n\"\n                           \"to add example files to samples/git-sample/ directory!\")\n        else()\n            message(STATUS \"Note: run\\n\"\n                           \"    ${openPMD_SOURCE_DIR}/share/openPMD/download_samples.sh\\n\"\n                           \"to add example files to samples/git-sample/ directory!\")\n        endif()\n    endif()\n\n    if(openPMD_HAVE_PYTHON)\n        # do we have mpi4py for MPI-parallel Python tests?\n        if(openPMD_HAVE_MPI)\n            execute_process(COMMAND ${Python_EXECUTABLE}\n                -m mpi4py\n                -c \"import mpi4py.MPI\"\n                RESULT_VARIABLE MPI4PY_RETURN\n                OUTPUT_QUIET ERROR_QUIET)\n\n            if(MPI4PY_RETURN EQUAL 0)\n                message(STATUS \"Found mpi4py: TRUE\")\n            else()\n                message(STATUS \"Could NOT find mpi4py (will NOT run MPI-parallel Python examples)\")\n            endif()\n        endif()\n    endif()\n\n    # C++ Unit tests\n    foreach(testname ${openPMD_TEST_NAMES})\n        if(${testname} MATCHES \"^Parallel.*$\")\n            if(openPMD_HAVE_MPI)\n                add_test(NAME MPI.${testname}\n                    COMMAND ${MPI_TEST_EXE} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${testname}Tests\n                    WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n                )\n            endif()\n        else()\n            add_test(NAME Serial.${testname}\n                COMMAND ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${testname}Tests\n                WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n            )\n        endif()\n    endforeach()\n\n    # Python Unit tests\n    if(openPMD_HAVE_PYTHON)\n        function(test_set_pythonpath test_name)\n            if(WIN32)\n                if(isMultiConfig)\n                    string(REGEX REPLACE \"/\" \"\\\\\\\\\" WIN_BUILD_BASEDIR ${openPMD_BINARY_DIR}/$<CONFIG>)\n                    string(REGEX REPLACE \"/\" \"\\\\\\\\\" WIN_BUILD_BINDIR ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/$<CONFIG>)\n                else()\n                    string(REGEX REPLACE \"/\" \"\\\\\\\\\" WIN_BUILD_BASEDIR ${openPMD_BINARY_DIR})\n                    string(REGEX REPLACE \"/\" \"\\\\\\\\\" WIN_BUILD_BINDIR ${openPMD_RUNTIME_OUTPUT_DIRECTORY})\n                endif()\n                string(REPLACE \";\" \"\\\\;\" WIN_PATH \"$ENV{PATH}\")\n                string(REPLACE \";\" \"\\\\;\" WIN_PYTHONPATH \"$ENV{PYTHONPATH}\")\n                set_property(TEST ${test_name}\n                    PROPERTY ENVIRONMENT\n                        \"PATH=${WIN_BUILD_BINDIR}\\\\${CMAKE_BUILD_TYPE}\\;${WIN_PATH}\\n\"\n                        \"PYTHONPATH=${WIN_BUILD_BASEDIR}\\\\${openPMD_INSTALL_PYTHONDIR}\\\\${CMAKE_BUILD_TYPE}\\;${WIN_PYTHONPATH}\"\n                )\n            else()\n                set_tests_properties(${test_name}\n                    PROPERTIES ENVIRONMENT\n                        \"PYTHONPATH=${openPMD_BINARY_DIR}/${openPMD_INSTALL_PYTHONDIR}:$ENV{PYTHONPATH}\"\n                )\n            endif()\n        endfunction()\n\n        if(openPMD_HAVE_HDF5)\n            if(EXAMPLE_DATA_FOUND)\n                add_test(NAME Unittest.py\n                    COMMAND ${Python_EXECUTABLE}\n                        ${openPMD_SOURCE_DIR}/test/python/unittest/Test.py -v\n                    WORKING_DIRECTORY\n                        ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n                )\n                test_set_pythonpath(Unittest.py)\n            endif()\n        endif()\n    endif()\n\n    # Examples\n    if(openPMD_BUILD_EXAMPLES)\n        # C++ Examples\n        # Current examples all use HDF5, elaborate if other backends are used\n        if(openPMD_HAVE_HDF5)\n            if(EXAMPLE_DATA_FOUND)\n                foreach(examplename ${openPMD_EXAMPLE_NAMES})\n                    if(${examplename} MATCHES \"^10.*$\")\n                        # streaming examples are done separately\n                    elseif(${examplename} MATCHES \"^.*_parallel$\")\n                        if(openPMD_HAVE_MPI)\n                            add_test(NAME MPI.${examplename}\n                                    COMMAND ${MPI_TEST_EXE} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${examplename}\n                                    WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n                                    )\n                        endif()\n                    else()\n                        add_test(NAME Serial.${examplename}\n                                COMMAND ${examplename}\n                                WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n                                )\n                    endif()\n                endforeach()\n            endif()\n        endif()\n        if(openPMD_HAVE_ADIOS2)\n            add_test(NAME Asynchronous.10_streaming\n                     COMMAND sh -c \"$<TARGET_FILE:10_streaming_write> & sleep 1; $<TARGET_FILE:10_streaming_read>\"\n                     WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY})\n        endif()\n    endif()\n\n    # Command Line Tools\n    if(openPMD_BUILD_CLI_TOOLS)\n        # all tools must provide a \"--help\"\n        foreach(toolname ${openPMD_CLI_TOOL_NAMES})\n            add_test(NAME CLI.help.${toolname}\n                COMMAND openpmd-${toolname} --help\n                WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n            )\n        endforeach()\n        if(openPMD_HAVE_HDF5 AND EXAMPLE_DATA_FOUND)\n            add_test(NAME CLI.ls\n                COMMAND openpmd-ls ../samples/git-sample/data%08T.h5\n                WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n            )\n        endif()\n    endif()\n\n    # Python CLI Modules\n    if(openPMD_HAVE_PYTHON)\n        # (Note that during setuptools install, these are furthermore installed as\n        #  console scripts and replace the all-binary CLI tools.)\n        foreach(pymodulename ${openPMD_PYTHON_CLI_MODULE_NAMES})\n             add_test(NAME CLI.py.help.${pymodulename}\n                 COMMAND ${Python_EXECUTABLE} -m openpmd_api.${pymodulename} --help\n                 WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n            )\n            test_set_pythonpath(CLI.py.help.${pymodulename})\n        endforeach()\n    endif()\n\n    # Python-based command line tools\n    if(openPMD_BUILD_CLI_TOOLS AND openPMD_HAVE_PYTHON)\n        # all tools must provide a \"--help\"\n        foreach(toolname ${openPMD_PYTHON_CLI_TOOL_NAMES})\n            configure_file(\n                ${openPMD_SOURCE_DIR}/src/cli/${toolname}.py\n                ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-${toolname}\n                COPYONLY\n            )\n            add_test(NAME CLI.help.${toolname}.py\n                COMMAND ${Python_EXECUTABLE}\n                    ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-${toolname} --help\n                WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n            )\n            test_set_pythonpath(CLI.help.${toolname}.py)\n        endforeach()\n\n        # openpmd-pipe (python) test\n        if( NOT WIN32\n            AND openPMD_HAVE_HDF5\n            AND openPMD_HAVE_ADIOS2\n            AND EXAMPLE_DATA_FOUND\n        )\n            if( openPMD_HAVE_MPI )\n                set(MPI_TEST_EXE\n                    ${MPIEXEC_EXECUTABLE}\n                    ${MPI_ALLOW_ROOT}\n                    #${MPIEXEC_NUMPROC_FLAG} 2\n                )\n                add_test(NAME CLI.pipe.py\n                    COMMAND sh -c\n                        \"${MPI_TEST_EXE} ${Python_EXECUTABLE}                      \\\n                            ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe       \\\n                            --infile ../samples/git-sample/data%T.h5               \\\n                            --outfile ../samples/git-sample/data%T.bp &&           \\\n                                                                                   \\\n                        ${MPI_TEST_EXE} ${Python_EXECUTABLE}                       \\\n                            ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe       \\\n                            --infile ../samples/git-sample/data00000100.h5         \\\n                            --outfile                                              \\\n                                ../samples/git-sample/single_iteration_%T.bp &&    \\\n                                                                                   \\\n                        ${MPI_TEST_EXE} ${Python_EXECUTABLE}                       \\\n                            ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe       \\\n                            --infile ../samples/git-sample/thetaMode/data%T.h5     \\\n                            --outfile                                              \\\n                                ../samples/git-sample/thetaMode/data_%T.bp &&      \\\n                                                                                   \\\n                        ${MPI_TEST_EXE} ${Python_EXECUTABLE}                       \\\n                            ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe       \\\n                            --infile ../samples/git-sample/thetaMode/data_%T.bp    \\\n                            --outfile ../samples/git-sample/thetaMode/data%T.json  \\\n                            --outconfig '                                          \\\n                                json.attribute.mode = \\\"short\\\"                  \\n\\\n                                json.dataset.mode = \\\"template_no_warn\\\"'          \\\n                        \"\n                    WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n                )\n            else()\n                add_test(NAME CLI.pipe.py\n                    COMMAND sh -c\n                        \"${Python_EXECUTABLE}                                      \\\n                            ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe       \\\n                            --infile ../samples/git-sample/data%T.h5               \\\n                            --outfile ../samples/git-sample/data%T.bp &&           \\\n                                                                                   \\\n                        ${Python_EXECUTABLE}                                       \\\n                            ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe       \\\n                            --infile ../samples/git-sample/thetaMode/data%T.h5     \\\n                            --outfile ../samples/git-sample/thetaMode/data%T.bp && \\\n                                                                                   \\\n                        ${Python_EXECUTABLE}                                       \\\n                            ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe       \\\n                            --infile ../samples/git-sample/thetaMode/data%T.bp     \\\n                            --outfile ../samples/git-sample/thetaMode/data%T.json  \\\n                        \"\n                    WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n                )\n            endif()\n            test_set_pythonpath(CLI.pipe.py)\n        endif()\n    endif()\n\n\n    # Python Examples\n    # Current examples all use HDF5, elaborate if other backends are used\n    if(openPMD_HAVE_PYTHON AND openPMD_HAVE_HDF5)\n        if(EXAMPLE_DATA_FOUND)\n            foreach(examplename ${openPMD_PYTHON_EXAMPLE_NAMES})\n                configure_file(\n                    ${openPMD_SOURCE_DIR}/examples/${examplename}.py\n                    ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${examplename}.py\n                    COPYONLY\n                )\n                if(openPMD_BUILD_TESTING)\n                    if(${examplename} MATCHES \"^10.*$\")\n                        # streaming examples are done separately\n                        continue()\n                    elseif(${examplename} MATCHES \"^.*_parallel$\")\n                        if(openPMD_HAVE_MPI AND MPI4PY_RETURN EQUAL 0)\n                            # see https://mpi4py.readthedocs.io/en/stable/mpi4py.run.html\n                            add_test(NAME Example.py.${examplename}\n                                COMMAND ${MPI_TEST_EXE} ${Python_EXECUTABLE} -m mpi4py\n                                    ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${examplename}.py\n                                WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n                            )\n                        else()\n                            continue()\n                        endif()\n                    else()\n                        add_test(NAME Example.py.${examplename}\n                            COMMAND ${Python_EXECUTABLE}\n                                ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${examplename}.py\n                            WORKING_DIRECTORY\n                                ${openPMD_RUNTIME_OUTPUT_DIRECTORY}\n                        )\n                    endif()\n                    test_set_pythonpath(Example.py.${examplename})\n                endif()\n            endforeach()\n            if(openPMD_HAVE_ADIOS2 AND openPMD_BUILD_TESTING AND NOT WIN32)\n                add_test(NAME Asynchronous.10_streaming.py\n                        COMMAND sh -c \"${Python_EXECUTABLE} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/10_streaming_write.py & sleep 1; ${Python_EXECUTABLE} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/10_streaming_read.py\"\n                        WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY})\n                test_set_pythonpath(Asynchronous.10_streaming.py)\n            endif()\n        endif()\n    endif()\nendif()\n\n# Status Message for Build Options ############################################\n#\nopenpmd_print_summary()\n\n[build-system]\nrequires = [\n    \"setuptools>=42\",\n    \"wheel\",\n    \"cmake>=3.22.0,<4.0.0\",\n    \"packaging>=23\",\n    \"pybind11>=2.13.0,<3.0.0\"\n]\nbuild-backend = \"setuptools.build_meta\"\n\nnumpy>=1.15.0\n\nimport os\nimport platform\nimport re\nimport subprocess\nimport sys\n\nfrom setuptools import Extension, setup\nfrom setuptools.command.build_ext import build_ext\n\n\nclass CMakeExtension(Extension):\n    def __init__(self, name, sourcedir=''):\n        Extension.__init__(self, name, sources=[])\n        self.sourcedir = os.path.abspath(sourcedir)\n\n\nclass CMakeBuild(build_ext):\n    def run(self):\n        from packaging.version import parse\n\n        try:\n            out = subprocess.check_output(['cmake', '--version'])\n        except OSError:\n            raise RuntimeError(\n                \"CMake 3.22.0+ must be installed to build the following \" +\n                \"extensions: \" +\n                \", \".join(e.name for e in self.extensions))\n\n        cmake_version = parse(re.search(\n            r'version\\s*([\\d.]+)',\n            out.decode()\n        ).group(1))\n        if cmake_version < parse('3.22.0'):\n            raise RuntimeError(\"CMake >= 3.22.0 is required\")\n\n        for ext in self.extensions:\n            self.build_extension(ext)\n\n    def build_extension(self, ext):\n        extdir = os.path.abspath(os.path.dirname(\n            self.get_ext_fullpath(ext.name)\n        ))\n        # required for auto-detection of auxiliary \"native\" libs\n        if not extdir.endswith(os.path.sep):\n            extdir += os.path.sep\n\n        pyv = sys.version_info\n        cmake_args = [\n            # Python: use the calling interpreter in CMake\n            # https://cmake.org/cmake/help/latest/module/FindPython.html#hints\n            # https://cmake.org/cmake/help/latest/command/find_package.html#config-mode-version-selection\n            '-DPython_ROOT_DIR=' + sys.prefix,\n            f'-DPython_FIND_VERSION={pyv.major}.{pyv.minor}.{pyv.micro}',\n            '-DPython_FIND_VERSION_EXACT=TRUE',\n            '-DPython_FIND_STRATEGY=LOCATION',\n            '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=' +\n            os.path.join(extdir, \"openpmd_api\"),\n            # '-DCMAKE_RUNTIME_OUTPUT_DIRECTORY=' + extdir,\n            '-DopenPMD_PYTHON_OUTPUT_DIRECTORY=' + extdir,\n            '-DopenPMD_USE_PYTHON:BOOL=ON',\n            # variants\n            '-DopenPMD_USE_MPI:BOOL=' + openPMD_USE_MPI,\n            # skip building cli tools, examples & tests\n            #   note: CLI tools provided as console scripts\n            '-DopenPMD_BUILD_CLI_TOOLS:BOOL=OFF',\n            '-DopenPMD_BUILD_EXAMPLES:BOOL=' + BUILD_EXAMPLES,\n            '-DopenPMD_BUILD_TESTING:BOOL=' + BUILD_TESTING,\n            # static/shared libs\n            '-DopenPMD_BUILD_SHARED_LIBS:BOOL=' + BUILD_SHARED_LIBS,\n            # Unix: rpath to current dir when packaged\n            #       needed for shared (here non-default) builds\n            '-DCMAKE_BUILD_WITH_INSTALL_RPATH:BOOL=ON',\n            '-DCMAKE_INSTALL_RPATH_USE_LINK_PATH:BOOL=OFF',\n            # Windows: has no RPath concept, all `.dll`s must be in %PATH%\n            #          or same dir as calling executable\n        ]\n        if HDF5_USE_STATIC_LIBRARIES is not None:\n            cmake_args.append('-DHDF5_USE_STATIC_LIBRARIES:BOOL=' +\n                              HDF5_USE_STATIC_LIBRARIES)\n        if ZLIB_USE_STATIC_LIBS is not None:\n            cmake_args.append('-DZLIB_USE_STATIC_LIBS:BOOL=' +\n                              ZLIB_USE_STATIC_LIBS)\n        if CMAKE_INTERPROCEDURAL_OPTIMIZATION is not None:\n            cmake_args.append('-DCMAKE_INTERPROCEDURAL_OPTIMIZATION=' +\n                              CMAKE_INTERPROCEDURAL_OPTIMIZATION)\n        if sys.platform == \"darwin\":\n            cmake_args.append('-DCMAKE_INSTALL_RPATH=@loader_path')\n        else:\n            # values: linux*, aix, freebsd, ...\n            #   just as well win32 & cygwin (although Windows has no RPaths)\n            cmake_args.append('-DCMAKE_INSTALL_RPATH=$ORIGIN')\n\n        cmake_args += extra_cmake_args\n\n        cfg = 'Debug' if self.debug else 'Release'\n        build_args = ['--config', cfg]\n\n        # Assumption: Windows builds are always multi-config (MSVC VS)\n        if platform.system() == \"Windows\":\n            cmake_args += [\n                '-DopenPMD_BUILD_NO_CFG_SUBPATH:BOOL=ON',\n                '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_{}={}'.format(\n                    cfg.upper(),\n                    os.path.join(extdir, \"openpmd_api\")\n                )\n            ]\n            if sys.maxsize > 2**32:\n                cmake_args += ['-A', 'x64']\n            build_args += ['--', '/m']\n        else:\n            cmake_args += ['-DCMAKE_BUILD_TYPE=' + cfg]\n            build_args += ['--', '-j2']\n\n        env = os.environ.copy()\n        env['CXXFLAGS'] = '{} -DVERSION_INFO=\\\\\"{}\\\\\"'.format(\n            env.get('CXXFLAGS', ''),\n            self.distribution.get_version()\n        )\n        if not os.path.exists(self.build_temp):\n            os.makedirs(self.build_temp)\n        subprocess.check_call(\n            ['cmake', ext.sourcedir] + cmake_args,\n            cwd=self.build_temp,\n            env=env\n        )\n        subprocess.check_call(\n            ['cmake', '--build', '.'] + build_args,\n            cwd=self.build_temp\n        )\n        # note that this does not call install;\n        # we pick up artifacts directly from the build output dirs\n\n\nwith open('./README.md', encoding='utf-8') as f:\n    long_description = f.read()\n\n# Allow to control options via environment vars.\n# Work-around for https://github.com/pypa/setuptools/issues/1712\n# note: changed default for SHARED, MPI, TESTING and EXAMPLES\nopenPMD_USE_MPI = os.environ.get('openPMD_USE_MPI', 'OFF')\nHDF5_USE_STATIC_LIBRARIES = os.environ.get('HDF5_USE_STATIC_LIBRARIES', None)\nZLIB_USE_STATIC_LIBS = os.environ.get('ZLIB_USE_STATIC_LIBS', None)\n# deprecated: backwards compatibility to <= 0.13.*\nBUILD_SHARED_LIBS = os.environ.get('BUILD_SHARED_LIBS', 'OFF')\nBUILD_TESTING = os.environ.get('BUILD_TESTING', 'OFF')\nBUILD_EXAMPLES = os.environ.get('BUILD_EXAMPLES', 'OFF')\n# end deprecated\nBUILD_SHARED_LIBS = os.environ.get('openPMD_BUILD_SHARED_LIBS',\n                                   BUILD_SHARED_LIBS)\nBUILD_TESTING = os.environ.get('openPMD_BUILD_TESTING',\n                               BUILD_TESTING)\nBUILD_EXAMPLES = os.environ.get('openPMD_BUILD_EXAMPLES',\n                                BUILD_EXAMPLES)\nCMAKE_INTERPROCEDURAL_OPTIMIZATION = os.environ.get(\n    'CMAKE_INTERPROCEDURAL_OPTIMIZATION', None)\n\n# extra CMake arguments\nextra_cmake_args = []\nfor k, v in os.environ.items():\n    extra_cmake_args_prefix = \"openPMD_CMAKE_\"\n    if k.startswith(extra_cmake_args_prefix) and \\\n       len(k) > len(extra_cmake_args_prefix):\n        extra_cmake_args.append(\"-D{0}={1}\".format(\n            k[len(extra_cmake_args_prefix):],\n            v))\n\n# https://cmake.org/cmake/help/v3.0/command/if.html\nif openPMD_USE_MPI.upper() in ['1', 'ON', 'TRUE', 'YES']:\n    openPMD_USE_MPI = \"ON\"\nelse:\n    openPMD_USE_MPI = \"OFF\"\n\n# Get the package requirements from the requirements.txt file\nwith open('./requirements.txt') as f:\n    install_requires = [line.strip('\\n') for line in f.readlines()]\n    if openPMD_USE_MPI == \"ON\":\n        install_requires.append('mpi4py>=2.1.0')\n\n# keyword reference:\n#   https://packaging.python.org/guides/distributing-packages-using-setuptools\nsetup(\n    name='openPMD-api',\n    # note PEP-440 syntax: x.y.zaN but x.y.z.devN\n    version='0.17.0.dev',\n    author='Axel Huebl, Franz Poeschel, Fabian Koller, Junmin Gu',\n    author_email='axelhuebl@lbl.gov, f.poeschel@hzdr.de',\n    maintainer='Axel Huebl',\n    maintainer_email='axelhuebl@lbl.gov',\n    description='C++ & Python API for Scientific I/O with openPMD',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    keywords=('openPMD openscience hdf5 adios mpi hpc research '\n              'file-format file-handling'),\n    url='https://www.openPMD.org',\n    project_urls={\n        'Documentation': 'https://openpmd-api.readthedocs.io',\n        'Doxygen': 'https://www.openpmd.org/openPMD-api',\n        'Reference': 'https://doi.org/10.14278/rodare.27',\n        'Source': 'https://github.com/openPMD/openPMD-api',\n        'Tracker': 'https://github.com/openPMD/openPMD-api/issues',\n    },\n    ext_modules=[CMakeExtension('openpmd_api_cxx')],\n    cmdclass=dict(build_ext=CMakeBuild),\n    # scripts=['openpmd-ls'],\n    zip_safe=False,\n    python_requires='>=3.8',\n    # tests_require=['pytest'],\n    install_requires=install_requires,\n    # see: src/bindings/python/cli\n    entry_points={\n        'console_scripts': [\n            'openpmd-ls = openpmd_api.ls.__main__:main',\n            'openpmd-pipe = openpmd_api.pipe.__main__:main'\n        ]\n    },\n    # we would like to use this mechanism, but pip / setuptools do not\n    # influence the build and build_ext with it.\n    # therefore, we use environment vars to control.\n    # ref: https://github.com/pypa/setuptools/issues/1712\n    # extras_require={\n    #     'mpi': ['mpi4py>=2.1.0'],\n    # },\n    # cmdclass={'test': PyTest},\n    # platforms='any',\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Natural Language :: English',\n        'Environment :: Console',\n        'Intended Audience :: Science/Research',\n        'Operating System :: OS Independent',\n        'Topic :: Scientific/Engineering',\n        'Topic :: Database :: Front-Ends',\n        'Programming Language :: C++',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3.12',\n        'Programming Language :: Python :: 3.13',\n        ('License :: OSI Approved :: '\n         'GNU Lesser General Public License v3 or later (LGPLv3+)'),\n    ],\n)\n\n# This is a Spack environment file.\n#\n# Activating and installing this environment will provide all dependencies\n# that are needed for full-feature development.\n#   https//spack.readthedocs.io/en/latest/environments.html#anonymous-environments\n#\nspack:\n  specs:\n  - adios~sz\n  - adios2\n  - cmake\n  - hdf5\n  - mpi\n  - python\n  - py-mpi4py\n  - py-numpy\n  - py-pandas\n  - py-dask\n  - py-pre-commit\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/osadcp-toolbox",
            "repo_link": "https://git.geomar.de/dam/osadcp_toolbox/",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/otter",
            "repo_link": "https://gitlab.com/qtb-hhu/marine/otter",
            "content": {
                "codemeta": "",
                "readme": "[![DOI](https://img.shields.io/badge/DOI-10.1038/s43247--024--01782--0-blue)](https://doi.org/10.1038/s43247-024-01782-0)\n\n\n[![Latest Release](https://gitlab.com/qtb-hhu/marine/otter/-/badges/release.svg)](https://gitlab.com/qtb-hhu/marine/otter/-/releases)\n# otter\n<div style=\"float: left;\">\n        <img src=\"img/otter_logo-removebg.png\" alt=\"Otter Logo\" width=\"200\" title=\"This images was created using GENAI (DALL-E 3).\"/>\n</div>\n<div style=\"text-align: right;\">\n<br>\n<br>\n<br>\n    The otter Framework combines community network analysis using Co-Occurrence networks, Louvain Clustering and Convergent Cross Mapping networks.\n<br>\n<br>\n<br>\n<br>\n</div>\n\n\n### Installation\nPython version 3.10\n```bash\npip install -r requirements.txt\n```\n\n### Pytest \n```bash\npytest\n```\n\n## Usage:\n\n### Run the app\n```bash\nstreamlit run Otter_APP.py\n```\n## Getting Started\n\n### 1. Set Prefix Path\nFill in the **Prefix Path** where the output tables will be saved.  \n- **Default**: `Tutorial/tables`  \n- This is the directory where all your results will be stored.\n\n### 2. Set Run ID\nInput the **Run ID**, which is the name for your experiment.  \n- **Default**: `MyExperimentRun`  \n- This name will be used to track the output of your experiment.\n\n### 3. Upload Abundance Table\nUpload the `abundance.csv` file, which contains the abundance time series.  \n- **Format**: The `abundance.csv` should follow the example provided in `tests/table/abundance.csv`.  \n- This table should include your time-series data for each ASV (Amplicon Sequence Variant).\n\n### 4. Upload Taxa Table\nUpload the `taxa.csv` file, which contains the taxonomic hierarchy for each ASV present in the abundance file.  \n- **Format**: Ensure this table includes the corresponding taxonomy information for each ASV.\n\n### 5. (Optional) Upload Environmental Table\nYou can optionally upload the `environmental.csv` file. This table should contain environmental parameters for each sample.  \n- **Format**: Ensure this file matches the sample names in the `abundance.csv`.\n\n### 6. Adjust Analysis Parameters\nYou can now adjust the parameters for the following calculations using the **expanding parameter cockpit**:  \n- **CON**: Co-Occurrence Network  \n- **CCM**: Convergent Cross Mapping  \n- **Permu**: Permutation tests\n\nMake sure to fine-tune these parameters based on your analysis needs.\n\n### 7. Run Calculations\nOnce parameters are set, you can create each network one by one or all at once:  \n- Use the **Create Network Buttons** from left to right to calculate each network (CON, CCM, etc.).  \n- Alternatively, click the **Run All** button to generate all networks in one go.\n\n## Visualizing Results\n\nAfter the network calculations are complete, you can switch to other tabs in the menu on the left-hand side to visualize your results.  \nEach tab provides a different visualization tool to explore the networks and clusters derived from your data.\n1. Environmental_Data\n2. Environmental_Cluster_Correlation_Heatmap\n3. Cluster Distribution \n4. Latentspace Embedding for Cluster Distances\n5. Alpha and Beta Diversity \n\n### Convergent Cross Mapping:\nThe CCM modul is based on https://github.com/PrinceJavier/causal_ccm.\n\n### Acknowledgements\nThanks to Prince Joseph Erneszer Javier for developing CCM package in python.\nThanks the Alfred-Wegener-Institute for funding parts of the development of the project.\n\n### Citation\nPlease cite Beyond blooms: the winter ecosystem reset determines microeukaryotic community dynamics in the Fram Strait. when using otter: https://www.nature.com/articles/s43247-024-01782-0 and the software https://zenodo.org/records/13840807.\n\n```\n@article{oldenburg2024beyond,\n  title={Beyond blooms: the winter ecosystem reset determines microeukaryotic community dynamics in the Fram Strait},\n  author={Oldenburg, Ellen and Kronberg, Raphael M and Metfies, Katja and Wietz, Matthias and von Appen, Wilken-Jon and Bienhold, Christina and Popa, Ovidiu and Ebenh{\\\"o}h, Oliver},\n  journal={Communications Earth \\& Environment},\n  volume={5},\n  number={1},\n  pages={643},\n  year={2024},\n  publisher={Nature Publishing Group UK London}\n}\n```\n\n\n\n",
                "dependencies": "numpy\npandas\nnetworkx\njupyter\nennemi\nscipy\nmatplotlib\nstatsmodels\ntqdm\npytest\nblack\npylint\nopenpyxl\nscikit-learn\nscikit-bio\nstreamlit\nseaborn\numap-learn\nplotly\nkaleido\nmatplotlib_venn\ndask\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/palladio",
            "repo_link": "https://github.com/PalladioSimulator",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/training-catalogue-for-photon-neutron",
            "repo_link": "https://github.com/pan-training/training-catalogue",
            "content": {
                "codemeta": "{\n  \"@context\": \"https://doi.org/10.5063/schema/codemeta-2.0\",\n  \"@type\": \"SoftwareSourceCode\",\n  \"identifier\": \"PaN training-catalogue\",\n  \"description\": \"Browsing, discovering and organising PaN sciences training resources.\",\n  \"name\": \"PaN Training Catalogue\",\n  \"codeRepository\": \"https://github.com/pan-training/training-catalogue\",\n  \"issueTracker\": \"https://github.com/pan-training/training-catalogue/issues\",\n  \"license\": \"https://opensource.org/licenses/BSD-3-Clause\",\n  \"version\": \"1.0\",\n  \"author\": [\n\n  ],\n  \"contIntegration\": \"\",\n  \"developmentStatus\": \"active\",\n  \"downloadUrl\": \"https://github.com/pan-training/training-catalogue/archive/master.zip\",\n  \"keywords\": [\n    \"training\",\n    \"software\"\n  ],\n  \"softwareVersion\":\"1.1\",\n  \"dateCreated\":\"2021-06-11\",\n  \"datePublished\":\"2021-06-11\",\n  \"programmingLanguage\": \"Ruby\"\n}\n\n",
                "readme": "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7015078.svg)](https://doi.org/10.5281/zenodo.7015078)\n\n# PaN Training Catalogue (based on the TeSS Trainning Catalogue from ELIXIR)\n\nThis repository contains the sourcecode of our [PaN Training Catalogue](https://pan-training.eu). This catalogue is based on the [TeSS Trainning Catalogue](https://github.com/ElixirTeSS/TeSS) from the [ELIXIR](https://elixir-europe.org) project and is used in our Photon and Neutron (PaN) projects [ExPaNDS](https://expands.eu) and [PaNOSC](https://panosc.eu).\n \n## Setup\n\n```\nsudo apt-get install git postgresql libpq-dev imagemagick openjdk-8-jre nodejs redis-server\n```\n\nClone the TeSS source code via git:\n\n```\ngit clone https://github.com/ElixirTeSS/TeSS.git\n```\n\n### RVM, Ruby, Gems\n\nIt is typically recommended to install Ruby with RVM. With RVM, you can specify the version of Ruby you want installed, plus a whole lot more (e.g. gemsets). Full installation instructions for RVM are available online. In short:\n\n```\nsudo apt-get install software-properties-common\nsudo apt-add-repository -y ppa:rael-gc/rvm\nsudo apt-get update\nsudo apt-get install rvm\nrvm user gemsets\n```\n\nTeSS was developed using Ruby 2.4.5. We recommend using version 2.7.4 for our PaN catalague. To install recommended version of ruby and create a gemset, you can do something like the following:\n\n```\nrvm install `cat .ruby-version`\n/bin/bash --login\nrvm get stable --auto-dotfiles\nrvm use --create `cat .ruby-version`@`cat .ruby-gemset`\n```\n\nBundler provides a consistent environment for Ruby projects by tracking and installing the exact gems and versions that are needed for your Ruby application.\n\n```\ngem install bundler\n```\n\n### Redis/Sidekiq\n\nWe installed Redis before... but start Sidekiq!\n\n```\nbundle exec sidekiq\n```\n\n...or as a daemon in the background for production:\n\n```\nbundle exec sidekiq -d -L log/sidekiq.log -e production\n```\n\nNote that program 'gem' (a package management framework for Ruby called RubyGems) gets installed when you install RVM so you do not have to install it separately.\n\nOnce you have Ruby, RVM and bundler installed, from the root folder of the app do:\n\n```\nbundle install\n```\n\nFollow the steps on the official GitHub and setup PostgrSQL [repo](https://github.com/ElixirTeSS/TeSS), Solr, ... In a first development instance is is necessary to add the database login information in `secrets.yml`. \n\n### Set up environment\n\n`bin/rails db:environment:set RAILS_ENV=development or production!!!`\n\n### Solr\n\nTeSS uses Apache Solr to power its search and filtering system.\n\nTo start solr, run:\n\n```\nbundle exec rake sunspot:solr:start\n```\n\nYou can replace start with stop or restart to stop or restart solr. You can use reindex to reindex all records.\n\n```\nbundle exec rake sunspot:solr:reindex\n```\n\n### Database and Config\n\nFrom the app's root directory, create several config files by copying the example files.\n\n```\ncp config/tess.example.yml config/tess.yml\ncp config/sunspot.example.yml config/sunspot.yml\ncp config/secrets.example.yml config/secrets.yml\n```\n\nCreate Postgres DB with user `tess_user` and edit `config/secrets.yml` to configure the database name, user and password defined before.\n\nEdit `config/secrets.yml` to configure the app's secret_key_base which you can generate with:\n\n```\nbundle exec rake secret\n```\n\nCreate the databases:\n\n```\nbundle exec rake db:create:all\n```\n\nStart Solr:\n\n```\nbundle exec rake sunspot:solr:start\nbundle exec rake sunspot:solr:reindex\n```\n\nCreate the database structure and load in seed data:\n\nNote: Ensure you have started Solr before running this command!\n\n```\n$ bundle exec rake db:setup\n```\n\n\n### Dev Server\n\nThe dev server can evaluated with\n\n```\nbundle exec sidekiq\n```\n\nand\n\n```\nbundle exec rails server\n```\n\nand accessed via: http://localhost:3000\n\n#### Setup Administrators\n\nOnce you have a local TeSS succesfully running, you may want to setup administrative users. To do this register a new account in TeSS through the registration page. Then go to the applications Rails console:\n\n```\nbundle exec rails c\n```\n\nFind the user and assign them the administrative role. This can be completed by running this (where myemail@domain.co is the email address you used to register with):\n\n```\n2.2.6 :001 > User.find_by_email('myemail@domain.co').update_attributes(role: Role.find_by_name('admin'))\n```\n\n## Deployment: Providing TeSS using an Application Server\n\nAfter setting up TeSS, the configuration of an application server (**Phusion Passenger** is an application server and it is often used to power Ruby sites) is required.\n\nOr my prefered setup with Nginx:\n\nhttps://www.phusionpassenger.com/library/config/nginx/intro.html\n\nWe need additinal packages:\n\n```\napt-get install apache2-dev apt-get install libcurl4-gnutls-dev\n```\n\nAfter successfull development deployment add the Passenger Gem with:\n\n```\nsudo apt-get install -y dirmngr gnupg\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 561F9B9CAC40B2F7\nsudo apt-get install -y apt-transport-https ca-certificates\n# Add our APT repository\nsudo sh -c 'echo deb https://oss-binaries.phusionpassenger.com/apt/passenger bionic main > /etc/apt/sources.list.d/passenger.list'\nsudo apt-get update\n# Install Passenger + Nginx module\nsudo apt-get install -y libnginx-mod-http-passenger\n```\n\nCheck the installation with:\n\n```\nsudo /usr/bin/passenger-config validate-install\nsudo /usr/sbin/passenger-memory-stats\n```\n\n...and add the recommended lines to your Nginx configuration file and finish the Passenger setup.\n\n\n```\nserver {\n\t# SSL configuration\n\tlisten 443;\n\tssl on;\n\tproxy_set_header X_FORWARDED_PROTO https;\n              proxy_set_header  X-Forwarded-For $proxy_add_x_forwarded_for;\n              proxy_set_header  Host $http_host;\n              proxy_set_header  X-Url-Scheme $scheme;\n              proxy_redirect    off;\n              proxy_max_temp_file_size 0;\n\tserver_name pan-training.hzdr.de;\n\tssl_certificate /etc/ssl/certs/pan.cert;\n    ssl_certificate_key /etc/ssl/private/pan.key;\n\tssl_session_timeout 1d;\n    ssl_session_cache shared:MozSSL:10m;  # about 40000 sessions\n    ssl_session_tickets off;\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384;\n    ssl_prefer_server_ciphers off;\n    root /var/www/catalogue/public;\n    passenger_enabled on;\n\tpassenger_ruby /usr/share/rvm/gems/ruby-2.4.5@tess/wrappers/ruby;\n\tpassenger_document_root /var/www/catalogue/public/;\n    passenger_sticky_sessions on; \n}\n```\n\nThen we initialize the production environment:\n\n```\nbundle exec rake db:setup RAILS_ENV=production\n```\n\nor clean init:\n\n```\nbundle exec rake db:reset RAILS_ENV=production\n```\n...and reindex Solr:\n\n```\nbundle exec rake sunspot:solr:start RAILS_ENV=production\nbundle exec rake sunspot:solr:reindex RAILS_ENV=production\n```\n\nCreate an admin user and assign it appropriate 'admin' role bu looking up that role in console in model Role (default roles should be created automatically):\n\n```\nbundle exec rails c -e production\n```\n\nThe first time and each time a css or js file is updated:\n\n```\nbundle exec rake assets:clean RAILS_ENV=production\nbundle exec rake assets:precompile RAILS_ENV=production\n```\n\nand reindexing the matadata:\n\n```\nbundle exec rake sunspot:solr:reindex RAILS_ENV=production\n```\n\nStatus Check and restart:\n\n```\nbundle exec rake sunspot:solr:start RAILS_ENV=production\nservice nginx restart\nbundle exec sidekiq -d -L log/sidekiq.log -C config/sidekiq.yml -e production\nservice redis-server restart\npassenger-memory-stats \npassenger-status\n```\n\nLogfiles:\n\n```\n/var/log/redis/redis-server.log\n/var/log/nginx/error.log\n/var/log/catalogue/passenger.log\n/var/log/catalogue/sidekiq.log\n/var/log/catalogue/production.log\n/var/log/catalogue/sunspot-solr-production.log\n```\n\n",
                "dependencies": "source 'https://rubygems.org'\n\n#ruby \"2.4.5\"\n\n# Bundle edge Rails instead: gem 'rails', github: 'rails/rails'\ngem 'rails', '~> 5.2'\n\n#see here https://github.com/rails/rails/issues/36970\n#gem \"bootsnap\", \">= 1.1.0\", require: false # New Rails 5.2 default gem\ngem \"bootsnap\", \">= 1.4.6\"\n\n# Use postgresql as the database for Active Record\ngem 'pg'\n\n# For installing PG on macs:\ngem 'lunchy'\n\n# Use SCSS for stylesheets\ngem 'sass-rails', '~> 5.0'\n\n# Use Uglifier as compressor for JavaScript assets\ngem 'uglifier', '>= 1.3.0'\n\n# See https://github.com/rails/execjs#readme for more supported runtimes\ngem 'therubyracer'#, platforms: :ruby\n\n# CRUD of resources via a UI\ngem 'rails_admin'\ngem 'haml', '~> 5.0.4' # Rails admin needs this, but doesn't fix the version to one that works with Rails 5.2\n\n# Authentication\ngem 'devise'\ngem 'omniauth_openid_connect'\n\n# Activity logging\ngem 'public_activity', '~> 1.6.1'\n\ngem 'simple_token_authentication', '~> 1.17'\n\ngem 'bootstrap-sass', '>= 3.4.1'\n\ngem 'font-awesome-sass', '~> 4.7.0'\n\ngem 'friendly_id', '~> 5.5.0'\n\ngem 'sunspot_rails', '~> 2.2.7'\n\ngem 'sunspot_solr', '= 2.2.0'\n\ngem 'progress_bar', '~> 1.1.0'\n\ngem 'activerecord-session_store'\n\ngem 'gravtastic', '~> 3.2.6'\n\ngem 'dynamic_sitemaps', github: 'lassebunk/dynamic_sitemaps', branch: 'master'\n\ngem 'whenever'\n\n# These are required for Sidekiq, to look up scientific topics\ngem 'httparty'\ngem 'sidekiq'\ngem 'slim'\n\n# Use jquery as the JavaScript library\ngem 'jquery-rails'\n\n# Turbolinks makes following links in your web application faster. Read more: https://github.com/rails/turbolinks\ngem 'turbolinks'\ngem 'jquery-turbolinks'\n\n# Build JSON APIs with ease. Read more: https://github.com/rails/jbuilder\ngem 'jbuilder'\n# bundle exec rake doc:rails generates the API under doc/api.\ngem 'sdoc', '~> 0.4.0', group: :doc\n\n# Gem for creating before_validation callbacks for stripping whitespace\ngem 'auto_strip_attributes', '~> 2.0'\n\n# Gem for validating URLs\ngem 'validate_url', '~> 1.0.15'\n\ngem 'simple_form'\n\n# Gem for rendering Markdown\ngem 'redcarpet', '~> 3.5.1'\n\n# Gem for paginating search results\ngem 'will_paginate'\n#gem 'will_paginate-bootstrap', '~> 1.0.1'\n\n# Gem for authorisation\ngem 'pundit', '~> 1.1.0'\n\n# Simple colour picker from a predefined list\ngem 'jquery-simplecolorpicker-rails'\n\n# For getting date of materials for the home page\n#gem 'by_star', '~> 2.2.1', git: 'git://github.com/radar/by_star'\ngem 'by_star', '~> 4.0.0', git: 'https://github.com/radar/by_star'\n\n# Use ActiveModel has_secure_password\n# gem 'bcrypt', '~> 3.1.7'\n\n# Use Unicorn as the app server\n# gem 'unicorn'\n\n# Use Capistrano for deployment\n# gem 'capistrano-rails', group: :development\n\ngem 'handlebars_assets'\n\ngem 'paperclip', '~> 5.2.1'\n\ngem 'icalendar', '~> 2.4.1'\n\ngem 'bootstrap-datepicker-rails', '~> 1.6.4.1'\n\ngem 'rack-cors', require: 'rack/cors'\n\ngem 'recaptcha', require: 'recaptcha/rails'\n\ngem 'linkeddata'\n\n# Used for lat/lon rake task\ngem 'geocoder'\ngem 'redis'\n\ngem 'active_model_serializers'\n\ngem 'private_address_check'\n\n# For the link monitor rake taks\ngem 'time_diff'\n\n#passenger\ngem \"passenger\", \">= 5.0.25\", require: \"phusion_passenger/rack_handler\"\n\nsource 'https://rails-assets.org' do\n  gem 'rails-assets-markdown-it', '~> 7.0.1'\n  gem 'rails-assets-moment', '~> 2.15.0'\n  gem 'rails-assets-eonasdan-bootstrap-datetimepicker', '~> 4.17.42'\n  gem 'rails-assets-devbridge-autocomplete', '~> 1.2.26'\n  gem 'rails-assets-clipboard', '~> 1.5.12'\nend\n\ngroup :test do\n  gem 'minitest', '5.10.3'\n  gem 'fakeredis'\n  gem 'rails-controller-testing'\nend\n\ngroup :development, :test do\n  # Call 'byebug' anywhere in the code to stop execution and get a debugger console\n  gem 'webmock', '~> 3.4.2'\n  gem 'byebug'\n  gem 'simplecov'\n  gem 'rubocop'\n  gem 'codacy-coverage', :require => false\nend\n\ngroup :development do\n  # Access an IRB console on exception pages or by using <%= console %> in views\n  gem 'web-console'\n  # Spring speeds up development by keeping your application running in the background. Read more: https://github.com/rails/spring\n  #gem 'spring'\n  gem 'listen'\nend\n\ngroup :production do\n  #gem 'passenger', '~> 5.0.25'\nend\n\ngem 'mimemagic', github: 'mimemagicrb/mimemagic', ref: '01f92d86d15d85cfd0f20dabd025dcbd36a8a60f'\n\ngem 'tess_api_client', :git => 'https://github.com/pan-training/training-catalogue-api-client.git'\n\n#analytics (gdpr compliant)\ngem 'ahoy_matey'\ngem 'blazer'\ngem 'faker', :git => 'https://github.com/faker-ruby/faker.git', :branch => 'master'\n\ngem 'rake_text'\ngem 'language_list', '~> 1.1'\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pasta-bit-vector",
            "repo_link": "https://github.com/pasta-toolbox/bit_vector/",
            "content": {
                "codemeta": "",
                "readme": "# pasta::bit_vector\n\n<p align=\"center\">\n   <img width=250 height=175 src=\"https://raw.githubusercontent.com/pasta-toolbox/bit_vector/main/docs/images/logo_pasta_bit_vector.svg\" />\n</p>\n\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![DOI](https://zenodo.org/badge/419381885.svg)](https://zenodo.org/badge/latestdoi/419381885)\n[![pasta::bit_vector CI](https://github.com/pasta-toolbox/bit_vector/actions/workflows/ctest.yml/badge.svg)](https://github.com/pasta-toolbox/bit_vector/actions/workflows/ctest.yml)\n[![codecov](https://codecov.io/gh/pasta-toolbox/bit_vector/branch/main/graph/badge.svg?token=2QD6ME44SU)](https://codecov.io/gh/pasta-toolbox/bit_vector)\n\nThis header-only library contains a highly tuned (uncompressed) bit vector implementation with multiple space efficient rank and select support data structures.\nOur fastest rank and select support has a space overhead of only ~3.51% and makes use of data level parallelism via SIMD instructions.\n\nIf you use this code in a scientific context, please cite our paper.\n```bibtex\n@inproceedings{Kurpicz2022CompactRankSelect,\n  author    = {Florian Kurpicz},\n  title     = {Engineering Compact Data Structures for Rank and Select Queries on Bit Vectors},\n  booktitle = {{SPIRE}},\n  series    = {Lecture Notes in Computer Science},\n  volume    = {13617},\n  pages     = {257--272},\n  publisher = {Springer},\n  year      = {2022},\n  doi       = {10.1007/978-3-031-20643-6\\_19}\n}\n```\n\n## Contents\nThis repository contains the following algorithms and data structures.\nOur [documentation][] contains in-depth on the usage of all these algorithms and data structures including easy to follow examples.\nYou can find the example in the screenshot below as text, too.\n\n[![Screenshot Documentation](https://raw.githubusercontent.com/pasta-toolbox/bit_vector/main/docs/images/screenshot_documentation_v1.0.0.png)](https://www.pasta-toolbox.org/bit_vector/)\n\n### Bit Vectors\nBit vectors play an important role in many compressed text indices, e.g., the FM-index.\nThis repository contains the following bit vector implementations:\n\n- highly tuned [uncompressed bit vector][] with access operator\n- compact [rank](include/pasta/bit_vector/support/rank.hpp) and [select](include/pasta/bit_vector/support/rank_select.hpp) support for the uncompressed bit vector based on\n\n> Dong Zhou and David G. Andersen and Michael Kaminsky,\n> Space-Efficient, High-Performance Rank and Select Structures on Uncompressed Bit Sequences,\n> SEA 2013.\n\n- improved [rank](include/pasta/bit_vector/support/flat_rank.hpp) and [select](include/pasta/bit_vector/support/flat_rank_select.hpp) support requiring the same amount of memory but providing faster rank (up to 8% speedup) and select (up to 16.5% speedup) queries, and\n- a very fast [rank](include/pasta/bit_vector/support/wide_rank.hpp) support that can also answer [select](include/pasta/bit_vector/support/wide_rank_select.hpp) queries.\n\n[uncompressed bit vector]: include/pasta/bit_vector/bit_vector.hpp\n\n### Easy to Use\n\nSince this is a header-only library, you have to simply add it to your projects include path to use it.\nA small example can be found below.\nWe refer to the [documentation][] for more information.\n\n  ```cpp\n  #include <pasta/bit_vector/bit_vector.hpp>\n  #include <pasta/bit_vector/support/flat_rank_select.hpp>\n\n  // Create a bit vector of size 1000 containing only zeros and flip every other bit.\n  pasta::BitVector bv(1000, 0);\n  for (size_t i = 0; i < bv.size(); ++i) {\n    if (i % 2 == 0) {  bv[i] = 1; }\n  }\n  // Print the bit vector to see that everything worked ;-)\n  for (auto it = bv.begin(); it != bv.end(); ++it) {\n    std::cout << ((*it == true) ? '1' : '0');\n  }\n  std::cout << std::endl;\n\n  // Create rank and select support and print the result of some queries.\n  pasta::FlatRankSelect rs(bv);\n  std::cout << rs.rank0(250) << \", \" << rs.rank1(250)\n            << \", \"\n            << rs.select0(250) << \", \" << rs.rank1(250)\n            << std::endl;\n  ```\n\n### Benchmarks and Tests\n\nThere exist an easy to use [benchmark][], which helps to compare the implementations in this repository.\nTo build the benchmark, run the CMake command with `-DPASTA_BIT_VECTOR_BUILD_BENCHMARKS=On`.\nOur tests are contained in the folder [tests][].\nTo build the tests, run the CMake command with `-DPASTA_BIT_VECTOR_BUILD_TESTS=On`.\n\nWe also conducted an extensive experimental evaluation.\nTo this end, we use our [rank and select benchmark][] where we compare our implementations with many other compact rank and select data structures.\n\nWe refer to our paper for a full description of the results, i.e., hardware, inputs, and competitors.\nBelow, you can find some of the figures we present in the paper.\n\n![Screenshot Documentation](https://raw.githubusercontent.com/pasta-toolbox/bit_vector/main/docs/images/space_requirements_v1.0.0.png)\n\n![Screenshot Documentation](https://raw.githubusercontent.com/pasta-toolbox/bit_vector/main/docs/images/rank_times_v1.0.0.png)\n\n![Screenshot Documentation](https://raw.githubusercontent.com/pasta-toolbox/bit_vector/main/docs/images/select_times_v1.0.0.png)\n\n![Screenshot Documentation](https://raw.githubusercontent.com/pasta-toolbox/bit_vector/main/docs/images/select_times_pasta_only_v1.0.0.png)\n\n[benchmark]: benchmarks/bit_vector_benchmark.cpp\n[rank and select benchmark]: https://github.com/pasta-toolbox/bit_vector_experiments\n[tests]: tests/\n\n## How to Get This\nBelow, we list all commands that are required to build the code in this repository.\nTo this end, we provide three CMake presets (_debug_, _release_, and _release with debug information_).\n\n- The debug preset creates a `debug` folder and uses the compiler flags `-DDEBUG -O0 -g -ggdb -fsanitize=address`.\n- The release preset creates a `build` folder and uses the compiler flags `-DNDEBUG -march=native -O3`.\n- The release with debug information preset creates a `build_with_debug_info` folder and uses the compiler flags `-DDEBUG -g -march=native -O3`.\n\nPer default, we use the following compiler flags: `-Wall -Wextra -Wpedantic -fdiagnostics-color=always`.\n\n### Requirements\npasta::bit_vector is written in C++20 and requires a compiler that [supports][] it.\nWe use [Ninja][] as build system.\nFor more information on how to use this library, please refer to our [documentation][].\n\n[supports]: https://en.cppreference.com/w/cpp/compiler_support\n[Ninja]: https://ninja-build.org/\n\n### tl;dr\nTo just clone the source code, use the following.\n```bash\ngit clone git@github.com:pasta-toolbox/bit_vector\ncd bit_vector\ngit submodule update --init --recursive\n```\nIf you also want to build the test, please continue with the following commands.\n```bash\ncmake --preset=[debug|build|relwithdeb]-DPASTA_BIT_VECTOR_BUILD_TESTS=On\ncmake --build --preset=[debug|release|relwithdeb]\nctest --test-dir [debug|build|relwithdeb]\n```\n\n[documentation]: https://www.pasta-toolbox.org/bit_vector/\n\n",
                "dependencies": "# ##############################################################################\n# CMakeLists.txt\n#\n# Copyright (C) 2021-2024 Florian Kurpicz <florian@kurpicz.org>\n#\n# pasta::bit_vector is free software: you can redistribute it and/or modify it\n# under the terms of the GNU General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# pasta::bit_vector is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# pasta::bit_vector.  If not, see <http://www.gnu.org/licenses/>.\n#\n# ##############################################################################\n\ncmake_minimum_required(VERSION 3.25)\n\nproject(\n  pasta_bit_vector\n  DESCRIPTION \"Bit Vector with Compact and Fast Rank and Select Support\"\n  LANGUAGES CXX\n  VERSION 1.0.1\n)\n\nset(CMAKE_CXX_STANDARD 20)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\ninclude(FetchContent)\n\nif (PROJECT_IS_TOP_LEVEL)\n  # Enable easy formatting\n  FetchContent_Declare(\n    Format.cmake\n    GIT_REPOSITORY https://github.com/TheLartians/Format.cmake\n    GIT_TAG v1.8.1\n  )\n  FetchContent_MakeAvailable(Format.cmake)\n\n  add_subdirectory(docs)\nendif ()\n\n# Options when compiling pasta::bit_vector Build tests\noption(PASTA_BIT_VECTOR_BUILD_TESTS \"Build pasta::bit_vector's tests.\" OFF)\n# Build benchmark tools\noption(PASTA_BIT_VECTOR_BUILD_BENCHMARKS\n       \"Build pasta::bit_vector's benchmarks.\" OFF\n)\n# Build pasta::bit_vector with code coverage options\noption(PASTA_BIT_VECTOR_COVERAGE_REPORTING\n       \"Enable coverage reporting for pasta::bit_vector\" OFF\n)\n\n# Optional code coverage (library compile options are only set if coverage\n# reporting is enabled\nadd_library(pasta_bit_vector_coverage_config INTERFACE)\nif (PASTA_BIT_VECTOR_COVERAGE_REPORTING)\n  target_compile_options(\n    pasta_bit_vector_coverage_config INTERFACE -fprofile-arcs -ftest-coverage\n  )\n  target_link_libraries(pasta_bit_vector_coverage_config INTERFACE gcov)\nendif ()\n\n# pasta::bit_vector interface definitions\nadd_library(pasta_bit_vector INTERFACE)\ntarget_include_directories(\n  pasta_bit_vector INTERFACE ${CMAKE_CURRENT_SOURCE_DIR}/include\n)\ntarget_link_libraries(\n  pasta_bit_vector INTERFACE pasta_utils pasta_bit_vector_coverage_config tlx\n)\n\n# Use FetchContent to load dependencies\nFetchContent_Declare(\n  pasta_utils\n  GIT_REPOSITORY https://github.com/pasta-toolbox/utils.git\n  GIT_TAG origin/main\n)\nFetchContent_MakeAvailable(pasta_utils)\n\nFetchContent_Declare(\n  tlx\n  GIT_REPOSITORY https://github.com/tlx/tlx.git\n  GIT_TAG origin/main\n)\nFetchContent_MakeAvailable(tlx)\n\n# Optional test\nif (PASTA_BIT_VECTOR_BUILD_TESTS)\n  enable_testing()\n  add_subdirectory(tests)\nendif ()\n\n# Optional benchmarks\nif (PASTA_BIT_VECTOR_BUILD_BENCHMARKS)\n  add_executable(bit_vector_benchmark benchmarks/bit_vector_benchmark.cpp)\n\n  target_link_libraries(\n    bit_vector_benchmark\n    PUBLIC pasta_bit_vector\n           tlx\n           pasta_utils\n           optimized\n           pasta_memory_monitor\n  )\nendif ()\n\n# ##############################################################################\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pdaf",
            "repo_link": "https://github.com/PDAF/PDAF",
            "content": {
                "codemeta": "",
                "readme": "\n# PDAF (Parallel Data Assimilation Framework)\n\nCopyright 2004-2024, Lars Nerger, Alfred Wegener Institute, Helmholtz Center\nfor Polar and Marine Research, Bremerhaven, Germany. \nFor license information, please see the file LICENSE.txt.\n\nFor full documentation and tutorial, see: http://pdaf.awi.de \n\n\n## Introduction\n\nPDAF is a framework for data assimilation.\nPDAF can be used to assess data assimilation methods with small models,\nto perform real data assimilation with high-dimensional models, and to\nteach ensemble data assimilation. \n\nPDAF provides \n- A parallel infrastructure, using MPI and OpenMP, to implement a\n  parallel data assimilation system based on existing numerical\n  models (typically of components of the Earth system). \n- A selection of ensemble data assimilation algorithms based on\n  the Kalman filter or nonlinear filters (see list below)\n- A selection of 3D variational methods, both with parameterized\n  and ensemble covariance matrix\n- Functions for ensemble diagnostics\n- Functionality to generate synthetic observations for data\n  assimilation studies (e.g. OSSEs)\n\nThe PDAF release provides also\n- Tutorial codes demontrating the implementation\n- Code templates to assist in the implementation\n- Toy models fully implemented with PDAF for the study of data\n  assimilation methods.\n- Model bindings for using PDAF with different models \n\n\n## First Steps with PDAF\n\nA good starting point for using PDAF is to run a tutorial example.\nThe directory /tutorial contains files demonstrating the application \nof PDAF with a simple 2-dimensional example.\n\nThe web site  http://pdaf.awi.de/trac/wiki/FirstSteps \nprovides hints on getting started with PDAF and \n  https://pdaf.awi.de/trac/wiki/PdafTutorial\nholds the tutorial slide sets that explain the implementation\nsteps and how to compile and run the tutorial examples. \n\n\n## Models\n\nThe directory models/ contains toy models that are fully implemented\nwith PDAF. These models can be used to assess the behavior of different\nassimilation algorithms. \n- **lorenz96/**\n  This directory contains the Lorenz-96 model, which is a widely used\n  model to assess data assimilation methods. Provided is a full\n  implementation of the data assimilation with various filters and options.\n  This model can be configured to have a sufficiently large state\n  dimension to test low-rank filter algorithms.\n- **lorenz63/**\n  This directory contains the Lorenz-63 model, which is a classical\n  3-variable model with chaotic dynamics. Provided is a full\n  implementation of the data assimilation with various filters and options.\n  The small state dimension and nonlinear dynamics make it a suitable\n  test case for the standard particle filter (PF).\n- **lorenz05b/**\n  This directory contains the model Lorenz-2005 model II. Provided is a full\n  implementation of the data assimilation with various filters and options.\n- **lorenz05c/**\n  This directory contains the two-scale model Lorenz-2005 model III.\n  Provided is a full implementation of the data assimilation with various\n  filters and options.\n\nInstructions on how to run data assimilation experiments with these models\nare provided on the PDAF web site.\n\n\n## Installation of the PDAF library\n\nThe PDAF library will be automatically built when compiling a tutorial case\nor one of the models. However, one can also separately build the library.\nIn order to build the PDAF library you need a Fortran 90 compiler, and\n'make'\n\n1. Choose a suitable include file for the make process and/or edit\none. See the directory make.arch/ for several provided include files.\nThere are include files for compilation with and without MPI.\n\nNote: PDAF is generally intended for parallel computing using MPI.\nHowever, it can be compiled for serial computing. To compile PDAF\nfor this case, a simplified MPI header file is included und should be\nin the include path. In addition, a dummy implementation of MPI, which\nbehaves like MPI in the single-process case, is provided in the\ndirectory nullmpi/. For the serial case, this file should also be\ncompiled and linked when PDAF is linked to a program.\n\n2. Set the environment variable $PDAF_ARCH to the name of the include\nfile (without ending .h). Alternatively you can specify PDAF_ARCH on\nthe command line when running 'make' in step 3.\n\n3. cd into the directory src/ and execute 'make' at the prompt. This will\ncompile the sources and generate a library file that includes the \nensemble filter methods in the directory lib/. \nTo generate the PDAF library including the 3D-Var methods and the \nsolvers from the external libraries in /external/ execute\n'make pdaf-var' at the prompt.\n \n\n\n## Test suite\n\nThe directory testsuite/ contains another set of example implementations.\nThis is more for 'internal use'. We use these implementations to validate PDAF. \nThe model is trivial: At each time step simply the time step size is added \nto the state vector. In this example all available filters are implemented.\n\n\n## Verifying your installation \n\nThe tutorial implementations can be verified as follows:\n\nYou can run the script\n**runtests.sh** in the main tutorial directory **tutorial**.\n\nThis script will compile and run all tutorial implementations. Afterwards\nthe outputs at the final time step are checked against reference outputs\nfrom the directory verification using a Python script. You can also compare\n the output files like out.online_2D_parallelmodel with reference files.\n(Note: The script runtests.sh uses the generic compile definitions for\nLinux with the gfotran compiler. For other systems, you might need to\nchange the settings for the make definitions files).\n\n\nThe testsuite also provides a functionality for verification:\n\nUsing 'make' one can run test cases for the verification which are\ncompared to reference outputs provided in the sub-directories\nof the directory  testsuite/tests_dummy1D for different computers\nand compilers. In particular the online case dummymodel_1D and the\noffline test offline_1D can be run. Scripts for serial (non-parallel)\nexecution as well as example scripts for running parallel test jobs on\ncomputers with SLURM or PBS batch systems are provided.\n\nAn installation of PDAF can be verified using the test suite as follows:\n1. prepare the include file in make.arch\n2. cd to testsuite/src\n3. Build and execute the online experiments:\n   'make pdaf_dummy_online' and\n   'make test_pdaf_online > out.test_pdaf_online'\n4. Build and execute the offline experiments:\n   'make pdaf_dummy_online' and\n   'make test_pdaf_offline > out.test_pdaf_offline'\n6. Check the files out.test_pdaf_online and out.test_pdaf_offline\n   At the end of the file, you see a list of Checks done using\n   a Python script. Here the outputs are compared with reference \n   outputs produced with gfortran and MacOS.\n   You can also diff the files to corresponding files in one of the\n   example-directories in ../tests_dummy1D. Here, also reference\n   output files, like output_lestkf0.dat are stored.\n\n\n## Data Assimilation Algorithms \n\nThe filter algorithms in PDAF are:\n\n**Filters with global analysis step**\n- **EnKF** (The classical perturbed-observations Ensemble Kalman filter)\n       [G. Evensen, J. Geophys. Res. 99 C5 (1994) 10143-10162,\n        G. Burgers et al., Mon. Wea. Rev. 126 (1998) 1719-1724]\n- **ESTKF** (Error Subspace Transform Kalman filter)\n       [L. Nerger et al. Mon. Wea. Rev. 140 (2012) 2335-2345, doi:10.1175/MWR-D-11-00102.1]\n- **ETKF** (Ensemble Transform Kalman filter)\n       [C. H. Bishop et al. Mon. Wea. Rev. 129 (2001) 420-436]\n       The implementation in PDAF follows that described for the LETKF, but as a global filter. \n- **SEIK** (Singular \"Evolutive\" Interpolated Kalman) filter\n       This is the full ensemble variant of the SEIK\n       (Singular \"Interpolated\" Extended Kalman) filter.\n       [SEIK: D.-T. Pham et al., C. R. Acad Sci., Ser. III, 326 (1009)\n        255-260, for the SEIK variant in PDAF see L. Nerger et al.,\n        Tellus 57A (2005) 715-735, doi:10.3402/tellusa.v57i5.14732]\t\n- **SEEK** (Singular \"Evolutive\" Extended Kalman) filter\n       [D.-T. Pham et al., J. Mar. Syst. 16 (1998) 323-340] \n- **NETF** (Nonlinear Ensemble Transform Filter)\n       [J. Toedter, B. Ahrens, Mon. Wea. Rev. 143 (2015) 1347-1367, doi:10.1175/MWR-D-14-00108.1]\n- **PF** (Particle filter with importance resampling)\n       [see S. Vetra-Carvalho et al., Tellus A 70 (2018) 1445364, doi:10.1080/16000870.2018.1445364]\n\n**Filters with localized analysis step**\n- **LESTKF** (Local Error Subspace Transform Kalman filter)\n       [L. Nerger et al. Mon. Wea. Rev. 140 (2012) 2335-2345, doi:10.1175/MWR-D-11-00102.1]\n- **LETKF** (Local Ensemble Transform Kalman filter)\n       [B. R. Hunt et al., Physica D 230 (2007) 112-126, doi:10.1016/j.physd.2006.11.008]\n- **LSEIK** (Local Singular \"Evolutive\" Interpolated Kalman) filter\n       [L. Nerger et al., Oce. Dyn. 56 (2006) 634-649, doi:10.1007/s10236-006-0083-0]\n- **LEnKF** (The classical perturbed-observations Ensemble Kalman filter with localization)\n       [G. Evensen, J. Geophys. Res. 99 C5 (1994) 10143-10162,\n        G. Burgers et al., Mon. Wea. Rev. 126 (1998) 1719-1724]\n- **LNETF** (Nonlinear Ensemble Transform Filter with observation localization)\n       [J. Toedter, B. Ahrens, Mon. Wea. Rev. 143 (2015) 1347-1367, doi:10.1175/MWR-D-14-00108.1]\n- **LKNETF** (Local Kalman-nonlinear ensemble transform filter)\n       [L. Nerger, Q. J. R. Meteorol Soc., 148 (2022) 620-640, doi:10.1002/qj.4221]\n\nAll filter algorithms are fully parallelized with MPI and optimized. The local filters \n(LSEIK, LETKF, LESTKF, LNETF, LKNETF) are in addition parallelized using OpenMP.\n\n**Smoother extensions** are included for the filters ESTKF/LESTKF, ETKF/LETKF, EnKF, NETF/LNETF.\n\n**3D-Var methods**\n\nNext to the ensemble filter methods, different 3D-Var methods are provided:\n- **parameterized 3D-Var**\n- **3D ensemble** Var with ensemble transformation by ESTKF or LESTKF\n- **hybrid 3D-Var** with ensemble transformation by ESTKF or LESTKF\n\nThe 3D-Var methods are implemented as incremental 3D-Var schemes following\nBannister, Q. J. Royal Meteorol. Soc., 143 (2017) 607-633, doi:10.1002/qj.2982. \n\n\nPDAF is written in Fortran (mainly Fortran 90 with some features from Fortran 2003). \nThe compilation and execution has been tested on the different systems ranging from\nnotebook computers to supercomputers, e.g.:\n- Linux\n- MacOS\n- Cray CLE\n- NEC Super-UX\n- Microsoft Windows 10 with Cygwin\n\n\n## Contact Information\n\nPlease send comments, suggestions, or bug reports to pdaf@awi.de\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/peakperformance",
            "repo_link": "https://github.com/JuBiotech/peak-performance",
            "content": {
                "codemeta": "",
                "readme": "[![PyPI version](https://img.shields.io/pypi/v/peak-performance)](https://pypi.org/project/peak-performance/)\n[![pipeline](https://github.com/jubiotech/peak-performance/workflows/pipeline/badge.svg)](https://github.com/JuBiotech/peak-performance/actions)\n[![coverage](https://codecov.io/gh/jubiotech/peak-performance/branch/main/graph/badge.svg)](https://app.codecov.io/gh/JuBiotech/peak-performance)\n[![documentation](https://readthedocs.org/projects/peak-performance/badge/?version=latest)](https://peak-performance.readthedocs.io/en/latest)\n[![DOI](https://joss.theoj.org/papers/10.21105/joss.07313/status.svg)](https://doi.org/10.21105/joss.07313)\n[![DOI](https://zenodo.org/badge/713469041.svg)](https://zenodo.org/doi/10.5281/zenodo.10255543)\n\n# About PeakPerformance\nPeakPerformance employs Bayesian modeling for chromatographic peak data fitting.\nThis has the innate advantage of providing uncertainty quantification while jointly estimating all peak parameters united in a single peak model.\nAs Markov Chain Monte Carlo (MCMC) methods are utilized to infer the posterior probability distribution, convergence checks and the aformentioned uncertainty quantification are applied as novel quality metrics for a robust peak recognition.\n\n# Installation\n\nIt is highly recommended to follow the following steps and install ``PeakPerformance`` in a fresh Python environment:\n1. Install the package manager [Mamba](https://github.com/conda-forge/miniforge/releases).\nChoose the latest installer at the top of the page, click on \"show all assets\", and download an installer denominated by \"Mambaforge-version number-name of your OS.exe\", so e.g. \"Mambaforge-23.3.1-1-Windows-x86_64.exe\" for a Windows 64 bit operating system. Then, execute the installer to install mamba and activate the option \"Add Mambaforge to my PATH environment variable\".\n\n⚠ If you have already installed Miniconda, you can install Mamba on top of it but there are compatibility issues with Anaconda.\n\nℹ The newest conda version should also work, just replace `mamba` with `conda` in step 2.\n\n2. Create a new Python environment in the command line using the provided [`environment.yml`](https://github.com/JuBiotech/peak-performance/blob/main/environment.yml) file from the repo.\n   Download `environment.yml` first, then navigate to its location on the command line interface and run the following command:\n```\nmamba env create -f environment.yml\n```\n\nNaturally, it is alternatively possible to just install ``PeakPerformance`` via pip:\n\n```bash\npip install peak-performance\n```\n\n# First steps\nBe sure to check out our thorough [documentation](https://peak-performance.readthedocs.io/en/latest). It contains not only information on how to install PeakPerformance and prepare raw data for its application but also detailed treatises about the implemented model structures, validation with both synthetic and experimental data against a commercially available vendor software, exemplary usage of diagnostic plots and investigation of various effects.\nFurthermore, you will find example notebooks and data sets showcasing different aspects of PeakPerformance.\n\n# How to contribute\nIf you encounter bugs while using PeakPerformance, please bring them to our attention by opening an issue. When doing so, describe the problem in detail and add screenshots/code snippets and whatever other helpful material you can provide.\nWhen contributing code, create a local clone of PeakPerformance, create a new branch, and open a pull request (PR).\n\n# How to cite\nHead over to Zenodo to [generate a BibTeX citation](https://doi.org/10.5281/zenodo.10255543) for the latest release.\nIn addition to the utilized software version, please cite our scientific publication over at the Journal of Open Source Software (JOSS).\nA detailed citation can be found in CITATION.cff and in the sidebar.\n\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools\", \"setuptools-scm\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"peak_performance\"\nversion = \"0.7.2\"\nauthors = [\n    {name = \"Jochen Nießer\", email = \"j.niesser@fz-juelich.de\"},\n    {name = \"Michael Osthege\", email = \"m.osthege@fz-juelich.de\"},\n]\ndescription = \"A Python toolbox to fit chromatography peaks with uncertainty.\"\nreadme = \"README.md\"\nrequires-python = \">=3.9\"\ndynamic = [\"dependencies\"]\nkeywords = [\"hplc\", \"mass-spectrometry\", \"uncertainty quantification\"]\nlicense = {text = \"AGPLv3\"}\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"Operating System :: OS Independent\",\n    \"License :: OSI Approved :: GNU Affero General Public License v3\",\n    \"Intended Audience :: Science/Research\",\n]\n\n[tool.setuptools.dynamic]\ndependencies = {file = [\"requirements.txt\"]}\noptional-dependencies = {dev = { file = [\"requirements-dev.txt\"] }}\n\n\n[tool.setuptools.packages.find]\ninclude = [\"peak_performance*\"]\n\n[project.urls]\nhomepage = \"https://jugit.fz-juelich.de/IBG-1/micropro/peak-performance\"\ndocumentation = \"https://jugit.fz-juelich.de/IBG-1/micropro/peak-performance\"\nrepository = \"https://jugit.fz-juelich.de/IBG-1/micropro/peak-performance\"\n\n[tool.pytest.ini_options]\nxfail_strict=true\n\n[tool.black]\nline-length = 100\n\n[tool.ruff]\nline-length = 140\nignore-init-module-imports = true\n\n[tool.coverage.run]\nomit = [\n    # exclude tests files from coverage calculation\n    \"**/test*.py\",\n    \"**/example\",\n    \"**/notebooks\",\n]\n\n[tool.mypy]\nignore_missing_imports = true\nexclude = [\n    'test_.*?\\.py$',\n]\n\narviz\nmatplotlib\nnumpy<2\npandas\npymc>=5.9.1\npytensor\nscipy\nopenpyxl\nnumpy\n\nfrom setuptools import setup\n\nsetup()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pecon",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/pedpy",
            "repo_link": "https://github.com/PedestrianDynamics/PedPy",
            "content": {
                "codemeta": "",
                "readme": "\n<div align=\"center\">\n    <img src=\"docs/source/_static/logo_text.svg\" height=\"100px\" alt=\"PedPy Logo\">\n</div>\n\n-----------------\n[![PyPI Latest Release](https://img.shields.io/pypi/v/pedpy.svg)](https://pypi.org/project/pedpy/)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pedpy)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7194992.svg)](https://doi.org/10.5281/zenodo.7194992)\n[![License](https://img.shields.io/pypi/l/pedpy.svg)](https://github.com/PedestrianDynamics/pedpy/blob/main/LICENSE)\n![ci workflow](https://github.com/PedestrianDynamics/pedestrian-trajectory-analyzer/actions/workflows/ci.yml/badge.svg)\n[![codecov](https://codecov.io/gh/PedestrianDynamics/PedPy/graph/badge.svg?token=X5C9NTKAVK)](https://codecov.io/gh/PedestrianDynamics/PedPy)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Documentation Status](https://readthedocs.org/projects/pedpy/badge/?version=latest)](http://pedpy.readthedocs.io/?badge=latest)\n[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7046/badge)](https://bestpractices.coreinfrastructure.org/projects/7046)\n[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu)\n\n# PedPy: Analysis of pedestrian dynamics based on trajectory files.  \n\n*PedPy* is a python module for pedestrian movement analysis. \nIt implements different measurement methods for density, velocity and flow.\n\nIf you use *PedPy* in your work, please cite it using the following information from zenodo:\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7194992.svg)](https://doi.org/10.5281/zenodo.7194992)\n\n\n## Getting started\n\n### Setup Python\n\nFor setting up your Python Environment a Python version >= 3.11 is recommended (our code is tested with 3.11, 3.12, and 3.13).\nTo avoid conflicts with other libraries/applications the usage of virtual environments is recommended, see [Python Documentation](https://docs.python.org/3/library/venv.html) for more detail.\n\n### Installing PedPy\n\nTo install the latest **stable** version of *PedPy* and its dependencies from PyPI:\n```bash\npython3 -m pip install pedpy\n```\n\nYou can also install the latest version of *PedPy* directly from the repository, by following these steps:\n\n1. Uninstall an installed version of *PedPy*:\n```bash\npython3 -m pip uninstall pedpy\n```\n\n2. Install latest version of *PedPy* from repository:\n```\npython3 -m pip install git+https://github.com/PedestrianDynamics/PedPy.git\n```\n\n### Usage\n\nFor first time users, have a look at the [getting started notebook](notebooks/getting_started.ipynb), as it shows the first steps to start an analysis with *PedPy*.\nA more detailed overview of *PedPy* is demonstrated in the [user guide notebook](notebooks/user_guide.ipynb).\nThe [fundamental diagram notebook](notebooks/fundamental_diagram.ipynb) shows how to use *PedPy* for computing the fundamental diagram of a series of experiments.\n\n#### Interactive online session\n\nIf you want to try out *PedPy* for the first time, you can find an interactive online environments for both notebooks here:\n\n- Getting started: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/PedestrianDynamics/PedPy/main?labpath=notebooks%2Fgetting_started.ipynb)\n- User guide: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/PedestrianDynamics/PedPy/main?labpath=notebooks%2Fuser_guide.ipynb)\n- Fundamental diagram: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/PedestrianDynamics/PedPy/main?labpath=notebooks%2Ffundamental_diagram.ipynb)\n\n**Note:** \nThe execution might be slower compared to a local usage, as only limited resources are available.\nIt is possible to also upload different trajectory files and run the analysis completely online, but this might not be advisable for long computations.\n\n#### Local usage of the notebooks\n\nFor local usage of the notebooks, you can either download the notebooks and [demo files](notebooks/demo-data) from the GitHub repository or clone the whole repository with:\n```bash \ngit clone https://github.com/PedestrianDynamics/pedpy.git\n```\n\nFor using either of the notebook some additional libraries need to be installed, mainly for plotting.\nYou can install the needed libraries with:\n\n```bash\npython3 -m pip install jupyter matplotlib\n```\n\nAfterward, you can start a jupyter server with:\n\n```bash\njupyter notebook\n```\n\nAfter navigating to one of the notebooks, you can see how the library can be used for different kinds of analysis.\n\nSome examples how the computed values can be visualized are also shown in the notebooks, e.g., density/velocity profiles, fundamental diagrams, N-T-diagrams, etc.\n\n![voronoi](figs/voronoi_diagrams.png)\n\n![density](figs/density_comparison.png)\n\n",
                "dependencies": "[project]\nname = \"PedPy\"\ndynamic = ['version']\nauthors = [{ name = \"T. Schrödter\" }]\ndescription = \"PedPy is a Python module for pedestrian movement analysis.\"\nreadme = \"README.md\"\nlicense = { file = \"LICENSE\" }\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n    \"Natural Language :: English\",\n    \"Programming Language :: Python :: 3 :: Only\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Programming Language :: Python :: 3.13\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n]\ndependencies = [\n    \"numpy~=2.1\",\n    \"pandas~=2.2.3\",\n    \"Shapely~=2.0.6\",\n    \"scipy~=1.14\",\n    \"matplotlib~=3.9\",\n    \"h5py~=3.11\",\n]\n\nrequires-python = \">=3.11\"\n\n[project.urls]\nhomepage = \"https://pedpy.readthedocs.io/\"\n\n[build-system]\nrequires = [\"setuptools>=42\", \"wheel\", \"versioningit\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.versioningit.vcs]\nmethod = \"git\"\ndefault-tag = \"0.0.0\"\n\n[tool.versioningit.format]\ndistance = \"{next_version}.dev{distance}\"\ndirty = \"{next_version}.dev{distance}\"\ndistance-dirty = \"{next_version}.dev{distance}\"\n\n[tool.versioningit.write]\nfile = \"pedpy/_version.py\"\ndefault-tag = \"0.0.0\"\ntemplate = \"\"\"\n__version__ = \"{version}\"\n__commit_hash__ = \"{rev}\"\n\"\"\"\n\n[tool.setuptools]\npackages = [\n    \"pedpy\", \"pedpy.data\", \"pedpy.internal\",\n    \"pedpy.io\",\n    \"pedpy.methods\",\n    \"pedpy.plotting\",\n]\n\n[tool.ruff]\nexclude = [\n    \".bzr\",\n    \".direnv\",\n    \".eggs\",\n    \".git\",\n    \".git-rewrite\",\n    \".hg\",\n    \".ipynb_checkpoints\",\n    \".mypy_cache\",\n    \".nox\",\n    \".pants.d\",\n    \".pyenv\",\n    \".pytest_cache\",\n    \".pytype\",\n    \".ruff_cache\",\n    \".svn\",\n    \".tox\",\n    \".venv\",\n    \".vscode\",\n    \"__pypackages__\",\n    \"_build\",\n    \"buck-out\",\n    \"build\",\n    \"dist\",\n    \"node_modules\",\n    \"site-packages\",\n    \"venv\",\n]\n\nline-length = 80\nsrc = [\"pedpy\"]\nextend-include = [\"*.ipynb\"]\ntarget-version = \"py310\"\n\n[tool.ruff.format]\nquote-style = \"double\"\nindent-style = \"space\"\nskip-magic-trailing-comma = false\nline-ending = \"auto\"\ndocstring-code-format = true\ndocstring-code-line-length = \"dynamic\"\n\n[tool.ruff.lint.isort]\ncase-sensitive = true\nknown-first-party = [\"pedpy\"]\n\n[tool.ruff.lint]\nselect = [\n    # pyflakes\n    \"F\",\n    # pep-8-naming\n    \"N\",\n    # pycodestyle\n    \"E\", \"W\", \"D\",\n    # flake8-2020\n    \"YTT\",\n    # flake8-bugbear\n    \"B\",\n    # flake8-quotes\n    \"Q\",\n    # flake8-debugger\n    \"T10\",\n    # flake8-gettext\n    \"INT\",\n    # pylint\n    \"PL\",\n    # flake8-pytest-style\n    \"PT\",\n    # misc lints\n    \"PIE\",\n    # flake8-pyi\n    \"PYI\",\n    # tidy imports\n    \"TID\",\n    # type-checking imports\n    \"TCH\",\n    # comprehensions\n    \"C4\",\n    # pygrep-hooks\n    \"PGH\",\n    # Ruff-specific rules\n    \"RUF\",\n    # flake8-bandit: exec-builtin\n    \"S102\",\n    # numpy\n    \"NPY\",\n    # Perflint\n    \"PERF\",\n    # flynt\n    \"FLY\",\n    # flake8-logging-format\n    \"G\",\n    # flake8-future-annotations\n    \"FA\",\n    # unconventional-import-alias\n    \"ICN001\",\n    # flake8-slots\n    \"SLOT\",\n    # flake8-raise\n    \"RSE\",\n    # pandas-vet\n    \"PD\",\n    \"RUF\"\n]\nignore = []\n\n# Allow unused variables when underscore-prefixed.\ndummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n\n[tool.ruff.lint.pylint]\n## Maximum number of arguments for function / method\nmax-args = 10\n## Maximum number of branch for function / method body.\nmax-branches = 15\n## Maximum number of locals for function / method body.\nmax-locals = 20\n## Maximum number of statements in function / method body.\nmax-statements = 50\n## Constant types to ignore when used as \"magic values\" (see: PLR2004).\nallow-magic-value-types = [\"int\", \"str\"]\n\n[tool.mypy]\npython_version = \"3.10\"\nnamespace_packages = true\nignore_missing_imports = false\ncheck_untyped_defs = true\ndisallow_any_generics = true\ndisallow_incomplete_defs = true\nno_implicit_optional = true\nno_implicit_reexport = true\nstrict_equality = true\nwarn_redundant_casts = true\nwarn_unused_ignores = false\nplugins = [\"numpy.typing.mypy_plugin\"]\nexclude = \"^(helper|docs|scripts|tests)(/|$)\"\n\n[[tool.mypy.overrides]]\nmodule = [\n    \"matplotlib.*\",\n    \"mpl_toolkits.*\",\n    \"pandas.*\",\n    \"shapely.*\",\n    \"numpy.*\",\n    \"scipy.*\",\n    \"setuptools.*\",\n    \"h5py.*\",\n]\nignore_missing_imports = true\n\nnumpy~=2.1\npandas~=2.2.3\nShapely~=2.0.6\nscipy~=1.14\nmatplotlib~=3.9\nh5py~=3.11.0\n\n# testing\npytest~=8.3\npytest-mock~=3.14\npytest-cov~=5.0\n \n# build tools\nsetuptools~=75.1\nclick~=8.1\nbuild~=1.1\n\n# ci\nmypy~=1.11\nmypy-extensions==1.0\nruff~=0.6\npre-commit~=3.8\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pepc",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/SLPP/pepc/pepc",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/perihub",
            "repo_link": "https://github.com/PeriHub/PeriHub",
            "content": {
                "codemeta": "",
                "readme": "<!--\nSPDX-FileCopyrightText: 2023 PeriHub <https://github.com/PeriHub/PeriHub>\n\nSPDX-License-Identifier: Apache-2.0\n-->\n\n# PeriHub - Empowering Research with Peridynamic Modeling\n\n[![Pipeline Status](https://img.shields.io/github/actions/workflow/status/PeriHub/PeriHub/CI.yml?branch=main)](https://github.com/PeriHub/PeriLab.jl/actions)\n[![docs](https://img.shields.io/badge/docs-v1-blue.svg)](https://perihub.github.io/PeriHub/)\n[![License](https://img.shields.io/badge/License-Apache-blue.svg)](https://github.com/PeriHub/PeriHub/blob/main/LICENSE.md)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8159334.svg)](https://doi.org/10.5281/zenodo.8159334)\n[![Docker Image](https://img.shields.io/docker/pulls/perihub/frontend)](https://hub.docker.com/r/perihub/frontend)\n[![YouTube](https://img.shields.io/youtube/channel/subscribers/UCeky7HtUGlOJ2OKknvl6YnQ)](https://www.youtube.com/@PeriHub)\n\nPeriHub is a powerful software solution that can significantly benefit research in various fields. It is an extension of the open-source PeriLab software, providing a numerical implementation of the peridynamic theory. With PeriHub, researchers gain access to a valuable tool for addressing specific challenges and exploring diverse use cases in materials science, engineering, and related disciplines.\n\n## Key Features\n\n- **Peridynamic Modeling:** PeriHub excels at facilitating peridynamic modeling, enabling researchers to analyze material behavior and complex systems. Its unique approach empowers users to explore new frontiers and deepen their understanding of material behavior.\n\n- **User-Friendly Interface:** PeriHub offers a user-friendly interface, making it accessible to both experienced researchers and newcomers in the field. The platform's ease of use ensures efficient simulations, analysis of results, and gaining valuable insights into material behavior.\n\n- **REST API and GUI Support:** Researchers can seamlessly interact with PeriHub using its REST API and GUI support, providing flexibility and convenience in conducting simulations and research tasks.\n\n- **High-Quality and Reliable:** Developed collaboratively by a dedicated group of experts, PeriHub adheres to high standards of quality, reliability, and FAIRness (Findability, Accessibility, Interoperability, and Reusability). The German Aerospace Center (DLR) has played a significant role in fostering an environment that encourages innovation and interdisciplinary collaboration throughout the software's development process.\n\n- **Portability and Scalability:** PeriHub utilizes Docker containers, ensuring seamless integration and deployment across various computing environments. This approach enhances the software's portability, scalability, and ease of use, making it even more practical for research purposes.\n\n### Overview\n\n![](docs/assets/images/PeriHub.svg)\n\n### Generate model\n\n![](docs/assets/gif/generateModel.gif)\n\n### View generated mesh\n\n![](docs/assets/gif/viewMesh.gif)\n\n### Edit input deck\n\n![](docs/assets/gif/editInputDeck.gif)\n\n### Submit model\n\n![](docs/assets/gif/runModel.gif)\n\n### Analyse results\n\n![](docs/assets/gif/analyseResults.gif)\n\n### Plot results\n\n![](docs/assets/gif/plotResults.gif)\n\n### Analyse fracture\n\n![](docs/assets/gif/analyseFracture.gif)\n\n# Getting Started with PeriHub Services\n\nTo get started with PeriHub, you can use Docker Compose to easily set up the required services. Here's a step-by-step guide:\n\n- Clone the repository\n\n```\ngit clone https://github.com/PeriHub/PeriHub.git\n```\n\n- Go into the PeriHub folder.\n\n```\ncd PeriHub\n```\n\n- Copy the .env file and edit its contents.\n\n```\ncp .env.example .env\n```\n\n- Run docker-compose.\n\n```\ndocker-compose up\n```\n\n- If docker finished building PeriHub, go to http://localhost:8080\n\n## Contact\n\n- [Jan-Timo Hesse](mailto:Jan-Timo.Hesse@dlr.de)\n\n## License\n\nPlease see the file [LICENSE.md](LICENSE.md) for further information about how the content is licensed.\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/perilab",
            "repo_link": "https://github.com/PeriHub/PeriLab.jl",
            "content": {
                "codemeta": "",
                "readme": "<!--\nSPDX-FileCopyrightText: 2023 Christian Willberg <christian.willberg@dlr.de>, Jan-Timo Hesse <jan-timo.hesse@dlr.de>\n\nSPDX-License-Identifier: BSD-3-Clause\n-->\n\n# `PeriLab` - Peridynamic Laboratory\n\n[docs-dev-img]: https://img.shields.io/badge/docs-dev-blue.svg\n[docs-dev-url]: https://perihub.github.io/PeriLab.jl/dev\n\n[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg\n[docs-stable-url]: https://perihub.github.io/PeriLab.jl/stable\n\n[ci-img]: https://github.com/perihub/PeriLab.jl/actions/workflows/CI.yml/badge.svg\n[ci-url]: https://github.com/perihub/PeriLab.jl/actions/workflows/CI.yml\n\n[cov-img]: https://codecov.io/gh/perihub/PeriLab.jl/branch/main/graph/badge.svg\n[cov-url]: https://codecov.io/gh/perihub/PeriLab.jl\n\n[code-style-img]: https://img.shields.io/badge/code%20style-blue-4495d1.svg\n[code-style-url]: https://github.com/invenia/BlueStyle\n\n[aqua-img]: https://raw.githubusercontent.com/JuliaTesting/Aqua.jl/master/badge.svg\n[aqua-url]: https://github.com/JuliaTesting/Aqua.jl\n\n[doi-img]: https://zenodo.org/badge/DOI/10.1016/j.softx.2024.101700.svg\n[doi-url]: https://doi.org/10.1016/j.softx.2024.101700\n\n[release-img]: https://img.shields.io/github/v/release/PeriHub/PeriLab.jl\n[release-url]: https://github.com/PeriHub/PeriLab.jl/releases\n\n[docker-img]: https://img.shields.io/docker/pulls/perihub/perilab\n[docker-url]: https://hub.docker.com/r/perihub/perilab\n\n[license-img]: https://img.shields.io/badge/License-BSD-blue.svg\n[license-url]: https://github.com/PeriHub/PeriLab.jl/LICENSE\n\n[youtube-img]: https://img.shields.io/youtube/channel/subscribers/UCeky7HtUGlOJ2OKknvl6YnQ\n[youtube-url]: https://www.youtube.com/@PeriHub\n\n| **Documentation** | **Build Status** |  **Quality** |\n|:----:|:----:|:----:|\n| [![][docs-stable-img]][docs-stable-url] [![][docs-dev-img]][docs-dev-url] | [![][ci-img]][ci-url] [![][cov-img]][cov-url] | [![][aqua-img]][aqua-url] |\n| **Deployment** | **License** | **Socials** |\n| [![][release-img]][release-url]  [![][docker-img]][docker-url] | [![][license-img]][license-url] [![][doi-img]][doi-url] | [![][youtube-img]][youtube-url] |\n\nWelcome to `PeriLab`, a powerful software solution designed for tackling Peridynamic problems.\n\n<p align=\"center\" style=\"font-size:0;\"><!--\n  PeriLab_crack      --><img align=\"middle\" src=\"https://raw.githubusercontent.com/PeriHub/PeriLab.jl/main/assets/PeriLab_crack.gif\" width=\"50%\"><!--\n  PeriLab_additive      --><img align=\"middle\" src=\"https://raw.githubusercontent.com/PeriHub/PeriLab.jl/main/assets/PeriLab_additive.gif\" width=\"50%\">\n</p>\n\n## Documentation\n\nExplore the comprehensive [documentation](https://perihub.github.io/PeriLab.jl/) for `PeriLab`.\n\n## Examples\n\nA few basic examples of `PeriLab` can be found in the [examples](https://github.com/PeriHub/PeriLab.jl/tree/main/examples) directory, or if you want to have a look at results go to our growing [PeriLab-Results service](https://perilab-results.nimbus-extern.dlr.de).\n\n## Features ⭐\n\n- 🚀 **Easy Installation**: PeriLab's straightforward installation process makes it accessible for researchers and engineers without extensive computational expertise.\n\n- ✒️ **Modularization**: The software is designed with a modular architecture that allows users to easily integrate their own material and damage models.\n\n- 🎨 **Formulations**: Bond-based, bond-associated, as well as oridnary and non-ordinary state-based peridynamic formulations can be used with PeriLab.\n\n- 🔩 **Material models**: PeriLab supports various material models, such as elastic, plastic, and more, enabling simulation of complex materials and structures.\n\n- 🔨 **Damage models**: Damage models such as critical stretch or an energy based criterium are included to simulate different types of damage, such as crack propagation or delamination, in their peridynamic simulations.\n\n- 🔥 **Additive Manufacturing**: PeriLab supports additive manufacturing, allowing users to create custom additive models for their simulations.\n\n- 🧲 **Multimodels**: PeriLab supports multimodels simulations, combining different types of peridynamics and damage models to create a comprehensive simulation environment.\n\n- ⚡ **MPI**: PeriLab supports parallel computing using Message Passing Interface (MPI) technology to improve simulation performance on high-performance clusters.\n\n- 💻 **HPC capabilities**: PeriLab is designed for high-performance computing (HPC) environments, allowing users to run large-scale simulations efficiently.\n\n- 📤📥 **Exodus Input/Output**: PeriLab uses the Exodus II data format for input and output, enabling easy transfer of data between simulation tools.\n\n- 🧮 **Abaqus Input**: PeriLab supports Abaqus input files, allowing users to create custom Abaqus models for their simulations.\n\n- ➗ **Bond filter**: The bond filter feature allows users to apply specific conditions to the bonds between particles in a simulation, influencing their behavior and interaction with other particles.\n\n- 🔧 **User specified Input/Output**: PeriLab provides flexible options for users to specify custom input and output files, enabling easy data manipulation and analysis.\n\n- 🧪 **Test Pipeline**: The PeriLab Source Code will be tested in a test pipeline to ensure its correctness and performance.\n\n## Installation\n\nThe `PeriLab`  package is available through the Julia package system and can be installed using the following commands:\n\n```julia\nusing Pkg\nPkg.add(\"PeriLab\")\n```\n\nThroughout the rest of this tutorial, we will assume that you have installed the\nPeriLab package and have already typed `using PeriLab` to bring all of the\nrelevant variables into your current namespace.\n\n## Getting Started with `PeriLab`\n\nJumpstart your exploration of the PeriLab simulation core with provided examples. Run the following commands in Julia:\n\n```julia PeriLab\nusing PeriLab\n\nPeriLab.get_examples()\nPeriLab.main(\"examples/DCB/DCBmodel.yaml\")\n```\n>Note: More details about the main functionalities in the yaml input deck [here](https://github.com/PeriHub/PeriLab.jl/blob/main/src/Support/Parameters/parameter_handling.jl).\n\n## Parallel Processing with `PeriLab` (MPI)\n\nTo handle large-scale problems efficiently, install [MPI](https://juliaparallel.org/MPI.jl/stable/usage/). Run PeriLab with two processors on a **Linux** system using the following commands:\n\n```sh\n$ julia\njulia> using MPI\njulia> MPI.install_mpiexecjl()\n```\n>Note: If you work with **Windows 10 or higher** you can use the [WSL](https://learn.microsoft.com/en-us/windows/wsl/install) environment.\n\nRun PeriLab with two processors:\n```sh\n$ mpiexecjl -n 2 julia --project=./ -e \"using PeriLab; PeriLab.main()\" examples/DCB/DCBmodel.yaml -v\n```\n\n>Note: For HPC configurations please refer to [here](https://juliaparallel.org/MPI.jl/stable/configuration/#configure_jll_binarys).\n\n## Installing with Docker 🐳\n\n To install PeriLab using the official Perihub/Perilab Docker image, follow these steps:\n\n1. **Install Docker**: Before you begin, ensure that you have Docker installed on your system. You can download and install Docker from the official website (https://www.docker.com/). Make sure your system meets the minimum requirements for running Docker.\n\n2. **Pull the Perihub/Perilab Docker image**: Use the following command in a terminal or command prompt to pull the latest Perihub/Perilab Docker image from the Docker Hub repository:\n\n   ```bash\n   docker pull perihub/perilab\n   ```\n\n3. **Run the Docker container**: Once the image has been downloaded, create a new directory for your PeriLab simulations and navigate to it in the terminal or command prompt. Run the following command to start the Docker container:\n\n   ```bash\n   docker run -it --rm -v <path_to_local_simulations_directory>:/app/simulations perihub/perilab bash\n   ```\n\n   Replace `<path_to_local_simulations_directory>` with the absolute path to a local directory where you want to store your PeriLab simulations. This command will open a new terminal session inside the Docker container.\n\nNow, you've successfully installed PeriLab using the official Perihub/Perilab Docker image. You can start running your own peridynamic simulations within the container.\n\n## `PeriLab` on `JuliaHub`\n\nExperience the convenience of using PeriLab as a ready-to-use application on JuliaHub. Simply create an [account](https://juliahub.com), navigate to the [applications page](https://juliahub.com/ui/Applications), and add the repository URL: https://github.com/PeriHub/PeriLab.jl.\n\nConfigure advanced options, such as _filename_, _dryrun_, _verbosity_, _debug_, and _silence_. Click __Start__ and monitor the job progress. Results will be available in a zipped folder.\n\nHit the __Start__ button and wait for the job to finish, the results will be available in a zipped folder.\n\n>Note: The free tier on `JuliaHub` offers 20 hours of computational time per month.\n\n## What's Next? 🚀\n\nHere are some exciting tasks on our roadmap:\n\n- 🔑 **Quasi-static solver**: A future development for PeriLab is extending its capabilities with a more robust quasi-static solver for larger systems and complex boundary conditions.\n\n- 👊 **Contact**: An upcoming feature in PeriLab is enhancing contact modeling to support advanced features like friction, adhesion, and contact forces based on temperature or other variables.\n\n- ➕ **More material and damage models**: PeriLab's future development plans include adding more sophisticated material models (e.g., viscoelastic-plastic) and damage models, expanding the software's applicability to a wider range of real-world phenomena.\n\n- 👬 **FEM/PD coupling**: A future enhancement for PeriLab is improving its FEM/PD coupling functionality by implementing more advanced techniques, such as a seamless data exchange between FEM and PD domains.\n\n- ✂️ **Distribution logic**: As part of its ongoing development, PeriLab will continue to incorporate new distribution logic for improved performance and reduced computational resources.\n\n- 🏎️ **Optimizations**: As part of its ongoing development, PeriLab will continue to focus on optimizing the simulation process by incorporating new techniques like parallel optimization algorithms for improved efficiency and reduced computational resources.\n\nFeel free to contribute and help us make PeriLab even better! 🙌\n\n## Contributing\n\nWe welcome contributions in various forms, including bug reports, documentation improvements, feature suggestions, and more. To get started, follow these steps and have a look at the [Contribution Guidelines](CONTRIBUTING.md):\n\n### Development\n1. **Clone the repository:**\n```sh\ngit clone https://github.com/PeriHub/PeriLab.jl\ncd PeriLab.jl\n```\n2. **Activate the environment and install dependencies:**\n```sh\n$ julia\njulia> ]\npkg> activate .\npkg> up\n```\n3. **Run the script:**\n```sh\n$ julia --project=. src/main.jl examples/DCB/DCBmodel.yaml\n```\n\n## Questions\nFor any questions or inquiries about PeriLab.jl, feel free to reach out to the authors via email.\n\n## Authors and acknowledgment\n<p>\n\n<a href=\"https://orcid.org/0000-0003-2433-9183\"><img src=\"https://orcid.org/assets/vectors/orcid.logo.icon.svg\" style=\"height:15px;width:auto;vertical-align: top;background-color:transparent;\"> </a>[Prof. Dr.-Ing. Christian Willberg](mailto::christian.willberg@h2.de)\n\n</p>\n\n<p>\n\n<a href=\"https://orcid.org/0000-0002-3006-1520\"><img src=\"https://orcid.org/assets/vectors/orcid.logo.icon.svg\"  style=\"height:15px;width:auto;vertical-align: top;background-color:transparent;\"> [M.Sc. Jan-Timo Hesse](mailto::jan-timo.hesse@dlr.de)\n\n</p>\n\n## Project status\n`PeriLab` is currently in development.\n\n\n## How to cite\nTo cite PeriLab in your publications please use the following [paper](https://doi.org/10.1016/j.softx.2024.101700).\n\n```s\n@Article{WillbergC2024,\nauthor={Willberg, Christian\nand Hesse, Jan-Timo\nand Pernatii, Anna},\ntitle={{PeriLab - Peridynamic Laboratory}},\njournal={SoftwareX},\nyear={2024},\npublisher={Elsevier},\nvolume={26},\nissn={2352-7110},\ndoi={10.1016/j.softx.2024.101700},\nurl={https://doi.org/10.1016/j.softx.2024.101700}\n}\n```\n\n## Partner\n\n\n| <img src=\"https://raw.githubusercontent.com/PeriHub/PeriLab.jl/main/assets/dlr.jpg\" height=\"200\" title=\"German Aerospace Center\"> | <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Logo_h2.svg/1280px-Logo_h2.svg.png\" height=\"200\" title=\"Magdeburg-Stendal University of Applied Science\"> |\n|:------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------:|\n| [German Aerospace Center](http://www.dlr.de/sy)                                                                               | [Magdeburg-Stendal University of Applied Science](http://www.h2.de)                                                                  |\n\n\n\n\n\n## Acknowledgments\n\n<p align=\"center\" style=\"font-size:0;\"><!--\n  <!--\n  DFG      --><img align=\"middle\" src=\"https://raw.githubusercontent.com/PeriHub/PeriLab.jl/main/assets/dfg.jpg\" height=\"120\">\n</p>\n\nThis project has benefited from funding by the [Deutsche\nForschungsgemeinschaft](https://www.dfg.de/) (DFG, German Research Foundation)\nthrough the following grant ''Gekoppelte Peridynamik-Finite-Elemente-Simulationen zur Schädigungsanalyse von Faserverbundstrukturen''. <br/><br/>Grant number: [WI 4835/5-1](https://gepris.dfg.de/gepris/projekt/456427423)\n<p align=\"center\" style=\"font-size:0;\"><!--\n  SACHSEN  --><img align=\"middle\" src=\"https://raw.githubusercontent.com/PeriHub/PeriLab.jl/main/assets/sachsen.jpg\" height=\"120\">\n</p>\n\n[M-ERA.NET](https://www.m-era.net/) funded project ''Exploring Multi-Method Analysis of composite structures and joints under consideration of uncertainties engineering and processing (EMMA)''\n\nThis measure is co-financed with tax funds on the basis of the budget passed by the [Saxon state parlament](https://www.landtag.sachsen.de/de). <br/><br/>Grant number: [3028223](https://www.m-era.net/materipedia/2020/emma).\n\n<p align=\"center\" style=\"font-size:0;\"><!--\n  HyTank  --><img align=\"middle\" src=\"https://raw.githubusercontent.com/PeriHub/PeriLab.jl/main/assets/hytank.jpg\" height=\"120\"><!--\n  -->\n</p>\n\n[Federal Ministry for Economic Affairs and Climate Action](https://www.bmwk.de/Navigation/DE/Home/home.html) funded project\n''Virtuelle Kennwertermittlung, Schadensprädiktion und Simulationsmethoden für geklebte Fügestellen eines LH2-Tanks in Faserverbundbauweise für die kommerzielle Luftfahrt (HYTANK)''.<br/><br/>\nGrant number: 20W2214G.\n\n",
                "dependencies": "name = \"PeriLab\"\nuuid = \"2ecefcea-59f0-46dd-9cfd-1d2b8cc5f112\"\nauthors = [\"Christian Willberg <christian.willberg@dlr.de>\", \"Jan-Timo Hesse <jan-timo.hesse@dlr.de>\"]\nversion = \"1.2.6\"\n\n[deps]\nAbaqusReader = \"bc6b9049-e460-56d6-94b4-a597b2c0390d\"\nArgParse = \"c7e460c6-2fb9-53a9-8c5b-16f535851c63\"\nCSV = \"336ed68f-0bac-5ca0-87d4-7b16caf5d00b\"\nCombinatorics = \"861a8166-3701-5b0c-9a16-15d98fcdc6aa\"\nDataFrames = \"a93c6f00-e57d-5684-b7b6-d8193f3e46c0\"\nDataStructures = \"864edb3b-99cc-5e75-8d2d-829cb0a9cfe8\"\nDates = \"ade2ca70-3891-5945-98fb-dc099432e06a\"\nDierckx = \"39dd38d3-220a-591b-8e3c-4c3a8c710a94\"\nExodus = \"f57ae99e-f805-4780-bdca-96e224be1e5a\"\nFastGaussQuadrature = \"442a2c76-b920-505d-bb47-c5924d526838\"\nJSON3 = \"0f8b85d8-7281-11e9-16c2-39a750bddbf1\"\nLibGit2 = \"76f85450-5226-5b5a-8eaa-529ad045b433\"\nLinearAlgebra = \"37e2e46d-f89d-539d-b4ee-838fcccc9c8e\"\nLogging = \"56ddb016-857b-54e1-b83d-db4d58db5568\"\nLoggingExtras = \"e6f89c97-d47a-5376-807f-9c37f3926c36\"\nLoopVectorization = \"bdcacae8-1622-11e9-2a5c-532679323890\"\nMPI = \"da04e1cc-30fd-572f-bb4f-1f8673147195\"\nMPIPreferences = \"3da0fdf6-3ccc-4f1b-acd9-58baa6c99267\"\nOrderedCollections = \"bac558e1-5e72-5ebc-8fee-abe8a469f55d\"\nPkg = \"44cfe95a-1eb2-52ea-b672-e2afdf69b78f\"\nPointInPoly = \"b33f1834-3304-4c26-96eb-60ff7015163d\"\nPointNeighbors = \"1c4d5385-0a27-49de-8e2c-43b175c8985c\"\nPrecompileTools = \"aea7be01-6a6a-4083-8856-8a6e6704d82a\"\nPrettyTables = \"08abe8d2-0d0c-5749-adfa-8a2ac140af0d\"\nPrintf = \"de0858da-6303-5e67-8744-51eddeeeb8d7\"\nProgressBars = \"49802e3a-d2f1-5c88-81d8-b72133a6f568\"\nRandom = \"9a3f8284-a2c9-5f02-9a11-845980a1fd5c\"\nRotations = \"6038ab10-8711-5258-84ad-4b1120ba62dc\"\nStaticArrays = \"90137ffa-7385-5640-81b9-e52037218182\"\nStatistics = \"10745b16-79ce-11e8-11f9-7d13ad32a3b2\"\nTensorOperations = \"6aa20fa7-93e2-5fca-9bc0-fbd0db3c71a2\"\nTensors = \"48a634ad-e948-5137-8d70-aa71f2a747f4\"\nTimerOutputs = \"a759f4b9-e2f1-59dc-863e-4aeb61b1ea8f\"\nYAML = \"ddb6d928-2868-570f-bddf-ab3f9cf99eb6\"\nZipArchives = \"49080126-0e18-4c2a-b176-c102e4b3760c\"\n\n[compat]\nAbaqusReader = \"^0.2.6\"\nAqua = \"^0.8\"\nArgParse = \"^1\"\nCSV = \"^0.10\"\nCombinatorics = \"^1\"\nDataFrames = \"^1\"\nDataStructures = \"^0.18\"\nDates = \"^1\"\nDierckx = \"^0.5\"\nExodus = \"^0.13\"\nFastGaussQuadrature = \"^1\"\nJSON3 = \"^1\"\nLibGit2 = \"^1\"\nLinearAlgebra = \"^1\"\nLogging = \"^1\"\nLoggingExtras = \"^1\"\nLoopVectorization = \"0.12\"\nMPI = \"^0.20\"\nMPIPreferences = \"^0.1\"\nOrderedCollections = \"^1\"\nPkg = \"^1\"\nPointInPoly = \"1.2.0\"\nPointNeighbors = \"^0.4\"\nPrecompileTools = \"^1\"\nPrettyTables = \"^2\"\nPrintf = \"^1\"\nProgressBars = \"^1\"\nRandom = \"^1\"\nRotations = \"^1\"\nStaticArrays = \"^1\"\nStatistics = \"^1\"\nTensorOperations = \"^4, 5\"\nTensors = \"^1\"\nTest = \"^1\"\nTestSetExtensions = \"^3\"\nTimerOutputs = \"^0.5\"\nYAML = \"^0.4\"\nZipArchives = \"^1, 2\"\njulia = \"^1.6\"\n\n[extras]\nAqua = \"4c88cf16-eb10-579e-8560-4a9242c79595\"\nJSON3 = \"0f8b85d8-7281-11e9-16c2-39a750bddbf1\"\nLogging = \"56ddb016-857b-54e1-b83d-db4d58db5568\"\nMPI = \"da04e1cc-30fd-572f-bb4f-1f8673147195\"\nTest = \"8dfed614-e22c-5e08-85e1-65c5234f0b40\"\nTestSetExtensions = \"98d24dd4-01ad-11ea-1b02-c9a08f80db04\"\n\n[targets]\ntest = [\"Aqua\", \"JSON3\", \"Logging\", \"MPI\", \"Test\", \"TestSetExtensions\"]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/perun",
            "repo_link": "https://github.com/Helmholtz-AI-Energy/perun",
            "content": {
                "codemeta": "",
                "readme": "<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/Helmholtz-AI-Energy/perun/main/docs/images/full_logo.svg\">\n</div>\n\n&nbsp;\n&nbsp;\n\n[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu)\n[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7253/badge)](https://bestpractices.coreinfrastructure.org/projects/7253)\n[![DOI](https://zenodo.org/badge/523363424.svg)](https://zenodo.org/badge/latestdoi/523363424)\n![PyPI](https://img.shields.io/pypi/v/perun)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/perun)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![codecov](https://codecov.io/gh/Helmholtz-AI-Energy/perun/graph/badge.svg?token=9O6FSJ6I3G)](https://codecov.io/gh/Helmholtz-AI-Energy/perun)\n[![](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)\n[![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n[![Documentation Status](https://readthedocs.org/projects/perun/badge/?version=latest)](https://perun.readthedocs.io/en/latest/?badge=latest)\n\nperun is a Python package that calculates the energy consumption of Python scripts by sampling usage statistics from your Intel, Nvidia or AMD hardware components. It can handle MPI applications, gather data from hundreds of nodes, and accumulate it efficiently. perun can be used as a command-line tool or as a function decorator in Python scripts.\n\nCheck out the [docs](https://perun.readthedocs.io/en/latest/) or a working [example](https://github.com/Helmholtz-AI-Energy/perun/blob/main/examples/torch_mnist/README.md)!\n\n## Key Features\n\n - Measures energy consumption of Python scripts using Intel RAPL, ROCM-SMI, Nvidia-NVML, and psutil\n - Capable of handling MPI application, gathering data from hundreds of nodes efficiently\n - Monitor individual functions using decorators\n - Tracks energy usage of the application over multiple executions\n - Easy to benchmark applications and functions\n - Experimental!: Can monitor any non-distributed command line application\n\n## Installation\n\nFrom PyPI:\n\n```console\npip install perun\n```\n\n> Extra dependencies like nvidia-smi, rocm-smi and mpi can be installed using pip as well:\n```console\npip install perun[nvidia, rocm, mpi]\n```\n\nFrom Github:\n\n```console\npip install git+https://github.com/Helmholtz-AI-Energy/perun\n```\n\n## Quick Start\n\n### Command Line\n\nTo use perun as a command-line tool, run the monitor subcommand followed by the path to your Python script and its arguments:\n\n```console\n$ perun monitor path/to/your/script.py [args]\n```\n\nperun will output two files, and HDF5 style containing all the raw data that was gathered, and a text file with a summary of the results.\n\n\n```text\nPERUN REPORT\n\nApp name: finetune_qa_accelerate\nFirst run: 2023-08-15T18:56:11.202060\nLast run: 2023-08-17T13:29:29.969779\n\n\nRUN ID: 2023-08-17T13:29:29.969779\n\n|   Round # | Host                | RUNTIME   | ENERGY     | CPU_POWER   | CPU_UTIL   | GPU_POWER   | GPU_MEM    | DRAM_POWER   | MEM_UTIL   |\n|----------:|:--------------------|:----------|:-----------|:------------|:-----------|:------------|:-----------|:-------------|:-----------|\n|         0 | hkn0432.localdomain | 995.967 s | 960.506 kJ | 231.819 W   | 3.240 %    | 702.327 W   | 55.258 GB  | 29.315 W     | 0.062 %    |\n|         0 | hkn0436.localdomain | 994.847 s | 960.469 kJ | 235.162 W   | 3.239 %    | 701.588 W   | 56.934 GB  | 27.830 W     | 0.061 %    |\n|         0 | All                 | 995.967 s | 1.921 MJ   | 466.981 W   | 3.240 %    | 1.404 kW    | 112.192 GB | 57.145 W     | 0.061 %    |\n\nThe application has been run 7 times. In total, it has used 3.128 kWh, released a total of 1.307 kgCO2e into the atmosphere, and you paid 1.02 € in electricity for it.\n```\n\nPerun will keep track of the energy of your application over multiple runs.\n\n#### Binary support (experimental)\n\nperun is capable of monitoring simple applications written in other languages, as long as they don't make use of MPI or are distributed over multiple computational nodes.\n\n```console\n$ perun monitor --binary path/to/your/executable [args]\n```\n\n### Function Monitoring\n\nUsing a function decorator, information can be calculated about the runtime, power draw and component utilization while the function is executing.\n\n```python\n\nimport time\nfrom perun import monitor\n\n@monitor()\ndef main(n: int):\n    time.sleep(n)\n```\n\nAfter running the script with ```perun monitor```, the text report will add information about the monitored functions.\n\n```text\nMonitored Functions\n\n|   Round # | Function                    |   Avg Calls / Rank | Avg Runtime     | Avg Power        | Avg CPU Util   | Avg GPU Mem Util   |\n|----------:|:----------------------------|-------------------:|:----------------|:-----------------|:---------------|:-------------------|\n|         0 | main                        |                  1 | 993.323±0.587 s | 964.732±0.499 W  | 3.244±0.003 %  | 35.091±0.526 %     |\n|         0 | prepare_train_features      |                 88 | 0.383±0.048 s   | 262.305±19.251 W | 4.541±0.320 %  | 3.937±0.013 %      |\n|         0 | prepare_validation_features |                 11 | 0.372±0.079 s   | 272.161±19.404 W | 4.524±0.225 %  | 4.490±0.907 %      |\n```\n\n### MPI\n\nPerun is compatible with MPI applications that make use of ```mpi4py```, and requires changes in the code or in the perun configuration. Simply replace the ```python``` command with ```perun monitor```.\n\n```console\nmpirun -n 8 perun monitor path/to/your/script.py\n```\n\n## Docs\n\nTo get more information, check out our [docs page](https://perun.readthedocs.io/en/latest/) or check the [examples](https://github.com/Helmholtz-AI-Energy/perun/tree/main/examples).\n\n## Citing perun\n\nIf you found perun usefull, please consider citing the conference paper:\n\n * Gutiérrez Hermosillo Muriedas, J.P., Flügel, K., Debus, C., Obermaier, H., Streit, A., Götz, M.: perun: Benchmarking Energy Consumption of High-Performance Computing Applications. In: Cano, J., Dikaiakos, M.D., Papadopoulos, G.A., Pericàs, M., and Sakellariou, R. (eds.) Euro-Par 2023: Parallel Processing. pp. 17–31. Springer Nature Switzerland, Cham (2023). https://doi.org/10.1007/978-3-031-39698-4_2.\n\n\n```bibtex\n@InProceedings{10.1007/978-3-031-39698-4_2,\n  author=\"Guti{\\'e}rrez Hermosillo Muriedas, Juan Pedro\n  and Fl{\\\"u}gel, Katharina\n  and Debus, Charlotte\n  and Obermaier, Holger\n  and Streit, Achim\n  and G{\\\"o}tz, Markus\",\n  editor=\"Cano, Jos{\\'e}\n  and Dikaiakos, Marios D.\n  and Papadopoulos, George A.\n  and Peric{\\`a}s, Miquel\n  and Sakellariou, Rizos\",\n  title=\"perun: Benchmarking Energy Consumption of High-Performance Computing Applications\",\n  booktitle=\"Euro-Par 2023: Parallel Processing\",\n  year=\"2023\",\n  publisher=\"Springer Nature Switzerland\",\n  address=\"Cham\",\n  pages=\"17--31\",\n  abstract=\"Looking closely at the Top500 list of high-performance computers (HPC) in the world, it becomes clear that computing power is not the only number that has been growing in the last three decades. The amount of power required to operate such massive computing machines has been steadily increasing, earning HPC users a higher than usual carbon footprint. While the problem is well known in academia, the exact energy requirements of hardware, software and how to optimize it are hard to quantify. To tackle this issue, we need tools to understand the software and its relationship with power consumption in today's high performance computers. With that in mind, we present perun, a Python package and command line interface to measure energy consumption based on hardware performance counters and selected physical measurement sensors. This enables accurate energy measurements on various scales of computing, from a single laptop to an MPI-distributed HPC application. We include an analysis of the discrepancies between these sensor readings and hardware performance counters, with particular focus on the power draw of the usually overlooked non-compute components such as memory. One of our major insights is their significant share of the total energy consumption. We have equally analyzed the runtime and energy overhead perun generates when monitoring common HPC applications, and found it to be minimal. Finally, an analysis on the accuracy of different measuring methodologies when applied at large scales is presented.\",\n  isbn=\"978-3-031-39698-4\"\n}\n```\n\n",
                "dependencies": "[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"perun\"\nversion = \"0.8.8\"\ndescription = \"Measure the energy used by your MPI+Python applications.\"\nauthors = [\n    {name = \"Gutiérrez Hermosillo Muriedas, Juan Pedro\", email=\"juan.muriedas@kit.edu\"}\n]\nmaintainers = [\n    {name = \"Gutiérrez Hermosillo Muriedas, Juan Pedro\", email=\"juan.muriedas@kit.edu\"}\n]\nreadme = \"README.md\"\nlicense = {file = \"LICENSE\"}\nkeywords = [\"energy\", \"monitoring\", \"mpi\", \"python\", \"hpc\", \"command-line\", \"benchmarking\"]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Environment :: Console\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: BSD License\",\n    \"Programming Language :: Python :: 3\",\n    \"Topic :: Scientific/Engineering\",\n    \"Topic :: System :: Monitoring\",\n    \"Topic :: System :: Benchmark\",\n    \"Topic :: System :: Hardware\",\n    \"Topic :: System :: Distributed Computing\",\n    \"Typing :: Typed\",\n]\n\nrequires-python = \">=3.8\"\n\ndependencies = [\n    \"py-cpuinfo>=5.0.0\",\n    \"numpy>=1.20.0\",\n    \"psutil>=5.9.0\",\n    \"h5py>=3.5.9\",\n    \"pandas>=1.3\",\n    \"tabulate>=0.9\",\n]\n\n\n[project.optional-dependencies]\nnvidia = [\"nvidia-ml-py>=12.535.77\"]\nmpi = [\"mpi4py>=3.1\"]\nrocm = [\"pyrsmi>=1.0.1\"]\ndocs = [\n    \"sphinx>=3\",\n    \"sphinx-autoapi>=3\",\n    \"sphinx-rtd-theme>=2\",\n    \"sphinx-autodoc-typehints>=1\",\n]\ndev = [\n    # Testing\n    \"pytest\",\n    \"coverage\",\n    \"hypothesis~=6.100\",\n\n    # QA\n    \"pre-commit\",\n\n    # Releases and changelogs\n    \"python-semantic-release>=8.1.1\",\n    \"build\"\n]\n\n[project.urls]\nHomepage = \"https://github.com/Helmholtz-AI-Energy/perun\"\nDocumentation = \"https://perun.readthedocs.io\"\nRepository = \"https://github.com/Helmholtz-AI-Energy/perun\"\nIssues = \"https://github.com/Helmholtz-AI-Energy/perun/issues\"\nChangelog = \"https://github.com/Helmholtz-AI-Energy/perun/blob/main/CHANGELOG.md\"\n\n[project.scripts]\nperun = \"perun.api.cli:cli\"\n\n# Semantic release\n[tool.semantic_release]\nbuild_command = \"python -m build\"\nversion_variables = [ \"perun/__init__.py:__version__\", \"docs/conf.py:release\" ]\nversion_toml = [\"pyproject.toml:project.version\"]\ncommit_message = \"chore: {version}\\n\\nAutomatically generated by python-semantic-release\"\ncommit_parser = \"angular\"\ntag_format = \"v{version}\"\n\n[tool.semantic_release.branches.main]\nmatch = \"main\"\nprerelease_token = \"rc\"\nprerelease = false\n\n[tool.semantic_release.branches.release-test]\nmatch = \"release-test\"\nprerelease_token = \"rc\"\nprerelease = true\n\n[tool.semantic_release.changelog]\ntemplate_dir = \"templates\"\nchagelog_file = \"CHANGELOG.md\"\nexclude_commit_patterns = []\n\n[tool.semantic_release.commit_parser_options]\nallowed_tags = [\"docs\", \"feat\", \"fix\", \"perf\", \"refactor\"]\nminor_tags = [\"feat\"]\npatch_tags = [\"fix\", \"perf\"]\ndefault_bump_level = 0\n\n\n[tool.semantic_release.commit_author]\nenv = \"GIT_COMMIT_AUTHOR\"\ndefault = \"semantic-release <semantic-release>\"\n\n# Ruff\n[tool.ruff]\ntarget-version = \"py38\"\nexclude = [\"docs\", \"examples\"]\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"I\", \"D\"]\nignore = [\"E501\"]\n\n[tool.ruff.lint.per-file-ignores]\n\"tests/*\" = [\"D\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"numpy\"\n\n[tool.ruff.format]\ndocstring-code-format = true\n\n# Testing\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\n\n[tool.coverage.run]\nsource = [\"perun\"]\ncommand_line = \"-m pytest\"\n\n[tool.coverage.report]\nformat = \"markdown\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/petrack",
            "repo_link": "https://jugit.fz-juelich.de/ped-dyn-emp/petrack",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/php-codemeta-crosswalk",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/php-codemeta-crosswalk",
            "content": {
                "codemeta": "{\n    \"@context\": \"https://doi.org/10.5063/schema/codemeta-2.0\",\n    \"@type\": \"SoftwareSourceCode\",\n    \"datePublished\": \"2023-10-06\",\n    \"dateCreated\": \"2023-07-31\",\n    \"description\": \" Codemeta conversions \",\n    \"name\": \"codemeta-crosswalk\",\n    \"license\": \"https://spdx.org/licenses/Apache-2.0.html\",\n    \"dateModified\": \"2024-05-06\",\n    \"softwareVersion\": \"1.0-Beta\",\n    \"developmentStatus\": \"active\",\n    \"programmingLanguage\": \"PHP\",\n    \"operatingSystem\": \"cross-platform\",\n    \"runtimePlatform\": \"PHP Interpreter\",\n    \"relatedLink\": \"https://1959e979-c58a-4d3c-86bb-09ec2dfcec8a.ka.bw-cloud-instance.org\",\n    \"codeRepository\": \"https://github.com/Ramy-Badr-Ahmed/codemetaCrosswalk\",\n    \"readme\": \"https://github.com/Ramy-Badr-Ahmed/codemetaCrosswalk/blob/main/README.md\",\n    \"identifier\": \"https://archive.softwareheritage.org/swh:1:dir:b00c5e6af70451deb1960215c2a4b34e53586820;origin=https://github.com/Ramy-Badr-Ahmed/codemetaCrosswalk;visit=swh:1:snp:9e3fdc7155f39bf4fdd94c0af65efa2c31178b15;anchor=swh:1:rev:8224c09eefbf9c1df87de7c0edbeae18e849ca32\",\n    \"author\": [\n        {\n            \"@type\": \"Person\",\n            \"email\": \"126559907+Ramy-Badr-Ahmed@users.noreply.github.com\",\n            \"familyName\": \"Ahmed\",\n            \"givenName\": \"Ramy-Badr\"\n        }\n    ],\n    \"keywords\": [\n        \"bibtex\",\n        \"biblatex\",\n        \"datacite\",\n        \"codemeta-json\",\n        \"codemeta-crosswalk\"\n    ],\n    \"softwareRequirements\": [\n        \"PHP >=8.0\",\n        \"illuminate/validation>=10.43\",\n        \"composer/spdx-licenses>=1.5\",\n        \"illuminate/support>=10.39\"\n    ],\n    \"maintainer\": {\n        \"@type\": \"Person\",\n        \"email\": \"126559907+Ramy-Badr-Ahmed@users.noreply.github.com\",\n        \"familyName\": \"Ahmed\",\n        \"givenName\": \"Ramy-Badr\"\n    },\n    \"applicationCategory\": [\n        \"codemeta.json\",\n        \"Metadata\",\n        \"Conversions of Metadata\"\n    ],\n    \"funder\": {\n        \"@type\": \"Organization\",\n        \"funding\": \"EU-Horizon: 101057264\",\n        \"@id\": \"https://doi.org/10.3030/101057264\",\n        \"name\": \"European Comission\"\n    }\n}\n\n",
                "readme": "![GitHub top language](https://img.shields.io/github/languages/top/Ramy-Badr-Ahmed/codemetaCrosswalk)\n![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/codemetaCrosswalk)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808884.svg)](https://doi.org/10.5281/zenodo.12808884)\n\n[![SWH](https://archive.softwareheritage.org/badge/swh:1:dir:8fe28098cf799244158d8204fe80c471fcf04a66/)](https://archive.softwareheritage.org/swh:1:dir:8fe28098cf799244158d8204fe80c471fcf04a66;origin=https://github.com/Ramy-Badr-Ahmed/codemetaCrosswalk;visit=swh:1:snp:7129240170329390ce30cc6819cde370f3595473;anchor=swh:1:rev:9a6b4f039e24bb96503eb8179f810ed6a4c8ed4c)\n\n# Codemeta.json Crosswalk\n\nThis repository has been developed as part of the [FAIR4CoreEOSC project](https://faircore4eosc.eu/eosc-core-components/eosc-research-software-apis-and-connectors-rsac) to address two project's pillars (Describe and Cite).\n\n> [!Note]\n>  A demonstrable version can be accessed here: <a href=\"https://1959e979-c58a-4d3c-86bb-09ec2dfcec8a.ka.bw-cloud-instance.org/\" target=\"_blank\">**Demo Version**</a>\n\nSample snapshot of the codemeta generator and converter demo:\n\n![snap.PNG](snap.PNG)\n\nIt currently addresses metadata conversions for the following use cases:\n\n| From     | To            |\n|----------|---------------|\n| CodeMeta | DataCite [^1] |\n| CodeMeta | BibLatex [^2] |\n| CodeMeta | BibTex [^3]   |\n\nThe codemeta conversion pattern to the above schemes is extendable to other metadata schemes as template classes located under `Schemes` directory. The initial keys correspondence is defined in this repository [^4].\n\n> [!Note]\n> There's a scheme template class that can help see this pattern. Please consult the crosswalk [^4] and scheme documentations to properly construct this class.\n\n## Installation Steps:\n\n    1) Clone this project.\n    \n    2) Open a console session and navigate to the cloned directory:\n    \n        Run \"composer install\"\n\n        This should involve installing the PHP REPL, PsySH\n\n    3) (optional) Add psysh to PATH\n            \n            Example, Bash: \n                    echo 'export PATH=\"$PATH:/The_Cloned_Directory/vendor/bin\"' >> ~/.bashrc\n                    source ~/.bashrc\n\n    4) (Optional) Create your local branch.\n\n## Usage:\n\n- In a console session inside the cloned directory, start the php REPL:    \n\n```php\n$ psysh     // if not added to PATH replace with: vendor/bin/psysh\n\nPsy Shell v0.12.0 (PHP 8.2.0 — cli) by Justin Hileman\n```\n\n- Define namespace:\n\n```php\n> namespace Conversions; \n> use Conversions;\n```\n\n- Specify codemeta.json path:\n\n```php\n> $codeMetaPath = 'CodeMeta/codeMeta.json'\n```\n\n> [!Note]\n> By default, codemeta.json is located under 'CodeMeta' directory where an example already exists.\n> `$codeMetaPath` can _also_ directly take codemeta.json as an array\n\n- Specify target scheme (as fully qualified class name)\n\n```php\n> $dataCite = \\Schemes\\DataCite::class\n> $bibLatex = \\Schemes\\BibLatex::class\n> $bibTex   = \\Schemes\\BibTex::class\n```\n\n> [!Note]\n> By default, scheme classes are located under 'Schemes' directory.\n\n- Get the conversion from the specified Codemeta.json:\n\n```php\n> $errors = NULL;    // initialise errors variable\n \n> $dataCiteFromCodeMeta = CodeMetaConversion::To($dataCite, $codeMetaPath, $errors)      // array-formatted\n\n> $bibLatexFromCodeMeta = CodeMetaConversion::To($bibLatex, $codeMetaPath, $errors)      // string-formatted\n\n> $bibTexFromCodeMeta = CodeMetaConversion::To($bibTex, $codeMetaPath, $errors)          // string-formatted\n```\n\n- Retrieve errors (if occurred) from the `Illuminate\\Support\\MessageBag()` instance:\n\n```php\n> $errors->messages()      // gets error messages as specified in CodeMeta/conversionsValidations.php\n\n> $errors->keys()          // gets codemeta keys where errors occurred\n\n> $errors->first()         // gets the first occurred error message\n\n> $errors->has('identifier')    // checks whether an error has occurred in the codemeta `identifier` key\n```\n\n> [!Note]\n> Validations use the `Illuminate\\Validation\\Validator` package.\n> Error messages and rules can be customised in `CodeMeta/conversionsValidations.php` as per the package syntax.\n\n\n### References\n[^1]: [DataCite Metadata Schema](https://schema.datacite.org/meta/kernel-4.3/doc/DataCite-MetadataKernel_v4.3.pdf).\n[^2]: [BibLATEX style extension for Software](https://ctan.math.washington.edu/tex-archive/macros/latex/contrib/biblatex-contrib/biblatex-software/software-biblatex.pdf).\n[^3]: [BibTex](https://en.wikipedia.org/wiki/BibTeX).\n[^4]: [Codemeta Crosswalk](https://github.com/codemeta/codemeta/blob/master/crosswalk.csv).        \n\n",
                "dependencies": "{\n    \"name\": \"codemeta/crosswalk\",\n    \"type\": \"project\",\n    \"description\": \"Codemeta conversions to other metadata schemes/styles. Part of the FAIRCORE4EOSC project for LZI-Dagstuhl\",\n    \"keywords\": [\"codemeta\", \"crosswalk\", \"bibtex\", \"datacite\", \"biblatex\"],\n    \"license\": \"MIT\",\n    \"require\": {\n        \"php\": \"^8.2\",\n        \"psy/psysh\": \"^0.12.0\",\n        \"illuminate/support\": \"^10.39\",\n        \"composer/spdx-licenses\": \"^1.5\",\n        \"illuminate/validation\": \"^10.43\"\n    },\n    \"autoload\": {\n        \"psr-4\": {\n            \"Conversions\\\\\": \"Conversions\",\n            \"Schemes\\\\\": \"Schemes\"\n        }\n    },\n    \"minimum-stability\": \"dev\",\n    \"prefer-stable\": true\n}\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/picongpu",
            "repo_link": "https://github.com/ComputationalRadiationPhysics/picongpu",
            "content": {
                "codemeta": "",
                "readme": "PIConGPU - Particle-in-Cell Simulations for the Exascale Era\n============================================================\n\n[![Code Status dev](https://gitlab.com/hzdr/crp/picongpu/badges/dev/pipeline.svg?key_text=dev)](https://gitlab.com/hzdr/crp/picongpu/pipelines/dev/latest)\n[![Documentation Status](https://readthedocs.org/projects/picongpu/badge/?version=latest)](http://picongpu.readthedocs.io)\n[![Doxygen](https://img.shields.io/badge/API-Doxygen-blue.svg)](http://computationalradiationphysics.github.io/picongpu)\n[![Language](https://img.shields.io/badge/language-C%2B%2B17-orange.svg)](https://isocpp.org/)\n[![License PIConGPU](https://img.shields.io/badge/license-GPLv3-blue.svg?label=PIConGPU)](https://www.gnu.org/licenses/gpl-3.0.html)\n[![License PMacc](https://img.shields.io/badge/license-LGPLv3-blue.svg?label=PMacc)](https://www.gnu.org/licenses/lgpl-3.0.html)\n\n[![PIConGPU Presentation Video](http://img.youtube.com/vi/nwZuG-XtUDE/0.jpg)](http://www.youtube.com/watch?v=nwZuG-XtUDE)\n[![PIConGPU Release](docs/logo/pic_logo_vert_158x360.png)](http://www.youtube.com/watch?v=nwZuG-XtUDE)\n\nIntroduction\n------------\n\nPIConGPU is a fully relativistic,\n[manycore](https://en.wikipedia.org/wiki/Manycore_processor),\n3D3V particle-in-cell ([PIC](http://en.wikipedia.org/wiki/Particle-in-cell))\ncode. The Particle-in-Cell algorithm is a central tool in plasma physics.\nIt describes the dynamics of a plasma by computing the motion of\nelectrons and ions in the plasma based on\n[Maxwell's equations](http://en.wikipedia.org/wiki/Maxwell%27s_equations).\n\nPIConGPU implements various numerical schemes to solve the PIC cycle.\nIts features for the electro-magnetic PIC algorithm include:\n- a central or Yee-lattice for fields\n- particle pushers that solve the equation of motion for charged and neutral\n  particles, e.g., the *Boris-* and the\n  [*Vay-Pusher*](http://dx.doi.org/10.1063/1.2837054)\n- Maxwell field solvers, e.g.\n  [*Yee's*](http://dx.doi.org/10.1109/TAP.1966.1138693) and\n  [*Lehe's*](http://dx.doi.org/10.1103/PhysRevSTAB.16.021301) scheme\n- rigorously charge conserving current deposition schemes, such as\n  [*Esirkepov*](http://dx.doi.org/10.1016/S0010-4655%2800%2900228-9)\n  and *EZ* (Esirkepov meets ZigZag)\n- macro-particle form factors ranging from NGP (0th order), CIC (1st),\n  TSC (2nd), PQS (3rd) to PCS (4th)\n\nand the electro-magnetic PIC algorithm is further self-consistently coupled to:\n- classical radiation reaction\n  ([DOI:10.1016/j.cpc.2016.04.002](http://dx.doi.org/10.1016/j.cpc.2016.04.002))\n- advanced field ionization methods\n  ([DOI:10.1103/PhysRevA.59.569](http://dx.doi.org/10.1103/PhysRevA.59.569),\n   [LV Keldysh](http://www.jetp.ac.ru/cgi-bin/dn/e_020_05_1307.pdf), BSI)\n\nBesides the electro-magnetic PIC algorithm and extensions to it, we developed\na wide range of tools and diagnostics, e.g.:\n- online, far-field radiation diagnostics for coherent and incoherent radiation\n  emitted by charged particles\n- full restart and output capabilities via [openPMD](http://openPMD.org),\n  including [parallel HDF5](http://hdfgroup.org/)\n- 2D and 3D live view and diagnostics tools\n- a large selection of extensible\n  [online-plugins](http://picongpu.readthedocs.io/en/latest/usage/plugins.html)\n\nAs one of our supported compute platforms, GPUs provide a computational\nperformance of several\n[TFLOP/s](http://en.wikipedia.org/wiki/FLOPS) at considerable lower invest and\nmaintenance costs compared to multi CPU-based compute architectures of similar\nperformance. The latest high-performance systems\n([TOP500](http://www.top500.org/)) are enhanced by accelerator hardware that\nboost their peak performance up to the multi-PFLOP/s level. With its\noutstanding performance and scalability to more than 18'000 GPUs,\nPIConGPU was one of the **finalists** of the 2013\n[Gordon Bell Prize](http://sc13.supercomputing.org/content/acm-gordon-bell-prize).\n\nPIConGPU is developed and maintained by the\n[Computational Radiation Physics Group](https://www.hzdr.de/db/Cms?pNid=2097)\nat the [Institute for Radiation Physics](http://www.hzdr.de/db/Cms?pNid=132)\nat [HZDR](http://www.hzdr.de/) in close collaboration with the Center\nfor Information Services and High Performance Computing\n([ZIH](http://tu-dresden.de/die_tu_dresden/zentrale_einrichtungen/zih)) of the\nTechnical University Dresden ([TUD](http://www.tu-dresden.de)). We are a\nmember of the [Dresden GPU Center of Excellence](http://ccoe-dresden.de/) that\ncooperates on a broad range of scientific GPU and manycore applications,\nworkshops and teaching efforts.\n\nAttribution\n-----------\n\nPIConGPU is a *scientific project*. If you **present and/or publish** scientific\nresults that used PIConGPU, you should set a **reference** to show your support.\n\nOur according **up-to-date publication** at **the time of your publication**\nshould be inquired from:\n- [REFERENCE.md](https://raw.githubusercontent.com/ComputationalRadiationPhysics/picongpu/master/REFERENCE.md)\n\nPlease also consider adding yourself to our [community map](https://github.com/ComputationalRadiationPhysics/picongpu-communitymap).\nWe would love to hear from you!\n\nOral Presentations\n------------------\n\nThe following slide should be part of **oral presentations**. It is intended to\nacknowledge the team maintaining PIConGPU and to support our community:\n\n(*coming soon*) presentation_picongpu.pdf\n(svg version, key note version, png version: 1920x1080 and 1024x768)\n\nSoftware License\n----------------\n\n*PIConGPU* is licensed under the **GPLv3+**. Furthermore, you can develop your\nown particle-mesh algorithms based on our general library *PMacc* that is\nshipped alongside PIConGPU. *PMacc* is *dual licensed* under both the\n**GPLv3+ and LGPLv3+**.\nFor a detailed description, please refer to [LICENSE.md](LICENSE.md)\n\n********************************************************************************\n\nInstall\n-------\n\nSee our notes in [INSTALL.rst](INSTALL.rst).\n\nUsers\n-----\n\nDear User, we hereby emphasize that we are still actively developing PIConGPU at great\nspeed and do, from time to time, break backwards compatibility.\n\nWhen using this software, please stick to the latest release or use the `dev` branch containing the\nlatest changes. It also contains a file `CHANGELOG.md` with the\nlatest changes (and how to update your simulations). Read it first before\nupdating between two versions! Also, we add a git `tag` according to a version\nnumber for each release.\n\nFor any questions regarding the usage of PIConGPU please **do not** contact the\ndevelopers and maintainers directly.\n\nInstead, please [open an issue on GitHub](https://github.com/ComputationalRadiationPhysics/picongpu/issues/new).\n\nBefore you post a question, browse the PIConGPU\n[documentation](https://github.com/ComputationalRadiationPhysics/picongpu/search?l=markdown),\n[wiki](https://github.com/ComputationalRadiationPhysics/picongpu/wiki) and the\n[issue tracker](https://github.com/ComputationalRadiationPhysics/picongpu/issues)\nto see if your question has been answered, already.\n\nPIConGPU is a collaborative project.\nWe thus encourage users to engage in answering questions of other users and post solutions to problems to the list.\nA problem you have encountered might be the future problem of another user.\n\nIn addition, please consider using the collaborative features of GitHub if you have questions or comments on code or documentation.\nThis will allow other users to see the piece of code or documentation you are referring to.\n\nMain ressources are in our [online manual](https://picongpu.readthedocs.io), the [user section](https://github.com/ComputationalRadiationPhysics/picongpu/wiki) of our wiki, documentation files in [`.md` (Markdown)](http://commonmark.org/help/) and [`.rst` (reStructuredText)](http://www.sphinx-doc.org/en/stable/rest.html) format in this repository and a [getting started video](http://www.youtube.com/watch?v=7ybsD8G4Rsk).\nFeel free to visit [picongpu.hzdr.de](http://picongpu.hzdr.de) to learn more about the PIC algorithm.\n\nSoftware Upgrades\n-----------------\n\nPIConGPU ships new and frequent changes to the code in the development branch `dev`.\n\nFrom time to time we publish a new release\nof PIConGPU. Before you pull the changes in, please read our\n[ChangeLog](CHANGELOG.md)!\nYou may have to update some of your simulation `.param` and `.cfg` files by\nhand since PIConGPU is an active project and new features often require changes\nin input files. Additionally, a full description of new features and fixed bugs\nin comparison to the previous release is provided in that file.\n\nIn case you decide to use *new, potentially buggy and experimental* features\nfrom our `dev` branch, be aware that you must participate or at least follow the development yourself. \nSyntax changes and in-development bugs will *not* be announced outside of their according pull\nrequests and issues.\n\nBefore drafting a new release, we open a new `release-*` branch from `dev` with\nthe `*` being the version number of the upcoming release. This branch only\nreceives bug fixes (feature freeze) and users are welcome to try it out\n(however, the change log and a detailed announcement might still be missing in\nit).\n\nDevelopers\n----------\n\n### How to participate\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md)\n\nIf you like to jump in right away, see  \n[![open \"good first issue\" issues](https://img.shields.io/github/issues-raw/ComputationalRadiationPhysics/picongpu/good%20first%20issue.svg?color=56cbef)](https://github.com/ComputationalRadiationPhysics/picongpu/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n\nActive Team\n-----------\n\n### Scientific Supervision\n\n- Dr. Michael Bussmann\n\n### Maintainers* and core developers\n\n- Dr. Sergei Bastrakov*\n- Finn-Ole Carstens\n- Dr. Alexander Debus\n- Dr. Marco Garten*\n- Dr. Axel Huebl*\n- Brian Edward Marre\n- Pawel Ordyna \n- Dr. Richard Pausch*\n- Franz Poeschel\n- Dr. Klaus Steiniger*\n- Rene Widera*\n\n### Former Members, Contributions and Thanks\n\nThe PIConGPU Team expresses its gratitude to:\n\nFlorian Berninger, Heiko Burau, Fabia Dietrich, Robert Dietrich, Carlchristian Eckert,\nSimeon Ehrig, Wen Fu, Ph.D., Alexander Grund, Sebastian Hahn, Anton Helm, Wolfgang Hoehnig,\nDr.-Ing. Guido Juckeland, Jeffrey Kelling, Maximilian Knespel, Dr. Remi Lehe,\nFelix Schmitt, Frank Winkler, Benjamin Schneider, Joseph Schuchart, Conrad Schumann,\nStefan Tietze, Marija Vranic, Ph.D., Benjamin Worpitz, Erik Zenker,\nSophie Rudat, Sebastian Starke, Alexander Matthes, Kseniia Bastrakova, \nBernhard Manfred Gruber, Jakob Trojok, Anton Lebedev, Nils Prinz,\nFelix Meyer, Lennert Sprenger, Manhui Wang, Maxence Thevenet, Ilja Goethel,\nMika Soren Voß, Lei Bifeng, Andrei Berceanu, Felix Meyer,\nLennert Sprenger and Nico Wrobel.\n\nKudos to everyone, mentioned or unmentioned, who contributed further in any\nway!\n\n********************************************************************************\n\n![image of an lwfa](docs/images/lwfa_iso.png \"LWFA\")\n![image of our strong scaling](docs/images/StrongScalingPIConGPU_log.png \"Strong Scaling\")\n\n",
                "dependencies": "-r lib/python/picongpu/requirements.txt\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pigx",
            "repo_link": "https://github.com/BIMSBbioinfo/pigx",
            "content": {
                "codemeta": "",
                "readme": "[![install with Guix badge](https://img.shields.io/badge/install%20with-guix-F4BB15.svg)](https://gnu.org/s/guix)\n\n<div align=\"center\">\n<img src=\"logo.svg\" alt=\"PiGx logo\" width=\"30%\" height=\"30%\" />\n</div>\n\n# What is PiGx?\n\nPiGx is a collection of genomics pipelines. More information can be found in [PiGx website](https://bioinformatics.mdc-berlin.de/pigx/)\n\nIt includes the following pipelines:\n\n- [PiGx BSseq](https://github.com/BIMSBbioinfo/pigx_bsseq) for raw\n  fastq read data of bisulfite experiments\n\n- [PiGx RNAseq](https://github.com/BIMSBbioinfo/pigx_rnaseq) for RNAseq samples\n\n- [PiGx scRNAseq](https://github.com/BIMSBbioinfo/pigx_scrnaseq) for\n  single cell dropseq analysis\n\n- [PiGx ChIPseq](https://github.com/BIMSBbioinfo/pigx_chipseq) for\n  reads from ChIPseq experiments\n\n- [PiGx CRISPR](https://github.com/BIMSBbioinfo/pigx_crispr) *(work in progress)*\n  for the analysis of sequence mutations in CRISPR-CAS9 targeted\n  amplicon sequencing data\n\nAll pipelines are easily configured with a sample sheet (in CSV\nformat) and a descriptive settings file (in YAML format).  For more\ndetailed information see the README.md file for each of the pipelines\nin the `pipelines` directory.\n\n## Publication\n\n**Wurmus R, Uyar B, Osberg B, Franke V, Gosdschan A, Wreczycka K, Ronen J,\nAkalin A**. [PiGx: Reproducible genomics analysis pipelines with GNU Guix.](https://www.ncbi.nlm.nih.gov/pubmed/30277498)\n**Gigascience**. 2018 Oct 2. doi: 10.1093/gigascience/giy123. PubMed PMID: 30277498.\n\n# Getting started\n\nTo run PiGx on your experimental data, describe your samples in a CSV\nfile `sample_sheet.csv`, provide a `settings.yaml` to override the\ndefaults defaults, and select the pipeline.\n\nTo generate a settings file template for any pipeline:\n\n```sh\npigx [pipeline] --init=settings\n```\n\nTo generate a sample sheet template for any pipeline:\n\n```sh\npigx [pipeline] --init=sample-sheet\n```\n\nHere's a simple example to run the RNAseq pipeline:\n\n```sh\npigx rnaseq my-sample-sheet.csv --settings my-settings.yaml\n```\n\nTo see all available options run `pigx --help`.\n\n\n# Install\n\nPre-built binaries for PiGx are available through GNU Guix, the\nfunctional package manager for reproducible, user-controlled software\nmanagement.  Install the complete pipeline bundle with the following\ncommand:\n\n```sh\nguix install pigx\n```\n\nIf you want to install PiGx from source, please make sure that all\nrequired dependencies are installed and then follow the common GNU\nbuild system steps after unpacking the [latest release\ntarball](https://github.com/BIMSBbioinfo/pigx/releases/latest):\n\n```sh\n./configure --prefix=/some/where\nmake install\n```\n\nYou can enable or disable each of the pipelines with the\n`--enable-PIPELINE` and `--disable-PIPELINE` arguments to the\n`configure` script.  `PIPELINE` is one of `bsseq`, `rnaseq`,\n`scrnaseq`, `chipseq`, and `crispr`.  For more options run\n`./configure --help`.\n\n\n# License\n\nAll PiGx pipelines are free software: you can redistribute PiGx and/or\nmodify it under the terms of the GNU General Public License as\npublished by the Free Software Foundation, either version 3 of the\nLicense, or (at your option) any later version.\n\nSee `LICENSE` for the full license text.\n\n",
                "dependencies": "#!/bin/sh\n\nautoreconf -vif\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pkgndep",
            "repo_link": "https://github.com/jokergoo/pkgndep",
            "content": {
                "codemeta": "",
                "readme": "# Analyzing Dependency Heaviness of R Packages\n\n[![R-CMD-check](https://github.com/jokergoo/pkgndep/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/pkgndep/actions)\n[![CRAN](https://www.r-pkg.org/badges/version/pkgndep)](https://cran.r-project.org/web/packages/pkgndep/index.html)\n\n\nWhen developing R packages, we should try to avoid directly setting\ndependencies on \"heavy packages\". The \"heaviness\" for a package means, the\nnumber of additional dependency packages it brings to. If your package directly depends\non a heavy package, it would bring several consequences:\n\n1. Users need to install a lot of additional packages when installing your\n   package which brings the risk that installation of some packages\n   may fail and it makes your package cannot be installed. \n2. The namespaces that are loaded into your R session after loading your package will be huge (you can see the loaded namespaces by `sessionInfo()`).\n3. You package will be \"heavy\" as well and it may take long time to load your package.\n\nIn the DESCRIPTION file of your package, there are \"direct dependency\npakcages\" listed in the `Depends`, `Imports` and `LinkingTo` fields. There are\nalso \"indirect dependency packages\" that can be found recursively for each of\nthe direct dependency packages. Here what we called \"dependency packages\" are\nthe union of the direct and indirect dependency packages.\n\nThere are also packages listed in `Suggests` and `Enhances` fields in\nDESCRIPTION file, but they are not enforced to be installed when installing\nyour package. Of course, they also have \"indirect dependency packages\". To get\nrid of the heavy packages that are not often used in your package, it is\nbetter to move them into the `Suggests`/`Enhances` fields and to load/install\nthem only when they are needed.\n\nHere the **pkgndep** package checks the heaviness of the dependency packages\nof your package. For each package listed in the `Depends`, `Imports`,\n`LinkingTo` and `Suggests`/`Enhances` fields in the DESCRIPTION file,\n**pkgndep** checks how many additional packages your package requires. The\nsummary of the dependency is visualized by a customized heatmap.\n\nAs an example, I am developing a package called\n[**cola**](https://github.com/jokergoo/cola/) which depends on [a lot of other\npackages](https://github.com/jokergoo/ComplexHeatmap/blob/master/DESCRIPTION).\nThe dependency heatmap looks like follows:\n\n![](https://user-images.githubusercontent.com/449218/140655626-f2062b6e-c11f-4dc0-b6b9-d954feffc4ad.png)\n\n\nIn the heatmap, rows are the packages listed in `Depends`, `Imports` and\n`Suggests` fields, columns are the additional dependency packages required for\neach row package. The barplots on the right show the number of required\npackage, the number of imported functions/methods/classes (parsed from\nNAMESPACE file) and the quantitative measure \"heaviness\" (the definition of\nheaviness will be introduced later).\n\nWe can see if all the packages are put in the `Depends` or `Imports` field\n(i.e. movig all suggsted packages to `Imports`), in total 248\npackages are required, which are really a lot. Actually some of the heavy\npackages such as **WGCNA**, **clusterProfiler** and **ReactomePA** (the last\nthree packages in the heatmap rows) are not very frequently used in **cola**,\nmoving them to `Suggests` field and using them only when they are needed\ngreatly helps to reduce the heaviness of **cola**. Now the number of required\npackages are reduced to only 64.\n\n## Citation\n\nGu Z. et al., pkgndep: a tool for analyzing dependency heaviness of R packages. Bioinformatics 2022. https://doi.org/10.1093/bioinformatics/btac449\n\nGu Z, On the Dependency Heaviness of CRAN/Bioconductor Ecosystem. Journal of Systems and Software 2023. https://doi.org/10.1016/j.jss.2023.111610\n\n## Installation\n\nThe **pkgndep** package can be installed from CRAN by\n\n```r\ninstall.packages(\"pkgndep\")\n```\n\n## Usage\n\nTo use this package:\n\n```r\nlibrary(pkgndep)\npkg = pkgndep(\"package-name\")\ndependency_heatmap(pkg)\n```\n\nor\n\n```r\npkg = pkgndep(\"path-of-the-package\")\ndependency_heatmap(pkg)\n```\n\nAn executable example:\n\n```r\nlibrary(pkgndep)\npkg = pkgndep(\"ComplexHeatmap\")\npkg\n```\n\n```\n## ComplexHeatmap, version 2.9.4\n## 30 additional packages are required for installing 'ComplexHeatmap'\n## 117 additional packages are required if installing packages listed in all fields in DESCRIPTION\n```\n\n```r\ndependency_heatmap(pkg)\n```\n\n![](https://user-images.githubusercontent.com/449218/140655659-2ca142c5-067f-4f76-a0d2-00d0aea49c96.png)\n\n## Heaviness database\n\nThere is an integrated dependency heaviness database for all R packages for a lot of R/Bioc versions. The database can be accessed by:\n\n```r\nheaviness_database()\n```\n\n## License\n\nMIT @ Zuguang Gu\n\n",
                "dependencies": "Package: pkgndep\r\nType: Package\r\nTitle: Analyze Dependency Heaviness of R Packages\r\nVersion: 1.99.1\r\nDate: 2023-07-30\r\nAuthors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"),\r\n                  comment = c('ORCID'=\"0000-0002-7395-8709\"))\r\nDepends: R (>= 4.0.0)\r\nImports: ComplexHeatmap (>= 2.6.0), GetoptLong, GlobalOptions, utils, grid, hash, \r\n    methods, BiocManager, brew, BiocVersion\r\nSuggests: knitr, rmarkdown, svglite, callr, rjson, Rook, igraph, ggplot2, ggrepel, \r\n    base64, testthat, cowplot\r\nDescription: A new metric named 'dependency heaviness' is proposed that measures the number \r\n    of additional dependency packages that a parent package brings to its child package \r\n    and are unique to the dependency packages imported by all other parents.  \r\n    The dependency heaviness analysis is visualized by a customized heatmap. \r\n    The package is described in <doi:10.1093/bioinformatics/btac449>. \r\n    We have also performed the dependency heaviness analysis on the CRAN/Bioconductor \r\n    package ecosystem and the results are implemented as a web-based database \r\n    which provides comprehensive tools for querying dependencies of individual R packages. \r\n    The systematic analysis on the CRAN/Bioconductor ecosystem is described in \r\n    <doi:10.1016/j.jss.2023.111610>. From 'pkgndep' version 2.0.0, the heaviness \r\n    database includes snapshots of the CRAN/Bioconductor ecosystems for many old R versions.\r\nURL: https://github.com/jokergoo/pkgndep\r\nVignetteBuilder: knitr\r\nLicense: MIT + file LICENSE\r\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pointneighborsjl",
            "repo_link": "https://github.com/trixi-framework/PointNeighbors.jl",
            "content": {
                "codemeta": "",
                "readme": "# PointNeighbors.jl\n\n[![Docs-stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://trixi-framework.github.io/PointNeighbors.jl/stable)\n[![Docs-dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://trixi-framework.github.io/PointNeighbors.jl/dev)\n[![Slack](https://img.shields.io/badge/chat-slack-e01e5a)](https://join.slack.com/t/trixi-framework/shared_invite/zt-sgkc6ppw-6OXJqZAD5SPjBYqLd8MU~g)\n[![Youtube](https://img.shields.io/youtube/channel/views/UCpd92vU2HjjTPup-AIN0pkg?style=social)](https://www.youtube.com/@trixi-framework)\n[![Build Status](https://github.com/trixi-framework/PointNeighbors.jl/workflows/CI/badge.svg)](https://github.com/trixi-framework/PointNeighbors.jl/actions?query=workflow%3ACI)\n[![Codecov](https://codecov.io/gh/trixi-framework/PointNeighbors.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/trixi-framework/PointNeighbors.jl)\n[![SciML Code Style](https://img.shields.io/static/v1?label=code%20style&message=SciML&color=9558b2&labelColor=389826)](https://github.com/SciML/SciMLStyle)\n[![License: MIT](https://img.shields.io/badge/License-MIT-success.svg)](https://opensource.org/license/mit/)\n[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.12702157.svg)](https://zenodo.org/doi/10.5281/zenodo.12702157)\n\n**PointNeighbors.jl** is a package for neighborhood search with fixed search radius in\n1D, 2D and 3D point clouds.\n\n## Features\n\n- Several implementations of neighborhood search with fixed search radius\n- Focus on fast incremental updates to be usable for particle-based simulations with\n  frequent updates\n- Designed as a \"playground\" to easily switch between different implementations and data\n  structures\n- Common API over all implementations\n- Extensive benchmark suite to study different implementations (work in progress)\n- GPU compatibility (work in progress)\n\n| Implementation  | Description | Features | Query | Update | GPU-compatible |\n| ------------- | ------------- | --- | :--: | :--: | :--: |\n| `GridNeighborhoodSearch` with `DictionaryCellList` | Grid-based NHS with Julia `Dict` backend | Infinite domain | Fast | Fast | ❌ |\n| `GridNeighborhoodSearch` with `FullGridCellList` | Grid-based NHS allocating all cells of the domain | Finite domain, but efficient memory layout for densely filled domain. | Faster | Fastest | ✅ |\n| `PrecomputedNeighborhoodSearch` | Precompute neighbor lists | Best for [TLSPH](https://trixi-framework.github.io/TrixiParticles.jl/stable/systems/total_lagrangian_sph/) without NHS updates. Not suitable for updates in every time step. | Fastest | Very slow | ❌ |\n\n## Benchmarks\n\nThe following benchmarks were conducted on an AMD Ryzen Threadripper 3990X using 128 threads.\n\nBenchmark of a single force computation step of a Weakly Compressible SPH (WCSPH) simulation:\n![wcsph](https://github.com/trixi-framework/PointNeighbors.jl/assets/44124897/ad5c378b-9ce2-4e6f-91dc-1e0da379b91f)\n\nBenchmark of an incremental update similar to a WCSPH simulation (note the log scale):\n![update](https://github.com/trixi-framework/PointNeighbors.jl/assets/44124897/71eac5c9-6aa5-4267-bc0b-4057c89f8b12)\n\nBenchmark of a full right-hand side evaluation of a WCSPH simulation (note the log scale):\n![rhs](https://github.com/trixi-framework/PointNeighbors.jl/assets/44124897/ac328a96-1b9f-4319-a785-dce9d862fd70)\n\n\n## Packages using PointNeighbors.jl\n\n- [TrixiParticles.jl](https://github.com/trixi-framework/TrixiParticles.jl)\n- [Peridynamics.jl](https://github.com/kaipartmann/Peridynamics.jl)\n- [PeriLab.jl](https://github.com/PeriHub/PeriLab.jl)\n\nIf you're using PointNeighbors.jl in your package, please feel free to open a PR adding it\nto this list.\n\n\n## Cite Us\n\nIf you use PointNeighbors.jl in your own research or write a paper using results obtained\nwith the help of PointNeighbors.jl, please cite it as\n```bibtex\n@misc{pointneighbors,\n  title={{P}oint{N}eighbors.jl: {N}eighborhood search with fixed search radius in {J}ulia},\n  author={Erik Faulhaber and Niklas Neher and Sven Berger and\n          Michael Schlottke-Lakemper and Gregor Gassner},\n  year={2024},\n  howpublished={\\url{https://github.com/trixi-framework/PointNeighbors.jl}},\n  doi={10.5281/zenodo.12702157}\n}\n```\n\n",
                "dependencies": "name = \"PointNeighbors\"\nuuid = \"1c4d5385-0a27-49de-8e2c-43b175c8985c\"\nauthors = [\"Erik Faulhaber <erik.faulhaber@uni-koeln.de>\"]\nversion = \"0.4.6-dev\"\n\n[deps]\nAdapt = \"79e6a3ab-5dfb-504d-930d-738a2a938a0e\"\nAtomix = \"a9b6321e-bd34-4604-b9c9-b65b8de01458\"\nGPUArraysCore = \"46192b85-c4d5-4398-a991-12ede77f4527\"\nKernelAbstractions = \"63c18a36-062a-441e-b654-da1e3ab1ce7c\"\nLinearAlgebra = \"37e2e46d-f89d-539d-b4ee-838fcccc9c8e\"\nPolyester = \"f517fe37-dbe3-4b94-8317-1923a5111588\"\nReexport = \"189a3867-3050-52da-a836-e630ba90ab69\"\nStaticArrays = \"90137ffa-7385-5640-81b9-e52037218182\"\n\n[compat]\nAdapt = \"3, 4\"\nAtomix = \"0.1, 1\"\nGPUArraysCore = \"0.1, 0.2\"\nKernelAbstractions = \"0.9\"\nLinearAlgebra = \"1\"\nPolyester = \"0.7.5\"\nReexport = \"1\"\nStaticArrays = \"1\"\njulia = \"1.9\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/postwrf",
            "repo_link": "https://github.com/anikfal/PostWRF",
            "content": {
                "codemeta": "",
                "readme": "# PostWRF\n\n[![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.8191714.svg)](https://zenodo.org/record/8191714)\n\n### Visualization and postprocessing of the WRF and ERA5 data\n\n**Plot the WRF and ERA5 data, in the same simple way as you run the WRF model!**\n\nPostWRF is a bunch of interactive tools, written in NCL and Bash scripts, to visualize and post-process the WRF model outputs (as well as ERA5 and RTTOV data, to some extent).\n\nPostWRF is useful for both the expert and less-experienced users. Students can plot the WRF and ERA5 outputs whithout struggling with coding and syntax errors. Expert users can also carry out common postprocessing tasks in a simple and straightforward way.\n\n## Main capabilities:\n- WRF Data extraction\n- WRF horizontal contour plot\n- WRF cross-section plot\n- WRF statistical diagrams\n- RTTOV input (from WRF) and output data generation\n- WRF data conversion to Geotiff\n- WRF Skew-T and windrose diagrams\n- ERA5 horizontal contour plot\n- ERA5 data extraction\n\n## Sample visualizations and postprocessing\n![github_postwrf](https://github.com/anikfal/PostWRF/assets/11738727/16be89c3-1bb1-4245-a430-1d07876563dd)\n\n\n## Installation:\nInstall NCL on a Linux machine (e.g. Fedora):\n```bash\nsudo dnf install ncl\n```\nThat's it! Enough for most of the PostWRF's capabilities!\n\n## Run PostWRF:\n1. ``` git clone git@github.com:anikfal/PostWRF.git ```\n2. ``` cd PostWRF ```\n3. ``` chmod +x postwrf.sh modules/*.sh modules_era/*.sh ```\n4. Copy or link your WRF or ERA5 files in the PostWRF directory\n5. ``` ./postwrf.sh ```\n6. Follow the instructions and give relevant information to visualize/postprocess your data\n\n\n## HTML Documentations:\nDocumentations with practical examples: https://postwrf.readthedocs.io/en/master\n\n## YouTube Training Videos:\nhttps://youtube.com/playlist?list=PL93HaRiv5QkCOWQ4E_Oeszi9DBOYrdNXD\n\n## Paper:\nFor more detailed information about the backend structure of the software, please read https://doi.org/10.1016/j.envsoft.2022.105591\n\nIf you find PostWRF helpful, please kindly cite it in your works.\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/potsdam-open-source-radio-interferometry-tool",
            "repo_link": "https://git.gfz-potsdam.de/vlbi-data-analysis/port",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/profasi",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/slbio/profasi",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/propulate",
            "repo_link": "https://github.com/Helmholtz-AI-Energy/propulate",
            "content": {
                "codemeta": "",
                "readme": "![Propulate Logo](https://raw.githubusercontent.com/Helmholtz-AI-Energy/propulate/refs/heads/main/LOGO_light.svg#gh-light-mode-only)\n![Propulate Logo](https://raw.githubusercontent.com/Helmholtz-AI-Energy/propulate/refs/heads/main/LOGO_dark.svg#gh-dark-mode-only)\n\n\n# Parallel Propagator of Populations\n\n[![DOI](https://zenodo.org/badge/495731357.svg)](https://zenodo.org/badge/latestdoi/495731357)\n[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu)\n[![License: BSD-3](https://img.shields.io/badge/License-BSD--3-blue)](https://opensource.org/licenses/BSD-3-Clause)\n![PyPI](https://img.shields.io/pypi/v/propulate)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/propulate)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)[![](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/7785/badge)](https://www.bestpractices.dev/projects/7785)\n[![](https://img.shields.io/badge/Contact-propulate%40lists.kit.edu-orange)](mailto:propulate@lists.kit.edu)\n[![Documentation Status](https://readthedocs.org/projects/propulate/badge/?version=latest)](https://propulate.readthedocs.io/en/latest/?badge=latest)\n[![codecov](https://codecov.io/gh/Helmholtz-AI-Energy/propulate/graph/badge.svg?token=ZG6PEXJOIO)](https://codecov.io/gh/Helmholtz-AI-Energy/propulate)[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/Helmholtz-AI-Energy/propulate/main.svg)](https://results.pre-commit.ci/latest/github/Helmholtz-AI-Energy/propulate/main)\n\n# **Click [here](https://www.scc.kit.edu/en/aboutus/16956.php) to watch our 3 min introduction video!**\n\n## What `Propulate` can do for you\n\n`Propulate` is an HPC-tailored software for solving optimization problems in parallel. It is openly accessible and easy\nto use. Compared to a widely used competitor, `Propulate` is consistently faster - at least an order of magnitude for a\nset of typical benchmarks - and in some cases even more accurate.\n\nInspired by biology, `Propulate` borrows mechanisms from biological evolution, such as selection, recombination, and\nmutation. Evolution begins with a population of solution candidates, each with randomly initialized genes. It is an\niterative \"survival of the fittest\" process where the population at each iteration can be viewed as a generation. For\neach generation, the fitness of each candidate in the population is evaluated. The genes of the fittest candidates are\nincorporated in the next generation.\n\nLike in nature, `Propulate` does not wait for all compute units to finish the evaluation of the current generation.\nInstead, the compute units communicate the currently available information and use that to breed the next candidate\nimmediately. This avoids waiting idly for other units and thus a load imbalance.\nEach unit is responsible for evaluating a single candidate. The result is a fitness level corresponding with that\ncandidate’s genes, allowing us to compare and rank all candidates. This information is sent to other compute units as\nsoon as it becomes available.\nWhen a unit is finished evaluating a candidate and communicating the resulting fitness, it breeds the candidate for the\nnext generation using the fitness values of all candidates it evaluated and received from other units so far.\n\n`Propulate` can be used for hyperparameter optimization and neural architecture search at scale.\nIt was already successfully applied in several accepted scientific publications. Applications include grid load\nforecasting, remote sensing, and structural molecular biology:\n\n> J. Debus, C. Debus, G. Dissertori, et al. **PETNet–Coincident Particle Event Detection using Spiking Neural Networks**.\n> 2024 Neuro Inspired Computational Elements Conference (NICE), La Jolla, CA, USA, pp. 1-9 ( 2024).\n> https://doi.org/10.1109/NICE61972.2024.10549584\n\n> D. Coquelin, K. Flügel, M. Weiel, et al. **AB-Training: A Communication-Efficient Approach for Distributed Low-Rank\n> Learning**. arXiv preprint (2024). https://doi.org/10.48550/arXiv.2405.01067\n\n> D. Coquelin, K. Flügel, M. Weiel, et al. **Harnessing Orthogonality to Train Low-Rank Neural Networks**. arXiv\n> preprint (2024). https://doi.org/10.48550/arXiv.2401.08505\n\n> A. Weyrauch, T. Steens, O. Taubert, et al. **ReCycle: Fast and Efficient Long Time Series Forecasting with Residual\n> Cyclic Transformers**. 2024 IEEE Conference on Artificial Intelligence (CAI), Singapore, pp. 1187-1194 (2024).\n> https://doi.org/10.1109/CAI59869.2024.00212\n\n> O. Taubert, F. von der Lehr, A. Bazarova, et al. **RNA contact prediction by data efficient deep learning**.\n> Communications Biology 6(1), 913 (2023). https://doi.org/10.1038/s42003-023-05244-9\n\n> D. Coquelin, K. Flügel, M. Weiel, et al. **Harnessing Orthogonality to Train Low-Rank Neural Networks**. arXiv\n> preprint (2023). https://doi.org/10.48550/arXiv.2401.08505\n\n> Y. Funk, M. Götz, and H. Anzt. **Prediction of optimal solvers for sparse linear systems using deep learning**.\n> Proceedings of the 2022 SIAM Conference on Parallel Processing for Scientific Computing (pp. 14-24). Society for\n> Industrial and Applied Mathematics (2022). https://doi.org/10.1137/1.9781611977141.2\n\n> D. Coquelin, R. Sedona, M. Riedel, and M. Götz. **Evolutionary Optimization of Neural Architectures in Remote Sensing\n> Classification Problems**. IEEE International Geoscience and Remote Sensing Symposium IGARSS, Brussels, Belgium,\n> pp. 1587-1590 (2021). https://doi.org/10.1109/IGARSS47720.2021.9554309\n\n## In more technical terms\n\n``Propulate`` is a massively parallel evolutionary hyperparameter optimizer based on the island model with asynchronous\npropagation of populations and asynchronous migration.\nIn contrast to classical GAs, ``Propulate`` maintains a continuous population of already evaluated individuals with a\nsoftened notion of the typically strictly separated, discrete generations.\nOur contributions include:\n- A novel parallel genetic algorithm based on a fully asynchronized island model with independently processing workers.\n- Massive parallelism by asynchronous propagation of continuous populations and migration via efficient communication using the message passing interface.\n- Optimized use efficiency of parallel hardware by minimizing idle times in distributed computing environments.\n\nTo be more efficient, the generations are less well separated than they usually are in evolutionary algorithms.\nNew individuals are generated from a pool of currently active, already evaluated individuals that may be from any\ngeneration.\nIndividuals may be removed from the breeding population based on different criteria.\n\nYou can find the corresponding publication [here](https://doi.org/10.1007/978-3-031-32041-5_6):\n> Taubert, O. *et al.* (2023). Massively Parallel Genetic Optimization Through Asynchronous Propagation of Populations.\n> In: Bhatele, A., Hammond, J., Baboulin, M., Kruse, C. (eds) High Performance Computing. ISC High Performance 2023.\n> Lecture Notes in Computer Science, vol 13948. Springer, Cham.\n> [doi.org/10.1007/978-3-031-32041-5_6](https://doi.org/10.1007/978-3-031-32041-5_6)\n\n## Documentation\n\nCheck out the full documentation at [https://propulate.readthedocs.io/](https://propulate.readthedocs.io/) :rocket:! Here you can find installation\ninstructions, tutorials, theoretical background, and API references.\n\n**:point_right: If you have any questions or run into any challenges while using `Propulate`, don't hesitate to post an\n[issue](https://github.com/Helmholtz-AI-Energy/propulate/issues) :bookmark:, reach out via [GitHub\ndiscussions](https://github.com/Helmholtz-AI-Energy/propulate/discussions) :octocat:, or contact us directly via e-mail\n:email: to [propulate@lists.kit.edu](mailto:propulate@lists.kit.edu).**\n\n## Installation\n\n- You can install the **latest stable release** from PyPI: ``pip install propulate``\n- If you need the **latest updates**, you can also install ``Propulate`` directly from the master branch.\nPull and run ``pip install .``.\n- If you want to run the **tutorials**, you can install the required dependencies via: ``pip install .\"[tutorials]\"``\n- If you want to **contribute** to ``Propulate`` as a developer, you need to install the required dependencies with the package:\n``pip install -e .\"[dev]\"``.\n\n``Propulate`` depends on [``mpi4py``](https://mpi4py.readthedocs.io/en/stable/) and requires an MPI implementation under\nthe hood. Currently, it is only tested with [OpenMPI](https://www.open-mpi.org/).\n\n## Quickstart\n*Below, you can find a quick recipe for how to use `Propulate` in general. Check out the official\n[ReadTheDocs](https://propulate.readthedocs.io/en/latest/tut_propulator.html) documentation for more detailed tutorials\nand explanations.*\n\nLet's minimize the sphere function $f_\\text{sphere}\\left(x,y\\right)=x^2 +y^2$ with `Propulate` as a quick example. The\nminimum is at $\\left(x, y\\right)=\\left(0,0\\right)$ at the orange star.\n![](./docs/images/sphere.png)\nFirst, we need to define the key ingredients that define our optimization problem:\n- The **search space** of the parameters to be optimized as a `Python` dictionary. `Propulate` can handle three different\n  parameter types:\n    - A tuple of `float` for a continuous parameter, e.g., `{\"learning_rate\": (0.0001, 0.01)}`\n    - A tuple of `int` for an ordinal parameter, e.g., `{\"conv_layers\": (2, 10)}`\n    - A tuple of `str` for a categorical parameter, e.g., `{\"activation\": (\"relu\", \"sigmoid\", \"tanh\")}`\n\n  Thus, an exemplary search space might look like this:\n  ```python\n  search_space = {\n      \"learning_rate\": (0.0001, 0.01),  # Search a continuous space between 0.0001 and 0.01.\n      \"num_layers\": (2, 10),  # Search the integer space between 2 and 10 (inclusive).\n      \"activation\": (\"relu\", \"sigmoid\", \"tanh\"),  # Search the categorical space with the specified possibilities.\n  }\n  ```\n\n  The sphere function has two continuous parameters, $x$ and $y$, and we consider $x,y\\in\\left[-5.12,5.12\\right]$. The\n  search space in our example thus looks like this:\n  ```python\n  limits = {\n      \"x\": (-5.12, 5.12),\n      \"y\": (-5.12, 5.12)\n  }\n  ```\n- The **loss function**. This is the function we want to minimize in order to find the best parameters. It can be any\n  `Python` function that\n  - takes a set of parameters as a `Python` dictionary as an input.\n  - returns a scalar loss value that determines how good the tested parameter set is.\n\n  In this example, the loss function whose minimum we want to find is the sphere function:\n  ```python\n  def sphere(params: Dict[str, float]) -> float:\n    \"\"\"\n    Sphere function: continuous, convex, separable, differentiable, unimodal\n\n    Input domain: -5.12 <= x, y <= 5.12\n    Global minimum 0 at (x, y) = (0, 0)\n\n    Parameters\n    ----------\n    params: Dict[str, float]\n        The function parameters.\n\n    Returns\n    -------\n    float\n        The function value.\n    \"\"\"\n    return numpy.sum(numpy.array(list(params.values())) ** 2).item()\n  ```\nNext, we need to define the **evolutionary operator** or propagator that we want to use to breed new individuals during the\noptimization process. `Propulate` provides a reasonable default propagator via a utility function:\n```python\n# Set up logger for Propulate optimization.\npropulate.set_logger_config()\n# Set up separate random number generator for Propulate optimization. DO NOT USE SOMEWHERE ELSE!\nrng = random.Random(\n    <your-random-seed> + mpi4py.MPI.COMM_WORLD.rank\n)\n# Set up evolutionary operator.\npropagator = propulate.get_default_propagator(\n    pop_size=config.pop_size,  # The breeding population size\n    limits=limits,  # The search-space limits\n    rng=rng,  # Random number generator\n)\n```\nWe also need to set up the asynchronous parallel evolutionary **optimizer**, that is a so-called ``Propulator`` instance:\n```python\n# Set up Propulator performing actual optimization.\npropulator = propulate.Propulator(\n    loss_fn=sphere,\n    propagator=propagator,\n    rng=rng,\n    generations=config.generations,\n    checkpoint_path=config.checkpoint,\n)\n```\nNow we can run the actual optimization. Overall, ``generations * mpi4py.MPI.COMM_WORLD.size`` evaluations will be\nperformed:\n```python\n# Run optimization and print summary of results.\npropulator.propulate()\npropulator.summarize()\n```\nThe output should look something like this:\n```text\n#################################################\n# PROPULATE: Parallel Propagator of Populations #\n#################################################\n\n[2024-03-12 14:37:01,374][propulate.propulator][INFO] - No valid checkpoint file given. Initializing population randomly...\n[2024-03-12 14:37:01,374][propulate.propulator][INFO] - Island 0 has 4 workers.\n[2024-03-12 14:37:01,374][propulate.propulator][INFO] - Island 0 Worker 0: In generation 0...\n[2024-03-12 14:37:01,374][propulate.propulator][INFO] - Island 0 Worker 3: In generation 0...\n[2024-03-12 14:37:01,374][propulate.propulator][INFO] - Island 0 Worker 2: In generation 0...\n[2024-03-12 14:37:01,374][propulate.propulator][INFO] - Island 0 Worker 1: In generation 0...\n[2024-03-12 14:37:01,377][propulate.propulator][INFO] - Island 0 Worker 3: In generation 10...\n[2024-03-12 14:37:01,377][propulate.propulator][INFO] - Island 0 Worker 1: In generation 10...\n[2024-03-12 14:37:01,378][propulate.propulator][INFO] - Island 0 Worker 0: In generation 10...\n[2024-03-12 14:37:01,378][propulate.propulator][INFO] - Island 0 Worker 2: In generation 10...\n\n...\n[2024-03-12 14:37:02,197][propulate.propulator][INFO] - Island 0 Worker 1: In generation 960...\n[2024-03-12 14:37:02,206][propulate.propulator][INFO] - Island 0 Worker 2: In generation 990...\n[2024-03-12 14:37:02,206][propulate.propulator][INFO] - Island 0 Worker 1: In generation 970...\n[2024-03-12 14:37:02,215][propulate.propulator][INFO] - Island 0 Worker 1: In generation 980...\n[2024-03-12 14:37:02,224][propulate.propulator][INFO] - Island 0 Worker 1: In generation 990...\n[2024-03-12 14:37:02,232][propulate.propulator][INFO] - OPTIMIZATION DONE.\nNEXT: Final checks for incoming messages...\n[2024-03-12 14:37:02,244][propulate.propulator][INFO] - ###########\n# SUMMARY #\n###########\nNumber of currently active individuals is 4000.\nExpected overall number of evaluations is 4000.\n[2024-03-12 14:37:03,703][propulate.propulator][INFO] - Top 1 result(s) on island 0:\n(1): [{'a': '2.91E-3', 'b': '-3.05E-3'}, loss 1.78E-5, island 0, worker 0, generation 956]\n```\n### Let's get your hands dirty\nDo the following to run the [example script](https://github.com/Helmholtz-AI-Energy/propulate/blob/master/tutorials/propulator_example.py):\n\n- Make sure you have a working MPI installation on your machine.\n- If you have not already done this, create a fresh virtual environment with ``Python``: ``$ python3 -m venv best-venv-ever``\n- Activate it: ``$ source best-venv-ever/bin/activate``\n- Upgrade ``pip``: ``$ pip install --upgrade pip``\n- Install ``Propulate``: ``$ pip install propulate``\n- Run the example script ``propulator_example.py``: ``$ mpirun --use-hwthread-cpus python propulator_example.py``\n\n## Acknowledgments\n*This work is supported by the Helmholtz AI platform grant.*\n![](./.figs/hai_kit_logos.svg)\n\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools >= 61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools]\npackages = [\"propulate\", \"propulate.propagators\", \"propulate.utils\"]\n\n[project]\nname = \"propulate\"\nversion = \"1.2.2\"\nauthors = [\n  { name=\"Marie Weiel, Oskar Taubert, Helmholtz AI\", email=\"propulate@lists.kit.edu\" },\n]\ndescription = \"Massively parallel genetic optimization through asynchronous propagation of populations\"\nreadme = \"README.md\"\nrequires-python = \">=3.9\"\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: BSD License\",\n    \"Development Status :: 3 - Alpha\",\n]\ndependencies = [\n\t\"deepdiff\",\n    \"matplotlib ~= 3.8.4\",\n\t\"mpi4py\",\n\t\"numpy\",\n    \"colorlog\",\n    \"Gpy ~= 1.13.1\",\n    \"h5py\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pre-commit\",\n    \"coverage\",\n    \"ruff\",\n    \"mypy\",\n    \"pytest\",\n    \"pytest-cov\",\n    \"pytest-mpi\",\n    \"sphinx-autoapi\",\n    \"sphinx-rtd-theme\",\n    \"sphinxcontrib-napoleon\",\n    \"sphinxemoji\",\n    \"torch\",\n    \"torchvision\",\n    \"torchmetrics\",\n]\n\ntest = [\n    \"coverage\",\n    \"pytest\",\n    \"pytest-cov\",\n    \"pytest-mpi\",\n]\n\ntutorials = [\n    \"torch\",\n    \"torchvision\",\n    \"torchmetrics\",\n    \"lightning\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/Helmholtz-AI-Energy/propulate\"\nIssues = \"https://github.com/Helmholtz-AI-Energy/propulate/issues\"\n\n[tool.mypy]\npython_version = \"3.9\"\nmodules = [\"propulate\"]\ndisallow_untyped_defs = true\ndisallow_incomplete_defs = true\nfiles = [\n    \"tutorials/\"\n]\n\n\n[tool.ruff]\n# Exclude a variety of commonly ignored directories.\nexclude = [\n    \".bzr\",\n    \".direnv\",\n    \".eggs\",\n    \".git\",\n    \".git-rewrite\",\n    \".hg\",\n    \".ipynb_checkpoints\",\n    \".mypy_cache\",\n    \".nox\",\n    \".pants.d\",\n    \".pyenv\",\n    \".pytest_cache\",\n    \".pytype\",\n    \".ruff_cache\",\n    \".svn\",\n    \".tox\",\n    \".venv\",\n    \".vscode\",\n    \"__pypackages__\",\n    \"_build\",\n    \"buck-out\",\n    \"build\",\n    \"dist\",\n    \"node_modules\",\n    \"site-packages\",\n    \"venv\",\n]\n\n# Same as Black.\nline-length = 132\nindent-width = 4\n\n# Assume Python 3.9.\ntarget-version = \"py39\"\n\n[tool.ruff.lint]\n# Enable Pyflakes (`F`) and a subset of the pycodestyle (`E`)  codes by default.\n# Unlike Flake8, Ruff doesn't enable pycodestyle warnings (`W`) or\n# McCabe complexity (`C901`) by default.\nselect = [\"N\", \"E4\", \"E7\", \"E9\", \"F\", \"D\"]\nignore = [\"D100\", \"D104\", \"D404\"]\n# Enable import sorting\nextend-select = [\"I\"]\n\n# Allow fix for all enabled rules (when `--fix`) is provided.\nfixable = [\"ALL\"]\nunfixable = []\n\n# Allow unused variables when underscore-prefixed.\ndummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"\n\n[tool.ruff.format]\n# Like Black, use double quotes for strings.\nquote-style = \"double\"\n\n# Like Black, indent with spaces, rather than tabs.\nindent-style = \"space\"\n\n# Like Black, respect magic trailing commas.\nskip-magic-trailing-comma = false\n\n# Like Black, automatically detect the appropriate line ending.\nline-ending = \"auto\"\n\n# Enable auto-formatting of code examples in docstrings. Markdown,\n# reStructuredText code/literal blocks and doctests are all supported.\n#\n# This is currently disabled by default, but it is planned for this\n# to be opt-out in the future.\ndocstring-code-format = false\n\n# Set the line length limit used when formatting code snippets in\n# docstrings.\n#\n# This only has an effect when the `docstring-code-format` setting is\n# enabled.\ndocstring-code-line-length = \"dynamic\"\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"numpy\"\n\n[tool.pytest.ini_options]\ntestpaths = [\n    \"tests\",\n]\n\n[tool.coverage.run]\nparallel = true\nomit = [\"*/tests/*\"]\n\n[tool.coverage.report]\nomit = [\"*/tests/*\"]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pia",
            "repo_link": "https://github.com/hzi-braunschweig/pia-system",
            "content": {
                "codemeta": "",
                "readme": "# PIA-System\n\n![logo](psa.app.web/src/assets/images/pia_logo.png)\n\n[![code style: prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg?style=flat-square)](https://github.com/prettier/prettier)\n[![DOI](https://zenodo.org/badge/319654384.svg)](https://zenodo.org/badge/latestdoi/319654384)\n\n[**P**rospective Mon**i**toring and Management - **A**pp](https://info-pia.de/) (PIA).\n\nPIA facilitates the data acquisition in health studies and takes into account the wishes of the participants, the study center and the research institution and thereby also supports the long-term motivation of the participants.\n\nPIA consists of a web application as well as mobile apps for Android and iOS that enables the participants to conduct health studies and study management, as well it can be used as a symptom diary for contact tracing.\nThe main goals of this project are:\n\n- Simplify the data collection process\n- (Long-term) motivation of users through persuasive technology\n- Focus on usability and user centered design\n- Focus on data protection and security\n\n### Built with\n\nIn the backend PIA is composed of [Node.js](https://nodejs.org/) microservices that are using [PostgreSQL](https://www.postgresql.org/) as a database.\nThe microservices are containerized using [Docker](https://www.docker.com/) and deployed with [Kubernetes](https://kubernetes.io/).\nAs frontends an [Angular](https://angular.io/) web app and a [Ionic](https://ionicframework.com/) powered iOS and Android mobile app are provided.\n\n## Getting started\n\n### Local development\n\nTo set up PIA for local development, please follow the [development guide](./docs/development.md).\n\n### Deployment\n\nTo deploy PIA to a (production) Kubernetes cluster, please follow the [deployment guide](./docs/deployment.md).\n\n<!--\n## Roadmap\n*TODO*\n-->\n\n## Contributing\n\nAny contributions you make are **greatly appreciated**.\nPlease fork the [gitlab repository](https://gitlab.com/pia-eresearch-system/pia).\n\n1. Fork the [PIA GitLab repository](https://gitlab.com/pia-eresearch-system/pia)\n2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)\n3. Make sure your Changes are formatted using [prettier](https://github.com/prettier/prettier) (`npx prettier --write .`)\n4. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)\n5. Push to the Branch (`git push origin feature/AmazingFeature`)\n6. Open a Pull Request\n\n## Licence\n\nDistributed under the AGPL-3.0 license. See [LICENSE](./LICENSES/AGPL-3.0-or-later.txt) for more information.\n\n## Contact\n\n[PiaPost@helmholtz-hzi.de](mailto:PiaPost@helmholtz-hzi.de)\n\n![HZI](psa.app.web/src/assets/images/hzi_logo.jpg)\n\n",
                "dependencies": "{\n  \"name\": \"root\",\n  \"private\": true,\n  \"license\": \"AGPL-3.0-or-later\",\n  \"scripts\": {\n    \"start\": \"skaffold dev\",\n    \"update-packages\": \"ncu --packageFile 'psa.*/package.json' -i -x '/@types/node$|@types/node-fetch|node-fetch|typeorm|parse5|chalk|ngx-.*|ng-.*|rxjs|jspdf.*|.*angular.*/'\",\n    \"update-packagelock\": \"rm -rf psa.*/node_modules psa.*/package-lock.json && lerna exec \\\"npm install --ignore-scripts --package-lock-only --no-audit\\\"\",\n    \"update-openapi\": \"REBUILD_OPENAPI=1 ./psa.utils.scripts/openapi/generate-merge-config.sh && openapi-merge-cli\",\n    \"update-third-party-licenses\": \"docker build --target raw -f psa.utils.ci-thirdparty-license-collector/Dockerfile -o . .\",\n    \"format\": \"prettier -w .\"\n  },\n  \"devDependencies\": {\n    \"lerna\": \"^4.0.0\",\n    \"npm-check-updates\": \"^12.0.2\",\n    \"openapi-merge-cli\": \"^1.3.1\",\n    \"prettier\": \"^2.5.1\"\n  }\n}\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ptylab",
            "repo_link": "https://github.com/PtyLab/PtyLab.py",
            "content": {
                "codemeta": "",
                "readme": "# PtyLab.py\n![Python 3.9+](https://img.shields.io/badge/python-3.9+-green.svg)\n\nPtyLab is an inverse modeling toolbox for Conventional (CP) and Fourier (FP) ptychography in a unified framework. For more information please check the [paper](https://opg.optica.org/oe/fulltext.cfm?uri=oe-31-9-13763&id=529026).\n\n## Getting started\n\nThe simplest way to get started is to check the below demo in Google Colab.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PtyLab/PtyLab.py/blob/main/demo.ipynb)\n![demo](assets/recon.gif)\n\nTo explore more use cases of PtyLab, check the [example_scripts](example_scripts) and [jupyter_tutorials](jupyter_tutorials) directories. However, please install the package first as described in the below sections.\n\n## Installation\n\nTo install the package from source,\n\n```bash\npip install git+https://github.com/PtyLab/PtyLab.py.git\n```\n\nThis package uses `cupy` to utilize GPU for faster reconstruction. Please check their [instructions](https://docs.cupy.dev/en/stable/install.html) for installing this dependency.\n\n### Development\n \nPlease clone this repository and navigate to the root folder\n```bash\ngit clone git@github.com:PtyLab/PtyLab.py.git\ncd PtyLab.py\n```\n\nInside a virtual environment (preferably with conda), please install `ptylab` and its dependencies from the pinned versions specified under `requirements.txt`:\n```bash\nconda create --name ptylab_venv python=3.11.5 # or python version satisfying \">=3.9, <3.12\"\nconda activate ptylab_venv\npip install -e . -r requirements.txt\n```\n\nTo use the GPU, `cupy` can be additionally installed in this environment.\n\n```bash\nconda install -c conda-forge cupy\n```\n\nIf you would like to contribute to this package, especially if it involves modifying dependencies, please checkout the [`CONTRIBUTING.md`](CONTRIBUTING.md) file.\n\n## Citation\n\nIf you use this package in your work, cite us as below. \n\n```tex\n@article{Loetgering:23,\n        author = {Lars Loetgering and Mengqi Du and Dirk Boonzajer Flaes and Tomas Aidukas and Felix Wechsler and Daniel S. Penagos Molina and Max Rose and Antonios Pelekanidis and Wilhelm Eschen and J\\\"{u}rgen Hess and Thomas Wilhein and Rainer Heintzmann and Jan Rothhardt and Stefan Witte},\n        journal = {Opt. Express},\n        number = {9},\n        pages = {13763--13797},\n        publisher = {Optica Publishing Group},\n        title = {PtyLab.m/py/jl: a cross-platform, open-source inverse modeling toolbox for conventional and Fourier ptychography},\n        volume = {31},\n        month = {Apr},\n        year = {2023},\n        doi = {10.1364/OE.485370},\n}\n```\n\n\n",
                "dependencies": "[tool.poetry]\nname = \"ptylab\"\nversion = \"0.2.1\"\ndescription = \"A cross-platform, open-source inverse modeling toolbox for conventional and Fourier ptychography\"\nauthors = [\"Lars Loetgering <lars.loetgering@fulbrightmail.org>\", \"PtyLab Team\"]\nreadme = \"README.md\"\npackages = [\n    { include = \"PtyLab\" }\n]\n\n[tool.poetry.dependencies]\npython = \">=3.9, <3.13\"\nnumpy = \">=1.22, <2.0.0\"\nmatplotlib = \"^3.7.2\"\nh5py = \"^3.9.0\"\nscipy = \"^1.11.1\"\nscikit-image = \"^0.21.0\"\nscikit-learn = \"^1.3.0\"\ntqdm = \"^4.65.0\"\npyqtgraph = \"^0.13.3\"\ntables = \"^3.8.0\"\nbokeh = \"^3.2.1\"\ntensorflow = \"^2.14.0\"\nPyQt5 = \"^5.15.10\"\nPyQt5-Qt5 = \"*\"\nblack = { version = \"^23.7.0\", optional = true }\nipykernel = { version = \"^6.25.0\", optional = true }\npre-commit = { version = \"^4.0.1\", optional = true }\n\n[tool.poetry.extras]\ndev = [\"black\", \"ipykernel\", \"pre-commit\"]\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\nabsl-py==2.1.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nappnope==0.1.4 ; python_version >= \"3.9\" and python_version < \"3.13\" and platform_system == \"Darwin\"\nasttokens==2.4.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nastunparse==1.6.3 ; python_version >= \"3.9\" and python_version < \"3.13\"\nblack==23.12.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nblosc2==2.5.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nbokeh==3.4.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\ncertifi==2024.6.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\ncffi==1.17.0 ; python_version >= \"3.9\" and python_version < \"3.13\" and implementation_name == \"pypy\"\ncfgv==3.4.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\ncharset-normalizer==3.3.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\nclick==8.1.7 ; python_version >= \"3.9\" and python_version < \"3.13\"\ncolorama==0.4.6 ; python_version >= \"3.9\" and python_version < \"3.13\" and (platform_system == \"Windows\" or sys_platform == \"win32\")\ncomm==0.2.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\ncontourpy==1.2.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\ncycler==0.12.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\ndebugpy==1.8.5 ; python_version >= \"3.9\" and python_version < \"3.13\"\ndecorator==5.1.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\ndistlib==0.3.9 ; python_version >= \"3.9\" and python_version < \"3.13\"\nexceptiongroup==1.2.2 ; python_version >= \"3.9\" and python_version < \"3.11\"\nexecuting==2.0.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nfilelock==3.16.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nflatbuffers==24.3.25 ; python_version >= \"3.9\" and python_version < \"3.13\"\nfonttools==4.53.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\ngast==0.4.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\ngoogle-pasta==0.2.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\ngrpcio==1.64.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nh5py==3.11.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nidentify==2.6.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nidna==3.7 ; python_version >= \"3.9\" and python_version < \"3.13\"\nimageio==2.34.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nimportlib-metadata==7.1.0 ; python_version >= \"3.9\" and python_version < \"3.10\"\nimportlib-resources==6.4.0 ; python_version >= \"3.9\" and python_version < \"3.10\"\nipykernel==6.29.5 ; python_version >= \"3.9\" and python_version < \"3.13\"\nipython==8.18.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\njedi==0.19.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\njinja2==3.1.4 ; python_version >= \"3.9\" and python_version < \"3.13\"\njoblib==1.4.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\njupyter-client==8.6.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\njupyter-core==5.7.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\nkeras==3.3.3 ; python_version >= \"3.9\" and python_version < \"3.13\"\nkiwisolver==1.4.5 ; python_version >= \"3.9\" and python_version < \"3.13\"\nlazy-loader==0.4 ; python_version >= \"3.9\" and python_version < \"3.13\"\nlibclang==18.1.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nmarkdown-it-py==3.0.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nmarkdown==3.6 ; python_version >= \"3.9\" and python_version < \"3.13\"\nmarkupsafe==2.1.5 ; python_version >= \"3.9\" and python_version < \"3.13\"\nmatplotlib-inline==0.1.7 ; python_version >= \"3.9\" and python_version < \"3.13\"\nmatplotlib==3.9.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nmdurl==0.1.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\nml-dtypes==0.3.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\nmsgpack==1.0.8 ; python_version >= \"3.9\" and python_version < \"3.13\"\nmypy-extensions==1.0.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nnamex==0.0.8 ; python_version >= \"3.9\" and python_version < \"3.13\"\nndindex==1.8 ; python_version >= \"3.9\" and python_version < \"3.13\"\nnest-asyncio==1.6.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nnetworkx==3.2.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nnodeenv==1.9.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nnumexpr==2.10.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nnumpy==1.26.4 ; python_version >= \"3.9\" and python_version < \"3.13\"\nopt-einsum==3.3.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\noptree==0.11.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\npackaging==24.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\npandas==2.2.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\nparso==0.8.4 ; python_version >= \"3.9\" and python_version < \"3.13\"\npathspec==0.12.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\npexpect==4.9.0 ; python_version >= \"3.9\" and python_version < \"3.13\" and sys_platform != \"win32\"\npillow==10.3.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nplatformdirs==4.2.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\npre-commit==4.0.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nprompt-toolkit==3.0.47 ; python_version >= \"3.9\" and python_version < \"3.13\"\nprotobuf==4.25.3 ; python_version >= \"3.9\" and python_version < \"3.13\"\npsutil==6.0.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nptyprocess==0.7.0 ; python_version >= \"3.9\" and python_version < \"3.13\" and sys_platform != \"win32\"\npure-eval==0.2.3 ; python_version >= \"3.9\" and python_version < \"3.13\"\npy-cpuinfo==9.0.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\npycparser==2.22 ; python_version >= \"3.9\" and python_version < \"3.13\" and implementation_name == \"pypy\"\npygments==2.18.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\npyparsing==3.1.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\npyqt5-qt5==5.15.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\npyqt5-sip==12.13.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\npyqt5==5.15.10 ; python_version >= \"3.9\" and python_version < \"3.13\"\npyqtgraph==0.13.7 ; python_version >= \"3.9\" and python_version < \"3.13\"\npython-dateutil==2.9.0.post0 ; python_version >= \"3.9\" and python_version < \"3.13\"\npytz==2024.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\npywavelets==1.6.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\npywin32==306 ; sys_platform == \"win32\" and platform_python_implementation != \"PyPy\" and python_version >= \"3.9\" and python_version < \"3.13\"\npyyaml==6.0.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\npyzmq==26.1.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nrequests==2.32.3 ; python_version >= \"3.9\" and python_version < \"3.13\"\nrich==13.7.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nscikit-image==0.21.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nscikit-learn==1.5.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nscipy==1.13.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nsetuptools==70.0.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nsix==1.16.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nstack-data==0.6.3 ; python_version >= \"3.9\" and python_version < \"3.13\"\ntables==3.9.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\ntensorboard-data-server==0.7.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\ntensorboard==2.16.2 ; python_version >= \"3.9\" and python_version < \"3.13\"\ntensorflow-io-gcs-filesystem==0.37.0 ; python_version >= \"3.9\" and python_version < \"3.12\"\ntensorflow==2.16.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\ntermcolor==2.4.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nthreadpoolctl==3.5.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\ntifffile==2024.5.22 ; python_version >= \"3.9\" and python_version < \"3.13\"\ntomli==2.0.1 ; python_version >= \"3.9\" and python_version < \"3.11\"\ntornado==6.4 ; python_version >= \"3.9\" and python_version < \"3.13\"\ntqdm==4.66.4 ; python_version >= \"3.9\" and python_version < \"3.13\"\ntraitlets==5.14.3 ; python_version >= \"3.9\" and python_version < \"3.13\"\ntyping-extensions==4.5.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\ntzdata==2024.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nurllib3==2.2.1 ; python_version >= \"3.9\" and python_version < \"3.13\"\nvirtualenv==20.27.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nwcwidth==0.2.13 ; python_version >= \"3.9\" and python_version < \"3.13\"\nwerkzeug==3.0.3 ; python_version >= \"3.9\" and python_version < \"3.13\"\nwheel==0.43.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nwrapt==1.16.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nxyzservices==2024.4.0 ; python_version >= \"3.9\" and python_version < \"3.13\"\nzipp==3.19.2 ; python_version >= \"3.9\" and python_version < \"3.10\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pyapi-rts",
            "repo_link": "https://github.com/KIT-IAI/PyAPI-RTS",
            "content": {
                "codemeta": "",
                "readme": "# PyAPI-RTS\n\n**A Python library to read and manipulate RSCAD draft files.**\n\nSee <a href=\"examples/simple_example/simple_example.ipynb\">examples/simple_example/simple_example.ipynb</a> for a short preview of the API or take a look into our <a href=\"docs/pyapi_rts.pdf\">documentation</a>.\n\n## Installation\n\nTo install this project, perform the following steps:\n\n1. Clone the project\n2. `cd` into the cloned directory\n3. `pip install poetry`\n4. `poetry install`\n\n## Generate classes from RSCAD components\n\nBefore the first use of the project, the classes for the components in the RSCAD master library need to be generated.\n\n1. Copy the files from the `COMPONENTS` directory into `pyapi_rts/pyapi_rts/class_extractor/COMPONENTS`.\n\n2. Run `poetry run python ./pyapi_rts/class_extractor/main.py`\n\nOther options for the class generation:\n\n- \\-d: Set to delete the output folder before new classes are generated\n- \\-o: Set to include the OBSOLETE folder in the generation. Recommended if you use .dfx files converted from older versions\n- \\-p: Set path to COMPONENTS folder\n- \\-t: Set thread count used to parse the files. Default: 8 \n\n! The progress bar is not accurate due to optimizations applied during generation.\n\n## Run tests\n\n`poetry run pytest`\n\n## Citing\n\n> M. Weber, J. Enzinger, H. K. Çakmak, U. Kühnapfel and V. Hagenmeyer, \"PyAPI-RTS: A Python-API for RSCAD Modeling,\" 2023 Open Source Modelling and Simulation of Energy Systems (OSMSES), Aachen, Germany, 2023, pp. 1-7, doi: [10.1109/OSMSES58477.2023.10089671](https://doi.org/10.1109/OSMSES58477.2023.10089671).\n\n",
                "dependencies": "[tool.poetry]\nname = \"pyapi_rts\"\nversion = \"0.1.0\"\ndescription = \"\"\nauthors = [\"Moritz Weber <moritz.weber@kit.edu>\"]\nexclude = [\"pyapi_rts/class_extractor\"]\n\n[tool.poetry.dependencies]\npython = \">=3.10,<3.12\"\nnetworkx = \"^2.8\"\nprogress = \"^1.6\"\nlark = \"^1.1.2\"\nipykernel = \"^6.15.1\"\n\n[tool.poetry.dev-dependencies]\npytest = \"^7.2.0\"\npytest-xdist = \"^2.5.0\"\npytest-cov = \"^3.0.0\"\ncoverage = \"^6.3.3\"\nSphinx = \"^4.5.0\"\nsphinx-rtd-theme = \"^1.0.0\"\npytest-profiling = \"^1.7.0\"\nblack = {version = \"^24.3.0\", allow-prereleases = true}\nmatplotlib = \"^3.5.2\"\nnumpy = \"^1.23.1\"\nvprof = \"^0.38\"\nscipy = \"^1.10.0\"\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pycomlink",
            "repo_link": "https://github.com/pycomlink/pycomlink",
            "content": {
                "codemeta": "",
                "readme": "[![CI](https://github.com/pycomlink/pycomlink/actions/workflows/main.yml/badge.svg?branch=master)](https://github.com/pycomlink/pycomlink/actions/workflows/main.yml)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/pycomlink/pycomlink/master)\n[![Documentation Status](https://readthedocs.org/projects/pycomlink/badge/?version=latest)](https://pycomlink.readthedocs.io/en/latest/)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4810169.svg)](https://doi.org/10.5281/zenodo.4810169)\n\nAnaconda Version [![Anaconda Version](https://anaconda.org/conda-forge/pycomlink/badges/version.svg)](https://anaconda.org/conda-forge/pycomlink) [![Anaconda-Server Badge](https://anaconda.org/conda-forge/pycomlink/badges/latest_release_date.svg)](https://anaconda.org/conda-forge/pycomlink)\n\npycomlink\n=========\n\nA python toolbox for deriving rainfall information from commercial microwave link (CML) data.\n\nInstallation\n------------\n\n`pycomlink` is tested with Python 3.9, 3.10 and 3.11. There have been problems with Python 3.8, see https://github.com/pycomlink/pycomlink/pull/120. Many things might work with older version, but there is no support for this.\n\nIt can be installed via [`conda-forge`](https://conda-forge.org/):\n\n    $ conda install -c conda-forge pycomlink\n\nIf you are new to `conda` or if you are unsure, it is recommended to [create a new conda environment, activate it](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands), [add the conda-forge channel](https://conda-forge.org/) and then install.\n\nInstallation via `pip` is also possible:\n\n    $ pip install pycomlink\n\nAt the time of writing, with `pycomlink v0.4.0` which dropped `tensorflow` as dependency, `pip` install works fine. But, if we add new dependencies in the future, we might again run into issues with `pip` install.\n\nTo run the example notebooks you will also need the [Jupyter Notebook](https://jupyter.org/)\nand `ipython`, both also available via `conda` or `pip`.\n\nIf you want to clone the repository for developing purposes follow these steps (installation of Jupyter Notebook included):\n\n    $ git clone https://github.com/pycomlink/pycomlink.git\n    $ cd pycomlink\n    $ conda env create -f environment_dev.yml\n    $ conda activate pycomlink-dev\n    $ cd ..\n    $ pip install -e pycomlink\n\nUsage\n-----\n\nThe following jupyter notebooks showcase some use cases of `pycomlink`\n\n * [Basic example CML processing workflow](http://nbviewer.jupyter.org/github/pycomlink/pycomlink/blob/master/notebooks/Basic%20CML%20processing%20workflow.ipynb)\n * [Compare interpolation methods](https://nbviewer.org/github/pycomlink/pycomlink/blob/master/notebooks/Compare%20interpolation%20methods.ipynb)\n * [Get radar data along CML paths](https://nbviewer.org/github/pycomlink/pycomlink/blob/master/notebooks/Get%20radar%20rainfall%20along%20CML%20paths.ipynb)\n * [Nearby-link approach for rain event detection from RAINLINK](https://nbviewer.org/github/pycomlink/pycomlink/blob/master/notebooks/Nearby%20link%20approach%20processing%20example.ipynb)\n * [Compare different WAA methods](https://nbviewer.org/github/pycomlink/pycomlink/blob/master/notebooks/Wet%20antenna%20attenuation.ipynb)\n * [Detect data gaps stemming from heavy rainfall events that cause a loss of connection along a CML](https://nbviewer.org/github/pycomlink/pycomlink/blob/master/notebooks/Blackout%20gap%20detection%20examples.ipynb)\n\nNote that the links point to static versions of the example notebooks. You can run all these notebook online via mybinder if you click on the \"launch binder\" buttom at the top.\n\nFeatures\n--------\n\n * Perform all required CML data processing steps to derive rainfall information from raw signal levels:\n    * data sanity checks\n    * ~~anomaly detection~~ (removed because using outdated `tensorflow` code)\n    * wet/dry classification\n    * baseline calculation\n    * wet antenna correction\n    * transformation from attenuation to rain rate\n * Generate rainfall maps from the data of a CML network\n * Validate you results against gridded rainfall data or rain gauges networks\n \nDocumentation\n-------------\nThe documentation is hosted by readthedocs.org: [https://pycomlink.readthedocs.io/en/latest/](https://pycomlink.readthedocs.io/en/latest/)\n\n",
                "dependencies": "numpy\nscipy\npandas\nmatplotlib\nnumba\nh5py\nxarray\nsparse\nnetcdf4\nshapely\npyproj\ntqdm\npykrige\nscikit-learn\nfuture\nbottleneck\npoligrain\n\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Dec  2 13:20:35 2014\n\n@author: chwala-c\n\"\"\"\n\nimport os\nfrom setuptools import setup, find_packages\n\nwith open(\"requirements.txt\", \"r\") as f:\n    INSTALL_REQUIRES = [rq for rq in f.read().split(\"\\n\") if rq != \"\"]\n\n# Utility function to read the README file.\n# Used for the long_description.  It's nice, because now 1) we have a top level\n# README file and 2) it's easier to type in the README file than to put a raw\n# string in below ...\ndef read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname)).read()\n\nsetup(\n    name = \"pycomlink\",\n    version = \"0.4.1\",\n    author = \"Christian Chwala\",\n    author_email = \"christian.chwala@kit.edu\",\n    description = (\"Python tools for CML (commercial microwave link) data processing\"),\n    license = \"BSD\",\n    keywords = \"microwave links precipitation radar cml\",\n    url = \"https://github.com/pycomlink/pycomlink\",\n    download_url = (\n        \"https://github.com/pycomlink/pycomlink/archive/0.4.1.tar.gz\"),\n    packages=find_packages(exclude=['test']),\n    include_package_data=True,\n    long_description=read('README.md'),\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Topic :: Scientific/Engineering :: Atmospheric Science\",\n        \"License :: OSI Approved :: BSD License\",\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n    ],\n    # A list of all available classifiers can be found at \n    # https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    install_requires=INSTALL_REQUIRES,\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pygms",
            "repo_link": "https://github.com/cmeessen/pyGMS",
            "content": {
                "codemeta": "",
                "readme": "# pyGMS\n\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![DOI](https://zenodo.org/badge/194238991.svg)](https://zenodo.org/badge/latestdoi/194238991)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/50e5df33317949d58e8d7bf7c40a336b)](https://www.codacy.com?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=cmeessen/pyGMS&amp;utm_campaign=Badge_Grade)\n[![Codacy Badge](https://api.codacy.com/project/badge/Coverage/50e5df33317949d58e8d7bf7c40a336b)](https://www.codacy.com?utm_source=github.com&utm_medium=referral&utm_content=cmeessen/pyGMS&utm_campaign=Badge_Coverage)\n\n`pyGMS` is a Python 3 module designed to analyse the rheological behaviour of\nlithosphere-scale 3D structural models that were created with the\n[GeoModellingSystem](https://www.gfz-potsdam.de/en/section/basin-modeling/infrastructure/gms/)\n(GMS, GFZ Potsdam). `pyGMS` was originally written for the\npurpose of plotting yield strength envelope cross sections for my PhD thesis.\n\n## Installation\n\nThis is a short version of the installation instructions. For a more detailed\nversion visit the\n[documentation](https://cmeessen.github.io/pyGMS/installation.html).\n\n```bash\n# Clone the repository\ngit clone git@github.com:cmeessen/pyGMS.git\n\n# Create an Anaconda environment\ncd pyGMS\nconda env create -f environment.yml\n\n# Install with pip\nconda activate pygms\npip install -e .\n\n# Install some dependencies to be able to see the kernel in Jupyter notebooks\nconda install -c conda-forge nb_conda_kernels\n```\n\n## Documentation\n\nPlease have a look at the\n[documentation](https://cmeessen.github.io/pyGMS/index.html) for information\non how to install and use pyGMS.\n\n## Contributing\n\nIf you find bugs, have a feature wish or a pull request, please open an\n[issue](https://github.com/cmeessen/pyGMS/issues).\n\n### Preparing a pull request\n\nBefore preparing a pull request make sure to\n\n- comment the code\n- update CHANGELOG\n- check code style (`make pycodestyle`)\n- add a test if the contribution adds a new feature or fixes a bug\n- update the documentation (`cd docs/sphinx && make html && make gh-pages`)\n- run `make coverage` (maintainers only)\n\n",
                "dependencies": "from setuptools import setup\nfrom setuptools import find_packages\nfrom pkg_resources import resource_filename\nfrom pyGMS import __version__\n\n# METADATA\nNAME = 'pygms-cmeessen'\nMODULE = 'pyGMS'\nVERSION = __version__\nAUTHOR = 'Christian Meeßen'\nAUTHOR_EMAIL = 'christian.meessen@gfz-potsdam.de'\nMAINTAINER = 'Christian Meeßen'\nMAINTAINER_EMAIL = 'christian.meessen@gfz-potsdam.de'\nURL = 'https://github.com/cmeessen/pyGMS'\nDESCRIPTION = 'A Python module to analyse models created with the GeoModellingSystem'\ntry:\n    with open(resource_filename(MODULE, '../README.md'), 'r') as fh:\n        LONG_DESCRIPTION = fh.read()\nexcept ImportError:\n    with open('README.md') as fh:\n        LONG_DESCRIPTION = fh.read()\nLONG_DESCRIPTION_TYPE = 'text/markdown'\nPACKAGE_DATA = find_packages()\nCLASSIFIERS = [\n    'Natural Language :: English',\n    'Programming Language :: Python :: 3',\n    'Programming Language :: Python :: 3.7',\n    'License :: OSI Approved :: GNU GPL-3.0',\n    'Operating System :: OS Independent',\n    'Topic :: Geophysics',\n]\n\n# DEPENDENCIES\nINSTALL_REQUIRES = [\n    'numpy',\n    'matplotlib',\n    'pandas',\n    'scipy'\n]\n\nif __name__ == '__main__':\n    setup(\n        name=NAME,\n        version=VERSION,\n        author=AUTHOR,\n        author_email=AUTHOR_EMAIL,\n        maintainer=MAINTAINER,\n        maintainer_email=MAINTAINER_EMAIL,\n        description=DESCRIPTION,\n        long_description=LONG_DESCRIPTION,\n        long_description_content_type=LONG_DESCRIPTION_TYPE,\n        url=URL,\n        packages=PACKAGE_DATA,\n        classifiers=CLASSIFIERS,\n        install_requires=INSTALL_REQUIRES,\n    )\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pyquickmaps",
            "repo_link": "https://git.geomar.de/open-source/pyquickmaps",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pysdc",
            "repo_link": "https://github.com/Parallel-in-Time/pySDC",
            "content": {
                "codemeta": "",
                "readme": "[![badge-ga](https://github.com/Parallel-in-Time/pySDC/actions/workflows/ci_pipeline.yml/badge.svg?branch=master)](https://github.com/Parallel-in-Time/pySDC/actions/workflows/ci_pipeline.yml)\n[![badge-ossf](https://bestpractices.coreinfrastructure.org/projects/6909/badge)](https://bestpractices.coreinfrastructure.org/projects/6909)\n[![badge-cc](https://codecov.io/gh/Parallel-in-Time/pySDC/branch/master/graph/badge.svg?token=hpP18dmtgS)](https://codecov.io/gh/Parallel-in-Time/pySDC)\n[![zenodo](https://zenodo.org/badge/26165004.svg)](https://zenodo.org/badge/latestdoi/26165004)\n[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu)\n[![SQAaaS badge shields.io](https://img.shields.io/badge/sqaaas%20software-silver-lightgrey)](https://api.eu.badgr.io/public/assertions/aS8J0NDTTjCyYP6iVufviQ \"SQAaaS silver badge achieved\")\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/pySDC?logo=pypi)](https://pypistats.org/packages/pysdc)\n\n# Welcome to pySDC!\n\nThe `pySDC` project is a Python implementation of the\nspectral deferred correction (SDC) approach and its flavors, esp. the\nmultilevel extension MLSDC and PFASST. It is intended for rapid\nprototyping and educational purposes. New ideas like e.g. sweepers or\npredictors can be tested and first toy problems can be easily\nimplemented.\n\n## Features\n\n-   Variants of SDC: explicit, implicit, IMEX, multi-implicit, Verlet,\n    multi-level, diagonal, multi-step\n-   Variants of PFASST: virtual parallel or MPI-based parallel,\n    classical of multigrid perspective\n-   8 tutorials: from setting up a first collocation problem to SDC,\n    PFASST and advanced topics\n-   Projects: many documented projects with defined and tested outcomes\n-   Many different examples, collocation types, data types already\n    implemented\n-   Works with [FEniCS](https://fenicsproject.org/),\n    [mpi4py-fft](https://mpi4py-fft.readthedocs.io/en/latest/) and\n    [PETSc](http://www.mcs.anl.gov/petsc/) (through\n    [petsc4py](https://bitbucket.org/petsc/petsc4py))\n-   Continuous integration via [GitHub\n    Actions](https://github.com/Parallel-in-Time/pySDC/actions) and\n    [Gitlab CI](https://gitlab.hzdr.de/r.speck/pysdc/-/pipelines) (through the [GitHub2Gitlab Action](https://github.com/jakob-fritz/github2lab_action))\n-   Fully compatible with Python 3.9 - 3.12, runs at least on Ubuntu\n\n## Getting started\n\nThe code is hosted on GitHub, see\n<https://github.com/Parallel-in-Time/pySDC>, and PyPI, see\n<https://pypi.python.org/pypi/pySDC>. While using `pip install pySDC`\nwill give you a core version of `pySDC` to work with,\nworking with the developer version is most often the better choice. We\nthus recommend to checkout the code from GitHub and install the\ndependencies e.g. by using a [conda](https://conda.io/en/latest/)\nenvironment. For this, `pySDC` ships with environment files\nwhich can be found in the folder `etc/`. Use these as e.g.\n\n``` bash\nconda env create -f etc/environment-base.yml\n```\n\nIf you want to install the developer version using `pip` directly from the GitHub repository, use this:\n\n```\n# optionally use venv\npython3 -m venv name_of_pySDC_env\n. ./name_of_pySDC_env/bin/activate\n# drop @5.5.0 if you want to install the develop version\npip install git+https://github.com/Parallel-in-Time/pySDC@5.5.0\n```\n\nTo check your installation, run\n\n``` bash\npytest pySDC/tests -m NAME\n```\n\nwhere `NAME` corresponds to the environment you chose (`base` in the\nexample above). You may need to update your `PYTHONPATH` by running\n\n``` bash\nexport PYTHONPATH=$PYTHONPATH:/path/to/pySDC/root/folder\n```\n\nin particular if you want to run any of the playgrounds, projects or\ntutorials. All `import` statements there assume that the\n`pySDC`\\'s base directory is part of `PYTHONPATH`.\n\nFor many examples, `LaTeX` is used for the plots, i.e. a\ndecent installation of this is needed in order to run those examples.\nWhen using `fenics` or `petsc4py`, a C++\ncompiler is required (although installation may go through at first).\n\nFor more details on `pySDC`, check out http://www.parallel-in-time.org/pySDC.\n\n## How to cite\n\nIf you use pySDC or parts of it for your work, great! Let us know if we\ncan help you with this. Also, we would greatly appreciate a citation of\n[this paper](https://doi.org/10.1145/3310410):\n\n> Robert Speck, **Algorithm 997: pySDC - Prototyping Spectral Deferred\n> Corrections**, ACM Transactions on Mathematical Software (TOMS),\n> Volume 45 Issue 3, August 2019, <https://doi.org/10.1145/3310410>\n\nThe current software release can be cited using Zenodo:\n[![zenodo](https://zenodo.org/badge/26165004.svg)](https://zenodo.org/badge/latestdoi/26165004)\n\n## Contributing\n\n`pySDC` code was originally developed by [Robert Speck (@pancetta)](https://github.com/pancetta),\nand is now maintained and developed by a small community of scientists interested in SDC methods.\nCheckout the [Changelog](./CHANGELOG.md) to see pySDC's evolution since 2016. It has a\nsoftware management plan (SWP), too, see [here](https://smw.dsw.elixir-europe.org/wizard/projects/c3dda921-b7b0-4f4d-b5dc-778b9780552d).\n\nAny contribution is dearly welcome! If you want to contribute, please take the time to read our [Contribution Guidelines](./CONTRIBUTING.md)\n(and don't forget to take a peek at our nice [Code of Conduct](./CODE_OF_CONDUCT.md) :wink:).\n\n## Acknowledgements\n\nThis project has received funding from the [European High-Performance\nComputing Joint Undertaking](https://eurohpc-ju.europa.eu/) (JU) under\ngrant agreement No 955701 ([TIME-X](https://www.time-x-eurohpc.eu/))\nand grant agreement No 101118139.\nThe JU receives support from the European Union's Horizon 2020 research\nand innovation programme and Belgium, France, Germany, and Switzerland.\nThis project also received funding from the [German Federal Ministry of\nEducation and Research](https://www.bmbf.de/bmbf/en/home/home_node.html)\n(BMBF) grants  16HPC047 and 16ME0679K. Supported by the European Union - NextGenerationEU. \nThe project also received help from the [Helmholtz Platform for Research Software Engineering - Preparatory Study (HiRSE_PS)](https://www.helmholtz-hirse.de/).\n\n<p align=\"center\">\n  <img src=\"./docs/img/EuroHPC.jpg\" height=\"105\"/> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n  <img src=\"./docs/img/LogoTime-X.png\" height=\"105\" /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n  <img src=\"./docs/img/BMBF_gefoerdert_2017_en.jpg\" height=\"105\" />\n</p>\n\n",
                "dependencies": "[build-system]\nrequires = [\"flit_core >=3.2,<4\"]\nbuild-backend = \"flit_core.buildapi\"\n\n[project]\nname = 'pySDC'\nversion = '5.5.2'\ndescription = 'A Python implementation of spectral deferred correction methods and the likes'\nlicense = {text = \"BSD-2-Clause\"}\nreadme = 'README.md'\nauthors=[\n    {name='Robert Speck', email='r.speck@fz-juelich.de'},\n    {name='Thibaut Lunet', email='thibaut.lunet@tuhh.de'},\n    {name='Thomas Baumann', email='t.baumann@fz-juelich.de'},\n    {name='Lisa Wimmer', email='wimmer@uni-wuppertal.de'},\n    {name='Ikrom Akramov', email='ikrom.akramov@tuhh.de'},\n    {name='Giacomo Rosilho De Souza', email='giacomo.rosilhodesouza@usi.ch'},\n    {name='Jakob Fritz', email='j.fritz@fz-juelich.de'},\n    ]\n\n\nclassifiers = [\n    \"Topic :: Scientific/Engineering :: Mathematics\",\n]\n\ndependencies = [\n    'numpy>=1.15.4',\n    'scipy>=0.17.1',\n    'matplotlib>=3.0',\n    'sympy>=1.0',\n    'numba>=0.35',\n    'dill>=0.2.6',\n    'qmat>=0.1.8',\n    ]\n\n[project.urls]\nHomepage = \"http://www.parallel-in-time.org/pySDC/\"\nRepository = \"https://github.com/Parallel-in-Time/pySDC/\"\nDocumentation = \"http://www.parallel-in-time.org/pySDC/\"\nTracker = \"https://github.com/Parallel-in-Time/pySDC/issues\"\n\n[project.optional-dependencies]\napps = [\n    'petsc4py>=3.10.0',\n    'mpi4py>=3.0.0',\n    'fenics>=2019.1.0',\n    'mpi4py-fft>=2.0.2'\n    ]\ndev = [\n    'ruff',\n    'pytest',\n    'pytest-cov',\n    'sphinx'\n    ]\n\n[tool.pytest.ini_options]\nmarkers = [\n    'base: all basic tests',\n    'fenics: tests relying on FEniCS',\n    'slow: tests taking much longer than bearable',\n    'mpi4py: tests using MPI parallelism (but no other external library such as petsc)',\n    'petsc: tests relying on PETSc/petsc4py',\n    'benchmark: tests for benchmarking',\n    'cupy: tests for cupy on GPUs',\n    'libpressio: tests using the libpressio library',\n    'monodomain: tests the monodomain project, which requires previous compilation of c++ code',\n    'pytorch: tests for PyTorch related things in pySDC'\n    ]\ntimeout = 300\n\n[tool.ruff]\nline-length = 120\nextend-exclude = [\n    'playgrounds',\n    'tests',\n    '*/data/*'\n    ]\n\n[tool.ruff.lint]\nselect = [\"C\", \"E\", \"F\", \"W\", \"B\"]\nignore = [\"E203\", # Whitespace before punctuation\n          \"E741\", # Ambiguous variable name\n          \"E402\", # Module level import not at top of cell\n          \"W605\", # Invalid escape sequence\n          \"F401\", # unused import\n          \"B023\", # function uses loop variable\n          \"B028\", # No excplicit stackvariable\n          \"C408\", # Unnecessary collection call\n          \"C417\", # Unnecessary map usage\n          # Newly added\n          \"C901\", # Complex name\n          \"E501\", # Line length, as enforced by black, but black ignores comments\n          \"E721\" # Type comparison\n          ]\n# W504 is not supported by ruff and does not need to be excluded\n\n[tool.ruff.lint.per-file-ignores]\n\"pySDC/tutorial/step_6/C_MPI_parallelization.py\" = [\"F401\"]\n\"pySDC/projects/Hamiltonian/solar_system.py\" = [\"F401\"]\n\n[tool.black]\nline-length = 120\nskip-string-normalization = true\nexclude = '''playgrounds/'''\n\n[tool.coverage.run]\nomit = ['*/pySDC/tests/*', '*/data/*', '*/pySDC/playgrounds/*', '*/pySDC/projects/deprecated/*', '*/pySDC/projects/*/tests/*']\nrelative_files = true\nconcurrency = ['multiprocessing']\nsource = ['pySDC']\n\n[tool.coverage.report]\nskip_empty = true\n# Regexes for lines to exclude from consideration\nexclude_lines = [\n    # Have to re-enable the standard pragma\n    'pragma: no cover',\n\n    # Don't complain about missing debug-only code:\n    'def __repr__',\n    'if self\\.debug',\n\n    # Don't complain if tests don't hit defensive assertion code:\n    'raise',\n\n    # Don't complain if non-runnable code isn't run:\n    'if 0:',\n    'if __name__ == .__main__.:',\n    'pass',\n    '@abc.abstractmethod',\n    '__author__*',\n    ]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/python-icat",
            "repo_link": "https://github.com/icatproject/python-icat",
            "content": {
                "codemeta": "",
                "readme": "|doi| |rtd| |pypi|\n\n.. |doi| image:: https://zenodo.org/badge/37250056.svg\n   :target: https://zenodo.org/badge/latestdoi/37250056\n\n.. |rtd| image:: https://img.shields.io/readthedocs/python-icat/latest\n   :target: https://python-icat.readthedocs.io/en/latest/\n   :alt: Documentation build status\n\n.. |pypi| image:: https://img.shields.io/pypi/v/python-icat\n   :target: https://pypi.org/project/python-icat/\n   :alt: PyPI version\n\npython-icat – Python interface to ICAT and IDS\n==============================================\n\nThis package provides a collection of modules for writing Python\nprograms that access an `ICAT`_ service using the SOAP interface.  It\nis based on Suds and extends it with ICAT specific features.\n\nDownload\n--------\n\nThe latest release version can be found at the\n`release page on GitHub`__.\n\n.. __: `GitHub release`_\n\n\nDocumentation\n-------------\n\nSee the `online documentation`__.\n\nExample scripts can be found in doc/examples.  This is mostly an\nunsorted collection of test scripts that I initially wrote for myself\nto try things out.\n\nAlmost all scripts use example_data.yaml as input for test data.  Of\ncourse for real production, the input will come from different\nsources, out of some workflow from the site.  But this would be\ndynamic and site specific and thus not suitable, neither for testing\nnor for the inclusion into example scripts.  So its easier to have\njust one blob of dummy input data in one single file.  That is also\nthe reason why the example scripts require PyYAML.\n\n.. __: `Read the Docs site`_\n\n\nCopyright and License\n---------------------\n\nCopyright 2013–2024\nHelmholtz-Zentrum Berlin für Materialien und Energie GmbH\n\nLicensed under the `Apache License`_, Version 2.0 (the \"License\"); you\nmay not use this file except in compliance with the License.\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\nimplied.  See the License for the specific language governing\npermissions and limitations under the License.\n\n\n.. _ICAT: https://icatproject.org/\n.. _GitHub release: https://github.com/icatproject/python-icat/releases/latest\n.. _Read the Docs site: https://python-icat.readthedocs.io/\n.. _Apache License: https://www.apache.org/licenses/LICENSE-2.0\n\n",
                "dependencies": "\"\"\"Python interface to ICAT and IDS\n\nThis package provides a collection of modules for writing Python\nprograms that access an `ICAT`_ service using the SOAP interface.  It\nis based on Suds and extends it with ICAT specific features.\n\n.. _ICAT: https://icatproject.org/\n\"\"\"\n\nimport setuptools\nfrom setuptools import setup\nimport setuptools.command.build_py\nimport distutils.command.sdist\nimport distutils.dist\nfrom distutils import log\nfrom pathlib import Path\nimport string\ntry:\n    import distutils_pytest\n    cmdclass = distutils_pytest.cmdclass\nexcept (ImportError, AttributeError):\n    cmdclass = dict()\ntry:\n    import gitprops\n    release = str(gitprops.get_last_release())\n    version = str(gitprops.get_version())\nexcept (ImportError, LookupError):\n    try:\n        from _meta import release, version\n    except ImportError:\n        log.warn(\"warning: cannot determine version number\")\n        release = version = \"UNKNOWN\"\n\ndocstring = __doc__\n\n\n# Enforcing of PEP 625 has been added in setuptools 69.3.0.  We don't\n# want this, we want to keep control on the name of the sdist\n# ourselves.  Disable it.\ndef _fixed_get_fullname(self):\n    return \"%s-%s\" % (self.get_name(), self.get_version())\n\ndistutils.dist.DistributionMetadata.get_fullname = _fixed_get_fullname\n\n\nclass meta(setuptools.Command):\n\n    description = \"generate meta files\"\n    user_options = []\n    meta_template = '''\nrelease = \"%(release)s\"\nversion = \"%(version)s\"\n'''\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        version = self.distribution.get_version()\n        log.info(\"version: %s\", version)\n        values = {\n            'release': release,\n            'version': version,\n        }\n        with Path(\"_meta.py\").open(\"wt\") as f:\n            print(self.meta_template % values, file=f)\n\n\nclass build_test(setuptools.Command):\n    \"\"\"Copy all stuff needed for the tests (example scripts, test data)\n    into the test directory.\n    \"\"\"\n    description = \"set up test environment\"\n    user_options = []\n    def initialize_options(self):\n        pass\n    def finalize_options(self):\n        pass\n    def run(self):\n        self.copy_test_scripts()\n        self.copy_test_data()\n\n    def copy_test_scripts(self):\n        destdir = Path(\"tests\", \"scripts\")\n        self.mkpath(str(destdir))\n        scripts = []\n        scripts += Path(\"doc\", \"examples\").glob(\"*.py\")\n        scripts += (Path(s) for s in self.distribution.scripts)\n        for script in scripts:\n            dest = destdir / script.name\n            self.copy_file(str(script), str(dest), preserve_mode=False)\n\n    def copy_test_data(self):\n        destdir = Path(\"tests\", \"data\")\n        self.mkpath(str(destdir))\n        etc = Path(\"etc\")\n        doc = Path(\"doc\")\n        examples = doc / \"examples\"\n        files = []\n        files += ( examples / f\n                   for f in (\"example_data.yaml\",\n                             \"ingest-datafiles.xml\", \"ingest-ds-params.xml\",\n                             \"ingest-sample-ds.xml\") )\n        files += ( examples / (\"icatdump-%s.%s\" % (ver, ext))\n                   for ver in (\"4.4\", \"4.7\", \"4.10\", \"5.0\")\n                   for ext in (\"xml\", \"yaml\") )\n        files += doc.glob(\"icatdata-*.xsd\")\n        files += examples.glob(\"metadata-*.xml\")\n        files += ( etc / f\n                   for f in (\"ingest-10.xsd\", \"ingest-11.xsd\", \"ingest.xslt\") )\n        for f in files:\n            dest = destdir / f.name\n            self.copy_file(str(f), str(dest), preserve_mode=False)\n\n\n# Note: Do not use setuptools for making the source distribution,\n# rather use the good old distutils instead.\n# Rationale: https://rhodesmill.org/brandon/2009/eby-magic/\nclass sdist(distutils.command.sdist.sdist):\n    def run(self):\n        self.run_command('meta')\n        super().run()\n        subst = {\n            \"version\": self.distribution.get_version(),\n            \"url\": self.distribution.get_url(),\n            \"description\": docstring.split(\"\\n\")[0],\n            \"long_description\": docstring.split(\"\\n\", maxsplit=2)[2].strip(),\n        }\n        for spec in Path().glob(\"*.spec\"):\n            with spec.open('rt') as inf:\n                with Path(self.dist_dir, spec).open('wt') as outf:\n                    outf.write(string.Template(inf.read()).substitute(subst))\n\n\nclass build_py(setuptools.command.build_py.build_py):\n    def run(self):\n        self.run_command('meta')\n        super().run()\n        package = self.distribution.packages[0].split('.')\n        outfile = self.get_module_outfile(self.build_lib, package, \"_meta\")\n        self.copy_file(\"_meta.py\", outfile, preserve_mode=0)\n\n\n# There are several forks of the original suds package around, most of\n# them short-lived.  Two of them have been evaluated with python-icat\n# and found to work: suds-jurko and the more recent suds-community.\n# The latter has been renamed to suds.  We don't want to force to use\n# one particular suds clone.  Therefore, we first try if (any clone\n# of) suds is already installed and only add suds to install_requires\n# if not.\nrequires = [\"setuptools\", \"lxml\", \"packaging\"]\ntry:\n    import suds\nexcept ImportError:\n    requires.append(\"suds\")\n\nwith Path(\"README.rst\").open(\"rt\", encoding=\"utf8\") as f:\n    readme = f.read()\n\nsetup(\n    name = \"python-icat\",\n    version = version,\n    description = docstring.split(\"\\n\")[0],\n    long_description = readme,\n    long_description_content_type = \"text/x-rst\",\n    url = \"https://github.com/icatproject/python-icat\",\n    author = \"Rolf Krahl\",\n    author_email = \"rolf.krahl@helmholtz-berlin.de\",\n    license = \"Apache-2.0\",\n    classifiers = [\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3.4\",\n        \"Programming Language :: Python :: 3.5\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Programming Language :: Python :: 3.13\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n    ],\n    project_urls = dict(\n        Documentation=\"https://python-icat.readthedocs.io/\",\n        Source=\"https://github.com/icatproject/python-icat/\",\n        Download=(\"https://github.com/icatproject/python-icat/releases/%s/\"\n                  % release),\n        Changes=(\"https://python-icat.readthedocs.io/en/stable\"\n                 \"/changelog.html#changes-%s\" % release.replace('.', '-')),\n    ),\n    packages = [\"icat\"],\n    package_dir = {\"\": \"src\"},\n    python_requires = \">=3.4\",\n    install_requires = requires,\n    scripts = [\n        \"src/scripts/icatdump.py\",\n        \"src/scripts/icatingest.py\",\n        \"src/scripts/wipeicat.py\"\n    ],\n    cmdclass = dict(cmdclass,\n                    meta=meta,\n                    build_py=build_py,\n                    build_test=build_test,\n                    sdist=sdist),\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pyxmake",
            "repo_link": "https://gitlab.com/dlr-sy/pyxmake",
            "content": {
                "codemeta": "",
                "readme": "[![PyPi](https://img.shields.io/pypi/v/pyx-core?label=PyPi)](https://pypi.org/project/pyx-core)\n[![doi](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.13352143-red.svg)](https://zenodo.org/records/13352143)\n\n# PyXMake\n> This subpackage belongs to [PyXMake](https://gitlab.com/dlr-sy/pyxmake) and contains all core functionalities. It is installed automatically with the parent project. However, it is also separately available as a build system dependency. Please refer to the linked [repository](https://gitlab.com/dlr-sy/pyxmake) for documentation and application examples.\n\n## Downloading\nUse GIT to get the latest code base. From the command line, use\n```\ngit clone https://gitlab.dlr.de/dlr-sy/pyxmake pyxmake\n```\nIf you check out the repository for the first time, you have to initialize all submodule dependencies first. Execute the following from within the repository. \n```\ngit submodule update --init --recursive\n```\nTo update all refererenced submodules to the latest production level, use\n```\ngit submodule foreach --recursive 'git pull origin $(git config -f $toplevel/.gitmodules submodule.$name.branch || echo master)'\n```\n## Installation\nPyXMake can be installed from source using [poetry](https://python-poetry.org). If you don't have [poetry](https://python-poetry.org) installed, run\n```\npip install poetry --pre --upgrade\n```\nto install the latest version of [poetry](https://python-poetry.org) within your python environment. Use\n```\npoetry update\n```\nto update all dependencies in the lock file or directly execute\n```\npoetry install\n```\nto install all dependencies from the lock file. Last, you should be able to import PyXMake as a python package.\n```python\nimport PyXMake\n```\n## Contact\n* [Marc Garbade](mailto:marc.garbade@dlr.de)\n",
                "dependencies": "# TOML file for PyXMake as a build system.\n#  \n# @note: TOML file            \n# Created on 20.09.2021    \n# \n# @version:  1.0    \n# ----------------------------------------------------------------------------------------------\n# @requires:\n#        - \n# \n# @change: \n#        -    \n#    \n# @author: garb_ma                                                     [DLR-SY,STM Braunschweig]\n# ----------------------------------------------------------------------------------------------\n[build-system]\nrequires = [\"poetry-core>=1.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.poetry]\nname = \"pyx-core\"\nversion = \"0.0.0dev\"\ndescription = \"All core functionalities for using PyXMake as a build system\"\nauthors = [\"Garbade, Marc <marc.garbade@dlr.de>\"]\nlicense = \"MIT\"\nreadme = \"README.md\"\npackages = [{include=\"PyXMake\", from=\"src\"}]\ninclude = [\"src/PyXMake/*\"]\nexclude = [\"src/PyXMake/API/*.py\",\n           \"src/PyXMake/Plugin/*.py\",\n           \"src/PyXMake/**/*.git*\"]\nrepository = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake\"\ndocumentation = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake/-/blob/master/README.md\"\nkeywords = [\"compilation\", \"documentation\", \"packaging\"]\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Topic :: Software Development :: Build Tools\",\n    \"Topic :: Software Development :: Libraries :: Python Modules\",\n    \"Programming Language :: Python :: 2\",\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\"\n]\n\n[tool.poetry.urls]\nChangelog = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake/-/blob/master/CHANGELOG.md\"\n\n[[tool.poetry.source]]\nname = \"PyPI\"\npriority = \"primary\"\n\n[[tool.poetry.source]]\nname = \"dlr-sy\"\nurl = \"https://gitlab.dlr.de/api/v4/groups/541/-/packages/pypi/simple\"\npriority = \"supplemental\"\n\n[tool.poetry.dependencies]\npython = \"~2.7 || ^3.5\"\nautodocsumm =  [{version = \"^0.2.6\", python = \"^3.7\"}]\nbeautifulsoup4 = [{version = \"^4.8\",  python = \"~2.7 || ^3.5,<3.7\"},\n                  {version = \">=4.11\", python = \"^3.7\"}]\ncmake = [{version = \"^3.26,<3.27.8\", python = \"~2.7 || ^3.5,<3.7\"},\n         {version = \">=3.26\", python = \"^3.7\"}]\ncontextlib2 = [{version = \"^0.6\", python = \"~2.7 || ^3.5,<3.7\"},\n               {version = \">=21.6\", python = \"^3.7\"}]\ncryptography = [{version = \"^3.1\", python = \"~2.7 || ^3.5,<3.7\"},\n                {version = \">=38.0\", python = \"^3.7\"}]\ndistlib = [{version = \">=0.3.5\"}]\ndnspython = [{version = \">=2.2\", python = \"^3.7\"}]\nfastapi = [{version = \">=0.79\", python = \"^3.7\"}]\nfuture = [{version = \">=0.18\", python = \"~2.7 || ^3.5\"}]\ngit-filter-repo = [{version = \">=2.34\", python = \"^3.7\"}]\nGitPython = [{version = \"~2.1\", python = \"~2.7\"}, \n             {version = \"^3,<3.1.15\", python = \"~3.5\"},\n             {version = \"^3,<3.1.19\", python = \"~3.6\"},\n             {version = \">=3.1.24\", python = \"^3.7\"}]\nhttpx = [{version = \"^0.24\", python = \"~3.7\"},\n         {version = \">=0.26\", python = \">=3.8\"}]\nJinja2 = [{version = \"^3.0\",  python = \"~3.6\"},\n          {version = \">=3.1\", python = \"^3.7\"}]\nkeyring = [{version = \"~18.0\", python = \"~2.7\"},\n           {version = \">=19,<24.1\", python = \"^3.5,<3.7\"},\n           {version = \">=23.0\", python = \"^3.7\"}]\nmarkupsafe = [{version = \"^2.0\", python = \"~3.6\"},\n              {version = \">=2.1\", python = \"^3.7\"}]\nnumpy = [{version = \"^1.16\", python = \"~2.7 || ~3.5\"},\n         {version = \"^1.18\", python = \"~3.6\"},\n         {version = \"^1.21\", python = \"~3.7\"},\n         {version = \">=1.22\", python = \"^3.8\"}]\npackaging = [{version = \"^20.9\", python = \"~2.7 || ^3.5,<3.7\"},\n             {version = \">=21.0\", python = \"^3.7\"}]\nparamiko = [{version = \">=2.9\", python = \"~2.7 || ^3.5\"}]\nPillow = [{version = \"~6.2\", python = \"~2.7 || ^3.5,<3.7\"},\n          {version = \">=9.2\", python = \"^3.7\"}]\npipreqs = [{version = \">=0.4\", python = \"~2.7 || ^3.5\"}]\npsutil = [{version = \">=5.8\", python = \"~2.7 || ^3.6\"},\n          {version = \">=5.8\", python = \"~3.5\", markers = \"sys_platform != 'windows'\"},\n          {version = \">=5.7\", python = \"~3.5\", markers = \"sys_platform == 'windows'\"}]\npydantic = [{version = \">=1.8\", python = \"^3.7\"}]\npyinstaller = [{version = \">=5.3\", python = \"^3.7,<3.12\"}]\npytest = [{version = \">=7.1\", python = \"^3.7\"}]\npytest-cov = [{version = \">=3,<5.0\", python = \"^3.7\"}]\npython-dateutil = [{version = \">=2.8\", python = \"^3.7\"}]\npython-multipart = [{version = \">=0.0.5\", python = \"^3.7\"}]\npythonsed = [{version = \"*\", platform = \"win32\"}]\npywin32 = [{version = \"228\", python = \"~2.7 || ^3.5,<3.7\", platform = \"win32\"},\n           {version = \"^306\", python = \"^3.7\", platform = \"win32\"}]\nrapidfuzz = [{version = \">=2.15,<3.3\", python = \"~3.7\"},\n             {version = \">=3,<4.0\", python = \"^3.8\"}]\nrecommonmark = [{version = \">=0.7\", python = \"^3.7\"}]\nrequests = [{version = \"^2.25\", python = \"^3.5,<3.7\"}, \n            {version = \">=2.28\", python = \"^3.7\"}]\nrequests-toolbelt = [{version = \">=0.8,<10\", python = \"~2.7 || ^3.5,<3.7\"},\n                     {version = \">=0.10\", python = \"^3.7\"}]\nscipy = [{version = \"^1.2\",   python = \"~2.7 || ~3.5\"},\n         {version = \"^1.5\",   python = \"~3.6\"},\n         {version = \"^1.6\",   python = \"~3.7\"},\n         {version = \"~1.8\",   python = \"~3.8\"},\n         {version = \">=1.10\", python = \">=3.9,<3.13\"}]\nsix = [{version = \">=1.15\", python = \"~2.7 || ^3.5\"}]\nsphinx = [{version = \"^3.0\", python = \"~3.6\"},\n          {version = \">=5.1\", python = \"^3.7\"}]\nsphinx-rtd-theme = [{version = \">=1.0\", python = \"^3.7\"}]\nsphinxcontrib-bibtex = [{version = \"^1.0\", python = \"~3.6\"},\n                        {version = \">=2.5\", python = \"^3.7\"}]\nsphinxemoji = [{version = \">=0.2\", python = \"^3.7\"}]\nstarlette = [{version = \">=0.19\", python = \"^3.7\"}]\nstdlib-list = [{version = \">=0.8\", python = \"~2.7 || ^3.5\"}]\nsetuptools = [{version = \"^39.0\", python = \"~2.7\"},\n              {version = \"^49.0\", python = \"~3.5\"},\n              {version = \"^58.0\", python = \"~3.6\"},\n              {version = \"^64.0\", python = \"~3.7\"},\n              {version = \"^70,<74\", python = \"^3.8\"}]\nsvn = [{version = \">=1.0\", python = \"~2.7 || ^3.5\"}]\ntomlkit = [{version = \">=0.5,<1\", python = \"~2.7 || ^3.5,<3.7\"},\n           {version = \">=0.12\", python = \"^3.7\"}]\nuvicorn = [{version = \">=0.17\", python = \"^3.7\"}]\nwhichcraft = [{version = \">=0.6\", python = \"~2.7 || ^3.5\"}]\n\n# All optional dependencies\npyx-poetry = [{version = \">=1.18\", python = \"~2.7 || ^3.5,<3.7\", optional = true},\n              {version = \"*\", python = \"^3.7\", optional = true}]\npyc-core = [{version = \">=1.10\", python = \"~2.7 || ^3.5,<3.7\", optional = true},\n            {version = \"*\", python = \"^3.7\", optional = true}]\n\n# All mandatory development dependencies\n[tool.poetry.group.dev.dependencies]\npyx-poetry = [{git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake.git\", branch=\"pyx_poetry\", python=\"^3.7\"}]\npyc-core = [{git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyCODAC.git\", python=\"^3.7\"}]\n\n[tool.poetry.extras]\nall = [\"pyx-poetry\",\"pyc-core\"]\ncore = [\"pyx-poetry\"]\ndevel = [\"pyc-core\"]\n\n[tool.poetry.scripts]\npoetry-auto-cli = \"PyXMake.VTL:run\"\npyxmake = \"PyXMake.VTL:main\"\n\n[tool.pyxmake.archive]\nname = \"pyx_core-master.tar.gz\"\nsource = \"src/PyXMake\"\n\n[tool.pyxmake.doxygen]\nname = \"PyXMake\"\ntitle = [\"PyXMake\", \"PyXMake Developer Guide\"]\nformat = \"Python\"\nsource = \"src\"\noutput = \"doc/pyx_core\"\nfilter = [{\"contains\" = [\"doc\",\"bin\",\"config\"]}, {\"endswith\" = [\"make\",\"scratch\",\"examples\"]}]\n\n[tool.pyxmake.sphinx]\nname = \"PyXMake\"\nsource = \"doc\"\noutput = \"doc\"\nfile = \"documentation\"\ntheme = \"pydata_sphinx_theme\"\n\n[tool.pyxmake.coverage]\nname = \"PyXMake\"\nsource = \"src\"\nexclude = [\"src/PyXMake/Plugin\",\n           \"src/PyXMake/VTL\"]\ninclude = [\"example/pyx_api.py\",\n           \"example/pyx_app.py\",\n           \"example/pyx_cmake.py\",\n           \"example/pyx_archive.py\",\n           \"example/pyx_gfortran.py\",\n           \"example/pyx_py2x.py.py\",\n           \"example/pyx_pyreq.py\",\n           \"example/pyx_cxx.py\",\n           \"example/pyx_doxygen.py\",\n           \"example/pyx_latex.py\",\n           \"example/pyx_sphinx.py\",\n           \"example/pyx_openapi.py\",\n           \"example/pyx_bundle.py\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/quast",
            "repo_link": "https://github.com/ablab/quast/",
            "content": {
                "codemeta": "",
                "readme": "[![GitHub release (latest by date)](https://img.shields.io/github/v/release/ablab/quast)](https://github.com/ablab/quast/releases/)\n[![BioConda Install](https://img.shields.io/conda/dn/bioconda/quast.svg?style=flag&label=BioConda%20install)](https://anaconda.org/bioconda/quast)\n[![SourceForge Download QUAST](https://img.shields.io/sourceforge/dt/quast.svg?style=flag&label=SourceForge%20download)](https://sourceforge.net/projects/quast/files/latest/download)\n[![PyPI version](https://badge.fury.io/py/quast.svg)](https://badge.fury.io/py/quast)\n[![GitHub Downloads](https://img.shields.io/github/downloads/ablab/quast/total.svg?style=social&logo=github&label=Download)](https://github.com/ablab/quast/releases)\n[![License](https://img.shields.io/badge/licence-GPLv2-blue)](https://www.gnu.org/licenses/old-licenses/gpl-2.0)\n\n<img src=\"quast_libs/html_saver/static/img/quast_logo_black.png\" width=\"300\" title=\"QUAST\">\n\n### Genome assembly evaluation tool\n\nQUAST stands for QUality ASsessment Tool. It evaluates genome/metagenome assemblies by computing various metrics.\nThe current QUAST toolkit includes the general QUAST tool for genome assemblies, \nMetaQUAST, the extension for metagenomic datasets, \nQUAST-LG, the extension for large genomes (e.g., mammalians), and Icarus, the interactive visualizer for these tools.\n\nThe QUAST package works both with and without reference genomes. \nHowever, it is much more informative if at least a close reference genome is provided along with the assemblies.\nThe tool accepts multiple assemblies, thus is suitable for comparison.\n\nThis README file gives a brief introduction into installation, basic usage and parsing of output of QUAST. \nA much more detailed description of these and many other topics is available in the\n[online manual](http://quast.sf.net/manual.html). \nThere are also many more installation methods for the latest stable release of the QUAST toolkit, \nall of them are discussed [here](http://quast.sf.net/install.html). For the cutting-edge version, \nplease clone our [GitHub repo](https://github.com/ablab/quast).\n\nThe [Gurevich Lab](https://helmholtz-hips.de/en/hmsb) at the [Helmholtz Institute for Pharmaceutical Research Saarland (HIPS)](https://helmholtz-hips.de/en/) \ncurrently maintains and develops the tool. For copyright information and citation instructions, \nplease refer to [LICENSE.txt](LICENSE.txt). We warmly welcome external contributions to the QUAST project. \nIf you would like to contribute, please review our [Contributor Covenant](CODE_OF_CONDUCT.md).\n\n#### System requirements\n\nLinux 64-bit and macOS are supported.\n\nFor the main pipeline:\n- Python3 (3.3 or higher)\n- Perl 5.6.0 or higher\n- GCC 4.7 or higher\n- GNU make and ar\n- zlib development files\n\nFor the optional submodules:\n- Time::HiRes perl module for GeneMark-ES (needed when using `--gene-finding --eukaryote`)\n- Java 1.8 or later for GRIDSS (needed for the structural variation detection)\n- R for GRIDSS (needed for the structural variation detection)\n\nMost of those tools are usually preinstalled on Linux. MacOS, however, requires to install\nthe Command Line Tools for Xcode to make them available. \n\nQUAST draws plots in two formats: HTML and PDF. If you need the PDF versions, make sure that you have installed \n[Matplotlib](https://matplotlib.org/). We recommend to use Matplotlib version 1.1 or higher. QUAST is fully tested with Matplotlib v.1.3.1.\nInstallation on Ubuntu (tested on Ubuntu 20.04):\n\n    sudo apt-get update && sudo apt-get install -y pkg-config libfreetype6-dev libpng-dev python3-matplotlib\n\n#### Installation\n\nQUAST automatically compiles all its sub-parts when needed (on the first use). \nThus, installation is not required. However, if you want to precompile everything and add quast.py to your `PATH`, you may choose either:\n\nBasic installation (about 120 MB):\n\n    ./setup.py install\n\nFull installation (about 540 MB, includes (1) tools for SV detection based on read pairs, which is used for more precise misassembly detection, \n(2) and tools/data for reference genome detection in metagenomic datasets):\n\n    ./setup.py install_full\n\nThe default installation location is `/usr/local/bin/` for the executable scripts, and `/usr/local/lib/` for \nthe python modules and auxiliary files. If you are getting a permission error during the installation, consider running setup.py with\n`sudo`, or create a virtual python environment and [install into it](http://docs.python-guide.org/en/latest/dev/virtualenvs/). \nAlternatively, you may use old-style installation scripts (`./install.sh` or `./install_full.sh`), which build QUAST package inplace.\n\n#### Usage\n\n    ./quast.py test_data/contigs_1.fasta \\\n               test_data/contigs_2.fasta \\\n            -r test_data/reference.fasta.gz \\\n            -g test_data/genes.txt \\\n            -1 test_data/reads1.fastq.gz -2 test_data/reads2.fastq.gz \\\n            -o quast_test_output\n\n#### Output\n\n    report.txt      summary table\n    report.tsv      tab-separated version, for parsing, or for spreadsheets (Google Docs, Excel, etc)  \n    report.tex      Latex version\n    report.pdf      PDF version, includes all tables and plots for some statistics\n    report.html     everything in an interactive HTML file\n    icarus.html     Icarus main menu with links to interactive viewers\n    contigs_reports/        [only if a reference genome is provided]\n      misassemblies_report  detailed report on misassemblies\n      unaligned_report      detailed report on unaligned and partially unaligned contigs\n    k_mer_stats/            [only if --k-mer-stats is specified]\n      kmers_report          detailed report on k-mer-based metrics\n    reads_stats/            [only if reads are provided]\n      reads_report          detailed report on mapped reads statistics\n\nMetrics based only on contigs:\n\n* Number of large contigs (i.e., longer than 500 bp) and total length of them.  \n* Length of the largest contig.  \n* N50 (length of a contig, such that all the contigs of at least the same length together cover at least 50% of the assembly).\n* Number of predicted genes, discovered either by GeneMark.hmm (for prokaryotes), GeneMark-ES or GlimmerHMM (for eukaryotes), \nor MetaGeneMark (for metagenomes).\n\nWhen a reference is given:\n\n* Numbers of misassemblies of different kinds (inversions, relocations, translocations, interspecies translocations (metaQUAST only) or local).\n* Number and total length of unaligned contigs.  \n* Numbers of mismatches and indels, over the assembly and per 100 kb.  \n* Genome fraction %, assembled part of the reference.  \n* Duplication ratio, the total number of aligned bases in the assembly divided by the total number of those in the reference. \nIf the assembly contains many contigs that cover the same regions, its duplication ratio will significantly exceed 1. \nThis occurs due to multiple reasons, including overestimating repeat multiplicities and overlaps between contigs.  \n* Number of genes in the assembly, completely or partially covered, based on a user-provided list of gene positions in the reference.  \n* NGA50, a reference-aware version of N50 metric. It is calculated using aligned blocks instead of contigs. \nSuch blocks are obtained after removing unaligned regions, and then splitting contigs at misassembly breakpoints. \nThus, NGA50 is the length of a block, such that all the blocks of at least the same length together cover at least 50% of the reference.  \n\n\n#### Contact & Info \n\n* Support email: [alexey.gurevich@helmholtz-hips.de](alexey.gurevich@helmholtz-hips.de)\n* Issue tracker: [https://github.com/ablab/quast/issues](https://github.com/ablab/quast/issues)\n* Website: [http://quast.sf.net](http://quast.sf.net)\n* Latest news: [https://x.com/quast_bioinf](https://x.com/quast_bioinf)\n    \n\n",
                "dependencies": "#!/bin/bash\n\n############################################################################\n# Copyright (c) 2022-2024 Helmholtz Institute for Pharmaceutical Research Saarland (HIPS), HZI\n# Copyright (c) 2015-2022 Saint Petersburg State University\n# Copyright (c) 2011-2015 Saint Petersburg Academic University\n# All Rights Reserved\n# See file LICENSE for details.\n############################################################################\n\n# installs general QUAST pipeline and general MetaQUAST pipeline. \n# MetaQUAST for de novo datasets (without references) will NOT be installed.\n\nquast_home=$(dirname \"$0\")\n\nstdout_log_fname=$quast_home/install_log.stdout\necho \"Starting QUAST test... (stdout redirected to $stdout_log_fname)\"\necho \"\" > $stdout_log_fname\necho \"Starting QUAST test\" >> $stdout_log_fname\n$quast_home/quast.py --test >> $stdout_log_fname\nreturn_code=$?\nif [ $return_code -ne 0 ]; then\n   echo 'ERROR! QUAST TEST FAILED!'\n   exit 1\nfi\necho \"Starting MetaQUAST test... (stdout redirected to $stdout_log_fname)\"\necho \"\" >> $stdout_log_fname\necho \"Starting MetaQUAST test\" >> $stdout_log_fname\n$quast_home/metaquast.py --test >> $stdout_log_fname\nreturn_code=$?\nif [ $return_code -ne 0 ]; then\n   echo 'ERROR! METAQUAST TEST FAILED!'\n   exit 1\nfi\necho 'QUAST INSTALLED SUCCESSFULLY!'\necho 'You can install full version of QUAST with ./install_full.sh (see manual.html)'\nexit 0\n\n#!/usr/bin/env python\n\n############################################################################\n# Copyright (c) 2022-2024 Helmholtz Institute for Pharmaceutical Research Saarland (HIPS), HZI\n# Copyright (c) 2015-2022 Saint Petersburg State University\n# Copyright (c) 2011-2015 Saint Petersburg Academic University\n# All Rights Reserved\n# See file LICENSE for details.\n############################################################################\n\nfrom __future__ import with_statement\nimport os\nimport sys\nfrom glob import glob\nfrom os.path import join, isfile, abspath, dirname, isdir\nimport shutil\n\nfrom quast_libs import qconfig\nqconfig.check_python_version()\nfrom quast_libs import qutils\n\nfrom quast_libs.log import get_logger\nlogger = get_logger(qconfig.LOGGER_DEFAULT_NAME)\nlogger.set_up_console_handler(debug=True)\n\ntry:\n    from setuptools import setup, find_packages\nexcept:\n    logger.error('setuptools is not installed or outdated!\\n\\n'\n                 'You can install or update setuptools using\\n'\n                 'pip install --upgrade setuptools (if you have pip)\\n'\n                 'or\\n'\n                 'sudo apt-get install python-setuptools (on Ubuntu)\\n'\n                 '\\n'\n                 'You may also use old-style installation scripts: ./install.sh or ./install_full.sh',\n                 exit_with_code=1)\n\nfrom quast_libs.glimmer import compile_glimmer\nfrom quast_libs.run_busco import download_augustus, download_all_db\nfrom quast_libs.search_references_meta import download_blast_binaries, download_blastdb\nfrom quast_libs.ca_utils.misc import compile_aligner\nfrom quast_libs.ra_utils.misc import compile_reads_analyzer_tools, compile_bwa, compile_bedtools, download_gridss\n\nname = 'quast'\nquast_package = qconfig.PACKAGE_NAME\n\n\nargs = sys.argv[1:]\n\n\ndef cmd_in(cmds):\n    return any(c in args for c in cmds)\n\n\nif abspath(dirname(__file__)) != abspath(os.getcwd()):\n    logger.error('Please change to ' + dirname(__file__) + ' before running setup.py')\n    sys.exit()\n\n\nif cmd_in(['clean', 'sdist']):\n    logger.info('Cleaning up binary files...')\n    compile_aligner(logger, only_clean=True)\n    compile_glimmer(logger, only_clean=True)\n    compile_bwa(logger, only_clean=True)\n    compile_bedtools(logger, only_clean=True)\n    for fpath in [fn for fn in glob(join(quast_package, '*.pyc'))]: os.remove(fpath)\n    for fpath in [fn for fn in glob(join(quast_package, 'html_saver', '*.pyc'))]: os.remove(fpath)\n    for fpath in [fn for fn in glob(join(quast_package, 'site_packages', '*', '*.pyc'))]: os.remove(fpath)\n\n    if cmd_in(['clean']):\n        if isdir('build'):\n            shutil.rmtree('build')\n        if isdir('dist'):\n            shutil.rmtree('dist')\n        if isdir(name + '.egg-info'):\n            shutil.rmtree(name + '.egg-info')\n        download_gridss(logger, only_clean=True)\n        download_blast_binaries(logger, only_clean=True)\n        download_blastdb(logger, only_clean=True)\n        if qconfig.platform_name != 'macosx':\n            download_augustus(logger, only_clean=True)\n            download_all_db(logger, only_clean=True)\n        logger.info('Done.')\n        sys.exit()\n\n\nif cmd_in(['test']):\n    ret_code = os.system('quast.py --test')\n    sys.exit(ret_code)\n\n\ndef write_version_py():\n    version_py = os.path.join(os.path.dirname(__file__), quast_package, 'version.py')\n\n    with open('VERSION.txt') as f:\n        v = f.read().strip().split('\\n')[0]\n    try:\n        import subprocess\n        git_revision = subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'],\n                                               stderr=open(os.devnull, 'w')).rstrip()\n    except:\n        git_revision = ''\n        pass\n\n    if not isinstance(git_revision, str):\n        git_revision = git_revision.decode('utf-8')\n    with open(version_py, 'w') as f:\n        f.write((\n            '# Do not edit this file, pipeline versioning is governed by git tags\\n'+\n                    '__version__ = \\'' + v + '\\'\\n' +\n                    '__git_revision__ = \\'%s\\'' % git_revision))\n    return v\n\nversion = write_version_py()\n\n\nif cmd_in(['tag']):\n    cmdl = 'git tag -a %s -m \"Version %s\" && git push --tags' % (version, version)\n    os.system(cmdl)\n    sys.exit()\n\n\nif cmd_in(['publish']):  \n    # make sure you updated pip and installed twine `pip install -U pip setuptools twine`\n    cmdl = 'git clean -dfx && python setup.py sdist && twine upload dist/*'\n    os.system(cmdl)\n    sys.exit()\n\n\ndef find_package_files(dirpath, package=quast_package):\n    paths = []\n    for (path, dirs, fnames) in os.walk(join(package, dirpath)):\n        for fname in fnames:\n            paths.append(qutils.relpath(join(path, fname), package))\n    return paths\n\n\ninstall_full = False\nif cmd_in(['install_full']):\n    install_full = True\n    args2 = []\n    for a_ in args:\n        if a_ == 'install_full':\n            args2.append('install')\n        else:\n            args2.append(a_)\n    args = args2\n\nmodules_failed_to_install = []\nif cmd_in(['install', 'develop', 'build', 'build_ext']):\n    try:\n        import matplotlib\n    except ImportError:\n        try:\n            pip.main(['install', 'matplotlib'])\n        except:\n            logger.warning('Cannot install matplotlib. Static plots will not be drawn (however, HTML will be)')\n\n    logger.info('* Compiling aligner *')\n    if not compile_aligner(logger):\n        modules_failed_to_install.append('Contigs aligners for reference-based evaluation (affects -R and many other options)')\n    logger.info('* Compiling Glimmer *')\n    if not compile_glimmer(logger):\n        modules_failed_to_install.append('Glimmer gene-finding tool (affects --glimmer option)')\n    logger.info('* Compiling read analysis tools *')\n    if not compile_reads_analyzer_tools(logger):\n        modules_failed_to_install.append('Read analysis tools (affects -1/--reads1 and -2/--reads2 options)')\n    if install_full:\n        logger.info('* Downloading GRIDSS *')\n        if not download_gridss(logger):\n            modules_failed_to_install.append('GRIDSS (affects -1/--reads1 and -2/--reads2 options)')\n        logger.info('* Downloading BLAST *')\n        if not download_blast_binaries(logger):\n            modules_failed_to_install.append('BLAST (affects metaquast.py in without references mode and --find-conserved-genes option)')\n        logger.info('* Downloading SILVA 16S rRNA gene database *')\n        if not download_blastdb(logger):\n            modules_failed_to_install.append('SILVA 16S rRNA gene database (affects metaquast.py in without references mode)')\n        if qconfig.platform_name != 'macosx':\n            logger.info('* Downloading and compiling Augustus *')\n            if not download_augustus(logger):\n                modules_failed_to_install.append('Augustus (affects --find-conserved-genes option)')\n            logger.info('* Downloading BUSCO databases *')\n            if not download_all_db(logger):\n                modules_failed_to_install.append('BUSCO databases (affects --find-conserved-genes option)')\n        else:\n            logger.notice('* BUSCO dependecies will not be installed (unavailable in OS X) *')\n    logger.info('')\n\n\nif qconfig.platform_name == 'macosx':\n    sambamba_files = [join('sambamba', 'sambamba_osx')]\nelse:\n    sambamba_files = [join('sambamba', 'sambamba_linux')]\n\nminimap_files = find_package_files('minimap2')\nbwa_files = [\n    join('bwa', fp) for fp in os.listdir(join(quast_package, 'bwa'))\n    if isfile(join(quast_package, 'bwa', fp)) and fp.startswith('bwa')]\nbedtools_files = [join('bedtools', 'bin', '*')]\nfull_install_tools = (\n    find_package_files('gridss') +\n    find_package_files('blast') +\n    [join(quast_package, 'busco', 'hmmsearch')]\n)\n\nsetup(\n    name=name,\n    version=version,\n    author='Alexey Gurevich, Vladislav Saveliev, Alla Mikheenko, and others',\n    author_email='alexey.gurevich@helmholtz-hips.de',\n    description='Genome assembly evaluation tool',\n    long_description='''QUAST evaluates genome assemblies.\nIt works both with and without reference genomes.\nThe tool accepts multiple assemblies, thus is suitable for comparison.''',\n    keywords=['bioinformatics', 'genome assembly', 'metagenome assembly', 'visualization'],\n    url='http://quast.sf.net',\n    download_url='https://sourceforge.net/projects/quast/files',\n    platforms=['Linux', 'OS X'],\n    license='GPLv2',\n\n    packages=find_packages(),\n    package_data={\n        quast_package:\n            find_package_files('test_data', package='') +\n            [\n            'README.md',\n            'CHANGES.txt',\n            'VERSION.txt',\n            'LICENSE.txt',\n            'manual.html',\n            ] +\n            find_package_files('html_saver') +\n            minimap_files +\n            find_package_files('genemark/' + qconfig.platform_name) +\n            find_package_files('genemark-es/' + qconfig.platform_name) +\n            find_package_files('genemark-es/lib') +\n            find_package_files('glimmer') +\n            bwa_files +\n            bedtools_files +\n            sambamba_files +\n           (full_install_tools if install_full else [])\n    },\n    include_package_data=True,\n    zip_safe=False,\n    scripts=['quast.py', 'metaquast.py', 'icarus.py', 'quast-lg.py'],\n    install_requires=[\n        'joblib',\n        'simplejson',\n    ],\n    classifiers=[\n        'Environment :: Console',\n        'Environment :: Web Environment',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: GNU General Public License v2 (GPLv2)',\n        'Natural Language :: English',\n        'Operating System :: MacOS :: MacOS X',\n        'Operating System :: POSIX',\n        'Operating System :: Unix',\n        'Programming Language :: Python',\n        'Programming Language :: JavaScript',\n        'Topic :: Scientific/Engineering',\n        'Topic :: Scientific/Engineering :: Bio-Informatics',\n        'Topic :: Scientific/Engineering :: Visualization',\n    ],\n    script_args=args,\n)\n\n\nif cmd_in(['install']):\n    not_installed_message = ''\n    if modules_failed_to_install:\n        not_installed_message = 'WARNING: some modules were not installed properly and\\n' \\\n                                'QUAST functionality will be restricted!\\n' \\\n                                'The full list of malformed modules and affected options:\\n'\n        not_installed_message += \"\\n\".join(map(lambda x: \" * \" + x, modules_failed_to_install))\n        not_installed_message += \"\\n\\n\"\n\n    if not install_full:\n        logger.info('''\n----------------------------------------------\nQUAST version %s installation complete.\n\n%sPlease run  ./setup.py test   to verify installation.\n\nFor help in running QUAST, please see the documentation available\nat quast.sf.net/manual.html, or run quast.py --help\n\nUsage:\n$ quast.py test_data/contigs_1.fasta \\\\\n           test_data/contigs_2.fasta \\\\\n        -r test_data/reference.fasta.gz \\\\\n        -g test_data/genes.txt \\\\\n        -o quast_test_output\n----------------------------------------------''' % (str(version), not_installed_message))\n\n    else:\n        logger.info('''\n----------------------------------------------\nQUAST version %s installation complete.\n\nThe full package is installed, with the features for reference\nsequence detection in MetaQUAST, and structural variant detection\nfor misassembly events refinement.\n\n%sPlease run  ./setup.py test   to verify installation.\n\nFor help in running QUAST, please see the documentation available\nat quast.sf.net/manual.html, or run quast.py --help\n\nUsage:\n$ quast.py test_data/contigs_1.fasta \\\\\n           test_data/contigs_2.fasta \\\\\n        -r test_data/reference.fasta.gz \\\\\n        -g test_data/genes.txt \\\\\n        -1 test_data/reads1.fastq.gz -2 test_data/reads2.fastq.gz \\\\\n        -o quast_test_output\n----------------------------------------------''' % (str(version), not_installed_message))\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/radiative-forcing-of-hypersonic-aircraft-trajectories",
            "repo_link": "https://github.com/johannespletzer/rf-of-hypersonic-trajectories/",
            "content": {
                "codemeta": "",
                "readme": "# Radiative forcing of hypersonic aircraft emission inventories\nThe software quantifies climate impact of hypersonic aircraft emission inventories as a number and within seconds instead of very long numerical simulations that produce Petabytes of data. The input requires water vapor, hydrogen and nitrogen oxide emission data along flight trajectories.\nThe repository provides a Python package, examples and an executable to calculate the climate impact (stratosphere adjusted radiative forcing) of hypersonic aircraft emission inventories. The radiative forcing of water vapour changes and ozone changes are calculated on the basis of water vapour, hydrogen and nitrogen oxide emissions. The current version is able to read in mat- and nc-files. NetCDF read in is currently optimised for data published online, e.g. for the aircraft design [STRATOFLY-MR3](https://zenodo.org/records/10818082).\n\nLatest software release: [![DOI](https://zenodo.org/badge/518852238.svg)](https://zenodo.org/badge/latestdoi/518852238)\n\n# Limitations\nInterpolation (30-38 km) and extrapolation surface-30 km are used. It is recommended to note the following:\n- The atmospheric and radiative sensitivites are based on results from [Pletzer et al (2024)](https://acp.copernicus.org/articles/24/1743/2024/). The atmospheric composition of the numerical climate model is based on surface emission inventories for 2050. \n- The class includes a function (`drop_vertical_levels()`) that drops emission in the troposphere or below specified altitude levels and excludes it from the climate calculation. Its use is strongly recommended as long as sensitivities are not yet extended to altitudes below 30 km.\n- The climate impact of emission inventories where the average flight altitude does not correspond to the typical hypersonic flight altitudes (about 24-40 km) should not be estimated.\n- Meaningful results can be expected for the radiative effect of water vapour changes due to water vapour emissions. This explicitly excludes the radiative effect of water vapour changes due to hydrogen and nitrogen oxide emissions.\n- Meaningful results can be expected for the radiative effect of ozone changes due to water vapour, hydrogen and nitrogen oxide emissions.\n\nPlease keep these limitations in mind when using the software.\n\n# Python environment requirements\nThe software requires various functions from the following python modules:\n\n- numpy\n- pandas\n- xarray\n- scipy\n- xlsxwriter\n- netcdf4\n- aerocalc3\n\nInstall the required packages with `pip install numpy pandas xarray scipy xlsxwriter netcdf4 aerocalc3`.\n\n# Getting started\nThe repo contains two example notebooks for processing of emission inventories in mat- and nc-format. Otherwise, the user can run the main.py executable which reads all emission inventory files within the folder and returns the calculated radiative forcing in an xlsx file. Execute main.py with `python3 main.py <path_to_your_emission_files>`. Please contact Johannes Pletzer for any questions.\n\n# Code quality\nThe code was formatted according to PEP 8 style with the help of the modules 'flake8', 'isort', 'pylint' and 'black'.\n\n# Acknowledgements\nDaniel Bodmer contributed with validation of model results by offering current state of the art hypersonic aircraft emission inventories on trajectory and route network level: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10818082.svg)](https://doi.org/10.5281/zenodo.10818082)\n\n\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/radplanbio",
            "repo_link": "https://github.com/ddRPB/rpb",
            "content": {
                "codemeta": "",
                "readme": "# ddRPB\n\n## Radiotherapy clinical research IT infrastructure\n\nRadiationDosePlan- Image/Biomarker-Outcome-platform (RPB) is a collection of open source software systems integrated via portal to deliver a core software infrastructure necessary to support the operation of non-commercial trials unit.\n\nThe RPB platform is a web-based solution which supports collection and exchange of radiotherapy specific research data in large scale multi-centre clinical and pre-clinical studies. It delivers a study management and electronic data capture system with special extensions dedicated to secure upload of medical imaging and treatment plans in DICOM format. These tools allow the trial personnel to handle multi-modal data conduction activities that yield to finalised patient cohort datasets.\n\n## Platform features\n\n* Managing multi-centre clinical trials\n* Patient identity management and pseudonymisation\n* Study subject randomisation (permuted block + strata)\n* Electronic case report forms (eCRFs) for clinical data collection\n* Patient reported outcomes (ePRO)\n* Linked medical imaging (DICOM) and treatment plans (DICOM-RT)\n* DICOM data de-identification and RTSTRUCT ROI naming harmonisation\n* DICOM viewer with DICOM-RT plugin\n* WebDAV access to DICOM data collected in particular project\n* Storage for laboratory assays and other tabulated data\n\n## Acknowledgment\n\nFor people who do use the RPB software platform as managed or self-operated solution supporting the conduction of their trials, when it is possible, we would appreciate your acknowledgement in publications and citation of the relevant papers:\n\n* \"This work was conducted in part using the RadPlanBio software platform.\"           \n\n```\n@article{skripcak_toward_2016,\n    title = {Toward {Distributed} {Conduction} of {Large}-{Scale} {Studies} in {Radiation} {Therapy} and {Oncology}: {Open}-{Source} {System} {Integration} {Approach}},\n    volume = {20},\n    issn = {2168-2194, 2168-2208},\n    shorttitle = {Toward {Distributed} {Conduction} of {Large}-{Scale} {Studies} in {Radiation} {Therapy} and {Oncology}},\n    url = {http://ieeexplore.ieee.org/document/7138574/},\n    doi = {10.1109/JBHI.2015.2450833},\n    number = {5},\n    urldate = {2017-12-08},\n    journal = {IEEE Journal of Biomedical and Health Informatics},\n    author = {Skripcak, Tomas and Just, Uwe and Simon, Monique and Buttner, Daniel and Luhr, Armin and Baumann, Michael and Krause, Mechthild},\n    month = sep,\n    year = {2016},\n    pages = {1397--1403}\n}\n```\nNote: In case RPB managed service utilisation (such as dedicated Institute/Department operating RPB platform instance used by others) additional acknowledgements/citations usually defined by service provider may apply.\n\n",
                "dependencies": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<project\n        xmlns=\"http://maven.apache.org/POM/4.0.0\"\n        xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n        xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"\n>\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>de.dktk.dd.rpb</groupId>\n    <artifactId>rpb-parent</artifactId>\n    <packaging>pom</packaging>\n    <version>1.0.0.14</version>\n    <name>radplanbio</name>\n    <description>RadPlanBio Parent Project</description>\n\n    <properties>\n\n        <!-- System -->\n        <jdk.version>1.8</jdk.version>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n\n        <!-- Spring -->\n        <spring.version>4.3.18.RELEASE</spring.version><!--old 4.1.6.RELEASE new 4.3.18.RELEASE-->\n        <spring-security.version>4.1.5.RELEASE</spring-security.version><!--old 3.2.7.RELEASE new 4.1.1.RELEASE-->\n\n        <!-- Testing/Mocking -->\n        <powermock.version>2.0.2</powermock.version>\n\n    </properties>\n\n    <modules>\n        <module>radplanbio-core</module>\n        <module>radplanbio-portal</module>\n        <module>radplanbio-participate</module>\n    </modules>\n\n    <build>\n        <pluginManagement>\n            <plugins>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-compiler-plugin</artifactId>\n                    <version>2.5.1</version>\n                    <configuration>\n                        <source>${jdk.version}</source>\n                        <target>${jdk.version}</target>\n                    </configuration>\n                </plugin>\n            </plugins>\n        </pluginManagement>\n    </build>\n\n    <dependencies>\n\n        <!-- Testing -->\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.12</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.mockito</groupId>\n            <artifactId>mockito-core</artifactId>\n            <version>2.23.4</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.powermock</groupId>\n            <artifactId>powermock-module-junit4</artifactId>\n            <version>${powermock.version}</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.powermock</groupId>\n            <artifactId>powermock-module-junit4-rule</artifactId>\n            <version>${powermock.version}</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.powermock</groupId>\n            <artifactId>powermock-api-mockito2</artifactId>\n            <version>${powermock.version}</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.powermock</groupId>\n            <artifactId>powermock-classloading-objenesis</artifactId>\n            <version>${powermock.version}</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.easytesting</groupId>\n            <artifactId>fest-assert</artifactId>\n            <version>1.4</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-test</artifactId>\n            <version>${spring.version}</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.seleniumhq.selenium</groupId>\n            <artifactId>selenium-java</artifactId>\n            <version>2.29.1</version>\n            <scope>test</scope>\n        </dependency>\n        <!--        read JSON data for tests from file-->\n        <dependency>\n            <groupId>com.googlecode.json-simple</groupId>\n            <artifactId>json-simple</artifactId>\n            <version>1.1.1</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.ws</groupId>\n            <artifactId>spring-ws-core</artifactId>\n            <version>2.0.0.RELEASE</version>\n        </dependency>\n        <dependency>\n            <groupId>commons-cli</groupId>\n            <artifactId>commons-cli</artifactId>\n            <version>1.0</version>\n        </dependency>\n\n    </dependencies>\n</project>\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rafcon",
            "repo_link": "https://github.com/DLR-RM/RAFCON",
            "content": {
                "codemeta": "",
                "readme": "\nRAFCON\n======\n\n.. figure:: documents/assets/Screenshot_Drill_Skill_Scaled.png\n   :figwidth: 100%\n   :width: 800px\n   :align: left\n   :alt: Screenshot showing RAFCON with a big state machine\n   :target: documents/assets/Screenshot_Drill_Skill_Scaled.png?raw=true\n\n* Documentation: Hosted on `Read the Docs <http://rafcon.readthedocs.io/en/latest/>`_\n* Homepage: `DLR-RM.github.io/RAFCON/ <https://dlr-rm.github.io/RAFCON/>`_\n* License: `EPL <https://github.com/DLR-RM/RAFCON/blob/master/LICENSE>`_\n\nDevelop your robotic tasks using an intuitive graphical user interface\n----------------------------------------------------------------------\n\nRAFCON uses hierarchical state machines, featuring concurrent state execution, to represent robot programs.\nIt ships with a graphical user interface supporting the creation of state machines and\ncontains IDE like debugging mechanisms. Alternatively, state machines can programmatically be generated\nusing RAFCON's API.\n\nUniversal application\n\n  RAFCON is written in Python, can be extended with plugins and is hard- and middleware independent.\n\nVisual programming\n\n  The sophisticated graphical editor can be used for the creation, execution and debugging of state machines.\n\nCollaborative working\n\n  Share and reuse your state machines in form of libraries, stored as JSON strings in text files.\n\n.. figure:: https://raw.githubusercontent.com/DLR-RM/RAFCON/master/documents/assets/RAFCON-sm-creation-preview.gif\n   :figwidth: 100%\n   :width: 570px\n   :align: left\n   :alt: Example on how to create a simple state machine\n\n\nInstallation preparations\n-------------------------\n\nBefore installing RAFCON, Python >=3.7, pip and setuptools are required on your system. Most of the other dependencies\nare automatically resolved by pip/setuptools, but not all of them. Those need be be installed manually, too:\n\nInstallation requirements\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: bash\n\n   sudo apt-get install python-dev python-pip build-essential glade python-gi-cairo\n   sudo -H pip install --upgrade pip\n   sudo -H pip install --upgrade setuptools\n\nGeneral requirements\n^^^^^^^^^^^^^^^^^^^^\n\n* Python >=3.7\n* pip (recent version required: v23 known to be working)\n* pdm (recent version required: v2.9.3 known to be working)\n\n\nInstalling RAFCON\n-----------------\n\n.. code-block:: bash\n\n   pip install rafcon --user\n\nThe ``--user`` flag is optional. If not set, RAFCON is installed globally (in this case you normaly need to have root privileges).\n\nIf during the installation the error ``ImportError: No module named cairo`` occurs, please install pycairo directly\nvia:\n\n.. code-block:: bash\n\n   pip install --user \"pycairo==1.19.1\"\n\nOf course you can also directly use the RAFCON sources from GitHub.\n\n.. code-block:: bash\n\n   cd /install/directory\n   git clone https://github.com/DLR-RM/RAFCON rafcon\n\n\nStart RAFCON\n------------\n\nNo matter which installation option you choose, RAFCON can be started from any location using (make sure\n``/usr/local/bin`` or ``~/.local/bin`` is in your ``PATH`` environment variable):\n\n.. code-block:: bash\n\n   rafcon\n\nOn a multi-python setup start rafcon using:\n\n.. code-block:: bash\n\n   python<your-version> -m rafcon\n\n\nUninstallation\n--------------\n\nIf you want to uninstall RAFCON, all you need to do is call\n\n.. code-block:: bash\n\n   pip uninstall rafcon\n\n",
                "dependencies": "[project]\nname = \"rafcon\"\nversion = \"2.1.4\" # Handled by bump2version\ndescription = \"Develop your robotic tasks with hierarchical state machines using an intuitive graphical user interface\"\nkeywords = [\"state machine\", \"robotic\", \"FSM\", \"development\", \"GUI\", \"visual programming\"]\nreadme = \"README.rst\"\nauthors = [\n    {name = \"Sebastian Brunner\", email = \"sebastian.brunner@dlr.de\"},\n    {name = \"Rico Belder\", email = \"rico.belder@dlr.de\"},\n    {name = \"Franz Steinmetz\", email = \"franz.steinmetz@dlr.de\"}\n]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Environment :: Console\",\n    \"Environment :: X11 Applications :: GTK\",\n    \"Framework :: Robot Framework\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Education\",\n    \"Intended Audience :: Manufacturing\",\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: Eclipse Public License 1.0 (EPL-1.0)\",\n    \"Natural Language :: English\",\n    \"Operating System :: Unix\",\n    \"Programming Language :: Python :: 3.6\",\n    \"Topic :: Scientific/Engineering\",\n    \"Topic :: Software Development\",\n    \"Topic :: Utilities\",\n]\ndependencies = [\n    \"PyGObject==3.42.0\",\n    \"astroid>=2.0.0,<2.9.0\",\n    \"gaphas>=2.1.0,<2.2.0\",\n    \"jsonconversion>=1.0.0\",\n    \"psutil>=5.0.0,<6.0.0\",\n    \"pycairo>=1.11.0,<1.21.0\",\n    \"pylint>=2.11.0,<2.12.0\",\n    \"simplegeneric>=0.5.0,<1.0.0\",\n    \"yaml_configuration>=0.2.5,<0.3.0\",\n    \"numpy==1.21.6; python_version >= \\\"3.7\\\" and python_full_version < \\\"3.7.1\\\"\",\n    \"numpy<2.0.0,>=1.22.4; python_version >= \\\"3.8\\\" and python_version < \\\"4.0\\\"\",\n    \"pandas==1.1.5; python_full_version < \\\"3.7.1\\\"\",\n    \"pandas<2.0.0,>=1.2.0; python_full_version >= \\\"3.7.1\\\"\",\n]\nlicense = {text = \"EPL\"}\nrequires-python = \">=3.7\"\n\n[project.urls]\nHomepage = \"https://github.com/DLR-RM/RAFCON\"\n\n[tool.pdm.dev-dependencies]\ndev = [\n    \"attrs==22.2.0\",\n    \"graphviz==0.18.2\",\n    \"matplotlib==2.1.1\",\n    \"monitoring==0.9.12\",\n    \"objgraph==3.5.0\",\n    \"profiling==0.1.3\",\n    \"pyuserinput==0.1.11\",\n    \"pytest-faulthandler~=1.6.0\",\n    \"pytest-mock>=1.9.0,<3\",\n    \"pytest-timeout\",\n    \"pytest>=6.0.0,<7.0.0\",\n    \"pyuserinput\",\n    \"libsass\",\n    \"coverage\",\n    \"tox>=3.28.0\",\n    \"bump2version>=1.0.1\",\n]\n\n[tool.pdm.build]\npackage-dir = \"source\"\nincludes = [\"source/rafcon\"]\nexcludes = [\"source/rafcon/share/ln\", \"source/rafcon/share/themes/RAFCON/templates\", \"tests\"]\n\n[tool.pdm.scripts]\npre_build.shell = \"\"\"\npython3 pre_build.py\n\"\"\"\n\n[project.scripts]\nrafcon_core = \"rafcon.core.start:main\"\n\n[project.gui-scripts]\nrafcon = \"rafcon.gui.start:main\"\nrafcon_execution_log_viewer = \"rafcon.gui.execution_log_viewer:main\"\n\n[build-system]\nrequires = [\"pdm-backend\"]\nbuild-backend = \"pdm.backend\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/random-simplex",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Random-Simplex",
            "content": {
                "codemeta": "",
                "readme": "![MATLAB](https://img.shields.io/badge/MATLAB-%23D00000.svg?style=plastic&logo=mathworks&logoColor=white)   ![Fortran](https://img.shields.io/badge/Fortran-%23734F96.svg?style=plastic&logo=fortran&logoColor=white) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/random-simplex?style=plastic)\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808899.svg)](https://doi.org/10.5281/zenodo.12808899)\n\n\n# Random Simplex Matrix\n\n## Overview\n\nThis repository contains a function that utilises the Dirichlet distribution method to generate points on the `(n−1)`-dimensional simplex. The `randomSimplexMatrix.m` generates `m x n` matrices where each row is a random sample from the `(n−1)`-dimensional simplex, i.e., it produces vectors where each element is a non-negative number and the sum of all elements in each vector is 1.\n\n### About\n\n[Mathematica Link](https://reference.wolfram.com/language/ref/Simplex.html)\n\n## Methodology\n\n1. **Generation of K Unit-Exponential Distributed Random Draws**:\n    - In each row (sample), the function generates K uniform random numbers `y_i` from the open interval `(0,1]`.\n    - These are transformed to unit-exponential distributed random numbers `x_i = -log(y_i)`.\n\n2. **Normalization**:\n    - Compute the sum `S` of all `x_i` values.\n\n3. **Calculation of Simplex Coordinates**:\n    - The coordinates `t_1, ..., t_K` of the final point on the unit simplex are computed as `t_i = x_i / S`.\n\n4. **Output**:\n    - Returns a matrix, where each row is a vector on the `(n-1)`-dimensional simplex.\n\n\n## Some Applications\n\n#### Stability Analysis\n\n- Polynomial Stability/Stability of discrete-time control systems: \n\n    > Use simplex sampling to generate coefficients for polynomials and analyse their stability by checking if all roots lie within the unit circle.\n\n- Lyapunov Functions: \n\n    > Construct Lyapunov functions with randomly sampled coefficients to study the stability of equilibrium points in dynamical systems.\n\n#### Bifurcation Analysis\n\n- Parameter Space Exploration:\n\n    > Investigate the behavior of dynamical systems under different parameter regimes by sampling parameters from a simplex. Identify bifurcation points where system behavior changes qualitatively.\n\n- Nonlinear Dynamics: \n\n    > Model/simulate nonlinear systems to study chaos, where initial conditions or parameters are sampled from a simplex.\n\n## Additional Scripts\n\n1. `simplexSpace.m`\n\n   > Demonstrates various plots and visualisations of simplex sampling.\n\n2. `MultivariateND.m`\n\n   > Showcases an application of simplex sampling for sampling from a multivariate normal distribution.\n\n3. `VoronoiDiagram.m`\n\n   > Visualise the Voronoi diagram of random points on a 2-dimensional simplex, divides regions based on proximity.\n\n\n## Example Usage\n\n```matlab\nn = 100;  % Number of columns (dimensionality of simplex)\nm = 1500;  % Number of rows (number of samples)\ny = randomSimplexMatrix(n, m);\ndisp('Generated simplex matrix:');\ndisp(y);\n```\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rankings-reloaded",
            "repo_link": "https://github.com/wiesenfa/challengeR",
            "content": {
                "codemeta": "",
                "readme": "Methods and open-source toolkit for analyzing and visualizing challenge\nresults\n================\n\n-   [Introduction](#introduction)\n-   [Installation](#installation)\n-   [Terms of use](#terms-of-use)\n-   [Usage](#usage)\n-   [Troubleshooting](#troubleshooting)\n-   [Changes](#changes)\n-   [Team](#team)\n-   [Reference](#reference)\n\n# Introduction\n\nThe current framework is a tool for analyzing and visualizing challenge\nresults in the field of biomedical image analysis and beyond.\n\nBiomedical challenges have become the de facto standard for benchmarking\nbiomedical image analysis algorithms. While the number of challenges is\nsteadily increasing, surprisingly little effort has been invested in\nensuring high quality design, execution and reporting for these\ninternational competitions. Specifically, results analysis and\nvisualization in the event of uncertainties have been given almost no\nattention in the literature.\n\nGiven these shortcomings, the current framework aims to enable fast and\nwide adoption of comprehensively analyzing and visualizing the results\nof single-task and multi-task challenges. This approach offers an\nintuitive way to gain important insights into the relative and absolute\nperformance of algorithms, which cannot be revealed by commonly applied\nvisualization techniques.\n\n# Installation\n\nRequires R version &gt;= 3.5.2 (<https://www.r-project.org>).\n\nFurther, a recent version of Pandoc (&gt;= 1.12.3) is required. RStudio\n(<https://rstudio.com>) automatically includes this so you do not need\nto download Pandoc if you plan to use rmarkdown from the RStudio IDE,\notherwise you’ll need to install Pandoc for your platform\n(<https://pandoc.org/installing.html>). Finally, if you want to generate\na PDF report you will need to have LaTeX installed (e.g. MiKTeX, MacTeX\nor TinyTeX).\n\nTo get the latest released version (master branch) of the R package from\nGitHub:\n\n``` r\nif (!requireNamespace(\"devtools\", quietly = TRUE)) install.packages(\"devtools\")\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\")\nBiocManager::install(\"Rgraphviz\", dependencies = TRUE)\ndevtools::install_github(\"wiesenfa/challengeR\", dependencies = TRUE)\n```\n\nIf you are asked whether you want to update installed packages and you\ntype “a” for all, you might need administrator permissions to update R\ncore packages. You can also try to type “n” for updating no packages. If\nyou are asked “Do you want to install from sources the packages which\nneed compilation? (Yes/no/cancel)”, you can safely type “no”.\n\nIf you get *warning* messages (in contrast to *error* messages), these\nmight not be problematic and you can try to proceed. If you encounter\nerrors during the setup, looking into the “Troubleshooting” section\nmight be worth it.\n\nFor Linux users: Some system libraries might be missing. Check the\noutput in the R console for further hints carefully during the\ninstallation of packages.\n\n# Terms of use\n\nCopyright (c) German Cancer Research Center (DKFZ). All rights reserved.\n\nchallengeR is available under license GPLv2 or any later version.\n\nIf you use this software for a publication, please cite:\n\nWiesenfarth, M., Reinke, A., Landman, B.A., Eisenmann, M., Aguilera\nSaiz, L., Cardoso, M.J., Maier-Hein, L. and Kopp-Schneider, A. Methods\nand open-source toolkit for analyzing and visualizing challenge results.\n*Sci Rep* **11**, 2369 (2021).\n<https://doi.org/10.1038/s41598-021-82017-6>\n\n# Usage\n\nEach of the following steps has to be run to generate the report: (1)\nLoad package, (2) load data, (3) perform ranking, (4) perform\nbootstrapping and (5) generation of the report\n\nYou can find R scripts for quickstart in the directory “vignettes”. An\noverview of all available plots is provided in the “Visualizations”\nvignette demonstrating the use of their corresponding plot functions as\nwell.\n\nHere, we provide a step-by-step guide that leads you to your final\nreport.\n\n## 1. Load package\n\nLoad package\n\n``` r\nlibrary(challengeR)\n```\n\n## 2. Load data\n\n### Data requirements\n\nData requires the following *columns*:\n\n-   *task identifier* in case of multi-task challenges (string or\n    numeric)\n-   *test case identifier* (string or numeric)\n-   *algorithm identifier* (string or numeric)\n-   *metric value* (numeric)\n\nIn case of missing metric values, a missing observation has to be\nprovided (either as blank field or “NA”).\n\nFor example, in a challenge with 2 tasks, 2 test cases and 2 algorithms,\nwhere in task “T2”, test case “case2”, algorithm “A2” didn’t give a\nprediction (and thus NA or a blank field for missing value is inserted),\nthe data set might look like this:\n\n| Task | TestCase | Algorithm | MetricValue |\n|:-----|:---------|:----------|------------:|\n| T1   | case1    | A1        |       0.266 |\n| T1   | case1    | A2        |       0.202 |\n| T1   | case2    | A1        |       0.573 |\n| T1   | case2    | A2        |       0.945 |\n| T2   | case1    | A1        |       0.372 |\n| T2   | case1    | A2        |       0.898 |\n| T2   | case2    | A1        |       0.908 |\n| T2   | case2    | A2        |          NA |\n\n### 2.1 Load data from file\n\nIf you have assessment data at hand stored in a csv file (if you want to\nuse simulated data, skip the following code line) use\n\n``` r\ndata_matrix <- read.csv(file.choose()) # type ?read.csv for help\n```\n\nThis allows to choose a file interactively, otherwise replace\n*file.choose()* by the file path (in style “/path/to/dataset.csv”) in\nquotation marks.\n\n### 2.2 Simulate data\n\nIn the following, simulated data is generated *instead* for illustration\npurposes (skip the following code chunk if you have already loaded\ndata). The data is also stored as “inst/extdata/data\\_matrix.csv” in the\nrepository.\n\n``` r\nif (!requireNamespace(\"permute\", quietly = TRUE)) install.packages(\"permute\")\n\nn <- 50\n\nset.seed(4)\nstrip <- runif(n,.9,1)\nc_ideal <- cbind(task=\"c_ideal\",\n            rbind(\n              data.frame(alg_name=\"A1\",value=runif(n,.9,1),case=1:n),\n              data.frame(alg_name=\"A2\",value=runif(n,.8,.89),case=1:n),\n              data.frame(alg_name=\"A3\",value=runif(n,.7,.79),case=1:n),\n              data.frame(alg_name=\"A4\",value=runif(n,.6,.69),case=1:n),\n              data.frame(alg_name=\"A5\",value=runif(n,.5,.59),case=1:n)\n            ))\n\nset.seed(1)\nc_random <- data.frame(task=\"c_random\",\n                       alg_name=factor(paste0(\"A\",rep(1:5,each=n))),\n                       value=plogis(rnorm(5*n,1.5,1)),case=rep(1:n,times=5)\n                       )\n\nstrip2 <- seq(.8,1,length.out=5)\na <- permute::allPerms(1:5)\nc_worstcase <- data.frame(task=\"c_worstcase\",\n                     alg_name=c(t(a)),\n                     value=rep(strip2,nrow(a)),\n                     case=rep(1:nrow(a),each=5)\n                     )\nc_worstcase <- rbind(c_worstcase,\n                data.frame(task=\"c_worstcase\",alg_name=1:5,value=strip2,case=max(c_worstcase$case)+1)\n          )\nc_worstcase$alg_name <- factor(c_worstcase$alg_name,labels=paste0(\"A\",1:5))\n\ndata_matrix <- rbind(c_ideal, c_random, c_worstcase)\n```\n\n## 3. Perform ranking\n\n### 3.1 Define challenge object\n\nCode differs slightly for single- and multi-task challenges.\n\nIn case of a single-task challenge use\n\n``` r\n# Use only task \"c_random\" in object data_matrix\ndataSubset <- subset(data_matrix, task==\"c_random\")\n\nchallenge <- as.challenge(dataSubset,\n                          # Specify which column contains the algorithms, \n                          # which column contains a test case identifier \n                          # and which contains the metric value:\n                          algorithm = \"alg_name\", case = \"case\", value = \"value\", \n                          # Specify if small metric values are better\n                          smallBetter = FALSE)\n```\n\n*Instead*, for a multi-task challenge use\n\n``` r\n# Same as above but with 'by=\"task\"' where variable \"task\" contains the task identifier\nchallenge <- as.challenge(data_matrix, \n                          by = \"task\", \n                          algorithm = \"alg_name\", case = \"case\", value = \"value\", \n                          smallBetter = FALSE)\n```\n\n### 3.2 Configure ranking\n\nDifferent ranking methods are available, choose one of them:\n\n-   for “aggregate-then-rank” use (here: take mean for aggregation)\n\n``` r\nranking <- challenge%>%aggregateThenRank(FUN = mean, # aggregation function, \n                                                     # e.g. mean, median, min, max, \n                                                     # or e.g. function(x) quantile(x, probs=0.05)\n                                         na.treat = 0, # either \"na.rm\" to remove missing data, \n                                                       # set missings to numeric value (e.g. 0) \n                                                       # or specify a function, \n                                                       # e.g. function(x) min(x)\n                                         ties.method = \"min\" # a character string specifying \n                                                             # how ties are treated, see ?base::rank\n                                        )  \n```\n\n-   *alternatively*, for “rank-then-aggregate” with arguments as above\n    (here: take mean for aggregation)\n\n``` r\nranking <- challenge%>%rankThenAggregate(FUN = mean,\n                                         ties.method = \"min\"\n                                        )\n```\n\n-   *alternatively*, for test-then-rank based on Wilcoxon signed rank\n    test\n\n``` r\nranking <- challenge%>%testThenRank(alpha = 0.05, # significance level\n                                    p.adjust.method = \"none\", # method for adjustment for\n                                                              # multiple testing, see ?p.adjust\n                                    na.treat = 0, # either \"na.rm\" to remove missing data,\n                                                  # set missings to numeric value (e.g. 0)\n                                                  # or specify a function, e.g. function(x) min(x)\n                                    ties.method = \"min\" # a character string specifying\n                                                        # how ties are treated, see ?base::rank\n                                   )\n```\n\n## 4. Perform bootstrapping\n\nPerform bootstrapping with 1000 bootstrap samples using one CPU\n\n``` r\nset.seed(123, kind = \"L'Ecuyer-CMRG\")\nranking_bootstrapped <- ranking%>%bootstrap(nboot = 1000)\n```\n\nIf you want to use multiple CPUs (here: 8 CPUs), use\n\n``` r\nlibrary(doParallel)\nlibrary(doRNG)\nregisterDoParallel(cores = 8)  \nregisterDoRNG(123)\nranking_bootstrapped <- ranking%>%bootstrap(nboot = 1000, parallel = TRUE, progress = \"none\")\nstopImplicitCluster()\n```\n\n## 5. Generate the report\n\nGenerate report in PDF, HTML or DOCX format. Code differs slightly for\nsingle- and multi-task challenges.\n\n### 5.1 For single-task challenges\n\n``` r\nranking_bootstrapped %>% \n  report(title = \"singleTaskChallengeExample\", # used for the title of the report\n         file = \"filename\", \n         format = \"PDF\", # format can be \"PDF\", \"HTML\" or \"Word\"\n         latex_engine = \"pdflatex\", #LaTeX engine for producing PDF output. Options are \"pdflatex\", \"lualatex\", and \"xelatex\"\n         clean = TRUE #optional. Using TRUE will clean intermediate files that are created during rendering.\n        ) \n```\n\nArgument *file* allows for specifying the output file path as well,\notherwise the working directory is used. If file is specified but does\nnot have a file extension, an extension will be automatically added\naccording to the output format given in *format*. Using argument\n*clean=FALSE* allows to retain intermediate files, such as separate\nfiles for each figure.\n\nIf argument “file” is omitted, the report is created in a temporary\nfolder with file name “report”.\n\n### 5.2 For multi-task challenges\n\nSame as for single-task challenges, but additionally consensus ranking\n(rank aggregation across tasks) has to be given.\n\nCompute ranking consensus across tasks (here: consensus ranking\naccording to mean ranks across tasks)\n\n``` r\n# See ?relation_consensus for different methods to derive consensus ranking\nmeanRanks <- ranking%>%consensus(method = \"euclidean\") \nmeanRanks # note that there may be ties (i.e. some algorithms have identical mean rank)\n```\n\nGenerate report as above, but with additional specification of consensus\nranking\n\n``` r\nranking_bootstrapped %>% \n  report(consensus = meanRanks,\n         title = \"multiTaskChallengeExample\",\n         file = \"filename\", \n         format = \"PDF\", # format can be \"PDF\", \"HTML\" or \"Word\"\n         latex_engine = \"pdflatex\"#LaTeX engine for producing PDF output. Options are \"pdflatex\", \"lualatex\", and \"xelatex\"\n        )\n```\n\nThe consensus ranking is given according to mean ranks across tasks if\nmethod=“euclidean” where in case of ties (equal ranks for multiple\nalgorithms) the average rank is used, i.e. ties.method=“average”.\n\n# Troubleshooting\n\nIn this section we provide an overview of issues that the users reported\nand how they were solved.\n\n## Issues related to RStudio\n\n### Issue: Rtools is missing\n\nWhile trying to install the current version of the repository:\n\n``` r\ndevtools::install_github(\"wiesenfa/challengeR\", dependencies = TRUE)\n```\n\nThe following warning showed up in the output:\n\n``` r\nWARNING: Rtools is required to build R packages, but is not currently installed.\n```\n\nTherefore, Rtools was installed via a separate executable:\n<https://cran.r-project.org/bin/windows/Rtools/> and the warning\ndisappeared.\n\n#### Solution:\n\nActually there is no need of installing Rtools, it is not really used in\nthe toolkit. Insted, choose not to install it when it is asked. See\ncomment in the installation section:\n\n“If you are asked whether you want to update installed packages and you\ntype “a” for all, you might need administrator rights to update R core\npackages. You can also try to type “n” for updating no packages. If you\nare asked “Do you want to install from sources the packages which need\ncompilation? (Yes/no/cancel)”, you can safely type “no”.”\n\n### Issue: Package versions are mismatching\n\nInstalling the current version of the tool from GitHub failed.\n\nThe error message was:\n\n``` r\nbyte-compile and prepare package for lazy loading\nError: (converted from warning) package 'ggplot2' was built under R version 3.6.3\nExecution halted\nERROR: lazy loading failed for package 'challengeR'\n* removing 'C:/Users/.../Documents/R/win-library/3.6/challengeR'\n* restoring previous 'C:/Users/.../Documents/R/win-library/3.6/challengeR'\nError: Failed to install 'challengeR' from GitHub:\n  (converted from warning) installation of package 'C:/Users/.../AppData/Local/Temp/Rtmp615qmV/file4fd419555eb4/challengeR_0.3.1.tar.gz' had non-zero exit status\n```\n\nThe problem was that some of the packages that were built under R3.6.1\nhad been updated, but the current installed version was still R3.6.1.\n\n#### Solution:\n\nThe solution was to update R3.6.1 to R3.6.3. Another way would have been\nto reset the single packages to the versions built under R3.6.1.\n\n### Issue: Package is missing\n\nInstalling the current version of the tool from GitHub failed.\n\n``` r\n devtools::install_github(\"wiesenfa/challengeR\", dependencies = TRUE)\n```\n\nThe error message was:\n\n``` r\nError: .onLoad failed in loadNamespace() for 'pkgload', details:\n  call: loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]])\n  error: there is no package called ‘backports’\n```\n\nThe problem was that the packages ‘backports’ had not been installed.\n\n#### Solution:\n\nThe solution was to install ‘backports’ manually.\n\n``` r\n install.packages(\"backports\")\n```\n\n### Issue: Packages are not detected correctly\n\nWhile trying to install the package after running the following\ncommands:\n\n``` r\nif (!requireNamespace(\"devtools\", quietly = TRUE)) install.packages(\"devtools\")\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\")\nBiocManager::install(\"Rgraphviz\", dependencies = TRUE)\ndevtools::install_github(\"wiesenfa/challengeR\", dependencies = TRUE)\n```\n\nThe error message was:\n\n``` r\nERROR:\n1: In file(con, \"r\") :\n URL 'https://bioconductor.org/config.yaml': status was 'SSL connect error'\n2: packages ‘BiocVersion’, ‘Rgraphviz’ are not available (for R version 3.6.1)\n```\n\n#### Solution:\n\nThe solution was to restart RStudio.\n\n## Issues related to MiKTeX\n\n### Issue: Missing packages\n\nWhile generating the PDF with MiKTeX (2.9), the following error showed\nup:\n\n``` r\nfatal pdflatex - gui framework cannot be initialized\n```\n\nThere is an issue with installing missing packages in LaTeX.\n\n##### Solution:\n\nOpen your MiKTeX Console –&gt; Settings, select “Always install missing\npackages on-the-fly”. Then generate the report. Once the report is\ngenerated, you can reset the settings to your preferred ones.\n\n### Issue: Unable to generate report\n\nWhile generating the PDF with MiKTeX (2.9):\n\n``` r\nranking_bootstrapped %>% \n  report(title = \"singleTaskChallengeExample\", # used for the title of the report\n         file = \"filename\", \n         format = \"PDF\", # format can be \"PDF\", \"HTML\" or \"Word\"\n         latex_engine = \"pdflatex\", #LaTeX engine for producing PDF output. Options are \"pdflatex\", \"lualatex\", and \"xelatex\"\n         clean = TRUE #optional. Using TRUE will clean intermediate files that are created during rendering.\n        ) \n```\n\nThe following error showed up:\n\n``` r\noutput file: filename.knit.md\n\n\"C:/Program Files/RStudio/bin/pandoc/pandoc\" +RTS -K512m -RTS filename.utf8.md --to latex --from markdown+autolink_bare_uris+tex_math_single_backslash --output filename.tex --self-contained --number-sections --highlight-style tango --pdf-engine pdflatex --variable graphics --lua-filter \"C:/Users/adm/Documents/R/win-library/3.6/rmarkdown/rmd/lua/pagebreak.lua\" --lua-filter \"C:/Users/adm/Documents/R/win-library/3.6/rmarkdown/rmd/lua/latex-div.lua\" --variable \"geometry:margin=1in\" \n\nError: LaTeX failed to compile filename.tex. See https://yihui.org/tinytex/r/#debugging for debugging tips.\n\n  Warning message:\nIn system2(..., stdout = if (use_file_stdout()) f1 else FALSE, stderr = f2) :\n  '\"pdflatex\"' not found\n```\n\n#### Solution:\n\nThe solution was to restart RStudio.\n\n# Changes\n\n#### Version 1.0.5\n\n-   Ensure reproducibility with parallel bootstrapping\n    ([T29361](https://phabricator.mitk.org/T29361))\n\n#### Version 1.0.4\n\n-   Fix NaN values cause error\n    ([T28746](https://phabricator.mitk.org/T28746))\n-   Fix Bars and dots don’t match in podium plot\n    ([T29167](https://phabricator.mitk.org/T29167))\n-   Fix y-axis of blob plots always scaled to 5\n    ([T28966](https://phabricator.mitk.org/T28966))\n\n#### Version 1.0.3\n\n-   Fix ggplot warning in various places of the report\n    ([T28710](https://phabricator.mitk.org/T28710))\n\n#### Version 1.0.2\n\n-   Fix error when all metric values are the same\n    ([T28453](https://phabricator.mitk.org/T28453))\n-   Fix wrong number of algorithms shown in report summary\n    ([T28465](https://phabricator.mitk.org/T28465))\n\n#### Version 1.0.1\n\n-   Fix error raised in case there are more tasks than algorithms\n    contained in the dataset\n    ([T28193](https://phabricator.mitk.org/T28193))\n-   Drop restriction that at least three algorithms are required for\n    bootstrapping ([T28194](https://phabricator.mitk.org/T28194))\n-   Avoid blank pages in PDF report when bootstrapping is disabled\n    ([T28201](https://phabricator.mitk.org/T28201))\n-   Handle tasks having only one case for bootstrapping\n    ([T28202](https://phabricator.mitk.org/T28202))\n-   Update citation ([T28210](https://phabricator.mitk.org/T28210))\n\n#### Version 1.0.0\n\n-   Revision of the underlying data structure\n-   Roxygen documentation for main functionality\n-   Vignettes for quickstart and overview of available plots\n    demonstrating the use of their corresponding plot functions\n-   Introduction of unit tests (package coverage &gt;70%)\n-   Troubleshooting section covering potential issues during setup\n-   Finally: Extensive bug fixes and improvements (for a complete\n    overview please check the [Phabricator\n    tasks](https://phabricator.mitk.org/search/query/vtj0qOqH5qL6/))\n\n#### Version 0.3.3\n\n-   Force line break to avoid that authors exceed the page in generated\n    PDF reports\n\n#### Version 0.3.2\n\n-   Correct names of authors\n\n#### Version 0.3.1\n\n-   Refactoring\n\n#### Version 0.3.0\n\n-   Major bug fix release\n\n#### Version 0.2.5\n\n-   Bug fixes\n\n#### Version 0.2.4\n\n-   Automatic insertion of missings\n\n#### Version 0.2.3\n\n-   Bug fixes\n-   Reports for subsets (top list) of algorithms: Use\n    e.g. `subset(ranking_bootstrapped, top=3) %>% report(...)` (or\n    `subset(ranking, top=3) %>% report(...)` for report without\n    bootstrap results) to only show the top 3 algorithms according to\n    the chosen ranking methods, where `ranking_bootstrapped` and\n    `ranking` objects as defined in the example. Line plot for ranking\n    robustness can be used to check whether algorithms performing well\n    in other ranking methods are excluded. Bootstrapping still takes\n    entire uncertainty into account. Podium plot and ranking heatmap\n    neglect excluded algorithms. Only available for single-task\n    challenges (for multi-task challenges not sensible because each task\n    would contain a different set of algorithms).\n-   Reports for subsets of tasks: Use\n    e.g. `subset(ranking_bootstrapped, tasks=c(\"task1\", \"task2\",\"task3\")) %>% report(...)`\n    to restrict report to tasks “task1”, “task2”,\"task3. You may want to\n    recompute the consensus ranking before using\n    `meanRanks=subset(ranking, tasks=c(\"task1\", \"task2\", \"task3\"))%>%consensus(method = \"euclidean\")`\n\n#### Version 0.2.1\n\n-   Introduction in reports now mentions e.g. ranking method, number of\n    test cases,…\n-   Function `subset()` allows selection of tasks after bootstrapping,\n    e.g. `subset(ranking_bootstrapped,1:3)`\n-   `report()` functions gain argument `colors` (default:\n    `default_colors`). Change e.g. to `colors=viridisLite::inferno`\n    which “is designed in such a way that it will analytically be\n    perfectly perceptually-uniform, both in regular form and also when\n    converted to black-and-white. It is also designed to be perceived by\n    readers with the most common form of color blindness.” See package\n    `viridis` for further similar functions.\n\n#### Version 0.2.0\n\n-   Improved layout in case of many algorithms and tasks (while probably\n    still not perfect)\n-   Consistent coloring of algorithms across figures\n-   `report()` function can be applied to ranked object before\n    bootstrapping (and thus excluding figures based on bootstrapping),\n    i.e. in the example `ranking %>% report(...)`\n-   bug fixes\n\n# Team\n\nThe developer team includes members from both division of Intelligent\nMedical Systems (IMSY) and Biostatistics at the German Cancer Research\nCenter (DKFZ):\n\n-   Manuel Wiesenfarth\n-   Annette Kopp-Schneider\n-   Annika Reinke\n-   Matthias Eisenmann\n-   Laura Aguilera Saiz\n-   Elise Récéjac\n-   Lena Maier-Hein\n-   Ali Emre Kavur\n\n# Reference\n\nWiesenfarth, M., Reinke, A., Landman, B.A., Eisenmann, M., Aguilera\nSaiz, L., Cardoso, M.J., Maier-Hein, L. and Kopp-Schneider, A. Methods\nand open-source toolkit for analyzing and visualizing challenge results.\n*Sci Rep* **11**, 2369 (2021).\n<https://doi.org/10.1038/s41598-021-82017-6>\n\n</br> <img src=\"Helmholtz_Imaging_Logo.svg\" height=\"70px\" /> </br></br>\n<img src=\"DKFZ_Logo.png\" height=\"100px\" />\n\n",
                "dependencies": "Package: challengeR\nType: Package\nTitle: Analyzing assessment data of biomedical image analysis competitions and visualization of results\nVersion: 1.0.5\nDate: 2023-08-10\nAuthor: Manuel Wiesenfarth, Matthias Eisenmann, Laura Aguilera Saiz, Annette Kopp-Schneider, Ali Emre Kavur\nMaintainer: Manuel Wiesenfarth <m.wiesenfarth@dkfz.de>\nDescription: Analyzing assessment data of biomedical image analysis competitions and visualization of results.\nLicense: GPL (>= 2)\nDepends:\n  R (>= 3.5.2),\n  ggplot2 (>= 3.3.0),\n  purrr (>= 0.3.3)\nImports:\n  dplyr (>= 0.8.5),\n  graph (>= 1.64.0),\n  knitr (>= 1.28),\n  methods (>= 3.6.0),\n  plyr (>= 1.8.6),\n  relations (>= 0.6-9),\n  reshape2 (>= 1.4.3),\n  rlang (>= 0.4.5),\n  rmarkdown (>= 2.1),\n  tidyr (>= 1.0.2),\n  viridisLite (>= 0.3.0)\nSuggests:\n  doParallel (>= 1.0.15),\n  doRNG (>= 1.8.6),\n  foreach (>= 1.4.8),\n  ggpubr (>= 0.2.5),\n  Rgraphviz (>= 2.30.0),\n  testthat (>= 2.1.0)\nVignetteBuilder: knitr\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.1.0\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rayx",
            "repo_link": "https://github.com/hz-b/rayx",
            "content": {
                "codemeta": "",
                "readme": "# RAYX\n\n<table>\n  <tr>\n    <td>\n      <img src=\"https://github.com/user-attachments/assets/d12229b0-7820-475f-8f02-6b2f253c5081\" alt=\"RAYX Logo\" width=\"600\">\n    </td>\n    <td>\n      <strong>RAYX</strong> is a powerful, multi-component simulation platform designed to streamline the design and optimization of beamlines in synchrotron light source facilities. At the core of the platform is <i>rayx-core</i>, a high-performance library that delivers precise light tracing capabilities on both CPUs and GPUs. This core library ensures that users can achieve detailed and accurate simulations at high speeds, making it an ideal solution for complex beamline designs.\n    </td>\n  </tr>\n</table>\n\nTo simplify the usage of _rayx-core_, the platform includes rayx, a command-line interface (CLI) tool designed for fast, one-shot tracing of beamlines. It provides comprehensive data on every ray-element intersection, making it especially valuable for generating large datasets efficiently. With its focus on ease of use, _rayx_ empowers users to quickly run simulations and retrieve detailed ray-tracing results.\n\nFor users who prefer a more visual approach, _rayx-ui_ offers a graphical user interface (GUI) that includes a 3D viewport of the beamline, enabling interactive design and exploration. This GUI provides an intuitive interface to construct and modify beamlines, allowing users to visualize their designs in real-time. _rayx-ui_ not only enhances the design process but also allows users to iteratively optimize configurations based on immediate visual feedback.\n\n## RAYX vs RAY-UI\n\nRAYX offers several advanced features, including:\n- Global (not sequential) tracing of beamlines\n- GPU utilization for accelerated tracing performance\n- A dedicated mode for tracing multiple beamlines with ease\n- Objects in RAYX can be grouped for simplified group transformations\n- A GUI for intuitive beamline design\n\n## Installing or Building RAYX\n\n[![testUbuntu](https://github.com/hz-b/rayx/actions/workflows/testUbuntu.yml/badge.svg?branch=master)](https://github.com/hz-b/rayx/actions/workflows/testUbuntu.yml) [![testWindows](https://github.com/hz-b/rayx/actions/workflows/testWindows.yml/badge.svg?branch=master)](https://github.com/hz-b/rayx/actions/workflows/testWindows.yml) [![testUbuntuClang](https://github.com/hz-b/rayx/actions/workflows/testUbuntuClang.yml/badge.svg?branch=master)](https://github.com/hz-b/rayx/actions/workflows/testUbuntuClang.yml) [![MDBookDeploy](https://github.com/hz-b/rayx/actions/workflows/mdBookDeploy.yml/badge.svg)](https://github.com/hz-b/rayx/actions/workflows/mdBookDeploy.yml)\n\nFor additional information, please visit our [Wiki](https://hz-b.github.io/rayx/). We are committed to delivering stable releases, which can be found [here](https://github.com/hz-b/rayx/releases). Please note that the `master` branch and other branches might be unstable, and building RAYX from the source could lead to unstable software. We recommend this only for developers and experienced users. If you experience issues with our distributed binaries or API, do not hesitate to [open an issue](https://github.com/hz-b/rayx/issues/new/choose). We are keen to provide assistance and develop features as the need arises.\n\n",
                "dependencies": "cmake_minimum_required(VERSION 3.15 FATAL_ERROR)\n\n# ---- Project ----\nproject(RAYX VERSION 0.21.7)\nset(CMAKE_CXX_STANDARD 20)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CUDA_STANDARD 20)\nset(CMAKE_CUDA_STANDARD_REQUIRED ON)\n# ------------------\n\n# ---- Options ----\noption(WERROR \"add -Werror option\" \"NO\") # inactive per default\noption(RAYX_ENABLE_CUDA \"This option enables the search for CUDA. Project will be compiled without cuda if not found.\" ON)\noption(RAYX_REQUIRE_CUDA \"If option 'RAYX_ENABLE_CUDA' is ON, this option will add the requirement that cuda must be found.\" OFF)\noption(RAYX_STATIC_LIB \"This option builds 'rayx-core' as a static library.\" OFF)\n\n# ------------------\n\n# ---- Code Coverage ----\nset(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/Extern/cmake)\n# ------------------\n\n# ---- CPack ----\nset(CPACK_PROJECT_NAME ${PROJECT_NAME})\nset(CPACK_RESOURCE_FILE_LICENSE \"${CMAKE_SOURCE_DIR}/LICENSE\")\nset(CPACK_PACKAGE_DESCRIPTION \"${PROJECT_NAME} - For simulating and designing beamlines at synchrotron light sources\")\nset(CPACK_PACKAGE_DESCRIPTION_SUMMARY \"${PROJECT_NAME} - For simulating and designing beamlines at synchrotron light sources\")\nset(CPACK_PACKAGE_VENDOR \"Helmhotz-Zentrum Berlin\")\nset(CPACK_PACKAGE_CONTACT \"jannis.maier@helmholtz-berlin.de\")\n\n# Disable GoogleTest installation\nset(INSTALL_GTEST OFF CACHE BOOL \"Disable installation of GoogleTest\" FORCE)\nset(INSTALL_GMOCK OFF CACHE BOOL \"Disable installation of GoogleMock\" FORCE)\n\n# Install directories\nif(APPLE)\n    set(INSTALL_DATA_DIR \"Library/Application Support/${PROJECT_NAME}\")\n    set(INSTALL_FONTS_DIR \"Library/Fonts/${PROJECT_NAME}\")\nelseif(UNIX AND NOT APPLE)\n    set(INSTALL_DATA_DIR \"share/${PROJECT_NAME}\")\n    set(INSTALL_FONTS_DIR \"share/fonts/${PROJECT_NAME}\")\nelseif(WIN32)\n    set(INSTALL_DATA_DIR \".\")\n    set(INSTALL_FONTS_DIR \".\")\nendif()\n# ------------------\n\n\n# ---- Subdirectories ----\nadd_subdirectory(Extern)\nadd_subdirectory(Intern)\n# ------------------\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rce",
            "repo_link": "https://github.com/rcenvironment/rce",
            "content": {
                "codemeta": "",
                "readme": "RCE is a distributed, workflow-driven integration environment.\nIt is used by engineers and scientists to analyze, optimize, and design complex systems (e.g., aircraft, ships, or satellites).\nUsing RCE, they can combine their specialized design and simulation tools into distributed workflows.\n\nSoftware website: [https://rcenvironment.de](https://rcenvironment.de)\n\n##### Issue Tracker\n\nSee the [main issue tracker](https://mantis.sc.dlr.de/roadmap_page.php) for the complete set of issues and an up-to-date roadmap.\nIt is currently read-only.\nPlease use the [GitHub issue tracker](https://github.com/rcenvironment/rce/issues) to report new issues.\n\n##### Changelog\n\nThe most relevant changes and new features in each release are listed on [this page](https://github.com/rcenvironment/rce/releases).\nSimilar information for older releases is available [here](https://github.com/rcenvironment/rce/wiki/Changelog-Overview).\nFor a more detailed, but also more technical list of changes, see the [main issue tracker](https://mantis.sc.dlr.de/changelog_page.php).\n\n##### Get RCE (as a ready-to-run software)\n\nFor Windows, a simple .zip file is provided to set up both client and server installations.\n\nOn Linux, .deb/.rpm packages as well as a simple .zip file are provided to set up both client and server installations.\n\n[Download latest release](https://software.dlr.de/updates/rce/10.x/products/standard/releases/latest/) | [Update site of latest release](https://software.dlr.de/updates/rce/10.x/repositories/standard/releases/latest/)\n\n##### License\n\nRCE is Open Source Software provided under the terms of the [Eclipse Public License (EPL)](http://opensource.org/licenses/EPL-1.0).\nThis source repository as well as related releases also contain software covered by other open source licenses.\nMore information is available in embedded licensing files.\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/reflectorch",
            "repo_link": "https://github.com/schreiber-lab/reflectorch",
            "content": {
                "codemeta": "",
                "readme": "# Reflectorch\n\n[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)](https://pytorch.org/)\n[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white)](https://numpy.org/)\n[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=for-the-badge&logo=scipy&logoColor=%white)](https://scipy.org/)\n[![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=black)](https://matplotlib.org/)\n[![YAML](https://img.shields.io/badge/yaml-%23ffffff.svg?style=for-the-badge&logo=yaml&logoColor=151515)](https://yaml.org/)\n[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-%23FFD700.svg?style=for-the-badge&logo=huggingface&logoColor=black)](https://huggingface.co/valentinsingularity/reflectivity)\n\n[![Python version](https://img.shields.io/badge/python-3.7%7C3.8%7C3.9%7C3.10%7C3.11%7C3.12-blue.svg)](https://www.python.org/)\n![CI workflow status](https://github.com/schreiber-lab/reflectorch/actions/workflows/ci.yml/badge.svg)\n![Repos size](https://img.shields.io/github/repo-size/schreiber-lab/reflectorch)\n[![CodeFactor](https://www.codefactor.io/repository/github/schreiber-lab/reflectorch/badge)](https://www.codefactor.io/repository/github/schreiber-lab/reflectorch)\n[![Jupyter Book Documentation](https://jupyterbook.org/badge.svg)](https://jupyterbook.org/)\n[![Documentation Page](https://img.shields.io/badge/Documentation%20Page-%23FFDD33.svg?style=flat&logo=read-the-docs&logoColor=black)](https://schreiber-lab.github.io/reflectorch/)\n<!-- [![Code style: Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff) -->\n\n\n**Reflectorch** is a machine learning Python package for the analysis of X-ray and neutron reflectometry data, written by [Vladimir Starostin](https://github.com/StarostinV/) & [Valentin Munteanu](https://github.com/valentinsingularity) at the University of Tübingen. It provides functionality for the fast simulation of reflectometry curves on the GPU, customizable setup of the physical parameterization model and neural network architecture via YAML configuration files, and prior-aware training of neural networks as described in our paper [Neural network analysis of neutron and X-ray reflectivity data incorporating prior knowledge](https://doi.org/10.1107/S1600576724002115).\n\n## Installation\n\n**Reflectorch** can be installed from [![PyPi](https://img.shields.io/badge/PyPi-3776AB.svg?style=flat&logo=pypi&logoColor=white)](https://pypi.org/project/reflectorch/) via ``pip``:\n\n<!-- or from [![conda-forge](https://img.shields.io/badge/conda--forge-44A833.svg?style=flat&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/reflectorch/) via ``conda``: -->\n\n```bash\npip install reflectorch\n```\n\n<!-- or\n\n```bash\nconda install -c conda-forge reflectorch\n``` -->\n\nAlternatively, one can clone the entire Github repository and install the package in editable mode:\n\n```bash\ngit clone https://github.com/schreiber-lab/reflectorch.git\npip install -e .\n```\n\nFor development purposes, the package can be installed together with the optional dependencies for building the distribution, testing and documentation:\n\n```bash\ngit clone https://github.com/schreiber-lab/reflectorch.git\npip install -e .[tests,docs,build]\n```\n\nUsers with Nvidia **GPU**s need to additionally install **Pytorch with CUDA support** corresponding to their hardware and operating system according to the instructions from the [Pytorch website](https://pytorch.org/get-started/locally/)\n\n## Get started\n\n[![Documentation Page](https://img.shields.io/badge/Documentation%20Page-%23FFDD33.svg?style=flat&logo=read-the-docs&logoColor=black)](https://schreiber-lab.github.io/reflectorch/)\n The full documentation of the package, containing tutorials and the API reference, was built with [Jupyter Book](https://jupyterbook.org/) and [Sphinx](https://www.sphinx-doc.org) and it is hosted at the address: [https://schreiber-lab.github.io/reflectorch/](https://schreiber-lab.github.io/reflectorch/).\n\n[![Interactive Notebook](https://img.shields.io/badge/Interactive%20Notebook-%23F9AB00.svg?style=flat&logo=google-colab&logoColor=black)](https://colab.research.google.com/drive/1rf_M8S_5kYvUoK0-9-AYal_fO3oFl7ck?usp=sharing)\nWe provide an interactive Google Colab notebook for exploring the basic functionality of the package: [![Explore reflectorch in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rf_M8S_5kYvUoK0-9-AYal_fO3oFl7ck?usp=sharing)<br>\n\n[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-%23FFD700.svg?style=flat&logo=huggingface&logoColor=black)](https://huggingface.co/valentinsingularity/reflectivity)\nConfiguration files and the corresponding pretrained model weights are hosted on Huggingface: [https://huggingface.co/valentinsingularity/reflectivity](https://huggingface.co/valentinsingularity/reflectivity).\n\n[![Docker](https://img.shields.io/badge/Docker-2496ED.svg?style=flat&logo=docker&logoColor=white)](https://hub.docker.com/)\nDocker images for reflectorch *will* be hosted on Dockerhub.\n\n\n## Citation\nIf you find our work useful in your research, please cite as follows:\n```\n@Article{Munteanu2024,\n  author    = {Munteanu, Valentin and Starostin, Vladimir and Greco, Alessandro and Pithan, Linus and Gerlach, Alexander and Hinderhofer, Alexander and Kowarik, Stefan and Schreiber, Frank},\n  journal   = {Journal of Applied Crystallography},\n  title     = {Neural network analysis of neutron and X-ray reflectivity data incorporating prior knowledge},\n  year      = {2024},\n  issn      = {1600-5767},\n  month     = mar,\n  number    = {2},\n  volume    = {57},\n  doi       = {10.1107/s1600576724002115},\n  publisher = {International Union of Crystallography (IUCr)},\n}\n```\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools>=61\", \"setuptools-scm\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"reflectorch\"\nversion = \"1.2.1\"\nauthors = [\n    {name = \"Vladimir Starostin\", email=\"vladimir.starostin@uni-tuebingen.de\"},\n    {name = \"Valentin Munteanu\", email=\"valentin.munteanu@uni-tuebingen.de\"}\n]\nmaintainers = [\n    {name = \"Valentin Munteanu\", email=\"valentin.munteanu@uni-tuebingen.de\"},\n    {name = \"Vladimir Starostin\", email=\"vladimir.starostin@uni-tuebingen.de\"},\n    {name = \"Alexander Hinderhofer\", email=\"alexander.hinderhofer@uni-tuebingen.de\"}\n]\ndescription = \"A Pytorch-based package for the analysis of reflectometry data\"\nkeywords = [\"reflectometry\", \"machine learning\"]\nreadme = \"README.md\"\nlicense = {file = \"LICENSE\"}\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"Operating System :: OS Independent\",\n    \"Environment :: GPU :: NVIDIA CUDA\",\n    \"License :: OSI Approved :: GNU General Public License v3 (GPLv3)\",\n    \"Development Status :: 4 - Beta\",\n    \"Topic :: Scientific/Engineering :: Physics\",\n    \"Intended Audience :: Science/Research\",\n]\nrequires-python = \">=3.7\"\ndependencies = [\n    \"numpy>=1.18.1,<2.0\",\n    \"torch>=1.8.1\",\n    \"scipy\",\n    \"tqdm\",\n    \"PyYAML\",\n    \"click\",\n    \"matplotlib\",\n    \"ipywidgets\",\n    \"huggingface_hub\",\n    \"safetensors\",\n]\n\n[project.optional-dependencies]\ntests = [\"pytest\", \"pytest-cov\"]\ndocs = [\"jupyter-book\", \"sphinx\"]\nbuild = [\"build\", \"twine\"]\n\n[project.urls]\nSource = \"https://github.com/schreiber-lab/reflectorch/\"\nIssues = \"https://github.com/schreiber-lab/reflectorch/issues\"\nDocumentation = \"https://schreiber-lab.github.io/reflectorch/\"\n\n[tool.setuptools]\npackages = [\"reflectorch\"]\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/remix",
            "repo_link": "https://gitlab.com/dlr-ve/esy/remix/framework",
            "content": {
                "codemeta": "",
                "readme": "# REMix\n\nREMix is addressing research questions in the field of energy system analysis.\nThe main focus is on the broad techno-economical assessment of possible future\nenergy system designs and analysis of interactions between technologies. This\nwill allow system analysts to inform policy makers and technology researchers\nto gain a better understanding of both the system and individual components.\n\nThe documentation of REMix is hosted online: [REMix docu](https://dlr-ve.gitlab.io/esy/remix/framework/).\n\nDo not hesitate to ask questions about REMix in the [openmod forum](https://forum.openmod.org/tag/remix).\n\n## Key Features\n\nTo know if REMix is apt for your project take into account these key features:\n\n**Large Models**:\nREMix is developed with large models in mind.\nThis means high spatial and technological resolutions.\n\n**Path Optimization**:\nMulti-year analyses are built into the framework.\n\n**Custom accounting approaches**:\nThe indicator module allows for a very flexible definition of what contributes\nto the objective functions.\n\n**Flexible modeling**:\nThere is not a single way of modeling technologies in REMix.\nWith the flexible [modelling concept](https://dlr-ve.gitlab.io/esy/remix/framework/dev/documentation/modeling-concept/index.html) you can find the best way of integrating your modeling needs.\n\n**Multi-criteria optimization**:\nApart from running a cost minimization, also other criteria like ecological or\nresilience indicators can be taken into account in the objective function.\n\n## How to use\n\nTo run a REMix model, users need a recent [GAMS](https://www.gams.com/)\ninstallation (version 37 or above).\n\nTo use REMix, clone this repository, create a new Python environment and\ninstall remix.framework through pip.\n\nEither clone with ssh:\n\n```bash\ngit clone git@gitlab.com:dlr-ve/esy/remix/framework.git\n```\n\nOr clone with https:\n\n```bash\ngit clone https://gitlab.com/dlr-ve/esy/remix/framework.git\n```\n\nFor the installation of the `remix.framework` package, you have two options:\n\n1. install from PyPI:\n\n```bash\nmamba create -n remix-env python  # make sure the Python version matches your GAMS version\nmamba activate remix-env\npip install remix.framework\n```\n\n2. install from the cloned repository:\n\n```bash\ncd framework\nconda create -n remix-env python\nconda activate remix-env\npip install -e .[dev]\n```\n\nPlease find the extensive [installation instructions in the online documentation](https://dlr-ve.gitlab.io/esy/remix/framework/dev/getting-started/install-remix.html).\n\nAdditionally, a data project is required which contains the parametrization of\nthe model scope and technologies. We provide\n[example projects](https://gitlab.com/dlr-ve/esy/remix/projects), which can be used\nto gain first experience with running the REMix optimization model.\n\nTo run your model, you can use the command line interface:\n\n```bash\nconda activate remix-env\nremix run --datadir=/path/to/data/folder/of/your/model/data\n```\n\nAll configuration options available with the command line tool are documented\nin the [technical documentation](https://dlr-ve.gitlab.io/esy/remix/framework/dev/documentation/tech-docs/index.html).\n\n## Citing REMix\n\n* [Wetzel et al. (2024): \"REMix: A GAMS-based framework for optimizing energy system models\"](https://doi.org/10.21105/joss.06330)\n\n\n## Latest publications using REMix\n\n* [Nitsch et al. (2024): \"The future role of Carnot batteries in Central Europe: Combining energy system and market perspective\"](https://doi.org/10.1016/j.est.2024.110959)\n* [Wetzel et al. (2023): \"Green energy carriers and energy sovereignty in a climate neutral European energy system\"](https://doi.org/10.1016/j.renene.2023.04.015)\n* [Gils et al. (2022): \"Model-related outcome differences in power system models with sector coupling - quantification and drivers\"](https://doi.org/10.1016/j.rser.2022.112177)[^1]\n* [Gils et al. (2021): \"Interaction of hydrogen infrastructures with other sector coupling options towards a zero-emission energy system in Germany\"](https://doi.org/10.1016/j.renene.2021.08.016)[^1]\n* [Sasanpour et al. (2021): \"Strategic policy targets and the contribution of hydrogen in a 100% renewable European power system\"](https://doi.org/10.1016/j.egyr.2021.07.005)[^1]\n\n## Contribute to REMix\n\nContributions are welcome, and they are greatly appreciated! Every bit\nhelps, and credit will always be given. To learn how to contribute to REMix we\nhave included a respective section in our\n[online documentation](https://dlr-ve.gitlab.io/esy/remix/framework/dev/contributing/index.html).\n\n## Acknowledgments\n\nThe preparation of the open source version of REMix was financed by the Helmholtz Association's Energy System Design research programme, which also enables the continuous maintenance of the framework. The methodological and content-related development of REMix was made possible by funding from the German Federal Ministries for Economic Affairs and Climate Protection (BMWK) and for Education and Research (BMBF) as part of the projects UNSEEN (BMWK, FKZ 03EI1004A), Sesame Seed (BMWK, FKZ 03EI1021B), Fahrplan Gaswende (BMWK, FKZ 03EI1030B), ReMoDigital (BMWK, FKZ 03EI1020B), and HINT (BMBF, FKZ 03SF0690) as well as the DLR-internal projects NaGsys and CarnotBat, which were also funded by the Helmholtz Association's Energy System Design research programme. The development of earlier REMix versions, which provided the basis for the published version, was made possible by funding from the projects MuSeKo (BMWK, FKZ 03ET4038B), INTEEVER-II (BMWK, FKZ 03ET4069A), START (BMBF, FKZ 03EK3046D), BEAM-ME (BMWK, FKZ 03ET4023A), INTEEVER (BMWK, FKZ 03ET4020A), Plan-DelyKaD (BMWK, FKZ 0325501), “Lastausgleich” (BMWK, FKZ 0328009), and “Elektromobilitaet” (BMWK, FKZ 0328005A) as well as the Helmholtz Association's Energy System Design research programme and its predecessors.\n\n## Footnotes\n\n[^1]: These papers were still using a non-open legacy version of the REMix framework\n\n",
                "dependencies": "[project]\nname = \"remix.framework\"\nkeywords = []\nreadme = \"README.md\"\nauthors = [\n    {name = \"REMix Developers\", email = \"remix@dlr.de\"},\n]\nlicense = {file = \"LICENSE\"}\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: BSD License\",\n    \"Operating System :: Microsoft :: Windows\",\n    \"Operating System :: POSIX\",\n    \"Operating System :: Unix\",\n    \"Programming Language :: Python\",\n    \"Programming Language :: Python :: 3 :: Only\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering\",\n]\nrequires-python = \">=3.9\"\ndependencies = [\n    \"jsonschema\",\n    \"networkx\",\n    \"numpy>=1.0.0,<2.0.0\",\n    \"pandas\",\n    \"pyyaml\",\n    \"requests\",\n    \"typer\",\n]\ndynamic = [\"version\", \"description\"]\n\n[project.urls]\nHomepage = \"https://gitlab.com/dlr-ve/esy/remix/framework\"\nRepository = \"https://gitlab.com/dlr-ve/esy/remix/framework.git\"\nDocumentation = \"https://dlr-ve.gitlab.io/esy/remix/framework/\"\nChangelog = \"https://gitlab.com/dlr-ve/esy/remix/framework/-/releases\"\n\n[project.optional-dependencies]\ndev = [\n    \"remix.framework[plotting]\",\n    \"black\",\n    \"build\",\n    \"coverage\",\n    \"flake8\",\n    \"isort\",\n    \"py\",\n    \"pytest\",\n    \"pytest-cov\",\n    \"twine\",\n]\npolars = [\n    \"polars\",\n]\nexcel = [\n    \"openpyxl\",\n]\nplotting = [\n    \"cartopy\",\n    \"fiona\",\n    \"shapely\",\n    \"ipykernel\",\n    \"matplotlib\",\n]\ntutorials = [\n    \"remix.framework[plotting]\",\n    ]\nall = [\n    \"remix.framework[dev]\",\n    \"remix.framework[polars]\",\n    \"remix.framework[excel]\",\n    \"remix.framework[plotting]\",\n    \"remix.framework[docs]\",\n]\ndocs = [\n    \"remix.framework[plotting]\",\n    \"jupytext\",\n    \"myst_parser\",\n    \"nbconvert\",\n    \"pandoc\",\n    \"pydata-sphinx-theme<=0.10.1\",\n    \"sphinx<7\",\n    \"sphinx-autobuild\",\n    \"sphinx-copybutton\",\n    \"sphinxcontrib-jquery\",\n    \"sphinx-design\",\n    \"sphinxcontrib.bibtex\",\n]\n\n[project.scripts]\nremix = \"remix.framework.cli.main:program\"\n\n[project.entry-points.\"remix.plugins\"]\nrun = \"remix.framework.cli.run:program_run\"\ntest = \"remix.framework.cli.test:program_test\"\ntransform = \"remix.framework.cli.transform:program_transform\"\nbuild_schemas = \"remix.framework.cli.build_schemas:program_build_schemas\"\n\n[build-system]\nrequires = [\"flit_core >=3.2,<4\"]\nbuild-backend = \"flit_core.buildapi\"\n\n[tool.isort]\nforce_single_line = true\nline_length = 120\nknown_first_party = \"remix\"\ndefault_section = \"THIRDPARTY\"\nforced_separate = \"test_remix\"\nskip = [\n    \".eggs\",\n    \"build\",\n    \"dist\"\n]\n\n[tool.pytest.ini_options]\npythonpath = [\n  \"remix\"\n]\n\n[tool.flit.sdist]\nexclude = [\n    \"docs/_build\",\n    \"*/**/*.gdx\",\n    \"*/**/diff.json\",\n    \"*/**/check.json\",\n    \"*/**/*.lst\",\n    \"*/**/cplex.o*\",\n    \"*/**/*.log\",\n    \"*/**/*.gsp\",\n    \"tutorials/**/data/\"\n]\ninclude = [\n    \"LICENSE*\",\n    \"docs/\",\n    \"testing/\",\n    \"tutorials/\",\n    \"*/**/valid.json\"\n]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/restore",
            "repo_link": "https://github.com/ReStoreCpp/ReStore",
            "content": {
                "codemeta": "",
                "readme": "# ReStore: In-Memory REplicated STORagE for Rapid Recovery in Fault-Tolerant Algorithms\n\nFault-tolerant distributed applications require mechanisms to recover data lost via a process failure.\nOn modern cluster systems it is typically impractical to request replacement resources after such a failure.\nTherefore, applications have to continue working with the remaining resources.\nThis requires redistributing the workload and that the non-failed processes reload the lost data.\nReStore is a C++ header-only library for MPI programs that enables recovery of lost data after (a) process failure(s).\nBy storing all required data in memory via an appropriate data distribution and replication, recovery is substantially faster than with standard checkpointing schemes that rely on a parallel file system.\nAs you as the application programmer can specify which data to load ReStore also supports shrinking recovery instead of recovery using spare compute nodes.\n\n## Including ReStore into your application\n\nTo use ReStore, first add the repository as a submodule into your project:\n```Bash\ngit submodule add --recursive https://github.com/ReStoreCpp/ReStore.git extern/ReStore\n```\n\nThen, include the following into your CMakeLists.txt:\n```CMake\n# Configure and link ReStore\nset(ReStore_BUILD_TESTS Off)\nset(ReStore_BUILD_BENCHMARKS Off)\nset(ReStore_ID_RANDOMIZATION On)\n\nadd_subdirectory(extern/ReStore)\ntarget_link_libraries(${YOUR_TARGETS_NAME} ReStore)\n```\n\nYou can use ID-randomization to break up access patterns in your `load` requests.\nIf enabled, the block IDs you provide will be permuted using a pseudorandom-projection.\nIf you then for example access a range of consecutive blocks IDs, e.g. after the PE which worked on these IDs failed; more PEs will be able to serve the request, resulting in a speedup.\nIf you request most or all of the data `submitted` in each `load`, turning ID-randomization of will be faster.\nSee Hespe and Hübner et al. (2022) [1] for details.\n\n## Code examples\n\n### The general use case\n\nThis example shows the general usage of ReStore.\n\n```cpp\n#include <core.hpp>\n\n// First, create the restore object.\nReStore::ReStore<YourAwesomeDatatype> store(\n    MPI_COMM_WORLD, // MPI communicator to use. ULFM currently supports only MPI_COMM_WORLD.\n    4,              // Replication level, 3 or 4 are sane defaults.\n    ReStore::OffsetMode::constant, // Currently, the only supported mode.\n    sizeof(YourAwesomeDatatype)    // Your block size, use at least 64 bytes.\n);\n\n// Next, submit you data to the ReStore, if a failure happened between creation of the ReStore\n// and the submission of the data, please re-create the ReStore.\nReStore::block_id_t localBlockId = 0;\nstore.submitBlocks(\n    // The serialization function; your can stream your data to the provided stream using\n    // the << operator.\n    [](const YourAwesomeDatatype& value, ReStore::SerializedBlockStoreStream& stream) {\n        // Either use:\n        stream << value;\n        // or, for big, already consecutively stored data:\n        stream.writeBytes(constBytePtr, sizeof(YourAwesomeDatatype));\n        },\n    // The enumerator function; should return nullopt if there are no more blocks to submit\n    // on this PE.\n    [localBlockId, ...]() {\n        auto ret = numberOfBlocksOnThisPE == localBlockId\n                        ? std::nullopt\n                        : std::make_optional(ReStore::NextBlock<YourAwesomeDatatype>(\n                            {globalBlockId(localBlockId), constRefToYourDataForThisBlock}));\n        localBlockId++;\n        return ret;\n    },\n    globalNumberOfBlocks\n);\n\n// A failure occurred; set ReStore's communicator to the fixed communicator obtained by\n// MPIX_Comm_shrink()\nstore.updateComm(newComm);\n\n// Next, request the data you need on each PE.\n// requestedBlocks is of type\n// std::vector<std::pair<ReStore::block_id_t, size_t>>\n// [ (firstBlockIdOfRange1, numberOfBlocks1), (firstBlockIdOfRange2, numberOfBlocks2), ...]\nstore.pullBlocks(\n    requestedBlocks,\n    //  De-serialization function.\n    [...] (const std::byte* dataPtr, size_t size, ReStore::block_id_t blockId) {\n        // ...\n});\n\n```\n### Data stored in a std::vector\n\nIf your data resides in a `std::vector`, you can use the ReStore-provided wrapper.\n\n```cpp\n#include <restore/core.hpp>\n#include <restore/restore_vector.hpp>\n\n// Create the ReStoreVector wrapper.\nReStore::ReStoreVector<YourAwesomeDatatype>> reStoreVectorWrapper(\n    blockSizeInBytes, // Can for example be used to group all dimensions of a single data point.\n    MPI_COMM_WORLD,\n    replicationLevel,\n    blocksPerPermutationRange, // defaults to 4096\n    paddingValue, // The value used to pad the data; defaults to 0\n);\n\n// Submit your data to the ReStore.\nconst auto numBlocksLocal = reStoreVectorWrapper->submitData(referenceToYourDataVector);\n\n// After a failure\nreStoreVectorWrapper.updateComm(newComm); // see above\n\nreStoreVectorWrapper.restoreDataAppendPullBlocks(\n    referenceToVectorContainingYourData, // ReStore will append the new data points at the end.\n    requestedBlocks, // see above\n);\n```\n\n### A simple load-balancer\n\nYou can use the ReStore-provided LoadBalancer.\nIf a PE fails, it will help you with calculating the new distribution of blocks to PEs.\nEach surviving PE will get an equal share of the blocks residing on each PE that failed.\nThis of course works for multiple rounds of failing PEs, too.\n\n```cpp\n#include <restore/core.hpp>\n#include <restore/equal_load_balancer.hpp>\n\n// Describes, which block range (firstBlockId, numberOfBlocks) resides on which PE.\nusing BlockRange     = std::pair<std::pair<ReStore::block_id_t, size_t>, ReStoreMPI::original_rank_t>;\nusing BlockRangeList = std::vector<BlockRange>;\n\n// Create the LoadBalancer object.\nReStore::EqualLoadBalancer loadBalancer(blockRangeList, numberOfPEs)\n\n// After a failure, let the LoadBalancer decide which PE gets which data points:\nconst auto newBlocks = _loadBalancer.getNewBlocksAfterFailureForPullBlocks(\n    ranksDiedSinceLastCall, myRankWhenCreatingTheLoadBalancer\n);\n// You can hand newBlocks to restore.pullBlocks() or the ReStoreVector wrapper.\n\n// If everyone completed the restoration successfully, we can commit to the new data distribution. If\n// there was another PE failure in the meantime, you can re-call getNewBlocksAfterFailureForPullBlocks.\n_loadBalancer.commitToPreviousCall();\n\n// Further failures, repeat the above steps.\n```\n\n## Publication\nIf you use ReStore in your research, please cite the following paper:\n\n```bibtex\n@inproceedings{restore,\n  author={Hübner, Lukas and Hespe, Demian and Sanders, Peter and Stamatakis, Alexandros},\n  booktitle={2022 IEEE/ACM 12th Workshop on Fault Tolerance for HPC at eXtreme Scale (FTXS)}, \n  title={ReStore: In-Memory REplicated STORagE for Rapid Recovery in Fault-Tolerant Algorithms}, \n  year={2022},\n  volume={},\n  number={},\n  pages={24-35},\n  doi={10.1109/FTXS56515.2022.00008}\n}\n```\n\n\n",
                "dependencies": "cmake_minimum_required(VERSION 3.16)\n\nif(\"${CMAKE_SOURCE_DIR}\" STREQUAL \"${CMAKE_CURRENT_SOURCE_DIR}\")\n    set(IS_SUBPROJECT OFF)\n    message(STATUS \"Building ReStore as a stand-alone project.\")\nelse()\n    set(IS_SUBPROJECT ON)\n    message(STATUS \"Building ReStore as a subbroject.\")\nendif()\n\n# Use ccache if available\nfind_program(CCACHE_PROGRAM ccache)\nif((NOT ${IS_SUBPROBJECT}) AND ${CCACHE_PROGRAM})\n    message(STATUS \"Found ccache; using it to speed up compilation.\")\n    set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE \"${CCACHE_PROGRAM}\")\nendif()\n\n# Project settings\nproject(ReStore VERSION 0.1.0 LANGUAGES CXX)\n\nset(CMAKE_CXX_STANDARD 17)\n\n# Find and link MPI\nfind_package(MPI REQUIRED)\nif(MPI_CXX_FOUND)\n  include_directories(BEFORE SYSTEM ${MPI_CXX_INCLUDE_PATH})\n  link_libraries(${MPI_LIBRARIES})\nendif()\n\n# Find and link threading library (e.g. pthreads)\nset(THREADS_PREFER_PTHREAD_FLAG ON)\nfind_package(Threads REQUIRED)\nlink_libraries(Threads::Threads)\n\n# Make additinal cmake modules findable\nlist(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\")\nlist(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/extern/sanitizers-cmake/cmake\")\n\n# Require out-of-source builds\ninclude(require_out_of_source_builds)\nrequire_out_of_source_builds()\n\ninclude(CMakeDependentOption)\n#include(CMakePackageConfigHelpers)\ninclude(CTest)\ninclude(GNUInstallDirs)\n\n# Load (memory, adress, ...) sanitizer support\ninclude(sanitizers)\n\n# Enable sorting targets (of external libraries) into folders\nset_property(GLOBAL PROPERTY USE_FOLDERS ON)\n\n# Are we building in DEBUG mode?\nif(CMAKE_BUILD_TYPE MATCHES Debug)\n  message(\"!!!!! Building in DEBUG mode !!!!!\")\n  # Enable extra checks in the STL\n  add_definitions(-D_GLIBCXX_DEBUG)\n  add_definitions(-D_GLIBCXX_DEBUG_PEDANTIC)\nendif(CMAKE_BUILD_TYPE MATCHES Debug)\n\noption(${PROJECT_NAME}_INSTALL \"Add ${PROJECT_NAME} to the install list\" ON)\n\n# Options to disable building the tests, examples, benchmarks, ...\noption(${PROJECT_NAME}_BUILD_TESTS \"Build unit tests\" ON)\noption(${PROJECT_NAME}_BUILD_BENCHMARKS \"Build benchmarks\" ON)\n\n# Other build options\noption(${PROJECT_NAME}_SIMULATE_FAILURES \"Simulate node failues (for example when running unit test).\" OFF)\noption(${PROJECT_NAME}_USE_FTMPI \"Use a fault-tolerant MPI implementation\" ON)\noption(${PROJECT_NAME}_ID_RANDOMIZATION \"Use randomization of block IDs.\" ON)\noption(${PROJECT_NAME}_WARNINGS_ARE_ERRORS \"Treat warnings as errors.\" OFF)\n\n# Output config\nif(NOT ${PROJECT_NAME}_SIMULATE_FAILURES AND NOT ${PROJECT_NAME}_USE_FTMPI)\n    message(FATAL_ERROR \"You have to either use a fault-tolerant MPI implementation or simulated failures (or both).\")\nendif()\n\nmessage(STATUS \"Using simulated failures: ${${PROJECT_NAME}_SIMULATE_FAILURES}\")\nmessage(STATUS \"Using a fault-tolerant MPI implementation: ${${PROJECT_NAME}_USE_FTMPI}\")\nmessage(STATUS \"Using block id randomization: ${${PROJECT_NAME}_ID_RANDOMIZATION}\")\n\n# Extra options when building tests\ncmake_dependent_option(\"${PROJECT_NAME}_SYSTEM_GTEST\" \"Use googletest version installed on the system\" OFF \"${PROJECT_NAME}_BUILD_TESTS\" OFF)\n\n# Library specific settings\nset(\"NEEDS_GOOGLETEST\" ${PROJECT_NAME}_BUILD_TESTS OR ${PROJECT_NAME}_BUILD_BENCHMARKS)\nset(\"NEEDS_CPPITERTOOLS\" ${PROJECT_NAME}_BUILD_TESTS OR ${PROJECT_NAME}_BUILD_BENCHMARKS)\nset(\"NEEDS_CXXOPTS\" ${PROJECT_NAME}_BUILD_BENCHMARKS)\nset(\"NEEDS_GOOGLEBENCHMARK\" ${PROJECT_NAME}_BUILD_BENCHMARKS)\nset(\"NEEDS_BACKWARDCPP\" ${PROJECT_NAME}_BUILD_TESTS OR ${PROJECT_NAME}_BUILD_BENCHMARKS)\nset(\"NEEDS_XXHASH\" ON)\n\nif(NEEDS_GOOGLETEST)\n    message(STATUS \"Using googletest library.\")\n    set(\"${PROJECT_NAME}_GTEST_DIR\" \"${CMAKE_CURRENT_LIST_DIR}/extern/googletest\" CACHE PATH \"Path to the googletest source directory\")\nendif()\n\nif(NEEDS_GOOGLEBENCHMARK)\n    message(STATUS \"Using googlebenchmark library.\")\n    set(\"${PROJECT_NAME}_GBENCHMARK_DIR\" \"${CMAKE_CURRENT_LIST_DIR}/extern/googlebenchmark\" CACHE PATH \"Path to the googlebenchmark source directory\")\nendif()\n\nif(NEEDS_CPPITERTOOLS)\n    message(STATUS \"Using cppitertools library.\")\n    set(\"${PROJECT_NAME}_CPPITERTOOLS_DIR\" \"${CMAKE_CURRENT_LIST_DIR}/extern/cppitertools\" CACHE PATH \"Path to the cppitertools source directory\")\nendif()\n\nif(NEEDS_CXXOPTS)\n    message(STATUS \"Using cxxopts library.\")\n    set(\"${PROJECT_NAME}_CXXOPTS_DIR\" \"${CMAKE_CURRENT_LIST_DIR}/extern/cxxopts\" CACHE PATH \"Path to the cxxopts source directory\")\nendif()\n\nif(NEEDS_BACKWARDCPP)\n    message(STATUS \"Using backwardcpp library.\")\n    set(\"${PROJECT_NAME}_BACKWARDCPP_DIR\" \"${CMAKE_CURRENT_LIST_DIR}/extern/backward-cpp\" CACHE PATH \"Path to the backward-cpp source directory\")\n    option(${PROJECT_NAME}_BACKWARD_ENABLED \"Enable pretty printing of stack traces when a test case fails.\" ON)\nelse()\n    option(${PROJECT_NAME}_BACKWARD_ENABLED \"Enable pretty printing of stack traces when a test case fails.\" OFF)\nendif()\n\n# Organize targets into folders\nset_property(GLOBAL PROPERTY USE_FOLDERS ON)\n\n# The target to be linked against by other targets. This library is an\n# interface target and as such does not generate any artefacts. It rather sets\n# include directories and required compiler flags.\nadd_library(\"${PROJECT_NAME}\" INTERFACE)\ntarget_sources(\"${PROJECT_NAME}\" INTERFACE ${CMAKE_CURRENT_SOURCE_DIR}/include)\ntarget_include_directories(\"${PROJECT_NAME}\" INTERFACE ${CMAKE_CURRENT_SOURCE_DIR}/include)\n\n### Compiler and linker settings ###\n# This interface reflects the requirements to the compiler for building targets\n# linking against.\nadd_library(\"${PROJECT_NAME}_compile_requirements\" INTERFACE)\n# We just want c++17 or above\ntarget_compile_features(\n    \"${PROJECT_NAME}_compile_requirements\" INTERFACE\n    cxx_std_17\n)\ntarget_link_libraries(\"${PROJECT_NAME}\" INTERFACE \"${PROJECT_NAME}_compile_requirements\")\n\n# The namespace alias can be used as link target if this project is a\n# subproject.\nadd_library(\"${PROJECT_NAME}::${PROJECT_NAME}\" ALIAS \"${PROJECT_NAME}\")\n\n# Not added as link target to avoid propagation of warning\n# flags. Only to be used by internal targets that compile the library.\nadd_library(\"${PROJECT_NAME}_warnings\" INTERFACE)\n\n# TODO: Use CheckCXXCompilerFlag for this?\nlist(\n    APPEND WARNING_FLAGS\n    \"-Wall\"\n    \"-Wextra\"\n    \"-Wconversion\"\n    \"-Wnon-virtual-dtor\"\n    \"-Woverloaded-virtual\"\n    \"-Wshadow\"\n    \"-Wsign-conversion\"\n    \"-Wundef\"\n    \"-Wunreachable-code\"\n    \"-Wunused\"\n)\n\nif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n    list(\n        APPEND WARNING_FLAGS\n        \"-Wcast-align\"\n        \"-Wpedantic\"\n    )\nendif()\n\nif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n    list(\n        APPEND WARNING_FLAGS\n        \"-Wcast-align\"\n        \"-Wpedantic\"\n        \"-Wnoexcept\"\n        \"-Wsuggest-attribute=const\"\n        \"-Wsuggest-attribute=noreturn\"\n        \"-Wsuggest-override\"\n    )\nendif()\n\nif(${PROJECT_NAME}_WARNINGS_ARE_ERRORS)\n  list(\n    APPEND WARNING_FLAGS\n    \"-Werror\"\n    )\nendif()\n\ntarget_compile_options(\n    \"${PROJECT_NAME}_warnings\" INTERFACE\n    $<BUILD_INTERFACE:${WARNING_FLAGS}>\n)\n\n# Populate some of the options to the source code\nif (${PROJECT_NAME}_SIMULATE_FAILURES) \n    target_compile_definitions(${PROJECT_NAME}_compile_requirements INTERFACE SIMULATE_FAILURES)\nendif()\n\nif (${PROJECT_NAME}_USE_FTMPI)\n    target_compile_definitions(${PROJECT_NAME}_compile_requirements INTERFACE USE_FTMPI)\nendif()\n\nif (${PROJECT_NAME}_ID_RANDOMIZATION)\n    target_compile_definitions(${PROJECT_NAME}_compile_requirements INTERFACE ID_RANDOMIZATION)\n    target_compile_definitions(${PROJECT_NAME}_compile_requirements INTERFACE DENSE_ALL_TO_ALL_IN_SUBMIT_BLOCKS)\nendif()\n\n### Libraries ###\n# Load and include cppitertools       \nif(NEEDS_CPPITERTOOLS)\n    if(NOT EXISTS \"${${PROJECT_NAME}_CPPITERTOOLS_DIR}/CMakeLists.txt\")\n        message(FATAL_ERROR \"Could not find cppitertools in ${${PROJECT_NAME}_CPPITERTOOLS_DIR}\")\n    endif()\n    message(STATUS \"Configuring cppitertools...\")\n    set(\"ENV{cppitertools_INSTALL_CMAKE_DIR}\" \"share\") # default value; supress the warning message\n    add_subdirectory(\"${${PROJECT_NAME}_CPPITERTOOLS_DIR}\")\n    message(STATUS \"cppitertools configured.\")\nendif()\n\n# Load and include cxxopts       \nif(NEEDS_CXXOPTS)\n    if(NOT EXISTS \"${${PROJECT_NAME}_CXXOPTS_DIR}/CMakeLists.txt\")\n        message(FATAL_ERROR \"Could not find cxxopts in ${${PROJECT_NAME}_CXXOPTS_DIR}\")\n    endif()\n    message(STATUS \"Configuring cxxopts...\")\n    set(\"ENV{cxxopts_INSTALL_CMAKE_DIR}\" \"share\") # default value; supress the warning message\n    add_subdirectory(\"${${PROJECT_NAME}_CXXOPTS_DIR}\")\n    message(STATUS \"cxxopts configured.\")\nendif()\n\n# Load and include googletest\nif(NEEDS_GOOGLETEST)\n    if(${PROJECT_NAME}_SYSTEM_GTEST)\n        find_package(GTest REQUIRED)\n    else()\n        if(NOT EXISTS \"${${PROJECT_NAME}_GTEST_DIR}/CMakeLists.txt\")\n            message(FATAL_ERROR \"Could not find googletest in ${${PROJECT_NAME}_GTEST_DIR}\")\n        endif()\n        \n        message(STATUS \"Configuring googletest...\")\n        add_subdirectory(\"${${PROJECT_NAME}_GTEST_DIR}\" EXCLUDE_FROM_ALL)\n        list(APPEND CMAKE_MODULE_PATH \"${${PROJECT_NAME}_GTEST_DIR}/contrib\")\n        message(STATUS \"googletest configured.\")\n    endif()\n\n    include(GoogleTest)\n    set_target_properties(gtest gmock PROPERTIES FOLDER googletest)\nendif()\n\n# Load and include googlebenchmark\nif(NEEDS_GOOGLEBENCHMARK)\n    message(STATUS \"Configuring googlebenchmark...\")\n    set(BENCHMARK_ENABLE_TESTING OFF CACHE BOOL \"Suppressing benchmark's tests\" FORCE)\n    add_subdirectory(\"${${PROJECT_NAME}_GBENCHMARK_DIR}\" EXCLUDE_FROM_ALL)\n    set_target_properties(benchmark PROPERTIES FOLDER googlebenchmark)\n    message(STATUS \"googlebenchmark configured.\")\nendif()\n\n# Load and include backward-cpp\nif (NEEDS_BACKWARDCPP)\n    message(STATUS \"Configuring backward-cpp...\")\n    set(Backward_DIR \"${PROJECT_SOURCE_DIR}/extern/backward-cpp\")\n    find_package(Backward)\n    message(STATUS \"backward-cpp configured.\")\nendif()\n\n# Load and include xxHash\nmessage(STATUS \"Configuring xxHash...\")\nset(${PROJECT_NAME}_XXHASH_DIR \"${CMAKE_CURRENT_LIST_DIR}/extern/xxHash\" CACHE PATH \"Path to the xxHash source directory\")\nset(BUILD_SHARED_LIBS OFF)\nset(XXHASH_BUILD_ENABLE_INLINE_API ON)\nset(XXHASH_BUILD_XXHSUM OFF)\nadd_subdirectory(\"${${PROJECT_NAME}_XXHASH_DIR}/cmake_unofficial\" \"${${PROJECT_NAME}_XXHASH_DIR/build}\" EXCLUDE_FROM_ALL)\n\n# Link xxHash with ReStore\ntarget_link_libraries(\"${PROJECT_NAME}\" INTERFACE xxhash)\n\n### Subdirectories ###\n# Build the unit tests\nif(${PROJECT_NAME}_BUILD_TESTS)\n    message(STATUS \"Building unit tests: YES\")\n    set(TEST_RUNNER_PARAMS \"\" CACHE STRING \"Options added to the test runner\")\n    enable_testing()\n    add_subdirectory(\"${CMAKE_CURRENT_LIST_DIR}/tests\")\nelse()\n    message(STATUS \"Building unit tests: NO\")\nendif()\n\n# Build the benchmarks\nif(${PROJECT_NAME}_BUILD_BENCHMARKS)\n    message(STATUS \"Building benchmarks: YES\")\n    add_subdirectory(\"${CMAKE_CURRENT_LIST_DIR}/benchmark\")\nelse()\n    message(STATUS \"Building benchmarks: NO\")\nendif()\n\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rgreat",
            "repo_link": "https://github.com/jokergoo/rGREAT",
            "content": {
                "codemeta": "",
                "readme": "# GREAT Analysis - Functional Enrichment on Genomic Regions\n\n[![R-CMD-check](https://github.com/jokergoo/rGREAT/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/rGREAT/actions)\n[![codecov](https://img.shields.io/codecov/c/github/jokergoo/rGREAT.svg)](https://codecov.io/github/jokergoo/rGREAT)\n[![bioc](https://bioconductor.org/shields/downloads/devel/rGREAT.svg)](https://bioconductor.org/packages/stats/bioc/rGREAT/) \n[![bioc](http://www.bioconductor.org/shields/years-in-bioc/rGREAT.svg)](http://bioconductor.org/packages/devel/bioc/html/rGREAT.html)\n\n\n**GREAT** ([Genomic Regions Enrichment of Annotations Tool](http://great.stanford.edu)) is a type of\nfunctional enrichment analysis directly performed on genomic regions. This package \nimplements the GREAT algorithm (the local GREAT analysis), also it supports directly \ninteracting with the GREAT web service (the online GREAT analysis). Both analysis \ncan be viewed by a Shiny application.\n\n## Install\n\n**rGREAT** is available on Bioconductor (http://bioconductor.org/packages/devel/bioc/html/rGREAT.html)\n\n```r\nif(!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n    install.packages(\"BiocManager\")\n}\nBiocManager::install(\"rGREAT\")\n```\n\nIf you want the latest version, install it directly from GitHub:\n\n```r\nlibrary(devtools)\ninstall_github(\"jokergoo/rGREAT\")\n```\n\n## Citation\n\nZuguang Gu, et al., rGREAT: an R/Bioconductor package for functional enrichment on genomic regions. \nBioinformatics, https://doi.org/10.1093/bioinformatics/btac745\n\n## Online GREAT analysis\n\nWith online GREAT analysis, the input regions will be directly submitted to GREAT server, and the results\nare automatically retrieved from GREAT server.\n\n```r\nset.seed(123)\ngr = randomRegions(nr = 1000, genome = \"hg19\")\n\njob = submitGreatJob(gr)\ntbl = getEnrichmentTables(job)\n```\n\n## Local GREAT analysis\n\n**rGREAT** also implements the GREAT algorithms locally and it can be seamlessly integrated\nto the Bioconductor annotation ecosystem. This means, theoretically, with **rGREAT**, it is possible to perform GREAT analysis\nwith any organism and with any type of gene set collection / ontology\n\n```r\nres = great(gr, \"MSigDB:H\", \"TxDb.Hsapiens.UCSC.hg19.knownGene\")\ntb = getEnrichmentTable(res)\n```\n\nTo apply `great()` on other organisms, set the `biomart_dataset` argument:\n\n```r\n# giant panda\ngreat(gr, \"GO:BP\", biomart_dataset = \"amelanoleuca_gene_ensembl\")\n```\n\n## License\n\nMIT @ Zuguang Gu\n\n",
                "dependencies": "Package: rGREAT\nType: Package\nTitle: GREAT Analysis - Functional Enrichment on Genomic Regions\nVersion: 2.5.7\nDate: 2024-03-07\nAuthors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"),\n                  comment = c('ORCID'=\"0000-0002-7395-8709\"))\nDepends: R (>= 4.0.0), GenomicRanges, IRanges, methods\nImports: graphics, rjson, GetoptLong (>= 0.0.9), RCurl, utils, stats, GlobalOptions,\n    shiny, DT, GenomicFeatures, digest, GO.db, progress, circlize,\n    AnnotationDbi, TxDb.Hsapiens.UCSC.hg19.knownGene, TxDb.Hsapiens.UCSC.hg38.knownGene,\n    org.Hs.eg.db, RColorBrewer, S4Vectors, GenomeInfoDb, foreach, doParallel, Rcpp\nSuggests: testthat (>= 0.3), knitr, rmarkdown, BiocManager, org.Mm.eg.db, msigdbr,\n    KEGGREST, reactome.db\nEnhances: BioMartGOGeneSets, UniProtKeywords\nVignetteBuilder: knitr\nbiocViews: GeneSetEnrichment, GO, Pathways, Software, Sequencing, WholeGenome,\n    GenomeAnnotation, Coverage\nDescription: GREAT (Genomic Regions Enrichment of Annotations Tool) is a type of\n    functional enrichment analysis directly performed on genomic regions. This package \n    implements the GREAT algorithm (the local GREAT analysis), also it supports directly \n    interacting with the GREAT web service (the online GREAT analysis). Both analysis \n    can be viewed by a Shiny application. rGREAT by default supports more than 600 organisms \n    and a large number of gene set collections, as well as self-provided gene sets and \n    organisms from users. Additionally, it implements a general method for dealing \n    with background regions.\nURL: https://github.com/jokergoo/rGREAT, http://great.stanford.edu/public/html/\nLicense: MIT + file LICENSE\nLinkingTo: Rcpp\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ribodetector",
            "repo_link": "https://github.com/hzi-bifo/RiboDetector",
            "content": {
                "codemeta": "",
                "readme": "## RiboDetector - Accurate and rapid RiboRNA sequences Detector based on deep learning\n\n### About Ribodetector\n<img src=\"RiboDetector_logo.png\" width=\"600\" />\n\n`RiboDetector` is a software developed to accurately yet rapidly detect and remove rRNA sequences from metagenomeic, metatranscriptomic, and ncRNA sequencing data. It was developed based on LSTMs and optimized for both GPU and CPU usage to achieve a **10** times on CPU and **50** times on a consumer GPU faster runtime compared to the current state-of-the-art software. Moreover, it is very accurate, with ~**10** times fewer false classifications. Finally, it has a low level of bias towards any GO functional groups. \n\n\n### Prerequirements\n\n#### 1. Create `conda` env and install `Python v3.8`\n\nTo be able to use `RiboDetector`, you need to install `Python v3.8` or `v3.9` (make sure you have version `3.8` because `3.7` cannot serialize a string larger than 4GiB) with `conda`:\n```shell\nconda create -n ribodetector python=3.8\nconda activate ribodetector\n```\n\n#### 2. Install `pytorch` in the ribodetector env if GPU is available\n\nTo install `pytorch` compatible with your CUDA version, please fellow this instruction:\nhttps://pytorch.org/get-started/locally/. Our code was tested with `pytorch v1.7`, `v1.7.1`, `v1.10.2`.\n\nNote: you can skip this step if you don't use GPU\n\n### Installation\n\n#### Using pip\n\n```shell\npip install ribodetector\n```\n\n#### Using conda\n```shell\nconda install -c bioconda ribodetector\n```\n\n### Usage\n\n#### GPU mode\n\n#### Example\n```shell\nribodetector -t 20 \\\n  -l 100 \\\n  -i inputs/reads.1.fq.gz inputs/reads.2.fq.gz \\\n  -m 10 \\\n  -e rrna \\\n  --chunk_size 256 \\\n  -o outputs/reads.nonrrna.1.fq outputs/reads.nonrrna.2.fq\n```\nThe command lind above excutes ribodetector for paired-end reads with mean length 100 using GPU and 20 CPU cores. The input reads do not need to be same length. RiboDetector supports reads with variable length. Setting `-l` to the mean read length is recommended. \n\n#### Full help\n```shell\nusage: ribodetector [-h] [-c CONFIG] [-d DEVICEID] -l LEN -i [INPUT [INPUT ...]]\n  -o [OUTPUT [OUTPUT ...]] [-r [RRNA [RRNA ...]]] [-e {rrna,norrna,both,none}] \n  [-t THREADS] [-m MEMORY] [--chunk_size CHUNK_SIZE] [-v]\n\nrRNA sequence detector\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -c CONFIG, --config CONFIG\n                        Path of config file\n  -d DEVICEID, --deviceid DEVICEID\n                        Indices of GPUs to enable. Quotated comma-separated device ID numbers. (default: all)\n  -l LEN, --len LEN     Sequencing read length (mean length). Note: the accuracy reduces for reads shorter than 40.\n  -i [INPUT [INPUT ...]], --input [INPUT [INPUT ...]]\n                        Path of input sequence files (fasta and fastq), the second file will be considered \n                        as second end if two files given.\n  -o [OUTPUT [OUTPUT ...]], --output [OUTPUT [OUTPUT ...]]\n                        Path of the output sequence files after rRNAs removal (same number of files as input).\n                        (Note: 2 times slower to write gz files)\n  -r [RRNA [RRNA ...]], --rrna [RRNA [RRNA ...]]\n                        Path of the output sequence file of detected rRNAs (same number of files as input)\n  -e {rrna,norrna,both,none}, --ensure {rrna,norrna,both,none}\n                        Ensure which classificaion has high confidence for paired end reads.\n                        norrna: output only high confident non-rRNAs, the rest are clasified as rRNAs;\n                        rrna: vice versa, only high confident rRNAs are classified as rRNA and the rest output as non-rRNAs;\n                        both: both non-rRNA and rRNA prediction with high confidence;\n                        none: give label based on the mean probability of read pair.\n                              (Only applicable for paired end reads, discard the read pair when their predicitons are discordant)\n  -t THREADS, --threads THREADS\n                        number of threads to use. (default: 10)\n  -m MEMORY, --memory MEMORY\n                        Amount (GB) of GPU RAM. (default: 12)\n  --chunk_size CHUNK_SIZE\n                        Use this parameter when having low memory. Parsing the file in chunks.\n                        Not needed when free RAM >=5 * your_file_size (uncompressed, sum of paired ends).\n                        When chunk_size=256, memory=16 it will load 256 * 16 * 1024 reads each chunk (use ~20 GB for 100bp paired end).\n  --log LOG             Log file name\n  -v, --version         Show program's version number and exit\n```\n\n#### CPU mode\n\n#### Example\n```shell\nribodetector_cpu -t 20 \\\n  -l 100 \\\n  -i inputs/reads.1.fq.gz inputs/reads.2.fq.gz \\\n  -e rrna \\\n  --chunk_size 256 \\\n  -o outputs/reads.nonrrna.1.fq outputs/reads.nonrrna.2.fq\n```\nThe above command line excutes ribodetector for paired-end reads with mean length 100 using 20 CPU cores. The input reads do not need to be same length. RiboDetector supports reads with variable length. Setting `-l` to the mean read length is recommended. If you need to save the log into a file, you can specify it with `--log <logfile>`\n\nNote: when using **SLURM** job submission system, you need to specify `--cpus-per-task` to the number you CPU cores you need and set `--threads-per-core` to 1.\n\n#### Full help\n\n```shell\n\nusage: ribodetector_cpu [-h] [-c CONFIG] -l LEN -i [INPUT [INPUT ...]] \n  -o [OUTPUT [OUTPUT ...]] [-r [RRNA [RRNA ...]]] [-e {rrna,norrna,both,none}] \n  [-t THREADS] [--chunk_size CHUNK_SIZE] [-v]\n\nrRNA sequence detector\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -c CONFIG, --config CONFIG\n                        Path of config file\n  -l LEN, --len LEN     Sequencing read length (mean length). Note: the accuracy reduces for reads shorter than 40.\n  -i [INPUT [INPUT ...]], --input [INPUT [INPUT ...]]\n                        Path of input sequence files (fasta and fastq), the second file will be considered as \n                        second end if two files given.\n  -o [OUTPUT [OUTPUT ...]], --output [OUTPUT [OUTPUT ...]]\n                        Path of the output sequence files after rRNAs removal (same number of files as input).\n                        (Note: 2 times slower to write gz files)\n  -r [RRNA [RRNA ...]], --rrna [RRNA [RRNA ...]]\n                        Path of the output sequence file of detected rRNAs (same number of files as input)\n  -e {rrna,norrna,both,none}, --ensure {rrna,norrna,both,none}\n                        Ensure which classificaion has high confidence for paired end reads.\n                        norrna: output only high confident non-rRNAs, the rest are clasified as rRNAs;\n                        rrna: vice versa, only high confident rRNAs are classified as rRNA and the rest output as non-rRNAs;\n                        both: both non-rRNA and rRNA prediction with high confidence;\n                        none: give label based on the mean probability of read pair.\n                              (Only applicable for paired end reads, discard the read pair when their predicitons are discordant)\n  -t THREADS, --threads THREADS\n                        number of threads to use. (default: 20)\n  --chunk_size CHUNK_SIZE\n                        chunk_size * 1024 reads to load each time.\n                        When chunk_size=1000 and threads=20, consumming ~20G memory, better to be multiples of the number of threads..\n  --log LOG             Log file name\n  -v, --version         Show program's version number and exit\n```\n\n**Note**: RiboDetector uses multiprocessing with shared memory, thus the memory use of a single process indicated in `htop` or `top` is actually the total memory used by RiboDector. Some job submission system like SGE mis-calculated the total memory use by adding up the memory use of all process. If you see this do not worry it will cause out of memory issue. \n\n<!-- ### Benchmarks\n\nWe benchmarked five different rRNA detection methods including RiboDetector on 8 benchmarking datasets as following: \n\n- 20M paired end reads simulated based on  rRNA sequences from Silva database, those sequences are distinct from sequences used for training and validation.\n\n- 20M paired end reads simulated based on 500K CDS sequences from OMA databases.\n\n- 27,206,792 paired end reads simulated based on 13,848 viral gene sequences downloaded from ENA database.\n\n- 7,917,920 real paired end amplicon sequencing reads targeting V1-V2 region  of  16s rRNA genes from oral microbiome study.\n\n- 6,330,381 paired end reads simulated from 106,880 human noncoding RNA sequences.\n\n- OMA_Silva dataset in figure C contains 1,027,675 paired end reads simulated on CDS sequences which share similarity to rRNA genes, the sequences with identity >=98% and query coverage >=90% to rRNAs were excluded.\n\n- HOMD dataset in figure C has 100,558 paired end reads simulated on CDS sequences from HOMD database which share similarity to the FP sequences of three tools, again sequences with identity >=98% and query coverage >=90% to rRNAs were excluded.\n\n- GO_FP_N_02 in figure C consisting of 678,250 paired end reads was simulated from OMA sequences which have the GO with FP reads ratio >=0.2 on 20M mRNA reads dataset for BWA, RiboDetector or SortMeRNA.\n\n![Benchmarking the performance and runtime of different rRNA sequences detection methods](./benchmarks/benchmarks.jpg)\n\nIn the above figures, the definitions of *FPNR* and *FNR* are:\n\n<img src=\"https://render.githubusercontent.com/render/math?math=\\large FPNR=100\\frac{false \\:predictions}{total \\: sequences}\">\n\n<img src=\"https://render.githubusercontent.com/render/math?math=\\large FNR=100\\frac{false \\:negatives}{total \\:positives}\">\n\nRiboDetector has a very high generalization ability and is capable of detecting novel rRNA sequences (Fig. C). -->\n\n### FAQ\n1. What should I set for `-l` when I have reads with variable length?\n> You can set the `-l` parameter to the mean read length if you have reads with variable length. The mean read length can be computed with `seqkit stats`. This parameter tells how many bases will be used to capture the sequences patterns for classification.  \n\n2. How does `-e` parameter work? What should I set (`rrna`, `norrna`, `none`, `both`)?\n> This parameter is only necessary for paired end reads. When setting to `rrna`, the paired read ends will be predicted as rRNA only if both ends were classified as rRNA. If you want to identify or remove rRNAs with high confidence, you should set it to `rrna`. Conversely, `norrna` will predict the read pair as nonrRNA only if both ends were classified as nonrRNA. This setting will only output nonrRNAs with high confidence. `both` will discard the read pairs with two ends classified inconsistently, only pairs with concordant prediction will be reported in the corresponding output. `none` will take the mean of the probabilities of both ends and decide the final prediction. This is also the default setting. \n\n3. I have very large input file but limited memory, what should I do?\n> You can set the `--chunk_size` parameter which specifies how many reads the software load into memory once.\n\n4. What should I do if RiboDetector hangs with SLURM?\n> The most likely cause is that the requested computational resource is not sufficient for the input file. You need to make sure you specified `--cpus-per-task` to the number you CPU cores you want to use and set `--threads-per-core` to 1 in the SLURM submission script or command. If the issue remains, you can try to reduce the memory use by setting `--chunk_size` parameter in `ribodetector` or `ribodetector_cpu` command.\n\n### Citation\nDeng ZL, Münch PC, Mreches R, McHardy AC. Rapid and accurate detection of ribosomal RNA sequences using deep learning. <i>Nucleic Acids Research</i>. 2022. (https://doi.org/10.1093/nar/gkac112)\n\n### Acknowledgements\nThe scripts from the `base` dir were from the template [pytorch-template\n](https://github.com/victoresque/pytorch-template) by [Victor Huang](https://github.com/victoresque) and other [contributors](https://github.com/victoresque/pytorch-template/graphs/contributors).\n\n",
                "dependencies": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\nfrom setuptools import find_packages, setup\n\nwith open(\"README.md\", \"r\", encoding='utf-8') as fh:\n    long_description = fh.read()\n\nrequired = [\n    \"pandas\",\n    \"tqdm\",\n    \"numpy\",\n    \"biopython\",\n    \"onnxruntime >= 1.10.0, <= 1.15.1\",\n    \"torch >= 1.7.1, <= 1.12.1\",\n]\n\nsetup(\n    name=\"ribodetector\",\n    version=\"0.3.1\",\n    python_requires=\">=3.8, <=3.12\",\n    author=\"Z-L Deng\",\n    author_email=\"dawnmsg@gmail.com\",\n    description=\"Accurate and rapid RiboRNA sequences Detector based on deep learning.\",\n    license=\"GPL-3 License\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/hzi-bifo/RiboDetector\",\n    packages=find_packages(include=[\"ribodetector\", \"ribodetector.*\"]),\n    package_data={'': ['*.json', '*.yaml', '*.pth', '*.onnx']},\n\n    include_package_data=True,\n\n    classifiers=[\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Scientific/Engineering :: Bio-Informatics\",\n        \"Intended Audience :: Science/Research\",\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n    ],\n    entry_points={\n        'console_scripts': [\n            'ribodetector = ribodetector.detect:main',\n            'ribodetector_cpu = ribodetector.detect_cpu:main',\n        ]\n    },\n    zip_safe=True,\n    install_requires=required\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rtlola-frontend",
            "repo_link": "https://github.com/reactive-systems/RTLola-Frontend",
            "content": {
                "codemeta": "",
                "readme": "# RTLola Frontend\n[![Crate](https://img.shields.io/crates/v/rtlola-frontend.svg)](https://crates.io/crates/rtlola-frontend)\n[![API](https://docs.rs/rtlola-frontend/badge.svg)](https://docs.rs/rtlola-frontend)\n[![License](https://img.shields.io/crates/l/rtlola-frontend)](https://crates.io/crates/rtlola-frontend)\n\nRTLola is a stream-based runtime verification framework.  It parses an RTLola specification, analyses it, and generates executable monitors for it.\nThe framework is separated into a front-end and several back-ends.\n\nThis crate summarizes the entire RTLola front-end, which includes several sub-modules:\n* A parser for RTLola specifications: [rtlola-parser](https://crates.io/crates/rtlola-parser) \n* The RTLola high-level intermediate representation including a strong static analysis: [rtlola-hir](https://crates.io/crates/rtlola-hir)\n* The RTLola error reporting: [rtlola-reporting](https://crates.io/crates/rtlola-reporting)\n* Procedural macros: [rtlola-macros](https://crates.io/crates/rtlola-macros)\n\n# Copyright\n\nCopyright (C) CISPA - Helmholtz Center for Information Security 2021-2023.  Authors: Jan Baumeister, Florian Kohn, Stefan Oswald, Frederik Scheerer, Malte Schledjewski, Maximilian Schwenger.\nBased on original work at Universität des Saarlandes (C) 2020.  Authors: Jan Baumeister, Florian Kohn, Malte Schledjewski, Maximilian Schwenger, Marvin Stenger, and Leander Tentrup.\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rtlola-interpreter",
            "repo_link": "https://github.com/reactive-systems/RTLola-Interpreter",
            "content": {
                "codemeta": "",
                "readme": "![RTLola logo](https://pages.cispa.de/rtlola/assets/img/logos/rtlola-logo-ultrawide-blue.png)\n# RTLola Interpreter Repository\n\nRTLola is a runtime monitoring framework.  It consists of a parser, analyzer, and interpreter for the RTLola specification language.\n\nThe project is split into two crates. The interpreter crate provides a library for interpreting RTLola specifications.\nThe CLI crate provides a command line interface to the interpreter.\n\n## RTLola Interpreter\n[![Crate](https://img.shields.io/crates/v/rtlola-interpreter.svg)](https://crates.io/crates/rtlola-interpreter)\n[![API](https://docs.rs/rtlola-interpreter/badge.svg)](https://docs.rs/rtlola-interpreter)\n[![License](https://img.shields.io/crates/l/rtlola-interpreter)](https://crates.io/crates/rtlola-interpreter)\n\nThis library crate provides two APIs to evaluate RTLola specifications through interpretation.\n\n## RTLola Interpreter Input Plugins\n[![Crate](https://img.shields.io/crates/v/rtlola-input-plugins.svg)](https://crates.io/crates/rtlola-input-plugins)\n[![API](https://docs.rs/rtlola-input-plugins/badge.svg)](https://docs.rs/rtlola-input-plugins)\n[![License](https://img.shields.io/crates/l/rtlola-input-plugins)](https://crates.io/crates/rtlola-input-plugins)\n\nThis crate contains the different input methods like the csv and pcap parser for the interpreter.\n\n## RTLola Interpreter CLI\n[![Crate](https://img.shields.io/crates/v/rtlola-cli.svg)](https://crates.io/crates/rtlola-cli)\n[![API](https://docs.rs/rtlola-cli/badge.svg)](https://docs.rs/rtlola-cli)\n[![License](https://img.shields.io/crates/l/rtlola-cli)](https://crates.io/crates/rtlola-cli)\n\nThis crate contains a CLI interface to the interpreter capable of reading csv and pcap files.\n\n# Copyright\n\nCopyright (C) CISPA - Helmholtz Center for Information Security 2024.  Authors: Jan Baumeister, Florian Kohn, Stefan Oswald, Frederik Scheerer, Maximilian Schwenger.\nBased on original work at Universität des Saarlandes (C) 2020.  Authors: Jan Baumeister, Florian Kohn, Malte Schledjewski, Maximilian Schwenger, Marvin Stenger, and Leander Tentrup.\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/s2downloader",
            "repo_link": "https://git.gfz-potsdam.de/fernlab/products/data-portal/s2downloader",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/sampledb",
            "repo_link": "https://github.com/sciapp/sampledb",
            "content": {
                "codemeta": "",
                "readme": "<img src=\"https://raw.githubusercontent.com/sciapp/sampledb/develop/docs/static/img/logo.svg\" align=\"right\" width=\"60\" height=\"60\" />\n\n# SampleDB\n\n[![MIT license](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![DOI](https://zenodo.org/badge/221237572.svg)](https://zenodo.org/badge/latestdoi/221237572)\n[![DOI](https://joss.theoj.org/papers/10.21105/joss.02107/status.svg)](https://doi.org/10.21105/joss.02107)\n\nSampleDB is a web-based sample and measurement metadata database.\n\n## Documentation\n\nYou can find the documentation for the current release at https://scientific-it-systems.iffgit.fz-juelich.de/SampleDB/.\n\n## Getting Started\n\nWe recommend using our pre-built Docker images for setting up `SampleDB`. You will need two containers, one for a PostgreSQL database and another for SampleDB itself, and a directory to store all files in.\n\nIf you would like to set up a development version of SampleDB instead, please see the [contribution guide](https://github.com/sciapp/sampledb/blob/develop/CONTRIBUTING.md).\n\nIf you do not have Docker installed yet, please [install Docker](https://docs.docker.com/engine/install/).\n\n### Using docker-compose\n\nFirst, get the [docker-compose.yml](https://raw.githubusercontent.com/sciapp/sampledb/develop/docker-compose.yml.dist) configuration file. You can git clone this repo or just get the file:\n\n```bash\ncurl https://raw.githubusercontent.com/sciapp/sampledb/develop/docker-compose.yml.dist --output docker-compose.yml\n```\n\nThen simply bring everything up with:\n\n```bash\ndocker compose up -d\n```\n\n### Using docker commands\n\nFirst, start your database container:\n\n```bash\ndocker run \\\n    -d \\\n    -e POSTGRES_PASSWORD=password \\\n    -e PGDATA=/var/lib/postgresql/data/pgdata \\\n    -v `pwd`/pgdata:/var/lib/postgresql/data/pgdata:rw \\\n    --restart=always \\\n    --name sampledb-postgres \\\n    postgres:15\n```\n\nNext, start the SampleDB container:\n\n```bash\ndocker run \\\n    -d \\\n    --link sampledb-postgres \\\n    -e SAMPLEDB_CONTACT_EMAIL=sampledb@example.com \\\n    -e SAMPLEDB_MAIL_SERVER=mail.example.com \\\n    -e SAMPLEDB_MAIL_SENDER=sampledb@example.com \\\n    -e SAMPLEDB_ADMIN_PASSWORD=password \\\n    -e SAMPLEDB_SQLALCHEMY_DATABASE_URI=postgresql+psycopg2://postgres:password@sampledb-postgres:5432/postgres \\\n    --restart=always \\\n    --name sampledb \\\n    -p 8000:8000 \\\n    sciapp/sampledb:0.30.0\n```\n\n### Once it's started\n\nThis will start a minimal SampleDB installation at `http://localhost:8000` and allow you to sign in with the username `admin` and the password `password` (which you should change immediately after signing in).\n\nTo learn how to further set up SampleDB, please follow the rest of the [Getting Started guide](https://scientific-it-systems.iffgit.fz-juelich.de/SampleDB/administrator_guide/getting_started.html).\n\n## Contributing\n\nIf you want to improve SampleDB, please read the [contribution guide](https://github.com/sciapp/sampledb/blob/develop/CONTRIBUTING.md) for a few notes on how to report issues or submit changes.\n\n## Support\n\nIf you run into any issues setting up or running SampleDB, please [open an issue on GitHub](https://github.com/sciapp/sampledb/issues/new).\n\nYou can also subscribe to the [SampleDB mailing list](https://lists.fz-juelich.de/mailman/listinfo/sampledb) to learn about new features and to discuss any questions regarding SampleDB.\n\n",
                "dependencies": "APScheduler==3.10.4\nautocommand==2.2.2\nbabel==2.16.0\nbcrypt==4.2.1\nbeautifulsoup4==4.12.3\nbleach==6.2.0\nblinker==1.9.0\nBrotli==1.1.0\ncertifi==2024.8.30\ncffi==1.17.1\nchardet==5.2.0\ncharset-normalizer==3.4.0\ncheroot==10.0.1\nCherryPy==18.10.0\nclick==8.1.7\ncolorhash==2.0.0\nconfigparser==7.1.0\ncryptography==43.0.3\ncssselect2==0.7.0\ndnspython==2.7.0\nemail_validator==2.2.0\nfido2==1.1.3\nFlask==3.1.0\nflask-babel==4.0.0\nFlask-HTTPAuth==4.8.0\nFlask-Login==0.6.3\nFlask-Mail==0.10.0\nFlask-MonitoringDashboard==3.3.2\nFlask-SQLAlchemy==3.1.1\nFlask-WTF==1.2.2\nflexcache==0.3\nflexparser==0.4\nfonttools==4.55.0\ngreenlet==3.1.1\nidna==3.10\nitsdangerous==2.2.0\njaraco.collections==5.1.0\njaraco.context==6.0.1\njaraco.functools==4.1.0\njaraco.text==4.0.0\nJinja2==3.1.4\nkaleido==0.2.1\nldap3==2.9.1\nMarkdown==3.7\nMarkupSafe==3.0.2\nmore-itertools==10.5.0\nnumpy==2.1.3\npackaging==24.2\npillow==10.4.0\nPint==0.24.4\nplatformdirs==4.3.6\nplotly==5.24.1\nportend==3.2.0\npsutil==6.1.0\npsycopg2-binary==2.9.10\npyasn1==0.6.1\npycparser==2.22\npydyf==0.11.0\npyotp==2.9.0\npyphen==0.15.0\npython-dateutil==2.9.0.post0\npytz==2024.2\nqrcode==8.0\nreportlab==4.2.5\nrequests==2.31.0\nscipy==1.14.1\nsetuptools==75.5.0\nsix==1.16.0\nsoupsieve==2.6\nSQLAlchemy==2.0.36\ntempora==5.7.0\ntenacity==9.0.0\ntinycss2==1.4.0\ntinyhtml5==2.0.0\ntyping_extensions==4.12.2\ntzlocal==2.0.0\nurllib3==2.2.3\nweasyprint==63.0\nwebencodings==0.5.1\nWerkzeug==3.1.3\nWTForms==3.2.1\nzc.lockfile==3.0.post1\nzopfli==0.2.3.post1\n\nimport importlib.util\nfrom setuptools import setup, find_packages\nimport os.path\n\nsetup_directory = os.path.abspath(os.path.dirname(__file__))\n\nwith open(os.path.join(setup_directory, 'README.md')) as readme_file:\n    long_description = readme_file.read()\n\nwith open(os.path.join(setup_directory, 'requirements.txt')) as requirements_file:\n    requirements = requirements_file.readlines()\n\nversion_file_path = os.path.join(setup_directory, 'sampledb', 'version.py')\nspec = importlib.util.spec_from_file_location(\"version\", version_file_path)\nversion = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(version)\n\n\nsetup(\n    name='sampledb',\n    version=version.__version__,\n    description='A sample and measurement metadata database',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    url='https://github.com/sciapp/sampledb',\n    author='Florian Rhiem',\n    author_email='f.rhiem@fz-juelich.de',\n    license='MIT',\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python :: 3',\n        'Framework :: Flask',\n        'Topic :: Scientific/Engineering',\n    ],\n    packages=find_packages(exclude=['tests', 'tests.*']),\n    install_requires=requirements,\n    package_data={\n        'sampledb': [\n            'static/*/*.*',\n            'static/*/*/*.*',\n            'static/*/*/*/*.*'\n        ],\n        'sampledb.logic': [\n            'unit_definitions.txt'\n        ],\n        'sampledb.frontend': [\n            'templates/*/*.*',\n            'templates/*/*/*.*',\n            'templates/*/*/*/*.*'\n        ]\n    }\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/saqc",
            "repo_link": "https://git.ufz.de/rdm-software/saqc",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/scalasca",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/perftools/scalasca",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/scanpy",
            "repo_link": "https://github.com/scverse/scanpy",
            "content": {
                "codemeta": "",
                "readme": "[![Stars](https://img.shields.io/github/stars/scverse/scanpy?style=flat&logo=GitHub&color=yellow)](https://github.com/scverse/scanpy/stargazers)\n[![PyPI](https://img.shields.io/pypi/v/scanpy?logo=PyPI)](https://pypi.org/project/scanpy)\n[![Downloads](https://static.pepy.tech/badge/scanpy)](https://pepy.tech/project/scanpy)\n[![Conda](https://img.shields.io/conda/dn/conda-forge/scanpy?logo=Anaconda)](https://anaconda.org/conda-forge/scanpy)\n[![Docs](https://readthedocs.com/projects/icb-scanpy/badge/?version=latest)](https://scanpy.readthedocs.io)\n[![Build Status](https://dev.azure.com/scverse/scanpy/_apis/build/status/scverse.scanpy?branchName=main)](https://dev.azure.com/scverse/scanpy/_build)\n[![Discourse topics](https://img.shields.io/discourse/posts?color=yellow&logo=discourse&server=https%3A%2F%2Fdiscourse.scverse.org)](https://discourse.scverse.org/)\n[![Chat](https://img.shields.io/badge/zulip-join_chat-%2367b08f.svg)](https://scverse.zulipchat.com)\n[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org/)\n\n# Scanpy – Single-Cell Analysis in Python\n\nScanpy is a scalable toolkit for analyzing single-cell gene expression data\nbuilt jointly with [anndata][].  It includes\npreprocessing, visualization, clustering, trajectory inference and differential\nexpression testing.  The Python-based implementation efficiently deals with\ndatasets of more than one million cells.\n\nDiscuss usage on the scverse [Discourse][]. Read the [documentation][].\nIf you'd like to contribute by opening an issue or creating a pull request, please take a look at our [contribution guide][].\n\n[anndata]: https://anndata.readthedocs.io\n[discourse]: https://discourse.scverse.org/\n[documentation]: https://scanpy.readthedocs.io\n\n[//]: # (numfocus-fiscal-sponsor-attribution)\n\nscanpy is part of the scverse project ([website](https://scverse.org), [governance](https://scverse.org/about/roles)) and is fiscally sponsored by [NumFOCUS](https://numfocus.org/).\nIf you like scverse and want to support our mission, please consider making a [donation](https://numfocus.org/donate-to-scverse) to support our efforts.\n\n<div align=\"center\">\n<a href=\"https://numfocus.org/project/scverse\">\n  <img\n    src=\"https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png\"\n    width=\"200\"\n  >\n</a>\n</div>\n\n\n## Citation\n\nIf you use `scanpy` in your work, please cite the `scanpy` publication as follows:\n\n> **SCANPY: large-scale single-cell gene expression data analysis**\n>\n> F. Alexander Wolf, Philipp Angerer, Fabian J. Theis\n>\n> _Genome Biology_ 2018 Feb 06. doi: [10.1186/s13059-017-1382-0](https://doi.org/10.1186/s13059-017-1382-0).\n\nYou can cite the scverse publication as follows:\n\n> **The scverse project provides a computational ecosystem for single-cell omics data analysis**\n>\n> Isaac Virshup, Danila Bredikhin, Lukas Heumos, Giovanni Palla, Gregor Sturm, Adam Gayoso, Ilia Kats, Mikaela Koutrouli, Scverse Community, Bonnie Berger, Dana Pe’er, Aviv Regev, Sarah A. Teichmann, Francesca Finotello, F. Alexander Wolf, Nir Yosef, Oliver Stegle & Fabian J. Theis\n>\n> _Nat Biotechnol._ 2023 Apr 10. doi: [10.1038/s41587-023-01733-8](https://doi.org/10.1038/s41587-023-01733-8).\n\n\n[contribution guide]: CONTRIBUTING.md\n\n",
                "dependencies": "[build-system]\nbuild-backend = \"hatchling.build\"\nrequires = [\"hatchling\", \"hatch-vcs\"]\n\n[project]\nname = \"scanpy\"\ndescription = \"Single-Cell Analysis in Python.\"\nrequires-python = \">=3.10\"\nlicense = \"BSD-3-clause\"\nauthors = [\n    {name = \"Alex Wolf\"},\n    {name = \"Philipp Angerer\"},\n    {name = \"Fidel Ramirez\"},\n    {name = \"Isaac Virshup\"},\n    {name = \"Sergei Rybakov\"},\n    {name = \"Gokcen Eraslan\"},\n    {name = \"Tom White\"},\n    {name = \"Malte Luecken\"},\n    {name = \"Davide Cittaro\"},\n    {name = \"Tobias Callies\"},\n    {name = \"Marius Lange\"},\n    {name = \"Andrés R. Muñoz-Rojas\"},\n]\nmaintainers = [\n    {name = \"Isaac Virshup\", email = \"ivirshup@gmail.com\"},\n    {name = \"Philipp Angerer\", email = \"phil.angerer@gmail.com\"},\n    {name = \"Alex Wolf\", email = \"f.alex.wolf@gmx.de\"},\n]\nreadme = \"README.md\"\nclassifiers = [\n    \"License :: OSI Approved :: BSD License\",\n    \"Development Status :: 5 - Production/Stable\",\n    \"Environment :: Console\",\n    \"Framework :: Jupyter\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n    \"Natural Language :: English\",\n    \"Operating System :: MacOS :: MacOS X\",\n    \"Operating System :: Microsoft :: Windows\",\n    \"Operating System :: POSIX :: Linux\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering :: Bio-Informatics\",\n    \"Topic :: Scientific/Engineering :: Visualization\",\n]\ndependencies = [\n    \"anndata>=0.8\",\n    \"numpy>=1.23\",\n    \"matplotlib>=3.6\",\n    \"pandas >=1.5\",\n    \"scipy>=1.8\",\n    \"seaborn>=0.13\",\n    \"h5py>=3.7\",\n    \"tqdm\",\n    \"scikit-learn>=1.1,<1.6.0\",\n    \"statsmodels>=0.13\",\n    \"patsy!=1.0.0\", # https://github.com/pydata/patsy/issues/215\n    \"networkx>=2.7\",\n    \"natsort\",\n    \"joblib\",\n    \"numba>=0.56\",\n    \"umap-learn>=0.5,!=0.5.0\",\n    \"pynndescent>=0.5\",\n    \"packaging>=21.3\",\n    \"session-info\",\n    \"legacy-api-wrap>=1.4\",  # for positional API deprecations\n    \"typing-extensions; python_version < '3.13'\",\n]\ndynamic = [\"version\"]\n\n[project.urls]\nDocumentation = \"https://scanpy.readthedocs.io/\"\nSource = \"https://github.com/scverse/scanpy\"\nHome-page = \"https://scanpy.org\"\nDiscourse = \"https://discourse.scverse.org/c/help/scanpy/37\"\nTwitter = \"https://twitter.com/scverse_team\"\n\n[project.scripts]\nscanpy = \"scanpy.cli:console_main\"\n\n[project.optional-dependencies]\ntest-min = [\n    \"pytest>=8.2\",\n    \"pytest-nunit\",\n    \"pytest-mock\",\n    \"pytest-cov\",\n    \"profimp\",\n]\ntest = [\n    \"scanpy[test-min]\",\n    # Optional but important dependencies\n    \"scanpy[leiden]\",\n    \"zarr\",\n    \"scanpy[dask]\",\n    \"scanpy[scrublet]\",\n]\ntest-full = [\n    \"scanpy[test]\",\n    # optional storage modes\n    \"zappy\",\n    # additional tested algorithms\n    \"scanpy[louvain]\",\n    \"scanpy[magic]\",\n    \"scanpy[skmisc]\",\n    \"scanpy[harmony]\",\n    \"scanpy[scanorama]\",\n    \"scanpy[dask-ml]\",\n]\ndoc = [\n    \"sphinx>=7\",\n    \"sphinx-book-theme>=1.1.0\",\n    \"scanpydoc>=0.14.1\",\n    \"sphinx-autodoc-typehints>=1.25.2\",\n    \"myst-parser>=2\",\n    \"myst-nb>=1\",\n    \"sphinx-design\",\n    \"sphinx-tabs\",\n    \"readthedocs-sphinx-search\",\n    \"sphinxext-opengraph\", # for nice cards when sharing on social\n    \"sphinx-copybutton\",\n    \"nbsphinx>=0.9\",\n    \"ipython>=7.20\",  # for nbsphinx code highlighting\n    \"matplotlib!=3.6.1\",\n    \"sphinxcontrib-bibtex\",\n    \"setuptools\", # undeclared dependency of sphinxcontrib-bibtex→pybtex\n    # TODO: remove necessity for being able to import doc-linked classes\n    \"scanpy[paga,dask-ml]\",\n    \"sam-algorithm\",\n]\ndev = [\n    # getting the dev version\n    \"setuptools_scm\",\n    # static checking\n    \"pre-commit\",\n    \"towncrier\",\n]\n# Algorithms\npaga = [\"igraph\"]\nlouvain = [\"igraph\", \"louvain>=0.6.0,!=0.6.2\"]  # Louvain community detection\nleiden = [\"igraph>=0.10\", \"leidenalg>=0.9.0\"]  # Leiden community detection\nbbknn = [\"bbknn\"]  # Batch balanced KNN (batch correction)\nmagic = [\"magic-impute>=2.0\"]  # MAGIC imputation method\nskmisc = [\"scikit-misc>=0.1.3\"]  # highly_variable_genes method 'seurat_v3'\nharmony = [\"harmonypy\"]  # Harmony dataset integration\nscanorama = [\"scanorama\"]  # Scanorama dataset integration\nscrublet = [\"scikit-image\"]  # Doublet detection with automatic thresholds\n# Acceleration\nrapids = [\"cudf>=0.9\", \"cuml>=0.9\", \"cugraph>=0.9\"]  # GPU accelerated calculation of neighbors\ndask = [\"dask[array]>=2022.09.2,<2024.8.0\"]  # Use the Dask parallelization engine\ndask-ml = [\"dask-ml\", \"scanpy[dask]\"]  # Dask-ML for sklearn-like API\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/testing\", \"src/scanpy\"]\n[tool.hatch.version]\nsource = \"vcs\"\nraw-options.version_scheme = \"release-branch-semver\"\n[tool.hatch.build.hooks.vcs]\nversion-file = \"src/scanpy/_version.py\"\n\n[tool.pytest.ini_options]\naddopts = [\n    \"--import-mode=importlib\",\n    \"--strict-markers\",\n    \"--doctest-modules\",\n    \"-ptesting.scanpy._pytest\",\n    \"--pyargs\",\n]\ntestpaths = [\"./tests\", \"./ci\", \"scanpy\"]\nnorecursedirs = [\"tests/_images\"]\nxfail_strict = true\nnunit_attach_on = \"fail\"\nmarkers = [\n    \"internet: tests which rely on internet resources (enable with `--internet-tests`)\",\n    \"gpu: tests that use a GPU (currently unused, but needs to be specified here as we import anndata.tests.helpers, which uses it)\",\n    \"anndata_dask_support: tests that require dask support in anndata\",\n]\nfilterwarnings = [\n    # legacy-api-wrap: internal use of positional API\n    \"error:The specified parameters:FutureWarning\",\n    # When calling `.show()` in tests, this is raised\n    \"ignore:FigureCanvasAgg is non-interactive:UserWarning\",\n\n    # We explicitly handle the below errors in tests\n    \"error:`anndata.read` is deprecated:FutureWarning\",\n    \"error:Observation names are not unique:UserWarning\",\n    \"error:The dtype argument is deprecated and will be removed:FutureWarning\",\n    \"error:The behavior of DataFrame\\\\.sum with axis=None is deprecated:FutureWarning\",\n    \"error:The default of observed=False is deprecated:FutureWarning\",\n    \"error:Series\\\\.__getitem__ treating keys as positions is deprecated:FutureWarning\",\n    \"error:The default value of 'ignore' for the `na_action` parameter in pandas\\\\.Categorical\\\\.map:FutureWarning\",\n    \"error:The provided callable.* is currently using:FutureWarning\",\n    \"error:The behavior of DataFrame concatenation with empty or all-NA entries is deprecated:FutureWarning\",\n    \"error:A value is trying to be set on a copy of a slice from a DataFrame\",\n    \"error:No data for colormapping provided via 'c'\",\n    \"error:\\\\n*The `scale` parameter has been renamed and will be removed\",\n    \"error:\\\\n*Passing `palette` without assigning `hue` is deprecated\",\n    \"error:\\\\n*Setting a gradient palette using color= is deprecated\",\n]\n\n[tool.coverage.run]\ndata_file = \"test-data/coverage\"\nsource_pkgs = [\"scanpy\"]\nomit = [\"tests/*\", \"src/testing/*\"]\n[tool.coverage.xml]\noutput = \"test-data/coverage.xml\"\n[tool.coverage.paths]\nsource = [\".\", \"**/site-packages\"]\n[tool.coverage.report]\nexclude_also = [\n    \"if __name__ == .__main__.:\",\n    \"if TYPE_CHECKING:\",\n    # https://github.com/numba/numba/issues/4268\n    '@(numba\\.|nb\\.)njit.*',\n]\n\n[tool.ruff]\nsrc = [\"src\"]\n\n[tool.ruff.format]\ndocstring-code-format = true\n\n[tool.ruff.lint]\nselect = [\n    \"E\",       # Error detected by Pycodestyle\n    \"F\",       # Errors detected by Pyflakes\n    \"W\",       # Warning detected by Pycodestyle\n    \"UP\",      # pyupgrade\n    \"I\",       # isort\n    \"TCH\",     # manage type checking blocks\n    \"TID251\",  # Banned imports\n    \"ICN\",     # Follow import conventions\n    \"PTH\",     # Pathlib instead of os.path\n    \"PYI\",     # Typing\n    \"PLR0917\", # Ban APIs with too many positional parameters\n    \"FBT\",     # No positional boolean parameters\n    \"PT\",      # Pytest style\n    \"SIM\",     # Simplify control flow\n]\nignore = [\n    # line too long -> we accept long comment lines; black gets rid of long code lines\n    \"E501\",\n    # module level import not at top of file -> required to circumvent circular imports for Scanpys API\n    \"E402\",\n    # E266 too many leading '#' for block comment -> Scanpy allows them for comments into sections\n    \"E262\",\n    # allow I, O, l as variable names -> I is the identity matrix, i, j, k, l is reasonable indexing notation\n    \"E741\",\n    # `Literal[\"...\"] | str` is useful for autocompletion\n    \"PYI051\",\n]\n[tool.ruff.lint.per-file-ignores]\n# Do not assign a lambda expression, use a def\n\"src/scanpy/tools/_rank_genes_groups.py\" = [\"E731\"]\n[tool.ruff.lint.isort]\nknown-first-party = [\"scanpy\", \"testing.scanpy\"]\nrequired-imports = [\"from __future__ import annotations\"]\n[tool.ruff.lint.flake8-tidy-imports.banned-api]\n\"pytest.importorskip\".msg = \"Use the “@needs” decorator/mark instead\"\n\"pandas.api.types.is_categorical_dtype\".msg = \"Use isinstance(s.dtype, CategoricalDtype) instead\"\n\"pandas.value_counts\".msg = \"Use pd.Series(a).value_counts() instead\"\n\"legacy_api_wrap.legacy_api\".msg = \"Use scanpy._compat.old_positionals instead\"\n\"numpy.bool\".msg = \"Use `np.bool_` instead for numpy>=1.24<2 compatibility\"\n\"numba.jit\".msg = \"Use `scanpy._compat.njit` instead\"\n\"numba.njit\".msg = \"Use `scanpy._compat.njit` instead\"\n[tool.ruff.lint.flake8-type-checking]\nexempt-modules = []\nstrict = true\n\n[tool.towncrier]\npackage = \"scanpy\"\ndirectory = \"docs/release-notes\"\nfilename = \"docs/release-notes/{version}.md\"\nsingle_file = false\npackage_dir = \"src\"\nissue_format = \"{{pr}}`{issue}`\"\ntitle_format = \"(v{version})=\\n### {version} {{small}}`{project_date}`\"\nfragment.bugfix.name = \"Bug fixes\"\nfragment.doc.name = \"Documentation\"\nfragment.feature.name = \"Features\"\nfragment.misc.name = \"Miscellaneous improvements\"\nfragment.performance.name = \"Performance\"\nfragment.breaking.name = \"Breaking changes\"\nfragment.dev.name = \"Development Process\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/scits",
            "repo_link": "https://github.com/jalalmostafa/SciTS",
            "content": {
                "codemeta": "",
                "readme": "# SciTS v2, 2023 update\n\nA tool to benchmark Time-series on different databases\n\n- reworked architecture\n- adds mixed, online workloads\n- adds regular and irregular ingestion modes.\n- adds multiple values per time series (\"Dimensions\")\n- adds limited queries\n- adds CLI arguments\n- adds ClientLatency metric to measure differences in local processing.\n\nfor questions on these features, please contact info@saninfo.de\n\nRequires .NET 7.x cross-platform framework.\n\n## Citation \n\n[![DOI](https://zenodo.org/badge/429005385.svg)](https://zenodo.org/badge/latestdoi/429005385)\n\nPlease cite our work:\n\n> Jalal Mostafa, Sara Wehbi, Suren Chilingaryan, and Andreas Kopmann. 2022. SciTS: A Benchmark for Time-Series Databases in Scientific Experiments and Industrial Internet of Things. In 34th International Conference on Scientific and Statistical Database Management (SSDBM 2022). Association for Computing Machinery, New York, NY, USA, Article 12, 1–11. https://doi.org/10.1145/3538712.3538723\n\n\n### Bibtex\n\n```bibtex\n@inproceedings{10.1145/3538712.3538723,\n    author = {Mostafa, Jalal and Wehbi, Sara and Chilingaryan, Suren and Kopmann, Andreas},\n    title = {SciTS: A Benchmark for Time-Series Databases in Scientific Experiments and Industrial Internet of Things},\n    year = {2022},☺\n    isbn = {9781450396677},\n    publisher = {Association for Computing Machinery},\n    address = {New York, NY, USA},\n    url = {https://doi.org/10.1145/3538712.3538723},\n    doi = {10.1145/3538712.3538723},\n    abstract = {Time-series data has an increasingly growing usage in Industrial Internet of Things (IIoT) and large-scale scientific experiments. Managing time-series data needs a storage engine that can keep up with their constantly growing volumes while providing an acceptable query latency. While traditional ACID databases favor consistency over performance, many time-series databases with novel storage engines have been developed to provide better ingestion performance and lower query latency. To understand how the unique design of a time-series database affects its performance, we design SciTS, a highly extensible and parameterizable benchmark for time-series data. The benchmark studies the data ingestion capabilities of time-series databases especially as they grow larger in size. It also studies the latencies of 5 practical queries from the scientific experiments use case. We use SciTS to evaluate the performance of 4 databases of 4 distinct storage engines: ClickHouse, InfluxDB, TimescaleDB, and PostgreSQL.},\n    booktitle = {Proceedings of the 34th International Conference on Scientific and Statistical Database Management},\n    articleno = {12},\n    numpages = {11},\n    keywords = {time-series databases, database management systems, industrial internet of things, scientific experiments, sensor data, time-series},\n    location = {Copenhagen, Denmark},\n    series = {SSDBM '22}\n}\n```\n\n# How to run\n\n1. Create your workload as `App.config` (case-sensitive) in `BenchmarkTool`.\n2. Edit the connection strings to your database servers in the workload file.\n3. Choose the target database in the workload file using `TargetDatabase` element.\n4. run `dotnet run --project BenchmarkTool write` if it's an ingestion workload,\nand `dotnet run --project BenchmarkTool read` if it's a query workload.\nx. Use `Scripts/ccache.sh <database-service-name>` to clear the cache between query tests.\n\n## Additional Command Line options:\n\n`dotnet run --project BenchmarkTool [action] [regular/irregular] [DatabaseNameDB]`\n\nAvailable Actions:\n\n* read: start the specified retrieval and aggregation workloads.\n* write: start the ingestion across specified batchsize, number of clients, dimensions.\n* mixed-AggQueries: start the online, mixed workload benchmark as a mixture of aggregated quieries and Ingestion-Parameters\n* mixed-LimitedQueries: start the online, mixed workload benchmark as a mixture of queried and ingested datapoints according the specified percentage parameter and the requested Ingestion-Parameters. E.g. 100% means that as much datapoints are retrieved as ingested.\n\n\n## System Metrics using Glances\n\nThis tool uses [glances](https://github.com/nicolargo/glances/).\n1. Install glances with all plugins on the database server using `pip install glances[all]`\n2. Run glances REST API on the database server using `glances -w --disable-webui`\n\n## Workload Definition Files\n\nyou can open Default-App.config edit it and save it as App.config.\nIt has following content:\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n<configuration>\n    <appSettings>\n<!-- Attention: This file \"AppDefault.config\" is to be renamed in \"App.config\", after updating the \"###\" and other fields.  -->\n  \n    <!-- Datalayerts connection settings -->\n        <add key=\"DatalayertsConnection\" value=\"https://datalayerts.com\" />\n        <add key=\"DatalayertsUser\" value=\"###\" />\n        <add key=\"DatalayertsPassword\" value=\"###\" />\n\n    <!-- Postgres connection settings -->\n        <add key=\"PostgresConnection\" value=\"Server=localhost;Port=5432;Database=postgres;User Id=postgres;Password=###;\" />\n\n    <!-- Timescale connection settings -->\n        <add key=\"TimescaleConnection\" value=\"Server=localhost;Port=6432;Database=postgres;User Id=postgres;Password=###;CommandTimeout=300\" />\n\n    <!-- InfluxDB connection settings --> \n        <add key=\"InfluxDBHost\" value=\"http://localhost:8086\" />  \n        <add key=\"InfluxDBToken\" value=\"u7Ek4P5s0Nle61QQF1nNA3ywL1JYZky6rHRXxkPBX5bY4H3YFJ6T4KApWSRhaKNj_kHgx70ZLBowB6Di4t2YXg==\" />\n        <add key=\"InfluxDBBucket\" value=\"scitsdb\" />\n        <add key=\"InfluxDBOrganization\" value=\"scits\" />  \n\n    <!-- Clickhouse connection settings -->\n        <add key=\"ClickhouseHost\" value=\"localhost\" />\n        <add key=\"ClickhousePort\" value=\"9000\" />\n        <add key=\"ClickhouseUser\" value=\"default\" />\n        <add key=\"ClickhouseDatabase\" value=\"default\" />\n \n    <!-- General Settings-->\n          <add key=\"PrintModeEnabled\" value=\"false\" />\n          <add key=\"TestRetries\" value=\"2\" />\n          <add key=\"DaySpan\" value=\"1\" />\n        <!-- Could be: DummyDB,  PostgresDB , DatalayertsDB , ClickhouseDB , TimescaleDB , InfluxDB -->\n          <add key=\"TargetDatabase\" value=\"DummyDB\" />\n          <add key=\"StartTime\" value=\"2022-01-01T00:00:00.00\" />\n          <add key=\"RegularTsScaleMilliseconds\" value=\"1000\" /> \n        <!-- Where to store metrics files: The Programm will split the files in \"[...]Read.csv\" and \"[...]Write.csv\" -->\n          <add key=\"MetricsCSVPath\" value=\"Metrics_Source_Month-Day\" />\n          \n    <!-- System Metrics Options -->\n          <add key=\"GlancesOutput\" value=\"Glances_Source_Month-Day.csv\"/>\n          <add key=\"GlancesUrl\" value=\"http://localhost:61208\" />\n          <add key=\"GlancesDatabasePid\" value=\"1\" />\n          <add key=\"GlancesPeriod\" value=\"1\" />\n          <add key=\"GlancesNIC\" value=\"lo\" />\n          <add key=\"GlancesDisk\" value=\"sda1\" />\n          <add key=\"GlancesStorageFileSystem\" value=\"/\" />\n        <!-- Insert multiple dimensionnrs, e.g.  1,6 ,12 ,50, 100, -->\n          <add key=\"DataDimensionsNrOptions\" value=\"1,6\" />  \n\n    <!-- Read Query Options -->\n        <!-- Could be: Agg, All, RangeQueryRawData, RangeQueryRawAllDimsData, RangeQueryRawLimitedData, RangeQueryRawAllDimsLimitedData  RangeQueryAggData, OutOfRangeQuery, DifferenceAggQuery, STDDevQuery -->\n          <add key=\"QueryType\" value=\"All\" />\n          <add key=\"AggregationIntervalHour\" value=\"1\" />\n          <add key=\"DurationMinutes\" value=\"60\" />\n          <add key=\"SensorsFilter\" value=\"1,2,3,4\" /> <!--  or \"All\" -->\n          <add key=\"SensorID\" value=\"1\" />\n          <add key=\"MaxValue\" value=\"0.9\" />\n          <add key=\"MinValue\" value=\"0.1\" />\n          <add key=\"FirstSensorID\" value=\"1\" />\n          <add key=\"SecondSensorID\" value=\"2\" />\n\n    <!-- Ingestion -->\n        <!-- Could be: regular, irregular -->\n          <add key=\"IngestionType\" value=\"regular\" /> \n        <!-- Coulde be:  33, 100 , 300  -->\n          <add key=\"MixedWLPercentageOptions\" value=\"33, 100,300\" />\n        <!-- Could be: array, column. Array is not fully implemented in all DBMS. -->\n          <add key=\"MultiDimensionStorageType\" value=\"column\" />\n        <!-- 10, 1000, 5000, 10000 , 50000 -->\n          <add key=\"BatchSizeOptions\" value=\" 100 , 1000, 6000 \" />\n        <!-- Number of concurrent clients e.g.(1,8,16) must be less than sensors. BatchSizes will be shared out between the clients -->\n          <add key=\"ClientNumberOptions\" value=\"1 , 8\" />\n          <add key=\"SensorNumber\" value=\"100\" />   \n\n    </appSettings>\n\n</configuration>\n```\n\n### Workload Files\n\nYou can choose from the available workloads by choosing a `*.config` file from `Workloads` folder.\nThe file to workload mapping is as follow:\n\n| Workload    | Workload file                      |\n| ----------- | ---------------------------------- |\n| 2022 WLs    |                                    |\n| ----------- |                                    |\n| Q1          | query-q1.config                    |\n| Q2          | query-q2.config                    |\n| Q3          | query-q3.config                    |\n| Q4          | query-q4.config                    |\n| Q5          | query-q5.config                    |\n| Batching    | ingestion-batching-1client.config  |\n| Concurrency | ingestion-batching-nclients.config |\n| Scaling     | ingestion-scaling.config           |\n|.............|....................................|\n|Collection   |                                    |\n|of 2023 WLs  | test2023.sh                        |\n\n\n#### Timescale\nWe discovered abnormal high latencies and other failures with NPGSQL, so we embedded a python script which does the queries.\nTherefore you need to configure the python location. In case of python 3.10, e.g.\nuse \"whereis libpython3.10.so\", and copy this path.\nThen you go into TimescaleDB.cd, and edit the string in line 134 [ Runtime.PythonDLL = \"/usr/lib/_Architecture_-linux-gnu/libpython3.10.so\"; ]\n\nSo it points to your python location\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/score-p",
            "repo_link": "https://gitlab.com/score-p/scorep",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/sdaas",
            "repo_link": "https://github.com/rizac/sdaas",
            "content": {
                "codemeta": "",
                "readme": "# <img align=\"left\" height=\"30\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/medien_kommunikation/Infothek/Mediathek/Bilder/GFZ/GFZ_Logo/GFZ-Logo_eng_RGB.svg\"> Sdaas <img align=\"right\" height=\"50\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/GFZ_Wortmarke_SVG_klein_en_edit.svg\">\n\n|Jump to: | [Installation](#installation) | [Usage](#usage) |  [Maintenance](#maintenance) | [Citation](#citation) |\n| - | - | - | - | - |\n\n<!-- **S**eismic **D**ata (and metadata) **A**mplitude **A**nomaly **S**core -->\n\nSimple, general and flexible tool for the identification of anomalies in \nseismic waveform amplitude, e.g.:\n\n - recording artifacts (e.g., anomalous noise, peaks, gaps, spikes)\n - sensor problems (e.g. digitizer noise)\n - metadata field errors (e.g. wrong stage gain in StationXML)\n\n**For any waveform analyzed, the program computes an amplitude anomaly \nscore in [0, 1] representing \nthe degree of belief of a waveform to be an outlier**. The score\ncan be used:\n - in any processing pipeline to\n   - pre-filter malformed data via a user-defined threshold\n   - assign robustness weights\n - as station installation / metadata checker, exploiting the scores\n   temporal trends. See e.g. Channel (a) in the figure: the abrupt onset/offset of persistently high anomaly scores roughly between March and May 2018 clearly indicates an installation problem that has been fixed\n\n![anomaly_scores_image](https://github.com/rizac/sdaas/blob/6b1dca95f4a5f931874c4aaedd278f4692dc0f96/outlierspaper-img008.png?raw=true)\nAnomaly scores for four different channels (a) to (d). Each dot represents a recorded waveform segment of variable length\n\nNotes:\n\nThis program uses a machine learning algorithm specifically designed\nfor outlier detection (Isolation forest) where\n\n  - scores <= 0.5 can be safely interpreted in all applications as \"no significant anomaly\" (see Isolation Forest original paper - Liu et al. 2008 - for theoretical details)\n    \n  - extreme score values are virtually impossible [by design](https://scikit-learn.org/stable/modules/calibration.html).\n    This has to be considered when setting a user defined threshold T to \n    discard malformed waveforms. In many application, setting T between 0.7 and \n    0.75 has proven to be a good compromise between \n    [precision and recall (F1 score)](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics)\n\n  - (Disclaimer) \"False positives\", i.e. relatively high anomaly scores even for well formed recordings have been sometimes observed in two specific cases: \n    \n      - recordings from stations with extremely and abnormaly low noise level (e.g. borhole installations)\n      - recordings containing strong and close earthquakes. This is not a problem to check metadata errors, as the scores trend of several recordings from a given sensor / station will not be affected (except maybe for few sparse slightly higer scores), but has to be considered when filtering out segments in specific studies (e.g. with strong motion data): in this cases, setting a higher threshold is advisable. A model trained for accelerometers only (usually employed with these kind of recordings) is under study\n   \n<!--\n<img align=\"right\" width=\"27%\" src=\"outlierspaper-img004.png\"><img align=\"right\"  width=\"29%\" src=\"outlierspaper-img005.png\">\n\nSimple program to compute amplitude anomaly scores (in [0, 1]) of seismic \ndata and metadata.\nGiven a set of waveforms and their metadata, it removes the waveform response\nand returns the anomaly score of the waveforms amplitudes.\n\nThis program can be used to filter out a set of malformed waveforms,\nassign robustness weights\nor to check the correctness of the metadata fields (e.g. Station inventory xml)\nby checking the anomaly score on a set of station recordings. \n-->\n\n\n## Installation\n\nAlways work within a virtual environment. From a terminal, in the directory\nwhere you cloned this repository (last argument of `git clone`),\n\n1. create a virtual environment (once). **Be sure you use Python>=3.7 (Type `python3 --version` to check)**:\n\n    ```bash\n    python3 -m venv .env\n    ```\n\n2. activate it (**to be done also every time you use this program**):\n    ```bash\n    source .env/bin/activate\n    ```\n   (then to deactivate, simply type ... `deactivate` on the terminal). \n   \n   Update `pip` and `setuptools` (not mandatory, but in rare cases it\n   could prevent errors during the installation):\n   ```\n   pip install --upgrade pip setuptools\n   ```\n\n3. Install the program (one line command):\n\n    with [requirements file](https://pip.pypa.io/en/stable/user_guide/#requirements-files):\n    ```\n    pip install -r ./requirements.txt && pip install -e .\n    ```\n    \n    or standard (use this mainly if you install sdaas on an already existing \n    virtualenv and you are concerned about breaking existing code):\n    \n    ```\n    pip install \"numpy>=1.15.4\" && pip install -e .\n    ```\n    \n    Notes:\n    \n    - The \"standard\" install actually checks the `setup.py` file \n      and avoids overwriting libraries already matching the required version. \n      The downside is that you might use library version that were not tested\n    \n    - `-e` is optional. With -e, you can update the installed program to the \n      latest release by simply issuing a `git pull`\n      \n    - although used to train, test and generate the underlying model,\n      `scikit learn` is not required for Security & maintainability \n      limitations. If you want to install it, type \n      `pip install scikit-learn>=0.21.3` or, in the\n      standard installation you can include scikit learn\n      with `pip install .[dev]` instead of `pip install .`\n\n      <details>\n      <summary>Reported scikit learn installation problems (click for details)</summary>\n      \n      Due to the specific version to be installed,\n      scikit might have problems installing. \n      \n      Few hints here:\n      - you might need to preinstall `cython` (`pip install cython`)\n      - you might need to `brew install libomp`, set the follwing env variables:\n        ```\n        export CC=/usr/bin/clang;export CXX=/usr/bin/clang++;export CPPFLAGS=\"$CPPFLAGS -Xpreprocessor -fopenmp\";export CFLAGS=\"$CFLAGS -I/usr/local/opt/libomp/include\";export CXXFLAGS=\"$CXXFLAGS -I/usr/local/opt/libomp/include\";export LDFLAGS=\"$LDFLAGS -Wl,-rpath,/usr/local/opt/libomp/lib -L/usr/local/opt/libomp/lib -lomp\"\n        ```\n      - and then install with the following flags:\n        ```\n        pip install --verbose --no-build-isolation \"scikit-learn==0.21.3\"\n        ```\n      \n      **(For any further detail, see\n      [scikit-learn installation page](https://scikit-learn.org/dev/developers/advanced_installation.html))**\n      \n      </details>\n\n#### Run tests (optional)\n\n```bash\npython -m unittest -fv\n```\n\n(`-f` is optional and means: stop at first failure, `-v`: verbose)\n\n\n## Usage\n\n`sdaas` can be used as command line application or as library in your Python code\n\n### As command line application\n\nAfter activating your virtual environment (see above) you can access the program as\ncommand line application in your terminal by typing `sdaas`. The application\ncan compute the score(s) of a single miniSEED file, a directory of miniSEED files, or \na FDSN url ([dataselect or station](https://www.fdsn.org/webservices/) url).\n\n**Please type `sdaas --help` for details and usages not covered in the examples below,\nsuch as computing scores from locally stored files**\n\n\n**Examples**\n\nCompute scores of waveforms fetched from a FDSN URL:\n\n```bash\n>>> sdaas \"http://geofon.gfz-potsdam.de/fdsnws/dataselect/1/query?net=GE&sta=EIL&cha=BH?&start=2019-01-01T00:00:00&end=2019-01-01T00:02:00\" -v\n[████████████████████████████████████████████████████████████████████████████████████████████████████████]100%  0d 00:00:00\nid start end anomaly_score\nGE.EIL..BHN 2019-01-01T00:00:09.100 2019-01-01T00:02:12.050 0.45\nGE.EIL..BHE 2019-01-01T00:00:22.350 2019-01-01T00:02:00.300 0.45\nGE.EIL..BHZ 2019-01-01T00:00:02.250 2019-01-01T00:02:03.550 0.45\n```\n*(note: The paremeter `-v` / verbose \nprints additional info before the scores table)*\n\nCompute scores from randomly selected segments of a given station and channel,\nand provide also a user-defined threshold (parameter `-th`) which will also \nappend a column \"class_label\" (1 = outlier - assigned to the scores \ngreater than the threshold and 0 = inlier)\n\n```bash\n>>> sdaas \"http://geofon.gfz-potsdam.de/fdsnws/station/1/query?net=GE&sta=EIL&cha=BH?&start=2019-01-01\" -v -th 0.7\n[██████████████████████████████████████████████████████████]100%  0d 00:00:00\nid start end anomaly_score class_label\nGE.EIL..BHN 2019-01-01T00:00:09.100 2019-01-01T00:02:12.050 0.45 0\nGE.EIL..BHN 2019-10-16T18:48:11.700 2019-10-16T18:51:02.300 0.83 1\nGE.EIL..BHN 2020-07-31T13:37:19.299 2020-07-31T13:39:41.299 0.45 0\nGE.EIL..BHN 2021-05-16T08:25:38.100 2021-05-16T08:28:03.150 0.53 0\nGE.EIL..BHN 2022-03-01T03:14:23.300 2022-03-01T03:17:01.750 0.45 0\nGE.EIL..BHE 2019-01-01T00:00:22.350 2019-01-01T00:02:00.300 0.45 0\nGE.EIL..BHE 2019-10-16T18:48:18.050 2019-10-16T18:51:09.650 0.83 1\nGE.EIL..BHE 2020-07-31T13:37:08.599 2020-07-31T13:39:28.499 0.45 0\nGE.EIL..BHE 2021-05-16T08:25:49.150 2021-05-16T08:28:14.800 0.49 0\nGE.EIL..BHE 2022-03-01T03:14:26.050 2022-03-01T03:16:41.900 0.45 0\nGE.EIL..BHZ 2019-01-01T00:00:02.250 2019-01-01T00:02:03.550 0.45 0\nGE.EIL..BHZ 2019-10-16T18:48:24.800 2019-10-16T18:50:47.300 0.45 0\nGE.EIL..BHZ 2020-07-31T13:37:08.249 2020-07-31T13:39:30.199 0.45 0\nGE.EIL..BHZ 2021-05-16T08:25:47.250 2021-05-16T08:28:10.850 0.47 0\nGE.EIL..BHZ 2022-03-01T03:14:40.800 2022-03-01T03:16:53.900 0.45 0\n```\n\nCompute scores from randomly selected segments of a given station and channel,\nbut aggregating scores per channel and returning their median:\n\n```bash\n>>> sdaas \"http://geofon.gfz-potsdam.de/fdsnws/station/1/query?net=GE&sta=EIL&cha=BH?&start=2019-01-01\" -v -agg median\n[██████████████████████████████████████████████████████████]100%  0d 00:00:00\nid start end median_anomaly_score\nGE.EIL..BHN 2019-01-01T00:00:09.100 2022-03-01T03:17:45.950 0.46\nGE.EIL..BHE 2019-01-01T00:00:22.350 2022-03-01T03:17:48.700 0.45\nGE.EIL..BHZ 2019-01-01T00:00:02.250 2022-03-01T03:17:39.300 0.45\n```\n\nSame as above, but save the scores table to CSV via the parameter `-sep` and\n`>` (normal redirection of the standard output `stdout` to file)\n\n```bash\n>>> sdaas \"http://geofon.gfz-potsdam.de/fdsnws/station/1/query?net=GE&sta=EIL&cha=BH?&start=2019-01-01\" -v -sep \",\" > /path/to/myfile.csv\n[████████████████████████████████████████████████████████████]100%  0d 00:00:00\n```\n*in this case, providing `-v` / verbose will also redirect the header row to\nCSV. Note that only the scores table is output to `stdout`, everything else is \nprinted to `stderr` and thus should still be visible on the terminal, as in the \nexample above*\n\n### As library in your Python code\n\nThis software can also be used as library in Python code (e.g. Jupyter Notebook)\nto work with [ObsPy](https://docs.obspy.org/) objects (ObsPy is already included \nin the installation): assuming you have one or more \n[Stream](https://docs.obspy.org/packages/autogen/obspy.core.stream.Stream.html)\nor [Trace](https://docs.obspy.org/packages/autogen/obspy.core.trace.Trace.html),\nwith relative [Inventory](https://docs.obspy.org/packages/obspy.core.inventory.html), \nthen\n\n<!-- IMPORTANT: -->\n<!-- EACH \"```python...```\" code snippet is executed also in \n`test_and_create_code_snippet` to check that it works. If you implement new \nsnippets here, add them in the test file as well -->\n\nCompute the scores in a stream or iterable of traces (e.g. list. tuple, generator),\nreturning the score of each Trace:\n\n```python\nfrom obspy.core.inventory.inventory import read_inventory\nfrom obspy.core.stream import read\n\nfrom sdaas.core import traces_scores\n\n# Load a Stream object and its inventory\n# (use as example the test directory of the package):\nstream = read('./tests/data/GE.FLT1..HH?.mseed')\ninventory = read_inventory('./tests/data/GE.FLT1.xml')\n\n# Compute the Stream anomaly score (3 scores, one for each Trace):\noutput = traces_scores(stream, inventory)\n```\nThen `output` is:\n```\n[0.45729656,  0.45199387,  0.45113142]\n```\n\nCompute the scores in a stream or iterable of traces, getting also the traces id (by \ndefault the tuple `(seed_id, start, end)`, where seed_id is the \n[Trace SEED identifier](https://docs.obspy.org/packages/autogen/obspy.core.trace.Trace.get_id.html)):\n\n```python\nfrom obspy.core.inventory.inventory import read_inventory\nfrom obspy.core.stream import read\n\nfrom sdaas.core import traces_idscores\n\n# Load a Stream object and its inventory\n# (use as example the test directory of the package):\nstream = read('./tests/data/GE.FLT1..HH?.mseed')\ninventory = read_inventory('./tests/data/GE.FLT1.xml')\n\n# Compute the Stream anomaly score:\noutput = traces_idscores(stream, inventory)\n```\nThen `output` is:\n```\n([('GE.FLT1..HHE', datetime.datetime(2011, 9, 3, 16, 38, 5, 550001), datetime.datetime(2011, 9, 3, 16, 42, 12, 50001)), ('GE.FLT1..HHN', datetime.datetime(2011, 9, 3, 16, 38, 5, 760000), datetime.datetime(2011, 9, 3, 16, 42, 9, 670000)), ('GE.FLT1..HHZ', datetime.datetime(2011, 9, 3, 16, 38, 8, 40000), datetime.datetime(2011, 9, 3, 16, 42, 9, 670000))], array([ 0.45729656,  0.45199387,  0.45113142]))\n```\n\nSame as above, with custom traces id (their SEED identifier only):\n\n```python\nfrom obspy.core.inventory.inventory import read_inventory\nfrom obspy.core.stream import read\n\nfrom sdaas.core import traces_idscores\n\n# Load a Stream object and its inventory\n# (use as example the test directory of the package):\nstream = read('./tests/data/GE.FLT1..HH?.mseed')\ninventory = read_inventory('./tests/data/GE.FLT1.xml')\n\n# Compute the Stream anomaly score:\noutput = traces_idscores(stream, inventory, idfunc=lambda t: t.get_id())\n```\nThen `output` is:\n```\n(['GE.FLT1..HHE', 'GE.FLT1..HHN', 'GE.FLT1..HHZ'], array([ 0.45729656,  0.45199387,  0.45113142]))\n```\n\nYou can also compute scores and ids from iterables of streams (e.g., when \nreading from files)...\n\n```python\nfrom sdaas.core import streams_scores\nfrom sdaas.core import streams_idscores\n```\n\n... or from a single trace:\n\n```python\nfrom sdaas.core import trace_score\n```\n\nFor instance, to compute the anomaly score of several streams\n(for each stream and for each trace therein, return the trace anomaly score):\n\n```python\nfrom obspy.core.inventory.inventory import read_inventory\nfrom obspy.core.stream import read\n\nfrom sdaas.core import streams_scores\n\n# Load a Stream objects and its inventory\n# (use as example the test directory of the package\n# and mock a list of streams by loading twice the same Stream):\nstreams = [read('./tests/data/GE.FLT1..HH?.mseed'),\n           read('./tests/data/GE.FLT1..HH?.mseed')]\ninventory = read_inventory('./tests/data/GE.FLT1.xml')\n\n# Compute Streams scores:\noutput = streams_scores(streams, inventory)\n```\nThen `output` is:\n```\n[ 0.45729656  0.45199387  0.45113142  0.45729656  0.45199387  0.45113142]\n```\n\nSame as above, computing the features and the scores separately for more \ncontrol:\n\n```python\nfrom obspy.core.inventory.inventory import read_inventory\nfrom obspy.core.stream import read\n\nfrom sdaas.core import trace_features, aa_scores\n\n# Load a Stream object and its inventory\n# (use as example the test directory of the package\n# and mock a list of streams by loading twice the same Stream):\nstreams = [read('./tests/data/GE.FLT1..HH?.mseed'),\n           read('./tests/data/GE.FLT1..HH?.mseed')]\ninventory = read_inventory('./tests/data/GE.FLT1.xml')\n\n# Compute Streams scores:\nfeats = []\nfor stream in streams:\n    for trace in stream:\n        feats.append(trace_features(trace, inventory))\noutput = aa_scores(feats)\n```\nThen `output` is:\n```\n[0.45729656, 0.45199387, 0.45113142, 0.45729656, 0.45199387, 0.45113142]\n```\n\n## Maintenance\n\nAlthough scikit learn is not used anymore for\n[maintainability limitations](https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations), \nyou can always consult the [README](./sdaas/core/models) \nexplaining how to manage create new scikit-learn models.\n\n\n## Citation\n\n**Software:**\n\n> Zaccarelli, Riccardo (2022). sdas - a Python tool computing an amplitude anomaly score of seismic data and metadata using simple machine‐Learning models. GFZ Data Services. https://doi.org/10.5880/GFZ.2.6.2023.009\n\n**Research article:**\n\n> Riccardo Zaccarelli, Dino Bindi, Angelo Strollo; Anomaly Detection in Seismic Data–Metadata Using Simple Machine‐Learning Models. Seismological Research Letters 2021; 92 (4): 2627–2639. doi: https://doi.org/10.1785/0220200339\n\n\n\n\n",
                "dependencies": "certifi==2022.12.7\ncharset-normalizer==2.1.1\ncontourpy==1.0.6\ncycler==0.11.0\ndecorator==5.1.1\nfonttools==4.38.0\ngreenlet==2.0.1\nidna==3.4\nkiwisolver==1.4.4\nlxml==4.9.1\nmatplotlib==3.6.2\nnumpy==1.23.5\nobspy==1.4.0\npackaging==22.0\nPillow==9.3.0\npyparsing==3.0.9\npython-dateutil==2.8.2\nrequests==2.28.1\nscipy==1.9.3\n# -e git+https://github.com/rizac/sdaas.git@89aaf28d7f6f77393d1ce32d62fba7f8f4895e8f#egg=sdaas\nsix==1.16.0\nSQLAlchemy==1.4.44\nurllib3==1.26.13\n\nfrom setuptools import setup, find_packages\n\n_README = \"\"\"\nPython program to compute amplitude anomaly score on one or more seismic\nwaveforms (data and metadata)\n\"\"\"\n\nsetup(\n    name='sdaas',\n    version='1.2.0',\n    description=_README,\n    url='https://github.com/rizac/sdaas',\n    packages=find_packages(exclude=['tests', 'tests.*']),\n    python_requires='>=3.6.9',\n    # Minimal requirements, for a complete list see requirements-*.txt\n    install_requires=[\n        'numpy>=1.15.4',\n        'obspy>=1.1.1'\n    ],\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). You can install these using the following syntax,\n    # for example:\n    # $ pip install -e .[jupyter,test]\n    extras_require={\n        # 'jupyter': [\n        #     'jupyter>=1.1.0'\n        # ],\n        'dev': [\n            'scikit-learn>=0.21.3'\n        ]\n    },\n    author='riccardo zaccarelli',\n    author_email='',  # FIXME: what to provide?\n    maintainer='Section 2.6 (Seismic Hazard and Risk Dynamics), GFZ Potsdam',  # FIXME\n    maintainer_email='',\n    classifiers=(\n        'Development Status :: 1 - Beta',\n        'Intended Audience :: Education',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: GNU General Public License v3',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python :: 3',\n        'Topic :: Scientific/Engineering',\n    ),\n    keywords=[\n        \"seismic waveform\",\n        \"isolation forest\",\n        \"outlier score\",\n        \"anomaly detection\",\n        \"machine learning\"\n    ],\n    license=\"GPL3\",\n    platforms=[\"any\"],  # FIXME: shouldn't be unix/macos? (shallow google search didn't help)\n    # package_data={\"smtk\": [\n    #    \"README.md\", \"LICENSE\"]},\n\n    # make the installation process copy also the iforest models (see MANIFEST.in)\n    # for info see https://python-packaging.readthedocs.io/en/latest/non-code-files.html\n    include_package_data=True,\n    zip_safe=False,\n    # To provide executable scripts, use entry points in preference to the\n    # \"scripts\" keyword. Entry points provide cross-platform support and allow\n    # pip to create the appropriate form of executable for the target platform.\n    entry_points={\n        'console_scripts': [\n            'sdaas=sdaas.run:cli_entry_point',\n        ],\n    },\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/seisbench",
            "repo_link": "https://github.com/seisbench/seisbench",
            "content": {
                "codemeta": "",
                "readme": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/seisbench/seisbench/main/docs/_static/seisbench_logo_subtitle_outlined.svg\" />\n</p>\n\n---\n\n[![PyPI - License](https://img.shields.io/pypi/l/seisbench)](https://github.com/seisbench/seisbench/blob/main/LICENSE)\n[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/seisbench/seisbench/main_push.yml?branch=main)](https://github.com/seisbench/seisbench)\n[![Read the Docs](https://img.shields.io/readthedocs/seisbench)](https://seisbench.readthedocs.io/en/latest/)\n[![PyPI](https://img.shields.io/pypi/v/seisbench)](https://pypi.org/project/seisbench/)\n[![Python 3.9](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5568813.svg)](https://doi.org/10.5281/zenodo.5568813)\n\nThe Seismology Benchmark collection (*SeisBench*) is an open-source python toolbox for\nmachine learning in seismology.\nIt provides a unified API for accessing seismic datasets and both training and applying machine learning algorithms to seismic data.\nSeisBench has been built to reduce the overhead when applying or developing machine learning techniques for seismological tasks.\n\n## Getting started\n\nSeisBench offers three core modules, `data`, `models`, and `generate`.\n`data` provides access to benchmark datasets and offers functionality for loading datasets.\n`models` offers a collection of machine learning models for seismology.\nYou can easily create models, load pretrained models or train models on any dataset.\n`generate` contains tools for building data generation pipelines.\nThey bridge the gap between `data` and `models`.\n\nThe easiest way of getting started is through our colab notebooks.\n\n| Examples                                         |  |\n|--------------------------------------------------|---|\n| Dataset basics                                   | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/01a_dataset_basics.ipynb) |\n| Model API                                        | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/01b_model_api.ipynb) |\n| Generator Pipelines                              | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/01c_generator_pipelines.ipynb) |\n| Applied picking                                  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/02a_deploy_model_on_streams_example.ipynb) |\n| Using DeepDenoiser                               | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/02b_deep_denoiser.ipynb) |\n| Depth phases and earthquake depth                | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/02c_depth_phases.ipynb) |\n| Training PhaseNet (advanced)                     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03a_training_phasenet.ipynb) |\n| Creating a dataset (advanced)                    | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03b_creating_a_dataset.ipynb) |\n| Building an event catalog with GaMMA (advanced)  | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03c_catalog_seisbench_gamma.ipynb) |\n| Building an event catalog with PyOcto (advanced) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03d_catalog_seisbench_pyocto.ipynb)        |\n\nAlternatively, you can clone the repository and run the same [examples](https://github.com/seisbench/seisbench/tree/main/examples) locally.\n\nFor more detailed information on Seisbench check out the [SeisBench documentation](https://seisbench.readthedocs.io/).\n\n## Installation\n\nSeisBench can be installed in two ways.\nIn both cases, you might consider installing SeisBench in a virtual environment, for example using [conda](https://docs.conda.io/en/latest/).\n\nThe recommended way is installation through pip.\nSimply run:\n```\npip install seisbench\n```\n\nAlternatively, you can install the latest version from source.\nFor this approach, clone the repository, switch to the repository root and run:\n```\npip install .\n```\nwhich will install SeisBench in your current python environment.\n\n### CPU only installation\n\nSeisBench is built on pytorch, which in turn runs on CUDA for GPU acceleration.\nSometimes, it might be preferable to install pytorch without CUDA, for example, because CUDA will not be used and the CUDA binaries are rather large.\nTo install such a pure CPU version, the easiest way is to follow a two-step installation.\nFirst, install pytorch in a pure CPU version [as explained here](https://pytorch.org/).\nSecond, install SeisBench the regular way through pip.\nExample instructions would be:\n```\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\npip install seisbench\n```\n\n## Contributing\nThere are many ways to contribute to SeisBench and we are always looking forward to your contributions.\nCheck out the [contribution guidelines](https://github.com/seisbench/seisbench/blob/main/CONTRIBUTING.md) for details on how to contribute.\n\n## Known issues\n\n- Some institutions and internet providers are blocking access to our data and model repository, as it is running on a non-standard port (2880).\n  This usually manifests in timeouts when trying to download data or model weights.\n  To verify the issue, try accessing [https://hifis-storage.desy.de:2880/](https://hifis-storage.desy.de:2880/) directly from the same machine.\n  As a mitigation, you can use our backup repository. Just run `seisbench.use_backup_repository()`.\n  Please note that the backup repository will usually show lower download speeds.\n  We recommend contacting your network administrator to allow outgoing access to TCP port 2880 on our server as a higher performance solution.\n- We've recently changed the URL of the SeisBench repository. To use the new URL update to SeisBench 0.4.1.\n  It this is not possible, you can use the following commands within your runtime to update the URL manually:\n  ```python\n  import seisbench\n  from urllib.parse import urljoin\n\n  seisbench.remote_root = \"https://hifis-storage.desy.de:2880/Helmholtz/HelmholtzAI/SeisBench/\"\n  seisbench.remote_data_root = urljoin(seisbench.remote_root, \"datasets/\")\n  seisbench.remote_model_root = urljoin(seisbench.remote_root, \"models/v3/\")\n  ```\n- On the Apple M1 and M2 chips, pytorch seems to not always work when installed directly within `pip install seisbench`.\n  As a workaround, follow the instructions at (https://pytorch.org/) to install pytorch and then install SeisBench as usual through pip.\n- EQTransformer model weights \"original\" in version 1 and 2 are incompatible with SeisBench >=0.2.3. Simply use `from_pretrained(\"original\", version=\"3\")` or `from_pretrained(\"original\", update=True)`. The weights will not differ in their predictions.\n\n## References\nReference publications for SeisBench:\n\n---\n\n* [SeisBench - A Toolbox for Machine Learning in Seismology](https://doi.org/10.1785/0220210324)\n\n  _Reference publication for software._\n\n---\n\n* [Which picker fits my data? A quantitative evaluation of deep learning based seismic pickers](https://doi.org/10.1029/2021JB023499)\n\n  _Example of in-depth bencharking study of deep learning-based picking routines using the SeisBench framework._\n\n---\n\n## Acknowledgement\n\nThe initial version of SeisBench has been developed at [GFZ Potsdam](https://www.gfz-potsdam.de/) and [KIT](https://www.gpi.kit.edu/) with funding from [Helmholtz AI](https://www.helmholtz.ai/).\nThe SeisBench repository is hosted by [HIFIS - Helmholtz Federated IT Services](https://www.hifis.net/).\n\n",
                "dependencies": "[build-system]\nrequires = [\"wheel\", \"setuptools >= 61.0.0\", \"setuptools_scm[toml]>=6.2\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"seisbench\"\ndynamic = [\"version\"]\ndescription = \"The seismological machine learning benchmark collection\"\nreadme = \"README.md\"\nlicense = { text = \"GPLv3\" }\nrequires-python = \">=3.9\"\nauthors = [\n    { name = \"Jack Woolam\", email = \"jack.woollam@kit.edu\" },\n    { name = \"Jannes Münchmeyer\", email = \"munchmej@gfz-potsdam.de\" },\n]\nmaintainers = [\n    { name = \"Jack Woolam\", email = \"jack.woollam@kit.edu\" },\n    { name = \"Jannes Münchmeyer\", email = \"munchmej@gfz-potsdam.de\" },\n]\nkeywords = [\"seismology\", \"machine learning\", \"signal processing\", \"earthquake\"]\nclassifiers = [\n    \"Intended Audience :: Science/Research\",\n    \"Topic :: Scientific/Engineering\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Operating System :: OS Independent\",\n    \"License :: OSI Approved :: GNU General Public License v3 (GPLv3)\",\n]\ndependencies = [\n    # < 2 pinning is required for obspy (https://github.com/obspy/obspy/issues/3484)\n    # remove once fixed with obspy 1.5\n    \"numpy>=1.21.6, <2.0\",\n    \"pandas>=1.1\",\n    \"h5py>=3.1\",\n    \"obspy>=1.3.1\",\n    \"tqdm>=4.52\",\n    \"torch>=1.10.0\",\n    \"scipy>=1.9\",\n    \"nest_asyncio>=1.5.3\",\n    \"bottleneck>=1.3\",\n]\n\n[tool.setuptools.packages.find]\ninclude = [\"seisbench*\"]\n\n[project.optional-dependencies]\ndevelopment = [\"flake8\", \"black\", \"isort\", \"pre-commit\"]\ntests = [\"pytest\", \"pytest-asyncio\"]\n\n[tool.setuptools_scm]\n\n[project.urls]\nGitHub = \"https://github.com/seisbench/seisbench\"\nDocumentation = \"https://seisbench.readthedocs.io/en/latest/\"\nIssues = \"https://github.com/seisbench/seisbench/issues\"\n\n[tool.pytest.ini_options]\nmarkers = [\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n]\n\nfrom setuptools import setup\n\nsetup()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/seiscomp",
            "repo_link": "https://github.com/SeisComP",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/sensor-management-system",
            "repo_link": "https://codebase.helmholtz.cloud/hub-terra/sms/orchestration",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/serghei",
            "repo_link": "https://gitlab.com/serghei-model/serghei",
            "content": {
                "codemeta": "",
                "readme": "# SERGHEI\n\nSimulation Environment for Geomorphology, Hydrodynamics and Ecohydrology in Integrated form\n\n[![doi](https://zenodo.org/badge/DOI/10.5281/zenodo.8159947.svg)](https://doi.org/10.5281/zenodo.8159947)\n[![BSD3 License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n[![doi](https://img.shields.io/badge/rsd-serghei-00a3e3.svg)](https://helmholtz.software/software/serghei)\n[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8B-yellow)](https://fair-software.eu)\n\n\n# Dependencies\n\nSERGHEI has the following dependencies:\n\n+ [Kokkos](https://github.com/kokkos/kokkos) handles the\nparallelization\n+ [Parallel NetCDF](https://github.com/Parallel-NetCDF/PnetCDF) writes\noutput files\n+ We use [R](https://www.r-project.org/) scripts for postprocessing (optional)\n\nBoth Kokkos and PNetCDF are linked as git submodules in this project. If you are unfamiliar with submodules, you can simply clone this repo together with the submodules with the `git clone --recurse-submodules` command.\n\nPnetCDF is often available in Linux distributions via package managers, and also as software modules in HPC systems. It is recommended to use these system wide installations, and only fall back on building the PNetCDF from source if there's no other option.\n\n# Building SERGHEI with CMake\nThe CMake workflow is the recommended approach to build SERGHEI. This assumes you have cloned the Kokkos submodule.\n\n1. Build Kokkos\n```\nbash buildKokkos [GPU_ARCHITECTURE]\n``` \nThe `GPU_ARCHITECTURE` argument is optional (it defaults to a shared memory CPU architecture). If provided it must be a valid [GPU architecture string as defined by Kokkos](https://kokkos.org/kokkos-core-wiki/keywords.html?highlight=ampere80#architectures), and matching your GPU architecture. \n\nThis configures and builds Kokkos with the target backend and architecture.\n\n2. Configure and build SERGHEI\n```\ncmake . [-DSERGHEI_OPTION -DSERGHEI_OPTION ...]\nmake\n```\n\nNote that when builing SERGHEI, there is no backend/architecture selection (this was done in the Kokkos build step).\n\nBuild options for SERGHEI are passed to CMake as usual, e.g., `-DSERGHEI_WRITE_HZ=ON`. Read the [documentation on the available build options](https://gitlab.com/serghei-model/serghei/-/wikis/CMake-build-options).\n\n# Non-Cmake installation\n\nThis procedure is deprecated, and only partially maintained for legacy reasons. It will be progressively phased-out.\n\n## Environment configuration\nThe environment needs to be configured properly for this procedure to be consistent.\n\n### Automatic configuration\nSERGHEI provides the `scripts/configure.sh` script to help you configure your environment for SERGHEI, including setting up environment variables. You can simply run\n```bash\nbash ./scripts/configure.sh -a\n```\nwhich will fetch the dependencies and install them in your local SERGEHI directory.\nIn case you have already the dependencies and wish to use existing paths, take a look at\n```\nbash ./scripts/configure.sh --help\n```\n\n### Understanding the (manual) environment configuration \nYou need to set the SERGHEIPATH environment variable to your local SERGHEI root path. \n```\nexport SERGHEIPATH=/path/to/serghei\n```\nKeep in mind doing so is not a persisting configuration.\nFor persisting configuration, you can set this in your `.bashrc` file. \n\nPaths in the build scripts, but also in other workflows in SERGHEI use the `SERGHEIPATH` environment variable.\n\n## Building SERGHEI with Make\nSERGHEI can be built through `make`. Before attempting to compile, the correct paths pointing to `Kokkos` and `PNetCDF` must be included in the `src/Makefile`. The default paths for these are `$SERGHEIPATH/Kokkos` and `$SERGHEIPATH/PnetCDF` or system paths. If you have these dependencies elsewhere, point the variable `PNETCDF_PATH` to the `PNetCDF` base folder and the variable `KOKKOS_PATH` to the `Kokkos` base folder. The automatic environment configuration should also do this for you.\n\nTo build SERGHEI, use either one of these:\n\n+ `make arch=cpu`: compiles the code for CPU (OpenMP + MPI)\n+ `make arch=gpu device=DEVICESM`: compiles the code for GPU (CudaUVM + MPI). DEVICESM is the keyword for the device architecture for your GPUs. The list of architecture keywords can be found \n[here](https://github.com/kokkos/kokkos/wiki/Compiling#table-43-architecture-variables). Commonly used may be, for example Ampere80, Volta70, Pascal61, etc.\n\nThis places an executable `serghei` into the directory `$SERGHEITPATH/bin` directory.\n\nFurther commands:\n+ `make clean`: cleans the program objects and the exe file\n+ `make kclean`: cleans the prgram objects, the exe file and the Kokkos\nobjects\n\n### Model component compilation flags\nSERHGEI's Makefile includes flags to control which model components are compiled. The default configuration of SERGHEI components is hardcoded. These flags allow you to control which model components are actually compiled. The possible flags are commented inside the Makefile, therefore, unless you either uncomment some of them, or pass them to Make through the command line, you will get the default compilation configuration.\nYou can control through the command line the setup, for example \n```\n$ make arch=cpu SERGHEI_SUBSURFACE_MODEL=1\n``` \nwill compile SERGHEI for CPUs, including the subsurface solver. \n\nAnother example, to compile for Volta GPUs, including subsurface and particle tracking modules is\n```\n$ make arch=gpu device=Volta70 SERGHEI_SUBSURFACE_MODEL=1 SERGHEI_PARTICLE_TRACKING=1\n``` \nTake a look at the [full list of compilation flags](https://gitlab.com/serghei-model/serghei/-/wikis/User-Guide/Build-flags).\n\n\n# Running SERGHEI\n\nOnce installed, SERGHEI can be invoked by:\n\n```\n$ mpirun -n N ./serghei inputDir/ outputDir/ M\n```\n\nwhere\n\n+ `N`: number of MPI tasks (subdomains). This must be in accordance\nwith the partition chosen in `parameters.input`\n+ `inputDir`: directory where the input files are located\n+ `outputDir`: directory where the output files will be located\n+ `M`: number of threads (OpenMP) or number of GPUs per resource set\n(GPUs)\n\n## Examples and test cases\n\nTake a look at the [collection of test cases](https://gitlab.com/serghei-model/serghei-tests).\n\nTo run the test case located at `cases/paraboloid2`, execute SERGHEI with \n\n```\n$ mpirun -n 2 /path/to/serghei/bin/serghei ../cases/paraboloid2/ output/ 4\n```\n\nDepending on the architecture, this command causes different things to\nhappen:\n\n1. If the code has been compiled for CPU, this means that it would be\n2 subdomains (MPI tasks) parallelized with 4 threads per subdomain\n(OpenMP).\n2. If the code has been compiled for GPU, this means that it would be\n8 subdomains (MPI tasks). The code is run on 2 nodes, each of them\ncontaining 4 GPUs.\n\nSimilarly, the use of `mpirun` is conditioned to the execution with\nMPI and the corresponding architecture. For example, the code can be\nrun just using:\n\n```\n/path/to/serghei/bin/serghei ./cases/paraboloid2/ output/ 1\n```\n\n# Known issues\n\n+ The `clang` compiler may fail to correctly load the OpenMP\nlibrary. Thus, if SERGHEI is compiled with `clang`, OpenMP may not be\navailable.\n+ `gcc-10` has trouble compiling Parallel NetCDF and throws a type\n  mismatch errors. The errors can be turned into warnings by passing\n  ```\n  FCFLAGS=\"-fallow-argument-mismatch\" FFLAGS=\"-fallow-argument-mismatch\"\n  ```\n  to `configure` and `make`. See [this github issue](https://github.com/Unidata/netcdf-fortran/issues/212).\n\n# How to cite \nPlease cite the software using the corresponding [SERGHEI Zenodo DOI](https://doi.org/10.5281/zenodo.8159947), and if necessary with the specific release DOI.\n\nYou can refer to the [SERGHEI-SWE paper](https://gmd.copernicus.org/articles/16/977/2023/) for the shallow water module SERGHEI-SWE.\n```\n@Article{Caviedes2023,\nAUTHOR = {Caviedes-Voulli\\`eme, D. and Morales-Hern\\'andez, M. and Norman, M. R. and \\\"Ozgen-Xian, I.},\nTITLE = {SERGHEI (SERGHEI-SWE) v1.0: a performance portable high-performance parallel-computing shallow-water solver for hydrology and environmental hydraulics},\nJOURNAL = {Geoscientific Model Development Discussions},\nVOLUME = {16}\nYEAR = {2023},\nPAGES = {977--1008},\nURL = {https://gmd.copernicus.org/articles/16/977/2023/},\nDOI = {10.5194/gmd-16-977-2023}\n}\n```\n\n",
                "dependencies": "#   This is the CMakeLists file for building and compiling SERGHEI\n#   To use, open a terminal, cd to the SERGHEI directory and type \"cmake .\"\n#   Then, type \"make\" to compile SERGHEI\n#   Note: If you changed any cmake options herein, before rebuilding, delete \"CMakeCache.txt\" to erase old cmake settings\n\ncmake_minimum_required (VERSION 3.20)\n#IF(${CMAKE_VERSION} VERSION_GREATER_EQUAL \"3.12.0\")\n#  MESSAGE(STATUS \"Setting policy CMP0074 to use <Package>_ROOT variables\")\n#  CMAKE_POLICY(SET CMP0074 NEW)\n#ENDIF()\n\n#   If needed, specify the CXX compiler here, otherwise comment out these lines\nset(CMAKE_C_COMPILER gcc)\nset(CMAKE_CXX_COMPILER g++)\n\n# Name the project\nproject (serghei LANGUAGES CXX)\n\n# Enable color output for messages\nstring(ASCII 27 Esc)\nset(ColourReset \"${Esc}[m\")\nset(ColourBold  \"${Esc}[1m\")\nset(Red         \"${Esc}[31m\")\nset(Green       \"${Esc}[32m\")\nset(Yellow      \"${Esc}[33m\")\nset(Blue        \"${Esc}[34m\")\nset(Magenta     \"${Esc}[35m\")\nset(Cyan        \"${Esc}[36m\")\nset(White       \"${Esc}[37m\")\nset(BoldRed     \"${Esc}[1;31m\")\nset(BoldGreen   \"${Esc}[1;32m\")\nset(BoldYellow  \"${Esc}[1;33m\")\nset(BoldBlue    \"${Esc}[1;34m\")\nset(BoldMagenta \"${Esc}[1;35m\")\nset(BoldCyan    \"${Esc}[1;36m\")\nset(BoldWhite   \"${Esc}[1;37m\")\nset(Gray        \"${Esc}[90m\")\nset(DebugColour ${Magenta})\nset(EnableColour ${Green})\nset(DisableColour ${Yellow})\nset(ncColour ${Cyan})\nset(MiscColour ${Blue})\nset(DefaultColour ${Gray}) # colour used for default options\n\n\nlist(APPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/cmake)\n\nif(NOT DEFINED CMAKE_CXX_STANDARD)\n  set(CMAKE_CXX_STANDARD 17)\nendif()\nset(CMAKE_CXX_STANDARD_REQUIRED On)\nset(CMAKE_CXX_EXTENSIONS Off)\n\n\n# find SERGHEI dependencies \nfind_package(PkgConfig REQUIRED)\nif (PkgConfig_FOUND)\n\tpkg_check_modules(PnetCDF REQUIRED pnetcdf)\nelse()\n\tset(PNETCDF_ROOT \"./PnetCDF\")\n\tset(PnetCDF_INCLUDE_DIRS ${PNETCDF_ROOT}/include)\n\tset(PnetCDF_LINK_LIBRARIES ${PNETCDF_ROOT}/lib/libpnetcdf.a)\nendif()\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\n#   Specify the path where Kokkos is built\nset(Kokkos_ROOT \"./kokkos/install\")\n\n#   Specify the path where KokkosKernels is built\nset(KokkosKernels_ROOT \"./kokkos-kernels/install\")\n\n#   Specify if using GPU or not\nset(Enable_GPU \"OFF\")\n\n#\tIf using GPU, specify the architecture\nif (Enable_GPU)\n\tif (NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n\t\tset(CMAKE_CUDA_ARCHITECTURES 80)\n\tendif()\nendif()\n\n#\tSpecify if the CI tests will be built\nset(Build_Tests \"ON\")\n\n# Specify SERGHEI options (You can add more below)\n#\tThe defaults should be consistent with what is defined in SERGHEI source code\n\nexecute_process(COMMAND git describe COMMAND tr -d '\\n' OUTPUT_VARIABLE serghei_git_version)\nadd_compile_definitions(SERGHEI_GIT_VERSION=\"${serghei_git_version}\")\nmessage(STATUS \"SERGHEI version: \" ${serghei_git_version})\n\nmessage(STATUS ${ColourBold} \"SERGHEI OPTIONS\" ${ColourReset} \n\" (\"\n${DefaultColour} \"Default \" ${ColourReset} \"| \"\n${EnableColour} \"Enabled \" ${ColourReset} \"| \"\n${DisableColour} \"Disabled \" ${ColourReset} \"| \"\n${DebugColour} \"Debug\"\n${ColourReset}\n\")\"\n)\n\noption(SERGHEI_SWE_GW \"Enable coupled shallow water and subsurface models\" OFF)\nif(SERGHEI_SWE_GW)\n\tadd_compile_definitions(SERGHEI_SWE_GW=1)\n\tmessage(STATUS ${EnableColour} \"Coupled shallow water and subsurface models enabled\" ${ColourReset})\n\tset(SERGHEI_SWE_MODEL ON)\n\tset(SERGHEI_SUBSURFACE_MODEL ON)\nelse()\n\tadd_compile_definitions(SERGHEI_SWE_GW=0)\n\tmessage(STATUS ${DefaultColour} \"Coupled shallow water and subsurface models disabled\" ${ColourReset})\nendif()\n\noption(SERGHEI_SUBSURFACE_MODEL \"Enable subsurface model\" OFF)\nif(SERGHEI_SUBSURFACE_MODEL)\n\tadd_compile_definitions(SERGHEI_SUBSURFACE_MODEL=1)\n\tif(NOT SERGHEI_SWE_GW)\n\t\tset(SERGHEI_SWE_MODEL OFF)\n\tendif()\n\tmessage(STATUS ${EnableColour} \"Subsurface model enabled\" ${ColourReset})\n  # Specify the path where KokkosKernels is built\n  #set(KokkosKernels_ROOT \"/Users/zli/Codes/kokkoskernels4-omp\")\nendif()\n\noption(SERGHEI_SWE_MODEL \"Enable shallow water model\" ON)\nif(SERGHEI_SWE_MODEL)\n\tadd_compile_definitions(SERGHEI_SWE_MODEL=1)\n\tmessage(STATUS ${DefaultColour} \"Shallow water model enabled\" ${ColourReset})\nelse()\n\tadd_compile_definitions(SERGHEI_SWE_MODEL=0)\n\tmessage(STATUS ${DisableColour} \"Shallow water model disabled\" ${ColourReset})\nendif()\n\n\noption(SERGHEI_PARTICLE_TRACKING \"Enable particle tracking\" OFF)\nif(SERGHEI_PARTICLE_TRACKING)\n\tadd_compile_definitions(SERGHEI_PARTICLE_TRACKING=1)\n\tmessage(STATUS ${EnableColour} \"Particle tracking enabled\" ${ColourReset})\nendif()\n\noption(SERGHEI_VEGETATION_MODEL \"Enable vegetation model\" OFF)\nif(SERGHEI_VEGETATION_MODEL)\n\tadd_compile_definitions(SERGHEI_VEGETATION_MODEL=1)\n\tmessage(STATUS ${EnableColour} \"Vegetation model enabled\" ${ColourReset})\nendif()\n\noption(SERGHEI_TOOLS \"Enable additional tools\" ON)\nif(SERGHEI_TOOLS)\n\tmessage(STATUS ${DefaultColour} \"Tools enabled\" ${ColourReset})\nelse()\n\tmessage(STATUS ${DisableColour} \"Tools disabled\" ${ColourReset})\n\tadd_compile_definitions(SERGHEI_TOOLS=0)\nendif()\n\noption(SERGHEI_FRICTION_MODEL \"Enable friction model\" ON)   \nif(SERGHEI_FRICTION_MODEL)\n\tmessage(STATUS ${DefaultColour} \"Friction model enabled (Manning default)\" ${ColourReset})\nelse()\n\tmessage(STATUS ${DisableColour} \"Friction model disabled\" ${ColourReset})\n\tadd_compile_definitions(SERGHEI_FRICTION_MODEL=0)\nendif()\n\noption(SERGHEI_FRICTION_CENTERED \"Enabled centered friction discretisation\" OFF)   \nif(SERGHEI_FRICTION_CENTERED)\n\tmessage(STATUS ${EnableColour} \"Centered friction discretisation enabled\" ${ColourReset})\n\tadd_compile_definitions(SERGHEI_POINTWISE_FRICTION=1)\nelse()\n\tmessage(STATUS ${DefaultColour} \"Centered friction discretisation disabled\" ${ColourReset})\nendif()\n\noption(SERGHEI_FRICTION_DARCY_WEISBACH \"Enable Darcy-Weisbach friction model\" OFF)   \nif(SERGHEI_FRICTION_DARCY_WEISBACH)\n\tadd_compile_definitions(SERGHEI_FRICTION_MODEL=2)\n\tmessage(STATUS ${EnableColour} \"Enabling Darcy-Weisbach friction model\" ${ColourReset})\nendif()\n\noption(SERGHEI_FRICTION_CHEZY \"Enable Chezy friction model\" OFF)   \nif(SERGHEI_FRICTION_CHEZY)\n\tadd_compile_definitions(SERGHEI_FRICTION_MODEL=3)\n\tmessage(STATUS ${EnableColour} \"Enabling Chezy friction model\" ${ColourReset})\nendif()\n\n\noption(SERGHEI_MESH_UNIFORM \"Set uniform cartesian grid mesh\" ON)\nif(SERGHEI_MESH_UNIFORM)\n\tmessage(STATUS ${DefaultColour} \"Mesh set to uniform cartesian grid\" ${ColourReset})\nelse()\n\tmessage(SEND_ERROR \"Non-uniform mesh not available\")\nendif()\n\n# Arithmetic precision options\noption(SERGHEI_FP32 \"Set single precision computations\" OFF)\nif(SERGHEI_FP32)\n\tmessage(STATUS ${MiscColour} \"Single precision set\" ${ColourReset})\n\tadd_compile_definitions(SERGHEI_REAL=1)\n\tset(SERGHEI_NC_FLOAT ON)\n\tset(SERGHEI_NC_DOUBLE OFF)\nelse()\n\tmessage(STATUS ${DefaultColour} \"Double precision set\" ${ColourReset})\n\tadd_compile_definitions(SERGHEI_REAL=2)\nendif()\n\n# I/O options\noption(SERGHEI_MAXFLOOD \"Enable maximum flood statistics\" OFF)\nif(SERGHEI_MAXFLOOD)\n\tmessage(STATUS ${EnableColour} \"Enabling maximum flood statistics\" ${ColourReset})\n\tadd_compile_definitions(SERGHEI_MAXFLOOD=1)\nendif()\n\noption(SERGHEI_WRITE_HZ \"Write water surface elevation output\" OFF)\nif(SERGHEI_WRITE_HZ)\n\tmessage(STATUS ${EnableColour} \"Writing water surface elevation output\" ${ColourReset})\n\tadd_compile_definitions(SERGHEI_WRITE_HZ=1)\nendif()\n\noption(SERGHEI_INPUT_NETCDF \"Enable NetCDF input files\" OFF)\nif(SERGHEI_INPUT_NETCDF)\n\tmessage(STATUS ${EnableColour} \"NetCDF input enabled\" ${ColourReset})\n\tadd_compile_definitions(SERGHEI_INPUT_NETCDF=1)\nendif()\n\noption(SERGHEI_WRITE_SUBDOMS \"Enable writing subdomain indices in NetCDF output\" OFF)\nif(SERGHEI_WRITE_SUBDOMS)\n\tmessage(STATUS ${EnableColour} \"Subdomain indices enabled in NetCDF output\" ${ColourReset})\n\tadd_compile_definitions(SERGHEI_WRITE_SUBDOMS=1)\nendif()\n\n# NetCDF options\n# A useful option for large problems is NC_64BIT_DATA.\noption(SERGHEI_NC_CLOBBER  \"Set NetCDF mode to NC_CLOBBER\" ON)\n\noption(SERGHEI_NC_64BIT_DATA  \"Set NetCDF mode to NC_64BIT_DATA\" OFF)\nif(SERGHEI_NC_64BIT_DATA)\n\tadd_compile_definitions(SERGHEI_NC_MODE=NC_64BIT_DATA)\n\tmessage(STATUS ${ncColour} \"NetCDF mode set to NC_64BIT_DATA\" ${ColourReset})\n\tset(SERGHEI_NC_CLOBBER OFF)\nendif()\n\n# You can set up SERGHEI_NC_MODE and SERGHEI_NC_REAL to control NetCDF file behavior.\n# SERGHEI_NC_REAL determines if data is written as double or float (default follows SERGHEI_REAL).\n# Override at compile time with SERGHEI_NC_REAL=NC_DOUBLE or SERGHEI_NC_REAL=NC_FLOAT.\noption(SERGHEI_NC_FLOAT \"Set single precision for NetCDF\" OFF)\nif(SERGHEI_NC_FLOAT)\n\tmessage(STATUS ${MiscColour} \"NetCDF output set to single precision\" ${ColourReset})\n\tadd_compile_definitions(SERGHEI_NC_REAL=NC_FLOAT)\n\tset(SERGHEI_NC_DOUBLE OFF)\nendif()\n\noption(SERGHEI_NC_DOUBLE \"Set double precision for NetCDF\" ON)\t\nif(SERGHEI_NC_DOUBLE)\n\tmessage(STATUS ${DefaultColour} \"NetCDF output set to double precision\" ${ColourReset})\nendif()\n\noption(SERGHEI_NC_ENABLE_NAN \"Enable writing NaN for cells outside the domain\" ON)\nif(SERGHEI_NC_ENABLE_NAN)\n\tmessage(STATUS ${DefaultColour} \"NetCDF NaN enabled\" ${ColourReset})\nelse()\n\tmessage(STATUS ${MiscColour} \"NetCDF NaN disabled\" ${ColourReset})\n\tadd_compile_definitions(SERGHEI_NC_ENABLE_NAN=0)\nendif()\n\n# debugging options\noption(SERGHEI_DEBUG_PARALLEL_DECOMPOSITION \"Enable parallel decomposition debugging\" OFF)\nif(SERGHEI_DEBUG_PARALLEL_DECOMPOSITION)\n\tadd_compile_definitions(SERGHEI_DEBUG_PARALLEL_DECOMPOSITION=1)\n\tmessage(STATUS ${DebugColour}   \"Parallel decomposition debugging enabled\" ${ColourReset})\n  set(CMAKE_BUILD_TYPE \"Debug\")\nendif()\n\noption(SERGHEI_DEBUG_WORKFLOW \"Enable workflow debugging\" OFF)\nif(SERGHEI_DEBUG_WORKFLOW)\n\tadd_compile_definitions(SERGHEI_DEBUG_WORKFLOW=1)\n\tmessage(STATUS ${DebugColour} \"Workflow debugging enabled\" ${ColourReset})\n  set(CMAKE_BUILD_TYPE \"Debug\")\nendif()\n\noption(SERGHEI_DEBUG_KOKKOS_SETUP  \"Enable Kokkos setup debugging\" OFF)\nif(SERGHEI_DEBUG_KOKKOS_SETUP)\n\tadd_compile_definitions(SERGHEI_DEBUG_KOKKOS_SETUP=1)\n\tmessage(STATUS ${DebugColour}  \"Kokkos setup debugging enabled\" ${ColourReset})\n  set(CMAKE_BUILD_TYPE \"Debug\")\nendif()\n\noption(SERGHEI_DEBUG_BOUNDARY  \"Enable boundary debugging\" OFF)\nif(SERGHEI_DEBUG_BOUNDARY)\n\tadd_compile_definitions(SERGHEI_DEBUG_BOUNDARY=1)\n\tmessage(STATUS ${DebugColour}  \"Boundary debugging enabled\" ${ColourReset})\n  set(CMAKE_BUILD_TYPE \"Debug\")\nendif()\n\noption(SERGHEI_DEBUG_DT \"Enable timestep debugging\" OFF)\nif(SERGHEI_DEBUG_DT)\n\tadd_compile_definitions(SERGHEI_DEBUG_DT=1)\n\tmessage(STATUS ${DebugColour}  \"Time stepping debugging enabled\" ${ColourReset})\n  set(CMAKE_BUILD_TYPE \"Debug\")\nendif()\n\noption(SERGHEI_DEBUG_TOOLS  \"Enable tools debugging\" OFF)\nif(SERGHEI_DEBUG_TOOLS)\n\tadd_compile_definitions(SERGHEI_DEUBG_TOOLS=1)\n\tmessage(STATUS ${DebugColour}  \"Tools debugging enabled\" ${ColourReset})\n  set(CMAKE_BUILD_TYPE \"Debug\")\nendif()\n\noption(SERGHEI_DEBUG_MASS_CONS \"Enable mass conservation debugging\" OFF)\nif(SERGHEI_DEBUG_MASS_CONS)\n\tadd_compile_definitions(SERGHEI_DEBUG_MASS_CONS=1)\n\tmessage(STATUS ${DebugColour}  \"Mass conservation debugging enabled\" ${ColourReset})\n  set(CMAKE_BUILD_TYPE \"Debug\")\nendif()\n\noption(SERGHEI_DEBUG_INFILTRATION \"Enable infiltration debugging\" OFF)\nif(SERGHEI_DEBUG_INFILTRATION)\n\tadd_compile_definitions(SERGHEI_DEBUG_INFILTRATION=1)\n\tmessage(STATUS ${DebugColour}  \"Infiltration debugging enabled\" ${ColourReset})\n  set(CMAKE_BUILD_TYPE \"Debug\")\nendif()\t\n\noption(SERGHEI_DEBUG_MPI  \"Enable MPI debugging\" OFF)\nif(SERGHEI_DEBUG_MPI)\n\tadd_compile_definitions(SERGHEI_DEBUG_MPI=1)\n\tmessage(STATUS ${DebugColour}  \"MPI debugging enabled\" ${ColourReset})\n  set(CMAKE_BUILD_TYPE \"Debug\")\nendif()\n\noption(SERGHEI_DEBUG_INPUT_NETCDF  \"Enable NetCDF input debugging\" OFF)\nif(SERGHEI_DEBUG_INPUT_NETCDF)\n\tadd_compile_definitions(SERGHEI_DEBUG_INPUT_NETCDF=1)\n\tmessage(STATUS ${DebugColour}  \"NetCDF input debugging enabled\" ${ColourReset})\n  set(CMAKE_BUILD_TYPE \"Debug\")\nendif()\n\noption(SERGHEI_DEBUG_OUTPUT  \"Enable output debugging\" OFF)\nif(SERGHEI_DEBUG_OUTPUT)\n\tadd_compile_definitions(SERGHEI_DEBUG_OUTPUT=1)\n\tmessage(STATUS ${DebugColour}  \"Output debugging enabled\" ${ColourReset})\n  set(CMAKE_BUILD_TYPE \"Debug\")\nendif()\n\nif(CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n\tmessage(STATUS ${DebugColour} \"Enabling debug mode due to enabled debugging flags\" ${ColourReset})\nendif()\n\n\n# this is to print variables\n#include(CMakePrintHelpers)\n#cmake_print_variables(serghei_git_version)\n\n# print out the compile defintions, useful to debug CMakeLists\n# get_directory_property( DirDefs DIRECTORY ${CMAKE_SOURCE_DIR} COMPILE_DEFINITIONS )\n# message(STATUS \"SERGHEI Compilation definitions\")\n#foreach( d ${DirDefs} )\n#    message( STATUS \"\\t\" ${d})\n# endforeach()\n\n#   Add the subdirectory where the source codes are stored\nadd_subdirectory(src)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/sfctools",
            "repo_link": "https://gitlab.com/dlr-ve/esy/sfctools/framework",
            "content": {
                "codemeta": "",
                "readme": "# sfctools - A toolbox for stock-flow consistent, agent-based models\n\nSfctools is a lightweight and easy-to-use Python framework for agent-based macroeconomic, stock-flow consistent (ABM-SFC) modeling. It concentrates on agents in economics and helps you to construct agents, helps you to manage and document your model parameters, assures stock-flow consistency, and facilitates basic economic data structures (such as the balance sheet). For more documentation, see https://sfctools-framework.readthedocs.io/en/latest/.\n\n## Installation\n\nWe recommend to install sfctools in a fresh Python 3.8 environment. For example, with conda, do\n\n    conda create --name sfcenv python=3.8\n    conda activate sfcenv\n    conda install pip\n\nThen, in a terminal of your choice, type:\n\n    pip install sfctools\n\nsee https://pypi.org/project/sfctools/\n\n## Usage with Graphical User Interface 'Attune'\n\nType\n\n    python -m sfctools attune\n\nto start the GUI.\n\n## Usage inside Python\n\nTry out this simple example:\n\n```python\nfrom sfctools import Agent, World \n\nclass SomeAgent(Agent):\n    def __init__(self, a):\n        super().__init__()\n        self.some_attribute = a\n\nmy_agent = SomeAgent(a='Hello')\nyour_agent = SomeAgent(a='World')\n\nmy_agents = World().get_agents_of_type(\"SomeAgent\")\nmy_message = my_agents[0].some_attribute\nyour_message = my_agents[1].some_attribute\n\nprint(\"%s says %s, %s says %s\" % (my_agent, my_message, your_agent, your_message))\n```\n\nThe resulting output will be\n\n```console \nSomeAgent__00001 says Hello, SomeAgent__00002 says World\n```\n\n## More Examples\n\nHave a look at the [documentation page](https://sfctools-framework.readthedocs.io/en/latest/doc_api_examples/examples_framework.html) for more examples. \n\n\n## Cite this Software\n\nYou can cite the software as follows: \n\nBaldauf, T., (2023). sfctools - A toolbox for stock-flow consistent, agent-based models. Journal of Open Source Software, 8(87), 4980, https://doi.org/10.21105/joss.04980\n\n\nYou can cite the software repository as follows:\n\nThomas Baldauf. (2023). sfctools - A toolbox for stock-flow consistent, agent-based models (1.1.0.2b). Zenodo. https://doi.org/10.5281/zenodo.8118870\n\n\n-----------------------------------\n\n| Corresponding author: Thomas Baldauf, German Aerospace Center (DLR), Curiestr. 4 70563 Stuttgart | thomas.baldauf@dlr.de |\n\n\n",
                "dependencies": "[tool.poetry]\nname = \"sfctools\"\nversion = \"1.1.9.2\"\ndescription = \"Framework for stock-flow consistent agent-based modeling, being developed at the German Aerospace Center (DLR) for and in the scientific context of energy systems analysis, however, it is widely applicable in other scientific fields.\"\n\nauthors = [\"Thomas Baldauf <thomas.baldauf@dlr.de>\"]\nlicense = \"MIT\"\nmaintainers = [\"Thomas Baldauf, Benjamin Fuchs <thomas.baldauf@dlr.de, benjamin.fuchs@dlr.de>\"]\nhomepage = 'https://gitlab.com/dlr-ve/esy/sfctools/framework'\ndocumentation = 'https://sfctools-framework.readthedocs.io/en/latest/'\nreadme = 'README.md'\nkeywords = [\"stock-flow-consistent\",\"agent-based\",\"agent\",\"macroeconomics\",\"computational economics\"]\n\npackages = [\n    { include = \"sfctools\" },\n]\n\n[tool.poetry.dependencies]\npython = \">=3.6,<=3.12\"\npyyaml = \">=3.0.3\" # \">=3.10\" # >=6.0\nsetuptools = \">=50.0.0\"\npandas = \"*\"# \">=1.3\"\nnumpy = \"*\"# \">=1.6.0\"\nattrs = \"*\" # \">=19.3.0\"\nmatplotlib = \">=2.0.0\" # \">=3.6\"\nseaborn = \"*\" # \">=0.9.0\" # \">=0.11.2\"\nnetworkx = \">=2.2\"\ngraphviz = \"*\" # \">=0.19\"\nPyQt5 =\">=5.9\"\n# pyqt5-tools =\"^5.14.4.3\"\n# pyqt5-plugins =\"^5.14.4.2\"\n# poetry = \"*\"\n# attrs = \"^19\" # \"19.3.0\"\nsympy= \">=1.10.0\" # \">=1.10\"\nscipy=\">1.9.1\" #  >1.9.1\n# pydot = \"*\" # \">=1.4\"\npyperclip = \">=1.5.0\"\nopenpyxl = \"*\" # \">=3.0.10\" might be needed in later versions\n# pdflatex = \"^0.1\" <- compatibility problem with cattrs and pdflatex\npytest-qt = \"*\"\npytest-cov = \"*\"\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/shepard",
            "repo_link": "https://gitlab.com/dlr-shepard",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/shockhash",
            "repo_link": "https://github.com/ByteHamster/ShockHash",
            "content": {
                "codemeta": "",
                "readme": "# ShockHash\n\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n![Build status](https://github.com/ByteHamster/ShockHash/actions/workflows/build.yml/badge.svg)\n\nA minimal perfect hash function (MPHF) maps a set S of n keys to the first n integers without collisions.\nPerfect hash functions have applications in databases, bioinformatics, and as a building block of various space-efficient data structures.\n\nShockHash (**s**mall, **h**eavily **o**verloaded **c**uc**k**oo **hash** tables) is an MPHF that achieves space very close to the lower bound,\nwhile still being fast to construct.\nIn contrast to the simple brute-force approach that needs to try e^n = 2.72^n different hash function seeds,\nShockHash significantly reduces the search space.\nInstead of sampling hash functions hoping for them to be minimal perfect, it samples random graphs,\nhoping for them to be a pseudoforest.\nIn its most space-efficient variant, it can reduce the running time to just 1.16^n,\nwhile still being asymptotically space optimal.\n\nStill being an exponential time algorithm, we integrate ShockHash into several partitioning frameworks.\nOur implementation inside the [RecSplit](https://github.com/vigna/sux/blob/master/sux/function/RecSplit.hpp) framework achieves the best space efficiency.\nUsing ShockHash inside our novel k-perfect hash function achieves fast queries\nwhile still being faster to construct and more space efficient than any previous approaches.\n\n### Library Usage\n\nClone this repo and add the following to your `CMakeLists.txt`.\nNote that the repo has submodules, so either use `git clone --recursive` or `git submodule update --init --recursive`.\n\n```cmake\nadd_subdirectory(path/to/ShockHash)\ntarget_link_libraries(YourTarget PRIVATE ShockHash)\n```\n\nThen use one of the following classes:\n\n- [ShockHash](https://github.com/ByteHamster/ShockHash/blob/main/include/ShockHash.h) is the original ShockHash algorithm integrated into the RecSplit framework.\n- [SIMDShockHash](https://github.com/ByteHamster/ShockHash/blob/main/include/SIMDShockHash.hpp) is the SIMD-parallel version of the original ShockHash algorithm. Both ShockHash and the RecSplit framework are SIMD-parallelized. If this implementation is used on a machine without SIMD support, it is slower than the non-SIMD version because SIMD operations are emulated.\n- [ShockHash2](https://github.com/ByteHamster/ShockHash/blob/main/include/ShockHash2.h) is the bipartite ShockHash algorithm. Only the inner ShockHash loop is SIMD-parallel, the RecSplit framework is not. If this implementation is used on a machine without SIMD support, the implementation uses sequential operations without explicitly emulating SIMD. To turn off SIMD, change to SIMD lanes of size 1 in [ShockHash2-internal.h](https://github.com/ByteHamster/ShockHash/blob/main/include/ShockHash2-internal.h).\n\nConstructing a ShockHash perfect hash function is then straightforward:\n\n```cpp\nstd::vector<std::string> keys = {\"abc\", \"def\", \"123\", \"456\"};\nshockhash::ShockHash<30, false> shockHash(keys, 2000); // ShockHash base case size n=30, bucket size b=2000\nstd::cout << shockHash(\"abc\") << \" \" << shockHash(\"def\") << \" \"\n          << shockHash(\"123\") << \" \" << shockHash(\"456\") << std::endl;\n// Output: 1 3 2 0\n```\n\nWe also give the base-case implementations without the RecSplit framework, which makes it easier to understand the main idea.\n\n- Original [ShockHash](https://github.com/ByteHamster/ShockHash/blob/main/benchmark/bijections/ShockHash1.h).\n- [Bipartite ShockHash](https://github.com/ByteHamster/ShockHash/blob/main/include/ShockHash2-internal.h). The outer loop that is also given in the pseudocode of the paper is given in `BijectionsShockHash2::findSeed`.\n\n### Construction performance\n\n[![Plots preview](https://raw.githubusercontent.com/ByteHamster/ShockHash/main/plots.png)](https://arxiv.org/abs/2310.14959)\n\n### Licensing\nShockHash is licensed exactly like `libstdc++` (GPLv3 + GCC Runtime Library Exception), which essentially means you can use it everywhere, exactly like `libstdc++`.\nYou can find details in the [COPYING](/COPYING) and [COPYING.RUNTIME](/COPYING.RUNTIME) files.\n\nIf you use [ShockHash](https://arxiv.org/abs/2308.09561) or [bipartite ShockHash](https://arxiv.org/abs/2310.14959) in an academic context or publication, please cite our papers:\n\n```\n@inproceedings{lehmann2023shockhash,\n  author = {Hans-Peter Lehmann and\n    Peter Sanders and\n    Stefan Walzer},\n  title = {{ShockHash}: Towards Optimal-Space Minimal Perfect Hashing Beyond Brute-Force},\n  booktitle = {{ALENEX}},\n  pages = {194--206},\n  publisher = {{SIAM}},\n  year = {2024},\n  doi = {10.1137/1.9781611977929.15}\n}\n\n@article{lehmann2023towardsArxiv,\n  author = {Hans-Peter Lehmann and\n    Peter Sanders and\n    Stefan Walzer},\n  title = {{ShockHash}: Towards Optimal-Space Minimal Perfect Hashing Beyond Brute-Force},\n  journal = {CoRR},\n  volume = {abs/2308.09561},\n  year = {2023},\n  doi = {10.48550/ARXIV.2308.09561}\n}\n```\n\n",
                "dependencies": "cmake_minimum_required (VERSION 3.25)\nproject(ShockHash\n    DESCRIPTION \"Various basic data structures\"\n    HOMEPAGE_URL \"https://github.com/ByteHamster/ShockHash\"\n    VERSION 1.0\n    LANGUAGES CXX)\n\nif(TARGET ShockHash)\n    return()\nendif()\n\nif (NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE \"Release\")\nendif ()\n\nif((CMAKE_BUILD_TYPE STREQUAL \"Release\" OR CMAKE_BUILD_TYPE STREQUAL \"RelWithDebInfo\") AND PROJECT_IS_TOP_LEVEL)\n    add_compile_options(-march=native)\nendif()\n\n# ---------------------------- Dependencies ----------------------------\nif(NOT TARGET vectorclass)\n    add_library(vectorclass INTERFACE)\n    target_include_directories(vectorclass INTERFACE extlib/vectorclass)\nendif()\n\nif(NOT TARGET Sux)\n    add_library(Sux INTERFACE)\n    target_include_directories(Sux SYSTEM INTERFACE extlib/sux)\nendif()\n\nif(NOT TARGET tlx)\n    set(TLX_INSTALL_INCLUDE_DIR tlx CACHE PATH \"Workaround for TLX breaking the first cmake call\")\n    add_subdirectory(extlib/tlx SYSTEM EXCLUDE_FROM_ALL)\nendif()\n\nif(NOT TARGET sdsl)\n    add_subdirectory(extlib/sdsl-lite/external SYSTEM EXCLUDE_FROM_ALL)\n    add_subdirectory(extlib/sdsl-lite/include SYSTEM EXCLUDE_FROM_ALL)\n    add_subdirectory(extlib/sdsl-lite/lib SYSTEM EXCLUDE_FROM_ALL)\n    target_compile_options(sdsl PRIVATE -w)\n    target_include_directories(sdsl SYSTEM INTERFACE extlib/sdsl-lite/include)\nendif()\n\nif(NOT TARGET simpleRibbon)\n    set(IPS2RA_DISABLE_PARALLEL ON CACHE PATH \"ips2ra's FindTBB greps a file that does not exist in recent TBB versions\")\n    add_subdirectory(extlib/simpleRibbon SYSTEM EXCLUDE_FROM_ALL)\n    find_package(TBB)\n    target_compile_options(ips2ra INTERFACE -D_REENTRANT)\n    target_link_libraries(ips2ra INTERFACE pthread atomic TBB::tbb)\nendif()\n\nif(NOT TARGET Ips2raShockHashSorter)\n    add_library(Ips2raShockHashSorter SHARED src/Sorter.cpp)\n    target_compile_features(Ips2raShockHashSorter PRIVATE cxx_std_20)\n    target_include_directories(Ips2raShockHashSorter PRIVATE include)\n    target_link_libraries(Ips2raShockHashSorter PUBLIC ips2ra tlx Sux)\n    target_compile_options(Ips2raShockHashSorter PRIVATE $<$<COMPILE_LANGUAGE:CXX>:-march=native>)\nendif()\n\nif(NOT TARGET ByteHamsterUtil)\n    add_subdirectory(extlib/util EXCLUDE_FROM_ALL)\nendif()\n\n# ---------------------------- Library Setup ----------------------------\n\nadd_library(ShockHash2Precompiled SHARED src/ShockHash2-precompiled.cpp)\ntarget_compile_features(ShockHash2Precompiled PRIVATE cxx_std_20)\ntarget_include_directories(ShockHash2Precompiled PRIVATE include)\ntarget_link_libraries(ShockHash2Precompiled PUBLIC ips2ra tlx Sux ByteHamsterUtil)\ntarget_compile_options(ShockHash2Precompiled PRIVATE $<$<COMPILE_LANGUAGE:CXX>:-march=native>)\n\nadd_library(ShockHash INTERFACE)\ntarget_include_directories(ShockHash INTERFACE include)\ntarget_compile_features(ShockHash INTERFACE cxx_std_20)\ntarget_link_libraries(ShockHash INTERFACE Sux SimpleRibbon Ips2raShockHashSorter ByteHamsterUtil ShockHash2Precompiled sdsl)\n\ninclude(${CMAKE_CURRENT_SOURCE_DIR}/extlib/cmake-findsse/FindSSE.cmake)\nFindSSE()\nif(SSE4_2_FOUND)\n    add_library(ShockHashSIMD INTERFACE)\n    target_link_libraries(ShockHashSIMD INTERFACE ShockHash vectorclass)\n    target_compile_options(ShockHashSIMD INTERFACE -DSIMD -march=native)\nendif()\n\n# ---------------------------- Benchmarks ----------------------------\nif(PROJECT_IS_TOP_LEVEL)\n    add_library(BenchmarkUtils INTERFACE)\n    target_include_directories(BenchmarkUtils INTERFACE benchmark)\n    target_include_directories(BenchmarkUtils INTERFACE test)\n    target_link_libraries(BenchmarkUtils INTERFACE tlx ByteHamster::Util)\n\n    # Warnings if this is the main project\n    target_compile_options(ShockHash INTERFACE $<$<COMPILE_LANGUAGE:CXX>:-Wall -Wextra -Wpedantic -Werror -Wno-error=stringop-overflow -frecord-gcc-switches>)\n\n    add_executable(Benchmark benchmark/benchmark_construction.cpp)\n    target_link_libraries(Benchmark PUBLIC BenchmarkUtils ShockHash)\n\n    if(SSE4_2_FOUND)\n        add_executable(BenchmarkSIMD benchmark/benchmark_construction.cpp)\n        target_link_libraries(BenchmarkSIMD PUBLIC BenchmarkUtils ShockHashSIMD)\n    endif()\n\n    add_executable(NumHashEvals benchmark/numHashEvals.cpp)\n    target_link_libraries(NumHashEvals PUBLIC BenchmarkUtils ShockHash)\n\n    add_executable(Bijections benchmark/bijections.cpp)\n    target_link_libraries(Bijections PUBLIC ShockHash BenchmarkUtils)\n\n    add_executable(GolombMemoTuner benchmark/golombMemoTuner.cpp)\n    target_link_libraries(GolombMemoTuner PUBLIC ShockHash BenchmarkUtils)\nendif()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/sichash",
            "repo_link": "https://github.com/ByteHamster/SicHash",
            "content": {
                "codemeta": "",
                "readme": "# SicHash\n\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n![Build status](https://github.com/ByteHamster/SicHash/actions/workflows/build.yml/badge.svg)\n\nA perfect hash function (PHF) maps a set S of n keys to the first m integers without collisions.\nIt is called _minimal_ perfect (MPHF) if m=n.\nPerfect hash functions have applications in databases, bioinformatics, and as a building block of various space-efficient data structures.\n\nSicHash is a (minimal) perfect hash function based on irregular cuckoo hashing, retrieval, and overloading.\nEach input key has a small number of choices for output positions.\nUsing cuckoo hashing, SicHash determines a mapping from each key to one of its choices,\nsuch that there are no collisions between keys.\nIt then stores the mapping from keys to their candidate index space-efficiently using\nthe [BuRR](https://github.com/lorenzhs/BuRR) retrieval data structure.\n\nSicHash offers a very good trade-off between construction performance, query performance, and space consumption.\n\n### Library Usage\n\nClone this repo and add the following to your `CMakeLists.txt`.\nNote that the repo has submodules, so either use `git clone --recursive` or `git submodule update --init --recursive`.\n\n```\nadd_subdirectory(path/to/SicHash)\ntarget_link_libraries(YourTarget PRIVATE SicHash)\n```\n\nConstructing a SicHash perfect hash function is then straightforward:\n\n```cpp\nstd::vector<std::string> keys = {\"abc\", \"def\", \"123\", \"456\"};\nsichash::SicHashConfig config;\nsichash::SicHash<true> hashFunc(keys, config);\nstd::cout << hashFunc(\"abc\") << std::endl;\n```\n\n### Construction Performance\n\n[![Plots preview](https://raw.githubusercontent.com/ByteHamster/SicHash/main/plots-construction.png)](https://arxiv.org/pdf/2210.01560)\n\n### Query Performance\n\n[![Plots preview](https://raw.githubusercontent.com/ByteHamster/SicHash/main/plots-query.png)](https://arxiv.org/pdf/2210.01560)\n\n### Reproducing Experiments\n\nThis repository contains the source code and our reproducibility artifacts for the benchmarks specific to SicHash.\nBenchmarks that compare SicHash to competitors are available in a different repository: https://github.com/ByteHamster/MPHF-Experiments\n\nWe provide an easy to use Docker image to quickly reproduce our results.\nAlternatively, you can look at the `Dockerfile` to see all libraries, tools, and commands necessary to compile SicHash.\n\n#### Building the Docker Image\n\nRun the following command to build the Docker image.\nBuilding the image takes about 5 minutes, as some packages (including LaTeX for the plots) have to be installed.\n\n```bash\ndocker build -t sichash --no-cache .\n```\n\nSome compiler warnings (red) are expected when building competitors and will not prevent building the image or running the experiments.\nPlease ignore them!\n\n#### Running the Experiments\nDue to the long total running time of all experiments in our paper, we provide run scripts for a slightly simplified version of the experiments.\nThey run fewer iterations and output fewer data points.\n\nYou can modify the benchmarks scripts in `scripts/dockerVolume` if you want to change the number of runs or data points.\nThis does not require the Docker image to recompile.\nDifferent experiments can be started by using the following command:\n\n```bash\ndocker run --interactive --tty -v \"$(pwd)/scripts/dockerVolume:/opt/dockerVolume\" sichash /opt/dockerVolume/figure-1.sh\n```\n\nThe number also refers to the figure in the paper.\n\n| Figure in paper | Launch command                | Estimated runtime  |\n| :-------------- | :---------------------------- | :----------------- |\n| 1               | /opt/dockerVolume/figure-1.sh | 10 minutes         |\n\nThe resulting plots can be found in `scripts/dockerVolume` and are called `figure-<number>.pdf`.\nMore experiments comparing SicHash with competitors can be found in a different repository: https://github.com/ByteHamster/MPHF-Experiments\n\n### License\n\nThis code is licensed under the [GPLv3](/LICENSE).\nIf you use the project in an academic context or publication, please cite [our paper](https://doi.org/10.1137/1.9781611977561.ch15):\n\n```\n@inproceedings{lehmann2023sichash,\n  author       = {Hans{-}Peter Lehmann and\n                  Peter Sanders and\n                  Stefan Walzer},\n  title        = {SicHash - Small Irregular Cuckoo Tables for Perfect Hashing},\n  booktitle    = {{ALENEX}},\n  pages        = {176--189},\n  publisher    = {{SIAM}},\n  year         = {2023},\n  doi          = {10.1137/1.9781611977561.CH15}\n}\n```\n\n",
                "dependencies": "cmake_minimum_required(VERSION 3.25)\nlist(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_SOURCE_DIR}/cmake\")\nproject(SicHash)\n\nif(TARGET SicHash)\n    return()\nendif()\n\nif (NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE \"Release\")\nendif ()\n\nif(CMAKE_BUILD_TYPE STREQUAL \"Release\" AND PROJECT_IS_TOP_LEVEL)\n    add_compile_options(-march=native)\nendif()\n\nset(TLX_INSTALL_INCLUDE_DIR tlx CACHE PATH \"Workaround for TLX breaking the first cmake call\")\nadd_subdirectory(extlib/tlx)\n\nadd_library(SicHash INTERFACE)\ntarget_include_directories(SicHash INTERFACE include)\ntarget_compile_features(SicHash INTERFACE cxx_std_20)\n\nadd_subdirectory(extlib/util EXCLUDE_FROM_ALL)\ntarget_link_libraries(SicHash INTERFACE ByteHamster::Util)\n\nadd_subdirectory(extlib/simpleRibbon EXCLUDE_FROM_ALL)\ntarget_link_libraries(SicHash INTERFACE SimpleRibbon ips2ra)\n\nadd_library(SicHash::sichash ALIAS SicHash)\n\nif(PROJECT_IS_TOP_LEVEL)\n    target_compile_options(SicHash INTERFACE $<$<COMPILE_LANGUAGE:CXX>:-Wall -Wextra -Wpedantic -Werror -frecord-gcc-switches>)\n\n    add_executable(Example src/example.cpp)\n    target_link_libraries(Example PRIVATE SicHash)\n    target_compile_features(Example PRIVATE cxx_std_20)\n\n    add_executable(Solvers src/solvers.cpp)\n    target_link_libraries(Solvers PRIVATE SicHash tlx)\n    target_compile_features(Solvers PRIVATE cxx_std_20)\n\n    add_executable(ConstructionSuccess src/constructionSuccess.cpp)\n    target_link_libraries(ConstructionSuccess PRIVATE SicHash tlx)\n    target_compile_features(ConstructionSuccess PRIVATE cxx_std_20)\n\n    add_executable(SicHashBenchmark src/sicHashBenchmark.cpp)\n    target_link_libraries(SicHashBenchmark PRIVATE SicHash tlx)\n    target_compile_features(SicHashBenchmark PRIVATE cxx_std_20)\n\n    add_executable(MaxLoadFactor src/maxLoadFactor.cpp)\n    target_link_libraries(MaxLoadFactor PRIVATE SicHash tlx)\n    target_compile_features(MaxLoadFactor PRIVATE cxx_std_20)\nendif()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/signal-processor",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Signal-Processor",
            "content": {
                "codemeta": "",
                "readme": "![Python](https://img.shields.io/badge/Python-3670A0?style=plastic&logo=python&logoColor=ffdd54) ![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=plastic&logo=scipy&logoColor=%white) ![NumPy](https://img.shields.io/badge/Numpy-777BB4.svg?style=plastic&logo=numpy&logoColor=white) ![Plotly](https://img.shields.io/badge/Plotly-239120.svg?style=plastic&logo=plotly&logoColor=white) ![Matplotlib](https://img.shields.io/badge/Matplotlib-%233F4F75.svg?style=plastic&logo=plotly&logoColor=white)  ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/SignalProcessor?style=plastic)\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13286179.svg)](https://doi.org/10.5281/zenodo.13286179)\n\n# SignalProcessor\n\nThis repository provides a Python package for generating, filtering, fitting, and analyzing signals. The package includes functionalities for creating noisy signals, applying filters, fitting damped sine waves, and performing statistical analysis.\n\n### Overview\n\n- Generate noisy sine wave signals (or import custom signals)\n- Apply Butterworth low-pass filters\n- Fit damped sine waves to filtered signals\n- Perform t-tests between filtered signals and fitted models\n- Compute and visualize Fourier Transforms\n\n### Installation\n\n1) Create and source virtual environment:\n```shell\npython -m venv env\nsource env/bin/activate  # On Windows use `env\\Scripts\\activate`\n```\n2) Install the dependencies:\n```shell\npip install -r requirements.txt\n```\n\n### Running Tests\nUsing unittest\n\n```shell\npython -m unittest discover -s tests\n```\n\n### Example\nAn example demonstrating generating a signal, applying filters, fitting models, and performing analysis, exists in the `main.py`.\n\n>[!Note]\n> An example plot has been uploaded to the `plots` directory.\n\n### Example Usage\n\nGenerate a Noisy Signal\n\n```shell\nimport numpy as np\nfrom src.signal_processor import SignalProcessor\n\ntimeVector = np.linspace(0, 1, 1000, endpoint = False)  # Or consider importing or modifying your time vector\n\ngenerator = SignalGenerator(timeVector)\n   \ngenerator.generateNoisySignal(frequency = 20, noiseStdDev = 0.6)\n\n  # or with defaults:\n    processor.generateNoisySignal()   # frequency = 10, noiseStdDev = 0.5\n```\n\nApply a Filter (`butter`, `bessel`, `highpass`). Default is `butter`.\n\n```shell\nfrom src.signal_filter import SignalFilter\n\nfilteredInstance = generator.generateNoisySignal() \\\n                            .applyFilter(filterType = 'butter', \n                                         filterOrder = 4, \n                                         cutOffFrequency = 0.2, \n                                         bType = 'lowpass')\n    # Or with different filter parameters:\n      filteredInstance.setFilterParameters('bessel', 5, 0.5, 'highpass').applyFilter()    \n```\n\nFit a damped sine wave to the filtered signal\n\n```shell    \nfrom src.signal_fitter import SignalFitter\n\n    # default sine wave parameters: amplitudeParam = 1.0, frequencyParam = 10.0, phaseParam = 0.0, decayRateParam = 0.1\nfittedInstance = filteredInstance.fitDampedSineWave()\n\n    # Or with custom parameters:\n      fittedInstance.setDampedSineWaveParameters(3.0, 12.0, np.pi / 6, 0.3)\n      fittedInstance.setDampedSineWaveBounds([0, 0, -np.pi/2, 0], [10, 20, np.pi/2, 1])\n      fittedInstance.fitDampedSineWave()      \n```\n\nPerform a t-test between the filtered signal and the fitted damped sine wave\n\n```shell\nfrom src.statistical_analyzer import StatisticalAnalyzer\n\nanalyzedInstance = fittedInstance.analyzeFit()\ntTestResults = analyzedInstance.getTTestResults()\nprint(f\"T-test result: statistic={tTestResults[0]}, p-value={tTestResults[1]}\")\n```\n\nPlot and save the results (will be saved under `plots` directory)\n\n```shell\nfrom src.signal_visualizer import SignalVisualizer\n\nvisualizer = SignalVisualizer(timeVector, generator.getNoisySignal(), \n                              filteredInstance.getFilteredSignal(), \n                              fittedInstance.getFittedSignal()\n                              )\nvisualizer.plotResults()\nvisualizer.plotInteractiveResults()\n```\n\n",
                "dependencies": "plotly\nnumpy\nmatplotlib\nscipy\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/simcats",
            "repo_link": "https://github.com/f-hader/SimCATS/",
            "content": {
                "codemeta": "",
                "readme": "<h1 align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/f-hader/SimCATS/main/SimCATS_symbol.svg\" alt=\"SimCATS logo\">\n  <br>\n</h1>\n\n<div align=\"center\">\n  <a href=\"https://github.com/f-hader/SimCATS/blob/main/LICENSE\">\n    <img src=\"https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg\" alt=\"License: CC BY-NC-SA 4.0\"/>\n  </a>\n  <a href=\"https://pypi.org/project/simcats/\">\n    <img src=\"https://img.shields.io/pypi/v/simcats.svg\" alt=\"PyPi Latest Release\"/>\n  </a>\n  <a href=\"https://simcats.readthedocs.io/en/latest/\">\n    <img src=\"https://img.shields.io/readthedocs/simcats\" alt=\"Read the Docs\"/>\n  </a>\n  <a href=\"https://doi.org/10.1109/TQE.2024.3445967\">\n    <img src=\"https://img.shields.io/badge/DOI (Paper)-10.1109/TQE.2024.3445967-007ec6.svg\" alt=\"DOI Paper\"/>\n  </a>\n  <a href=\"https://doi.org/10.5281/zenodo.13805205\">\n    <img src=\"https://img.shields.io/badge/DOI (Code)-10.5281/zenodo.13805205-007ec6.svg\" alt=\"DOI Code\"/>\n  </a>\n</div>\n\n# SimCATS\n\nSimulation of CSDs for Automated Tuning Solutions (`SimCATS`) is a Python framework for simulating charge stability \ndiagrams (CSDs) typically measured during the tuning process of qubits.\n\n## Installation\n\nThe framework supports Python versions 3.7 - 3.11 and installs via pip:\n```\npip install simcats\n```\n\nAlternatively, the `SimCATS` package can be installed by cloning the GitHub repository, navigating to the folder \ncontaining the `setup.py` file and executing\n\n```\npip install .\n```\n\nFor the installation in development/editable mode, use the option `-e`.\n\n## Examples / Tutorials\nAfter installing the package, a good starting point is a look into the Jupyter Notebook \n`example_SimCATS_simulation_class.ipynb`, which provides an overview of the usage of the simulation class offered by \nthe framework. \nFor more detailed examples and explanations of the geometric ideal CSD simulation using Total Charge Transitions (TCTs), look at the Jupyter Notebook `example_SimCATS_IdealCSDGeometric.ipynb`. This notebook also includes a hint\nregarding the generation of required labels for training algorithms that might need line labels defined as start and\nend points or require semantic information about particular transitions.\n\n## Tests\n\nThe tests are written for the `PyTest` framework but should also work with the `unittest` framework.\n\nTo run the tests, install the packages `pytest`, `pytest-cov`, and `pytest-xdist` with\n\n```\npip install pytest pytest-cov pytest-xdist\n```\n\nand run the following command:\n\n```\npytest --cov=simcats -n auto --dist loadfile .\\tests\\\n```\n\nThe argument \n- `--cov=simcats` enables a coverage summary of the `SimCATS` package,\n- `-n auto` enables the test to run with multiple threads (auto will choose as many threads as possible, but can be replaced with a specific number of threads to use), and\n- `--dist loadfile` specifies that each file should be executed only by one thread.\n\n<!-- start sec:documentation -->\n## Documentation\n\nThe official documentation is hosted on [ReadtheDocs](https://simcats.readthedocs.io), but can also be built locally.\nTo do this, first install the packages `sphinx`, `sphinx-rtd-theme`, `sphinx-autoapi`, `myst-nb `, and `jupytext` with\n\n```\npip install sphinx sphinx-rtd-theme sphinx-autoapi myst-nb jupytext\n```\n\nand then, in the `docs` folder, execute the following command:\n\n```\n.\\make html\n```\n\nTo view the generated HTML documentation, open the file `docs\\build\\html\\index.html`.\n<!-- end sec:documentation -->\n\n## Structure of SimCATS\n\nThe primary user interface for `SimCATS` is the class `Simulation`, which combines all the necessary functionalities to\nmeasure (simulate) a CSD and adjust the parameters for the simulated measurement. The class `Simulation` and default\nconfigurations for the simulation (`default_configs`) can be imported directly from `simcats`. Aside from that,\n`SimCATS` contains the subpackages `ideal_csd`, `sensor`, `distortions`, and `support_functions`, described in\nthe following sections.\n\n### Module `simulation`\n\nAn instance of the simulation class requires\n\n-   an implementation of the `IdealCSDInterface` for the simulation of ideal CSD data,\n-   an implementation of the `SensorInterface` for the simulation of the sensor (dot) reaction based on the ideal CSD\ndata, and\n-   (optionally) implementations of the desired types of distortions, which can be implementations from `OccupationDistortionInterface`, `SensorPotentialDistortionInterface`, or `SensorResponseDistortionInterface`.\n\nWith an initialized instance of the `Simulation` class, it is possible to run simulations using the `measure` function\n(see `example_SimCATS_simulation_class.ipynb`).\n\n### Subpackage `ideal_csd`\n\nThis subpackage contains the `IdealCSDInterface` used by the `Simulation` class  and an implementation of\nthe `IdealCSDInterface` (`IdealCSDGeometric`) based on our geometric simulation approach.\nAdditionally, it contains in the subpackage `geometric` the functions used by `IdealCSDGeometric`, including the\nimplementation of the total charge transition (TCT) definition and functions for calculating the occupations using TCTs.\n\n### Subpackage `distortions`\n\nThe distortions subpackage contains the `DistortionInterface` from which the `OccupationDistortionInterface`, the \n`SensorPotentialDistortionInterface`, and the `SensorResponseDistortionInterface` are derived. Distortion functions used\nin the `Simulation` class have to implement these specific interfaces. Implemented distortions included in the\nsubpackage are:\n\n-   white noise, generated by sampling from a normal distribution,\n-   pink noise, generated using the package colorednoise ([https://github.com/felixpatzelt/colorednoise](https://github.com/felixpatzelt/colorednoise)),\n-   random telegraph noise (RTN), generated using the algorithm described in [\"Toward Robust Autotuning of Noisy Quantum Dot Devices\" by Ziegler et al.](https://doi.org/10.1103/PhysRevApplied.17.024069) (RTN is called sensor jumps there),\n-   dot jumps, simulated using the algorithm described in [\"Toward Robust Autotuning of Noisy Quantum Dot Devices\" by Ziegler et al.](https://doi.org/10.1103/PhysRevApplied.17.024069) (In the `Simulation` class, this is applied to a whole block of rows or columns, but there is also a function for applying it linewise.), and\n-   lead transition blurring, simulated using Gaussian or Fermi-Dirac blurring.\n\nThe implementations also offer the option to set ratios (parameter `ratio`) for the occurrence of the distortion (e.g. dot jumps may only happen sometimes and not in every measurement). Moreover, it is also possible to sample the\nnoise parameters from a given sampling range using an object of type `ParameterSamplingInterface`.\nClasses for randomly sampling from a normal distribution or a uniform distribution within a given range are available in\nthe subpackage `support_functions`.\nIn this case, the strength is randomly chosen from the given range for every measurement.\nAdditionally, it is possible to specify that this range should be a smaller subrange of the provided range.\nThis allows restricting distortion fluctuations during a simulation while enabling a large variety of different strengths\nfor the initialization of the objects. <br>\nRTN, dot jumps, and lead transition blurring are applied in the pixel domain. However, the jump length or the blurring strength should be consistent in the voltage domain even if the resolution changes. Therefore, the parameters\nare given in the voltage domain and adjusted according to the resolution in terms of pixel per voltage. <br>\nFor a simulated measurement with a continuous voltage sweep involving an averaging for each pixel, the noise strength of the\nwhite and pink noise should be adjusted if the resolution (volt per pixel) changes, due to smoothing out the noise. This smoothing depends on the type of averaging used and is not incorporated in the default implementation.\n\n### Subpackage `sensor`\n\nThis subpackage contains the `SensorInterface` that defines how a sensor simulation must be implemented to be used by the `Simulation` class. The `SensorPeakInterface` provides the desired representation for the definition of the Coulomb peaks the sensor uses. `SensorGeneric` implements the `SensorInterface` and offers functions for simulating the sensor response and potential. It offers the possibility to simulate with a single peak or multiple sensor peaks. Current implementations of the `SensorPeakInterface` are `SensorPeakGaussian` and `SensorPeakLorentzian`.\n\n### Subpackage `support_functions`\n\nThis subpackage contains support functions, which are used by the end user and by different functions of the framework.  \n- `fermi_filter1d` is an implementation of a one-dimensional Fermi-Dirac filter.\n- `plot_csd` plots one and two-dimensional CSDs. The function can also plot ground truth data (see `example_SimCATS_simulation_class.ipynb` for examples).  \n- `rotate_points` simply rotates coordinates (stored in a (n, 2) shaped array) by a given angle. It is especially used during the generation of the ideal data.\n- `ParameterSamplingInterface` defines an interface for randomly sampled (fluctuated) strengths of distortions.\n  - `NormalSamplingRange` and `UniformSamplingRange` are implementations of the `ParameterSamplingInterface`.\n\n## Citations\n\n```bibtex\n@article{hader2024simcats,\n  author={Hader, Fabian and Fleitmann, Sarah and Vogelbruch, Jan and Geck, Lotte and Waasen, Stefan van},\n  journal={IEEE Transactions on Quantum Engineering}, \n  title={Simulation of Charge Stability Diagrams for Automated Tuning Solutions (SimCATS)}, \n  year={2024},\n  volume={5},\n  pages={1-14},\n  doi={10.1109/TQE.2024.3445967}\n}\n```\n\n## License, CLA, and Copyright\n\n[![CC BY-NC-SA 4.0][cc-by-nc-sa-shield]][cc-by-nc-sa]\n\nThis work is licensed under a\n[Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License][cc-by-nc-sa].\n\n[![CC BY-NC-SA 4.0][cc-by-nc-sa-image]][cc-by-nc-sa]\n\n[cc-by-nc-sa]: http://creativecommons.org/licenses/by-nc-sa/4.0/\n[cc-by-nc-sa-image]: https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png\n[cc-by-nc-sa-shield]: https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg\n\nContributions must follow the Contributor License Agreement. For more information, see the CONTRIBUTING.md file at the top of the GitHub repository.\n\nCopyright © 2024 Forschungszentrum Jülich GmbH - Central Institute of Engineering, Electronics and Analytics (ZEA) - Electronic Systems (ZEA-2)\n\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"simcats\"\nversion = \"1.2.0\" # also update in docs (conf.py) and simcats (__init__.py)\nlicense = { text=\"CC BY-NC-SA 4.0\" }\nauthors = [\n    { name=\"Fabian Hader\", email=\"f.hader@fz-juelich.de\" },\n    { name=\"Sarah Fleitmann\", email=\"s.fleitmann@fz-juelich.de\" },\n    { name=\"Fabian Fuchs\", email=\"f.fuchs@fz-juelich.de\" },\n    { name=\"Jan Vogelbruch\", email=\"j.vogelbruch@fz-juelich.de\" },\n]\ndescription = \"\"\"\\\n    SimCATS is a python framework for simulating charge stability diagrams (CSDs) typically measured during the tuning process of qubits.\\\n    \"\"\"\nrequires-python = \">=3.7\"\nreadme = \"README.md\"\nclassifiers = [\n    'Development Status :: 5 - Production/Stable',\n    'Intended Audience :: Science/Research',\n    'Programming Language :: Python',\n    'Programming Language :: Python :: 3',\n    'Programming Language :: Python :: 3.7',\n    'Programming Language :: Python :: 3.8',\n    'Programming Language :: Python :: 3.9',\n    'Programming Language :: Python :: 3.10',\n    'Programming Language :: Python :: 3.11',\n    'Topic :: Scientific/Engineering',\n    'Typing :: Typed',\n]\ndependencies = [\n    'bezier',\n    'colorednoise',\n    'diplib',\n    'matplotlib',\n    'numpy',\n    'opencv-python',\n    'scipy',\n    'sympy'\n]\n\n[project.urls]\nhomepage = \"https://github.com/f-hader/SimCATS\"\ndocumentation = \"https://simcats.readthedocs.io\"\nsource = \"https://github.com/f-hader/SimCATS\"\ntracker = \"https://github.com/f-hader/SimCATS/issues\"\n\n#!/usr/bin/env python\n\nimport setuptools\n\nif __name__ == \"__main__\":\n    setuptools.setup()\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/simona",
            "repo_link": "https://github.com/jokergoo/simona",
            "content": {
                "codemeta": "",
                "readme": "# simona: Semantic Similarity on Bio-Ontologies\n\n\n## Introduction\n\nThis package implements infrastructures for ontology analysis by offering \nefficient data structures, fast ontology traversal methods, and elegant visualizations. \nIt provides a robust toolbox supporting over 70 methods for semantic similarity analysis.\n\nMost methods implemented in **simona** are from\nthe [supplementary file](https://academic.oup.com/bib/article/18/5/886/2562801#supplementary-data)\nof the paper [\"Mazandu et al., Gene Ontology semantic similarity tools: survey\non features and challenges for biological knowledge discovery. Briefings in\nBioinformatics 2017\"](https://doi.org/10.1093/bib/bbw067).\n\n## Citation\n\nZuguang Gu. simona: a comprehensive R package for semantic similarity analysis on bio-ontologies. bioRxiv, 2023. https://doi.org/10.1101/2023.12.03.569758\n\n## Install\n\n**simona** is available on [Bioconductor](https://bioconductor.org/packages/release/bioc/html/simona.html).\nIt can be installed by:\n\n```r\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"simona\")\n```\n\nOr the devel version:\n\n```r\ndevtools::install_github(\"jokergoo/simona\")\n```\n\n## Usage\n\nCreat an ontology object:\n\n```r\nlibrary(simona)\nparents  = c(\"a\", \"a\", \"b\", \"b\", \"c\", \"d\")\nchildren = c(\"b\", \"c\", \"c\", \"d\", \"e\", \"f\")\ndag = create_ontology_DAG(parents, children)\ndag\n```\n\n```\nAn ontology_DAG object:\n  Source: Ontology \n  6 terms / 6 relations\n  Root: a \n  Terms: a, b, c, d, ...\n  Max depth: 3 \n  Aspect ratio: 0.67:1 (based on the longest distance from root)\n                0.68:1 (based on the shortest distance from root)\n```\n\nFrom GO:\n\n```r\ndag = create_ontology_DAG_from_GO_db(\"BP\", org_db = \"org.Hs.eg.db\")\ndag\n```\n\n```\nAn ontology_DAG object:\n  Source: GO BP / GO.db package\n  28140 terms / 56449 relations\n  Root: GO:0008150\n  Terms: GO:0000001, GO:0000002, GO:0000003, GO:0000011, ...\n  Max depth: 18\n  Aspect ratio: 342.43:1 (based on the longest distance from root)\n                780.22:1 (based on the shortest distance from root)\n  Relations: is_a, part_of\n  Annotations are available.\n\nWith the following columns in the metadata data frame:\n  id, name, definition\n```\n\nImport from an `.obo` file:\n\n```r\ndag = import_obo(\"https://purl.obolibrary.org/obo/po.obo\")\ndag\n```\n\n```\nAn ontology_DAG object:\n  Source: po, releases/2023-07-13 \n  1656 terms / 2512 relations\n  Root: _all_ \n  Terms: PO:0000001, PO:0000002, PO:0000003, PO:0000004, ...\n  Max depth: 13 \n  Aspect ratio: 25.08:1 (based on the longest distance from root)\n                39.6:1 (based on the shortest distance from root)\n  Relations: is_a, part_of\n\nWith the following columns in the metadata data frame:\n  id, short_id, name, namespace, definition\n```\n\nThe following IC methods are provided:\n\n```\n> all_term_IC_methods()\n [1] \"IC_offspring\"     \"IC_height\"        \"IC_annotation\"    \"IC_universal\"\n [5] \"IC_Zhang_2006\"    \"IC_Seco_2004\"     \"IC_Zhou_2008\"     \"IC_Seddiqui_2010\"\n [9] \"IC_Sanchez_2011\"  \"IC_Meng_2012\"     \"IC_Wang_2007\"\n```\n\nThe following semantic similarity methods are provided:\n\n```\n> all_term_sim_methods()\n [1] \"Sim_Lin_1998\"         \"Sim_Resnik_1999\"      \"Sim_FaITH_2010\"      \n [4] \"Sim_Relevance_2006\"   \"Sim_SimIC_2010\"       \"Sim_XGraSM_2013\"     \n [7] \"Sim_EISI_2015\"        \"Sim_AIC_2014\"         \"Sim_Zhang_2006\"      \n[10] \"Sim_universal\"        \"Sim_Wang_2007\"        \"Sim_GOGO_2018\"       \n[13] \"Sim_Rada_1989\"        \"Sim_Resnik_edge_2005\" \"Sim_Leocock_1998\"    \n[16] \"Sim_WP_1994\"          \"Sim_Slimani_2006\"     \"Sim_Shenoy_2012\"     \n[19] \"Sim_Pekar_2002\"       \"Sim_Stojanovic_2001\"  \"Sim_Wang_edge_2012\"  \n[22] \"Sim_Zhong_2002\"       \"Sim_AlMubaid_2006\"    \"Sim_Li_2003\"         \n[25] \"Sim_RSS_2013\"         \"Sim_HRSS_2013\"        \"Sim_Shen_2010\"       \n[28] \"Sim_SSDD_2013\"        \"Sim_Jiang_1997\"       \"Sim_Kappa\"           \n[31] \"Sim_Jaccard\"          \"Sim_Dice\"             \"Sim_Overlap\"         \n[34] \"Sim_Ancestor\" \n```\n\nThe following group similarity methods are provided:\n\n```\n> all_group_sim_methods()\n [1] \"GroupSim_pairwise_avg\"            \"GroupSim_pairwise_max\"           \n [3] \"GroupSim_pairwise_BMA\"            \"GroupSim_pairwise_BMM\"           \n [5] \"GroupSim_pairwise_ABM\"            \"GroupSim_pairwise_HDF\"           \n [7] \"GroupSim_pairwise_MHDF\"           \"GroupSim_pairwise_VHDF\"          \n [9] \"GroupSim_pairwise_Froehlich_2007\" \"GroupSim_pairwise_Joeng_2014\"    \n[11] \"GroupSim_SimALN\"                  \"GroupSim_SimGIC\"                 \n[13] \"GroupSim_SimDIC\"                  \"GroupSim_SimUIC\"                 \n[15] \"GroupSim_SimUI\"                   \"GroupSim_SimDB\"                  \n[17] \"GroupSim_SimUB\"                   \"GroupSim_SimNTO\"                 \n[19] \"GroupSim_SimCOU\"                  \"GroupSim_SimCOT\"                 \n[21] \"GroupSim_SimLP\"                   \"GroupSim_Ye_2005\"                \n[23] \"GroupSim_SimCHO\"                  \"GroupSim_SimALD\"                 \n[25] \"GroupSim_Jaccard\"                 \"GroupSim_Dice\"                   \n[27] \"GroupSim_Overlap\"                 \"GroupSim_Kappa\" \n```\n\nThere is also a visualization on the complete DAG:\n\n```r\nsig_go_ids = readRDS(system.file(\"extdata\", \"sig_go_ids.rds\", package = \"simona\"))\ndag_circular_viz(dag, highlight = sig_go_ids, reorder_level = 3, \n  legend_labels_from = \"name\")\n```\n\n![](https://github.com/jokergoo/simona/assets/449218/ada30534-182e-4513-93bf-9819e84b8604)\n\n\n## License\n\nMIT @ Zuguang Gu\n\n",
                "dependencies": "Package: simona\nType: Package\nTitle: Semantic Similarity on Bio-Ontologies\nVersion: 1.3.13\nDate: 2024-09-17\nAuthors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"),\n                  comment = c('ORCID'=\"0000-0002-7395-8709\"))\nDepends: R (>= 4.1.0)\nImports: methods, Rcpp, matrixStats, GetoptLong, grid, GlobalOptions,\n         igraph, Polychrome, S4Vectors, xml2 (>= 1.3.3), circlize, ComplexHeatmap,\n         grDevices, stats, utils, shiny\nSuggests: knitr, testthat, BiocManager, GO.db, org.Hs.eg.db, proxyC, AnnotationDbi,\n          Matrix, DiagrammeR, ragg, png, InteractiveComplexHeatmap, UniProtKeywords,\n          simplifyEnrichment, AnnotationHub, jsonlite\nLinkingTo: Rcpp\nVignetteBuilder: knitr\nDescription: This package implements infrastructures for ontology analysis by offering \n    efficient data structures, fast ontology traversal methods, and elegant visualizations. \n    It provides a robust toolbox supporting over 70 methods for semantic similarity analysis.\nbiocViews: Software, Annotation, GO, BiomedicalInformatics\nURL: https://github.com/jokergoo/simona\nBugReports: https://github.com/jokergoo/simona/issues\nSystemRequirements: Perl, Java\nLicense: MIT + file LICENSE\nRoxygenNote: 7.3.2\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/simpa",
            "repo_link": "https://github.com/IMSY-DKFZ/simpa",
            "content": {
                "codemeta": "",
                "readme": "<div align=\"center\">\n\n![Logo](https://github.com/IMSY-DKFZ/simpa/raw/main/docs/source/images/simpa_logo.png?raw=true \"SIMPA Logo\")\n\n[![Documentation Status](https://readthedocs.org/projects/simpa/badge/?version=develop)](https://simpa.readthedocs.io/en/develop/?badge=develop)\n![Build Status](https://github.com/IMSY-DKFZ/simpa/actions/workflows/automatic_testing.yml/badge.svg)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/IMSY-DKFZ/simpa/blob/main/LICENSE.md)\n[![Pypi Badge](https://img.shields.io/pypi/v/simpa)](https://pypi.org/project/simpa/)\n[![PyPI downloads](https://img.shields.io/pypi/dw/simpa?color=gr&label=pypi%20downloads)](https://pypi.org/project/simpa/)\n\n</div>\n\n# The toolkit for Simulation and Image Processing for Photonics and Acoustics (SIMPA)\n\nSIMPA aims to facilitate realistic image simulation for optical and acoustic imaging modalities by\nproviding adapters to crucial modelling steps, such as volume generation; optical modelling; acoustic\nmodelling; and image reconstruction. SIMPA provides a communication layer between various modules\nthat implement optical and acoustic forward and inverse models.\nNon-experts can use the toolkit to create sensible simulations from default parameters in an end-to-end fashion. Domain experts are provided with the functionality to set up a highly customisable\npipeline according to their specific use cases and tool requirements.\nThe paper that introduces SIMPA including visualisations and explanations can be found here: [https://doi.org/10.1117/1.JBO.27.8.083010](https://doi.org/10.1117/1.JBO.27.8.083010)\n\n* [Getting started](#getting-started)\n* [Simulation examples](#simulation-examples)\n* [Documentation](#documentation)\n* [Reproducibility](#reproducibility)\n* [Contributing](#how-to-contribute)\n* [Performance profiling](#performance-profiling)\n* [Troubleshooting](#troubleshooting)\n* [Citation](#citation)\n* [Funding](#funding)\n\nThe toolkit is still under development and is thus not fully tested and may contain bugs. \nPlease report any issues that you find in our Issue Tracker: https://github.com/IMSY-DKFZ/simpa/issues. \nAlso make sure to double check all value ranges of the optical and acoustic tissue properties \nand to assess all simulation results for plausibility.\n\n# Getting started\n\nIn order to use SIMPA in your project, SIMPA has to be installed as well as the external tools that make the actual simulations possible.\nFinally, to connect everything, SIMPA has to find all the binaries of the simulation modules you would like to use.\nThe SIMPA path management takes care of that.\n\n* [SIMPA installation instructions](#simpa-installation-instructions)\n* [External tools installation instructions](#external-tools-installation-instructions)\n* [Path Management](#path-management)\n* [Testing](#run-manual-tests)\n\n## SIMPA installation instructions\n\nThe recommended way to install SIMPA is a manual installation from the GitHub repository, please follow steps 1 - 3:\n\n1. `git clone https://github.com/IMSY-DKFZ/simpa.git`\n2. `cd simpa`\n3. `git checkout main`\n4. `git pull`\n\nNow open a python instance in the 'simpa' folder that you have just downloaded. Make sure that you have your preferred\nvirtual environment activated (we also recommend python 3.10)\n1. `pip install .` or `pip install -e .` for an editable mode. \n2. Test if the installation worked by using `python` followed by `import simpa` then `exit()`\n\nIf no error messages arise, you are now setup to use SIMPA in your project.\n\nYou can also install SIMPA with pip. Simply run:\n\n`pip install simpa`\n\nYou also need to manually install the pytorch library to use all features of SIMPA.\nTo this end, use the pytorch website tool to figure out which version to install:\n[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)\n\n## External tools installation instructions\n\nIn order to get the full SIMPA functionality, you should install all third party toolkits that make the optical and \nacoustic simulations possible. \n\n### mcx (Optical Forward Model)\n\nDownload the latest nightly build of [mcx](http://mcx.space/) on [this page](http://mcx.space/nightly/github/) for your operating system:\n\n- Linux: `mcx-linux-x64-github-latest.zip`\n- MacOS: `mcx-macos-x64-github-latest.zip`\n- Windows: `mcx-windows-x64-github-latest.zip`\n\nThen extract the files and set `MCX_BINARY_PATH=/.../mcx/bin/mcx` in your path_config.env.\n\n### k-Wave (Acoustic Forward Model)\n\nPlease follow the following steps and use the k-Wave install instructions \nfor further (and much better) guidance under:\n\n[http://www.k-wave.org/](http://www.k-wave.org/)\n\n1. Install MATLAB with the core, image processing and parallel computing toolboxes activated at the minimum.\n2. Download the kWave toolbox (version >= 1.4)\n3. Add the kWave toolbox base path to the toolbox paths in MATLAB\n4. If wanted: Download the CPP and CUDA binary files and place them in the k-Wave/binaries folder\n5. Note down the system path to the `matlab` executable file.\n\n## Path management\n\nAs a pipelining tool that serves as a communication layer between different numerical forward models and\nprocessing tools, SIMPA needs to be configured with the paths to these tools on your local hard drive.\nYou have a couple of options to define the required path variables. \n### Option 1: \nEnsure that the environment variables defined in `simpa_examples/path_config.env.example` are accessible to your script during runtime. This can be done through any method you prefer, as long as the environment variables are accessible through `os.environ`. \n### Option 2:\nImport the `PathManager` class to your project using\n`from simpa.utils import PathManager`. If a path to a `.env` file is not provided, the `PathManager` looks for a `path_config.env` file (just like the\none we provided in the `simpa_examples/path_config.env.example`) in the following places, in this order:\n1. The optional path you give the PathManager\n2. Your $HOME$ directory\n3. The current working directory\n4. The SIMPA home directory path\n   \nFor this option, please follow the instructions in the `simpa_examples/path_config.env.example` file. \n\n## Run manual tests\nTo check the success of your installation ot to assess how your contributions affect the Simpa simulation outcomes, you can run the manual tests automatically. Install the testing requirements by doing `pip install .[testing]` and run the `simpa_tests/manual_tests/generate_overview.py` file. This script runs all manual tests and generates both a markdown and an HTML file that compare your results with the reference results.\n\n# Simulation examples\n\nTo get started with actual simulations, SIMPA provides an [example package](simpa_examples) of simple simulation \nscripts to build your custom simulations upon. The [minimal optical simulation](simpa_examples/minimal_optical_simulation.py)\nis a nice start if you have MCX installed.\n\nGenerally, the following pseudo code demonstrates the construction and run of a simulation pipeline:\n\n```python\nimport simpa as sp\n\n# Create general settings \nsettings = sp.Settings(general_settings)\n\n# Create specific settings for each pipeline element \n# in the simulation pipeline\nsettings.set_volume_creation_settings(volume_creation_settings)\nsettings.set_optical_settings(optical_settings)\nsettings.set_acoustic_settings(acoustic_settings)\nsettings.set_reconstruction_settings(reconstruction_settings)\n\n# Set the simulation pipeline\nsimulation_pipeline = [sp.VolumeCreationModule(settings),\n                       sp.OpticalModule(settings),\n                       sp.AcousticModule(settings),\n                       sp.ReconstructionModule(settings)]\n    \n# Choose a PA device with device position in the volume\ndevice = sp.CustomDevice()\n\n# Simulate the pipeline\nsp.simulate(simulation_pipeline, settings, device)\n```\n\n# Reproducibility\n\nFor reproducibility, we provide the exact version number including the commit hash in the simpa output file.\nThis can be accessed via `simpa.__version__` or by checking the tag `Tags.SIMPA_VERSION` in the output file.\nThis way, you can always trace back the exact version of the code that was used to generate the simulation results.\n\n# Documentation\n\nThe updated version of the SIMPA documentation can be found at [https://simpa.readthedocs.io/en/develop](https://simpa.readthedocs.io/en/develop).\n\n## Building the documentation\n\nIt is also easily possible to build the SIMPA documentation from scratch.\nWhen the installation succeeded, and you want to make sure that you have the latest documentation\nyou should do the following steps in a command line:\n\n1. Make sure that you've installed the optional dependencies needed for the documentation by running `pip install .[docs]`\n2. Navigate to the `simpa/docs` directory\n2. If you would like the documentation to have the https://readthedocs.org/ style, type `pip install sphinx-rtd-theme`\n3. Type `make html`\n4. Open the `index.html` file in the `simpa/docs/build/html` directory with your favourite browser.\n\n# How to contribute\n\nPlease find a more detailed description of how to contribute as well as code style references in our\n[contribution guidelines](CONTRIBUTING.md).\n\nTo contribute to SIMPA, please fork the SIMPA github repository and create a pull request with a branch containing your \nsuggested changes. The core developers will then review the suggested changes and integrate these into the code \nbase.\n\nPlease make sure that you have included unit tests for your code and that all previous tests still run through. Please also run the pre-commit hooks and make sure they are passing.\nDetails are found in our [contribution guidelines](CONTRIBUTING.md).\n\nThere is a regular SIMPA status meeting every Friday on even calendar weeks at 10:00 CET/CEST, and you are very welcome to participate and\nraise any issues or suggest new features. If you want to join this meeting, write one of the core developers.\n\nPlease see the github guidelines for creating pull requests: [https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests)\n\n\n# Performance profiling\n\nWhen changing the SIMPA core, e.g., by refactoring/optimizing, or if you are curious about how fast your machine runs\nSIMPA, you can run the SIMPA [benchmarking scripts](simpa_examples/benchmarking/run_benchmarking.sh). Make sure to install the necessary dependencies via \n`pip install .[profile]` and then run:\n\n```bash\nbash ./run_benchmark.sh\n```\n\nonce for checking if it works and then parse [--number 100] to run it at eg 100 times for actual benchmarking.\nPlease see [benchmarking.md](docs/source/benchmarking.md) for a complete explanation.\n\n\n# Understanding SIMPA\n\n**Tags** are identifiers in SIMPA used to categorize settings and components within simulations, making configurations\nmodular, readable, and manageable. Tags offer organizational, flexible, and reusable benefits by acting as keys in\nconfiguration dictionaries.\n\n**Settings** in SIMPA control simulation behavior. They include:\n\n- **Global Settings**: Apply to the entire simulation, affecting overall properties and parameters.\n- **Component Settings**: Specific to individual components, allowing for detailed customization and optimization of\neach part of the simulation.\n\nSettings are defined in a hierarchical structure, where global settings are established first, followed by\ncomponent-specific settings. This approach ensures comprehensive and precise control over the simulation process.\nFor detailed information, users can refer to the [understanding SIMPA documentation](./docs/source/understanding_simpa.md).\n\n# Troubleshooting\n\nIn this section, known problems are listed with their solutions (if available):\n\n## 1. Error reading hdf5-files when using k-Wave binaries:\n   \nIf you encounter an error similar to:\n\n    Error using h5readc\n    The filename specified was either not found on the MATLAB path or it contains unsupported characters.\n\nLook up the solution in [this thread of the k-Wave forum](http://www.k-wave.org/forum/topic/error-reading-h5-files-when-using-binaries).  \n\n## 2. KeyError: 'time_series_data'\n\nThis is the error which will occur for ANY k-Wave problem. For the actual root of the problem, please either look above in\nthe terminal for the source of the bug or run the scripts in Matlab to find it manually.\n      \n# Citation\n\nIf you use the SIMPA tool, we would appreciate if you cite our Journal publication in the Journal of Biomedical Optics:\n\nGröhl, Janek, Kris K. Dreher, Melanie Schellenberg, Tom Rix, Niklas Holzwarth, Patricia Vieten, Leonardo Ayala, Sarah E. Bohndiek, Alexander Seitel, and Lena Maier-Hein. *\"SIMPA: an open-source toolkit for simulation and image processing for photonics and acoustics.\"* **Journal of Biomedical Optics** 27, no. 8 (2022).\n\n```Bibtex\n@article{2022simpatoolkit,\n  title={SIMPA: an open-source toolkit for simulation and image processing for photonics and acoustics},\n  author={Gr{\\\"o}hl, Janek and Dreher, Kris K and Schellenberg, Melanie and Rix, Tom and Holzwarth, Niklas and Vieten, Patricia and Ayala, Leonardo and Bohndiek, Sarah E and Seitel, Alexander and Maier-Hein, Lena},\n  journal={Journal of Biomedical Optics},\n  volume={27},\n  number={8},\n  year={2022},\n  publisher={SPIE}\n}\n```\n\n# Funding\n\nThis project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. [101002198]).\n\n![ERC](https://github.com/IMSY-DKFZ/simpa/raw/main/docs/source/images/LOGO_ERC-FLAG_EU_.jpg \"ERC\")\n\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools\", \"setuptools-scm\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"simpa\"\ndynamic = [\"version\"]\nauthors = [\n    {name = \"Division of Intelligent Medical Systems (IMSY), DKFZ\", email = \"k.dreher@dkfz-heidelberg.de\"},\n    {name = \"Janek Groehl <janekgroehl@live.de>\"}\n]\ndescription = \"Simulation and Image Processing for Photonics and Acoustics\"\nlicense = {text = \"MIT\"}\nreadme = \"README.md\"\nkeywords = [\"simulation\", \"photonics\", \"acoustics\"]\nrequires-python = \">=3.9\"\ndependencies = [\n    \"matplotlib>=3.5.0\",       # Uses PSF-License (MIT compatible)\n    \"numpy>=1.21.4\",           # Uses BSD-License (MIT compatible)\n    \"scipy>=1.13.0\",           # Uses BSD-like-License (MIT compatible)\n    \"pynrrd>=0.4.2\",           # Uses MIT-License (MIT compatible)\n    \"scikit-image>=0.18.3\",    # Uses BSD-License (MIT compatible)\n    \"xmltodict>=0.12.0\",       # Uses MIT-License (MIT compatible)\n    \"h5py>=3.6.0\",             # Uses BSD-License (MIT compatible)\n    \"pandas>=1.3.4\",           # Uses BSD-License (MIT compatible)\n    \"coverage>=6.1.2\",         # Uses Apache 2.0-License (MIT compatible)\n    \"Deprecated>=1.2.13\",      # Uses MIT-License (MIT compatible)\n    \"torch>=1.10.0\",           # Uses BSD-License (MIT compatible)\n    \"python-dotenv>=0.19.2\",   # Uses BSD-License (MIT compatible)\n    \"pacfish>=0.4.4\",          # Uses BSD-License (MIT compatible)\n    \"requests>=2.26.0\",        # Uses Apache 2.0-License (MIT compatible)\n    \"wget>=3.2\",               # Is Public Domain (MIT compatible)\n    \"jdata>=0.5.2\",            # Uses Apache 2.0-License (MIT compatible)\n    \"pre-commit>=3.2.2\",       # Uses MIT-License (MIT compatible)\n    \"PyWavelets\",              # Uses MIT-License (MIT compatible)\n    \"scikit-learn>=1.1.0\",     # Uses BSD-License (MIT compatible)\n]\n\n[project.optional-dependencies]\ndocs = [\n    \"sphinx-rtd-theme>=2.0.0,<3.0.0\",       # Uses MIT-License (MIT compatible)\n    \"Sphinx>=5.1.1,<6.0.0\",                 # Uses BSD-License (MIT compatible)\n    \"myst-parser>=0.18.0,<1.1\"              # Uses MIT-License (MIT compatible)\n]\nprofile = [\n    \"pytorch_memlab>=0.3.0\",                # Uses MIT-License (MIT compatible)\n    \"line_profiler>=4.0.0\",                 # Uses BSD-License (MIT compatible)\n    \"memory_profiler>=0.61.0\",              # Uses BSD-License (MIT compatible)\n    \"tabulate>=0.9.0\"                       # Uses MIT-License (MIT compatible)\n\n]\ntesting = [\n    \"mdutils>=1.4.0\",          # Uses MIT-License (MIT compatible)\n    \"pypandoc>=1.13\",          # Uses MIT-License (MIT compatible)\n    \"pypandoc_binary>=1.13\"    # Uses MIT-License (MIT compatible)\n    ]\n\n[project.urls]\nHomepage = \"https://github.com/IMSY-DKFZ/simpa\"\nDocumentation = \"https://simpa.readthedocs.io/en/main/\"\nRepository = \"https://github.com/IMSY-DKFZ/simpa\"\n\n[tool.setuptools.packages.find]\ninclude = [\"simpa\", \"simpa_tests\", \"simpa_examples\"]\n\n[tool.setuptools_scm]\n\n[tool.autopep8]\nmax_line_length = 120\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/simplifyenrichment",
            "repo_link": "https://github.com/jokergoo/simplifyEnrichment",
            "content": {
                "codemeta": "",
                "readme": "# Simplify Functional Enrichment Results\n\n[![R-CMD-check](https://github.com/jokergoo/simplifyEnrichment/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/simplifyEnrichment/actions)\n[![bioc](http://www.bioconductor.org/shields/downloads/devel/simplifyEnrichment.svg)](https://bioconductor.org/packages/stats/bioc/simplifyEnrichment/) \n[![bioc](http://www.bioconductor.org/shields/years-in-bioc/simplifyEnrichment.svg)](http://bioconductor.org/packages/devel/bioc/html/simplifyEnrichment.html)\n\n### Features\n\n- A new method (binary cut) is proposed to efficiently cluster functional terms (_e.g._ GO terms) into groups from the semantic similarity matrix.\n- Summaries of functional terms in each cluster are visualized by word clouds.\n\n### Citation\n\nZuguang Gu, et al., simplifyEnrichment: an R/Bioconductor package for Clustering and Visualizing Functional Enrichment Results, _Genomics, Proteomics & Bioinformatics 2022_. [https://doi.org/10.1016/j.gpb.2022.04.008](https://doi.org/10.1016/j.gpb.2022.04.008).\n\n### Install\n\n`simplifyEnrichment` is available on [Bioconductor](http://www.bioconductor.org/packages/devel/bioc/html/simplifyEnrichment.html), you can install it by:\n\n```r\nif (!requireNamespace(\"BiocManager\", quietly=TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"simplifyEnrichment\")\n```\n\nIf you want to try the latest version, install it directly from GitHub:\n\n```r\nlibrary(devtools)\ninstall_github(\"jokergoo/simplifyEnrichment\")\n```\n\n### Usage\n\nAs an example, I first generate a list of random GO IDs.\n\n```r\nlibrary(simplifyEnrichment)\nset.seed(888)\ngo_id = random_GO(500)\nhead(go_id)\n# [1] \"GO:0003283\" \"GO:0060032\" \"GO:0031334\" \"GO:0097476\" \"GO:1901222\"\n# [6] \"GO:0018216\"\n```\n\nThen generate the GO similarity matrix, split GO terms into clusters and visualize it.\n\n```r\nmat = GO_similarity(go_id)\nsimplifyGO(mat)\n```\n\n![](https://user-images.githubusercontent.com/449218/89673686-133c8600-d8e7-11ea-89fe-5221cb64d819.png)\n\n\n### License\n\nMIT @ Zuguang Gu\n\n",
                "dependencies": "Package: simplifyEnrichment\nType: Package\nTitle: Simplify Functional Enrichment Results\nVersion: 1.99.0\nDate: 2024-09-13\nAuthors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"),\n                  comment = c('ORCID'=\"0000-0002-7395-8709\"))\nDepends: R (>= 4.0.0)\nImports: simona,\n         ComplexHeatmap (>= 2.7.4),\n         grid,\n         circlize,\n         GetoptLong,\n         digest, \n         tm,\n         GO.db,\n         AnnotationDbi,\n         slam,\n         methods,\n         clue,\n         grDevices,\n         stats,\n         utils,\n         cluster (>= 1.14.2),\n         colorspace,\n         GlobalOptions (>= 0.1.0)\nSuggests: knitr,\n          ggplot2,\n          cowplot,\n          mclust,\n          apcluster,\n          MCL,\n          dbscan,\n          igraph,\n          gridExtra,\n          dynamicTreeCut,\n          testthat,\n          gridGraphics,\n          flexclust,\n          BiocManager,\n          InteractiveComplexHeatmap (>= 0.99.11),\n          shiny,\n          shinydashboard,\n          cola,\n          hu6800.db,\n          rmarkdown,\n          genefilter,\n          gridtext,\n          fpc\nDescription: A new clustering algorithm, \"binary cut\", for clustering similarity matrices of functional terms\n         is implemeted in this package. It also provides functions for visualizing, summarizing and comparing the clusterings.\nbiocViews: Software, Visualization, GO, Clustering, GeneSetEnrichment\nURL: https://github.com/jokergoo/simplifyEnrichment, https://simplifyEnrichment.github.io\nVignetteBuilder: knitr\nLicense: MIT + file LICENSE\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.3.1\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/smash",
            "repo_link": "https://github.com/smash-transport/smash",
            "content": {
                "codemeta": "",
                "readme": "# SMASH\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3484711.svg)](https://doi.org/10.5281/zenodo.3484711)\n\nSMASH (Simulating Many Accelerated Strongly-interacting Hadrons) is a relativistic hadronic transport approach for the dynamical description of heavy-ion reactions.\nPlease see [Phys. Rev. C 94, 054905 (2016)](https://arxiv.org/abs/1606.06642) for details and, if you are using SMASH, cite this reference together with the [software DOI](https://doi.org/10.5281/zenodo.3484711) for the specific code version employed.\nA BibTeX entry for the software DOI is found on the respective Zenodo pages.\n\nSee [CONTRIBUTING](CONTRIBUTING.md) for development hints.\nA complete [User Guide](https://theory.gsi.de/~smash/userguide/current/) as well as a more detailed [development documentation](http://theory.gsi.de/~smash/doc/current/) are available for the latest version of the code.\nFor documentation of older versions, refer to links in the [releases pages](https://github.com/smash-transport/smash/releases).\n\nIf Pythia is used, please cite the following references (both article and the codebase release you used):\n* [_A comprehensive guide to the physics and usage of PYTHIA 8.3_](https://scipost.org/SciPostPhysCodeb.8), C. Bierlich et al; SciPost Phys. Codebases 8 (2022), DOI: `10.21468/SciPostPhysCodeb.8`, also available on [arXiv](https://arxiv.org/abs/2203.11601);\n* [SciPost Phys. Codebases 8-r8.3](https://scipost.org/SciPostPhysCodeb.8-r8.3) (2022), DOI: `10.21468/SciPostPhysCodeb.8-r8.3`.\n\nReport issues [on GitHub](https://github.com/smash-transport/smash/issues) or contact us by  [✉️ email](mailto:elfner@itp.uni-frankfurt.de).\n\n## How to build and install SMASH\n\nIn the following you can find a minimal quick start guide.\nRefer to the [INSTALL](INSTALL.md) file for more detailed information.\n\n### Prerequisites\n\nSMASH is known to compile and work on 64-bit little endian machines (most CPUs are such) with UNIX-like operating systems (e.g. GNU/Linux, MacOS) and one of the following compilers (which have the required C++17 features):\n\n| Compiler   | Required version |\n|  :---:     |       :---:      |\n| GCC        |  8.0 or higher   |\n| Clang      |  7.0 or higher   |\n| Apple clang| 11.0 or higher   |\n\nAny different operating system and/or compiler and/or endianness is not officially supported and SMASH will ask you to continue at your own risk before compilation.\n\nSMASH requires the following tools and libraries:\n\n| Software | Required version |\n|  :---:   |       :---:      |\n| [CMake](https://cmake.org) | 3.16 or higher |\n| [GNU Scientific Library (GSL)](https://www.gnu.org/software/gsl/) | 2.0  or higher |\n| [Eigen3 library](http://eigen.tuxfamily.org) | 3.0  or higher |\n| [Pythia](https://pythia.org) | 8.310 |\n\nSupport for ROOT, HepMC3 and Rivet output is automatically enabled if a suitable version is found on the system:\n\n| Software | Required version |\n|  :---:   |       :---:      |\n| ROOT     | 5.34 or higher   |\n| HepMC3   | 3.2.3 or higher  |\n| Rivet    | 3.1.4 or higher  |\n\n### Compilation and installation\n\nFrom within the SMASH repository, use the following commands to build the codebase in a separate directory:\n```console\nmkdir build\ncd build\ncmake -DPythia_CONFIG_EXECUTABLE=/path/to/pythia8310/bin/pythia8-config ..\nmake\n```\nPlease note that the `make` command builds everything (executables, tests and libraries) and might take a while.\nYou can use `make smash` if you are interest in the SMASH executable only or use `make smash_shared` to exclusively build the libraries (needed e.g. in another project using SMASH as library).\n\nYou can run SMASH with specific settings (e.g. at a given collision energy or impact parameter) by modifying the config.yaml file, for example with\n```console\nvi config.yaml\n./smash\n```\nRefer to the [section below](README.md#running-smash-with-example-input-files) for more information.\n\nIf you want to install SMASH system-wide (into `/usr/local`) use\n```console\nmake install\n```\n\n⚠️ **NOTE:** All commands above are the bare minimum needed for an installation.\nIt is not guaranteed that this minimum setup is appropriate for your needs or your specific computing environment.\nFor example, several different options can be passed e.g. to the `cmake` command.\nWe strongly advise you to further refer to the [INSTALL](INSTALL.md) file for more guidance, especially if you encounter any issues.\n\n\n## Using the Docker containers\n\nAs an alternative to building or installing SMASH, a Docker image of the latest or recently tagged version can be pulled from the Github container registry.\nGet the newest version with\n```console\ndocker pull ghcr.io/smash-transport/smash:newest\n```\n\nStart the container with\n```console\ndocker run -it ghcr.io/smash-transport/smash:newest\n```\n\nA ready-to-use executable of SMASH is found in the `smash_bin` directory.\nRun it as explained below.\nIf needed, SMASH can also be built inside the container as explained in the previous section (the SMASH source files and Pythia are also found in the `/SMASH` directory).\n\nTwo container versions of SMASH are offered:\n* a small version (`ghcr.io/smash-transport/smash`) with a minimal set of dependencies\npre-installed and\n* a large version with all possible external dependencies, e.g. ROOT, HepMC and Rivet, already included (`ghcr.io/smash-transport/smash-max`).\n\nNote that running SMASH inside of a Docker container might negatively affect performance.\nMore information about containers usage can be found [here](containers/README.md).\n\n#### Note for users with ARM CPUs (e.g. Apple M1/M2 chips)\n\nOur Docker images are prepared for the x86-64 CPU architecture.\nTo make them compatible with computers with ARM CPUs (like in the case of Apple M1 and M2 chips),\n`docker` must be launched with the `--platform=linux/amd64` option.\nFor example:\n```console\ndocker run --platform=linux/amd64 -it ghcr.io/smash-transport/smash:newest\n```\nHowever, this is not always guaranteed to work and it might be necessary to build an image for the ARM architecture, as described [here](containers/README.md).\n\n## Running SMASH with Example Input Files\n\nSMASH ships example configuration files for running in the collider, box, sphere, and list mode (`Modus` in the configuration jargon).\nBy default, i.e. by running `./smash`, the simulation is set up from the collider configuration file, called _config.yaml_, and using the default particles and decay modes files (_particles.txt_ and _decaymodes.txt_, respectively).\nThey are located in the repository ***input*** folder.\n\nAdditionally, example configuration files for the box, sphere and list modus can be found in the respective directories ***input/{box,sphere,list}***.\nIf needed, e.g. in the case of a box simulation, different default particles and decay modes files can be used.\nExamples for these are also provided in ***input/box***.\n\nFinally, for the list modus, an input list file to be read in is required and an example is provided as _input/list/example_list0_.\n\nIn general, to run SMASH with a non-default configuration file, use the `-i` command.\nFor example, for the sphere or list example file, from the ***build*** folder, use:\n```console\n    ./smash -i ../input/sphere/config.yaml\n    ./smash -i ../input/list/config.yaml\n```\n\nFurthermore, if using non-default particles and decay modes files is necessary, these can be specified through the `-p` and `-d` options.\nIn the box and the dileptons example, always from the ***build*** folder, this means:\n```console\n./smash -i ../input/box/config.yaml -p ../input/box/particles.txt -d ../input/box/decaymodes.txt\n./smash -i ../input/dileptons/config.yaml -d ../input/dileptons/decaymodes.txt\n```\n\nAll available command line options for SMASH can be viewed with\n```console\n./smash -h\n```\nTo run SMASH completely silently for production runs, we recommend to suppress the standard output via e.g.\n```console\n./smash > /dev/null\n```\nand it might be useful to redirect warnings and error messages, that will still be displayed, to a file:\n```console\n./smash > /dev/null 2> /path/to/error-and-warnings-file\n```\n\n\n## License\n\nSMASH is licensed under the terms of the GNU General Public License, Version 3 or above.\nThe build scripts are licensed under terms of the BSD 3-clause license.\nFor more information, see [LICENSE](LICENSE).\n\n\n## Projects Using SMASH\n\nSMASH source and documentation are provided to check and reproduce published results of the authors.\nCooperation and joint projects with outside researchers are encouraged and comparison to results by experimental collaborations is supported.\nIf you are interested in starting a project, please contact us to avoid interference with current thesis topics.\nIf your project involves changes to the code, please refer to [CONTRIBUTING](CONTRIBUTING.md) for coding guidelines and helpful tools.\nSMASH can also be used as a 3rd party library, for examples see the ***examples*** folder in the repository.\n\n",
                "dependencies": "########################################################\n#\n#    Copyright (c) 2012-2024\n#      SMASH Team\n#\n#    BSD 3-clause license\n#\n#########################################################\n\n# Minimum cmake version this is tested on\ncmake_minimum_required(VERSION 3.16)\n\n# The name, version and language of our project\nproject(SMASH VERSION 3.1 LANGUAGES CXX)\n\n# Fail if cmake is called in the source directory\nif(CMAKE_SOURCE_DIR STREQUAL CMAKE_BINARY_DIR)\n    message(FATAL_ERROR \"You don't want to configure in the source directory!\")\nendif()\n\n# Restrict supported operating systems\nif(NOT UNIX AND NOT APPLE OR CMAKE_SIZEOF_VOID_P LESS 8)\n    option(FORCE_USE_ANY_SYSTEM \"Force cmake to setup and compile on any OS/architecture.\" OFF)\n    if(NOT FORCE_USE_ANY_SYSTEM)\n        if(CMAKE_SIZEOF_VOID_P LESS 8)\n            set(AUX_MSG \"32-bit architecture\")\n        endif()\n        if(NOT UNIX AND NOT APPLE)\n            list(APPEND AUX_MSG \"operating system\")\n            list(JOIN AUX_MSG \"/\" AUX_MSG)\n        endif()\n        message(FATAL_ERROR \" \\n\" # See e.g. https://stackoverflow.com/a/51035045 for formatting\n                            \" Using SMASH on your ${AUX_MSG} is not officially supported.\\n\"\n                            \" Please, contact the SMASH development team (see README) if you would\\n\"\n                            \" like to collaborate with us to get the code run on your system.\\n\"\n                            \" At your own risk you can pass -DFORCE_USE_ANY_SYSTEM =TRUE to\\n\"\n                            \" cmake in order to setup and compile SMASH on your system.\\n\"\n                            \" Alternatively you can use the provided Docker containers (see README).\\n\"\n        )\n    endif()\nendif()\n\n# Restrict supported compilers\nif(NOT CMAKE_CXX_COMPILER_ID MATCHES \"^((Apple)?Clang|GNU)$\")\n    option(FORCE_USE_ANY_COMPILER \"Force cmake to setup and compile with any compiler.\" OFF)\n    if(NOT FORCE_USE_ANY_COMPILER)\n        message(FATAL_ERROR \" \\n\"\n                            \" Compiling SMASH with the specified compiler is not officially supported.\\n\"\n                            \" Please, contact the SMASH development team (see README) if you would\\n\"\n                            \" like to collaborate with us to get the codebase support your compiler.\\n\"\n                            \" At your own risk you can pass -DFORCE_USE_ANY_COMPILER=TRUE to cmake in\\n\"\n                            \" order to setup and try to compile SMASH with the chosen compiler.\\n\"\n                            \" Alternatively you can use the provided Docker containers (see README).\\n\"\n        )\n    endif()\nendif()\n\n# Before starting, tell cmake where to find our modules and include utilities\nset(CMAKE_MODULE_PATH \"${CMAKE_CURRENT_SOURCE_DIR}/cmake\")\ninclude(Utilities)\n\n# The variable SMASH_VERSION is already set via the project() command, since we specify there a\n# VERSION option. Here we check if .git information is available and increase verbosity\ninclude(GetGitRevisionDescription)\ngit_describe(SMASH_VERSION_VERBOSE)\nif(SMASH_VERSION_VERBOSE)\n    set(SMASH_VERSION \"${SMASH_VERSION_VERBOSE}\")\nelse()\n    set(SMASH_VERSION \"${CMAKE_PROJECT_NAME}-${SMASH_VERSION}\")\nendif()\n\n# Define installation top-level directory name inside which SMASH files are installed in sub-folders\nstring(TOLOWER \"${SMASH_VERSION}\" SMASH_INSTALLATION_SUBFOLDER)\n\n# SMASH will be shipped as shared library and hence we want to build position independent code\ninclude(CheckPIESupported)\ncheck_pie_supported(LANGUAGES CXX)\nif(NOT CMAKE_CXX_LINK_PIE_SUPPORTED)\n    message(ATTENTION \"PIE is not supported at link time for C++. \"\n                      \"PIE link options will not be passed to linker.\")\nendif()\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\n\n# Fail early if a too old officially supported compiler is used\nif(CMAKE_CXX_COMPILER_ID MATCHES \"^((Apple)?Clang|GNU)$\")\n    unset(MINIMUM_CXX_COMPILER_VERSION)\n    if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n        set(MINIMUM_CXX_COMPILER_VERSION \"8\")\n    elseif(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n        set(MINIMUM_CXX_COMPILER_VERSION \"7\")\n    elseif(CMAKE_CXX_COMPILER_ID STREQUAL \"AppleClang\")\n        set(MINIMUM_CXX_COMPILER_VERSION \"11\")\n    endif()\n    if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS ${MINIMUM_CXX_COMPILER_VERSION})\n        message(FATAL_ERROR \" At least version ${MINIMUM_CXX_COMPILER_VERSION}\"\n                            \" of ${CMAKE_CXX_COMPILER_ID} compiler is required.\\n\"\n                            \" Version in use is ${CMAKE_CXX_COMPILER_VERSION}\")\n    endif()\nendif()\n\n# Request a given C++ standard\nset(CXX_SMASH_STANDARD \"17\")\nif(NOT \"${CMAKE_CXX_STANDARD}\")\n    set(CMAKE_CXX_STANDARD ${CXX_SMASH_STANDARD})\nendif()\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\n\n# needed for clang-tidy\nset(CMAKE_EXPORT_COMPILE_COMMANDS \"ON\")\n\n# Set a default value for CMAKE_BUILD_TYPE, otherwise we get something which is none of the options.\nif(NOT DEFINED CMAKE_BUILD_TYPE OR CMAKE_BUILD_TYPE STREQUAL \"\")\n    set(CMAKE_BUILD_TYPE\n        Release\n        CACHE STRING\n              \"Choose the type of build, options are: Debug Release RelWithDebInfo MinSizeRel Profiling.\"\n              FORCE)\nelseif(NOT CMAKE_BUILD_TYPE MATCHES \"^(Debug|Release|RelWithDebInfo|MinSizeRel|Profiling)$\")\n    message(FATAL_ERROR \" \\n\"\n                        \" Invalid build configuration specified, CMAKE_BUILD_TYPE=${CMAKE_BUILD_TYPE}.\\n\"\n                        \" Valid values are: Debug, Release, RelWithDebInfo, MinSizeRel or Profiling.\\n\"\n    )\nelseif(CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n    message(ATTENTION \"Consider adding '-O0' compiler flag for best debug information.\")\nendif()\n\n# Retrieve architecture information about endianness\ninclude(TestBigEndian)\ntest_big_endian(IS_BIG_ENDIAN)\nif(IS_BIG_ENDIAN)\n    message(STATUS \"Big endian architecture detected.\")\n    option(FORCE_USE_ON_BIG_ENDIAN \"Force cmake to setup and compile on big endian machine.\" OFF)\n    if(NOT FORCE_USE_ON_BIG_ENDIAN)\n        message(FATAL_ERROR \" \\n\"\n                            \" Using SMASH on big endian machines is not officially supported.\\n\"\n                            \" Please, contact the SMASH development team (see README) if you would like\\n\"\n                            \" to collaborate with us to get the code run on big endian architectures.\\n\"\n                            \" At your own risk you can pass -DFORCE_USE_ON_BIG_ENDIAN=TRUE to cmake in\\n\"\n                            \" order to setup and compile SMASH on a big endian machine.\\n\"\n                            \" Alternatively you can use the provided Docker containers (see README).\\n\"\n        )\n    endif()\n    add_definitions(\"-DBIG_ENDIAN_ARCHITECTURE\")\nelse()\n    message(STATUS \"Little endian architecture detected.\")\n    add_definitions(\"-DLITTLE_ENDIAN_ARCHITECTURE\")\nendif()\n\n# Let Clang users on Linux be able to use libc++\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n    option(CLANG_USE_LIBC++\n           \"If turned on clang will explicitly be asked to use libc++ (otherwise it uses the system default)\"\n           OFF)\n    if(CLANG_USE_LIBC++)\n        message(STATUS \"Prepare compilation in order to use LLVM libc++ implementation\")\n        set(MESSAGE_QUIET ON)\n        add_compiler_flag(-stdlib=libc++ CXX_FLAGS CMAKE_CXX_FLAGS CXX_RESULT _use_libcxx)\n        unset(MESSAGE_QUIET)\n        if(_use_libcxx AND \"${CMAKE_SYSTEM_NAME}\" STREQUAL \"Linux\")\n            link_libraries(c++abi supc++)\n        endif()\n    endif()\nendif()\n\n# add 3rd-party libraries (before setting compiler flags etc)\ninclude_directories(SYSTEM \"${CMAKE_CURRENT_SOURCE_DIR}/3rdparty/Cuba-4.2.2\")\ninclude_directories(SYSTEM \"${CMAKE_CURRENT_SOURCE_DIR}/3rdparty/einhard\")\ninclude_directories(SYSTEM \"${CMAKE_CURRENT_SOURCE_DIR}/3rdparty/yaml-cpp-0.7.0/include\")\nadd_subdirectory(3rdparty)\n\n# Set up compile options passed to all targets. From CMake documentation: \"The flags in\n# CMAKE_<LANG>_FLAGS will be passed to the compiler before those in the per-configuration\n# CMAKE_<LANG>_FLAGS_<CONFIG> variant, and before flags added by the add_compile_options() or\n# target_compile_options() commands.\"\nadd_compiler_flags_if_supported(\"-fno-math-errno\" # Tell the compiler to ignore errno setting of\n                                                  # math functions. This can help the compiler\n                                                  # considerably in optimizing mathematical\n                                                  # expressions.\n                                \"-march=native\" # The '-march=native' flag is not supported e.g. on\n                                                # Apple M1 machines\n                                CXX_FLAGS CMAKE_CXX_FLAGS)\n\n# CMake provides 4 build configurations (Debug, Release, RelWithDebInfo, MinSizeRel) together with\n# sensible values for the corresponding CMAKE_(C|CXX)_FLAGS_<CONFIG> variables (cached and marked as\n# advanced). Here we need to set those for Profiling build, only. Since the relevant variables are\n# created and cached by CMake, we need to force their value in the cache if their value is false,\n# e.g. empty.\n#\n# See https://cristianadam.eu/20190223/modifying-the-default-cmake-build-types/ for more infos.\nif(NOT CMAKE_CXX_FLAGS_PROFILING)\n    add_compiler_flags_if_supported(\"-O3\" \"-DNDEBUG\" \"-pg\" CXX_FLAGS SUPPORTED_CXX_FLAGS_PROFILING)\nendif()\nif(NOT CMAKE_C_FLAGS_PROFILING)\n    add_compiler_flags_if_supported(\"-O3\" \"-DNDEBUG\" \"-pg\" C_FLAGS SUPPORTED_C_FLAGS_PROFILING)\nendif()\nif(NOT CMAKE_CXX_FLAGS_PROFILING)\n    set(CMAKE_CXX_FLAGS_PROFILING \"${SUPPORTED_CXX_FLAGS_PROFILING}\"\n        CACHE STRING \"Flags used by the C++ compiler during profile builds.\" FORCE)\nendif()\nif(NOT CMAKE_C_FLAGS_PROFILING)\n    set(CMAKE_C_FLAGS_PROFILING \"${SUPPORTED_C_FLAGS_PROFILING}\"\n        CACHE STRING \"Flags used by the C compiler during profile builds.\" FORCE)\nendif()\nif(NOT CMAKE_EXE_LINKER_FLAGS_PROFILING)\n    set(CMAKE_EXE_LINKER_FLAGS_PROFILING \"${CMAKE_EXE_LINKER_FLAGS_RELEASE} -pg\"\n        CACHE STRING \"Flags used by the linker during profile builds.\" FORCE)\nendif()\nmark_as_advanced(CMAKE_CXX_FLAGS_PROFILING CMAKE_C_FLAGS_PROFILING CMAKE_EXE_LINKER_FLAGS_PROFILING)\n\n# have binary in the build directory\nset(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR})\n\n# enable standard CTest\ninclude(CTest)\n\n# subdirectories where the code is\nadd_subdirectory(src)\nadd_subdirectory(doc)\n\n# Don’t make the install target depend on the all target (i.e. also all tests, if on)\nset(CMAKE_SKIP_INSTALL_ALL_DEPENDENCY TRUE)\n\n# Copy the default input files to the installation directory\ninstall(DIRECTORY input/ DESTINATION \"share/${SMASH_INSTALLATION_SUBFOLDER}/input_files\")\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/smg2s",
            "repo_link": "https://github.com/SMG2S/SMG2S",
            "content": {
                "codemeta": "",
                "readme": "# Sparse Matrix Generator with Given Spectrum\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2692117.svg)](https://doi.org/10.5281/zenodo.2692117)\n\n-------------------------------------------------------------------------------\n\n\n* [Overview](#overview)\n    * [What is SMG2S?](#what-is-smg2s?)\n    * [Cite SMG2S](#cite-smg2s)\n    * [Gallery: Sparsity Patterns](#some-sparsity-patterns)\n    * [Contact and Contributation](#contact-and-contributation)\n* [Documentation](#documentation)\n    * [Getting SMG2S](#getting-smg2s)\n    * [Dependencies](#dependencies)\n    * [Quick start](#quick-start)\n    * [Installation](#installation)\n    * [Use SMG2S with own project](#use-smg2s-with-own-project)\n        * [header-only](#header-only)\n        * [CMake](#cmake)\n    * [Usage](#usage)\n        * [Parallel vector and sparse matrix](#parallel-vector-and-sparse-matrix)\n            * [parVectorMap class](#parvectormap-class)\n            * [parVector class](#parvector-class)\n            * [parMatrixSparse class](#parmatrixsparse-class)\n        * [Building blocks of SMG2S](#building-blocks-of-smg2s)\n        * [Assembling the building blocks](#assembling-the-building-blocks)\n        * [Mini-app](#mini-app)\n    * [Format of Given Spectrum Files](#format-of-given-spectrum-files)\n        * [Real eigenvalues for non-Symmetric matrices](#real-eigenvalues-for-non-symmetric-matrices)\n        * [Complex eigenvalues for non-Hermtian matrices](#complex-eigenvalues-for-non-hermtian-matrices)\n        * [Conjugate eigenvalues for non-Symmetric matrices](#conjugate-eigenvalues-for-non-symmetric-matrices)\n    * [Parallel I/O](#parallel-i/o)\n        * [I/O for parallel vector](#i/o-for-parallel-vector)\n        * [I/O for parallel sparse matrix](#i/o-for-parallel-sparse-matrix)\n    * [Interface](#interface)\n      * [Interface to C](#interface-to-c)\n    * [Plotting and Validation](#plotting-and-validation)\n\n-------------------------------------------------------------------------------\n## Overview\n\nAuthor [Xinzhe Wu](https://brunowu.github.io) @ [Maison de la Simulation](http://www.maisondelasimulation.fr), France (2016-2019).\n\n                                  @ [SDL Quantum Materials](https://www.fz-juelich.de/en/ias/jsc/about-us/structure/simulation-and-data-labs/sdl-quantum-materials), Forschungszentrum Juelich GmbH, Germany (2019-present).\n\n****\n\n### What is SMG2S?\n\n**SMG2S** is able to generate large-scale non-Hermitian and non-Symmetric matrices in parallel with the spectral distribution functions or eigenvalues given by users, and the spectrum of generated matrix is the same as the one specified by the users. SMG2S can be used to benchmark the iterative solvers for both linear systems and eigenvalue problems on supercomputers using the generated very large test matrices with customized spectral properties.\n\nAs a matrix generator, SMG2S provides:\n\n- generating of both Non-Hermitian and Non-Symmetric sparse matrix\n\n- generated matrices are naturally sparse with non-trivial sparsity pattern\n\n- Given Spectrum: the spectrum of generated matrix is the same as the one specified by the users\n\n- Sparsity patterns are diverse and controllable\n\nAs a software, SMG2S provides:\n\n* a collection of C++ header only files\n* C++ templated implementation for different data type\n* parallel implementation based on [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) which is able to efficiently generate very large sparse matrices in parallel on supercomputers\n* an easy-to-use C interface\n* a verification module based on Python for the sparsity pattern plotting and spectrum verification of small size of generated matrix.\n* Efficient parallel IO to store the generated matrix into [MatrixMarket format](https://math.nist.gov/MatrixMarket/formats.html)\n\n### Cite SMG2S\n\nIf you find SMG2S useful in your project, we kindly request that you cite the following paper:\n\n*Wu, Xinzhe, Serge G. Petiton, and Yutong Lu. \"A Parallel Generator of Non-Hermitian Matrices computed from Given Spectra.\" Concurrency and Computation: Practice and Experience, 32(20), e5710, 2020. [[DOI]](https://doi.org/10.1002/cpe.5710) [[PDF]](https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cpe.5710?casa_token=UUntHdbHvo4AAAAA:CHJa3O1_B-15_eHKY09LuWdh5TNs_trh_IXa_qDuNZLeTKcxa4CQt9WzrNsU1XSWxunknU8GeXP9Ihv9)*\n\n### Some Sparsity Patterns\n\nHere are some sparsity patterns of matrices generated by SMG2S.\n\n![Matrix Generation Pattern](docs/figure/pattern.png)\n\n### Contact and Contribution\n\nFeel free to contact by email address: **xin DOT wu AT fz BAR juelich DOT de**.\n\n## Documentation\n\n### Getting SMG2S\n\nSMG2S is able to available on the Github. The most updated version of SMG2S can be gotten either by the following `git` command:\n\n```bash\ngit clone https://github.com/SMG2S/SMG2S.git\n```\n\nMoreover a released version can be downloaded [here](https://github.com/SMG2S/SMG2S/releases)\n\n### Dependencies\n\nSMG2S is developed in `C++14` and `MPI`, and it is compiled with `CMake`. So the following software and compiler should be available before the installation of SMG2S.\n\n1. a `C++` compiler with `C++14` support\n\n2. `MPI`: message passing interface\n\n3. `CMake`: version >= `3.8`\n\n### Quick start\n\nSMG2S provides an executable `smg2s.exe` that the users can compile and start to play with SMG2S without installation as follows. \n\n```bash\ncd SMG2S\nmkdir build & cd build\ncmake .. \nmake -j\n```\n\nThen the executable `smg2s.exe`is available, and it can be run as follows:\n\n```bash\n  mpirun -np ${PROCS} ./smg2s.exe -D ${dim} -L ${diag_l} -U ${diag_u} -O ${offset} -C ${nbOne} -S ${sparsity} -M {no-herm or non-symm}\n```\n\nin which the command line parsers provides the customization of following parameters:\n\n```bash\nusage: ./smg2s.exe [options] ...\noptions:\n  -D, --dim           Dimension of matrix to be generated (int [=1000])\n  -L, --diagL         offset of lower diagonal of initial matrix (int [=-10])\n  -U, --diagU         offset of upper diagonal of initial matrix (int [=-5])\n  -O, --nilpOffset    offset of diagonal of a nilpotent (int [=5])\n  -C, --continous     Continuous length in Nilpotent matrix (int [=2])\n  -S, --sparsity      sparsity of initial matrix (NOT THE FINAL GENERATED ONES) (double [=0.95])\n  -M, --mattype       Matrix type to be generated: non-symmetric or non-Hermitian (string [=non-herm])\n  -?, --help          print this message\n```\n\n### Installation\n\nSMG2S relies on CMake for compiling and installation. A CMake flag `CMAKE_INSTALL_PREFIX` should be provided for the path of installation.\n\n```bash\ncd SMG2S\nmkdir build & cd build\ncmake .. -DCMAKE_INSTALL_PREFIX=${PATH_TO_INSTALL}\nmake -j install\n```\n\n### Use SMG2S with own project\n\n#### header-only\n\nSMG2S is a collection of C++ header files. If users want to use SMG2S with C++, they can just copy SMG2S headers into their project.\n\n#### CMake\n\nSMG2S is installed as a CMake package, and it can be detected by the CMake `find_package` command. If the installation path is not in the default searching path of CMake, a CMake flag `CMAKE_PREFIX_PATH` should be provided which links to the installation path of SMG2S.\n\nSo in your own project which want to use SMG2S:\n\n```bash\nmkdir build & cd build\ncmake .. -DCMAKE_PREFIX_PATH=${INSTALLED_PATH_OF_SMG2S}\nmake -j\n```\n\nand in the `CMakeLists.txt` of own project, it should provide some content as follows:\n\n```cmake\ncmake_minimum_required(VERSION 3.8)\nproject(YOUR-OWN-PROJECT)\n#find installation of SMG2S\nfind_package( smg2s REQUIRED CONFIG)\n# for C++ code\nadd_executable(smg2s-app test_parMatrix.cpp)\ntarget_link_libraries(smg2s-app PUBLIC SMG2S::smg2s)\n# for C-interface code\nadd_executable(test_c.exe test_c.c)\ntarget_link_libraries(test_c.exe PRIVATE SMG2S::smg2s2c)\n```\n\nIn case that the support of `C++14` is disabled by some compilers, please insert also the following lines into your `CMakeLists.txt` before the usage of SMG2S.\n\n```cmake\ninclude(CheckCXXCompilerFlag)\nCHECK_CXX_COMPILER_FLAG(\"-std=c++14\" COMPILER_SUPPORTS_CXX14)\nif(COMPILER_SUPPORTS_CXX14)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14\")\nelse()\n     message([FATAL_ERROR] \"The compiler ${CMAKE_CXX_COMPILER} has no C++14 support. Please use a different C++ compiler.\")\nendif()\n```\n\n### Usage\n\n#### Parallel vector and sparse matrix\n\nIn SMG2S, the parallelisation is supported through that both vectors and sparse matrices are naturally distributed across a 1D MPI grid of processes. SMG2S grants the freedom to the user to decide the way to distribute the vector and sparse matrices across a group of processes, e.g, for a parallel vector, user can decide the range of global indices dedicated to each process. \n\n##### parVectorMap class\n\n`parVectorMap` is a class which determines the way to distribute a vector or sparse matrix across multiple MPI processes:\n\n- This class is to create a mapping from a fixed-size vector to multiple MPI procs in 1D grid.\n\n- This class can also be used to create more distributed vectors and sparse matrices following the same way.\n\n- For each MPI proc, a piece of vector with indexing `[lower_bound, upper_bound)` is allocated.\n\n- This class is templated such that different `integer` can be used to describes the dimension of vector and matrices.\n\n- This class provides a series of member funtions for querying, please refer to [parVectorMap full API](https://smg2s.github.io/SMG2S/classpar_vector_map.html) for more details.\n\nHere is an example:\n\n```cpp\nMPI_Init(&argc, &argv);\n\nint world_size;\nint world_rank;\nint probSize = 7;\nMPI_Comm_size(MPI_COMM_WORLD, &world_size);\nMPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\nint span, lower_b, upper_b;\n\nspan = int(ceil(double(probSize)/double(world_size)));\n\nif(world_rank == world_size - 1){\n    lower_b = world_rank * span;\n    upper_b = probSize - 1 + 1;\n}else{\n    lower_b = world_rank * span;\n    upper_b = (world_rank + 1) * span - 1 + 1;\n}\nauto parVecMap = parVectorMap<int>(MPI_COMM_WORLD, lower_b, upper_b);\n```\n\n##### parVector class\n\n`parVector` class construts a dense vector across a group of MPI processes. \n\n- It is able to be initialized by a `parVectorMap` object, and following the distribution scheme determined by this `parVectorMap` object.\n\n- It is also able to be directly initialized by the range of global indices of a vector owned by each MPI process, the same as the construction of a `parVectorMap` object.\n\n- This class provides a series of member funtions for querying and manuplating of a distributed vector, please refer to [parVector full API](https://smg2s.github.io/SMG2S/classpar_vector.html) for more details.\n\n- This class is also templated which allows to build a vector with different scalar types, either real or complex, either double precision or single precision....\n\n##### parMatrixSparse class\n\n`parMatrixSparse`class constructs a sparse matrix across a group of MPI processes.\n\n- The sparse matrix is distributed on a 1D MPI process grid by row.\n  \n  - a `parMatrixSparse` object can be constructed with a `parVectorMap` obejct, such that the distribution of rows of a sparse matrix follows the way determined by this `paraVectorMap` object\n  \n  - a `parMatrixSparse`object can be also constributed with a `parVector`object, what makes this sparse matrix share the same distribution scheme with this vector.\n  \n  - For any operations, which takes both sparse matrix and vector, they should also share the same distribution scheme\n\n- This class provides a series of member funtions for querying, manuplating and mathematical operations of a distributed sparse matrices,  please refer to [parMatrixSparse full API](https://smg2s.github.io/SMG2S/classpar_matrix_sparse.html) for more details.\n\n \n\n#### Building blocks of SMG2S\n\nIn order to generate a sparse matrix with user-provided spectrum, SMG2S requires the customization of three building blocks by the users:\n\n1. user-provided spectrum\n\n2. a nilpotent matrix\n\n3. initial matrix\n\nRoughly, the workflow of SMG2S to generate a sparse matrix is that:\n\n1. a strict lower-triangular matrix is generated, which is of the same size as the matrices to be generated. This matrix can be any shape, \n   \n   - for generating non-Symmetric matrices of real eigenvalues or non-Hermtian matrices, the only constraint is to be strict lower-triangluar. \n   \n   - for generating non-Synmmetric matrices of conjugated eigenvalues, this matrix can be any strict lower-triangluar and the diagonal next to the main diagonal to be empty.\n\n   - In SMG2S, a simple struct `initMat` is provided which stores 4 parameters determining a initial matrix: `diag_l`, `diag_u`, `scale` and `sparisty`. It means that the between diagonal with offset `diag_l` and `diag_u` of lower-triangular part is filled with non-zeros elements randomly generated between `0` and `1`. The values of these elements can also be scaled with the parameter `scale`. The parameter `sparisty` determines that sparsity of inital matrix, not the final generated matrices.\n\n   - Here is the details of [`initMat`](https://smg2s.github.io/SMG2S/structinit_mat.html)\n\n2. the user-provided spectrum is stored in a `parVector` object, which shares the same distribution scheme as initial matrix. \n   \n   The spectrum can be generated:\n   \n   - inplace through the manuplating member functions of the `parVector` class\n   \n   - by loading from local text files following a specific formats, [click](#format-of-given-spectrum-files) for more details.\n\n   - parallel I/Os are provided which loads the spectrums from local, for more details, please visit [I/O for loading spectrum](https://smg2s.github.io/SMG2S/group__group1.html).\n\n   The spectrum vector is to set on the initial matrix in a way that:\n   \n   - for non-symmetric matrices with real eigenvalues or non-Hermtian matrices, the spectrum will be set directly on the main diagonal.\n   \n   - for non-symmetric matrices with conjugated eigenvalues, the real parts of eigenvalues are set on the main diagonal, and their imaginary parts are set either on the lower or upper diagonal next to the diagonal following a built-in mechanism in SMG2S. (That's why the lower diagonal next to the main diagonal of initial matrix is expected to be empty for generating non-Symmetric matrices with conjugated eigenvalues).\n\n3. the initial matrix is either right or left multiplied by a nilpotent which manages to more its entries around and keeps the spectrum at the same time. For more details, please see the algorithm shown in our paper: [[PDF]](https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cpe.5710?casa_token=UUntHdbHvo4AAAAA:CHJa3O1_B-15_eHKY09LuWdh5TNs_trh_IXa_qDuNZLeTKcxa4CQt9WzrNsU1XSWxunknU8GeXP9Ihv9).\n\n    - a class named `Nilpotent` is implemented in SMG2S, which determines the nilpotent matrix used in SMG2S.\n\n    - this class provides multiple constructors of a nilpotent matrix, either with some simple parameters, or a user-provided vector. please visit [Nilpotent class](https://smg2s.github.io/SMG2S/group__group1.html) for more details.\n\n\n#### Assembling the building blocks\n\nSMG2S provides the generation of matrices in three different categories:\n\n1. non-Hermtian matrices with complex eigenvalues\n\n2. non-Symmetric matrices with real eigenvalues\n\n3. non-Symmetric matrices with conjugated eigenvalues\n\nFor each categories, SMG2S provides three functions which allows the users having different levels of freedom to control and customize the properties of generated matrices.\n\n- 1st level: users need to provide multiple simple parameters (for inital matrix and nilpotent) and a local text file containing the eigenvalues. The distribution of matrix over MPI processes is established by using the built-in scheme in SMG2S.\n\n- 2nd level: users need to provide multiple simple parameters (for inital matrix and nilpotent). The spectrum is generated by the user on the fly and stored in a `parVector` object. The generated matrix shares the same distribution scheme as the spectrum vector.\n\n- 3rd level: users need to provide multiple simple parameters for the nilpotent. The initial matrix is provided by the users with the manuplating operations provided by SMG2S. The spectrum is also generated by the user and stored in a `parVector` object which shares the same distribution with the initial matrix.\n\nFor more details about the APIs, please visit [here](https://smg2s.github.io/SMG2S/group__group2.html).\n\n\n#### Mini-app\n\nHere is an mini-app of SMG2S which generates non-Hermitian and non-Symmetric matrices with different types of input spectrum.\n\n\n```cpp\n#include <mpi.h>\n#include <smg2s-interface.hpp>\n\nint main(int argc, char** argv) \n{\n    MPI_Init(&argc, &argv);\n    \n    int world_size;\n    int world_rank;\n    int probSize = 7;\n    int l_diag = -7;\n    int u_diag = -3;\n    int nbOne = 2;\n    int offset = 1;\n    double sparsity = 0.5;\n\n    /* construct a nilpotent object for generation */\n    Nilpotent<int> nilp = Nilpotent<int>(nbOne, offset, probSize);\n    \n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    int span, lower_b, upper_b;\n    span = int(floor(double(probSize)/double(world_size)));\n\n    if(world_rank == world_size - 1){\n        lower_b = world_rank * span;\n        upper_b = probSize - 1 + 1;\n    }else{\n        lower_b = world_rank * span;\n        upper_b = (world_rank + 1) * span - 1 + 1;\n    }\n\n    /* construct a parVecMap object which determines the distribution scheme of vectors and matrices*/\n    auto parVecMap = parVectorMap<int>(MPI_COMM_WORLD, lower_b, upper_b);\n    \n    /* example 1, generation of a non-Hermtian matrix */\n    // 1. generate the spectrum on the fly\n    parVector<std::complex<double>, int> spec1 = parVector<std::complex<double>, int>(parVecMap);\n    for(int i = lower_b; i < upper_b; i++){\n        std::complex<double> v(i+1, i+2);\n        spec1.SetValueGlobal(i, v);\n    }\n    // 2. generation \n    auto mat = nonherm<std::complex<double>, int>(probSize, nilp, initMat<int>(l_diag, u_diag, sparsity), spec1);\n\n    /* example 2, generation of a non-Symmetric matrix with real eigenvalues */\n    // 1. generate the spectrum on the fly\n    parVector<double, int> spec2 = parVector<double, int>(parVecMap);\n    for(int i = lower_b; i < upper_b; i++){\n        spec2.SetValueGlobal(i, i+1);\n    }\n    // 2. generation \n    auto mat2 = nonsymm<double , int>(probSize, nilp, initMat<int>(l_diag, u_diag, sparsity), spec2);\n\n    /* example 3, generation of a non-Symmetric matrix with conjugated eigenvalues */\n    // 1. generate the spectrum on the fly\n    parVector<std::complex<double>, int> spec3 = parVector<std::complex<double>, int>(parVecMap);\n\n    for(int i = lower_b; i < upper_b; i++){\n        if(i % 2 == 0){\n            std::complex<double> v(i/2 + 1, i/2 + 2);\n            spec3.SetValueGlobal(i, v);\n        }else{\n            std::complex<double> v(i/2 + 1, -i/2 - 2);\n            spec3.SetValueGlobal(i, v);\n        }\n        if(i == probSize - 1){\n            std::complex<double> v(i + 1, 0);\n            spec3.SetValueGlobal(i, v);\n        }\n    }\n    // 2. generation \n    auto mat3 = nonsymmconj<double , int>(probSize, nilp, initMat<int>(l_diag, u_diag, sparsity), spec3);\n\n    MPI_Finalize();\n}\n\n```\n\n\n### Format of Given Spectrum Files\n\nSMG2S is able to load user-provided spectrum in parallel from local text files. However, the provided files should conform into a specific format.\n\n1. The first line is the comment part which includes the scalar types of given spectrum. This line should be: `%%SMG2S vector in complex scalar` and `%%SMG2S vector in real scalar` for the eigenvalues in complex or real scalar type, respectively. **Attention**, for this line, the keyword `complex` or `real` should always be there and conform with the type of user-provided spectrum. The parallel IO of SMG2S queries at first this line to check if the provided eigenvalues are complex or real.\n\n2. The second line indicates the number of given eigenvaues in the files. For the ones with `3` complex values, it is `3 3 3`, and for the ones with `3` real eigenvalues, it should be `3 3`.\n\n3. Starting from the `3rd` line, it is the main content of this file. It can have either `2` or `3` columns, which depends on the scalar types of eigenvalues. For the case with complex values, the first column indicates the coordinates for each eigenvalue, the second column contains the real part of eigenvalues, and the third column is for the imaginary part of eigenvalues. For the case with real values, the two columns contain the indexing and values of eigenvalues, respectively. **Attention**, the indexing is `1`-based, rather than `0`-based.  \n\n#### Real eigenvalues for non-Symmetric matrices\n\nFor the case with real eigenvalues for non-Symmetric matrices, the given spectrum file format should be as follows:\n\n```\n%%SMG2S vector in real scalar\n3 3 \n1 10\n2 3.4790\n3 5.0540\n```\n\n#### Complex eigenvalues for non-Hermtian matrices\n\nFor the complex values for non-Hermitian matrices which are not supposed to be conjugated, the given spectrum is stored in three columns, the first column is the coordinates, the second column is the real part of complex values, and the third column is the imaginary part of complex values. Here is an example with `3` eigenvalues:\n\n    %%SMG2S vector in complex scalar\n    3 3 3\n    1 10 6.5154\n    2 10.6288 3.4790\n    3 10.7621 5.0540\n\n#### Conjugate eigenvalues for non-Symmetric matrices\n\nFor the non-Symmetric matrices whose entries are all in real scalar, they can have conjugate eigenvalues which are in complex scalar. So in order to generate non-Symmetric test matrices with given conjugated eigenvalues, the give spectrum are always stored in complex form, with three columns.\n\n**Attention**\n\nFor the non-Symmetric matrices, if one eigenvalue is complex, there is another value that they two are symmetric to the real axis in the real-imaginary plain, this is their conjugated eigenvalue. So when setting up the spectral file, one eigenvalue `a+bi` with `b != 0` should be closely followed by another eigenvalue `a-bi`. For the eigenvalues with their imaginary part to be `0`, they are stored with their imaginary part being 0. Here is an example\n\n    %%SMG2S vector in complex scalar\n    9 9 9\n    1 10.6288 -3.4790\n    2 10.6288 3.4790\n    3 2.332 0\n    4 10.7621 5.0540\n    5 10.7621 -5.0540\n    6 -2.332 0\n    7 -11.02 0\n    8 21.21 4.4\n    9 21.21 -4.4\n\n### Parallel I/O\n\n#### I/O for parallel vector\n\n- SMG2S provides the input funtionalities which is able to load spectrum from local in parameters. For more details, please visit [I/O for loading spectrum](https://smg2s.github.io/SMG2S/group__group1.html).\n\n- SMG2S provides also the output functions as member functions of  `parVector` class which saves a `parVector` object into local text files (the same format as [Format of Given Spectrum Files](#format-of-given-spectrum-files)):\n\n    - [writeToTxt](https://smg2s.github.io/SMG2S/classpar_vector.html#ae52e90a3105c377140f432251e2a3a8b) saves a vector with real scalar types.\n\n    - [writeToTxtCmplx](https://smg2s.github.io/SMG2S/classpar_vector.html#aab9f38beb4a793bf20c43431eb665d36) saves a vector with complex scalar types.\n\n#### I/O for parallel sparse matrix\n\n- SMG2S provides the writing functions which saves a `parMatrixSparse` in parallel into a local files with MatrixMarket format.\n\n    - [writeToMatrixMarket](https://smg2s.github.io/SMG2S/classpar_matrix_sparse.html#ae0bb25445d859997c267d01028de2457) saves a sparse matrix with real scalar types.\n\n    - [writeToMatrixMarketCmplx](https://smg2s.github.io/SMG2S/classpar_matrix_sparse.html#ae2106220d853b165ff53a4a643fc47d9) saves a sparse with complex scalar types.\n\n\n### Interface\n\n#### Interface to C\n\nSMG2S provides a interface to `C` for the cases with double precision scalar and both `int` and `long` for integer. In summary:\n\n- For the scalars, only `double precision` are supported.\n\n- For the integer, both `int` and `long` are supported.\n\n- Namings for C-intefaces:\n\n    - for `Nilpotent`, `initMat` and `parVectorMap`, the interfaces for the case with `long` integer has a letter `L` in some part of related function names.\n\n    - for `parVector` and `parMatrixSparse` and other independant functions which don't belong to any classes and structs, their function names starts with one of the four prefix `ds_`, `dl_`, `zs_` and `zl_`. Here, `d` refers to `double`, `s` refers to `int`, `z` refers to `dcomplex_t` and `l` refers to `long`.\n\n    - for the meaning of each function, please refer to the coresponding `C++` functions for more details.\n\n    - Here's a full [list](https://smg2s.github.io/SMG2S/group__group5.html) of APIs for the `C` interface.\n\nA basic example of usage, which is equivalent to the previous `C++` [Mini-app](#mini-app):\n\n```c\n#include <C/c-smg2s.h>\n#include <mpi.h>\n#include <math.h>\n\nint main(int argc, char* argv[]) {\n\n    MPI_Init(&argc, &argv);\n\n    int world_size;\n    int world_rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int probSize = 7;\n    int span, lower_b, upper_b;\n\n    int diagl = -5;\n    int diagu = -3;\n    int offset = 1;\n    int nbOne = 2;\n    double sparsity = 0.5;\n    /* construct a nilpotent object for generation */\n    nilp_t *nilp = newNilp_2(nbOne, offset, probSize);\n\n    span = (int)ceil((double)probSize/(double)world_size);\n\n    if(world_rank == world_size - 1){\n        lower_b = world_rank * span;\n        upper_b = probSize - 1 + 1;\n    }else{\n        lower_b = world_rank * span;\n        upper_b = (world_rank + 1) * span - 1 + 1;\n    }\n\n    /* construct a initMat object for SMG2S*/\n    initMatrix_t *initMat = newInitMatrix_3(diagl, diagu, sparsity);  \n    /* construct a parVecMap object which determines the distribution scheme of vectors and matrices*/\n    parVecMap_t *p = newParVecMap(MPI_COMM_WORLD, lower_b, upper_b);\n\n    /* example 1, generation of a non-Hermtian matrix */\n    // 1. generate the spectrum on the fly\n    zs_parVec_t *spec1 = new_zs_ParVec_2(p);\n    for(int i = lower_b; i < upper_b; i++){\n        dcomplex_t v = {i+1, i+2};\n        zs_parVecSetVal(spec1, i, v);\n    }\n    // 2. generation \n    zs_parMatSparse_t *mat1 = zs_nonherm_2(probSize, nilp, initMat, spec1);    \n    zs_parMatSparse_destory(mat1);\n\n    /* example 2, generation of a non-Symmetric matrix with real eigenvalues */\n    // 1. generate the spectrum on the fly\n    ds_parVec_t *spec2 = new_ds_ParVec_2(p);\n    for(int i = lower_b; i < upper_b; i++){\n        ds_parVecSetVal(spec2, i, i + 1);\n    }\n    // 2. generation \n    ds_parMatSparse_t *mat2 = ds_nonsymm_2(probSize, nilp, initMat, spec2);    \n    ds_parMatSparse_destory(mat2);\n\n    /* example 3, generation of a non-Symmetric matrix with conjugated eigenvalues */\n    // 1. generate the spectrum on the fly\n    zs_parVec_t *spec3 = new_zs_ParVec_2(p);\n    for(int i = lower_b; i < upper_b; i++){\n        if(i % 2 == 0){\n            dcomplex_t v = {i/2 + 1, i/2 + 2};\n            zs_parVecSetVal(spec3, i, v);\n        }else{\n            dcomplex_t v = {i/2 + 1, -i/2 - 2};\n            zs_parVecSetVal(spec3, i, v);\n        }\n        if(i == probSize - 1){\n            dcomplex_t v = {i + 1, 0};\n            zs_parVecSetVal(spec3, i, v);\n        }\n    }\n    // 2. generation \n    ds_parMatSparse_t *mat3 = ds_nonsymmconj_2(probSize, nilp, initMat, spec3);    \n    ds_parMatSparse_destory(mat3);\n\n    initMatrix_destory(initMat);\n    zs_parVec_destory(spec1);\n    ds_parVec_destory(spec2);\n    zs_parVec_destory(spec3);\n    nilp_destory (nilp);\n\n    MPI_Finalize();\n}\n```\n\n### Plotting and Validation\n\nSMG2S provides also a simple `python` script in [scripts/verification.py](scripts/verification.py), which provides a `spy` plotting function for the structure of a sparse matrix, and a verification function which compares the difference between the input spectrum and the spectrum of generated matrices.\n\n**Attention**, the spectrum of generated matrices are computed by the direct solver `eig` of `Python` package `numpy.linalg`, so this script is supposed to use for small size of matrices.\n\n```bash\nusage: verification.py [-h] [--matpath MATPATH] [--specpath SPECPATH] [--verify]\n\nverification of matrices generated matrices to keep given spectra\n\noptional arguments:\n  -h, --help           show this help message and exit\n  --matpath MATPATH    path of matrix to be plotted or verified. Matrix should be in MatrixMarket format\n  --specpath SPECPATH  path of spectrum to be verified which is used to generate the related matrix. Vector should be in SMG2S vector file format\n  --verify             if only plotting patterns or also verifying the spectrum: default false\n```\n\nThe [example 1](examples/ex1.cpp) provided by SMG2S can be a good starting point for the verification, since it provies multiple types of spectrums, and the spectrums and related matrices are all saved to local files through the parallel I/O of SMG2S.\n\nBelow are examples of the outputs of `verification.py` with two different sparse matrices generated with a same given spectrum. For more examples, please visit [docs/figure](docs/figure).\n\n![](docs/figure/verification_4.png)\n\n![](docs/figure/verification_5.png)\n\n",
                "dependencies": "cmake_minimum_required(VERSION 3.8)\n\n#project setting\nproject(SMG2S LANGUAGES C CXX VERSION 1.2.0)\n\ninclude(GNUInstallDirs)\n\n# MPI compiler\nfind_package(MPI REQUIRED)\n\ninclude(CheckCXXCompilerFlag)\nCHECK_CXX_COMPILER_FLAG(\"-std=c++14\" COMPILER_SUPPORTS_CXX14)\nif(COMPILER_SUPPORTS_CXX14)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14\")\nelse()\n     message([FATAL_ERROR] \"The compiler ${CMAKE_CXX_COMPILER} has no C++14 support. Please use a different C++ compiler.\")\nendif()\n\nadd_library(smg2s INTERFACE)\n\ntarget_include_directories(smg2s INTERFACE \n                                \"$<BUILD_INTERFACE:${CMAKE_CURRENT_LIST_DIR}/include/>\"\n                                $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>\n    )\n            \ntarget_link_libraries(smg2s INTERFACE MPI::MPI_CXX)\n\n# Generate SMG2S executable\nadd_executable(smg2s.exe smg2s.cpp)\ntarget_link_libraries(smg2s.exe PRIVATE smg2s )\n\ninstall( TARGETS smg2s\n  EXPORT smg2s-headers\n  LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n  INCLUDES DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n  ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n  )\n\ninstall(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/include/ DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n  FILES_MATCHING\n    PATTERN \"*.hpp\"\n)\n\ninstall(EXPORT smg2s-headers\n  NAMESPACE SMG2S::\n  FILE smg2s-header.cmake\n  EXPORT_LINK_INTERFACE_LIBRARIES\n  DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\n  )\n\nINSTALL(TARGETS smg2s.exe DESTINATION ${CMAKE_INSTALL_BINDIR})\n\n\nfile(GLOB C_WRAPPERS \"src/C/*.cc\")\nadd_library(smg2s2c ${C_WRAPPERS})\ntarget_include_directories(smg2s2c INTERFACE \n                            \"$<BUILD_INTERFACE:${CMAKE_CURRENT_LIST_DIR}/include/>\"\n                            $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>\n    )\n\ntarget_link_libraries(smg2s2c PUBLIC smg2s)\n\n\ninstall( TARGETS smg2s2c\n    EXPORT smg2s-c\n    LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n    INCLUDES DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n    ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n)\n\ninstall(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/include/ DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n    FILES_MATCHING\n    PATTERN \"*.h\"\n)\n\ninstall(EXPORT smg2s-c\n    NAMESPACE SMG2S::\n    FILE smg2s-c.cmake\n    EXPORT_LINK_INTERFACE_LIBRARIES\n    DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\n)\n\ninclude(CMakePackageConfigHelpers)\nconfigure_package_config_file(\n    \"cmake/Config.cmake.in\"\n    \"${CMAKE_CURRENT_BINARY_DIR}/smg2s-config.cmake\"\n    INSTALL_DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\n)\n\ninstall( FILES\n  \"${CMAKE_CURRENT_BINARY_DIR}/smg2s-config.cmake\"\n  DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\n  )\n\n###tests\nadd_subdirectory(tests)\n\n###examples\nadd_subdirectory(examples)\n\n# SMG2S test\nenable_testing()\nadd_test(Test_Size_10000_w_proc1 ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 1 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 10000)\nadd_test(Test_Size_20000_w_proc2 ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 2 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 20000)\n\nadd_test(Test_Size_10000_s_proc1 ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 1 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 10000)\nadd_test(Test_Size_10000_s_proc2 ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 2 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 10000)\n\nadd_test(Test_Size_10000_w_proc1_nonsymm ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 1 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 10000 -M non-symm)\nadd_test(Test_Size_20000_w_proc2_nonsymm ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 2 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 20000 -M non-symm)\n\nadd_test(Test_Size_10000_s_proc1_nonsymm ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 1 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 10000 -M non-symm)\nadd_test(Test_Size_10000_s_proc2_nonsymm ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 2 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 10000 -M non-symm)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/somesy",
            "repo_link": "https://github.com/Materials-Data-Science-and-Informatics/somesy",
            "content": {
                "codemeta": "{\n  \"@context\": [\n    \"https://doi.org/10.5063/schema/codemeta-2.0\",\n    \"https://w3id.org/software-iodata\",\n    \"https://raw.githubusercontent.com/jantman/repostatus.org/master/badges/latest/ontology.jsonld\",\n    \"https://schema.org\",\n    \"https://w3id.org/software-types\"\n  ],\n  \"@type\": \"SoftwareSourceCode\",\n  \"author\": [\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Mustafa\",\n      \"familyName\": \"Soylu\",\n      \"email\": \"m.soylu@fz-juelich.de\",\n      \"@id\": \"https://orcid.org/0000-0003-2637-0432\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Anton\",\n      \"familyName\": \"Pirogov\",\n      \"email\": \"a.pirogov@fz-juelich.de\",\n      \"@id\": \"https://orcid.org/0000-0002-5077-7497\"\n    }\n  ],\n  \"name\": \"somesy\",\n  \"description\": \"A CLI tool for synchronizing software project metadata.\",\n  \"version\": \"0.4.3\",\n  \"keywords\": [\n    \"metadata\",\n    \"FAIR\"\n  ],\n  \"maintainer\": [\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Mustafa\",\n      \"familyName\": \"Soylu\",\n      \"email\": \"m.soylu@fz-juelich.de\",\n      \"@id\": \"https://orcid.org/0000-0003-2637-0432\"\n    }\n  ],\n  \"license\": [\n    \"https://spdx.org/licenses/MIT\"\n  ],\n  \"softwareHelp\": \"https://materials-data-science-and-informatics.github.io/somesy\",\n  \"codeRepository\": \"https://github.com/Materials-Data-Science-and-Informatics/somesy\",\n  \"buildInstructions\": \"https://materials-data-science-and-informatics.github.io/somesy\",\n  \"contributor\": [\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Jens\",\n      \"familyName\": \"Bröder\",\n      \"email\": \"j.broeder@fz-juelich.de\",\n      \"@id\": \"https://orcid.org/0000-0001-7939-226X\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Volker\",\n      \"familyName\": \"Hofmann\",\n      \"email\": \"v.hofmann@fz-juelich.de\",\n      \"@id\": \"https://orcid.org/0000-0002-5149-603X\"\n    },\n    {\n      \"@type\": \"Person\",\n      \"givenName\": \"Stefan\",\n      \"familyName\": \"Sandfeld\",\n      \"email\": \"s.sandfeld@fz-juelich.de\",\n      \"@id\": \"https://orcid.org/0000-0001-9560-4728\"\n    }\n  ],\n  \"url\": \"https://materials-data-science-and-informatics.github.io/somesy\"\n}\n",
                "readme": "[\n![Docs](https://img.shields.io/badge/read-docs-success)\n](https://materials-data-science-and-informatics.github.io/somesy)\n[\n![CI](https://img.shields.io/github/actions/workflow/status/Materials-Data-Science-and-Informatics/somesy/ci.yml?branch=main&label=ci)\n](https://github.com/Materials-Data-Science-and-Informatics/somesy/actions/workflows/ci.yml)\n[\n![Test Coverage](https://materials-data-science-and-informatics.github.io/somesy/main/coverage_badge.svg)\n](https://materials-data-science-and-informatics.github.io/somesy/main/coverage)\n[\n![Docs Coverage](https://materials-data-science-and-informatics.github.io/somesy/main/interrogate_badge.svg)\n](https://materials-data-science-and-informatics.github.io/somesy)\n[\n![PyPIPkgVersion](https://img.shields.io/pypi/v/somesy)\n](https://pypi.org/project/somesy/)\n[\n![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7701/badge)\n](https://bestpractices.coreinfrastructure.org/projects/7701)\n[\n![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)\n](https://fair-software.eu)\n\n<!-- --8<-- [start:abstract] -->\n\n<div style=\"text-align: center;\">\n    <img alt=\"HMC Logo\" src=\"https://github.com/Materials-Data-Science-and-Informatics/Logos/raw/main/Somesy/Somesy_Logo_Text.png\" style=\"width: 50%; height: 50%;\" />\n</div>\n\n# somesy\n\nSomesy (**so**ftware **me**tadata **sy**nc) is a CLI tool to avoid messy software project metadata by keeping it in sync.\n\n## Description\n\nMany development tools either declare or need information about the software project they are used in, such as: the project name, description, version, repository url, license or project authors.\nMost such tools come with configuration files and conventions that are specific to the programming language or chosen technology.\nEmerging best practices for [FAIR](https://www.go-fair.org/fair-principles/) software metadata require to add even _more_ files where such metadata must be stated.\n\nIf good project metadata was a fire-and-forget issue, this would be acceptable, but software is never standing still - maintainers change, contributors come and go, the version number is regularly increased, the project might be moved to a different location.\nProperly maintaining this kind of information in various files scattered around the project is usually _tedious, error-prone and time consuming manual labor_.\n\n**Somesy automates the synchronization of software project metadata and frees your time to focus on your _actual_ work**.\n\n<!-- --8<-- [end:abstract] -->\n\n**You can find more information on configuring, using and contributing to `somesy` in the\n[documentation](https://materials-data-science-and-informatics.github.io/somesy/main).**\n\n<!-- --8<-- [start:quickstart] -->\n\n## Getting Started\n\n### Platform Support\n\nStarting with version **0.3.0**, `somesy` supports Linux, MacOS and Windows.\n\nMake sure that you use the latest version in order to avoid any problems.\n\n### Installing somesy\n\nSomesy requires Python `>=3.8`. To get a first impression, you can install the\nlatest stable version of somesy from PyPI using `pip`:\n\n```bash\npip install somesy\n```\n\n### Configuring somesy\n\nYes, somesy is _another_ tool with its own configuration. However, for your\nproject metadata it is hopefully the last file you need, and the only one you\nhave to think about, `somesy` will take care of the others for you!\n\nTo get started, create a file named `somesy.toml`:\n\n<!-- --8<-- [start:somesytoml] -->\n\n```toml\n[project]\nname = \"my-amazing-project\"\nversion = \"0.1.0\"\ndescription = \"Brief description of my amazing software.\"\n\nkeywords = [\"some\", \"descriptive\", \"keywords\"]\nlicense = \"MIT\"\nrepository = \"https://github.com/username/my-amazing-project\"\n\n# This is you, the proud author of your project:\n[[project.people]]\ngiven-names = \"Jane\"\nfamily-names = \"Doe\"\nemail = \"j.doe@example.com\"\norcid = \"https://orcid.org/0000-0000-0000-0001\"\nauthor = true      # is a full author of the project (i.e. appears in citations)\nmaintainer = true  # currently maintains the project (i.e. is a contact person)\n\n# this person is an acknowledged contributor, but not author or maintainer:\n[[project.people]]\ngiven-names = \"Another\"\nfamily-names = \"Contributor\"\nemail = \"a.contributor@example.com\"\norcid = \"https://orcid.org/0000-0000-0000-0002\"\n# ... but for scientific publications, this contributor should be listed as author:\npublication_author = true\n\n[config]\nverbose = true     # show detailed information about what somesy is doing\n```\n\n<!-- --8<-- [end:somesytoml] -->\n\nAlternatively, you can also add the somesy configuration to an existing\n`pyproject.toml`, `package.json`, `Project.toml`, or `fpm.toml` file. The somesy [manual](https://materials-data-science-and-informatics.github.io/somesy/main/manual/#somesy-input-file) contains examples showing how to do that.\n\n### Using somesy\n\nOnce somesy is installed and configured, somesy can take over and manage your project metadata.\nNow you can run `somesy` simply by using\n\n```bash\nsomesy sync\n```\n\nThe information in your `somesy.toml` is used as the **primary and\nauthoritative** source for project metadata, which is used to update all\nsupported (and enabled) _target files_. You can find an overview of supported\nformats further below.\n\nBy default, `somesy` will create (if they did not exist) or update `CITATION.cff` and `codemeta.json` files in your repository.\nIf you happen to use\n\n- `pyproject.toml` (in Python projects),\n- `package.json` (in JavaScript projects),\n- `Project.toml` (in Julia projects),\n- `fpm.toml` (in Fortran projects),\n- `pom.xml` (in Java projects),\n- `mkdocs.yml` (in projects using MkDocs),\n- `Cargo.toml` (in Rust projects)\n\nthen somesy would also update the respective information there.\n\nYou can see call available options with `somesy --help`,\nall of these can also be conveniently set in your `somesy.toml` file.\n\n### Somesy as a pre-commit hook\n\n<!-- --8<-- [start:precommit] -->\n\nWe highly recommend to use `somesy` as a [pre-commit hook](https://pre-commit.com/).\nA pre-commit hook runs on every commit to automatically point out issues or fix them on the spot,\nso if you do not use pre-commit in your project yet, you should start today!\nWhen used this way, `somesy` can fix most typical issues with your project\nmetadata even before your changes can leave your computer.\n\nTo add `somesy` as a pre-commit hook, add it to your `.pre-commit-config.yaml`\nfile in the root folder of your repository:\n\n```yaml\nrepos:\n  # ... (your other hooks) ...\n  - repo: https://github.com/Materials-Data-Science-and-Informatics/somesy\n    rev: \"v0.4.3\"\n    hooks:\n      - id: somesy\n```\n\nNote that `pre-commit` gives `somesy` the [staged](https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F) version of files,\nso when using `somesy` with pre-commit, keep in mind that\n\n- if `somesy` changed some files, you need to `git add` them again (and rerun pre-commit)\n- if you explicitly run `pre-commit`, make sure to `git add` all changed files (just like before a commit)\n\n<!-- --8<-- [end:precommit] -->\n\n## Supported File Formats\n\nHere is an overview of all the currently supported files and formats.\n\n| Input Formats  | Status |     | Target Formats                | Status |\n| -------------- | ------ | --- | ----------------------------- | ------ |\n| (.)somesy.toml | ✓      |     | pyproject.toml _(poetry)_     | ✓      |\n| pyproject.toml | ✓      |     | pyproject.toml _(setuptools)_ | ✓(1.)  |\n| package.json   | ✓      |     | package.json _(JavaScript)_   | ✓(2.)  |\n| Project.toml   | ✓      |     | Project.toml _(Julia)_        | ✓      |\n| fpm.toml       | ✓      |     | fpm.toml _(Fortran)_          | ✓(3.)  |\n|                | ✓      |     | pom.toml _(Java)_             | ✓(4.)  |\n| Cargo.toml     | ✓      |     | Cargo.toml _(Rust)_           | ✓      |\n|                |        |     | mkdocs.yml                    | ✓(5.)  |\n|                |        |     | CITATION.cff                  | ✓      |\n|                |        |     | codemeta.json                 | ✓(6.)  |\n\n**Notes:**\n\n1. note that `somesy` does not support setuptools _dynamic fields_\n2. `package.json` only supports one author, so `somesy` will pick the _first_ listed author\n3. `fpm.toml` only supports one author and maintainer, so `somesy` will pick the _first_ listed author and maintainer\n4. `pom.xml` has no concept of `maintainers`, but it can have multiple licenses (somesy only supports one main project license)\n5. `mkdocs.yml` is a bit special, as it is not a project file, but a documentation file. `somesy` will only update it if it exists and is enabled in the configuration\n6. unlike other targets, `somesy` will _re-create_ the `codemeta.json` (i.e. do not edit it by hand!)\n\n<!-- --8<-- [end:quickstart] -->\n\n<!-- --8<-- [start:citation] -->\n\n## How to Cite\n\nIf you want to cite this project in your scientific work,\nplease use the [citation file](https://citation-file-format.github.io/)\nin the [repository](https://github.com/Materials-Data-Science-and-Informatics/somesy/blob/main/CITATION.cff).\n\n<!-- --8<-- [end:citation] -->\n<!-- --8<-- [start:acknowledgements] -->\n\n## Acknowledgements\n\nWe kindly thank all\n[authors and contributors](https://materials-data-science-and-informatics.github.io/somesy/latest/credits).\n\n<div>\n<img style=\"vertical-align: middle;\" alt=\"HMC Logo\" src=\"https://github.com/Materials-Data-Science-and-Informatics/Logos/raw/main/HMC/HMC_Logo_M.png\" width=50% height=50% />\n&nbsp;&nbsp;\n<img style=\"vertical-align: middle;\" alt=\"FZJ Logo\" src=\"https://github.com/Materials-Data-Science-and-Informatics/Logos/raw/main/FZJ/FZJ.png\" width=30% height=30% />\n</div>\n<br />\n\nThis project was developed at the Institute for Materials Data Science and Informatics\n(IAS-9) of the Jülich Research Center and funded by the Helmholtz Metadata Collaboration\n(HMC), an incubator-platform of the Helmholtz Association within the framework of the\nInformation and Data Science strategic initiative.\n\n<!-- --8<-- [end:acknowledgements] -->\n\n",
                "dependencies": "[tool.poetry]\nname = \"somesy\"\nversion = \"0.4.3\"\ndescription = \"A CLI tool for synchronizing software project metadata.\"\nauthors = [\"Mustafa Soylu <m.soylu@fz-juelich.de>\", \"Anton Pirogov <a.pirogov@fz-juelich.de>\"]\nmaintainers = [\"Mustafa Soylu <m.soylu@fz-juelich.de>\"]\nlicense = \"MIT\"\n\nreadme = \"README.md\"\nrepository = \"https://github.com/Materials-Data-Science-and-Informatics/somesy\"\nhomepage = \"https://materials-data-science-and-informatics.github.io/somesy\"\ndocumentation = \"https://materials-data-science-and-informatics.github.io/somesy\"\n\nkeywords = [\"metadata\", \"FAIR\"]\nclassifiers = [\n    \"Operating System :: POSIX :: Linux\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n    \"Topic :: Software Development :: Libraries :: Application Frameworks\",\n    \"License :: OSI Approved :: MIT License\",\n]\n\n# the Python packages that will be included in a built distribution:\npackages = [{include = \"somesy\", from = \"src\"}]\n\n# always include basic info for humans and core metadata in the distribution,\n# include files related to test and documentation only in sdist:\ninclude = [\n  \"*.md\", \"LICENSE\", \"LICENSES\", \".reuse/dep5\", \"CITATION.cff\", \"codemeta.json\",\n  \"mkdocs.yml\", \"docs\", \"tests\",\n  { path = \"mkdocs.yml\", format = \"sdist\" },\n  { path = \"docs\", format = \"sdist\" },\n  { path = \"tests\", format = \"sdist\" },\n]\n\n[tool.poetry.dependencies]\npython = \"^3.8\"\npydantic = {extras = [\"email\"], version = \"^2.8.2\"}\nruamel-yaml = \"^0.18.6\"\ntomlkit = \"^0.13.0\"\nimportlib-metadata = \"^8.0.0\"\ntyper = {extras = [\"all\"], version = \"^0.12.3\"}\ncffconvert = \"^2.0.0\"\nwrapt = \"^1.16.0\"\npackaging = \"^24.1\"\njinja2 = \"^3.1.4\"\ndefusedxml = \"^0.7.1\"\n\n[tool.poetry.group.dev.dependencies]\npoethepoet = \"^0.27.0\"\npre-commit = \"^3.5.0\"\npytest = \"^8.3.1\"\npytest-cov = \"^5.0.0\"\nhypothesis = \"^6.108.4\"\nlicensecheck = \"^2024.2\"\npytest-mock = \"^3.14.0\"\n\n[tool.poetry.group.docs]\noptional = true\n\n[tool.poetry.group.docs.dependencies]\nmkdocs = \"^1.6.0\"\nmkdocstrings = {extras = [\"python\"], version = \"^0.25.1\"}\nmkdocs-material = \"^9.5.30\"\nmkdocs-gen-files = \"^0.5.0\"\nmkdocs-literate-nav = \"^0.6.1\"\nmkdocs-section-index = \"^0.3.9\"\nmkdocs-macros-plugin = \"^1.0.5\"\nmarkdown-include = \"^0.8.1\"\npymdown-extensions = \"^10.8.1\"\nmarkdown-exec = {extras = [\"ansi\"], version = \"^1.9.3\"}\nmkdocs-coverage = \"^1.1.0\"\nmike = \"^2.1.2\"\nanybadge = \"^1.14.0\"\ninterrogate = \"^1.7.0\"\nblack = \"^24.4.2\"\nmkdocs-exclude = \"^1.0.2\"\n\n[tool.poetry.scripts]\nsomesy = \"somesy.main:app\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n# NOTE: You can run the following with \"poetry poe TASK\"\n[tool.poe.tasks]\ninit-dev = { shell = \"pre-commit install\" }\nlint = \"pre-commit run\"  # pass --all-files to check everything\ntest = \"pytest\"  # pass --cov to also collect coverage info\ndocs = \"mkdocs build\"  # run this to generate local documentation\nlicensecheck = \"licensecheck\"  # run this when you add new deps\n\n# Tool Configurations\n# -------------------\n\n[tool.pytest.ini_options]\npythonpath = [\"src\"]\naddopts = \"--cov-report=term-missing:skip-covered\"\nfilterwarnings = [\n\"ignore::DeprecationWarning:pkg_resources.*\",\n\"ignore::DeprecationWarning:pyshacl.*\",\n# Example:\n# \"ignore::DeprecationWarning:importlib_metadata.*\",\n]\n\n[tool.coverage.run]\nsource = [\"somesy\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"if self.debug:\",\n    \"if settings.DEBUG\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n    \"if 0:\",\n    \"if TYPE_CHECKING:\",\n    \"if __name__ == .__main__.:\",\n    \"class .*\\\\bProtocol\\\\):\",\n    \"@(abc\\\\.)?abstractmethod\",\n]\n\n[tool.semantic_release]\nversion_variable = \"src/somesy/__init__.py:__version__\"\n\n[tool.ruff.lint]\nextend-select = [\"I\", \"D\", \"B\", \"S\", \"W\"]\nignore = [\"D203\", \"D213\", \"D407\", \"B008\"]\n\n[tool.ruff.lint.per-file-ignores]\n\"**/{tests,docs}/*\" = [\"ALL\"]\n\n[tool.licensecheck]\nusing = \"poetry\"\n\n[tool.mypy]\ndisable_error_code = [\"attr-defined\"]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/spatialdata-framework",
            "repo_link": "https://github.com/scverse/spatialdata/",
            "content": {
                "codemeta": "",
                "readme": "![SpatialData banner](https://github.com/scverse/spatialdata/blob/main/docs/_static/img/spatialdata_horizontal.png?raw=true)\n\n# SpatialData: an open and universal framework for processing spatial omics data.\n\n[![Tests][badge-tests]][link-tests]\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/scverse/spatialdata/main.svg)](https://results.pre-commit.ci/latest/github/scverse/spatialdata/main)\n[![codecov](https://codecov.io/gh/scverse/spatialdata/branch/main/graph/badge.svg?token=X19DRSIMCU)](https://codecov.io/gh/scverse/spatialdata)\n[![documentation badge](https://readthedocs.org/projects/scverse-spatialdata/badge/?version=latest)](https://spatialdata.scverse.org/en/latest/)\n[![DOI](https://zenodo.org/badge/487366481.svg)](https://zenodo.org/badge/latestdoi/487366481)\n[![Downloads](https://static.pepy.tech/badge/spatialdata)](https://pepy.tech/project/spatialdata)\n\nSpatialData is a data framework that comprises a FAIR storage format and a collection of python libraries for performant access, alignment, and processing of uni- and multi-modal spatial omics datasets. This repository contains the core spatialdata library. See the links below to learn more about other packages in the SpatialData ecosystem.\n\n- [spatialdata-io](https://github.com/scverse/spatialdata-io): load data from common spatial omics technologies into spatialdata.\n- [spatialdata-plot](https://github.com/scverse/spatialdata-plot): Static plotting library for spatialdata.\n- [napari-spatialdata](https://github.com/scverse/napari-spatialdata): napari plugin for interactive exploration and annotation of spatial data.\n\n[//]: # \"numfocus-fiscal-sponsor-attribution\"\n\nThe spatialdata project uses a [consensus based governance model](https://scverse.org/about/roles/) and is fiscally sponsored by [NumFOCUS](https://numfocus.org/). Consider making a [tax-deductible donation](https://numfocus.org/donate-to-scverse) to help the project pay for developer time, professional services, travel, workshops, and a variety of other needs.\n\nThe spatialdata project also received support by the Chan Zuckerberg Initiative.\n\n<div align=\"center\">\n  <a href=\"https://numfocus.org/project/scverse\">\n    <img height=\"60px\" \n         src=\"https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png\" \n         align=\"center\">\n  </a>\n</div>\n<br>\n\n![SpatialDataOverview](https://github.com/scverse/spatialdata/assets/1120672/cb91071f-12a7-4b8e-9430-2b3a0f65e52f)\n\n- **The library is currently under review.** We expect there to be changes as the community provides feedback. We have an announcement channel for communicating these changes, please see the contact section below.\n- The SpatialData storage format is built on top of the [OME-NGFF](https://ngff.openmicroscopy.org/latest/) specification.\n\n## Getting started\n\nPlease refer to the [documentation][link-docs]. In particular:\n\n- [API documentation][link-api].\n- [Design doc][link-design-doc].\n- [Example notebooks][link-notebooks].\n\nAnother useful resource to get started is the source code of the [`spatialdata-io`](https://github.com/scverse/spatialdata-io) package, which shows example of how to read data from common technologies.\n\n## Installation\n\nCheck out the docs for more complete [installation instructions](https://spatialdata.scverse.org/en/stable/installation.html). To get started with the \"batteries included\" installation, you can install via pip:\n\n```bash\npip install \"spatialdata[extra]\"\n```\n\nor via conda:\n\n```bash\nmamba install -c conda-forge spatialdata napari-spatialdata spatialdata-io spatialdata-plot\n```\n\n## Limitations\n\n- Code only manually tested for Windows machines. Currently the framework is being developed using Linux, macOS and Windows machines, but it is automatically tested only for Linux and macOS machines.\n\n## Contact\n\nTo get involved in the discussion, or if you need help to get started, you are welcome to use the following options.\n\n- <ins>Chat</ins> via [`scverse` Zulip](https://scverse.zulipchat.com/#narrow/stream/315824-spatial) (public or 1 to 1).\n- <ins>Forum post</ins> in the [scverse discourse forum](https://discourse.scverse.org/).\n- <ins>Bug report/feature request</ins> via the [GitHub issue tracker][issue-tracker].\n- <ins>Zoom call</ins> as part of the SpatialData Community Meetings, held every 2 weeks on Thursday, [schedule here](https://hackmd.io/enWU826vRai-JYaL7TZaSw).\n\nFinally, especially relevant for for developers that are building a library upon `spatialdata`, please follow this channel for:\n\n- <ins>Announcements</ins> on new features and important changes [Zulip](https://imagesc.zulipchat.com/#narrow/stream/329057-scverse/topic/spatialdata.20announcements).\n\n## Citation\n\nMarconato, L., Palla, G., Yamauchi, K.A. et al. SpatialData: an open and universal data framework for spatial omics. Nat Methods (2024). https://doi.org/10.1038/s41592-024-02212-x\n\n<!-- Links -->\n\n[scverse-discourse]: https://discourse.scverse.org/\n[issue-tracker]: https://github.com/scverse/spatialdata/issues\n[changelog]: https://spatialdata.readthedocs.io/latest/changelog.html\n[design doc]: https://scverse-spatialdata.readthedocs.io/en/latest/design_doc.html\n[link-docs]: https://spatialdata.scverse.org/en/latest/\n[link-api]: https://spatialdata.scverse.org/en/latest/api.html\n[link-design-doc]: https://spatialdata.scverse.org/en/latest/design_doc.html\n[link-notebooks]: https://spatialdata.scverse.org/en/latest/tutorials/notebooks/notebooks.html\n[badge-tests]: https://github.com/scverse/spatialdata/actions/workflows/test.yaml/badge.svg\n[link-tests]: https://github.com/scverse/spatialdata/actions/workflows/test.yaml\n\n",
                "dependencies": "[build-system]\nbuild-backend = \"hatchling.build\"\nrequires = [\"hatchling\", \"hatch-vcs\"]\n\n\n[project]\nname = \"spatialdata\"\ndescription = \"Spatial data format.\"\nauthors = [\n    {name = \"scverse\"},\n]\nmaintainers = [\n    {name = \"scverse\", email = \"giov.pll@gmail.com\"},\n]\nurls.Documentation = \"https://spatialdata.scverse.org/en/latest\"\nurls.Source = \"https://github.com/scverse/spatialdata.git\"\nurls.Home-page = \"https://github.com/scverse/spatialdata.git\"\nrequires-python = \">=3.10, <3.13\" # include 3.13 once multiscale-spatial-image conflicts are resolved\ndynamic= [\n  \"version\" # allow version to be set by git tags\n]\nlicense = {file = \"LICENSE\"}\nreadme = \"README.md\"\ndependencies = [\n    \"anndata>=0.9.1\",\n    \"click\",\n    \"dask-image\",\n    \"dask>=2024.4.1,<=2024.11.2\",\n    \"fsspec\",\n    \"geopandas>=0.14\",\n    \"multiscale_spatial_image>=2.0.2\",\n    \"networkx\",\n    \"numba\",\n    \"numpy\",\n    \"ome_zarr>=0.8.4\",\n    \"pandas\",\n    \"pooch\",\n    \"pyarrow\",\n    \"rich\",\n    \"setuptools\",\n    \"shapely>=2.0.1\",\n    \"spatial_image>=1.1.0\",\n    \"scikit-image\",\n    \"scipy\",\n    \"typing_extensions>=4.8.0\",\n    \"xarray>=2024.10.0\",\n    \"xarray-schema\",\n    \"xarray-spatial>=0.3.5\",\n    \"zarr<3\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"bump2version\",\n]\ndocs = [\n    \"sphinx>=4.5\",\n\t\"sphinx-autobuild\",\n    \"sphinx-book-theme>=1.0.0\",\n    \"myst-nb\",\n    \"sphinxcontrib-bibtex>=1.0.0\",\n    \"sphinx-autodoc-typehints\",\n    \"sphinx-design\",\n    # For notebooks\n    \"ipython>=8.6.0\",\n    \"sphinx-copybutton\",\n    \"sphinx-pytest\",\n]\ntest = [\n    \"pytest\",\n    \"pytest-cov\",\n    \"pytest-mock\",\n    \"torch\",\n]\nbenchmark = [\n    \"asv\",\n]\ntorch = [\n    \"torch\"\n]\nextra  = [\n    \"napari-spatialdata[all]\",\n    \"spatialdata-plot\",\n    \"spatialdata-io\",\n]\n\n[tool.coverage.run]\nsource = [\"spatialdata\"]\nomit = [\n    \"**/test_*.py\",\n]\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\nxfail_strict = true\naddopts = [\n#    \"-Werror\",  # if 3rd party libs raise DeprecationWarnings, just use filterwarnings below\n    \"--import-mode=importlib\",  # allow using test files with same name\n    \"-s\"  # print output from tests\n]\n# info on how to use this https://stackoverflow.com/questions/57925071/how-do-i-avoid-getting-deprecationwarning-from-inside-dependencies-with-pytest\nfilterwarnings = [\n    #     \"ignore:.*U.*mode is deprecated:DeprecationWarning\",\n]\n\n[tool.black]\nline-length = 120\ntarget-version = ['py310']\ninclude = '\\.pyi?$'\nexclude = '''\n(\n  /(\n      \\.eggs\n    | \\.git\n    | \\.hg\n    | \\.mypy_cache\n    | \\.tox\n    | \\.venv\n    | _build\n    | buck-out\n    | build\n    | dist\n  )/\n)\n'''\n\n[tool.jupytext]\nformats = \"ipynb,md\"\n\n[tool.hatch.build.targets.wheel]\npackages = ['src/spatialdata']\n\n[tool.hatch.version]\nsource = \"vcs\"\n\n[tool.hatch.build.hooks.vcs]\nversion-file = \"_version.py\"\n\n[tool.hatch.metadata]\nallow-direct-references = true\n\n[tool.ruff]\nexclude = [\n    \".git\",\n    \".tox\",\n    \"__pycache__\",\n    \"build\",\n    \"docs/_build\",\n    \"dist\",\n    \"setup.py\",\n]\nline-length = 120\ntarget-version = \"py310\"\n\n[tool.ruff.lint]\nignore = [\n    # Do not assign a lambda expression, use a def -> lambda expression assignments are convenient\n    \"E731\",\n    # allow I, O, l as variable names -> I is the identity matrix, i, j, k, l is reasonable indexing notation\n    \"E741\",\n    # Missing docstring in public package\n    \"D104\",\n    # Missing docstring in public module\n    \"D100\",\n    # Missing docstring in __init__\n    \"D107\",\n    # Missing docstring in magic method\n    \"D105\",\n    # Do not perform function calls in argument defaults.\n    \"B008\",\n    # Missing docstring in magic method\n    \"D105\",\n]\nselect = [\n    \"D\", # flake8-docstrings\n    \"I\", # isort\n    \"E\", # pycodestyle\n    \"F\", # pyflakes\n    \"W\", # pycodestyle\n    \"Q\", # flake8-quotes\n    \"SIM\", # flake8-simplify\n    \"TID\",  # flake-8-tidy-imports\n    \"NPY\",  # NumPy-specific rules\n    \"PT\",  # flake8-pytest-style\n    \"B\", # flake8-bugbear\n    \"UP\", # pyupgrade\n    \"C4\", # flake8-comprehensions\n    \"BLE\", # flake8-blind-except\n    \"T20\",  # flake8-print\n    \"RET\", # flake8-raise\n    \"PGH\", # pygrep-hooks\n]\nunfixable = [\"B\", \"C4\", \"UP\", \"BLE\", \"T20\", \"RET\"]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"numpy\"\n\n[tool.ruff.lint.per-file-ignores]\n    \"tests/*\" = [\"D\", \"PT\", \"B024\"]\n    \"*/__init__.py\" = [\"F401\", \"D104\", \"D107\", \"E402\"]\n    \"docs/*\" = [\"D\",\"B\",\"E\",\"A\"]\n    \"src/spatialdata/transformations/transformations.py\" = [\"D101\",\"D102\", \"D106\", \"B024\", \"T201\", \"RET504\", \"UP006\", \"UP007\"]\n    \"src/spatialdata/transformations/operations.py\" = [\"D101\",\"D102\", \"D106\", \"B024\",\"D401\", \"T201\", \"RET504\", \"RET506\", \"RET505\", \"RET504\", \"UP006\", \"UP007\"]\n    \"src/spatialdata/transformations/ngff/*.py\" = [\"D101\",\"D102\", \"D106\", \"D401\", \"E501\",\"RET506\", \"RET505\", \"RET504\", \"UP006\", \"UP007\"]\n    \"src/spatialdata/transformations/*\" = [\"RET\", \"D\", \"UP006\", \"UP007\"]\n    \"src/spatialdata/models/models.py\" = [\"D101\", \"B026\"]\n    \"src/spatialdata/dataloader/datasets.py\" = [\"D101\"]\n    \"tests/test_models/test_models.py\" = [\"NPY002\"]\n    \"tests/conftest.py\"= [\"E402\"]\n    \"benchmarks/*\" = [\"ALL\"]\n\n\n# pyupgrade typing rewrite TODO: remove at some point from per-file ignore\n# \"UP006\", \"UP007\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/spatialio",
            "repo_link": "https://codebase.helmholtz.cloud/ufz-sdi/spatialio",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/spechomo",
            "repo_link": "https://git.gfz-potsdam.de/geomultisens/spechomo",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/spex",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/spiralize",
            "repo_link": "https://github.com/jokergoo/spiralize",
            "content": {
                "codemeta": "",
                "readme": "# Visualize Data on Spirals <img width=\"150\" src=\"https://user-images.githubusercontent.com/449218/121876090-723e0900-cd09-11eb-8d0d-82fbeeb83997.png\" align=\"right\">\n\n\n[![R-CMD-check](https://github.com/jokergoo/spiral/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/spiral/actions)\n[![CRAN](https://www.r-pkg.org/badges/version/spiralize)](https://cran.r-project.org/web/packages/spiralize/index.html)\n[![CRAN](https://cranlogs.r-pkg.org/badges/grand-total/spiralize)](https://cran.r-project.org/web/packages/spiralize/index.html)\n\n\n## Features\n\nThe package **spiralize** visualizes data along an [Archimedean spiral](https://en.wikipedia.org/wiki/Archimedean_spiral).\nIt has two major advantages for visualization:\n\n1. It is able to visualize data with very long axis with high resolution.\n2. It is efficient for time series data to reveal periodic patterns.\n\n## Documentation\n\nhttps://jokergoo.github.io/spiralize/\n\n## Citation\n\nZuguang Gu, et al., spiralize: an R package for Visualizing Data on Spirals, Bioinformatics, 2021. https://doi.org/10.1093/bioinformatics/btab778\n\n## Install\n\nThe package is available on CRAN and can be installed by:\n\n```r\ninstall.packages(\"spiralize\")\n```\n\nIf you want the latest version, install it directly from GitHub:\n\n```r\nlibrary(devtools)\ninstall_github(\"jokergoo/spiralize\")\n```\n\n## Usage\n\nIt includes three steps:\n\n1. initialize the spiral,\n2. add a track,\n3. add graphics to the track.\n\nStep 2 and 3 can be applied multiple times to allow multiple-track visualization along the spiral.\n\nThe code for making spiral plot looks likes follows:\n\n```r\nlibrary(spiralize)\nspiral_initialize(...)\nspiral_track(...)\nspiral_points(...)\n...\n```\n\n## Graphics\n\nComplex plots are baiscally constructed from simple graphics. Here there are following low-level graphics functions:\n\n- `spiral_points()`\n- `spiral_lines()`\n- `spiral_rect()`\n- `spiral_segments()`\n- `spiral_polygon()`\n- `spiral_bars()`\n- `spiral_text()`\n- `spiral_axis()`\n- `spiral_yaxis()`\n- `spiral_raster()`\n\nParticularlly, horizon chart is very suitable to put on the spiral, thus there is one function for this:\n\n- `spiral_horizon()`\n\nSpiral plot can also visualize dendrograms with large number of leaves, thus there are following two functions:\n\n- `spiral_dendrogram()`\n- `spiral_phylo()` \n\n\n## Examples\n\n1. Difference of **ggplot2** daily downloads to the mean of the current year (2015-2021). Each loop contains 52 weeks so that same weeks in different years locate at the same angle in the polar coordinates.\n\n![](https://user-images.githubusercontent.com/449218/122206221-671de100-cea1-11eb-823e-6c48de851667.png)\n\n\n2. A phylogenetic life tree with 50645 species. \n\n![](https://user-images.githubusercontent.com/449218/123804978-fbe6fc80-d8ed-11eb-93d8-d3f83d552dde.png)\n\n3. The spiral COVID-19 Shiny app\n\n![](https://user-images.githubusercontent.com/449218/154753102-d66b3588-eca1-471b-bdfe-2c147ed257f5.gif)\n\n\n## License\n\nMIT @ Zuguang Gu\n\n",
                "dependencies": "Package: spiralize\nType: Package\nTitle: Visualize Data on Spirals\nVersion: 1.1.0\nDate: 2024-06-14\nAuthors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"),\n                  comment = c('ORCID'=\"0000-0002-7395-8709\"))\nDepends: R (>= 4.0.0), grid\nImports: GlobalOptions (>= 0.1.1), \n         GetoptLong (>= 0.1.8), \n         circlize,\n         stats,\n         methods,\n         grDevices,\n         lubridate,\n         utils,\n         ComplexHeatmap\nSuggests: knitr, \n          rmarkdown,\n          grImport,\n          grImport2,\n          jpeg,\n          png,\n          tiff,\n          cranlogs,\n          cowplot,\n          dendextend,\n          bezier,\n          magick,\n          ape\nDescription: It visualizes data along an Archimedean spiral <https://en.wikipedia.org/wiki/Archimedean_spiral>, \n    makes so-called spiral graph or spiral chart. \n    It has two major advantages for visualization: 1. It is able to visualize data with very long axis with high \n    resolution. 2. It is efficient for time series data to reveal periodic patterns.\nVignetteBuilder: knitr\nURL: https://github.com/jokergoo/spiralize, https://jokergoo.github.io/spiralize/\nLicense: MIT + file LICENSE\nNeedsCompilation: no\nRoxygenNote: 7.3.1\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/spirit",
            "repo_link": "https://github.com/spirit-code/spirit",
            "content": {
                "codemeta": "",
                "readme": "SPIRIT\n=============================\n**SPIN SIMULATION FRAMEWORK**<br />\n\n\n![Logo](https://imgur.com/iWc1kuE.png \"Spirit Logo\")\n\n\n&nbsp;\n\n\n**Core Library:**\n\n| Branch   | Build Status | Python Package Coverage | Core Library Coverage |\n| :------- | :----------: | :---------------------: | :-------------------: |\n| master:  | ![CI](https://github.com/spirit-code/spirit/workflows/CI/badge.svg?branch=master) | [![Coverage Status](https://coveralls.io/repos/github/spirit-code/spirit/badge.svg?branch=master)](https://coveralls.io/github/spirit-code/spirit?branch=master) | [![Coverage Status](https://codecov.io/gh/spirit-code/spirit/branch/master/graph/badge.svg)](https://codecov.io/gh/spirit-code/spirit/branch/master) |\n| develop: | ![CI](https://github.com/spirit-code/spirit/workflows/CI/badge.svg?branch=develop) | [![Coverage Status](https://coveralls.io/repos/github/spirit-code/spirit/badge.svg?branch=develop)](https://coveralls.io/github/spirit-code/spirit?branch=develop) | [![Coverage Status](https://codecov.io/gh/spirit-code/spirit/branch/develop/graph/badge.svg)](https://codecov.io/gh/spirit-code/spirit/branch/develop) |\n\n**[Python package](https://pypi.org/project/spirit/):** [![PyPI version](https://badge.fury.io/py/spirit.svg)](https://badge.fury.io/py/spirit)\n\n\n&nbsp;\n\n\nThe code is released under [MIT License](LICENSE.txt).<br />\nIf you intend to *present and/or publish* scientific results or visualisations for which you used Spirit,\nplease cite [`G. P. Müller et al., Phys. Rev. B 99, 224414 (2019)`](https://link.aps.org/doi/10.1103/PhysRevB.99.224414) and read the [docs/REFERENCE.md](docs/REFERENCE.md).\n\n**This is an open project and contributions and collaborations are always welcome!!**\nSee [docs/CONTRIBUTING.md](docs/CONTRIBUTING.md) on how to contribute or write an email to m.sallermann@fz-juelich.de<br />\nFor contributions and affiliations, see [docs/CONTRIBUTORS.md](docs/CONTRIBUTORS.md).\n\nPlease note that a version of the *Spirit Web interface* is hosted by the Research Centre Jülich at\nhttp://juspin.de\n\n\n&nbsp;\n\n<!--\n![nur ein Beispiel](https://commons.wikimedia.org/wiki/File:Example_de.jpg \"Beispielbild\")\n-->\n\n![Skyrmions](http://imgur.com/JgPj8t5.jpg \"Skyrmions on a 2D grid\")\n\n&nbsp;\n\n\n\nContents\n--------\n\n1. [Introduction](#Introduction)\n2. [Getting started with the Desktop User Interface](#Desktop)\n3. [Getting started with the Python Package](#Python)\n\n---------------------------------------------\n\n\n\n&nbsp;\n\n\n\nIntroduction <a name=\"Introduction\"></a>\n---------------------------------------------\n\n#### A modern framework for magnetism science on clusters, desktops & laptops and even your Phone\n\n**Spirit** is a **platform-independent** framework for spin dynamics, written in C++14.\nIt combines the traditional cluster work, using using the command-line, with modern\nvisualisation capabilites in order to maximize scientists' productivity.\n\n> \"It is unworthy of excellent men to lose hours like slaves in\n>  the labour of calculation which could safely be relegated to\n>  anyone else if machines were used.\"\n> - Gottfried Wilhelm Leibniz\n\n*Our goal is to build such machines*. The core library of the *Spirit* framework provides an\n**easy to use API**, which can be used from almost any programming language,\nand includes ready-to-use python bindings.\nA **powerful desktop user interface** is available, providing real-time visualisation and\ncontrol of parameters.\n\n### *Physics Features*\n\n- Atomistic Spin Lattice Heisenberg Model including also DMI and dipole-dipole\n- **Spin Dynamics simulations** obeying the\n  [Landau-Lifschitz-Gilbert equation](https://en.wikipedia.org/wiki/Landau%E2%80%93Lifshitz%E2%80%93Gilbert_equation)\n- Direct **Energy minimisation** with different solvers\n- **Minimum Energy Path calculations** for transitions between different\n  spin configurations, using the GNEB method\n\n### *Highlights of the Framework*\n\n- Cross-platform: everything can be built and run on Linux, OSX and Windows\n- Standalone core library with C API which can be used from almost any programming language\n- **Python package** making complex simulation workflows easy\n- Desktop UI with powerful, live **3D visualisations** and direct control of most system parameters\n- Modular backends including **parallelisation on GPU** (CUDA) and **CPU** (OpenMP)\n\n### *Documentation*\n\nMore details may be found at [spirit-docs.readthedocs.io](http://spirit-docs.readthedocs.io)\nor in the [Reference section](docs/README.md) including\n\n- [Unix/OSX build instructions](docs/Build_Unix_OSX.md)\n- [Windows build instructions](docs/Build_Windows.md)\n- [Input File Reference](core/docs/Input.md)\n\nThere is also a [Wiki](https://iffwiki.fz-juelich.de/index.php/Spirit \"Click me...\"),\nhosted by the Research Centre Jülich.\n\n---------------------------------------------\n\n\n\n&nbsp;\n\n\n\nGetting started with the Desktop Interface <a name=\"Desktop\"></a>\n---------------------------------------------\n\nSee the build instructions for [Unix/OSX](docs/Build_Unix_OSX.md) or\n[Windows](docs/Build_Windows.md) on how to get the desktop user interface.\n\n![Desktop UI with Isosurfaces in a thin layer](http://imgur.com/QUcN4aG.jpg \"Isosurfaces in a thin layer\")\n\nThe user interface provides a powerful OpenGL visualisation window\nusing the [VFRendering](https://github.com/FlorianRhiem/VFRendering) library.\nIt provides functionality to\n\n- Control Calculations\n- Locally insert Configurations (homogeneous, skyrmions, spin spiral, ... )\n- Generate homogeneous Transition Paths\n- Change parameters of the Hamiltonian\n- Change parameters of the Method and Solver\n- Configure the Visualization (arrows, isosurfaces, lighting, ...)\n\nSee the [UI-QT Reference](docs/UI-Qt.md) for the key bindings of the various features.\n\n*Unfortunately, distribution of binaries for the Desktop UI is not possible due\nto the restrictive license on QT-Charts.*\n\n---------------------------------------------\n\n\n\n&nbsp;\n\n\n\nGetting started with the Python Package <a name=\"Python\"></a>\n---------------------------------------------\n\nTo install the *Spirit python package*, either build and install from source\n([Unix/OSX](docs/Build_Unix_OSX.md), [Windows](docs/Build_Windows.md)) or\nsimply use\n\n    pip install spirit\n\nWith this package you have access to powerful Python APIs to run and control\ndynamics simulations or optimizations.\nThis is especially useful for work on clusters, where you can now script your\nworkflow, never having to re-compile when testing, debugging or adding features.\n\nThe most simple example of a **spin dynamics simulation** would be\n``` python\nfrom spirit import state, simulation\nwith state.State(\"input/input.cfg\") as p_state:\n    simulation.start(p_state, simulation.METHOD_LLG, simulation.SOLVER_SIB)\n```\nWhere `SOLVER_SIB` denotes the semi-implicit method B and the starting configuration\nwill be random.\n\nTo add some meaningful content, we can change the **initial configuration** by\ninserting a Skyrmion into a homogeneous background:\n``` python\ndef skyrmion_on_homogeneous(p_state):\n    from spirit import configuration\n    configuration.plus_z(p_state)\n    configuration.skyrmion(p_state, 5.0, phase=-90.0)\n```\n\nIf we want to calculate a **minimum energy path** for a transition, we need to generate\na sensible initial guess for the path and use the **GNEB method**. Let us consider\nthe collapse of a skyrmion to the homogeneous state:\n``` python\nfrom spirit import state, chain, configuration, transition, simulation\n\n### Copy the system and set chain length\nchain.image_to_clipboard(p_state)\nnoi = 7\nchain.set_length(p_state, noi)\n\n### First image is homogeneous with a Skyrmion in the center\nconfiguration.plus_z(p_state, idx_image=0)\nconfiguration.skyrmion(p_state, 5.0, phase=-90.0, idx_image=0)\nsimulation.start(p_state, simulation.METHOD_LLG, simulation.SOLVER_VP, idx_image=0)\n### Last image is homogeneous\nconfiguration.plus_z(p_state, idx_image=noi-1)\nsimulation.start(p_state, simulation.METHOD_LLG, simulation.SOLVER_VP, idx_image=noi-1)\n\n### Create transition of images between first and last\ntransition.homogeneous(p_state, 0, noi-1)\n\n### GNEB calculation\nsimulation.start(p_state, simulation.METHOD_GNEB, simulation.SOLVER_VP)\n```\nwhere `SOLVER_VP` denotes a direct minimization with the velocity projection algorithm.\n\nYou may also use *Spirit* order to **extract quantitative data**, such as the energy.\n``` python\ndef evaluate(p_state):\n    from spirit import system, quantities\n    M = quantities.get_magnetization(p_state)\n    E = system.get_energy(p_state)\n    return M, E\n```\n\nObviously you may easily create significantly more complex workflows and use Python\nto e.g. pre- or post-process data or to distribute your work on a cluster and much more!\n",
                "dependencies": "######### CMake Version ############################################\ncmake_minimum_required( VERSION 3.12 )\n### Distinguish between Clang and AppleClang\ncmake_policy( SET CMP0025 NEW )\n####################################################################\n\n\n\n######### Build options ############################################\n### CMake Verbosity\nset( SPIRIT_PRINT_SOURCES     OFF  CACHE BOOL \"Print Spirit Headers and Sources from CMake.\" )\n### These decide which projects are built\nset( SPIRIT_BUILD_FOR_JS      OFF  CACHE BOOL \"Build the JavaScript library.\" )\nset( SPIRIT_BUILD_FOR_JULIA   OFF  CACHE BOOL \"Build the shared library for Julia.\" )\nset( SPIRIT_BUILD_FOR_PYTHON  ON   CACHE BOOL \"Build the shared library for Python.\" )\nset( SPIRIT_BUILD_FOR_CXX     ON   CACHE BOOL \"Build the static library for C++ applications\" )\n### Feature switches for Spirit\nset( SPIRIT_ENABLE_PINNING    OFF  CACHE BOOL \"Enable pinning individual or rows of spins.\" )\nset( SPIRIT_ENABLE_DEFECTS    OFF  CACHE BOOL \"Enable defects and disorder in the lattice.\" )\n### Options for Spirit\nset( SPIRIT_BUILD_TEST        ON   CACHE BOOL \"Build unit tests for the Spirit library.\" )\nset( SPIRIT_TEST_COVERAGE     OFF  CACHE BOOL \"Build in debug mode with special flags for coverage checks.\" )\nset( SPIRIT_USE_CUDA          OFF  CACHE BOOL \"Use CUDA to speed up certain parts of the code.\" )\nset( SPIRIT_USE_OPENMP        OFF  CACHE BOOL \"Use OpenMP to speed up certain parts of the code.\" )\nset( SPIRIT_USE_THREADS       OFF  CACHE BOOL \"Use std threads to speed up certain parts of the code.\" )\nset( SPIRIT_USE_FFTW          ON   CACHE BOOL \"If available, use the FFTW library instead of kissFFT.\" )\n### Set the scalar type used in the Spirit library\nset( SPIRIT_SCALAR_TYPE \"double\" CACHE STRING \"The scalar type to be used in the Spirit library.\" )\n### Set the compute capability for CUDA compilation\nset( SPIRIT_CUDA_ARCH   \"sm_60\"  CACHE STRING \"The CUDA compute architecture to use in case of a CUDA build.\" )\n####################################################################\n### CMake Verbosity\noption( SPIRIT_PRINT_SOURCES    \"Print Headers and Sources from Cmake.\"                          OFF )\n### Decide UI\noption( SPIRIT_UI_USE_IMGUI     \"Build the ImGUI user interface instead of the console version.\" OFF )\noption( SPIRIT_UI_CXX_USE_QT    \"Build the QT user interface instead of the console version.\"    ON  )\n### Bundle option\noption( SPIRIT_BUNDLE_APP       \"On installation, bundle the executable with its dependencies.\"  OFF )\n### Option for building on the IFF cluster\noption( SPIRIT_USER_PATHS_IFF   \"Use the compiler and library paths etc. for the IFF Cluster.\"   OFF )\n####################################################################\n\n\n\n####################################################################\n### Set a default build type in case none is passed\nif( NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES )\n    message( STATUS \">> Setting build type to 'Release' as none was specified.\" )\n    set( CMAKE_BUILD_TYPE Release CACHE STRING \"Choose the type of build.\" FORCE )\n    # Set the possible values of build type for cmake-gui\n    set_property( CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\" )\nelseif( CMAKE_BUILD_TYPE )\n    message( STATUS \">> Did not need to set build type, using: ${CMAKE_BUILD_TYPE}\" )\nelse()\n    message( STATUS \">> Did not need to set build type. Configuration types: ${CMAKE_CONFIGURATION_TYPES}\" )\nendif()\n### Set a default install directory in case none is passed\nif( CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT OR NOT CMAKE_INSTALL_PREFIX )\n    set( CMAKE_INSTALL_PREFIX \"${CMAKE_BINARY_DIR}/install\" CACHE PATH \"default install path\" FORCE )\n    message( STATUS \">> No installation directory given. Using: '${CMAKE_INSTALL_PREFIX}'\" )\nelse()\n    message( STATUS \">> Installation directory given: '${CMAKE_INSTALL_PREFIX}'\" )\nendif()\n### Prevent in-source builds\n# set(CMAKE_DISABLE_SOURCE_CHANGES ON) # we need source changes for the generated VERSION.txt\nset( CMAKE_DISABLE_IN_SOURCE_BUILD ON )\n### Position independent code\nset( CMAKE_POSITION_INDEPENDENT_CODE ON )\n### We need at least C++14\nset( CMAKE_CXX_STANDARD 14 )\n### Set the cmake subdirectory\nlist( APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_SOURCE_DIR}/CMake\" )\n####################################################################\n\n\n\n####################################################################\n### Depending on compiler versions it may be necessary to specify\n### the compiler. Either pass them in via command-line or use\n### the CUDA_TOOLKIT_ROOT_DIR variable.\nif( SPIRIT_USE_CUDA )\n    ### Deactivate OpenMP\n    set( SPIRIT_USE_OPENMP OFF )\n    ### Set cuda toolkit path\n    if( NOT CUDA_TOOLKIT_ROOT_DIR )\n        if( APPLE OR UNIX )\n            set( CUDA_TOOLKIT_ROOT_DIR /usr/local/cuda )\n            # set( CUDA_TOOLKIT_ROOT_DIR /opt/cuda )\n        elseif( WIN32 )\n            set( CUDA_TOOLKIT_ROOT_DIR \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/\" )\n            message( WARNING \">> We are on Windows... CUDA_TOOLKIT_ROOT_DIR may need to be passed to cmake...\" )\n        endif()\n    endif()\n    ### Set compilers\n    if( APPLE OR UNIX )\n        if( DEFINED CUDA_TOOLKIT_ROOT_DIR )\n            message( STATUS \">> CUDA toolkit root dir: ${CUDA_TOOLKIT_ROOT_DIR}\" )\n            if( NOT DEFINED CMAKE_C_COMPILER )\n                message( STATUS \">> Set C compiler accordingly: ${CMAKE_C_COMPILER}\" )\n            endif()\n            if( NOT DEFINED CMAKE_CXX_COMPILER )\n                message( STATUS \">> Set CXX compiler accordingly: ${CMAKE_CXX_COMPILER}\" )\n            endif()\n        else()\n            message( STATUS \">> No CUDA toolkit root dir specified\" )\n        endif()\n    elseif( WIN32 )\n        # MESSAGE( STATUS \">> We are on Windows... CUDA untested\" )\n    endif()\nendif()\n####################################################################\n\n\n\n######### Determine the compiler ###################################\n### IFF cluster paths\nif( SPIRIT_USER_PATHS_IFF )\n    message( STATUS \">> Using IFF Paths\" )\n    ### GCC compiler\n    set( USER_COMPILER_C    \"gcc\" )\n    set( USER_COMPILER_CXX  \"g++\" )\n    set( USER_PATH_COMPILER \"/usr/local/gcc6/bin\" )\n    ### Qt location\n    set( USER_PATH_QT       \"/usr/local/qt5\" )\nendif()\n### User Paths\n### Set the following if you do not want cmake to choose your compiler\n# set( USER_COMPILER_C    \"gcc\" )\n# set( USER_COMPILER_CXX  \"g++\" )\n# set( USER_PATH_COMPILER \"/usr/bin\" )\n### Set the following if you need cmake to find your Qt installation\n# set( USER_PATH_QT       \"~/QT/5.7\" )\n### Choose the right compiler\ninclude( ChooseCompiler )\n####################################################################\n\n\n\n######### Project name #############################################\nproject( spirit )\n### Print compiler info\nmessage( STATUS \">> Please check the CMAKE_CXX_COMPILER to make sure it's the right one\" )\nmessage( STATUS \">> CMAKE_C_COMPILER:   ${CMAKE_C_COMPILER}\" )\nmessage( STATUS \">> CMAKE_CXX_COMPILER: ${CMAKE_CXX_COMPILER}\" )\n####################################################################\n\n\n\n######### Platform-specific Flags ##################################\n### Platform-specific flags\nif( APPLE )\n    set( PLATFORM_NAME \"Apple\" )\n    if( SPIRIT_BUNDLE_APP )\n        message( STATUS \">> Going to create a .app bundle\" )\n        set( OS_BUNDLE MACOSX_BUNDLE )\n    endif()\nelseif( UNIX )\n    set( PLATFORM_NAME \"UNIX\" )\nelseif( WIN32 )\n    set( PLATFORM_NAME \"Win32\" )\n    add_compile_definitions( NOMINMAX _CRT_SECURE_NO_WARNINGS )\nendif()\n### Compiler-specific flags\nif( \"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\" )\n    if( CMAKE_CXX_COMPILER_VERSION VERSION_LESS 5.1 )\n        message( FATAL_ERROR \"GCC version must be at least 5.1!\" )\n    endif()\nelseif( \"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\" )\n    ### Disable unnecessary warnings on Windows, such as C4996 and C4267, C4244\n    set( CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DNOMINMAX /wd4018 /wd4244 /wd4267 /wd4661 /wd4996\" )\nendif()\n###\nmessage( STATUS \">> We are on the platform: ${PLATFORM_NAME}\" )\nmessage( STATUS \">> CMAKE_CXX_COMPILER_ID:  ${CMAKE_CXX_COMPILER_ID}\" )\nmessage( STATUS \">> CMAKE_CXX_FLAGS:        ${CMAKE_CXX_FLAGS}\" )\nmessage( STATUS \">> CMAKE_EXE_LINKER_FLAGS: ${CMAKE_EXE_LINKER_FLAGS}\" )\n####################################################################\n\n\n\n####################################################################\nif( SPIRIT_BUILD_TEST )\n    enable_testing()\nendif()\n###\nif( SPIRIT_USE_CUDA )\n    enable_language( CUDA )\nendif()\n###\nif( SPIRIT_USE_OPENMP )\n    include( FindOpenMP )\n    if( OPENMP_FOUND )\n        set( CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${OpenMP_C_FLAGS}\" )\n        set( CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}\" )\n        set( CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${OpenMP_EXE_LINKER_FLAGS}\" )\n    endif()\nendif()\n###\nif( SPIRIT_SKIP_HTST )\n    add_definitions( \"-DSPIRIT_SKIP_HTST\" )\n    message( STATUS \">> Skipping compilation of HTST!\" )\nendif()\n###\nif( SPIRIT_BUILD_FOR_JS )\n    set( SPIRIT_BUILD_FOR_CXX OFF )\n    set( SPIRIT_USE_CUDA      OFF )\n    set( SPIRIT_USE_OPENMP    OFF )\n    set( SPIRIT_UI_CXX_USE_QT OFF )\n    set( SPIRIT_UI_USE_IMGUI  ON  )\n\n    if( ${CMAKE_SYSTEM_NAME} MATCHES \"Emscripten\" )\n        message( WARNING \"You set SPIRIT_BUILD_FOR_JS to ON but your are not using emscripten. That might not work\" )\n    endif()\nendif()\n### Need to use pthread if emscripten is used\nif( SPIRIT_BUILD_FOR_JS AND SPIRIT_UI_USE_IMGUI )\n    set( CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pthread -s USE_PTHREADS=1 -s USE_PTHREADS=1 -s WASM=1\" )\nendif()\n####################################################################\n\n\n\n######### Add subdirectory projects ################################\nadd_subdirectory( thirdparty/qhull )\nset( qhull_INCLUDE_DIRS\n    ${CMAKE_CURRENT_LIST_DIR}/thirdparty/qhull/src\n    ${CMAKE_CURRENT_LIST_DIR}/thirdparty/qhull/src/libqhullcpp )\nif( CMAKE_BUILD_TYPE MATCHES \"[dD]ebug\" )\n    set( qhull_LIBS qhullcpp_d qhullstatic_rd)\nelse()\n    set( qhull_LIBS qhullcpp qhullstatic_r )\nendif()\n### Spirit library is built in any case\nadd_subdirectory( core )\n### Web UI\nif( SPIRIT_BUILD_FOR_JS )\n    if( SPIRIT_UI_USE_IMGUI )\n        add_definitions( -DSPIRIT_UI_USE_IMGUI )\n    endif()\n    add_subdirectory( VFRendering )\n    add_subdirectory( ui-cpp )\n    add_subdirectory( ui-web )\n### CXX UI\nelseif( SPIRIT_BUILD_FOR_CXX )\n    if( SPIRIT_UI_CXX_USE_QT )\n        add_definitions( -DSPIRIT_UI_CXX_USE_QT )\n    elseif( SPIRIT_UI_USE_IMGUI )\n        add_definitions( -DSPIRIT_UI_USE_IMGUI )\n    endif()\n    add_subdirectory( VFRendering )\n    add_subdirectory( ui-cpp )\nendif()\n####################################################################\n\n\n\n################ Install ###########################################\ninstall( DIRECTORY ${CMAKE_CURRENT_LIST_DIR}/docs/\n         DESTINATION docs/Spirit/\n         COMPONENT spirit_root_files )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/README.md\n               ${CMAKE_CURRENT_LIST_DIR}/VERSION.txt\n         DESTINATION ./\n         COMPONENT spirit_root_files )\n\nif( SPIRIT_BUILD_FOR_CXX )\n    install( DIRECTORY input DESTINATION bin COMPONENT spirit_root_files )\nendif()\n\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/LICENSE.txt DESTINATION ./ COMPONENT spirit_licenses )\n\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/thirdparty/qhull/COPYING.txt DESTINATION licenses RENAME qhull.txt COMPONENT spirit_licenses )\n\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/cub/LICENSE.TXT   DESTINATION licenses RENAME cub.txt       COMPONENT spirit_licenses )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/Eigen/COPYING.BSD DESTINATION licenses RENAME eigen.txt     COMPONENT spirit_licenses )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/fmt/LICENSE.rst   DESTINATION licenses RENAME fmt.rst       COMPONENT spirit_licenses )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/kiss_fft/COPYING  DESTINATION licenses RENAME kiss_fft.txt  COMPONENT spirit_licenses )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/ovf/README.md     DESTINATION licenses RENAME ovf.md        COMPONENT spirit_licenses )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/spectra/LICENSE   DESTINATION licenses RENAME spectra.txt   COMPONENT spirit_licenses )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/termcolor/LICENSE DESTINATION licenses RENAME termcolor.txt COMPONENT spirit_licenses )\n\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/thirdparty/Lyra/LICENSE.txt DESTINATION licenses RENAME lyra.txt COMPONENT spirit_licenses )\n\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/filesystem/LICENSE       DESTINATION licenses RENAME filesystem.txt       COMPONENT spirit_licenses )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/glad/LICENSE             DESTINATION licenses RENAME glad.md              COMPONENT spirit_licenses )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/glfw/LICENSE.md          DESTINATION licenses RENAME glfw.md              COMPONENT spirit_licenses )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/imgui/LICENSE.txt        DESTINATION licenses RENAME imgui.txt            COMPONENT spirit_licenses )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/implot/LICENSE           DESTINATION licenses RENAME implot.txt           COMPONENT spirit_licenses )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/json/LICENSE.MIT         DESTINATION licenses RENAME json.txt             COMPONENT spirit_licenses )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/nativefiledialog/LICENSE DESTINATION licenses RENAME nativefiledialog.txt COMPONENT spirit_licenses )\ninstall( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/stb/LICENSE              DESTINATION licenses RENAME stb.txt              COMPONENT spirit_licenses )\n####################################################################\n\n\n\n######### Write VERSION.txt ########################################\nfile( WRITE \"${CMAKE_SOURCE_DIR}/VERSION.txt\" \"${SPIRIT_META_NAME_VERSION}\" )\n####################################################################\ncd build\nmake install $1\n[tool.black]\nline-length = 88\n\n[tool.pylint.messages_control]\ndisable = [\n    \"duplicate-code\",\n    \"line-too-long\",\n    \"too-few-public-methods\",\n    \"too-many-ancestors\",\n    \"too-many-arguments\",\n    \"too-many-branches\",\n    \"too-many-instance-attributes\",\n    \"too-many-lines\",\n    \"too-many-locals\",\n    \"too-many-return-statements\",\n    \"too-many-statements\",\n    \"no-member\"\n]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/stable-baselines3",
            "repo_link": "https://github.com/DLR-RM/stable-baselines3",
            "content": {
                "codemeta": "",
                "readme": "<!-- [![pipeline status](https://gitlab.com/araffin/stable-baselines3/badges/master/pipeline.svg)](https://gitlab.com/araffin/stable-baselines3/-/commits/master) -->\n[![CI](https://github.com/DLR-RM/stable-baselines3/workflows/CI/badge.svg)](https://github.com/DLR-RM/stable-baselines3/actions/workflows/ci.yml)\n[![Documentation Status](https://readthedocs.org/projects/stable-baselines/badge/?version=master)](https://stable-baselines3.readthedocs.io/en/master/?badge=master) [![coverage report](https://gitlab.com/araffin/stable-baselines3/badges/master/coverage.svg)](https://github.com/DLR-RM/stable-baselines3/actions/workflows/ci.yml)\n[![codestyle](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n\n# Stable Baselines3\n\n<img src=\"docs/\\_static/img/logo.png\" align=\"right\" width=\"40%\"/>\n\nStable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of [Stable Baselines](https://github.com/hill-a/stable-baselines).\n\nYou can read a detailed presentation of Stable Baselines3 in the [v1.0 blog post](https://araffin.github.io/post/sb3/) or our [JMLR paper](https://jmlr.org/papers/volume22/20-1364/20-1364.pdf).\n\n\nThese algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\n\n**Note: Despite its simplicity of use, Stable Baselines3 (SB3) assumes you have some knowledge about Reinforcement Learning (RL).** You should not utilize this library without some practice. To that extent, we provide good resources in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/rl.html) to get started with RL.\n\n## Main Features\n\n**The performance of each algorithm was tested** (see *Results* section in their respective page),\nyou can take a look at the issues [#48](https://github.com/DLR-RM/stable-baselines3/issues/48) and [#49](https://github.com/DLR-RM/stable-baselines3/issues/49) for more details.\n\nWe also provide detailed logs and reports on the [OpenRL Benchmark](https://wandb.ai/openrlbenchmark/sb3) platform.\n\n\n| **Features**                | **Stable-Baselines3** |\n| --------------------------- | ----------------------|\n| State of the art RL methods | :heavy_check_mark: |\n| Documentation               | :heavy_check_mark: |\n| Custom environments         | :heavy_check_mark: |\n| Custom policies             | :heavy_check_mark: |\n| Common interface            | :heavy_check_mark: |\n| `Dict` observation space support  | :heavy_check_mark: |\n| Ipython / Notebook friendly | :heavy_check_mark: |\n| Tensorboard support         | :heavy_check_mark: |\n| PEP8 code style             | :heavy_check_mark: |\n| Custom callback             | :heavy_check_mark: |\n| High code coverage          | :heavy_check_mark: |\n| Type hints                  | :heavy_check_mark: |\n\n\n### Planned features\n\nSince most of the features from the [original roadmap](https://github.com/DLR-RM/stable-baselines3/issues/1) have been implemented, there are no major changes planned for SB3, it is now *stable*.\nIf you want to contribute, you can search in the issues for the ones where [help is welcomed](https://github.com/DLR-RM/stable-baselines3/labels/help%20wanted) and the other [proposed enhancements](https://github.com/DLR-RM/stable-baselines3/labels/enhancement).\n\nWhile SB3 development is now focused on bug fixes and maintenance (doc update, user experience, ...), there is more active development going on in the associated repositories:\n- newer algorithms are regularly added to the [SB3 Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib) repository\n- faster variants are developed in the [SBX (SB3 + Jax)](https://github.com/araffin/sbx) repository\n- the training framework for SB3, the RL Zoo, has an active [roadmap](https://github.com/DLR-RM/rl-baselines3-zoo/issues/299)\n\n## Migration guide: from Stable-Baselines (SB2) to Stable-Baselines3 (SB3)\n\nA migration guide from SB2 to SB3 can be found in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html).\n\n## Documentation\n\nDocumentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)\n\n## Integrations\n\nStable-Baselines3 has some integration with other libraries/services like Weights & Biases for experiment tracking or Hugging Face for storing/sharing trained models. You can find out more in the [dedicated section](https://stable-baselines3.readthedocs.io/en/master/guide/integrations.html) of the documentation.\n\n\n## RL Baselines3 Zoo: A Training Framework for Stable Baselines3 Reinforcement Learning Agents\n\n[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL).\n\nIt provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n\nIn addition, it includes a collection of tuned hyperparameters for common environments and RL algorithms, and agents trained with those settings.\n\nGoals of this repository:\n\n1. Provide a simple interface to train and enjoy RL agents\n2. Benchmark the different Reinforcement Learning algorithms\n3. Provide tuned hyperparameters for each environment and RL algorithm\n4. Have fun with the trained agents!\n\nGithub repo: https://github.com/DLR-RM/rl-baselines3-zoo\n\nDocumentation: https://rl-baselines3-zoo.readthedocs.io/en/master/\n\n## SB3-Contrib: Experimental RL Features\n\nWe implement experimental features in a separate contrib repository: [SB3-Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib)\n\nThis allows SB3 to maintain a stable and compact core, while still providing the latest features, like Recurrent PPO (PPO LSTM), CrossQ, Truncated Quantile Critics (TQC), Quantile Regression DQN (QR-DQN) or PPO with invalid action masking (Maskable PPO).\n\nDocumentation is available online: [https://sb3-contrib.readthedocs.io/](https://sb3-contrib.readthedocs.io/)\n\n## Stable-Baselines Jax (SBX)\n\n[Stable Baselines Jax (SBX)](https://github.com/araffin/sbx) is a proof of concept version of Stable-Baselines3 in Jax, with recent algorithms like DroQ or CrossQ.\n\nIt provides a minimal number of features compared to SB3 but can be much faster (up to 20x times!): https://twitter.com/araffin2/status/1590714558628253698\n\n\n## Installation\n\n**Note:** Stable-Baselines3 supports PyTorch >= 2.3\n\n### Prerequisites\nStable Baselines3 requires Python 3.9+.\n\n#### Windows\n\nTo install stable-baselines on Windows, please look at the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/install.html#prerequisites).\n\n\n### Install using pip\nInstall the Stable Baselines3 package:\n```sh\npip install 'stable-baselines3[extra]'\n```\n\nThis includes an optional dependencies like Tensorboard, OpenCV or `ale-py` to train on atari games. If you do not need those, you can use:\n```sh\npip install stable-baselines3\n```\n\nPlease read the [documentation](https://stable-baselines3.readthedocs.io/) for more details and alternatives (from source, using docker).\n\n\n## Example\n\nMost of the code in the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms.\n\nHere is a quick example of how to train and run PPO on a cartpole environment:\n```python\nimport gymnasium as gym\n\nfrom stable_baselines3 import PPO\n\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10_000)\n\nvec_env = model.get_env()\nobs = vec_env.reset()\nfor i in range(1000):\n    action, _states = model.predict(obs, deterministic=True)\n    obs, reward, done, info = vec_env.step(action)\n    vec_env.render()\n    # VecEnv resets automatically\n    # if done:\n    #   obs = env.reset()\n\nenv.close()\n```\n\nOr just train a model with a one liner if [the environment is registered in Gymnasium](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#registering-envs) and if [the policy is registered](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html):\n\n```python\nfrom stable_baselines3 import PPO\n\nmodel = PPO(\"MlpPolicy\", \"CartPole-v1\").learn(10_000)\n```\n\nPlease read the [documentation](https://stable-baselines3.readthedocs.io/) for more examples.\n\n\n## Try it online with Colab Notebooks !\n\nAll the following examples can be executed online using Google Colab notebooks:\n\n- [Full Tutorial](https://github.com/araffin/rl-tutorial-jnrr19)\n- [All Notebooks](https://github.com/Stable-Baselines-Team/rl-colab-notebooks/tree/sb3)\n- [Getting Started](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/stable_baselines_getting_started.ipynb)\n- [Training, Saving, Loading](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/saving_loading_dqn.ipynb)\n- [Multiprocessing](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/multiprocessing_rl.ipynb)\n- [Monitor Training and Plotting](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/monitor_training.ipynb)\n- [Atari Games](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/atari_games.ipynb)\n- [RL Baselines Zoo](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/rl-baselines-zoo.ipynb)\n- [PyBullet](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/pybullet.ipynb)\n\n\n## Implemented Algorithms\n\n| **Name**         | **Recurrent**      | `Box`          | `Discrete`     | `MultiDiscrete` | `MultiBinary`  | **Multi Processing**              |\n| ------------------- | ------------------ | ------------------ | ------------------ | ------------------- | ------------------ | --------------------------------- |\n| ARS<sup>[1](#f1)</sup>   | :x: | :heavy_check_mark: | :heavy_check_mark: | :x: | :x: | :heavy_check_mark: |\n| A2C   | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n| CrossQ<sup>[1](#f1)</sup>   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |\n| DDPG  | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |\n| DQN   | :x: | :x: | :heavy_check_mark: | :x:                 | :x:                | :heavy_check_mark: |\n| HER   | :x: | :heavy_check_mark: | :heavy_check_mark: | :x: | :x: | :heavy_check_mark: |\n| PPO   | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: |\n| QR-DQN<sup>[1](#f1)</sup>  | :x: | :x: | :heavy_check_mark: | :x:                 | :x:                | :heavy_check_mark: |\n| RecurrentPPO<sup>[1](#f1)</sup>   | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: |\n| SAC   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |\n| TD3   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |\n| TQC<sup>[1](#f1)</sup>   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x: | :heavy_check_mark: |\n| TRPO<sup>[1](#f1)</sup>  | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: |\n| Maskable PPO<sup>[1](#f1)</sup>   | :x: | :x: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark:  |\n\n<b id=\"f1\">1</b>: Implemented in [SB3 Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib) GitHub repository.\n\nActions `gymnasium.spaces`:\n * `Box`: A N-dimensional box that contains every point in the action space.\n * `Discrete`: A list of possible actions, where each timestep only one of the actions can be used.\n * `MultiDiscrete`: A list of possible actions, where each timestep only one action of each discrete set can be used.\n * `MultiBinary`: A list of possible actions, where each timestep any of the actions can be used in any combination.\n\n\n\n## Testing the installation\n### Install dependencies\n```sh\npip install -e .[docs,tests,extra]\n```\n### Run tests\nAll unit tests in stable baselines3 can be run using `pytest` runner:\n```sh\nmake pytest\n```\nTo run a single test file:\n```sh\npython3 -m pytest -v tests/test_env_checker.py\n```\nTo run a single test:\n```sh\npython3 -m pytest -v -k 'test_check_env_dict_action'\n```\n\nYou can also do a static type check using `mypy`:\n```sh\npip install mypy\nmake type\n```\n\nCodestyle check with `ruff`:\n```sh\npip install ruff\nmake lint\n```\n\n## Projects Using Stable-Baselines3\n\nWe try to maintain a list of projects using stable-baselines3 in the [documentation](https://stable-baselines3.readthedocs.io/en/master/misc/projects.html),\nplease tell us if you want your project to appear on this page ;)\n\n## Citing the Project\n\nTo cite this repository in publications:\n\n```bibtex\n@article{stable-baselines3,\n  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},\n  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},\n  journal = {Journal of Machine Learning Research},\n  year    = {2021},\n  volume  = {22},\n  number  = {268},\n  pages   = {1-8},\n  url     = {http://jmlr.org/papers/v22/20-1364.html}\n}\n```\n\nNote: If you need to refer to a specific version of SB3, you can also use the [Zenodo DOI](https://doi.org/10.5281/zenodo.8123988).\n\n## Maintainers\n\nStable-Baselines3 is currently maintained by [Ashley Hill](https://github.com/hill-a) (aka @hill-a), [Antonin Raffin](https://araffin.github.io/) (aka [@araffin](https://github.com/araffin)), [Maximilian Ernestus](https://github.com/ernestum) (aka @ernestum), [Adam Gleave](https://github.com/adamgleave) (@AdamGleave), [Anssi Kanervisto](https://github.com/Miffyli) (@Miffyli) and [Quentin Gallouédec](https://gallouedec.com/) (@qgallouedec).\n\n**Important Note: We do not provide technical support, or consulting** and do not answer personal questions via email.\nPlease post your question on the [RL Discord](https://discord.com/invite/xhfNqQv), [Reddit](https://www.reddit.com/r/reinforcementlearning/), or [Stack Overflow](https://stackoverflow.com/) in that case.\n\n\n## How To Contribute\n\nTo any interested in making the baselines better, there is still some documentation that needs to be done.\nIf you want to contribute, please read [**CONTRIBUTING.md**](./CONTRIBUTING.md) guide first.\n\n## Acknowledgments\n\nThe initial work to develop Stable Baselines3 was partially funded by the project *Reduced Complexity Models* from the *Helmholtz-Gemeinschaft Deutscher Forschungszentren*, and by the EU's Horizon 2020 Research and Innovation Programme under grant number 951992 ([VeriDream](https://www.veridream.eu/)).\n\nThe original version, Stable Baselines, was created in the [robotics lab U2IS](http://u2is.ensta-paristech.fr/index.php?lang=en) ([INRIA Flowers](https://flowers.inria.fr/) team) at [ENSTA ParisTech](http://www.ensta-paristech.fr/en).\n\n\nLogo credits: [L.M. Tenkes](https://www.instagram.com/lucillehue/)\n\n",
                "dependencies": "[tool.ruff]\n# Same as Black.\nline-length = 127\n# Assume Python 3.9\ntarget-version = \"py39\"\n\n[tool.ruff.lint]\n# See https://beta.ruff.rs/docs/rules/\nselect = [\"E\", \"F\", \"B\", \"UP\", \"C90\", \"RUF\"]\n# B028: Ignore explicit stacklevel`\n# RUF013: Too many false positives (implicit optional)\nignore = [\"B028\", \"RUF013\"]\n\n[tool.ruff.lint.per-file-ignores]\n# Default implementation in abstract methods\n\"./stable_baselines3/common/callbacks.py\" = [\"B027\"]\n\"./stable_baselines3/common/noise.py\" = [\"B027\"]\n# ClassVar, implicit optional check not needed for tests\n\"./tests/*.py\" = [\"RUF012\", \"RUF013\"]\n\n[tool.ruff.lint.mccabe]\n# Unlike Flake8, default to a complexity level of 10.\nmax-complexity = 15\n\n[tool.black]\nline-length = 127\n\n[tool.mypy]\nignore_missing_imports = true\nfollow_imports = \"silent\"\nshow_error_codes = true\nexclude = \"\"\"(?x)(\n    tests/test_logger.py$\n    | tests/test_train_eval_mode.py$\n  )\"\"\"\n\n[tool.pytest.ini_options]\n# Deterministic ordering for tests; useful for pytest-xdist.\nenv = [\"PYTHONHASHSEED=0\"]\n\nfilterwarnings = [\n    # Tensorboard warnings\n    \"ignore::DeprecationWarning:tensorboard\",\n    # Gymnasium warnings\n    \"ignore::UserWarning:gymnasium\",\n    # tqdm warning about rich being experimental\n    \"ignore:rich is experimental\",\n]\nmarkers = [\n    \"expensive: marks tests as expensive (deselect with '-m \\\"not expensive\\\"')\",\n]\n\n[tool.coverage.run]\ndisable_warnings = [\"couldnt-parse\"]\nbranch = false\nomit = [\n    \"tests/*\",\n    \"setup.py\",\n    # Require graphical interface\n    \"stable_baselines3/common/results_plotter.py\",\n    # Require ffmpeg\n    \"stable_baselines3/common/vec_env/vec_video_recorder.py\",\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"raise NotImplementedError()\",\n    \"if typing.TYPE_CHECKING:\",\n]\n\nimport os\n\nfrom setuptools import find_packages, setup\n\nwith open(os.path.join(\"stable_baselines3\", \"version.txt\")) as file_handler:\n    __version__ = file_handler.read().strip()\n\n\nlong_description = \"\"\"\n\n# Stable Baselines3\n\nStable Baselines3 is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of [Stable Baselines](https://github.com/hill-a/stable-baselines).\n\nThese algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\n\n\n## Links\n\nRepository:\nhttps://github.com/DLR-RM/stable-baselines3\n\nBlog post:\nhttps://araffin.github.io/post/sb3/\n\nDocumentation:\nhttps://stable-baselines3.readthedocs.io/en/master/\n\nRL Baselines3 Zoo:\nhttps://github.com/DLR-RM/rl-baselines3-zoo\n\nSB3 Contrib:\nhttps://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n\n## Quick example\n\nMost of the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms using Gym.\n\nHere is a quick example of how to train and run PPO on a cartpole environment:\n\n```python\nimport gymnasium\n\nfrom stable_baselines3 import PPO\n\nenv = gymnasium.make(\"CartPole-v1\", render_mode=\"human\")\n\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10_000)\n\nvec_env = model.get_env()\nobs = vec_env.reset()\nfor i in range(1000):\n    action, _states = model.predict(obs, deterministic=True)\n    obs, reward, done, info = vec_env.step(action)\n    vec_env.render()\n    # VecEnv resets automatically\n    # if done:\n    #   obs = vec_env.reset()\n\n```\n\nOr just train a model with a one liner if [the environment is registered in Gymnasium](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/) and if [the policy is registered](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html):\n\n```python\nfrom stable_baselines3 import PPO\n\nmodel = PPO(\"MlpPolicy\", \"CartPole-v1\").learn(10_000)\n```\n\n\"\"\"  # noqa:E501\n\n\nsetup(\n    name=\"stable_baselines3\",\n    packages=[package for package in find_packages() if package.startswith(\"stable_baselines3\")],\n    package_data={\"stable_baselines3\": [\"py.typed\", \"version.txt\"]},\n    install_requires=[\n        \"gymnasium>=0.29.1,<1.1.0\",\n        \"numpy>=1.20,<3.0\",\n        \"torch>=2.3,<3.0\",\n        # For saving models\n        \"cloudpickle\",\n        # For reading logs\n        \"pandas\",\n        # Plotting learning curves\n        \"matplotlib\",\n    ],\n    extras_require={\n        \"tests\": [\n            # Run tests and coverage\n            \"pytest\",\n            \"pytest-cov\",\n            \"pytest-env\",\n            \"pytest-xdist\",\n            # Type check\n            \"mypy\",\n            # Lint code and sort imports (flake8 and isort replacement)\n            \"ruff>=0.3.1\",\n            # Reformat\n            \"black>=24.2.0,<25\",\n        ],\n        \"docs\": [\n            \"sphinx>=5,<9\",\n            \"sphinx-autobuild\",\n            \"sphinx-rtd-theme>=1.3.0\",\n            # For spelling\n            \"sphinxcontrib.spelling\",\n            # Copy button for code snippets\n            \"sphinx_copybutton\",\n        ],\n        \"extra\": [\n            # For render\n            \"opencv-python\",\n            \"pygame\",\n            # Tensorboard support\n            \"tensorboard>=2.9.1\",\n            # Checking memory taken by replay buffer\n            \"psutil\",\n            # For progress bar callback\n            \"tqdm\",\n            \"rich\",\n            # For atari games,\n            \"ale-py>=0.9.0\",\n            \"pillow\",\n        ],\n    },\n    description=\"Pytorch version of Stable Baselines, implementations of reinforcement learning algorithms.\",\n    author=\"Antonin Raffin\",\n    url=\"https://github.com/DLR-RM/stable-baselines3\",\n    author_email=\"antonin.raffin@dlr.de\",\n    keywords=\"reinforcement-learning-algorithms reinforcement-learning machine-learning \"\n    \"gymnasium gym openai stable baselines toolbox python data-science\",\n    license=\"MIT\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    version=__version__,\n    python_requires=\">=3.9\",\n    # PyPI package information.\n    project_urls={\n        \"Code\": \"https://github.com/DLR-RM/stable-baselines3\",\n        \"Documentation\": \"https://stable-baselines3.readthedocs.io/\",\n        \"Changelog\": \"https://stable-baselines3.readthedocs.io/en/master/misc/changelog.html\",\n        \"SB3-Contrib\": \"https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\",\n        \"RL-Zoo\": \"https://github.com/DLR-RM/rl-baselines3-zoo\",\n        \"SBX\": \"https://github.com/araffin/sbx\",\n    },\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n    ],\n)\n\n# python setup.py sdist\n# python setup.py bdist_wheel\n# twine upload --repository-url https://test.pypi.org/legacy/ dist/*\n# twine upload dist/*\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/stmlab",
            "repo_link": "https://gitlab.com/dlr-sy/stmlab",
            "content": {
                "codemeta": "",
                "readme": "[![PyPi](https://img.shields.io/pypi/v/stmlab?label=PyPi)](https://pypi.org/project/stmlab/)\r\n[![doi](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.13844636-red.svg)](https://zenodo.org/records/13844636)\r\n\r\n# STMLab\r\n> This Python package provides an independent standard runtime environment for software projects developed by the [Department of Structural Mechanics](https://www.dlr.de/en/sy/about-us/departments/structural-mechanics) at the [Institute of Lightweight Structures](https://www.dlr.de/en/sy) of the [German Aerospace Center](https://www.dlr.de/en) It uses the [Jupyter](https://jupyter.org/) project as its graphical user interface. Two types of installation procedures are available. A community version can be installed and executed using pip. An enterprise version with yet unpublished software projects is available as an offline installer on request.\r\n\r\n## Downloading\r\nUse GIT to get the latest code base. From the command line, use\r\n```\r\ngit clone https://gitlab.dlr.de/dlr-sy/stmlab stmlab\r\n```\r\nIf you check out the repository for the first time, you have to initialize all submodule dependencies first. Execute the following from within the repository. \r\n```\r\ngit submodule update --init --recursive\r\n```\r\nTo update all refererenced submodules to the latest production level, use\r\n```\r\ngit submodule foreach --recursive 'git pull origin $(git config -f $toplevel/.gitmodules submodule.$name.branch || echo master)'\r\n```\r\n## Installation\r\nSTMLab can be installed from source using [poetry](https://python-poetry.org). If you don't have [poetry](https://python-poetry.org) installed, run\r\n```\r\npip install poetry --pre --upgrade\r\n```\r\nto install the latest version of [poetry](https://python-poetry.org) within your python environment. Use\r\n```\r\npoetry update\r\n```\r\nto update all dependencies in the lock file or directly execute\r\n```\r\npoetry install\r\n```\r\nto install all dependencies from the lock file. Last, you should be able to import STMLab as a python package.\r\n```python\r\nimport stmlab\r\n```\r\n## Contact\r\n* [Marc Garbade](mailto:marc.garbade@dlr.de)\n",
                "dependencies": "# TOML file for STMLab.\n#  \n# @note: TOML file            \n# Created on 20.03.2024    \n# \n# @version:  1.0    \n# ----------------------------------------------------------------------------------------------\n# @requires:\n#        - \n# \n# @change: \n#        -    \n#    \n# @author: garb_ma                                                     [DLR-SY,STM Braunschweig]\n# ----------------------------------------------------------------------------------------------\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.poetry]\nname = \"stmlab\"\nversion = \"0.10.3rc\"\ndescription = \"JupyterLab environment for software projects developed by the Department of Structural Mechanics at the German Aerospace Center\"\nauthors = [\"Garbade, Marc <marc.garbade@dlr.de>\"]\nlicense = \"MIT\"\nreadme = \"README.md\"\npackages = [{include=\"stmlab\", from=\"src\"}]\nrepository = \"https://gitlab.com/dlr-sy/stmlab\"\ndocumentation = \"https://gitlab.com/dlr-sy/stmlab/-/blob/main/README.md\"\nkeywords = [\"stmlab\",\"jupyter\",\"manager\"]\nclassifiers = [\n    \"Development Status :: 3 - Alpha\",\n    \"Topic :: Scientific/Engineering\",\n    \"Programming Language :: Python :: 2\",\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\"\n]\n\n[tool.poetry.urls]\nChangelog = \"https://gitlab.com/dlr-sy/stmlab/-/blob/master/CHANGELOG.md\"\n\n[[tool.poetry.source]]\nname = \"PyPI\"\npriority = \"primary\"\n\n[[tool.poetry.source]]\nname = \"dlr-sy\"\nurl = \"https://gitlab.dlr.de/api/v4/groups/541/-/packages/pypi/simple\"\npriority = \"supplemental\"\n\n[tool.poetry.dependencies]\npython = \"~2.7 || ^3.5\"\ntyper = [{version = \">=0.12\", python = \"^3.7\"}]\npyx-core = [{version = \">=1.18\", python = \"~2.7 || ^3.5,<3.7\"},\n            {version = \"*\", python = \"^3.7\"}]\npyc-core = [{version = \">=1.10\", python = \"~2.7 || ^3.5,<3.7\", optional = true},\n            {version = \"*\", python = \"^3.7\", optional = true}]\n\n# All optional dependencies\nbeos = [{version = \">=1.3\", python = \"~2.7 || ^3.5\", optional = true}]\nboxbeam = [{version = \"1.3\", python = \"~2.7 || ^3.5\", optional = true}]\ndamapper = [{version = \">=1.7\", python = \"^3.6\", optional = true}]\ndisplam = [{version = \">=1.3\", python = \"~2.7 || ^3.5\", optional = true}]\nmcodac = [{version = \">=1.2\", python = \"~2.7 || ^3.5\"}]\nvampire = [{version = \">=0.2.5\", python = \"^3.7\", source = \"dlr-sy\", optional = true}]\n\npyx-client = [{version = \">=1.18\", python = \"^3.7\", optional = true}]\npyc-client = [{version = \">=1.10\", python = \"^3.7\", optional = true}]\npyx-webservice = [{version = \">=1.18\", python = \"^3.7\", optional = true}]\npyc-webservice = [{version = \">=1.10\", python = \"^3.7\", optional = true}]\n\n# Additional dependencies for development\n[tool.poetry.group.dev.dependencies]\npyx-poetry = [{version = \">=1.18\", python=\"^3.7\"}]\npyx-core = [{git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake.git\", python = \"^3.7\"}]\npyc-core = [{git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyCODAC.git\", python = \"^3.7\"}]\n\n[tool.poetry.group.lock.dependencies]\nsetuptools = [{version = \"^39.0\", python = \"~2.7\"},\n              {version = \"^49.0\", python = \"~3.5\"},\n              {version = \"^58.0\", python = \"~3.6\"},\n              {version = \"^64.0\", python = \"~3.7\"},\n              {version = \"^70,<74\", python = \"^3.8\"}]\nsympy = [{version = \"^1.10\", python = \"~3.7\"},\n         {version = \"^1.11\", python = \"^3.8\"}]\ntyping-extensions = [{version = \"^4.7\", python = \"~3.7\"},\n                     {version = \">=4.8\", python = \"^3.8\"}]\nvtk = [{version = \"^9,<9.4\", python = \"^3.5,<3.8\"},\n       {version = \"*\", python = \"^3.8\"}]\n\n[tool.poetry.extras]\nall = [\"beos\",\"boxbeam\",\"displam\",\"pyc-core\",\"pyc-webservice\",\"pyx-webservice\"]\nserver = [\"pyc-webservice\",\"pyx-webservice\"]\nclient = [\"pyc-client\",\"pyx-client\"]\ndisplam = [\"displam\"]\nbeos = [\"beos\"]\nboxbeam = [\"boxbeam\"]\ndamapper = [\"damapper\"]\nmcodac = [\"mcodac\"]\nvampire = [\"vampire\"]\ncodac = [\"damapper\",\"pyc-core\"]\n\n[tool.poetry.scripts]\nstmlab = \"stmlab.command:main\"\n\n[tool.pyxmake.pyinstaller]\nname = \"stmlab\"\nicon = \"src/stmlab/assets/favicon.ico\"\nsource = \"src/stmlab\"\nfile = \"__exe__.py\"\noutput = \"bin\"\nmode = \"onedir\"\nverbosity = \"2\"\n\n[tool.pyxmake.sphinx]\nname = \"'Structural Mechanics Lab'\"\nicon = \"doc/stmlab/pics/stm_lab_logo_bubbles.png\"\nsource = \"doc/stmlab/source\"\noutput = \"doc/stmlab\"\nfile = \"stmlab\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/stream2segment",
            "repo_link": "https://github.com/rizac/stream2segment",
            "content": {
                "codemeta": "",
                "readme": "# <img align=\"left\" height=\"30\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/medien_kommunikation/Infothek/Mediathek/Bilder/GFZ/GFZ_Logo/GFZ-Logo_eng_RGB.svg\"> Stream2segment <img align=\"right\" height=\"50\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/GFZ_Wortmarke_SVG_klein_en_edit.svg\">\n\n|Jump to: | [Usage](#usage) | [Installation](#installation) | [Development and Maintenance](#development-and-maintenance) | [Citation](#citation) |\n| - | - | - | - | - |\n\nA Python library and command line application to download, process and visualize \nevent-based seismic waveform  segments, specifically designed to manage big \nvolumes of data.\n\nThe key aspects with respect to widely-used similar applications is the use of\na Relational database management system (RDBMS) to store downloaded data and \nmetadata. The main advantages of this approach are: \n\n* **Storage efficiency**: no huge amount of files, no complex, virtually \n  unusable directory structures. Moreover, a database prevents data and metadata \n  inconsistency by design, and allows more easily to track what has already \n  been downloaded in order to customize and improve further downloads\n\n* **Simple Python objects representing stored data and relationships**, easy \n  to work with in any kind of custom code accessing the database. For instance, a \n  segment is represented by a `Segment` object with its data, metadata and related \n  objects easily accessible through its attributes, e.g., `segment.stream()`, \n  `segment.maxgap_numsamples`, `segment.event.magnitude`, \n  `segment.station.network`, `segment.channel.orientation_code` and so on.\n  \n* **A powerful segments selection** made even easier by means of a simplified\n  syntax: map any attribute described above to a selection expression\n  (e.g. `segment.event.magnitude: \"[4, 5)\"`) and with few lines you can compose \n  complex database queries such as e.g., *\"get all downloaded segments within a \n  given magnitude range, with well-formed data and no gaps, \n  from broadband channels only and a given specific network\"*\n\n\n\n## Usage\n\nFor full details, please consult the [wiki page](https://github.com/rizac/stream2segment/wiki)\n\nStream2segment is a Python library and command line application available \nafter installation via the command `s2s` on the terminal. By typing `s2s --help` you\nwill see all available subcommands for downloading \nand managing data, launching Python processing functions, creating class labels for segments \nannotation, or producing graphical output, as shown below:\n\n![S2s GUI](https://raw.githubusercontent.com/wiki/rizac/stream2segment/images/screenshot_gui.png)\n\n<!--\n<table>\n\t<tr>\n\t\t<td align=\"center\"><img width=\"90%\" src=\"https://geofon.gfz-potsdam.de/software/stream2segment/processgui.png\"/></td>\n\t\t<td align=\"center\"><img width=\"90%\" src=\"https://geofon.gfz-potsdam.de/software/stream2segment/s2s_dinfogui.png\"/></td>\n\t</tr>\n\t<tr>\n\t\t<td>The <code>s2s show ...</code> command opens a GUI in the browser where downloaded data and customizable plots are shown</td>\n\t\t<td> The <code>s2s dl dstats ...</code> command opens an HTML page in the browser where download statistics can be shown</td>\n\t</tr>\n</table>\n\n\n<sub>Both image linked from https://geofon.gfz-potsdam.de/software/stream2segment/</sub>\n-->\n\nYou start the program via the command `init` ( \n`s2s init --help` for details) to create several fully documented\nexamples files that you can immediately start to configure and modify\n(see the **[gitHub wiki page](https://github.com/rizac/stream2segment/wiki)** for details).\nIn a nutshell: \n\n 1. **A download configuration file** in YAML syntax. Edit the file \n    (all documentation is provided in the file as block comments) and \n    start downloading by typing:\n   \n    ```console\n    s2s download -c <config_file> ...\n    ```\n   \n    > **Note** the path of the database where to store the downloaded data\n      must be input in the config file. The supported database types are SQLite \n      and Postgres: for massive downloads (as a rule of thumb: &ge; 1 million segments)\n      we suggest to use Postgres. In any case, we **strongly** suggest running the program \n      on computers with at least **16GB** of RAM.\n\n    > **Note**  massive downloads are time-consuming operations where it is likely to miss\n      some data due to any kind of temporary connection problems. Consequently, **it is advisable\n      to perform the same massive download at least twice with the same configuration**  \n      (subsequent runs will be faster as data will not be re-downloaded unnecessarily)\n\n 2. **A Jupyter notebook tutorial with examples for processing downloaded data**,\n    for user who prefer this approach instead of the processing module described\n    below (online version **[here](https://github.com/rizac/stream2segment/wiki/Using-Stream2segment-in-your-Python-code)**)\n\n 3. **Two Python modules** (with relative configuration in YAML syntax):\n \n    1. `paramtable.py`: process downloaded data and produce a tabular output (CSV, HDF) by executing the \n       module as script (see code block after `if __name__ == \"__main__\"` in the module for details):\n       ```console\n       python paramtable.py ...\n       ```\n         \n    2. `gui.py`: visualize downloaded data in the user browser via the plots defined in the module (an example in the figure above):\n       ```console\n       s2s show -d download.yaml -p gui.py -c gui.yaml ...\n       ```\n       (Type `s2s show --help` for details).\n       \n    > **Note**: the associated YAML files (`paramtable.yaml`, `gui.yaml`) are not \n      mandatory but enforce the good practice of separating configuration settings (YAML)\n      and the actual Python code. This way you can experiment \n      the same code with several settings by only creating different YAML files\n\n\n## Installation\n\nThis program has been installed and tested on Ubuntu (14 and later) and macOS\n(El Capitan and later).\n\nIn case of installation problems, we suggest you to proceed in this order:\n\n 1. Look at [Installation Notes](#installation-notes) to check if the problem\n    has already been observed and a solution proposed\n 2. Google for the solution (as always)\n 3. [Ask for help](https://github.com/rizac/stream2segment/issues)\n\n\n### 1 Requirements\n\nIn this section we assume that you already have Python (**3.5 or later**) \nand the required database software. The latter should not be needed if you use\n[SQLite](https://docs.python.org/3/library/sqlite3.html) or if the\ndatabase is already installed remotely, so basically you are concerned only if you\nneed to download data locally (on your computer) on a Postgres database.\n\n\n#### 1.1 macOS\n\nOn macOS (El Capitan and later) all required software is generally already\npreinstalled. We suggest you to go to the next step and look at the\n[Installation Notes](#installation-notes) in case of problems\n(to install software on macOS, we recommend to use [brew](https://brew.sh/)).\n\n<details>\n<summary>Details</summary>\n\nIn few cases, on some computers we needed to run one or more of the following\ncommands (it's up to you to run them now or later, only those really needed):\n\n```\nxcode-select --install\nbrew install openssl\nbrew install c-blosc\nbrew install git\n```\n\n</details>\n\n#### 1.2 Ubuntu\n\nUbuntu does not generally have all required packages pre-installed. The bare minimum\nof the necessary packages can be installed with the `apt-get` command:\n\n```\nsudo apt-get install git python3-pip python3-dev  # python 3\n```\n\n<details>\n<summary>Details</summary>\n\nIn few cases, on some computers we needed to run one or more of the following\ncommands (it's up to you to run them now or later, only those really needed):\n\nUpgrade `gcc` first:\n\t\n```\nsudo apt-get update\nsudo apt-get upgrade gcc\n```\n\nThen:\n\n```\nsudo apt-get update\nsudo apt-get install libpng-dev libfreetype6-dev \\\n\tbuild-essential gfortran libatlas-base-dev libxml2-dev libxslt-dev python-tk\n```\n\n\n</details>\n\n### 2 Clone repository\n\nGit-clone (basically: download) this repository to a specific folder of your choice:\n```\ngit clone https://github.com/rizac/stream2segment.git ./stream2segment\n```\nand move into the repository root:\n```\ncd stream2segment\n```\n\n### 3 Install and activate Python virtualenv\n\nWe strongly recommend to use Python virtual environment to avoid conflicts\nwith already installed packages on your operating system (if you already\nhave a virtual environment, just activate it and go to the next section).\n\nConda users (e.g. Anaconda, Miniconda) can skip this section and check the [Conda documentation](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) instead.\n\nMake virtual environment in a \"stream2segment/env\" directory (env is a convention,\nbut it's ignored by `git commit` so better keeping it. You can also use \".env\"\nwhich makes it usually hidden in Ubuntu. Also on Ubuntu, you might need to install\n`venv` first via `sudo apt-get install python3-venv`)\n```\npython3 -m venv ./env\n```\n\nTo activate your virtual environment, type:\n\n ```\n source env/bin/activate\n ```\nor `source env/bin/activate.csh` (depending on your shell)\n\n> <sub>Activation needs to be done __each time__ we will run the program.</sub>\n> <sub>To check you are in the right env, type: `which pip` and you should see it's\n  pointing inside the env folder</sub>\n\n\n### 4 Install Stream2segment\n\n**Important reminders before installing**: \n  - From now on you are supposed to be in the stream2segment directory,\n     (where you cloned the repository) with your Python virtualenv activated\n  - In case of errors, check the [Installation notes below](#installation-Notes)\n\nInstall the required packages with the tested versions listed in `requirements.txt` \n(if you are working on an existing environment with stuff \nalready installed in it, **please read the [first installation note](#installation-notes) below** \nbefore proceeding):\n```console\npip install --upgrade pip setuptools wheel && pip install -r ./requirements.txt\n```\n > <sub>type `requirements.dev.txt` instead of `requirements.txt` if you want to install also test packages, e.g., you want to contribute to the code and/or run tests</sub>\n\nInstall this package:\n```console\npip install -e .\n```\n\n(optional) install jupyter notebook or jupyterlab \n(see [Jupyter page for details](https://jupyter.org/install)), e.g.:\n```console\npip install jupyterlab\n```\n\nThe program is now installed. To double-check the program functionalities,\nyou can run tests (see below) and report the problem in case of failure.\nIn any case, before reporting a problem remember to check first the\n[Installation Notes](#installation-notes)\n\n\n### 5 Installation Notes\n\n- in case of a message like `ERROR: No matching distribution found for <package_name>`,\n  try to skip the requirements file:\n  ```console\n  pip install --upgrade pip setuptools wheel && pip install -e .\n  ```  \n  This will install packages satisfying a *minimum* required \n  version instead of the *exact* version passing tests: while less safe in general, this approach\n  lets `pip` handle the best versions to use, with more chance of\n  success in this case. **You can choose this strategy not only in case of mismatching distributions, \n  but also while working on a virtual environment with already installed packages, \n  because it has less chance of breaking existing code.**\n\n- In older ObsPy version, numpy needs to be installed first. If you see an error \n  like \"you need to install numpy first\", open \"requirements.txt\" and copy the \n  line which starts with numpy. Supposing it's `numpy==0.1.12`, then run \n  `pip install numpy==0.1.12` before re-running the `pip install ...` command \n  above\n\n- When installing the program (`pip install -e .`), `-e` is optional and \n  makes the package editable, meaning that you can edit the repository and make all \n  changes immediately available, without re-installing the package. This is useful \n  when, e.g., `git pull`-ing new versions frequently.\n  \n- (update January 2021) On macOS (version 11.1, with Python 3.8 and 3.9):\n\n  - if the installation fails with a lot of printout, and you spot a\n    \"Failed building wheel for psycopg2\", see  \n    <!--, try to execute:\n    ```\n    export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/opt/openssl/lib/ && pip ./installme-dev\n    ```\n    (you might need to change the path of openssl below). Credits \n    -->\n    [here](https://stackoverflow.com/a/61159643/3526777) and\n    [here](https://stackoverflow.com/a/39800677/3526777)\n \n  - If the error message is \"Failed building wheel for tables\",\n    then try to install c-blosc (on macOS,  `brew install c-blosc`) <!-- and re-run `installme-dev` installation command\n    (with the `export` command above, if needed) -->\n \n\n- If you see (we experienced this while running tests, thus we can guess you should see\n  it whenever accessing the program for the first time):\n  ```\n  This system supports the C.UTF-8 locale which is recommended.\n  You might be able to resolve your issue by exporting the\n  following environment variables:\n\n    export LC_ALL=C.UTF-8\n    export LANG=C.UTF-8\n  ```\n  Then edit your `~/.profile` (or `~/.bash_profile` on Mac) and put the two lines starting\n  with 'export', and execute `source ~/.profile` (`source ~/.bash_profile` on Mac) and\n  re-execute the program.  \n\n- On Ubuntu 12.10, there might be problems with libxml (`version libxml2_2.9.0' not found`). \n  Move the file or create a link in the proper folder. The problem has been solved looking\n  at http://phersung.blogspot.de/2013/06/how-to-compile-libxml2-for-lxml-python.html\n\nAll following issues should be solved by installing all dependencies as described in\nthe section [Prerequisites](#prerequisites). If you did not install them, here the solutions\nto common problems you might have and that we collected from several Ubuntu installations:\n\n- For numpy installation problems (such as `Cannot compile 'Python.h'`) , the fix \n  has been to update gcc and install python3-dev (python2.7-dev if you are using Python2.7,\n  discouraged): \n  ```\n  sudo apt-get update\n  sudo apt-get upgrade gcc\n  sudo apt-get install python3-dev\n  ```\n   For details see [here](http://stackoverflow.com/questions/18785063/install-numpy-in-python-virtualenv)\n \n - For scipy problems, `build-essential gfortran libatlas-base-dev` are required for scipy.\n   For details see [here](http://stackoverflow.com/questions/2213551/installing-scipy-with-pip/3865521#3865521)\n \n - For lxml problems, `libxml2-dev libxslt-dev` are required. For details see [here](http://lxml.de/installation.html)\n \n - For matplotlib problems (matplotlib is not used by the program but from imported libraries),\n   `libpng-dev libfreetype6-dev` are required. For details see\n   [here](http://stackoverflow.com/questions/25593512/cant-install-matplotlib-using-pip) and\n   [here]( http://stackoverflow.com/questions/28914202/pip-install-matplotlib-fails-cannot-build-package-freetype-python-setup-py-e)\n\n\n\n## Development and Maintenance\n\n### 1 Run tests\n\nStream2segment has been highly tested (current test coverage is above 90%)\non Python version >= 3.5+. Although automatic continuous integration (CI) systems are not\nin place, we do our best to regularly tests under new Python and package versions. \nRemember that tests are time-consuming (some minutes currently).\nHere some examples depending on your needs:\n\n```\npytest -xvvv -W ignore ./tests/\n```\n\n```\npytest -xvvv -W ignore --dburl postgresql://<user>:<password>@localhost/<dbname> ./tests/\n```\n\n<!--\n```\npytest -xvvv -W ignore --cov=./stream2segment --cov-report=html ./tests/\n```\n-->\n\n```\npytest -xvvv -W ignore --dburl postgresql://<user>:<password>@localhost/<dbname> --cov=./stream2segment --cov-report=html ./tests/\n```\n\nWhere the options denote:\n\n- `-x`: stop at first error\n- `-vvv`: increase verbosity,\n- `-W ignore`: do not print Python warnings issued during tests. You can omit the `-W`\n  option to turn warnings on and inspect them, but consider that a lot of redundant\n  messages will be printed: in case of test failure, it is hard to spot the relevant error\n  message. Alternatively, try `-W once` - warn once per process - and `-W module` -warn\n  once per calling module.\n- `--cov`: track code coverage, to know how much code has been executed during tests, and\n  `--cov-report`: type of report (if html, you will have to opened 'index.html' in the\n  project directory 'htmlcov')\n- `--dburl`: Additional database to use.\n  The default database is an in-memory sqlite database (e.g., no file will be created),\n  thus this option is basically for testing the program also on postgres. In the example,\n  the postgres is installed locally (`localhost`) but it does not need to.\n  *Remember that a database with name `<dbname>` must be created first in postgres, and\n  that the data in any given postgres database will be overwritten if not empty*\n\n\n> <sub>Note on coding: although PEP8 recommends 79 character length, the program used initially a 100\n  characters max line width, which is being reverted to 79 (you might see mixed\n  lengths in the modules). It seems that [among new features planned for Python 4 there is\n  an increment to 89.5 characters](https://charlesleifer.com/blog/new-features-planned-for-python-4-0/).\n  If true, we might stick to that in the future</sub>\n  \n  \n### 2 Updating dependencies\n\nIn the absence of Continuous Integration in place, from times to times, it is necessary\n  to update the dependencies, to make `pip install` more likely to work (at least for\n  some time). The procedure is:\n  ```\n\tpip install -e .\n\tpip freeze > ./requirements.tmp\n\tpip install -e \".[dev]\"\n\tpip freeze > ./requirements.dev.tmp\n  ```\n**Remember to comment the line of stream2segment\n  from all requirements** (as it should be installed as argument of pip:\n  `pip install <options> .`, and not inside the requirements file).\n\n  Run tests (see above) with warnings on: fix what might go wrong, and eventually you can\n  replace the old `requirements.txt` and `requirements.dev.txt` with the `.tmp` file\n  created. \n\n### 3 Updating wiki\n  \n  Requirements (to be done once):\n   - `jupyter` installed.\n   - The git repository `stream2segment.wiki` which you can clone from the \n     stream2segment/wiki URL on the GitHub page. The repository must\n     be cloned next to (on the same parent directory of) the\n     stream2segment repository\n     \n  The wiki is simply a git project composed of Markdown (.md) files, where\n  `Home.md` implements the landing page of the wiki on the browser, and thus\nusually hosts the table of contents with links to other markdown files `.md` \n  in the directory. Currently, two of those `.md` files are generated from the \n  notebooks `.ipynb` inside stream2segment:\n  \n  - ./resources/templates/\n    - Using-Stream2segment-in-your-Python-code.ipynb\n    - The-Segment-object.ipynb\n  \n#### 3.1 Update existing notebook\n\n1. Edit the notebook in stream2segment/resources/templates:\n  `jupyter notebook stream2segment/resources/templates`\n  Execute the whole notebook to update it, then `git push` as usual\n   \n2. Create `.md` versions of the notebook for the wiki. From the stream2segment \n   repository as `cwd`:\n   ```bash\n    F='Using-Stream2segment-in-your-Python-code';jupyter nbconvert --to markdown ./stream2segment/resources/templates/$F.ipynb --output-dir ../stream2segment.wiki \n   ```\n   (repeat for every notebook file, e.g. `The-Segment-object`. Note only the file name,\n   no file extension needed)\n   \n3. Commit and push to the `stream2segment.wiki` repo:\n   `cd ../stream2segment.wiki`, then as usual `git commit` and `git push`. One line command:\n   `(cd ../stream2segment.wiki && git commit -am 'updating wiki' && git push)`\n    \n#### 3.2 Add a new notebook\n  \nCreate the notebook (`jupyter notebook stream2segment/resources/templates`). \n**Choose a meaningful file name: use upper case when needed, type hyphens '-'\ninstead of spaces**: the file name will be used as title to show the page\nonline (replacing hyphens with spaces).\nOnce the notebook is created and executed:\n     \n- (optional) If you want to include the notebook also as example in the `s2s init` command,\n     look at `stream2segment/cli.py`  \n  \n- Make the notebook being executed during tests (see examples in `tests/misc/test_notebook.py`)\n     and run tests to check everything works.\n  \n- Make the notebook visible in the wiki by adding a reference to it\n     (the notebook URL is the file name with no extension, I guess case- insensitive). \n     A reference can be added in several places:\n     - In the file `_Sidebar.md` (in the wiki repository)\n       which will show it in the sidebar on GitHub\n     - In `Home.md`\n     - In some other notebook (see example in\n       `Using-stream2segment-in-you-Python-code.ipynb`). In this case, note that\n       you might need to update also the referencing notebook\n       (see points 2-3 [above](#to-update-one-of-those-existing-notebooks))\n\n- Create the markdown file and commit to the wiki (see points 2-3 above under\n     `To update one of those existing notebooks`)\n\n\n\n## Citation\n\n**Software:**\n> Zaccarelli, Riccardo (2018): Stream2segment: a tool to download, process and visualize event-based seismic waveform data. GFZ Data Services.  [http://doi.org/10.5880/GFZ.2.4.2019.002](http://doi.org/10.5880/GFZ.2.4.2019.002)\n\n\n**Research article:**\n> Riccardo Zaccarelli, Dino Bindi, Angelo Strollo, Javier Quinteros and Fabrice Cotton. Stream2segment: An Open‐Source Tool for Downloading, Processing, and Visualizing Massive Event‐Based Seismic Waveform Datasets. *Seismological Research Letters* (2019). [https://doi.org/10.1785/0220180314](https://doi.org/10.1785/0220180314)\n\n\n",
                "dependencies": "blinker==1.6.2\nblosc2==2.0.0\ncertifi==2023.5.7\ncharset-normalizer==3.1.0\nclick==8.1.3\ncontourpy==1.0.7\ncycler==0.11.0\nCython==0.29.34\ndecorator==5.1.1\nFlask==2.3.2\nfonttools==4.39.4\ngreenlet==2.0.2\nidna==3.4\nitsdangerous==2.1.2\nJinja2==3.1.2\nkiwisolver==1.4.4\nlxml==4.9.2\nMarkupSafe==2.1.2\nmatplotlib==3.7.1\nmsgpack==1.0.5\nnumexpr==2.8.4\nnumpy==1.24.3\nobspy==1.4.0\npackaging==23.1\npandas==2.0.1\nPillow==9.5.0\npsutil==5.9.5\npsycopg2==2.9.6\npy-cpuinfo==9.0.0\npyparsing==3.0.9\npython-dateutil==2.8.2\npytz==2023.3\nPyYAML==6.0\nrequests==2.30.0\nscipy==1.10.1\nsix==1.16.0\nSQLAlchemy==2.0.13\n-e git+https://github.com/rizac/stream2segment.git@ea3d7f4b7e4b89f9f58e93f375569272f50e8af2#egg=stream2segment\ntables==3.8.0\ntyping_extensions==4.5.0\ntzdata==2023.3\nurllib3==2.0.2\nWerkzeug==2.3.4\n\n\"\"\"A setuptools based setup module.\nTaken from:\nhttps://github.com/pypa/sampleproject/blob/master/setup.py\n\nSee also:\nhttp://python-packaging-user-guide.readthedocs.org/en/latest/distributing/\n\nAdditional links:\nhttps://packaging.python.org/en/latest/distributing.html\nhttps://github.com/pypa/sampleproject\n\"\"\"\nfrom __future__ import print_function\n\n# Always prefer setuptools over distutils\nfrom setuptools import setup, find_packages\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, 'README.md'), encoding='utf-8') as f:\n    long_description = f.read()\n\n# http://stackoverflow.com/questions/2058802/how-can-i-get-the-version-defined-in-setup-py-setuptools-in-my-package\nversion = \"\"\nwith open(path.join(here, 'stream2segment', 'resources', 'program_version')) as version_file:\n    version = version_file.read().strip()\n\nsetup(\n    name='stream2segment',\n\n    # Versions should comply with PEP440.  For a discussion on single-sourcing\n    # the version across setup.py and the project code, see\n    # https://packaging.python.org/en/latest/single_source_version.html\n    version=version,\n\n    description='A python project to download, process and visualize '\n                'event-based seismic waveforms',\n    long_description=long_description,\n\n    # The project's main homepage.\n    url='https://github.com/rizac/stream2segment',\n\n    # Author details\n    author='riccardo zaccarelli',\n    author_email='rizac@gfz-potsdam.de',  # FIXME: what to provide?\n\n    # Choose your license\n    license='GNU',\n\n    # See https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        'Development Status :: 4 - Beta',\n\n        # Indicate who your project is intended for\n        'Intended Audience :: Science/Research',\n        'Topic :: Scientific/Engineering',\n\n        # Pick your license as you wish (should match \"license\" above)\n        'License :: OSI Approved :: GNU License',\n\n\n        # Specify the Python versions you support here.\n        # 'Programming Language :: Python :: 3.5',\n        # 'Programming Language :: Python :: 3.6',\n        # 'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n    ],\n\n    # What does your project relate to?\n    keywords='download seismic waveforms related to events',\n\n    # You can just specify the packages manually here if your project is\n    # simple. Or you can use find_packages().\n    packages=find_packages(exclude=['contrib', 'docs', 'tests', 'htmlcov']),\n\n    # Alternatively, if you want to distribute just a my_module.py, uncomment\n    # this:\n    #   py_modules=[\"my_module\"],\n\n    # List run-time dependencies here.  These will be installed by pip when\n    # your project is installed. For info see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=['PyYAML>=3.12',\n                      'numpy>=1.13.1',\n                      'tables>=3.5.2',\n                      'pandas>=0.20.3',\n                      'obspy>=1.0.3',\n                      'Flask>=0.12.3',\n                      'psycopg2>=2.7.3.1',\n                      'psutil>=5.3.1',\n                      'SQLAlchemy>=1.1.14',\n                      'click>=6.7'\n                      ],\n\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). You can install these using the following syntax,\n    # for example:\n    # $ pip install -e .[dev,test]  (pip install -e \".[dev,test]\" in zsh)\n    extras_require={\n        # use latest versions. Without boundaries\n        'dev': ['pep8>=1.7.0',\n                'pylint>=1.7.2',\n                'pytest>=3.2.2',\n                'pytest-cov>=2.5.1',\n                'pytest-mock>=1.6.2'],\n        'jupyter': ['jupyter>=1.0.0']\n    },\n\n    # If there are data files included in your packages that need to be\n    # installed, specify them here.  If using Python 2.6 or less, then these\n    # have to be included in MANIFEST.in as well.\n    #\n    # package_data={\n    #    'sample': ['package_data.dat'],\n    # },\n\n    # make the installation process copy also the package data (see MANIFEST.in)\n    # for info see https://python-packaging.readthedocs.io/en/latest/non-code-files.html\n    include_package_data=True,\n\n    # Although 'package_data' is the preferred approach, in some case you may\n    # need to place data files outside of your packages. See:\n    # http://docs.python.org/3.4/distutils/setupscript.html#installing-additional-files # noqa\n    # In this case, 'data_file' will be installed into '<sys.prefix>/my_data'\n    #\n    # data_files=[('my_data', ['data/data_file'])],\n\n    # To provide executable scripts, use entry points in preference to the\n    # \"scripts\" keyword. Entry points provide cross-platform support and allow\n    # pip to create the appropriate form of executable for the target platform.\n    entry_points={\n        'console_scripts': [\n            'stream2segment=stream2segment.cli:cli',\n            's2s=stream2segment.cli:cli',\n        ],\n    },\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/sumo",
            "repo_link": "https://github.com/eclipse-sumo/sumo",
            "content": {
                "codemeta": "",
                "readme": "<a href=\"https://sumo.dlr.de/docs\"><p align=\"center\"><img width=50% src=\"https://raw.githubusercontent.com/eclipse/sumo/main/docs/web/docs/images/sumo-logo.svg\"></p></a>\n\nEclipse SUMO - Simulation of Urban MObility\n===========================================\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13907886.svg )](https://doi.org/10.5281/zenodo.13907886  )\n[![Windows](https://github.com/eclipse-sumo/sumo/actions/workflows/build-windows.yml/badge.svg)](https://github.com/eclipse-sumo/sumo/actions/workflows/build-windows.yml)\n[![Linux](https://github.com/eclipse-sumo/sumo/actions/workflows/build-linux.yml/badge.svg)](https://github.com/eclipse-sumo/sumo/actions/workflows/build-linux.yml)\n[![macOS](https://github.com/eclipse-sumo/sumo/actions/workflows/build-macos.yml/badge.svg)](https://github.com/eclipse-sumo/sumo/actions/workflows/build-macos.yml)\n[![sonarcloud security](https://sonarcloud.io/api/project_badges/measure?project=org.eclipse.sumo&metric=security_rating)](https://sonarcloud.io/summary/overall?id=org.eclipse.sumo)\n[![Translation status](https://hosted.weblate.org/widgets/eclipse-sumo/-/svg-badge.svg)](https://hosted.weblate.org/engage/eclipse-sumo/)\n![Repo Size](https://img.shields.io/github/repo-size/eclipse/sumo.svg)\n\nWhat is SUMO\n------------\n\n[\"Simulation of Urban MObility\" (SUMO)](https://sumo.dlr.de/) is an open source,\nhighly portable, microscopic traffic simulation package designed to handle\nlarge road networks and different modes of transport.\n\n<p align=\"center\"><img width=70% src=\"https://raw.githubusercontent.com/eclipse/sumo/main/docs/web/docs/images/multiple-screenshots.png\"></p>\n\nIt is mainly developed by employees of the [Institute of Transportation Systems\nat the German Aerospace Center](https://www.dlr.de/ts/en/).\n\n\nWhere to get it\n---------------\n\nYou can download SUMO via our [downloads site](https://sumo.dlr.de/docs/Downloads.html).\n\nAs the program is still under development (and is being extended continuously), we advice you to\nuse the latest sources from our GitHub repository. Using a command line client,\nexecute the following command:\n\n```\ngit clone --recursive https://github.com/eclipse-sumo/sumo\n```\n\nContact\n-------\n\nTo stay informed, we have a mailing list for SUMO, which\n[you can subscribe](https://dev.eclipse.org/mailman/listinfo/sumo-user) to.\nMessages to the list can be sent to sumo-user@eclipse.org.\nSUMO announcements will be made through the sumo-announce@eclipse.org list;\n[you can subscribe](https://dev.eclipse.org/mailman/listinfo/sumo-announce) to it as well.\nFor further contact information, have a look at [this page](https://sumo.dlr.de/docs/Contact.html).\n\n\nBuild and Installation\n----------------------\n\nFor Windows we provide pre-compiled binaries and CMake files to generate Visual Studio projects.\nIf you want to develop under Windows, please also clone the dependent libraries using:\n\n```\ngit clone --recursive https://github.com/DLR-TS/SUMOLibraries\n```\n\nIf you're using Linux, you should have a look whether your distribution already contains sumo.\nThere is also a [ppa for ubuntu users](https://launchpad.net/~sumo) and an\n[open build service instance](https://build.opensuse.org/project/show/science:dlr).\nIf you want to build SUMO yourself, the steps for ubuntu are:\n\n```\nsudo apt-get install cmake python g++ libxerces-c-dev libfox-1.6-dev libgdal-dev libproj-dev libgl2ps-dev swig\ncd <SUMO_DIR> # please insert the correct directory name here\nexport SUMO_HOME=\"$PWD\"\ncmake -B build .\ncmake --build build -j$(nproc)\n```\n\nFor [detailed build instructions, have a look at our Documentation](https://sumo.dlr.de/docs/Developer/Main.html#build_instructions).\n\n\nGetting started\n---------------\n\nTo get started with SUMO, take a look at the docs/tutorial and examples directories,\nwhich contain some example networks with routing data and configuration files.\nThere is also user documentation provided in the docs/ directory and on the\nhomepage.\n\nDocumentation\n---------------\n\n- The main documentation is at [sumo.dlr.de/docs](https://sumo.dlr.de/docs). Note that this tracks the [development version](https://sumo.dlr.de/docs/FAQ.html#why_does_sumo_not_behave_as_documented_in_this_wiki).\n- A mirror of the main documentation is at [sumo.sourceforge.net/docs](https://sumo.sourceforge.net/docs).\n- An offline version of the documentation is part of every release and can be accessed via `docs/userdoc/index.html`.\n\nImproving SUMO\n--------------\n\nPlease use the [GitHub issue tracking tool](https://github.com/eclipse-sumo/sumo/issues) for bugs and requests,\nor file them to the sumo-user@eclipse.org list. Before\nfiling a bug, please consider to check with a current repository checkout\nwhether the problem has already been fixed.\n\nWe welcome patches, pull requests and other contributions! For details see [our contribution guidelines](CONTRIBUTING.md).\n\nWe use [Weblate for translating SUMO](https://hosted.weblate.org/projects/eclipse-sumo/). If you\nwant to add translation strings or a language, see [our contribution guidelines](CONTRIBUTING.md#translating) and\n[this page](https://sumo.dlr.de/docs/Developer/Translating.html) for more information.\n\n\nLicense\n-------\n\nSUMO is licensed under the [Eclipse Public License Version 2](https://eclipse.org/legal/epl-v20.html).\nThe licenses of the different libraries and supplementary code information are in the\nsubdirectories and in the [Documentation](https://sumo.dlr.de/docs/Libraries_Licenses.html).\n\n",
                "dependencies": "# set VERSION to empty string\ncmake_policy(SET CMP0048 NEW)\n# do not expand quoted variables in if statements\ncmake_policy(SET CMP0054 NEW)\n# find python by path\nset(Python_FIND_STRATEGY LOCATION)\n# Options\noption(CHECK_OPTIONAL_LIBS \"Try to download / enable all optional libraries (use only EPL clean libraries, if set to false)\" ON)\noption(MULTITHREADED_BUILD \"Use all available cores for building (applies to Visual Studio only)\" ON)\noption(PROFILING \"Enable output of profiling data (applies to gcc/clang builds only)\")\noption(PPROF \"Link the pprof profiler library (applies to gcc/clang builds only)\")\noption(COVERAGE \"Enable output of coverage data (applies to gcc/clang builds only)\")\noption(SUMO_UTILS \"Enable generation of a shared library for the utility functions for option handling, XML parsing etc.\")\noption(FMI \"Enable generation of an FMI library for SUMO\" ON)\noption(NETEDIT \"Enable build of netedit\" ON)\noption(ENABLE_PYTHON_BINDINGS \"Build Python Bindings\" ON)\noption(ENABLE_JAVA_BINDINGS \"Build Java Bindings\" ON)\noption(ENABLE_CS_BINDINGS \"Build C# Bindings\")\noption(CCACHE_SUPPORT \"Enable ccache support if installed\" ON)\noption(TCMALLOC \"Use tcmalloc if installed\" ON)\noption(VERBOSE_SUB \"Let sub-commands be more verbose\")\nset(BINARY_SUFFIX \"\" CACHE STRING \"Append the suffix to every generated binary\")\nset(COMPILE_DEFINITIONS \"\" CACHE STRING \"Macros or defines to add when compiling\")\nset(JUPEDSIM_CUSTOMDIR \"\" CACHE PATH \"Custom install location of jupedsim\")\nset(PROJECT_NAME \"SUMO\" CACHE STRING \"Project/Solution name\")\n\n# Set a default build type if none was specified\n# you may use -DCMAKE_BUILD_TYPE:STRING=Debug from the command line\nset(default_build_type \"Release\")\n\nset(CMAKE_ORIGINAL_BUILD_TYPE \"${CMAKE_BUILD_TYPE}\")\nif (NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n  message(STATUS \"Setting build type to '${default_build_type}' as none was specified.\")\n  set(CMAKE_BUILD_TYPE \"${default_build_type}\" CACHE\n      STRING \"Choose the type of build.\" FORCE)\n  # Set the possible values of build type for cmake-gui\n  set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"RelWithDebInfo\")\nendif ()\n\ncmake_minimum_required(VERSION 3.5)\nproject(\"${PROJECT_NAME}\" CXX C)\nset(PACKAGE_VERSION \"git\")\n\n# Check if libraries have to be found, depending on SUMO_LIBRARIES\nset(SUMO_LIBRARIES \"$ENV{SUMO_LIBRARIES}\" CACHE PATH \"Location of SUMOLibraries dependencies\")\n\n# some user place SUMOLibraries in the same SUMO folder\nif (NOT SUMO_LIBRARIES AND EXISTS \"${CMAKE_SOURCE_DIR}/../SUMOLibraries\")\n    set(SUMO_LIBRARIES \"${CMAKE_SOURCE_DIR}/../SUMOLibraries\")\nendif ()\n\nif (CCACHE_SUPPORT)\n    if (MSVC)\n        file(GLOB SCCACHE_ROOT \"${SUMO_LIBRARIES}/sccache-*\")\n        find_program(SCCACHE_PROGRAM sccache ${SCCACHE_ROOT})\n        if (SCCACHE_PROGRAM)\n            message(STATUS \"Found sccache: ${SCCACHE_PROGRAM}\")\n            set(CMAKE_C_COMPILER_LAUNCHER \"${SCCACHE_PROGRAM}\")\n            set(CMAKE_CXX_COMPILER_LAUNCHER \"${SCCACHE_PROGRAM}\")\n            if (CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n                string(REPLACE \"/Zi\" \"/Z7\" CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG}\")\n                string(REPLACE \"/Zi\" \"/Z7\" CMAKE_C_FLAGS_DEBUG \"${CMAKE_C_FLAGS_DEBUG}\")\n            elseif (CMAKE_BUILD_TYPE STREQUAL \"RelWithDebInfo\")\n                string(REPLACE \"/Zi\" \"/Z7\" CMAKE_CXX_FLAGS_RELWITHDEBINFO \"${CMAKE_CXX_FLAGS_RELWITHDEBINFO}\")\n                string(REPLACE \"/Zi\" \"/Z7\" CMAKE_C_FLAGS_RELWITHDEBINFO \"${CMAKE_C_FLAGS_RELWITHDEBINFO}\")\n            endif()\n        endif()\n    else()\n        find_program(CCACHE_PROGRAM ccache)\n        if (CCACHE_PROGRAM)\n            message(STATUS \"Found ccache: ${CCACHE_PROGRAM}\")\n            set(CMAKE_C_COMPILER_LAUNCHER \"${CCACHE_PROGRAM}\")\n            set(CMAKE_CXX_COMPILER_LAUNCHER \"${CCACHE_PROGRAM}\")\n        endif()\n    endif()\nendif()\n\nset(CMAKE_COLOR_MAKEFILE ON)\nset(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} \"${CMAKE_SOURCE_DIR}/build_config/cmake_modules/\")\n\nset(ENABLED_FEATURES \"${CMAKE_SYSTEM} ${CMAKE_SYSTEM_PROCESSOR} ${CMAKE_CXX_COMPILER_ID} ${CMAKE_CXX_COMPILER_VERSION} ${CMAKE_BUILD_TYPE}\")\n\nif (COMPILE_DEFINITIONS)\n    add_compile_definitions(${COMPILE_DEFINITIONS})\nendif ()\n\n# declare flags for compilers\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n    set (GNU_COMPILER True)\nendif ()\nif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n    set (CLANG_COMPILER True)\nendif ()\n\n# C++14 is needed by Google Test >= 1.13, for all the other parts C++11 should be enough.\n# This will silently fall back to C++11 if 14 is not supported by the compiler.\nset(CMAKE_CXX_STANDARD 14)\n\n# compiler specific flags\nif (GNU_COMPILER OR CLANG_COMPILER)\n    # set flags for clang in windows\n    if (CLANG_COMPILER AND WIN32)\n        # flags for clang in windows\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall -Wextra\")\n        # disable debug build (due a problem with Runtime Libraries)\n        set(CMAKE_CONFIGURATION_TYPES \"Release\")\n        set(CMAKE_BUILD_TYPE \"Release\")\n    else ()\n        # flags for clang and gcc in linux\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pthread -Wall -pedantic -Wextra\")\n    endif ()\n    if (PROFILING)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pg\")\n        set(ENABLED_FEATURES \"${ENABLED_FEATURES} Profiling\")\n        set(BINARY_SUFFIX \"${BINARY_SUFFIX}P\")\n    endif ()\n    if (COVERAGE)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} --coverage -O0\")\n        set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} --coverage\")\n        set(ENABLED_FEATURES \"${ENABLED_FEATURES} Coverage\")\n    endif ()\nelseif (MSVC)\n    # disable warnings (all versions)\n    option(DISABLEWARNING_C5264 \"'const' variable is not used\" ON)\n    option(DISABLEWARNING_C5267 \"definition of implicit operator/constructor/destructor is deprecated\" ON)\n    option(DISABLEWARNING_C4355 \"'this' : used in base member initializer list\" ON)\n    option(DISABLEWARNING_C4266 \"'function' : no override available for virtual member function from base 'type'\" ON)\n    option(DISABLEWARNING_C4127 \"conditional expression is constant\" ON)\n    option(DISABLEWARNING_C26495 \"'variable' is uninitialized. Always initialize a member variable\" ON)\n    # Specific warnings from MSVC15\n    if (MSVC_VERSION LESS 2000)\n        option(DISABLEWARNING_C4619 \"there is no warning number xxxx\" ON)\n        option(DISABLEWARNING_C4774 \"format string expected not string literal\" ON)\n        option(DISABLEWARNING_C4571 \"structured exceptions (SEH) are no longer caught\" ON)\n    endif()\n    # 5264\n    if (DISABLEWARNING_C5264)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /wd5264\")\n    endif (DISABLEWARNING_C5264)\n    # 5267\n    if (DISABLEWARNING_C5267)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /wd5267\")\n    endif (DISABLEWARNING_C5267)\n    # 4355\n    if (DISABLEWARNING_C4355)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /wd4355\")\n    endif (DISABLEWARNING_C4355)\n    # 4266\n    if (DISABLEWARNING_C4266)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /wd4266\")\n    endif (DISABLEWARNING_C4266)\n    # 4127\n    if (DISABLEWARNING_C4127)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /wd4127\")\n    endif (DISABLEWARNING_C4127)\n    # 26495\n    if (DISABLEWARNING_C26495)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /wd26495\")\n    endif (DISABLEWARNING_C26495)\n    # 4619\n    if (DISABLEWARNING_C4619)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /wd4619\")\n    endif (DISABLEWARNING_C4619)\n    # 4774\n    if (DISABLEWARNING_C4774)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /wd4774\")\n    endif (DISABLEWARNING_C4774)\n    # 4571\n    if (DISABLEWARNING_C4571)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /wd4571\")\n    endif (DISABLEWARNING_C4571)\n    # enabling /WX is not possible due to warnings in external headers\n    if (CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n        # this is a workaround to disable lots of warnings for replacing /W3 with /W4 in VS 2019\n        # there is a policy for that starting with CMake 3.15\n        if (MSVC_VERSION GREATER 1900)\n            string(REPLACE \"/W3\" \"/Wall\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\n        else ()\n            string(REPLACE \"/W3\" \"/W4\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\n        endif ()\n    else ()\n        # /Wall brings MSVC 2013 to complete halt\n        if (MSVC_VERSION GREATER 1900)\n            set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} /Wall\")\n        else ()\n            set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} /W4\")\n        endif ()\n    endif ()\n    if (MULTITHREADED_BUILD)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /MP\")\n    endif ()\n    # SWIG generates large obj files\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /bigobj\")\n    # backward compatible mutex API, see https://github.com/actions/runner-images/issues/10004\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /D_DISABLE_CONSTEXPR_MUTEX_CONSTRUCTOR\")\n    # exporting symbols for shared libraries needs to enabled explicitly\n    set(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS ON)\n    # add option for enabling console in release mode\n    option(CONSOLE_RELEASE \"Enable console in SUMO-GUI and NETEDIT in release (use only for debug purposes)\" false)\nendif ()\n\n# special debug flags\nset(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -D_DEBUG\")\nif (CLANG_COMPILER)\n    if (WIN32)\n        set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -fsanitize=undefined,integer -fsanitize-blacklist=${CMAKE_SOURCE_DIR}/build_config/clang_sanitize_blacklist.txt\")\n        add_compile_definitions(_ITERATOR_DEBUG_LEVEL=0)\n    else ()\n        set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -fsanitize=undefined,address,integer -fno-omit-frame-pointer -fsanitize-blacklist=${CMAKE_SOURCE_DIR}/build_config/clang_sanitize_blacklist.txt\")\n    endif ()\nendif ()\n\n# we need to build position independent code when generating a shared library\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\nif (SUMO_UTILS)\n    set(ENABLED_FEATURES \"${ENABLED_FEATURES} SumoUtilsLibrary\")\nendif ()\nif (FMI)\n    set(ENABLED_FEATURES \"${ENABLED_FEATURES} FMI\")\nendif ()\n\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_SOURCE_DIR}/bin)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_SOURCE_DIR}/bin)\n# force Visual Studio to leave out the Release / Debug dirs\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY_DEBUG ${CMAKE_SOURCE_DIR}/bin)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY_RELEASE ${CMAKE_SOURCE_DIR}/bin)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY_DEBUG ${CMAKE_SOURCE_DIR}/bin)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE ${CMAKE_SOURCE_DIR}/bin)\n# Debug messages\nmessage(STATUS \"CMAKE_BINARY_DIR: \" ${CMAKE_BINARY_DIR})\nmessage(STATUS \"CMAKE_SOURCE_DIR: \" ${CMAKE_SOURCE_DIR})\nmessage(STATUS \"\")\nmessage(STATUS \"Platform: \")\nmessage(STATUS \"    Host: \" ${CMAKE_HOST_SYSTEM} \" \" ${CMAKE_HOST_SYSTEM_PROCESSOR})\nmessage(STATUS \"    Target: \" ${CMAKE_SYSTEM} \" \" ${CMAKE_SYSTEM_PROCESSOR})\nmessage(STATUS \"    CMake: \" ${CMAKE_VERSION})\nmessage(STATUS \"    CMake generator: \" ${CMAKE_GENERATOR})\nmessage(STATUS \"    CMake build tool: \" ${CMAKE_BUILD_TOOL})\nmessage(STATUS \"    Compiler: \" ${CMAKE_CXX_COMPILER_ID} \" \" ${CMAKE_CXX_COMPILER_VERSION})\nif (CMAKE_GENERATOR MATCHES Xcode)\n    message(STATUS \"    Xcode: \" ${XCODE_VERSION})\nendif ()\nmessage(STATUS \"\")\n\nif (SKBUILD OR ${CMAKE_VERSION} VERSION_LESS 3.12.0)\n    find_package(PythonInterp REQUIRED)\n    find_package(PythonLibs)\nelse()\n    find_package(Python REQUIRED COMPONENTS Interpreter Development)\n    # define variables for compatibility. refactor once older cmake unsupported\n    if (Python_FOUND)\n        set(PYTHONLIBS_FOUND ${Python_FOUND})\n        set(PYTHON_EXECUTABLE ${Python_EXECUTABLE} CACHE FILEPATH \"Python executable\")\n        set(PYTHON_LIBRARIES ${Python_LIBRARIES} CACHE FILEPATH \"Python libraries\")\n        set(PYTHON_INCLUDE_DIRS ${Python_INCLUDE_DIRS} CACHE PATH \"Python include folder\")\n        set(PYTHON_LIBRARY_DIRS ${Python_LIBRARY_DIRS})\n    endif()\nendif()\nmessage(STATUS \"Found Python: \" ${PYTHON_EXECUTABLE})\nif (MSVC AND ENABLE_PYTHON_BINDINGS)\n    # recheck that the platform of the generator and python matches\n    execute_process(COMMAND ${PYTHON_EXECUTABLE} -c \"import sys; print(sys.maxsize > 2**32)\"\n                    OUTPUT_VARIABLE IS_PYTHON64 OUTPUT_STRIP_TRAILING_WHITESPACE)\n    if (${CMAKE_MODULE_LINKER_FLAGS} STREQUAL \"/machine:x64\")\n        if (${IS_PYTHON64} STREQUAL \"False\")\n            message(WARNING \"Did not find Python 64 bit. Please set PYTHON_EXECUTABLE, PYTHON_INCLUDE_DIR and PYTHON_LIBRARY manually.\")\n            set(PYTHON_INCLUDE_DIRS \"\")\n        endif()\n    else()\n        if (${IS_PYTHON64} STREQUAL \"True\")\n            message(WARNING \"Did not find Python 32 bit. Please set PYTHON_EXECUTABLE, PYTHON_INCLUDE_DIR and PYTHON_LIBRARY manually.\")\n            set(PYTHON_INCLUDE_DIRS \"\")\n        endif()\n    endif()\n    if (NOT PYTHON_DEBUG_LIBRARY AND \"${CMAKE_BUILD_TYPE}\" STREQUAL \"Debug\")\n        message(WARNING \"Did not find Python debug library. Please reinstall your Python and enable the Python debug libraries in the installer.\")\n        set(PYTHON_INCLUDE_DIRS \"\")\n    endif()\nendif()\n\nexecute_process(COMMAND ${PYTHON_EXECUTABLE} -c \"import setuptools\" RESULT_VARIABLE PYTHON_SETUPTOOLS_MISSING ERROR_QUIET)\nexecute_process(COMMAND ${PYTHON_EXECUTABLE} -c \"import build; import pip; build.__version__\" RESULT_VARIABLE PYTHON_BUILD_MISSING ERROR_QUIET)\n\n# exclusive from Apple\nif (APPLE)\n    # we know that openGL is deprecated for newer MacOS\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DGL_SILENCE_DEPRECATION\")\n\n    # Xcode 15 on macOS sonoma complains about duplicate libraries for the linker, so we disable the warning for xcode >= 15\n    if (CLANG_COMPILER)\n        string(REGEX MATCH \"^[0-9]+\" ClangMajorVersion \"${CMAKE_CXX_COMPILER_VERSION}\")\n        if (ClangMajorVersion GREATER_EQUAL 15)\n            set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -Wl,-no_warn_duplicate_libraries\")\n        endif()\n    endif()\n\n    # Recommendation, also add a \"brew --prefix\" custom command to detect a homebrew build environment\n    execute_process(COMMAND brew --prefix RESULT_VARIABLE DETECT_BREW OUTPUT_VARIABLE BREW_PREFIX ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE)\n    if (${DETECT_BREW} EQUAL 0)\n        link_directories(${BREW_PREFIX}/lib)\n        include_directories(${BREW_PREFIX}/include)\n        message(STATUS \"Brew detected: ${BREW_PREFIX}\")\n    endif ()\n\n    # Detect if the \"port\" command is valid on this system; if so, return full path\n    execute_process(COMMAND which port RESULT_VARIABLE DETECT_MACPORTS OUTPUT_VARIABLE MACPORTS_PREFIX ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE)\n    if (${DETECT_MACPORTS} EQUAL 0)\n        # MACPORTS_PREFIX contains now something like \"/opt/local/bin/port\", so we get the parent directory\n        get_filename_component(MACPORTS_PREFIX ${MACPORTS_PREFIX} DIRECTORY)\n        # \"/opt/local\" is where MacPorts lives, add `/lib` suffix and link\n        # http://stackoverflow.com/questions/1487752/how-do-i-instruct-cmake-to-look-for-libraries-installed-by-macports\n        link_directories(/opt/X11/lib ${MACPORTS_PREFIX}/../lib)\n        include_directories(SYSTEM /opt/X11/include ${MACPORTS_PREFIX}/../include)\n        message(STATUS \"Macports detected: ${MACPORTS_PREFIX}\")\n    endif ()\nendif (APPLE)\n\n# Specifically define variable WIN32 for compilations under windows (due an error in Shawn)\nif (GNU_COMPILER AND WIN32)\n    option(USE_MINGW_64BITS \"Use 64 bits libraries for the compilation with MinGW\" true)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DWIN32 -D_WIN32_WINNT=0x0501\")\n    set(MINGW32 1)\nendif ()\n\nif (NOT JUPEDSIM_CUSTOMDIR)\n#     check if set JuPedSim directory from SUMOLibraries\n    if (SUMO_LIBRARIES AND WIN32)\n        file(GLOB JUPEDSIM_CUSTOMDIR_SUMOLIBRARIES \"${SUMO_LIBRARIES}/JuPedSim-*\")\n        set(JUPEDSIM_CUSTOMDIR \"${JUPEDSIM_CUSTOMDIR_SUMOLIBRARIES}\")\n    else ()\n        if (EXISTS \"${CMAKE_SOURCE_DIR}/../jupedsim-install\")\n            set(JUPEDSIM_CUSTOMDIR \"${CMAKE_SOURCE_DIR}/../jupedsim-install\")\n        endif ()\n    endif ()\n    if (JUPEDSIM_CUSTOMDIR)\n        message(STATUS \"Using JuPedSim from ${JUPEDSIM_CUSTOMDIR}\")\n    endif ()\nendif ()\n\n# check if SUMOLibraries was found (Only in Windows)\nif (SUMO_LIBRARIES AND WIN32)\n    # set option for install debug and release runtime dlls\n    option(INSTALL_DLL_RUNTIMES \"Copy debug and release runtimes for MSVC\" true)\n    # Special option for MinGW32\n    message(STATUS \"Using 64 bit libraries from SUMO_LIBRARIES placed in \" ${SUMO_LIBRARIES})\n    file(GLOB XERCES_PATH \"${SUMO_LIBRARIES}/xerces-c-3.?.?\")\n    file(GLOB ZLIB_PATH \"${SUMO_LIBRARIES}/3rdPartyLibs/zlib-?.?.*\")\n    file(GLOB PROJ_PATH \"${SUMO_LIBRARIES}/proj-?.?.?\")\n    file(GLOB FOX_PATH \"${SUMO_LIBRARIES}/fox-?.?.??\")\n    file(GLOB FREETYPE_PATH \"${SUMO_LIBRARIES}/3rdPartyLibs/freetype-2.??.?\")\n    file(GLOB EIGEN3_PATH \"${SUMO_LIBRARIES}/eigen-3.?.?\")\n    file(GLOB GETTEXT_PATH \"${SUMO_LIBRARIES}/gettext-?.??\")\n    # set all paths in prefix\n    set(CMAKE_PREFIX_PATH \"${CMAKE_PREFIX_PATH};${EIGEN3_PATH};${XERCES_PATH};${ZLIB_PATH};${PROJ_PATH};${FOX_PATH};${FREETYPE_PATH};${GETTEXT_PATH};${GETTEXT_PATH}/tools/gettext\")\n    # save in variable SUMO_LIBRARIES_DLL all paths to dll files\n    file(GLOB SUMO_LIBRARIES_DLL \"${XERCES_PATH}/bin/*.dll\"\n                                 \"${PROJ_PATH}/bin/*.dll\"\n                                 \"${FOX_PATH}/bin/*.dll\"\n                                 \"${ZLIB_PATH}/bin/*.dll\"\n                                 \"${FREETYPE_PATH}/bin/*.dll\"\n                                 \"${GETTEXT_PATH}/bin/*.dll\"\n\t\t\t\t\t\t\t\t \"${GDAL_PATH}/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/libspatialite-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/bzip2-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/geos-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/curl-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/libpng-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/libssh2-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/openssl-*/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/sqlite-*/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/tiff-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/libxml2-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/libiconv-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/openssl-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/sqlite-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/freexl-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/libexpat-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/librttopo-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/minizip-*/bin/*.dll\"\n                                 \"${SUMO_LIBRARIES}/3rdPartyLibs/bzip2-*/bin/*.dll\"\n                                 )\n    # check if MSVC dll runtimes have to be copied\n    if (INSTALL_DLL_RUNTIMES)\n        file(GLOB MSVC_DLLS \"${SUMO_LIBRARIES}/runtimes/*.dll\")\n        list(APPEND SUMO_LIBRARIES_DLL ${MSVC_DLLS})\n    endif(INSTALL_DLL_RUNTIMES)\n    if (MSVC_VERSION GREATER 1919)\n        # fmt in SUMOLibraries only works with MSVC 2019 and later\n        file(GLOB fmt_PATH \"${SUMO_LIBRARIES}/3rdPartyLibs/fmt-??.?.?\")\n    endif ()\n    # declare flag for use google test\n    set(USE_GOOGLETEST true)\n    # check if use google test\n    if (WIN32)\n        if (GNU_COMPILER AND NOT USE_MINGW_64BITS)\n            message(STATUS \"Disabled Google Test in Mingw32\")\n            set(USE_GOOGLETEST false)\n        elseif(CLANG_COMPILER)\n            message(STATUS \"Disabled Google Test in Clang (Windows)\")\n            set(USE_GOOGLETEST false)\n        endif ()\n    endif()\n    if (USE_GOOGLETEST)\n        file(GLOB GTEST_ROOT \"${SUMO_LIBRARIES}/googletest-?.??.?\")\n        message(STATUS \"Found Google test: \" ${GTEST_ROOT})\n        include_directories(\"${GTEST_ROOT}/include\")\n        # set both google test library\n        set(GTEST_BOTH_LIBRARIES \"${GTEST_ROOT}/lib/gtest.lib\" \"${GTEST_ROOT}/lib/gtest_main.lib\")\n        # set google test DLLs\n        file(GLOB GTEST_DLL \"${GTEST_ROOT}/bin/gtest*.dll\")\n        set(GTEST_FOUND true)\n    endif ()\n    file(GLOB SWIG_EXECUTABLE \"${SUMO_LIBRARIES}/swigwin-*/swig.exe\")\n    file(GLOB MVN_EXECUTABLE \"${SUMO_LIBRARIES}/apache-maven-*/bin/mvn.cmd\")\n    file(GLOB TEXTTEST_EXECUTABLE \"${SUMO_LIBRARIES}/TextTest-*/texttest.exe\")\nelse ()\n    # for Linux and Mac only\n    find_package(GTest)\nendif ()\n\nfind_package(XercesC REQUIRED)\nif (XercesC_FOUND)\n    include_directories(SYSTEM ${XercesC_INCLUDE_DIRS})\nendif (XercesC_FOUND)\n\nfind_package(Proj)\nif (PROJ_FOUND)\n    include_directories(SYSTEM ${PROJ_INCLUDE_DIR})\n    set(ENABLED_FEATURES \"${ENABLED_FEATURES} Proj\")\nendif (PROJ_FOUND)\n\nfind_package(FOX)\nif (FOX_FOUND)\n    set(OpenGL_GL_PREFERENCE LEGACY)\n    find_package(OpenGL)\n    if (OPENGL_FOUND AND OPENGL_GLU_FOUND)\n        include_directories(SYSTEM ${FOX_INCLUDE_DIR})\n        add_definitions(${FOX_CXX_FLAGS})\n        add_definitions(-DFLOAT_MATH_FUNCTIONS)\n        set(HAVE_FOX 1)\n        set(ENABLED_FEATURES \"${ENABLED_FEATURES} GUI\")\n    else ()\n        message(WARNING \"Fox is present but OpenGL or GLU cannot be found, GUI will be disabled.\")\n    endif ()\nendif (FOX_FOUND)\n\nfind_package(Freetype)\nif (FREETYPE_FOUND)\n    include_directories(SYSTEM ${FREETYPE_INCLUDE_DIRS})\nelse (FREETYPE_FOUND)\n    message(WARNING \"FreeType cannot be found. If you are using SUMO libraries, update SUMO_LIBRARIES folder with git pull. Otherwise, specify manually FreeType include folder and libs.\")\nendif (FREETYPE_FOUND)\n\nif (SUMO_LIBRARIES AND WIN32)\n    find_package(fmt PATHS ${fmt_PATH} NO_DEFAULT_PATH)\n    if (fmt_FOUND)\n        message(STATUS \"Found FMT (SUMOLibraries): \" ${fmt_DIR})\n        include_directories(SYSTEM \"${fmt_PATH}/include\")\n        set(FMT_LIBRARY fmt::fmt)\n        set(HAVE_FMT 1)\n        set(ENABLED_FEATURES \"${ENABLED_FEATURES} FMT\")\n    endif (fmt_FOUND)\nelseif (NOT MSVC)\n    find_package(fmt QUIET)\n    if (fmt_FOUND)\n        set(HAVE_FMT 1)\n        set(FMT_LIBRARY fmt::fmt)\n        # next 2 lines could fix the include path for mambaforge fmt on DLR machines (maybe outdated cmake?)\n        # get_target_property(fmt_INCLUDE_DIR fmt::fmt INTERFACE_INCLUDE_DIRECTORIES)\n        # include_directories(SYSTEM ${fmt_INCLUDE_DIR})\n        set(ENABLED_FEATURES \"${ENABLED_FEATURES} FMT\")\n    endif (fmt_FOUND)\nendif ()\n\nfind_package(X11)\nif (X11_FOUND)\n    link_directories(${X11_LIBRARY_DIR})\n    include_directories(SYSTEM ${X11_INCLUDE_DIR})\nendif (X11_FOUND)\n\nfind_package(ZLIB)\nif (ZLIB_FOUND)\n    set(HAVE_ZLIB 1)\n    link_directories(${ZLIB_LIBRARY_DIR})\n    include_directories(SYSTEM ${ZLIB_INCLUDE_DIR})\nendif ()\n\nfind_package(Intl)\nif (Intl_FOUND)\n    set(HAVE_INTL 1)\n    link_directories(${Intl_LIBRARY_DIR})\n    include_directories(SYSTEM ${Intl_INCLUDE_DIRS})\n    set(ENABLED_FEATURES \"${ENABLED_FEATURES} Intl\")\nendif ()\n\nfind_package(SWIG 3.0)\nif (SWIG_FOUND)\n    set(ENABLED_FEATURES \"${ENABLED_FEATURES} SWIG\")\nendif ()\n\nif (TCMALLOC)\n    find_library(TCMALLOC_LIBRARY NAMES tcmalloc_minimal)\n    if (TCMALLOC_LIBRARY)\n        set(ENABLED_FEATURES \"${ENABLED_FEATURES} tcmalloc\")\n    else ()\n        set(TCMALLOC_LIBRARY \"\")\n    endif ()\nendif ()\n\n# Eigen (for overhead wire / electric circuit simulation)\nfind_package(Eigen3 3.2)\nif (EIGEN3_FOUND)\n    include_directories(SYSTEM ${EIGEN3_INCLUDE_DIR})\n    set(HAVE_EIGEN 1)  # see config.h.cmake for #cmakedefine\n    set(ENABLED_FEATURES \"${ENABLED_FEATURES} Eigen\")\nendif (EIGEN3_FOUND)\n\nif (CHECK_OPTIONAL_LIBS)\n    file(GLOB GDAL_PATH \"${SUMO_LIBRARIES}/gdal-?.?.?\")\n    file(GLOB FFMPEG_PATH \"${SUMO_LIBRARIES}/FFMPEG-?.?.?\")\n    file(GLOB OSG_PATH \"${SUMO_LIBRARIES}/OSG-?.?.?\")\n    file(GLOB GL2PS_PATH \"${SUMO_LIBRARIES}/gl2ps-?.?.?\")\n    file(GLOB GEOS_PATH \"${SUMO_LIBRARIES}/3rdPartyLibs/geos-*\")\n    set(CMAKE_PREFIX_PATH \"${CMAKE_PREFIX_PATH};${GDAL_PATH};${FFMPEG_PATH};${OSG_PATH};${GL2PS_PATH};${GEOS_PATH}\")\n    file(GLOB SUMO_OPTIONAL_LIBRARIES_DLL \"${GDAL_PATH}/bin/*.dll\" \"${FFMPEG_PATH}/bin/*.dll\" \"${OSG_PATH}/bin/*.dll\" \"${GL2PS_PATH}/bin/*.dll\" \"${JUPEDSIM_CUSTOMDIR}/bin/*.dll\")\n\n    # GDAL (for geopositioning)\n    find_package(GDAL)\n    if (GDAL_FOUND)\n        include_directories(SYSTEM ${GDAL_INCLUDE_DIR})\n        set(HAVE_GDAL 1)\n        set(ENABLED_FEATURES \"${ENABLED_FEATURES} GDAL\")\n    else (GDAL_FOUND)\n        set(GDAL_LIBRARY \"\")\n    endif (GDAL_FOUND)\n\n    if (FOX_FOUND)\n        # FFMPEG (for recording videos)\n        find_package(FFMPEG)\n        if (FFMPEG_FOUND)\n            include_directories(SYSTEM ${FFMPEG_INCLUDE_DIR})\n            set(HAVE_FFMPEG 1)\n            set(ENABLED_FEATURES \"${ENABLED_FEATURES} FFmpeg\")\n        endif ()\n\n        # OSG (For 3D view)\n        find_package(OpenSceneGraph 3.4.0 COMPONENTS osgGA osgViewer osgUtil osgDB osgText)\n        if (OPENSCENEGRAPH_FOUND)\n            include_directories(SYSTEM ${OPENSCENEGRAPH_INCLUDE_DIRS})\n            set(HAVE_OSG 1)\n            set(ENABLED_FEATURES \"${ENABLED_FEATURES} OSG\")\n        endif ()\n\n        # GL2PS (deprecated, will be changed by FreeType)\n        find_package(GL2PS)\n        if (GL2PS_FOUND)\n            include_directories(SYSTEM ${GL2PS_INCLUDE_DIR})\n            set(HAVE_GL2PS 1)\n            set(ENABLED_FEATURES \"${ENABLED_FEATURES} GL2PS\")\n        endif (GL2PS_FOUND)\n    endif ()\n\n    find_package(jupedsim CONFIG HINTS ${JUPEDSIM_CUSTOMDIR} QUIET)\n    if (jupedsim_FOUND)\n        find_package(GEOS)\n        if (GEOS_FOUND)\n            include_directories(SYSTEM ${GEOS_INCLUDE_DIR})\n            set(JPS_VERSION \"${jupedsim_VERSION_MAJOR}${jupedsim_VERSION_MINOR}${jupedsim_VERSION_PATCH}\")\n            set(ENABLED_FEATURES \"${ENABLED_FEATURES} JuPedSim\")\n            message(STATUS \"Found JuPedSim: ${jupedsim_DIR} ${jupedsim_VERSION}\")\n        else (GEOS_FOUND)\n            message(STATUS \"Could NOT find GEOS! JuPedSim requires GEOS.\")\n        endif (GEOS_FOUND)\n    else (jupedsim_FOUND)\n        message(STATUS \"Could NOT find JuPedSim! Skipping JuPedSim integration.\")\n    endif (jupedsim_FOUND)\n\n    # add optional libraries\n    list(APPEND SUMO_LIBRARIES_DLL ${SUMO_OPTIONAL_LIBRARIES_DLL})\nendif (CHECK_OPTIONAL_LIBS)\n\ninclude_directories(${CMAKE_CURRENT_BINARY_DIR}/src)\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR}/src)\n\nset(commonlibs\n        utils_distribution utils_handlers utils_shapes utils_options\n        utils_xml utils_geom utils_common utils_importio utils_iodevices utils_traction_wire foreign_tcpip\n        ${XercesC_LIBRARIES} ${ZLIB_LIBRARIES} ${PROJ_LIBRARY} ${Intl_LIBRARIES} ${FMT_LIBRARY})\nif (WIN32)\n    set(commonlibs ${commonlibs} ws2_32)\nendif ()\nif (PPROF)\n    set(commonlibs ${commonlibs} profiler)\nendif ()\nset(commonvehiclelibs\n        utils_emissions foreign_phemlight foreign_phemlight_V5 utils_vehicle ${commonlibs} ${FOX_LIBRARY})\n\n# set custom name and folder for ALL_BUILD and ZERO_CHECK in visual studio solutions\nset_property(GLOBAL PROPERTY USE_FOLDERS ON)\nset_property(GLOBAL PROPERTY PREDEFINED_TARGETS_FOLDER \"CMake\")\n\n# installation\nif (MSVC)\n    if (CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)\n        set(CMAKE_INSTALL_PREFIX \"sumo-${PACKAGE_VERSION}\")\n    endif()\n    install(DIRECTORY bin/ DESTINATION bin\n            FILES_MATCHING\n            PATTERN \"*.bat\"\n            PATTERN \"*.dll\"\n            PATTERN \"*d.dll\" EXCLUDE\n            PATTERN \"gtest*.dll\" EXCLUDE)\nelse ()\n    include(GNUInstallDirs)\nendif ()\nif (SKBUILD OR MSVC)\n    set(DATA_PATH \"\")\nelse ()\n    set(DATA_PATH \"share/sumo/\")\nendif ()\nif (SKBUILD)\n    set(EXCLUDE_LIBSUMO \"libsumo\")\n    set(EXCLUDE_LIBTRACI \"libtraci\")\nendif ()\ninstall(DIRECTORY data/ DESTINATION ${DATA_PATH}data)\ninstall(DIRECTORY tools/ DESTINATION ${DATA_PATH}tools\n        USE_SOURCE_PERMISSIONS\n        PATTERN \"build_config/sumo\" EXCLUDE  # needed for the scikit build\n        PATTERN \"calibration\" EXCLUDE\n        PATTERN \"lisum-core\" EXCLUDE\n        PATTERN \"lisum-gui\" EXCLUDE\n        PATTERN \"sumolib4matlab/src\" EXCLUDE\n        PATTERN \"${EXCLUDE_LIBSUMO}\" EXCLUDE\n        PATTERN \"${EXCLUDE_LIBTRACI}\" EXCLUDE\n        PATTERN \"traas\" EXCLUDE\n        PATTERN \"traci4matlab/src\" EXCLUDE\n        PATTERN \"__pycache__\" EXCLUDE\n        PATTERN \"*.pyc\" EXCLUDE\n        PATTERN \"*.egg-info\" EXCLUDE\n        PATTERN \".git\" EXCLUDE)\nif (DATA_PATH)\n    install(CODE \"execute_process(COMMAND ${CMAKE_COMMAND} -E create_symlink ../../bin \\$ENV{DESTDIR}${CMAKE_INSTALL_PREFIX}/${DATA_PATH}bin)\")\nendif ()\n\ninstall(EXPORT SUMOConfig DESTINATION ${DATA_PATH}cmake NAMESPACE SUMO::)\n\nif (NOT ${PYTHON_SETUPTOOLS_MISSING})\n    set(TOOLS_DIR \"${CMAKE_SOURCE_DIR}/tools\")\n    if (${PYTHON_BUILD_MISSING})\n        install(CODE \"execute_process(COMMAND ${PYTHON_EXECUTABLE} build_config/setup-sumolib.py clean --all install --root=\\$ENV{DESTDIR}/ --prefix=${CMAKE_INSTALL_PREFIX} --optimize=1 WORKING_DIRECTORY ${TOOLS_DIR})\"\n                COMPONENT pysumolib)\n        install(CODE \"execute_process(COMMAND ${PYTHON_EXECUTABLE} build_config/setup-traci.py clean --all install --root=\\$ENV{DESTDIR}/ --prefix=${CMAKE_INSTALL_PREFIX} --optimize=1 WORKING_DIRECTORY ${TOOLS_DIR})\"\n                COMPONENT pytraci)\n    else ()\n        if (NOT VERBOSE_SUB)\n            set(PYTHON_BUILD_OPTS \"-C--quiet\")\n        endif ()\n        add_custom_target(pure_wheels ALL\n                          COMMAND ${PYTHON_EXECUTABLE} ./build_config/version.py ./build_config/setup-sumolib.py ./setup.py\n                          COMMAND ${PYTHON_EXECUTABLE} -m build ${PYTHON_BUILD_OPTS} --wheel . -o ${CMAKE_CURRENT_BINARY_DIR}\n                          COMMAND ${PYTHON_EXECUTABLE} ./build_config/version.py ./build_config/setup-traci.py ./setup.py\n                          COMMAND ${PYTHON_EXECUTABLE} -m build ${PYTHON_BUILD_OPTS} --wheel . -o ${CMAKE_CURRENT_BINARY_DIR}\n                          WORKING_DIRECTORY ${TOOLS_DIR})\n        install(CODE \"execute_process(COMMAND ${PYTHON_EXECUTABLE} -m pip install --root=\\$ENV{DESTDIR}/ --prefix=${CMAKE_INSTALL_PREFIX} -f ${CMAKE_BINARY_DIR} --no-index traci)\"\n                COMPONENT pytraci)\n    endif ()\nendif ()\n\nif (SUMO_LIBRARIES AND WIN32)\n    # filter release DLLs\n    execute_process(COMMAND ${PYTHON_EXECUTABLE} ${CMAKE_SOURCE_DIR}/tools/build_config/filterDebugDLL.py ${SUMO_LIBRARIES_DLL}\n                    OUTPUT_VARIABLE SUMO_LIBRARIES_RELEASE_DLL)\n    # copy debug dlls\n    foreach(DLL_PATH ${SUMO_LIBRARIES_DLL} ${GTEST_DLL})\n        get_filename_component(DLL_BASENAME ${DLL_PATH} NAME)\n        set(TARGET_DLL ${CMAKE_SOURCE_DIR}/bin/${DLL_BASENAME})\n        list(FIND TARGET_ALL_DLLS ${TARGET_DLL} DLL_KNOWN)\n        list(FIND SUMO_LIBRARIES_RELEASE_DLL ${DLL_PATH} IS_RELEASE_DLL)\n        list(FIND GTEST_DLL ${DLL_PATH} IS_TEST_DLL)\n        if (DLL_KNOWN EQUAL -1)\n            add_custom_command(OUTPUT ${TARGET_DLL}\n                               COMMAND ${CMAKE_COMMAND} -E copy ${DLL_PATH} ${TARGET_DLL}\n                               DEPENDS ${DLL_PATH})\n            if (IS_TEST_DLL GREATER -1)\n                list(APPEND TARGET_TEST_DLLS ${TARGET_DLL})\n            elseif (IS_RELEASE_DLL GREATER -1)\n                list(APPEND TARGET_ALL_DLLS ${TARGET_DLL})\n            else ()\n                list(APPEND TARGET_ALL_DLLS $<$<CONFIG:Debug>:${TARGET_DLL}>)\n            endif ()\n        endif()\n    endforeach(DLL_PATH)\n    # copy OSG dlls\n    if (HAVE_OSG)\n        file(GLOB OSG_PLUGIN_DLL \"${OSG_PATH}/bin/osgPlugins*/osgdb_osg.dll\")\n        get_filename_component(OSG_PLUGIN_DLL_BASENAME ${OSG_PLUGIN_DLL} NAME)\n        get_filename_component(OSG_PLUGIN_DIR ${OSG_PLUGIN_DLL} DIRECTORY)\n        get_filename_component(OSG_PLUGIN_DIR ${OSG_PLUGIN_DIR} NAME)\n        set(OSG_TARGET ${CMAKE_SOURCE_DIR}/bin/${OSG_PLUGIN_DIR}/${OSG_PLUGIN_DLL_BASENAME})\n        add_custom_command(OUTPUT ${OSG_TARGET}\n                           COMMAND ${CMAKE_COMMAND} -E copy_directory ${OSG_PATH}/bin/${OSG_PLUGIN_DIR} ${CMAKE_SOURCE_DIR}/bin/${OSG_PLUGIN_DIR}\n                           DEPENDS ${OSG_PLUGIN_DLL})\n    endif (HAVE_OSG)\n    # proj 7 needs to copy \"share/proj\" folder in SUMO_HOME/share\n    set(PROJ_DATA ${CMAKE_SOURCE_DIR}/share/proj/proj.db)\n    add_custom_command(OUTPUT ${PROJ_DATA}\n                       COMMAND ${CMAKE_COMMAND} -E copy_directory ${PROJ_PATH}/share/proj ${CMAKE_SOURCE_DIR}/share/proj\n                       DEPENDS ${PROJ_PATH}/share/proj/proj.db)\n    install(DIRECTORY share/ DESTINATION ${DATA_PATH}share)\nendif()\n# set install dll targets, for non-Windows they depend on nothing\nadd_custom_target(install_dll DEPENDS ${TARGET_ALL_DLLS} ${OSG_TARGET} ${PROJ_DATA})\nadd_custom_target(install_test_dll DEPENDS ${TARGET_TEST_DLLS})\nset_property(TARGET install_dll PROPERTY FOLDER \"CMake\")\nset_property(TARGET install_test_dll PROPERTY FOLDER \"CMake\")\n\n# i18n target\nfind_package(Gettext)\nif (Gettext_FOUND)\n    set(I18N_DATA ${CMAKE_SOURCE_DIR}/data/locale/tr/LC_MESSAGES/sumo.mo)\n    add_custom_command(OUTPUT ${I18N_DATA}\n                       COMMAND ${PYTHON_EXECUTABLE} ${CMAKE_SOURCE_DIR}/tools/build_config/i18n.py -m\n                       DEPENDS ${CMAKE_SOURCE_DIR}/data/po/tr_gui.po)\nelse ()\n    message(WARNING \"Gettext tools not found, translation files will not be generated.\")\nendif()\nadd_custom_target(install_mo DEPENDS ${I18N_DATA})\n\n# java targets\nfind_program(MVN_EXECUTABLE mvn)\nif (NOT VERBOSE_SUB)\n    set(MVN_OPTS \"-q\")\nendif ()\nfind_package(Java COMPONENTS Development)\nif (MVN_EXECUTABLE AND Java_FOUND)\n    if (NOT DEFINED ENV{JAVA_HOME})\n        get_filename_component(JAVA_BIN ${Java_JAVAC_EXECUTABLE} DIRECTORY)\n        get_filename_component(JAVA_HOME ${JAVA_BIN} DIRECTORY)\n        set(MVN_COMMAND_PREFIX ${CMAKE_COMMAND} -E env JAVA_HOME=${JAVA_HOME})\n        set(MVN_REPO \"-Dmaven.repo.local=${CMAKE_BINARY_DIR}/m2\")\n    endif ()\n    add_custom_target(traas\n        COMMAND ${MVN_COMMAND_PREFIX} ${MVN_EXECUTABLE} ${MVN_REPO} ${MVN_OPTS} --batch-mode -f tools/contributed/traas/pom.xml clean install\n        COMMAND ${CMAKE_COMMAND} -E copy tools/contributed/traas/target/traas-1.1.jar bin/TraaS.jar\n        WORKING_DIRECTORY ${CMAKE_SOURCE_DIR})\n    add_custom_target(lisum\n        COMMAND ${MVN_COMMAND_PREFIX} ${MVN_EXECUTABLE} ${MVN_REPO} ${MVN_OPTS} --batch-mode -f tools/contributed/lisum/pom.xml clean install\n        COMMAND ${CMAKE_COMMAND} -E copy tools/contributed/lisum/lisum-core/target/lisum-core-1.0.2.jar bin/lisum-core.jar\n        COMMAND ${CMAKE_COMMAND} -E copy tools/contributed/lisum/lisum-gui/target/lisum-gui-1.1.jar bin/lisum-gui.jar\n        WORKING_DIRECTORY ${CMAKE_SOURCE_DIR})\n    add_dependencies(lisum traas)\n    set(JAVA_TARGETS traas lisum)\n    foreach (JAVAT ${JAVA_TARGETS})\n        set_property(TARGET ${JAVAT} PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE)\n        set_property(TARGET ${JAVAT} PROPERTY FOLDER \"java\")\n    endforeach ()\nendif ()\n\n# doc targets\nadd_custom_target(doxygen\n    COMMAND rm -rf docs/doxygen\n    COMMAND mkdir docs/doxygen\n    COMMAND doxygen sumo.doxyconf > doxygen.log 2>&1\n    WORKING_DIRECTORY ${CMAKE_SOURCE_DIR})\nset_property(TARGET doxygen PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE)\nset_property(TARGET doxygen PROPERTY FOLDER \"doc\")\n\nadd_custom_target(userdoc\n    COMMAND ../../tools/build_config/buildPyDoc.py -p ../pydoc -c\n    COMMAND mkdocs build -d ../userdoc\n    WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}/docs/web)\nset_property(TARGET userdoc PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE)\nset_property(TARGET userdoc PROPERTY FOLDER \"doc\")\n\nif (MVN_EXECUTABLE)\n    add_custom_target(javadoc\n        COMMAND rm -rf docs/javadoc\n        COMMAND mkdir docs/javadoc\n        COMMAND ${MVN_EXECUTABLE} ${MVN_OPTS} --batch-mode -f tools/contributed/traas/pom.xml javadoc:javadoc\n        COMMAND mv tools/contributed/traas/target/site/apidocs docs/javadoc/traas\n        WORKING_DIRECTORY ${CMAKE_SOURCE_DIR})\n    set_property(TARGET javadoc PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE)\n    set_property(TARGET javadoc PROPERTY FOLDER \"doc\")\n    set(JAVADOC_TARGET javadoc)\nendif ()\n\nfind_program(HELPMAN_EXECUTABLE help2man)\nif (HELPMAN_EXECUTABLE)\n    add_custom_target(man\n        COMMAND rm -rf docs/man\n        COMMAND mkdir docs/man\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"A microscopic, multi-modal traffic simulation\" bin/sumo > docs/man/sumo.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"GUI version of the simulation SUMO\" bin/sumo-gui > docs/man/sumo-gui.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Builds vehicle routes for SUMO using detector values\" bin/dfrouter > docs/man/dfrouter.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Shortest path router and DUE computer for the microscopic traffic simulation SUMO\" bin/duarouter > docs/man/duarouter.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Router for the microscopic traffic simulation SUMO based on junction turning ratios\" bin/jtrrouter > docs/man/jtrrouter.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Import O/D-matrices and trips using macroscopic traffic assignment for SUMO\" bin/marouter > docs/man/marouter.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Generates routes of persons throughout a day for the microscopic traffic simulation SUMO\" bin/activitygen > docs/man/activitygen.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Importer of O/D-matrices for the traffic simulation SUMO\" bin/od2trips > docs/man/od2trips.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Road network importer / builder for the traffic simulation SUMO\" bin/netconvert > docs/man/netconvert.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Road network editor for the traffic simulation SUMO\" bin/netedit > docs/man/netedit.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Road network generator for the microscopic traffic simulation SUMO\" bin/netgenerate > docs/man/netgenerate.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Importer of polygons and POIs for the traffic simulation SUMO\" bin/polyconvert > docs/man/polyconvert.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"TraCITestClient for the traffic simulation SUMO\" bin/TraCITestClient > docs/man/TraCITestClient.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Builds and writes an emissions map for SUMO\\\\\\'s emission models\" bin/emissionsMap > docs/man/emissionsMap.1\n        COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Computes emissions by driving a time line using SUMO\\\\\\'s emission models\" bin/emissionsDrivingCycle > docs/man/emissionsDrivingCycle.1\n        WORKING_DIRECTORY ${CMAKE_SOURCE_DIR})\n    set_property(TARGET man PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE)\n    set_property(TARGET man PROPERTY FOLDER \"doc\")\n    set(MAN_TARGET man)\nendif ()\n\nadd_custom_target(doc)\nadd_dependencies(doc doxygen userdoc ${JAVADOC_TARGET} ${MAN_TARGET})\nset_property(TARGET doc PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE)\nset_property(TARGET doc PROPERTY FOLDER \"doc\")\n\n# coverage targets\nif (COVERAGE)\n    find_program(LCOV_EXECUTABLE lcov)\n    if (LCOV_EXECUTABLE)\n        add_custom_target(lcov\n            COMMAND rm -rf docs/lcov\n            COMMAND mkdir docs/lcov\n            COMMAND ${LCOV_EXECUTABLE} -d . --capture --output-file docs/lcov/lcov.info\n            COMMAND ${LCOV_EXECUTABLE} --remove docs/lcov/lcov.info '/usr/*' --output-file docs/lcov/lcov.info\n            COMMAND genhtml -o docs/lcov/html docs/lcov/lcov.info\n            WORKING_DIRECTORY ${CMAKE_SOURCE_DIR})\n        add_custom_target(lcov-reset\n            COMMAND ${LCOV_EXECUTABLE} -d . -z\n            WORKING_DIRECTORY ${CMAKE_SOURCE_DIR})\n    else ()\n        message(WARNING \"COVERAGE is enabled but lcov was not found.\")\n    endif ()\nendif ()\n\n# testing\nenable_testing()\nadd_subdirectory(src)\nif (GTEST_FOUND)\n    add_subdirectory(unittest)\nendif ()\nif (TEXTTEST_EXECUTABLE AND EXISTS ${CMAKE_SOURCE_DIR}/tests/runCiTests.bat)\n    add_test(NAME texttest COMMAND ${CMAKE_SOURCE_DIR}/tests/runCiTests.bat ${TEXTTEST_EXECUTABLE} $<CONFIG>)\nelse ()\n    find_program(TEXTTEST_FOUND \"texttest\")\n    if (TEXTTEST_FOUND AND EXISTS ${CMAKE_SOURCE_DIR}/tests/runTests.sh)\n        add_test(texttest ${CMAKE_SOURCE_DIR}/tests/runTests.sh -b ci -v ci)\n    endif()\nendif()\n\n# example and dist targets\nadd_custom_target(examples\n    COMMAND ${PYTHON_EXECUTABLE} tools/extractTest.py -x -f tests/examples.txt -p docs/examples/runAll.py\n    WORKING_DIRECTORY ${CMAKE_SOURCE_DIR})\nadd_dependencies(examples sumo netconvert dfrouter duarouter jtrrouter)\nset_property(TARGET examples PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE)\nset_property(TARGET examples PROPERTY FOLDER \"doc\")\n\nfind_program(PYINSTALLER_FOUND \"pyinstaller\")\nif (PYINSTALLER_FOUND AND EXISTS ${CMAKE_SOURCE_DIR}/tools/game/runner.spec)\n    add_custom_target(game\n        COMMAND pyinstaller --noconfirm ${CMAKE_SOURCE_DIR}/tools/game/runner.spec\n    )\nendif()\n\nadd_test(exampletest ${PYTHON_EXECUTABLE} ${CMAKE_SOURCE_DIR}/docs/examples/runAll.py)\nfind_package(Git)\nif (GIT_FOUND)\n    add_custom_target(dist\n        COMMAND rm -rf sumo-${PACKAGE_VERSION} sumo-${PACKAGE_VERSION}.zip sumo-src-${PACKAGE_VERSION}.tar.gz sumo-src-${PACKAGE_VERSION}.zip sumo-gui-macos-${PACKAGE_VERSION}.zip\n        COMMAND ${GIT_EXECUTABLE} archive --prefix sumo-${PACKAGE_VERSION}/ -o sumo-${PACKAGE_VERSION}.zip HEAD\n        COMMAND unzip -q sumo-${PACKAGE_VERSION}.zip\n        COMMAND rm -r sumo-${PACKAGE_VERSION}/tests\n        COMMAND cp -a docs/tutorial docs/examples sumo-${PACKAGE_VERSION}/docs\n        COMMAND find tools/contributed/saga/ tools/contributed/traci4matlab -type f | grep -v .git | xargs cp --parents --target-dir sumo-${PACKAGE_VERSION}\n        COMMAND mkdir sumo-${PACKAGE_VERSION}/include\n        COMMAND cp ${CMAKE_BINARY_DIR}/src/version.h sumo-${PACKAGE_VERSION}/include\n        COMMAND zip -rq sumo-src-${PACKAGE_VERSION}.zip sumo-${PACKAGE_VERSION}\n        COMMAND tar -czf sumo-src-${PACKAGE_VERSION}.tar.gz sumo-${PACKAGE_VERSION}\n        COMMAND cp -a docs/userdoc docs/pydoc docs/javadoc docs/man sumo-${PACKAGE_VERSION}/docs\n        COMMAND rm -r sumo-${PACKAGE_VERSION}/docs/web/docs/images\n        COMMAND ln -s ../../userdoc/images sumo-${PACKAGE_VERSION}/docs/web/docs/images\n        COMMAND tar -czf sumo_${PACKAGE_VERSION}.orig.tar.gz --exclude \"*.jar\" sumo-${PACKAGE_VERSION}\n        COMMAND rm -rf sumo-${PACKAGE_VERSION} sumo-${PACKAGE_VERSION}.zip\n        WORKING_DIRECTORY ${CMAKE_SOURCE_DIR})\n    add_dependencies(dist examples doc ${JAVA_TARGETS})\n    set_property(TARGET dist PROPERTY FOLDER \"CMake\")\n    set_property(TARGET dist PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE)\n\n    add_custom_target(distcheck\n        COMMAND rm -rf sumo-${PACKAGE_VERSION}\n        COMMAND unzip -q sumo-src-${PACKAGE_VERSION}.zip\n        COMMAND cd sumo-${PACKAGE_VERSION} && mkdir _cmake_build _cmake_install && cd _cmake_build\n                && cmake -DCMAKE_INSTALL_PREFIX=../_cmake_install -DCHECK_OPTIONAL_LIBS=${CHECK_OPTIONAL_LIBS} .. || (echo \"ERROR: the cmake configuration failed.\" && false)\n                && make -j8 || (echo \"ERROR: the compilation failed.\" && false)\n                && make test || (echo \"ERROR: the test suite failed.\" && false)\n                && make install || (echo \"ERROR: the install target failed.\" && false)\n        COMMAND rm -rf sumo-${PACKAGE_VERSION}\n        WORKING_DIRECTORY ${CMAKE_SOURCE_DIR})\n    add_dependencies(distcheck dist)\n    set_property(TARGET distcheck PROPERTY FOLDER \"CMake\")\n    set_property(TARGET distcheck PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE)\nendif ()\nmessage(STATUS \"Enabled features: ${ENABLED_FEATURES}\")\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/supervillain",
            "repo_link": "https://github.com/evanberkowitz/supervillain",
            "content": {
                "codemeta": "",
                "readme": "# supervillain\n\n[![DOI](https://zenodo.org/badge/679369801.svg)](https://zenodo.org/badge/latestdoi/679369801)\n\nSupervillain is a python package for studying the Villain model.\n\n# Installation + Development\n\nNavigate to the cloned repo and try\n\n```\npip install .  # for development use pip install -e . \n./test/end-to-end.py\n```\n\nIf pip installation succeeds so too should the example script, which by default samples the (φ, n) formulation of the model on a small lattice.\n\nsupervillain has documentation built with [sphinx](https://www.sphinx-doc.org/en/master/).\nTo build the documentation once you \n\n```\nsphinx-build . _build\n```\n\nand then can open `_build/index.html` in a browser.\nIf you are developing you can replace `sphinx-build` with [`sphinx-autobuild`](https://pypi.org/project/sphinx-autobuild/) to get live updates to the documentation as you go.\n\n",
                "dependencies": "[project]\n\nname = \"supervillain\"\ndescription = \"Quantum monte carlo for the Villain model\"\nauthors = [\n    \n]\n\ndynamic = [\"dependencies\", \"version\"]\n\n[options]\n\ninstall_requires = [\n    ]\n\n[tool.setuptools]\npy-modules = [\n    \"supervillain\",\n]\n\n[tool.setuptools.dynamic]\ndependencies = { file = [\"requirements.txt\"] }\nversion = { attr = \"supervillain.meta.version\" }\n\n# MATH\n\nnumpy>=1.21\nscipy>=1.8\nnumba>=0.60\npandas>=2.1\n\n# IO\n\nh5py>=3.7\ntqdm>=4.64\n\n# TESTING\n\npytest >= 8.0.0\npytest-cov >= 4.0.0\n\n# DOCS\n\nsphinx\nsphinx-git==11.0.0\nsphinx-rtd-theme==1.1.0\nsphinxcontrib-bibtex==2.5.0\nsphinx-math-dollar\nsphinx_toolbox\nsphinx-favicon\npybtex\npybtex-docutils\nnbstripout\n\n# PLOTTING\n\nmatplotlib>=3.5.2\n\n#!/usr/bin/env python\n\nimport setuptools\n\nif __name__ == \"__main__\":\n    setuptools.setup()\n\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/swh-client",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/php-swh-client",
            "content": {
                "codemeta": "{\n    \"@context\": \"https://doi.org/10.5063/schema/codemeta-2.0\",\n    \"@type\": \"SoftwareSourceCode\",\n    \"datePublished\": \"2023-10-06\",\n    \"dateCreated\": \"2023-01-31\",\n    \"description\": \" Software-Heritage API-Client \",\n    \"name\": \"swh-client\",\n    \"license\": \"https://spdx.org/licenses/Apache-2.0.html\",\n    \"dateModified\": \"2024-05-06\",\n    \"softwareVersion\": \"1.0-Beta\",\n    \"codeRepository\": \"https://github.com/Ramy-Badr-Ahmed/swh-client\",\n    \"identifier\": \"https://archive.softwareheritage.org/swh:1:dir:40caa1be82f300bda332b03e69aa4591e3eb2235;origin=https://github.com/Ramy-Badr-Ahmed/swh-client;visit=swh:1:snp:0940a0d4cac24a623a199e8bfa115a54dabf0b70;anchor=swh:1:rev:749a9a916ada5ddf01465773be8862489bc9328a\",\n    \"developmentStatus\": \"active\",\n    \"readme\": \"https://github.com/Ramy-Badr-Ahmed/swh-client/blob/main/README.md\",\n    \"programmingLanguage\": \"PHP\",\n    \"operatingSystem\": \"cross-platform\",\n    \"runtimePlatform\": \"PHP Interpreter\",\n    \"relatedLink\": \"https://1959e979-c58a-4d3c-86bb-09ec2dfcec8a.ka.bw-cloud-instance.org\",\n    \"author\": [\n        {\n            \"@type\": \"Person\",\n            \"givenName\": \"Ramy-Badr\",\n            \"familyName\": \"Ahmed\",\n            \"email\": \"126559907+Ramy-Badr-Ahmed@users.noreply.github.com\"\n        }\n    ],\n    \"keywords\": [\n        \"software-heritage\",\n        \"swhid\",\n        \"software-heritage-api-client\",\n        \"swh-connector\",\n        \"swh-client\",\n        \"software-heritage-dag\",\n        \"swh-metadata\",\n        \"swh-webclient\",\n        \"swh-graph\",\n        \"swh-model\",\n        \"swh-endpoints\",\n        \"swh-api\",\n        \"swh-archiver\"\n    ],\n    \"softwareRequirements\": [\n        \"PHP >=8.0\",\n        \"guzzlehttp/guzzle: >=7.2\"\n    ],\n    \"maintainer\": {\n        \"@type\": \"Person\",\n        \"givenName\": \"Ramy-Badr\",\n        \"familyName\": \"Ahmed\",\n        \"email\": \"126559907+Ramy-Badr-Ahmed@users.noreply.github.com\"\n    },\n    \"applicationCategory\": [\n        \"API Connectors\",\n        \"Research Software\",\n        \"Metadata\"\n    ],\n    \"funder\": {\n        \"@type\": \"Organization\",\n        \"funding\": \"EU-Horizon: 101057264\",\n        \"@id\": \"https://doi.org/10.3030/101057264\",\n        \"name\": \"European Comission\"\n    }\n}\n\n",
                "readme": "![GitHub top language](https://img.shields.io/github/languages/top/Ramy-Badr-Ahmed/swh-client)\n![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/swh-client)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808864.svg)](https://doi.org/10.5281/zenodo.12808864)\n\n[![SWH](https://archive.softwareheritage.org/badge/swh:1:dir:ce683dcced024cb3af1db3b01bbe86f2a9b08028/)](https://archive.softwareheritage.org/swh:1:dir:ce683dcced024cb3af1db3b01bbe86f2a9b08028;origin=https://github.com/Ramy-Badr-Ahmed/swh-client;visit=swh:1:snp:63102a06d859d7d3bcccf1bfe5ade84d8e54e2d5;anchor=swh:1:rev:fb18ecd48c6d62947316845716fc578030ccf749)\n\n# SWH API Client\n\nThis is a PHP API client/connector for [Software Heritage (SWH) web API](https://archive.softwareheritage.org/api/) - currently in Beta phase. The client is wrapped round the [`Illuminate Http package`](https://packagist.org/packages/illuminate/http) and the [`GuzzleHTTP`](https://docs.guzzlephp.org/en/stable/index.html) library.\n\n>[!Note]\n> _**Detailed documentation**_ can be found in the [wiki pages](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki) of this very repository.\n>\n>  A demonstrable version (some features) can be accessed here: <a href=\"https://1959e979-c58a-4d3c-86bb-09ec2dfcec8a.ka.bw-cloud-instance.org/\" target=\"_blank\">**Demo Version**</a>\n>> Working on new features and fixes will be gladly considered. Please feel free to report.\n\n## Installation Steps:\n\n    1) Clone this project.\n    \n    2) Open a console session and navigate to the cloned directory:\n    \n        Run \"composer install\"\n\n        This should involve installing the PHP REPL, PsySH\n\n    3) (Optional) Acquire SWH tokens for increased SWH-API Rate-Limits.\n    \n    4) Prepare .env file and add tokens:   \n    \n        4.1) Rename/Copy the cloned \".env.example\" file to .env\n                cp .env.example .env   \n                \n        4.2) (Optional) Edit these two token keys:\n        \n                SWH_TOKEN_PROD=Your_TOKEN_FROM_SWH_ACCOUNT              # step 3)                 \n                SWH_TOKEN_STAGING=Your_STAGING_TOKEN_FROM_SWH_ACCOUNT   # step 3)                 \n\n    5) (optional) Add psysh to PATH.\n\n## Quickstart:\n\nIn a console session inside the cloned directory, start the php REPL:\n\n```php\n$ psysh     // if not added to PATH replace with: vendor/bin/psysh\n\nPsy Shell v0.12.0 (PHP 8.2.0 — cli) by Justin Hileman\n```\n\nThis will open a REPL console-based session where one can test the functionality of the api classes and their methods before building a suitable workflow/use-cases.\n\n### Presets\n\nAs a one-time configuration parameter, you can set the desired returned data type by SWH (default JSON):\n\n```php\n> namespace Module\\HTTPConnector;\n> use Module\\HTTPConnector;         \n\n> HTTPClient::setOptions(responseType:'object')     // json/collect/object available\n```\n\n> * More details on the default configs: [Default Configurations](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#default-configurations)\n> * More details on further options set: [Preset Configurations](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki).\n\n### Visits\n\nRetrieve Latest Full Visit in the SWH archive:\n\n```php\n> namespace Module\\OriginVisits;\n> use Module\\OriginVisits; \n\n> $visitObject = new SwhVisits('https://github.com/torvalds/linux/');\n\n> $visitObject->getVisit('latest', requireSnapshot: true)\n```\n\n> More details on further swh visits methods: [SwhVisits](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#ii-swhvisits).\n\n### DAG Model:\n\nAs graph Nodes, retrieve node Contents, Edges or find a Path to other nodes (top-bottom):\n\n```php\n> namespace Module\\DAGModel;\n> use Module\\DAGModel; \n\n> $snpNode = new GraphNode('swh:1:snp:bcfd516ef0e188d20056c77b8577577ac3ca6e58')\n\n> $snpNode->nodeHopp()   // node contents\n\n> $snpNode->nodeEdges()  // node edges keyed by the respective name\n\n> $revNode = new GraphNode('swh:1:rev:9cf5bf02b583b93aa0d149cac1aa06ee4a4f655c')\n\n> $revNode->nodeTraversal('deps/nghttp2/lib/includes/nghttp2/nghttp2ver.h.in') //  traverse to a deeply nested file\n```\n\nMore details on:\n\n> * General [Node Methods](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#iii-graphnode).\n> * The Graph methods:\n>   * [Graph contents](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#iv-graphhopping)\n>   * [Graph edges](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#v-graphedges)\n>   * [Graph paths](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#vi-graphtraversal)\n\n### Archive\n\nYou can specify repositories URL w/o paths and archive to SWH using one of the two variants (`static/non-static methods`):\n\n```php\n> namespace Module\\Archival;\n> use Module\\Archival; \n    \n> $saveRequest = new Archive('https://github.com/torvalds/linux/')    // Example 1\n> $saveRequest->save2Swh()\n    \n> $newSaveRequest = Archive::repository('https://github.com/hylang/hy/tree/stable/hy/core')  // Example 2\n\n    // in both cases: the returned POST response contains the save request id and date\n```\n\nEnquire about archival status using the id/date of the archival request (available in the initial POST response)\n\n```php\n> $saveRequest->getArchivalStatus($saveRequestDateOrID)     // current status is returned \n> $saveRequest->trackArchivalStatus($saveRequestDateOrID)   // tracks until archival has succeeded\n```\n\n> More details on further archive methods: [Archive](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#vii-archive).\n\n### EBNF Grammar\n\nValidate a given swhID. `TypeError` is thrown for non-valid swhIDs.\n\n```php\n> namespace Module\\DataType; \n> use Module\\DataType; \n         \n$snpID = new SwhcoreId('swh:1:snp:bcfd516ef0e188d20056c77b8577577ac3ca6e5Z') // throws TypeError Exception\n```\n> Full details of the SWHID persistent Identifiers: [Syntax](https://docs.softwareheritage.org/devel/swh-model/persistent-identifiers.html#syntax)\n\n>[!Note]\n> Todo: Core identifiers with qualifiers.\n\n### MetaData\n\nReturns a list of metadata authorities that provided metadata on the given target\n\n```php\n> namespace Module\\MetaData;\n> use Module\\MetaData; \n\n> SwhMetaData::getOriginMetaData('https://github.com/torvalds/linux/')\n```\n\n> More details on further metadata methods: [Metadata](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#viii-metadata).\n\n",
                "dependencies": "{\n  \"name\": \"swh/client\",\n  \"type\": \"project\",\n  \"description\": \"SWH connector and client that interacts with SWH data model. Part of the FAIRCORE4EOSC project for LZI-Dagstuhl\",\n  \"keywords\": [\"software-heritage\", \"archive\", \"metadata\"],\n  \"license\": \"MIT\",\n  \"require\": {\n    \"php\": \"^8.2\",\n    \"psy/psysh\": \"^0.12.0\",\n    \"illuminate/support\": \"^10.47\",\n    \"illuminate/validation\": \"^10.43\",\n    \"guzzlehttp/guzzle\": \"^7.8\",\n    \"php-ds/php-ds\": \"^1.5\",\n    \"illuminate/http\": \"^10.47\",\n    \"illuminate/log\": \"^10.47\"\n  },\n  \"autoload\": {\n    \"psr-4\": {\n      \"Module\\\\\": \"Module/\"\n    }\n  },\n  \"minimum-stability\": \"dev\",\n  \"prefer-stable\": true\n}\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/t8code",
            "repo_link": "https://github.com/DLR-AMR/t8code",
            "content": {
                "codemeta": "",
                "readme": "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7034838.svg)](https://doi.org/10.5281/zenodo.7034838)\n[![t8code CI](https://github.com/DLR-AMR/t8code/actions/workflows/tests_cmake_testsuite.yml/badge.svg)](https://github.com/DLR-AMR/t8code/actions/workflows/tests_cmake_testsuite.yml)\n\n<p align=\"center\">\n  <img width=\"300px\" src=t8code_logo.png>\n</p>\n\n### Introduction\n\nt8code (spoken as \"tetcode\") is a C/C++ library to manage parallel adaptive meshes with various element types.\nt8code uses a collection (a forest) of multiple connected adaptive space-trees in parallel and scales to at least one million MPI ranks and over 1 Trillion mesh elements.\nIt is licensed under the GNU General Public License 2.0 or later. Copyright (c) 2015 the developers.\n\nt8code is intended to be used as a thirdparty library for numerical simulation codes or any other applications that require meshes.\n\n<table>\n    <tr>\n        <td><img src=\"https://github.com/DLR-AMR/t8code/blob/main/doc/pictures/cmesh_tet_holes.png?raw=true\" height=\"200\" /></td> \n        <td><img src=\"https://github.com/DLR-AMR/t8code/blob/main/doc/pictures/flow_around_circle_sim2.jpg?raw=true\" height=\"181\" /></td>\n    </tr>\n      <tr>\n        <td><img src=\"https://github.com/DLR-AMR/t8code/blob/main/doc/pictures/mesh_3d_hybrid_cutout.jpg?raw=true\" height=\"200\" /></td>\n        <td><img src=\"https://github.com/DLR-AMR/t8code/blob/main/doc/pictures/AirplaneWithTetMesh.png?raw=true\" height=\"200\" /></td>\n    </tr>\n</table>\n\nt8code, or T8 for short, supports the following element types (also different types in the same mesh):\n\n- 0D: vertices\n- 1D: lines\n- 2D: quadrilaterals and triangles\n- 3D: hexahedra, tetrahedra, prisms and pyramids\n\nAmong others, t8code offers the following functionalities:\n\n- Create distributed adaptive meshes over complex domain geometries\n- Adapt meshes according to user given refinement/coarsening criteria\n- Establish a 2:1 balance\n- (Re-)partition a mesh (and associated data) among MPI ranks\n- Manage ghost (halo) elements and data\n- Hierarchical search in the mesh\n- Curved mesh elements\n\nt8code uses space-filling curves (SFCs) to manage the adaptive refinement and efficiently store the mesh elements and associated data.\nA modular approach makes it possible to exchange the underlying SFC without changing the high-level algorithms.\nThus, we can use and compare different refinement schemes and users can implement their own refinement rules if so desired.\n\nCurrently t8code offers the following implementations by default:\n  - lines use a 1D Morton curve with 1:2 refinement\n  - quadrilateral/hexahedral elements are inherited from the p4est submodule, using the Morton curve 1:4, 1:8 refinement; \n  - triangular/tetrahedral are implemented using the Tetrahedral Morton curve, 1:4, 1:8 refinement;\n  - prisms are implemented using the triangular TM curve and a line curve, 1:8 refinement.\n  - pyramids are implemented using the Pyramidal Morton curve and the TM curve for its tetrahedral children, 1:10 (for pyramids) / 1:8 (for tetrahedra) refinement.\n  - The code supports hybrid meshes including any of the above element types (of the same dimension).\n\nYou find more information on t8code in the [t8code Wiki](https://github.com/DLR-AMR/t8code/wiki).\n\nFor a brief introduction in AMR and the algorithms used by t8code we recommend to read our [overview paper](https://elib.dlr.de/194377/1/t8code_overview_IMR2023.pdf).\n\n### Setup\n\nWe provide a short guide to install t8code in our Wiki [Installation guide](https://github.com/DLR-AMR/t8code/wiki/Installation).\n\n  \n### Getting started\n  \n  To get familiar with t8code and its algorithms and data structures we recommend executing the tutorial examples in `tutorials`\n  and read the corresponding Wiki pages starting with [Step 0 - Helloworld](https://github.com/DLR-AMR/t8code/wiki/Step-0---Hello-World).\n  \n  A sophisticated example of a complete numerical simulation is our finite volume solver of the advection equation in `example/advection`.\n\n\n### Documentation\n\nt8code uses [Doxygen](https://doxygen.nl/) to generate the code documentation. You can build the documentation with\n\n```\nmake doxygen\n```\n\nand then find the generated files in the `/doc` subfolder.\n\nYou can also find the documentation of our releases on the [t8code website](https://dlr-amr.github.io/t8code/pages/documentation.html).\n\n### License and contributing\nt8code is licensed under GPLv2 (see [COPYING](COPYING)). We appreciate\ncontributions from the community and refer to [CONTRIBUTING.md](CONTRIBUTING.md)\nfor more details.\n\nNote that we strive to be a friendly, inclusive open-source\ncommunity and ask all members of our community to adhere to our\n[`CODE_OF_CONDUCT.md`](CODE_OF_CONDUCT.md). \n\nTo get in touch, [open an issue](https://github.com/DLR-AMR/t8code/issues/new)\nor write an email to one of the principal developers.\n\n### Julia wrapper\n\nWe offer [T8code.jl](https://github.com/DLR-AMR/T8code.jl) - an official\n[Julia](https://julialang.org/) package allowing to call t8code routines from\nthe [Julia](https://julialang.org/) programming language. From within a Julia\nsession do\n```julia\njulia> import Pkg; Pkg.add([\"T8code\", \"MPI\"])\n```\nto install the package on your system.\n\n### Publications\n  \n  An (incomplete) list of publications related to t8code:\n    \n  [1] **Overview Paper**: \n  Holke, Johannes and Burstedde, Carsten and Knapp, David and Dreyer, Lukas and Elsweijer, Sandro and Ünlü, Veli and Markert, Johannes and Lilikakis, Ioannis and Böing, Niklas and Ponnusamy, Prasanna and Basermann, Achim  (2023) *t8code v. 1.0 - Modular Adaptive Mesh Refinement in the Exascale Era*. SIAM International Meshing Round Table 2023, 06.03.2023 - 09.03.2023, Amsterdam, Niederlande. \n  [Full text available](https://elib.dlr.de/194377/1/t8code_overview_IMR2023.pdf)\n  \n    \n  [2] **Original PhD thesis**: \n  Holke, Johannes *Scalable algorithms for parallel tree-based adaptive mesh refinement with general element types*, PhD thesis at University of Bonn, 2018,\n      [Full text available](https://bonndoc.ulb.uni-bonn.de/xmlui/handle/20.500.11811/7661)\n   \n      \n  [3] **Tetrahedral and triangular Space-filling curve**:\n  Burstedde, Carsten and Holke, Johannes *A Tetrahedral Space-Filling Curve for Nonconforming Adaptive Meshes*, SIAM Journal on Scientific Computing, 2016, [10.1137/15M1040049](https://epubs.siam.org/doi/10.1137/15M1040049)\n  \n  \n  [4] **Coarse mesh partitioning**:\n  Burstedde, Carsten and Holke, Johannes *Coarse mesh partitioning for tree-based AMR*, SIAM Journal on Scientific Computing, 2017, [10.1137/16M1103518](https://epubs.siam.org/doi/10.1137/16M1103518)\n  \n  \n  [5] **Ghost computation**:\n  Holke, Johannes and Knapp, David and Burstedde, Carsten *An Optimized, Parallel Computation of the Ghost Layer for Adaptive Hybrid Forest Meshes*, SIAM Journal on Scientific Computing, 2021, [10.1137/20M1383033](https://epubs.siam.org/doi/abs/10.1137/20M1383033)\n \n  \n  [6] **Geometry controlled refinement for hexahedra**:\n  Elsweijer, Sandro and Holke, Johannes and Kleinert, Jan and Reith, Dirk  (2022) *Constructing a Volume Geometry Map for Hexahedra with Curved Boundary Geometries*.   In: SIAM International Meshing Roundtable Workshop 2022.  SIAM International Meshing Roundtable Workshop 2022, 22. - 25. Feb. 2022, [Full text available](https://elib.dlr.de/186570/1/ConstructingAVolumeGeometryMapForHexahedraWithCurvedBoundaryGeometries.pdf) \n  \n### Theses with t8code relations\n\n  An (incomplete) list of theses written with or about t8code:\n  \n\n  [A] **Prism space-filling curve**: \n  Knapp, David (2017) *Adaptive Verfeinerung von Prismen*. Bachelor's thesis, Rheinische Friedrich-Wilhems-Universität Bonn.\n  \n  \n  [B] **Pyramidal space-filling curve**: \n  Knapp, David (2020) *A space-filling curve for pyramidal adaptive mesh refinement*. Master's thesis, Rheinische Friedrich-Wilhems-Universität Bonn. [Full text available](https://www.researchgate.net/publication/346789160_A_space-filling_curve_for_pyramidal_adaptive_mesh_refinement)\n  \n   \n  [C] **DG solver based on t8code**: \n  Dreyer, Lukas (2021) *The local discontinuous galerkin method for the advection-diffusion equation on adaptive meshes*.  Master's thesis, Rheinische Friedrich-Wilhems-Universität Bonn.\n  [Full text available](https://elib.dlr.de/143969/1/masterthesis_dreyer.pdf) \n  \n  \n  [D] **Geometry controlled refinement for hexahedra (Part 1)**: \n  Elsweijer, Sandro (2021) *Curved Domain Adaptive Mesh Refinement with Hexahedra*.  Tech report, Hochschule Bonn-Rhein-Sieg.\n  [Full text available](https://elib.dlr.de/186571/1/masterprojekt-2_elsweijer_ABGABEVERSION_TITEL.pdf)\n  \n\n  [E] **Subelement and resolving hanging faces in 2D**: \n  Becker, Florian (2021) *Removing hanging faces from tree-based adaptive meshes for numerical simulation*, Master's thesis, Universität zu Köln.\n  [Full text available](https://elib.dlr.de/187499/1/RemovingHangingFacesFromTreeBasedAMR.pdf)\n  \n  \n  [F] **Coarsening as post-processing to reduce simulation file size**: \n  Spataro, Luca  (2021) *Lossy data compression for atmospheric chemistry using adaptive mesh coarsening*.  Master's thesis, Technische Universität München.\n  [Full text available](https://elib.dlr.de/144997/1/master-thesis-final-spataro.pdf)\n \n  \n  [G] **Geometry controlled refinement for hexahedra (Part 2)**: \n  Elsweijer, Sandro (2022) *Evaluation and generic application scenarios for curved hexahedral adaptive mesh refinement*.  Master's thesis, Hochschule Bonn-Rhein-Sieg.  [10.13140/RG.2.2.34714.11203](<https://doi.org/10.13140/RG.2.2.34714.11203>) [Full text available](https://elib.dlr.de/186561/1/sandro_elsweijer-evaluation_and_generic_application_scenarios_for_curved_hexahedral_adaptive_mesh_refinement.pdf)\n  \n  \n  [H] **Multigrid and other preconditioners for DG**: \n  Böing, Niklas  (2022) *Evaluation of preconditioners for implicit solvers of local DG for the advection-diffusion equation* (*Untersuchung von Präkonditionierern für implizite Löser für das Local DG-Verfahren zur Lösung der Advektions-Diffusionsgleichung*).  Master's thesis, Universität zu Köln.\n[Full text available](https://elib.dlr.de/186347/1/Untersuchung%20von%20Pr%C3%A4konditionierern%20f%C3%BCr%20implizite%20L%C3%B6ser%20f%C3%BCr%20das%20Local%20DG-Verfahren%20zur%20L%C3%B6sung%20der%20Advektions-Diffusionsgleichung.pdf)\n  \n\n  [I] **Removing elements from the mesh (cutting holes)**: \n  Lilikakis, Ioannis  (2022) *Algorithms for tree-based adaptive meshes with incomplete trees*.  Master's thesis, Universität zu Köln.    \n [Full text may be available in future](https://elib.dlr.de/191968/)\n \n  [J] **Curved tetrahedra**:\n  Fussbroich Jakob (2023) *Towards high-order, hybrid adaptive mesh refinement: Implementation and evaluation of curved unstructured mesh elements*. Master's thesis. Technische Hochschule Köln.\n  [Full text available](https://elib.dlr.de/200442/)\n\n  [K] **Hanging node resolution 3D**:\n  Tabea Leistikow (2024) *Derivation and implementation of a hanging nodes resolution scheme for hexahedral non-conforming meshes in t8code*. Master's thesis, Universität zu Köln.\n  Full text currently not available.\n\n  ### Citing t8code\n  \n  If you use t8code in any of your publications, please cite the [github repository](https://doi.org/10.5281/zenodo.7034838), [1] and [2]. For publications specifically related to \n- **the tetrahedral index**, please cite [3].\n- **coarse mesh partitioning**, please cite [4].\n- **construction and handling of the ghost layer**, please cite [5].\n- **geometry controlled refinement**, please cite [6] (general) and [J] (tetrahedral).\n- **hanging node resolution and/or subelements**, please cite [E] and [K].\n\nIf you use any functionality described in the theses, we encourage you to cite them as well.\n\n",
                "dependencies": "cmake_minimum_required( VERSION 3.16 )\n\ninclude(cmake/GitProjectVersion.cmake)\n\nproject( T8CODE\n  DESCRIPTION \"Parallel algorithms and data structures for tree-based AMR with arbitrary element shapes.\"\n  LANGUAGES C CXX\n  VERSION \"${T8CODE_VERSION_MAJOR}.${T8CODE_VERSION_MINOR}.${T8CODE_VERSION_PATCH}\" )\ninclude( CTest )\n\noption( T8CODE_BUILD_AS_SHARED_LIBRARY \"Whether t8code should be built as a shared or a static library\" ON )\noption( T8CODE_BUILD_PEDANTIC \"Compile t8code with `-pedantic` as done in the Github CI.\" OFF )\noption( T8CODE_BUILD_WALL \"Compile t8code with `-Wall` as done in the Github CI.\" OFF )\noption( T8CODE_BUILD_WERROR \"Compile t8code with `-Werror` as done in the Github CI.\" OFF )\noption( T8CODE_BUILD_EXTRA_WARNINGS \"Compile t8code with extra warnings as done in the Github CI.\" OFF )\noption( T8CODE_EXPORT_COMPILE_COMMANDS \"Export the compile commands as json. Can be used by IDEs for code completion (e.g. intellisense, clangd)\" OFF )\noption( T8CODE_BUILD_TESTS \"Build t8code's automated tests\" ON )\noption( T8CODE_BUILD_TUTORIALS \"Build t8code's tutorials\" ON )\noption( T8CODE_BUILD_EXAMPLES \"Build t8code's examples\" ON )\noption( T8CODE_BUILD_BENCHMARKS \"Build t8code's benchmarks\" ON )\noption( T8CODE_BUILD_FORTRAN_INTERFACE \"Build t8code's Fortran interface\" OFF )\noption( T8CODE_ENABLE_LESS_TESTS \"Tests not as thoroughly to speed up the test suite. Tests the same functionality. (WARNING: Use with care.)\" OFF )\n\noption( T8CODE_ENABLE_MPI \"Enable t8code's features which rely on MPI\" ON )\noption( T8CODE_ENABLE_VTK \"Enable t8code's features which rely on VTK\" OFF )\noption( T8CODE_ENABLE_OCC \"Enable t8code's features which rely on OpenCASCADE\" OFF )\noption( T8CODE_ENABLE_NETCDF \"Enable t8code's features which rely on netCDF\" OFF )\n\noption( T8CODE_USE_SYSTEM_SC \"Use system-installed sc library\" OFF )\noption( T8CODE_USE_SYSTEM_P4EST \"Use system-installed p4est library\" OFF )\n\noption( T8CODE_BUILD_DOCUMENTATION \"Build t8code's documentation\" OFF )\n\ninclude(CMakeDependentOption)\ncmake_dependent_option( T8CODE_BUILD_DOCUMENTATION_SPHINX \"Build t8code's documentation using sphinx\" OFF \"T8CODE_BUILD_DOCUMENTATION\" OFF )\n\nset(T8CODE_CUSTOM_PARALLEL_TEST_COMMAND \"\" CACHE STRING \"Define a custom command for parallel tests , e.g.: mpirun -np 8 (overwrites standard mpirun -np 4 if build with mpi)\")\nset(T8CODE_CUSTOM_SERIAL_TEST_COMMAND \"\" CACHE STRING \"Define a custom command for serial tests.\")\n\n# Set a default build type if none was specified\nset(default_build_type \"Release\")\n\nif(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n  message(STATUS \"Setting build type to '${default_build_type}' as none was specified.\")\n  set(CMAKE_BUILD_TYPE \"${default_build_type}\" CACHE\n      STRING \"Choose the type of build. Build types available: Release Debug RelWithDebInfo\" FORCE)\n  # Set the possible values of build type for cmake-gui\n  set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS\n    \"Debug\" \"Release\" \"RelWithDebInfo\")\nendif()\n\nif( NOT DEFINED CMAKE_C_STANDARD ) \n  set( CMAKE_C_STANDARD 11 )\nelseif( CMAKE_C_STANDARD LESS 11 )\n  message( FATAL_ERROR \"The CMAKE_C_STANDARD variable has been set to ${CMAKE_C_STANDARD}, but t8code requires C11\" )\nendif()\nset( CMAKE_C_STANDARD_REQUIRED ON )\nset( CMAKE_C_EXTENSIONS OFF )\n\nif( NOT DEFINED CMAKE_CXX_STANDARD ) \n  set( CMAKE_CXX_STANDARD 20 )\nelseif( CMAKE_CXX_STANDARD LESS 20 )\n  message( FATAL_ERROR \"The CMAKE_CXX_STANDARD variable has been set to ${CMAKE_CXX_STANDARD}, but t8code requires C++20\" )\nendif()\nset( CMAKE_CXX_STANDARD_REQUIRED ON )\nset( CMAKE_CXX_EXTENSIONS OFF )\n\nset(CMAKE_MODULE_PATH \"${PROJECT_SOURCE_DIR}/cmake\" ${CMAKE_MODULE_PATH})\n\nif( T8CODE_BUILD_FORTRAN_INTERFACE )\n    enable_language( Fortran )\nendif()\n\nif( T8CODE_ENABLE_MPI )\n    if( T8CODE_BUILD_FORTRAN_INTERFACE )\n      find_package( MPI COMPONENTS C Fortran REQUIRED )\n    else()\n      find_package( MPI COMPONENTS C REQUIRED )\n    endif()\n\n    if( NOT MPIEXEC_EXECUTABLE )\n        message( FATAL_ERROR \"MPIEXEC was not found\" )\n    endif()\n    set( mpi ON ) # This is very dirty and will be fixed in the libsc repo (https://github.com/cburstedde/libsc/pull/178)\n    # set( SC_ENABLE_MPI ON ) # When the fix gets merged, replace the previous line with this one\nendif()\n\nif( T8CODE_ENABLE_VTK )\n    find_package( VTK REQUIRED COMPONENTS\n        IOXML CommonExecutionModel CommonDataModel\n        IOGeometry IOXMLParser IOParallelXML IOPLY\n        ParallelMPI FiltersCore vtksys CommonCore zlib IOLegacy)\n    if(VTK_FOUND)\n        message(\"Found VTK\")\n    endif (VTK_FOUND)\nendif( T8CODE_ENABLE_VTK )\n\nif( T8CODE_ENABLE_OCC )\n    find_package( OpenCASCADE REQUIRED COMPONENTS\n    TKBO TKPrim TKTopAlgo\n    TKGeomAlgo TKBRep\n    TKG3d TKG2d TKMath TKernel )\n    if(OpenCASCADE_FOUND)\n        message(\"Found OpenCASCADE\")\n    endif (OpenCASCADE_FOUND)\nendif( T8CODE_ENABLE_OCC )\n\nif( T8CODE_ENABLE_NETCDF )\n    find_package( netCDF REQUIRED )\n    if(netCDF_FOUND)\n        message(\"Found netCDF\")\n        include(cmake/CheckNetCDFPar.cmake)\n    endif (netCDF_FOUND)\nendif( T8CODE_ENABLE_NETCDF )\n\n# Override default for this libsc option\nset( BUILD_SHARED_LIBS ON CACHE BOOL \"Build libsc as a shared library\" )\n\n# Prevent `libsc` and `p4est` from overwriting the default install prefix.\nset(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT FALSE)\n\n# Rpath options necessary for shared library install to work correctly in user projects.\nset(CMAKE_INSTALL_NAME_DIR ${CMAKE_INSTALL_PREFIX}/lib)\nset(CMAKE_INSTALL_RPATH ${CMAKE_INSTALL_PREFIX}/lib)\nset(CMAKE_INSTALL_RPATH_USE_LINK_PATH true)\n\nif ( T8CODE_USE_SYSTEM_SC )\n    find_package( SC REQUIRED PATHS /path/to/system/sc )\nelse()\n    add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/sc )\nendif()\n\nif ( T8CODE_USE_SYSTEM_P4EST )\n    find_package( P4EST REQUIRED PATHS /path/to/system/p4est )\nelse()\n    add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/p4est )\nendif()\n\nadd_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/src )\n\nif ( T8CODE_BUILD_TESTS )\n    add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/test )\nendif()\n\nif ( T8CODE_BUILD_TUTORIALS )\n    add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/tutorials )\nendif()\n\nif ( T8CODE_BUILD_EXAMPLES )\n    add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/example )\nendif()\n\nif ( T8CODE_BUILD_BENCHMARKS )\n    add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/benchmarks )\nendif()\n\nif ( T8CODE_BUILD_DOCUMENTATION )\n    add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/doc )\nendif()\n\nif( T8CODE_BUILD_FORTRAN_INTERFACE )\n    add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/api/t8_fortran_interface )\n\n    if( NOT T8CODE_ENABLE_MPI )\n        message( FATAL_ERROR \"Fortran API only available when MPI is enabled.\" )\n    endif()\nendif()\n\ninclude (cmake/CPackConfig.cmake)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tamarin-prover",
            "repo_link": "https://github.com/tamarin-prover/tamarin-prover",
            "content": {
                "codemeta": "",
                "readme": "The Tamarin prover repository\n=============================\n[![master branch build-status](https://travis-ci.org/tamarin-prover/tamarin-prover.svg?branch=develop)](https://travis-ci.org/tamarin-prover/tamarin-prover)\n\nThis README describes the organization of the repository of the Tamarin prover\nfor security protocol verification. Its intended audience are interested\nusers and future developers of the Tamarin prover. For installation\nand usage instructions of the Tamarin prover see chapter 2 of the manual:\nhttps://tamarin-prover.github.io/manual/master/book/002_installation.html\n\n\nDeveloping and contributing\n---------------------------\n\nSee [contributing instructions](CONTRIBUTING.md) for instructions on how to develop,\ntest and release changes to the Tamarin prover source code.\n\n\nVersion Numbering Policy\n-----------------------\n\nWe use version numbers with four components.\n\n - The first component is the major version number. It indicates complete\n   rewrites of the codebase.\n - The second component is the minor version number. We use odd minor version\n   numbers to denote development releases intended for early adopters. We use\n   even minor version numbers to denote public releases, which are also\n   published.\n - The third component indicates bugfix releases.\n - The fourth component indicates documentation and meta-data changes.\n\nWe ensure that the external interface of a version of the Tamarin prover is backwards\ncompatible with the external interface of all versions that agree on the major\nand minor version number.\n\nWe announce all releases of the Tamarin prover on:\nhttp://tamarin-prover.github.io\n\n\nManual\n------\n\nThe manual is available as PDF or HTML at https://tamarin-prover.github.io/manual/index.html\n\nExperimental improved graph output\n----------------------------------\n\nYou can use our experimental improved graph output which may be\nhelpful for very large graphs that can be created for complicated\nprotocols. To enable this feature read the instructions about\n[improved graphs](/misc/cleandot/README.md).\n\nSpthy code editors\n------------------\n\nThe project contains support for spthy syntax highlighting and support\nin the [etc](/etc/) directory. This includes support for [Sublime Text](/etc/SUBLIME_TEXT.md), [VIM](/etc/spthy.vim) and [Notepad++](/etc/notepad_plus_plus_spthy.xml).\n\nExternal tools\n------------------\n\nExternal tools may use the [Tree-sitter](https://tree-sitter.github.io/tree-sitter/) grammar\nin the [tree-sitter/](/tree-sitter/) directory.\n\n\nExample Protocol Models\n-----------------------\n\nAll example protocol models are found in the directory\n\n    ./examples/\n\nAll models that we consider stable\nare part of every installation of the Tamarin prover. See\n`tamarin-prover.cabal` for the list of installed protocols. We use the\nfollowing sub-directories to organize the models.\n\n~~~~\naccountability/ case studies using the accountability implementation presented in\n                the \"Verifying Accountability for Unbounded Sets of Participants\" paper\ncsf12/          the AKE case studies from our CSF'12 paper.\nclassic/        classic security protocols like the ones from\n                [SPORE](http://www.lsv.ens-cachan.fr/Software/spore/table.html)\nloops/          experiments for testing loop-invariants and protocols with\n                non-monotonic state\nrelated_work/   examples from related work on protocols with loops or\n                non-monotonic state\nexperiments/    all other experiments\nake/            more AKE examples including ID-based and tripartite group KE\n                protocols based on bilinear pairing\nfeatures/       (small) models that demonstrate a given feature\nccs15/\t        the observational equivalence case studies from our CCS'15 paper\ncsf-18/         the XOR case studies from the CSF'18 paper\n~~~~\n\nFeel free to add more sub-directories and describe them here.\n\nIn general, we try use descriptive names for files containing the models. We\nalso document all our findings as comments in the protocol model.  Moreover,\nwe use the following header in all files to make their context more explicit.\n\n~~~~\n/*\n   Protocol:    Example\n   Modeler:     Simon Meier, Benedikt Schmidt\n   Date:        January 2012\n\n   Status:      working\n\n   Description of protocol.\n\n*/\n~~~~\n\n",
                "dependencies": "flags: {}\npackages:\n- '.'\n- lib/theory/\n- lib/term/\n- lib/utils/\n- lib/sapic/\n- lib/export/\n- lib/accountability/\nresolver: lts-22.39\nghc-options:\n  \"$everything\": -Wall\nnix:\n  packages: [ zlib ]\n\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tbt-segmentation",
            "repo_link": "https://github.com/DLR-FT/TBT-Segmentation",
            "content": {
                "codemeta": "",
                "readme": "<!--\nSPDX-FileCopyrightText: 2023 German Aerospace Center (DLR)\n\nSPDX-License-Identifier: CC-BY-NC-ND-4.0\n-->\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13807484.svg)](https://doi.org/10.5281/zenodo.13807484)\n\n> This tool is based on the paper [Temporal Behavior Trees: Robustness and Segmentation](https://doi.org/10.1145/3641513.3650180) and its companion poster [Temporal Behavior Trees - Segmentation](https://doi.org/10.1145/3641513.3652534), which were published at [HSCC'24](https://hscc.acm.org/2024/).\n\n> If you encounter any issues, have questions, or need assistance, feel free to reach out:  sebastian dot schirmer at dlr dot de\n\n## Table of Content\n- [Temporal Behavior Trees: Robustness and Segmentation](#temporal-behavior-trees-robustness-and-segmentation)\n  - [Getting Started](#getting-started)\n  - [Folder Structure](#folder-structure)\n  - [Brief Summary of the Supported Operators](#brief-summary-of-the-supported-operators)\n  - [Docker Environment](#docker-environment)\n  - [How to Interpret the Output Format](#how-to-interpret-the-output-format)\n  - [Contributors](#contributors)\n  - [Contributing](#contributing)\n  - [Changes](#changes)\n  - [License](#license)\n\n# Temporal Behavior Trees: Robustness and Segmentation\n\nTemporal Behavior Trees (TBT) are a specification formalism for monitoring behaviors.\nThey are inspired by behavior trees that are commonly used to program robotic applications, but allow to specify temporal properties in their leaf nodes. \nTherefore, they can be easily retrofitted to existing behavior trees.\n\nFor instance, consider the following behavior tree that specifies the landing sequence of an unmanned aircraft (1) *move to position*, (2) *stay in position*, (3) *move to touchdown*, and (4) *descend*:\n\n<p align=\"center\">\n<img src=\"https://github.com/DLR-FT/TBT-Segmentation/blob/main/figs/BehaviorTree.JPG\" width=\"400\">\n</p>\n\nGiven such a TBT specification and a trace, i.e., a sequence of events of a system, we can compute the corresponding robustness.\nRobustness provides an quantitative interpretation *how* much the TBT specification was satisfied or violated.\n\nFurther, we can use a TBT specification to segment a trace.\nThat means that we assign portions of the provided specification to segments of the given trace.\nSuch a segmentation then helps to better explain which portions of the specification were satisfied or violated.\n\nIt is also useful to visualize the resulting segmentation, as shown below for the landing maneuver:\n\n<p align=\"center\">\n<img src=\"https://github.com/DLR-FT/TBT-Segmentation/blob/main/figs/Segmentation.png\" width=\"400\">\n</p>\n\n## Getting Started\nRequires Rust to compile source code and Python for visualization.\n1. [Install Rust](https://www.rust-lang.org/)\n1. Specify a TBT, e.g., as done [here](specification/shiplanding_formula.tbt). The grammar can be found [here](src/tbt.pest)\n1. [Provide a Trace by implementing ``get_trace``](src/user_defined/get_trace.rs)\n2. [Replace the ``user_defined``-function by your own (Line 5)](src/lib.rs)\n3. Call ``cargo build`` or ``cargo build --release`` \n4. Call ``cargo run -- --help`` to get help on the command-line-usage\n5. Call ``cargo test`` to see if the tests are successful\n \nFor instance:\n\n``cargo run --release -- -u -s specification/shiplanding_formula_combined.tbt -f ./res/logs_wind_front_Lateral/`` runs segmentation using subsampling on a provided logfile. \nFor this example, ``get_trace`` is already provided.\n\nUsing the [visualization script](scripts/visualize_ship_landing.py), we can easily plot a segmentation by, e.g., ``python visualize_ship_landing.py plot -b Lateral -s 5000 10000 20000 -e 0 -l ../res/logs_wind_front_Lateral/`` where ``5000, 10000, 20000`` represent beginning of segments (omitting 0), ``-b`` states the expected behavior and is used to plot the dotted lines, and ``-e`` represents the number of skipped entries due to subsampling. There is also the option to save a plot to inspect it in a docker environment using ``-p``.\nWe can also replay the flight by, e.g.,  ``python visualize_ship_landing.py live -l ../res/logs_wind_front_Lateral/ -b Lateral -f 0.005 0.1 2.0``.\n\nFor more information call ``python visualize_ship_landing.py --help``.\n\n> Using ``--toml <FILE>`` as command-line-argument generates a .toml-file for the computed segmentations.\n\n## Folder Structure\n- [figs](figs) are resources used for this readme document\n- [res](res) contains the logfiles used in the HSCC paper\n  - The logfolder name specifies the wind direction (*front* or *side*) and the anticipated maneuver (*45Deg*, *Lateral*, *Oblique*, or *Straight*)\n  - Each flight consists of two csv-files: one for the ship and one for the aircraft\n  - The files contain the position, the velocity, and the angles for the ship and the aircraft\n- [scripts](scripts) provides auxiliary resources\n  - [Makefile](scripts/Makefile) is used for Rust profiling\n  - [clean.sh](scripts/clean.sh) is used for cleaning up the repository\n  - [infer_parameters_visualization.py](scripts/infer_parameters_visualization.py) is used for the [run.sh](scripts/run.sh) script to extract the segments from the produced output files\n  - [run.sh](scripts/run.sh) is a script that executes our segmentation tool on all the available [logfiles](res) and produces png files to further analyze ([infer_parameters_visualization.py](scripts/infer_parameters_visualization.py))\n  - [visualize_ship_landing.py](scripts/visualize_ship_landing.py) is a script that is used to produced the png files that show the flight and the computed segments\n- [specification](specification) contains some example specifications  \n- [src](src) contains the source code  \n    - [lib.rs](src/lib.rs) provides a package of TBTs functions that can be used by others \n      - It requires a user to provide two functions [get_trace()](src/lib.rs) and [get_events_per_second()](src/lib.rs)\n      - This repository provides these functions for the ship landing: [user_defined/](src/user_defined/)\n    - [main.rs](src/main.rs) is an example that uses [lib.rs](src/lib.rs) and the user-defined functions [tree/](src/tree/)\n    - [stl.rs](src/stl.rs) provides the syntax and semantics for STL formulas\n    - [behaviortree.rs](src/behaviortree.rs) provides the syntax and semantics for TBTs\n    - [command_line_parser.rs](src/command_line_parser.rs) is used to interface with the command line\n    - [functions.rs](src/functions.rs) is used to represent atomic functions that are used by the TBT parser\n    - [parser.rs](src/parser.rs) is a TBT parser that reads a .tbt-file and produces a ```Tree```\n    - [tbt.pest](src/tbt.pest) represents the grammar used by the TBT parser\n    - [toml_out.rs](src/toml_out.rs) is used to produce a .toml-file\n    - [csv_reader.rs](src/csv_reader.rs) represent auxiliary functions such as reading a csv-file\n    - [table.rs](src/table.rs) represents the main data structure for the dynamic programming\n    - [user_defined/](src/user_defined/) is an example implementation for the *UserProvidedFunctions* required by [lib.rs](src/lib.rs)\n      - [get_trace.rs](src/user_defined/get_trace.rs) reads the trace data used in the experiment (ie it reads a csv-file and provides a ``Trace``)\n- [tests](tests/) contains multiple test cases that can be executed to test whether the compilation works\n- [Dockerfile](Dockerfile) just c/p the whole repository and builds it to produce a docker container that then can run [run.sh](scripts/run.sh) to procude the HSCC artifacts\n\n> To use the TBT tool for a different use-case, a user needs to provide the *UserProvidedFunction* ([get_trace()](src/lib.rs) and [get_tree()](src/lib.rs)) similar to what has been done here for the ship landing ([tree/](src/tree/)). I.e., he/she needs to extract logdata into a *Trace* struct and needs to build the TBT.\n\n## Brief Summary of the Supported Operators\n\nTBT ``T:=``\n- ``Fallback([T_1,...,T_n])``: At least one of the subtrees must eventually be satisfied.\n- ``Sequence([T_1, T_2])``: Each subtree must be satisfied in order from left to right.\n- ``Parallel(m, [T_1,...,T_n])``: At least ``m`` of the subtrees must be simultaneously satisfied.\n- ``Timeout(t, T)``: The subtree must be satisfied by a finite prefix of length ``t``.\n- ``Kleene(n, T)``: There must be ``n`` repetitions of the subtree to be satisfied.\n- ``Leaf(S)``: STL formula ``S`` must be satisfied.\n\nSTL ``S:=``\n- ``Atomic(function)``: The function must return a positive number to be satisfied, otherwise it is violated.\n- ``Conjuntion(S_1, S_2)``: Both subformulas must be satisfied.\n- ``Disjunction(S_1, S_2)``: One of the subformulas or both must be satisfied.\n- ``Neg(S)``: The subformulas must be violated.\n- ``Next(S)``: The subformula must be satisfied in the next step.\n- ``Eventually(S)``: Eventually the subformula must be satisfied.\n- ``Globally(S)``: The subformula must always be satisfied.\n- ``Until(S_1, S_2)``: The subformula ``S_1`` must be satisfied *until* ``S_2`` is satisfied.\n- ``EventuallyInterval(l, u, S)``: Eventually within ``l`` and ``u`` steps, the subformula must be satisfied.\n- ``GloballyInterval(l, u, S)``: Always within ``l`` and ``u`` steps, the subformula must be satisfied.\n- ``UntilInterval(l, u, S_1, S_2)``: Within ``l`` and ``u`` steps, the subformula ``S_1`` must be satisfied *until* ``S_2`` is satisfied.\n\nThe TBT operators are defined [here](src/behaviortree.rs) and the STL operators are defined [here](src/stl.rs). \n\nA TBT parser is implemented that is based on this [grammar](src/tbt.pest).\n\nFor more details, we refer to the [paper](https://doi.org/10.1145/3641513.3650180).\n\n## Docker Environment\n1. Install [Docker](https://docs.docker.com/engine/install/)\n2. Builder Docker Image: ``docker build -t tbt .``\n3. Run Docker Container: ``docker run -it --rm --name tbt-container tbt bash``\n4. To test if the container is working reproduce paper results by (being in the docker bash):\n   - Run ``. scripts/run.sh`` that takes all logfiles and computes the segmentation.    \n      - The script calls the tool as defined [here](#getting-started) for each logfolder that exists in [res](res).\n      - The results for each run are stored in the respective logfolder.\n   - Check results of each logfiles that are located in the following subfolder: ``cd ./res/``\n   - The files in the folders [res/<folder>](res) are called ``subsampling_result.txt`` and ``subsamplingAndLazy_result.txt``.\n   - Besides the result-files, for each segmentation, the script produces a `.png`-plot. Every `.png`-plot that has a name that ends with `aX`, where `X` is a number, represents an alternative segmentations where the number corresponds to the alternative in the result-file.  For instance, Figure 9 of the HSCC paper can then be found [here](res/logs_wind_front_Oblique/subsampling_result_a3.png).\n   - (Optional) To display plots copy them from the docker container to your host machine; dont use the docker bash.\n     -  ``docker cp <container_id>:<location_png/results> <location_to_be_stored>`` (copy whole folder or individual files), e.g., ``docker cp e7ba94d69e94:/app/res ./docker``\n     -  to get container_id call ``docker ps``\n\n> The Dockerfile uses multiple stages. The first stage builds the executable using rust/cargo and the second stage uses a debian environment to execute it. Therefore, there are no cargo-commands available in the container while running.\n\n> Also, there is a line ending issue, when building the docker image in a Windows environment. We recommend building the image on a Linux machine to avoid this issue (WSL is an option for Windows systems).\n \n## How to Interpret the Output Format\nRunning e.g. `cargo run --release -- -l -c -u -s <your-folder>/TBT-Segmentation/specification/shiplanding_formula_combined.tbt -f <your-foulder>/TBT-Segmentation/res/logs_wind_front_Lateral/` produces an output that contains the following lines:\n\n>Constants: {\n>  \"Lateral_AngleToShip\": 90.0, ... }\n\nthat represent all read constants in the provided TBT specification.\n\n> AP(0) in_pos ([\"uas_x\", \"uas_y\", \"uas_z\", \"ship_x\", \"ship_y\", \"ship_z\", \"ship_heading\"]): (2.5 - sqrt((((((ship_x + (30 * cos((deg_to_rad(135) + ship_heading)))) - uas_x) ^ 2) + (((ship_y + (30 * sin((deg_to_rad(135) + ship_heading)))) - uas_y) ^ 2)) + (((ship_z + 20) - uas_z) ^ 2))))\n\na sequence of the parsed atomic propositions. \n\n> SETTING: <br>\n> Logfile: /root/canbedeleted/TBT-Segmentation/res/logs_wind_front_Lateral/ <br>\n\nrepresents the logfile name.\n\n> Approximations: (lazy evaluation= true, subsampling = true(delta: 100))\n\nshow which approximations are enabled.\nIn this case, lazy evauation and subsampling with a delta of 100 are enabled.\n\n> Trace length: 252\n\nprovides that the length of the trace is 252 after subsampling. \nI.e. the original file has >25.000 entries.\n\n> Temporal behavior tree: <br>\n  Sequence(22)[\n    Fallback(20)[ <...> ]]\n\nshows a pretty print of the used TBT with its node identifiers.\nHere, the root node has ID 22.\n\n> Created tree table with 733,194 entries. <br>\nCreated formula table with 828,828 entries.\n\nare information on the table used for dynamic programming.\n\n> Statistics: Robustness value is 0.05925286 with 3,277,611 total tree lookups and 1,503,504 formula lookups\n\nprovides information how effective dynamic programming was.\n\n> Get segmentation after 0 seconds.\n\nstates that it took 0 seconds to compute a segmentation.\n\n> Approximate segmentation with robustness 0.05925286 and subsampling delta of 0.5564443 is:\n\nis the beginning of the segmentation. The following lines provide information on the segments.\n\n> lower:          0   upper:         78   value:      0.31250286  segment: Leaf(0 move_to_position_lateral)\n\nrepresent one segment.\nIt states that the leaf node `move_to_position_lateral` was assigned to the segment that begins at index 0 and ends at index 78. \nFurther its robustness value is 0.31, i.e., the trace segment did satisfy this node.\n\n\n## Contributors\n- Sebastian Schirmer\n  \n## Contributing\n\nPlease see [the contribution guidelines](CONTRIBUTING.md) for further information about how to contribute.\n\n## Changes\n\nPlease see the [Changelog](CHANGELOG.md) for notable changes of the material.\n\n## License\n\nPlease see the file [LICENSE.md](LICENSE.md) for further information about how the content is licensed.\n\n",
                "dependencies": "# SPDX-FileCopyrightText: 2023 German Aerospace Center (DLR)\n# SPDX-License-Identifier: Apache-2.0\n\n[package]\nname = \"tbt-segmentation\"\nversion = \"1.0.0\"\nedition = \"2021\"\nauthors = [\"Sebastian Schirmer <sebastian.schirmer@dlr.de>\"]\ndescription = \"A segmentation algorithm for TBT specifications\"\nreadme = \"README.md\"\nhomepage = \"www.dlr.de/ft/ulf\"\nrepository = \"https://github.com/DLR-FT/TBT-Segmentation\"\nlicense = \"Apache-2.0\"\nkeywords = [\"segmentation\", \"cyber-physical systems\", \"offline analysis\"]\n\n[dependencies]\npest = \"2.7.9\"\npest_derive = \"2.7.9\"\ncsv = \"1.2.2\"\nclap = \"2.33.3\"\nnum-format = \"0.4.4\"\ntoml = \"0.8.13\"\nserde = { version = \"1.0\", features = [\"derive\"] }\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tereno-doi",
            "repo_link": "https://jugit.fz-juelich.de/ibg-3/ibg3_data_management/software/projects/tereno-doi/tdoi",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tetrax",
            "repo_link": "https://codebase.helmholtz.cloud/micromagnetic-modeling/tetrax",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tigl",
            "repo_link": "https://github.com/DLR-SC/tigl",
            "content": {
                "codemeta": "",
                "readme": "<p><img src=\"doc/images/logo.png\" alt=\"TiGL Logo\" title=\"TiGL Logo\" style=\"background-color:white;padding:5px;\"/></p>\n\n[![CI workflow for main branch](https://github.com/DLR-SC/tigl/actions/workflows/main.yml/badge.svg)](https://github.com/DLR-SC/tigl/actions/workflows/main.yml)\n[![codecov](https://codecov.io/gh/dlr-sc/tigl/branch/master/graph/badge.svg)](https://codecov.io/gh/dlr-sc/tigl)\n[![Apache 2.0](https://img.shields.io/crates/l/k)](https://github.com/DLR-SC/tigl/blob/cpacs_3/LICENSE.txt)\n[![Install with conda](https://anaconda.org/dlr-sc/tigl3/badges/version.svg)](https://anaconda.org/dlr-sc/tigl3/badges/version.svg)\n[![Cite-us](https://img.shields.io/badge/doi-10.1007%2Fs11786--019--00401--y-blue)](https://doi.org/10.1007/s11786-019-00401-y) \n[![Documentation](https://img.shields.io/badge/docs-online-green)](https://dlr-sc.github.io/tigl/doc/latest/) \n\nThe **Ti**GL **G**eometry **L**ibrary can be used for the computation and processing of aircraft geometries \nstored inside [CPACS](https://github.com/DLR-LY/CPACS) files. TiGL offers many geometry related functions such as\n - Point retrieval functions to compute points on the aircraft surface\n - Intersection functions to compute the intersection of the aircraft with planes\n - Export functions for standard CAD file formats (STEP + IGES) or mesh formats, \n   including VTK, Collada, and STL.\n   \nThe TiGL library uses the OpenCASCADE CAD kernel to represent the airplane geometry \nby NURBS surfaces. The library provides external interfaces for C, C++, Python, Java, MATLAB, and FORTRAN.\n\nTiGL is shipped with the Qt based _TiGL Viewer_ for visualizing aircraft\ngeometries or viewing CAD files.\n\n![Screenshot of the TiGL Viewer](doc/images/tiglviewer-web.jpg)\n\n# Downloads\n\n - Pre-Compiled Releases:  https://github.com/DLR-SC/tigl/wiki/Downloads\n - Nightly Builds:    https://github.com/DLR-SC/tigl/actions?query=workflow%3A%22Continuous+Integration%22+event%3Aschedule\n\n# News\n\nPlease head over to our TiGL website: https://dlr-sc.github.io/tigl/#news\n\n# Cite us\n\nTiGL is available as Open Source and we encourage anyone to make use of it. If you are applying TiGL in a scientific environment and publish any related work, please cite the following article:\n\nSiggel, M., Kleinert, J., Stollenwerk, T. et al.:  *TiGL: An Open Source Computational Geometry Library for Parametric Aircraft Design*, Math.Comput.Sci. (2019). https://doi.org/10.1007/s11786-019-00401-y\n\nA free copy of the paper is offered here: https://rdcu.be/bIGUH \n\n\n\n",
                "dependencies": "# Set a default build type if none was specified\r\nif(NOT DEFINED CMAKE_BUILD_TYPE)\r\n  message(STATUS \"Setting build type to 'Release' as none was specified.\")\r\n  set(CMAKE_BUILD_TYPE Release CACHE STRING \"Choose the type of build.\" FORCE)\r\n  # Set the possible values of build type for cmake-gui\r\n  set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\"\r\n    \"MinSizeRel\" \"RelWithDebInfo\")\r\nendif()\r\n\r\ncmake_minimum_required (VERSION 3.11.0)\r\n\r\nproject (TIGL VERSION 3.4.0)\r\nset(TIGL_VERSION 3.4.0)\r\n\r\n# enable C++11 support\r\nset(CMAKE_CXX_STANDARD 11)\r\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\r\n\r\nset(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)\r\n\r\nif(NOT DEFINED CMAKE_INSTALL_LIBDIR)\r\n    set(CMAKE_INSTALL_LIBDIR \"lib\")\r\nendif(NOT DEFINED CMAKE_INSTALL_LIBDIR)\r\n\r\n\r\nif(NOT DEFINED CMAKE_INSTALL_BINDIR)\r\n    set(CMAKE_INSTALL_BINDIR \"bin\")\r\nendif(NOT DEFINED CMAKE_INSTALL_BINDIR)\r\n\r\n# these settings are required in order to create fully relocatable\r\n# libraries on osx\r\nset(CMAKE_MACOSX_RPATH ON)\r\nset(CMAKE_SKIP_BUILD_RPATH FALSE)\r\nset(CMAKE_BUILD_WITH_INSTALL_RPATH FALSE)\r\nset(CMAKE_INSTALL_RPATH \"${CMAKE_INSTALL_PREFIX}/lib\")\r\nset(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)\r\n\r\n# convert path to absolute (required for some scripts)\r\nif (NOT IS_ABSOLUTE ${CMAKE_INSTALL_PREFIX})\r\n   set (CMAKE_INSTALL_PREFIX ${PROJECT_BINARY_DIR}/${CMAKE_INSTALL_PREFIX})\r\nendif()\r\n\r\noption(TIGL_BINDINGS_INSTALL_CPP \"Install TiGL's CPP bindings\" OFF)\r\n\r\nOPTION(TIGL_NIGHTLY \"Creates a nightly build of tigl (includes git sha into tigl version)\" OFF)\r\nmark_as_advanced(TIGL_NIGHTLY)\r\nif(TIGL_NIGHTLY)\r\n    message(STATUS \"Nightly build enabled\")\r\n\r\n    # get git revision for daily builds\r\n    include(GetGitRevisionDescription)\r\n    get_git_head_revision(REFSPEC HASHVAR)\r\n\r\n    if(NOT ${HASHVAR} STREQUAL \"GITDIR-NOTFOUND\")\r\n        set(TIGL_REVISION ${HASHVAR})\r\n    endif()\r\nendif(TIGL_NIGHTLY)\r\n\r\nset(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/${CMAKE_INSTALL_LIBDIR})\r\n\r\ninclude(UseOpenCASCADE)\r\n\r\n# search TiXI\r\nset(TIXI_PATH \"\" CACHE PATH \"TiXI installation prefix\")\r\nset(CMAKE_PREFIX_PATH \"${TIXI_PATH};${CMAKE_PREFIX_PATH}\")\r\nfind_package( tixi3 3.0.3 REQUIRED CONFIG)\r\n\r\n\r\nfind_package( PythonInterp )\r\n\r\nOPTION(TIGL_USE_GLOG \"Enables advanced logging (requires google glog)\" OFF)\r\nif(TIGL_USE_GLOG)\r\n    find_package( GLOG REQUIRED )\r\n    if(NOT GLOG_FOUND)\r\n      message(STATUS \"Google GLOG not found. Advanced logging disabled.\")\r\n    endif()\r\nendif(TIGL_USE_GLOG)\r\n\r\n# enable parallel builds in Visual Studio\r\nif (MSVC)\r\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /MP\")\r\nendif()\r\n\r\n# check features\r\ninclude(CheckCXXSourceCompiles)\r\nset(src_make_unqiue \"#include <memory>\\nint main(){\\n std::make_unique<int>(5)\\;\\n return 0\\;\\n}\\n\")\r\nCHECK_CXX_SOURCE_COMPILES(${src_make_unqiue} HAVE_STDMAKE_UNIQUE)\r\n\r\n# code coverage analysis\r\nIF (CMAKE_BUILD_TYPE STREQUAL \"Debug\")\r\n        # enable extensive debug output\r\n        OPTION(DEBUG_EXTENSIVE \"Switch on extensive debug output\" OFF)\r\n        OPTION(TIGL_ENABLE_COVERAGE \"Enable GCov coverage analysis (defines a 'coverage' target and enforces static build of tigl)\" OFF)\r\n        IF(TIGL_ENABLE_COVERAGE)\r\n                MESSAGE(STATUS \"Coverage enabled\")\r\n                INCLUDE(CodeCoverage)\r\n                SETUP_TARGET_FOR_COVERAGE(coverage-unittests TiGL-unittests tests/unittests coverageReport-unit $ENV{COVERAGE_ARGS})\r\n                SETUP_TARGET_FOR_COVERAGE(coverage-integrationtests TiGL-integrationtests tests/integrationtests coverageReport-integration $ENV{COVERAGE_ARGS})\r\n                SETUP_TARGET_FOR_COVERAGE_COBERTURA(coverage-cobertura TiGL-unittests coverage $ENV{COVERAGE_ARGS})\r\n        ELSE()\r\n                MESSAGE(STATUS \"Coverage disabled\")\r\n        ENDIF()\r\nENDIF()\r\n\r\n# visual leak detector, useful for debugging under windows\r\nif(WIN32)\r\n    if(CMAKE_BUILD_TYPE STREQUAL \"Debug\")\r\n        OPTION(TIGL_USE_VLD \"Enable Visual Leak Detector.\" OFF)\r\n        if(TIGL_USE_VLD)\r\n            find_package( VLD REQUIRED )\r\n            add_definitions(-DHAVE_VLD=1)\r\n            include_directories(${VLD_INCLUDE_DIRS})\r\n        endif(TIGL_USE_VLD)\r\n    endif(CMAKE_BUILD_TYPE STREQUAL \"Debug\")\r\nendif(WIN32)\r\n\r\n\r\nif(WIN32)\r\n    # avoid export of oce/occt classes into tigl library\r\n    \r\nendif(WIN32)\r\n\r\nadd_subdirectory(thirdparty)\r\n\r\n# style checks\r\nadd_custom_target(checkstyle)\r\nadd_custom_target(checkstylexml)\r\n\r\nadd_subdirectory(src)\r\nadd_subdirectory(TIGLViewer)\r\nadd_subdirectory(bindings)\r\nadd_subdirectory(examples)\r\n\r\n\r\n#create gtests, override gtest standard setting\r\noption(TIGL_BUILD_TESTS \"Build TIGL Testsuite\" OFF)\r\n\r\nif(TIGL_BUILD_TESTS)\r\n  enable_testing()\r\n  option(gtest_force_shared_crt \"\" ON)\r\n  mark_as_advanced(gtest_force_shared_crt gtest_build_tests gtest_build_samples gtest_disable_pthreads)\r\n  add_subdirectory (\"thirdparty/googletest\" EXCLUDE_FROM_ALL)\r\n  add_subdirectory(tests)\r\nendif(TIGL_BUILD_TESTS)\r\n\r\ninclude(createDoc)\r\n\r\n\r\nset(CPACK_DEBIAN_PACKAGE_MAINTAINER \"Martin Siggel\") #required for debian/ubuntu\r\nset(CPACK_PACKAGE_VENDOR \"www.dlr.de/sc\")\r\nif(TIGL_NIGHTLY)\r\n  string(SUBSTRING ${TIGL_REVISION} 0 8 TIGL_REV_SHORT)\r\n  set(CPACK_PACKAGE_VERSION ${TIGL_VERSION}-r${TIGL_REV_SHORT})\r\n  set(CPACK_PACKAGE_VERSION_PATCH ${TIGL_VERSION_PATCH}-r${TIGL_REV_SHORT})\r\nelse()\r\n  set(CPACK_PACKAGE_VERSION ${TIGL_VERSION})\r\n  set(CPACK_PACKAGE_VERSION_PATCH ${TIGL_VERSION_PATCH})\r\nendif()\r\nset(CPACK_PACKAGE_VERSION_MAJOR ${TIGL_VERSION_MAJOR})\r\nset(CPACK_PACKAGE_VERSION_MINOR ${TIGL_VERSION_MINOR})\r\n\r\nset(CPACK_RESOURCE_FILE_LICENSE ${PROJECT_SOURCE_DIR}/LICENSE.txt)\r\nset(CPACK_PACKAGE_INSTALL_REGISTRY_KEY \"TIGL\") \r\n\r\nset(CPACK_NSIS_MUI_ICON ${PROJECT_SOURCE_DIR}/TIGLViewer/TIGLViewer.ico)\r\nset(CPACK_NSIS_MUI_UNIICON ${PROJECT_SOURCE_DIR}/TIGLViewer/TIGLViewer.ico)\r\n\r\n# set installer icon\r\nif(WIN32)\r\n    set(CPACK_PACKAGE_ICON  ${PROJECT_SOURCE_DIR}/TIGLViewer/gfx\\\\\\\\TIGLViewerNSIS.bmp)\r\nelseif(APPLE)\r\n    set(CPACK_PACKAGE_ICON  ${PROJECT_SOURCE_DIR}/TIGLViewer/gfx/TiGL-Viewer3.icns)\r\nendif()\r\n\r\n# set generators\r\nif(CPACK_GENERATOR)    \r\n    #already set\r\nelseif(APPLE)\r\n    set(CPACK_GENERATOR DragNDrop)\r\nelseif(WIN32)\r\n    set(CPACK_GENERATOR \"NSIS;ZIP\")\r\nelse()\r\n    set(CPACK_GENERATOR TGZ)\r\nendif()\r\n\r\n# set path variable for installer\r\nset(CPACK_NSIS_MODIFY_PATH ON)\r\nif(CMAKE_SIZEOF_VOID_P EQUAL 8)\r\n  set(CPACK_NSIS_INSTALL_ROOT \"$PROGRAMFILES64\")\r\n  set(CPACK_CUSTOM_INITIAL_DEFINITIONS \"!define CPACK_REQUIRIRE_64BIT\")\r\nelse()\r\n  set(CPACK_NSIS_INSTALL_ROOT \"$PROGRAMFILES\")\r\nendif()\r\n\r\nif (APPLE)\r\n  set(CPACK_PACKAGE_EXECUTABLES\r\n    \"TiGL-Viewer3\" \"TiGL Viewer 3\"\r\n  )\r\nelse(APPLE)\r\n  set(CPACK_PACKAGE_EXECUTABLES\r\n    \"tiglviewer-3\" \"TiGL Viewer 3\"\r\n  )\r\nendif(APPLE)\r\n\r\ninclude(CPack)\r\ncpack_add_component(viewer DISPLAY_NAME \"TiGL Viewer + 3rd Party DLLs\")\r\ncpack_add_component(headers DISPLAY_NAME \"Headers\")\r\ncpack_add_component(cpp_bindings DISPLAY_NAME \"Internal C++ Bindings\")\r\ncpack_add_component(interfaces DISPLAY_NAME \"Interfaces/Bindings\")\r\ncpack_add_component(docu DISPLAY_NAME \"Documentation\")\r\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tigramite",
            "repo_link": "https://github.com/jakobrunge/tigramite",
            "content": {
                "codemeta": "",
                "readme": "# Tigramite – Causal inference for time series datasets\n![logo](docs/_images/tigramite_logo_header.png)\nVersion 5.2\n(Python Package)\n\n[Github](https://github.com/jakobrunge/tigramite.git)\n\n[Documentation](https://jakobrunge.github.io/tigramite/)\n\n[Tutorials](https://github.com/jakobrunge/tigramite/tree/master/tutorials/)\n\n## Overview\n\nIt's best to start with our [Overview/review paper: Causal inference for time series](https://github.com/jakobrunge/tigramite/blob/master/tutorials/Runge_Causal_Inference_for_Time_Series_NREE.pdf)\n\n__Update:__ Tigramite now has a new CausalEffects class that allows to estimate (conditional) causal effects and mediation based on assuming a causal graph. Have a look at the tutorial.\n\nFurther, Tigramite provides several causal discovery methods that can be used under different sets of assumptions. An application always consists of a method and a chosen conditional independence test, e.g. PCMCIplus together with ParCorr. The following two tables give an overview of the assumptions involved:\n\n| Method | Assumptions         | Output |\n| :-- | :-- | :-- |\n|         |   (in addition to Causal Markov Condition and Faithfulness)   |    |\n| PCMCI  | Causal stationarity, no contemporaneous causal links, no hidden variables |  Directed lagged links, undirected contemporaneous links (for tau_min=0)  |\n| PCMCIplus | Causal stationarity, no hidden variables    | Directed lagged links, directed and undirected contemp. links (Time series CPDAG) |\n| LPCMCI | Causal stationarity    | Time series PAG |\n| RPCMCI  | No contemporaneous causal links, no hidden variables |  Regime-variable and causal graphs for each regime with directed lagged links, undirected contemporaneous links (for tau_min=0)  |\n| J-PCMCI+ | Multiple datasets, causal stationarity, no hidden system confounding, except if context-related   | Directed lagged links, directed and undirected contemp. links (Joint time series CPDAG) |\n\n\n| Conditional independence test | Assumptions                                                                                            |\n| :-- | :-- | \n| ParCorr                       | univariate, continuous variables with linear dependencies and Gaussian noise                           |\n| RobustParCorr                 | univariate, continuous variables with linear dependencies, robust for different marginal distributions |\n| ParCorrWLS                    | univariate, continuous variables with linear dependencies, can account for heteroskedastic data        |\n| GPDC / GPDCtorch              | univariate, continuous variables with additive dependencies                                            |\n| CMIknn                        | multivariate, continuous variables with more general dependencies (permutation-based test)             |\n| Gsquared                      | univariate discrete/categorical variables                                                              |\n| CMIsymb                       | multivariate discrete/categorical variables (permutation-based test)                                   |\n| RegressionCI                  | mixed datasets with univariate discrete/categorical and (linear) continuous variables                  |\n\nRemark: With the conditional independence test wrapper class PairwiseMultCI you can turn every univariate test into a multivariate test.\n\n## General Notes\n\nTigramite is a causal inference for time series python package. It allows to efficiently estimate causal graphs from high-dimensional time series datasets (causal discovery) and to use graphs for robust forecasting and the estimation and prediction of direct, total, and mediated effects. Causal discovery is based on linear as well as non-parametric conditional independence tests applicable to discrete or continuously-valued time series. Also includes functions for high-quality plots of the results. Please cite the following papers depending on which method you use:\n\n- Overview: Runge, J., Gerhardus, A., Varando, G. et al. Causal inference for time series. Nat Rev Earth Environ (2023). https://doi.org/10.1038/s43017-023-00431-y\n\n- PCMCI: J. Runge, P. Nowack, M. Kretschmer, S. Flaxman, D. Sejdinovic, Detecting and quantifying causal associations in large nonlinear time series datasets. Sci. Adv. 5, eaau4996 (2019). https://advances.sciencemag.org/content/5/11/eaau4996\n- PCMCI+: J. Runge (2020): Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets. Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence, UAI 2020,Toronto, Canada, 2019, AUAI Press, 2020. http://auai.org/uai2020/proceedings/579_main_paper.pdf\n- LPCMCI: Gerhardus, A. & Runge, J. High-recall causal discovery for autocorrelated time series with latent confounders Advances in Neural Information Processing Systems, 2020, 33. https://proceedings.neurips.cc/paper/2020/hash/94e70705efae423efda1088614128d0b-Abstract.html\n- RPCMCI: Elena Saggioro, Jana de Wiljes, Marlene Kretschmer, Jakob Runge; Reconstructing regime-dependent causal relationships from observational time series. Chaos 1 November 2020; 30 (11): 113115. https://doi.org/10.1063/5.0020538\n- Generally: J. Runge (2018): Causal Network Reconstruction from Time Series: From Theoretical Assumptions to Practical Estimation. Chaos: An Interdisciplinary Journal of Nonlinear Science 28 (7): 075310. https://aip.scitation.org/doi/10.1063/1.5025050\n- Nature Communications Perspective paper: https://www.nature.com/articles/s41467-019-10105-3\n- Mediation class: J. Runge et al. (2015): Identifying causal gateways and mediators in complex spatio-temporal systems. Nature Communications, 6, 8502. http://doi.org/10.1038/ncomms9502\n- Mediation class: J. Runge (2015): Quantifying information transfer and mediation along causal pathways in complex systems. Phys. Rev. E, 92(6), 62829. http://doi.org/10.1103/PhysRevE.92.062829\n- CMIknn: J. Runge (2018): Conditional Independence Testing Based on a Nearest-Neighbor Estimator of Conditional Mutual Information. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics. http://proceedings.mlr.press/v84/runge18a.html\n- CausalEffects: J. Runge, Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables, Advances in Neural Information Processing Systems, 2021, 34. https://proceedings.neurips.cc/paper/2021/hash/8485ae387a981d783f8764e508151cd9-Abstract.html\n\n## Features\n\n- flexible conditional independence test statistics adapted to\n  continuously-valued, discrete and mixed data, and different assumptions about\n  linear or nonlinear dependencies\n- handling of missing values and masks\n- p-value correction and (bootstrap) confidence interval estimation\n- causal effect class to  non-parametrically estimate (conditional) causal effects and also linear mediated causal effects\n- prediction class based on sklearn models including causal feature selection\n\n## Required python packages\n\n- python=3.7/3.8/3.9/3.10\n- numpy <1.24,>=1.18\n- scipy>=1.10.0\n- numba==0.56.4\n\n## Optional packages depending on used functions\n- scikit-learn>=1.2   # Gaussian Process (GP) Regression\n- matplotlib>=3.7.0   # Plotting\n- seaborn>=0.12.2     # Plotting\n- networkx>=3.0       # Plotting\n- torch>=1.13.1       # GPDC pytorch version (in conda install pytorch)\n- gpytorch>=1.9.1     # GPDC gpytorch version\n- dcor>=0.6           # GPDC distance correlation version\n- joblib>=1.2.0       # CMIsymb shuffle parallelization\n- ortools>=9.2        # RPCMCI\n\n## Installation\n\npython setup.py install\n\nThis will install tigramite in your path.\n\nTo use just the ParCorr, CMIknn, and CMIsymb independence tests, only numpy/numba and scipy are required. For other independence tests more packages are required:\n\n- GPDC: scikit-learn is required for Gaussian Process regression and dcor for distance correlation\n\n- GPDCtorch: gpytorch is required for Gaussian Process regression\n\nNote: Due to incompatibility issues between numba and numpy, we currently enforce soft dependencies on the versions.\n\n## User Agreement\n\nBy downloading TIGRAMITE you agree with the following points: TIGRAMITE is provided without any warranty or conditions of any kind. We assume no responsibility for errors or omissions in the results and interpretations following from application of TIGRAMITE.\n\nYou commit to cite above papers in your reports or publications.\n\n\n## License\n\nCopyright (C) 2014-2025 Jakob Runge\n\nSee license.txt for full text.\n\nGNU General Public License v3.0\n\nTIGRAMITE is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version. TIGRAMITE is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n\n",
                "dependencies": "\"\"\"\nInstall tigramite\n\"\"\"\nfrom __future__ import print_function\nimport pathlib\nimport os\nfrom setuptools import setup, Extension\nfrom setuptools.command.build_ext import build_ext\nimport json\n\n# Handle building against numpy headers before installing numpy\nclass UseNumpyHeadersBuildExt(build_ext):\n    \"\"\"\n    Subclassed build_ext command.\n    Allows for numpy to be imported after it is automatically installed.\n    This lets us use numpy.get_include() while listing numpy as a needed\n    dependency.\n    \"\"\"\n\n    def run(self):\n        self.distribution.fetch_build_eggs([\"numpy\"])\n        # Import numpy here, only when headers are needed\n        import numpy\n\n        # Add numpy headers to include_dirs\n        self.include_dirs.append(numpy.get_include())\n        # Call original build_ext command\n        build_ext.run(self)\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\n# Define the minimal classes needed to install and run tigramite\nINSTALL_REQUIRES =  [\"numpy>=1.18\", \"scipy>=1.10.0\", \"six\"]\n# Define all the possible extras needed\nEXTRAS_REQUIRE = {\n    \"all\": [\n        \"scikit-learn>=1.2\",  # Gaussian Process (GP) Regression\n        \"matplotlib>=3.7.0\",  # plotting\n        \"seaborn>=0.12.2\",    # plotting\n        \"networkx>=3.0\",      # plotting\n        \"pytorch>=1.13.1\",    # GPDC torch version\n        \"gpytorch>=1.9.1\",    # GPDC gpytorch version\n        \"dcor>=0.6\",          # GPDC distance correlation version\n        \"joblib>=1.2.0\",      # CMIsymb shuffle parallelization and others\n        \"ortools>=9.2\",       # RPCMCI\n        \"numba>=0.58\",        # CMIknn and CMIsymb and derived classes\n    ]\n}\n\n# Define the packages needed for testing\nTESTS_REQUIRE = [\"nose\", \"pytest\", \"networkx>=3.0\", \"scikit-learn>=1.2\",\n                 \"pytorch>=1.13.1\", \"gpytorch>=1.9.1\", \"dcor>=0.6\"]\nEXTRAS_REQUIRE[\"test\"] = TESTS_REQUIRE\n# Define the extras needed for development\nEXTRAS_REQUIRE[\"dev\"] = EXTRAS_REQUIRE[\"all\"]\n\n# Use a custom build to handle numpy.include_dirs() when building\nCMDCLASS = {\"build_ext\": UseNumpyHeadersBuildExt}\n\n# Run the setup\nsetup(\n    name=\"tigramite\",\n    version=\"5.2.6.7\",\n    packages=[\"tigramite\", \"tigramite.independence_tests\", \"tigramite.toymodels\"],\n    license=\"GNU General Public License v3.0\",\n    description=\"Tigramite causal inference for time series\",\n    author=\"Jakob Runge\",\n    author_email=\"jakob@jakob-runge.com\",\n    url=\"https://github.com/jakobrunge/tigramite/\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    keywords=\"causal inference, causal discovery, prediction, time series\",\n    cmdclass=CMDCLASS,\n    install_requires=INSTALL_REQUIRES,\n    extras_require=EXTRAS_REQUIRE,\n    test_suite=\"tests\",\n    tests_require=TESTS_REQUIRE,\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: Science/Research\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Scientific/Engineering :: Mathematics\",\n        \"License \"\n        \":: OSI Approved \"\n        \":: GNU General Public License v3 or later (GPLv3+)\",\n        \"Programming Language :: Python\",\n    ],\n)\n\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/timeio",
            "repo_link": "https://codebase.helmholtz.cloud/ufz-tsm/tsm-orchestration",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/timeseries-management",
            "repo_link": "https://git.gfz-potsdam.de/id2/tsm/tsm.docker-compose",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/tixi",
            "repo_link": "https://github.com/DLR-SC/tixi",
            "content": {
                "codemeta": "",
                "readme": "# TIXI #\r\n\r\n[![CI](https://github.com/DLR-SC/tixi/actions/workflows/main.yml/badge.svg)](https://github.com/DLR-SC/tixi/actions/workflows/main.yml)\r\n\r\n - Binary Downloads:  https://github.com/DLR-SC/tixi/wiki/Downloads\r\n - API Documentation: http://dlr-sc.github.io/tixi/\r\n - Issue Tracker:     https://github.com/DLR-SC/tixi/issues\r\n - Wiki:              https://github.com/DLR-SC/tixi/wiki\r\n\r\n## Installation ##\r\n - with Conda: [![Anaconda-Server Badge](https://anaconda.org/dlr-sc/tixi3/badges/installer/conda.svg)](https://anaconda.org/DLR-SC/tixi3)\r\n - with Linux package manager: [OpenBuildService](https://software.opensuse.org/download.html?project=science:dlr&package=tixi3)\r\n\r\n\r\n## Description ##\r\nTiXI is a fast and simple XML interface library and could be used from applications written in C, C++, Fortran, JAVA and Python. The library can be directly integrated into a code by third party software or can be used by users who don't want to deal with the complexity of XML when creating a new application. Although simplified and somewhat restricted compared to a full-fledged XML processing library the user can, for example, create documents, create and delete nodes, and add and remove element attributes. Routines to read and write simple text nodes and additionally specialized nodes holding integer and floating point numbers are part of this API. Furthermore, routines to process aggregates of these simple types have been implemented. For the processing of geometric data, reading and writing of multidimensional arrays or arrays of vectors, i.e. coordinates of points are supported. The library has been designed to hide the implementation details so that the underlying XML library, currently libxml2, can be replaced by another one without changing the XML processing API in the applications.\r\n\r\nReading a text attribute could be as easy as:\r\n\r\n```\r\ntixiGetTextAttribute( handle, elementPath, attributeName, &attributeValue );\r\n```\r\n\r\nGetting a double value would look like this:\r\n```\r\ntixiGetDoubleElement( handle, elementPath, &x );\r\n```\r\n\r\n## Multi Language Support ##\r\nThe TIXI library is written in C, but there are interfaces and wrappers for C++, Fortran, Python, JAVA and Matlab. Take a look at our examples for [C](https://github.com/DLR-SC/tixi/wiki/CExamples) and [Fortran](https://github.com/DLR-SC/tixi/wiki/Fortran%20Examples).\r\n\n",
                "dependencies": "cmake_minimum_required (VERSION 3.1)\n\nif (EXISTS ${CMAKE_BINARY_DIR}/conan_toolchain.cmake)\n    cmake_policy(SET CMP0091 NEW)\n    include(${CMAKE_BINARY_DIR}/conan_toolchain.cmake)\nendif()\n\n# Set a default build type if none was specified\nif(NOT DEFINED CMAKE_BUILD_TYPE)\n    message(STATUS \"Setting build type to 'Release' as none was specified.\")\n    set(CMAKE_BUILD_TYPE Release CACHE STRING \"Choose the type of build.\" FORCE)\n    # Set the possible values of build type for cmake-gui\n    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\"\n      \"MinSizeRel\" \"RelWithDebInfo\")\nendif()\n\nproject (TIXI)\n\n# convert path to absolute (required for some scripts)\nif (NOT IS_ABSOLUTE ${CMAKE_INSTALL_PREFIX})\n    set (CMAKE_INSTALL_PREFIX ${PROJECT_BINARY_DIR}/${CMAKE_INSTALL_PREFIX})\nendif()\n\nset(TIXI_VERSION_MAJOR 3)\nset(TIXI_VERSION_MINOR 3)\nset(TIXI_VERSION_PATCH 0)\nset(TIXI_VERSION \"${TIXI_VERSION_MAJOR}.${TIXI_VERSION_MINOR}.${TIXI_VERSION_PATCH}\")\n\n# set name of the tixi library\nset(TIXI_LIB_NAME tixi${TIXI_VERSION_MAJOR})\n\nset(ADD_INCLUDE_PATH \"\" CACHE PATH \"Additional include path for package search\")\nset(ADD_LIB_PATH \"\" CACHE PATH     \"Additional library path for package search\")\n\nset(CMAKE_INCLUDE_PATH \"${CMAKE_INCLUDE_PATH}\" ${ADD_INCLUDE_PATH})\nset(CMAKE_LIBRARY_PATH \"${CMAKE_LIBRARY_PATH}\" ${ADD_LIB_PATH})\n\nset(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)\n\nif(NOT DEFINED CMAKE_INSTALL_LIBDIR)\n    set(CMAKE_INSTALL_LIBDIR \"lib\")\nendif(NOT DEFINED CMAKE_INSTALL_LIBDIR)\n\n\nif(NOT DEFINED CMAKE_INSTALL_BINDIR)\n    set(CMAKE_INSTALL_BINDIR \"bin\")\nendif(NOT DEFINED CMAKE_INSTALL_BINDIR)\n\nif(NOT DEFINED CMAKE_INSTALL_INCLUDE_DIR)\n    set(CMAKE_INSTALL_INCLUDE_DIR \"include/${TIXI_LIB_NAME}\")\nendif(NOT DEFINED CMAKE_INSTALL_INCLUDE_DIR)\n\nif(NOT DEFINED CONFIG_INSTALL_DIR)\n    set(CONFIG_INSTALL_DIR \"${CMAKE_INSTALL_LIBDIR}/${TIXI_LIB_NAME}\")\nendif(NOT DEFINED CONFIG_INSTALL_DIR)\n\nif (NOT DEFINED CMAKE_DEBUG_POSTFIX)\n    set(CMAKE_DEBUG_POSTFIX \"-d\")\nendif (NOT DEFINED CMAKE_DEBUG_POSTFIX)\n\ninclude(CheckCCompilerFlag)\ncheck_c_compiler_flag(\"-Wno-deprecated-declarations\" C_COMPILER_HAS_NO_DEPRECATED_DECL_OPTION)\n\noption (TIXI_ENABLE_FORTRAN \"Enable Fortran examples and binding\" OFF)\n\nfind_package(PythonInterp)\n\nset(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/${CMAKE_INSTALL_LIBDIR})\n# create library output path\nfile(MAKE_DIRECTORY ${LIBRARY_OUTPUT_PATH})\n\nset(CMAKE_MACOSX_RPATH 1)\n\n# code coverage analysis\nIF (CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n        OPTION(TIXI_ENABLE_COVERAGE \"Enable GCov coverage analysis (defines a 'coverage' target and enforces static build of tixi)\" OFF)\n        IF(TIXI_ENABLE_COVERAGE)\n                MESSAGE(STATUS \"Coverage enabled\")\n                INCLUDE(CodeCoverage)\n                SETUP_TARGET_FOR_COVERAGE(coverage ${PROJECT_NAME}-unittests coverageReport \"--gtest_output=xml\")\n                MESSAGE(STATUS \"Enabeling memcheck\")\n                INCLUDE(Valgrind)\n                SETUP_TARGET_FOR_VALGRIND(memcheck ${PROJECT_NAME}-unittests valgrind)\n        ELSE()\n                MESSAGE(STATUS \"Coverage disabled\")\n        ENDIF()\nENDIF()\n\n# visual leak detector, useful for debugging under windows\nif(WIN32)\n    if(CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n        OPTION(TIXI_USE_VLD \"Enable Visual Leak Detector.\" OFF)\n        if(TIXI_USE_VLD)\n            find_package( VLD REQUIRED )\n        endif(TIXI_USE_VLD)\n    endif(CMAKE_BUILD_TYPE STREQUAL \"Debug\")\nendif(WIN32)\n\n#create tixi library\nadd_subdirectory(src)\n\n#create bindings to different languages (fortran, java ...)\nadd_subdirectory(bindings)\n\n\n#create gtests, override gtest standard setting\noption(TIXI_BUILD_TESTS \"Build TIXI Testsuite\" OFF)\nif(TIXI_BUILD_TESTS)\n    enable_testing() \n    option(gtest_force_shared_crt \"\" ON)\n    mark_as_advanced(gtest_force_shared_crt gtest_build_tests gtest_build_samples gtest_disable_pthreads)\n    add_subdirectory (\"thirdparty/googletest\" EXCLUDE_FROM_ALL)\n    add_subdirectory(tests)\nendif(TIXI_BUILD_TESTS)\n\n#demos\nadd_subdirectory(examples/Demo)\nif (TIXI_ENABLE_FORTRAN)\n  add_subdirectory(examples/fortran77)\nendif(TIXI_ENABLE_FORTRAN)\n\n# create the doc\ninclude(createDoc)\n\nset(CPACK_DEBIAN_PACKAGE_MAINTAINER \"Martin Siggel\") #required for debian/ubuntu\nset(CPACK_PACKAGE_VENDOR \"www.dlr.de/sc\")\nset(CPACK_PACKAGE_VERSION ${TIXI_VERSION})\nset(CPACK_PACKAGE_VERSION_MAJOR ${TIXI_VERSION_MAJOR})\nset(CPACK_PACKAGE_VERSION_MINOR ${TIXI_VERSION_MINOR})\nset(CPACK_PACKAGE_VERSION_PATCH ${TIXI_VERSION_PATCH})\nset(CPACK_RESOURCE_FILE_LICENSE ${PROJECT_SOURCE_DIR}/LICENSE)\nset(CPACK_PACKAGE_INSTALL_REGISTRY_KEY \"TIXI\") \n\n# set path variable for installer\nset(CPACK_NSIS_MODIFY_PATH ON)\nif(CMAKE_SIZEOF_VOID_P EQUAL 8)\n    set(CPACK_NSIS_INSTALL_ROOT \"$PROGRAMFILES64\")\n    set(CPACK_CUSTOM_INITIAL_DEFINITIONS \"!define CPACK_REQUIRIRE_64BIT\")\nelse()\n    set(CPACK_NSIS_INSTALL_ROOT \"$PROGRAMFILES\")\nendif()\n\ninclude(CPack)\ncpack_add_component(Runtime DISPLAY_NAME \"3rd Party Libraries\")\ncpack_add_component(headers DISPLAY_NAME \"Headers\")\ncpack_add_component(interfaces DISPLAY_NAME \"Interfaces/Bindings\")\ncpack_add_component(docu DISPLAY_NAME \"Documentation\")\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tomato-tools",
            "repo_link": "https://git.geomar.de/open-source/tomato-toolboxes/tomato",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tomobear",
            "repo_link": "https://github.com/KudryashevLab/TomoBEAR",
            "content": {
                "codemeta": "",
                "readme": "# TomoBEAR\n\n[![DOI](https://zenodo.org/badge/675692608.svg)](https://zenodo.org/badge/latestdoi/675692608)\n\n**TomoBEAR** is a configurable and customizable modular pipeline for streamlined processing of cryo-electron tomographic data and preliminary subtomogram averaging (StA) based on best practices in the scientific research group of Dr. Misha Kudryashev[^1][^2].\n\n![TomoBEAR Social Media Logo Image](images/TomoBEAR_gitlogo.png)\n\nImplementation details and benchmarks you can find in our publication:\n</br> Balyschew N, Yushkevich A, Mikirtumov V, Sanchez RM, Sprink T, Kudryashev M. Streamlined Structure Determination by Cryo-Electron Tomography and Subtomogram Averaging using TomoBEAR. *Nat Commun* **14**, 6543 (2023). doi: [10.1038/s41467-023-42085-w](https://www.nature.com/articles/s41467-023-42085-w)\n\n> **Warning**\n> <br/> This software is currently in pre-release state. New features may still appear and refactoring may still take place between all the current and future 0.x.y versions until 1.0.0 will be ready to be released. Binaries are not currently shipped.\n\n## Contents\n\n- [Quick start](#quick-start)\n- [Documentation and licensing](#documentation-and-licensing)\n- [Changes and releases](#changes-and-releases)\n- [Feedback and contribution](#feedback-and-contribution)\n- [Citation](#citation)\n- [Acknowledgements](#acknowledgements)\n- [Contacts](#contacts)\n\n## Quick start\n\n### Video-tutorials\n\nWe have prepared a range of short (8-12 min) video-tutorials explaining setup, usage and example output of the ```TomoBEAR``` to help you get started with ```TomoBEAR``` based on the [ribosome tutorial](https://github.com/KudryashevLab/TomoBEAR/wiki/Tutorials):\n* [Video 1](https://youtu.be/2uizkE616tE): how to get the latest ```TomoBEAR``` version and configure ```TomoBEAR``` and its dependencies;\n* [Video 2](https://youtu.be/N93tfAXp990): description of the project configuration file and the pipeline execution;\n* [Video 3](https://youtu.be/qbkRtMJp0eI): additional configuration file parameters description, ```TomoBEAR```-```IMOD```-```TomoBEAR``` loop for checking tilt series alignment results and fiducials refinement (if needed);\n* [Video 4](https://youtu.be/BP2T_Y7BiDo): checking on further intermediate results (alignment, CTF-correction, reconstruction, template matching).\n\n### Pipeline structure\n\nIn the following picture you can see a flow chart of the main `TomoBEAR` processing steps. As the basic input data you can use raw frame movies or already assembled tilt stacks. More on input formats you [can read here](https://github.com/KudryashevLab/TomoBEAR/wiki/Usage.md#input-data-file-formats).\n\n![Schematic Pipeline Image](images/pipeline_upd.png)\n\nBlue boxes outline the steps that are performed fully automatically, green boxes may require human intervention. The steps encapsulated in the red frame represent the functionality of live data processing. More detailed diagram [is located on wiki](https://github.com/KudryashevLab/TomoBEAR/wiki).\n\n> **Note**\n> <br/> Full MATLAB (source code) version of `TomoBEAR` supports workstations and single interactive nodes with GPUs on the computing clusters at the moment. We are also working towards enabling the support of binaries on the mentioned systems as well as support of both source code and binary versions of the `TomoBEAR` on HPC clusters.\n\n## Documentation and licensing\n\nDetailed information on the installation, setup and usage as well as tutorials and example results can be found in the corresponding [wiki](https://github.com/KudryashevLab/TomoBEAR/wiki).\n\nPlease, see the [LICENSE file](LICENSE.md) for the information about how the content of this repository is licensed.\n\n## Changes and releases\n\nThe [CHANGELOG file](CHANGELOG.md) contains all notable changes corresponding to the different `TomoBEAR` releases, which are available at the [Releases page](https://github.com/KudryashevLab/TomoBEAR/releases).\n\nIf you want to clone a specific ```TomoBEAR``` version, please refer to the **Setup > Get source code and binary > Clone specific version** section on the wiki page [Installation and Setup](https://github.com/KudryashevLab/TomoBEAR/wiki/Installation-and-Setup.md).\n\n## Feedback and contribution\n\nIn case of any questions, issues or suggestions you may interact with us by one of the following ways:\n* open an issue/bug report, feature request or post a question using [Issue Tracker](https://github.com/KudryashevLab/TomoBEAR/issues);\n* write an e-mail to [Misha Kudryashev](mailto:misha.kudryashev@gmail.com) or [Artsemi Yushkevich](mailto:Artsemi.Yushkevich@mdc-berlin.de);\n* start a discussion in [Discussions](https://github.com/KudryashevLab/TomoBEAR/discussions);\n\nIf you wish to contribute, please, fork this repository and make a pull request back with your changes and a short description. For further details on contribution plase read our [contribution guidelines](CONTRIBUTING.md). \n\n## Citation\n\nIf you use `TomoBEAR` or its parts in your research, please **cite both** `TomoBEAR` and **all external software packages** which you have used under `TomoBEAR`.\n\nThe `TomoBEAR` modules dependencies on third-party software are listed on the page [Modules](https://github.com/KudryashevLab/TomoBEAR/wiki/Modules.md) and the list of the corresponding references to cite is located on the page [External Software](https://github.com/KudryashevLab/TomoBEAR/wiki/External-Software.md).\n\n## Acknowledgements\n\nWe are grateful to the following organizations:\n- Buchmann family and [BMLS (Buchmann Institute for Molecular Life Sciences)](https://www.bmls.de) for supporting this project with their starters stipendia for PhD students;\n- [DFG (Deutsche Forschungsgesellschaft)](https://www.dfg.de) for funding the project.\n\nAs well we are grateful to the [structural biology scientific research group of Werner Kühlbrandt](https://www.biophys.mpg.de/2207989/werner_kuehlbrandt) at the [MPIBP (Max Planck Institute of Biophysics)](https://www.biophys.mpg.de) and the [MPIBP](https://www.biophys.mpg.de) in Frankfurt (Hesse), Germany for support.\n\nThe authors thank as well the following people:\n* Dr. Daniel Castano-Diez, Dr. Kendra Leigh and Dr. Christoph Diebolder and Dr. Wolfgang Lugmayr for useful discussions;\n* Uljana Kravchenko, Xiaofeng Chu, Giulia Glorani for testing the developmental versions and providing feedback,\n* Ricardo Sanchez for producing MATLAB version of the [SUSAN framework](https://github.com/rkms86/SUSAN) in order to be compatible with TomoBEAR;\n* Juan Castillo from the Max Planck Institute for Biophysics for the IT support at the Max Planck for Biophysics,\n* the high-performance computing team at the MDC for supporting our operation at the Max-Cluster.\n\nWe would like to acknowledge as well that TomoBEAR contains modified pieces of MATLAB source code of the Dynamo package developed by Dr. Daniel Castaño-Díez et al.: https://www.dynamo-em.org.\n\n## Contacts\n* Prof. Dr. Misha Kudryashev[^1][^2] ([e-mail](mailto:misha.kudryashev@gmail.com?subject=[GitHub]%20TomoBEAR)) - `TomoBEAR` project leader, Principal Investigator;\n\n* Nikita Balyschew[^2] - `TomoBEAR` core version developer, alumni Ph.D. student.\n\n* Artsemi Yushkevich[^1] ([e-mail](mailto:Artsemi.Yushkevich@mdc-berlin.de?subject=[GitHub]%20TomoBEAR)) - `TomoBEAR` contributing developer, Ph.D. student.\n\n* Vasilii Mikirtumov[^1] ([e-mail](mailto:mikivasia@gmail.com?subject=[GitHub]%20TomoBEAR)) - `TomoBEAR` application engineer, Ph.D. student.\n\n\n[^1]: [In situ Structural Biology Group](https://www.mdc-berlin.de/kudryashev) at the [MDCMM (Max Delbrück Center of Molecular Medicine)](https://www.mdc-berlin.de) in Berlin, Germany.\n\n[^2]: [Independent Research Group (Sofja Kovaleskaja)](https://www.biophys.mpg.de/2149775/members) at the Department of Structural Biology at [MPIBP (Max Planck Institute of Biophysics)](https://www.biophys.mpg.de/en) in Frankfurt (Hesse), Germany;\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/treams",
            "repo_link": "https://github.com/tfp-photonics/treams",
            "content": {
                "codemeta": "",
                "readme": "![Version](https://img.shields.io/github/v/tag/tfp-photonics/treams)\n[![PyPI](https://img.shields.io/pypi/v/treams)](https://pypi.org/project/treams)\n![License](https://img.shields.io/github/license/tfp-photonics/treams)\n![build](https://github.com/tfp-photonics/treams/actions/workflows/build.yml/badge.svg)\n[![docs](https://github.com/tfp-photonics/treams/actions/workflows/docs.yml/badge.svg)](https://tfp-photonics.github.io/treams)\n![doctests](https://github.com/tfp-photonics/treams/actions/workflows/doctests.yml/badge.svg)\n![tests](https://github.com/tfp-photonics/treams/actions/workflows/tests.yml/badge.svg)\n[![coverage](https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Ftfp-photonics%2Ftreams%2Fhtmlcov%2Fendpoint.json)](https://htmlpreview.github.io/?https://github.com/tfp-photonics/treams/blob/htmlcov/index.html)\n\n# treams\n\nThe package `treams` provides a framework to simplify computations of the\nelectromagnetic scattering of waves at finite and at periodic arrangements of particles\nbased on the T-matrix method.\n\n## Installation\n\n### Installation using pip\n\nTo install the package with pip, use\n\n```sh\npip install treams\n```\n\nIf you're using the system wide installed version of python, you might consider the\n``--user`` option.\n\n## Documentation\n\nThe documentation can be found at https://tfp-photonics.github.io/treams.\n\n## Publications\n\nWhen using this code please cite:\n\n[D. Beutel, I. Fernandez-Corbaton, and C. Rockstuhl, treams - A T-matrix scattering code for nanophotonic computations, arXiv (preprint), 2309.03182 (2023).](https://doi.org/10.48550/arXiv.2309.03182)\n\nOther relevant publications are\n* [D. Beutel, I. Fernandez-Corbaton, and C. Rockstuhl, Unified Lattice Sums Accommodating Multiple Sublattices for Solutions of the Helmholtz Equation in Two and Three Dimensions, Phys. Rev. A 107, 013508 (2023).](https://doi.org/10.1103/PhysRevA.107.013508)\n* [D. Beutel, P. Scott, M. Wegener, C. Rockstuhl, and I. Fernandez-Corbaton, Enhancing the Optical Rotation of Chiral Molecules Using Helicity Preserving All-Dielectric Metasurfaces, Appl. Phys. Lett. 118, 221108 (2021).](https://doi.org/10.1063/5.0050411)\n* [D. Beutel, A. Groner, C. Rockstuhl, C. Rockstuhl, and I. Fernandez-Corbaton, Efficient Simulation of Biperiodic, Layered Structures Based on the T-Matrix Method, J. Opt. Soc. Am. B, JOSAB 38, 1782 (2021).](https://doi.org/10.1364/JOSAB.419645)\n\n\n## Features\n\n* [x] T-matrix calculations using a spherical or cylindrical wave basis set\n* [x] Calculations in helicity and parity (TE/TM) basis\n* [x] Scattering from clusters of particles\n* [x] Scattering from particles and clusters arranged in 3d-, 2d-, and 1d-lattices\n* [x] Calculation of light propagation in stratified media\n* [x] Band calculation in crystal structures\n\n",
                "dependencies": "[build-system]\nrequires = [\n    \"setuptools<=70.1.1\",\n    \"wheel\",\n    \"Cython\",\n    \"numpy\",\n    \"scipy\",\n    \"setuptools_scm>=6.2\"\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.isort]\nprofile = \"black\"\n\n[tool.setuptools_scm]\n\n[tool.pylint.messages_control]\nextension-pkg-whitelist = \"treams\"\n\n[tool.cibuildwheel]\narchs = [\"auto64\"]\nskip = [\"pp*\", \"*musllinux*\"]\ntest-command = \"python -m pytest {project}/tests/unit || cd .\"\ntest-extras = [\"test\", \"io\"]\n\n[tool.pytest.ini_options]\ndatadir = \"tests/datadir\"\n\n\"\"\"Packaging of treams.\"\"\"\nimport os\n\nimport numpy as np\nfrom setuptools import Extension, setup\nfrom setuptools.command.build_ext import build_ext as _build_ext\n\ntry:\n    from Cython.Build import cythonize\nexcept ImportError:\n    cythonize = None\n\n\nif os.name == \"nt\":\n    link_args = [\n        \"-static-libgcc\",\n        \"-static-libstdc++\",\n        \"-Wl,-Bstatic,--whole-archive\",\n        \"-lwinpthread\",\n        \"-Wl,--no-whole-archive\",\n    ]\n    compile_args = [\"-DMS_WIN64\"]\n\n    class build_ext(_build_ext):\n        \"\"\"build_ext for Windows.\"\"\"\n\n        def finalize_options(self):\n            \"\"\"Set compiler to gcc.\"\"\"\n            super().finalize_options()\n            self.compiler = \"mingw32\"\n\n        # https://cython.readthedocs.io/en/latest/src/tutorial/appendix.html\n        def build_extensions(self):\n            \"\"\"Add Windows specific compiler and linker arguments.\"\"\"\n            if self.compiler.compiler_type == \"mingw32\":\n                for e in self.extensions:\n                    e.extra_compile_args = compile_args\n                    e.extra_link_args = link_args\n            super().build_extensions()\n\nelse:\n    build_ext = _build_ext\n\n\n# https://cython.readthedocs.io/en/latest/src/userguide/source_files_and_compilation.html#distributing-cython-modules\ndef no_cythonize(extensions, **_ignore):\n    \"\"\"Add c and c++ code to source archive.\"\"\"\n    for extension in extensions:\n        sources = []\n        for sfile in extension.sources:\n            path, ext = os.path.splitext(sfile)\n            if ext in (\".pyx\", \".py\"):\n                if extension.language == \"c++\":\n                    ext = \".cpp\"\n                else:\n                    ext = \".c\"\n                sfile = path + ext\n            sources.append(sfile)\n        extension.sources[:] = sources\n    return extensions\n\n\nkeys = {\"include_dirs\": [np.get_include()]}\ncompiler_directives = {\"language_level\": \"3\"}\nif os.environ.get(\"CYTHON_COVERAGE\", False):\n    keys[\"define_macros\"] = [(\"CYTHON_TRACE_NOGIL\", \"1\")]\n    compiler_directives[\"linetrace\"] = True\n\nextension_names = [\n    \"treams.coeffs\",\n    \"treams.config\",\n    \"treams.cw\",\n    \"treams.pw\",\n    \"treams.sw\",\n    \"treams.lattice._dsum\",\n    \"treams.lattice._esum\",\n    \"treams.lattice._gufuncs\",\n    \"treams.lattice._misc\",\n    \"treams.lattice.cython_lattice\",\n    \"treams.special._bessel\",\n    \"treams.special._coord\",\n    \"treams.special._gufuncs\",\n    \"treams.special._integrals\",\n    \"treams.special._misc\",\n    \"treams.special._ufuncs\",\n    \"treams.special._waves\",\n    \"treams.special._wigner3j\",\n    \"treams.special._wignerd\",\n    \"treams.special.cython_special\",\n]\nextensions = [\n    Extension(name, [f\"src/{name.replace('.', '/')}.pyx\"], **keys)\n    for name in extension_names\n]\n\nif cythonize is not None:\n    try:\n        extensions = cythonize(extensions, compiler_directives=compiler_directives)\n    except ValueError:\n        extensions = no_cythonize(extensions)\nelse:\n    extensions = no_cythonize(extensions)\n\nsetup(ext_modules=extensions, cmdclass={\"build_ext\": build_ext})\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tridec-cloud",
            "repo_link": "https://github.com/locationtech-archived/geoperil",
            "content": {
                "codemeta": "",
                "readme": "<!--\nGeoPeril - A platform for the computation and web-mapping of hazard\nspecific geospatial data, as well as for serving functionality to handle,\nshare, and communicate threat specific information in a collaborative\nenvironment.\n\nCopyright (C) 2021 GFZ German Research Centre for Geosciences\n\nSPDX-License-Identifier: Apache-2.0\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n  http://apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the Licence is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the Licence for the specific language governing permissions and\nlimitations under the Licence.\n\nContributors:\n  Johannes Spazier (GFZ)\n  Sven Reissland (GFZ)\n  Martin Hammitzsch (GFZ)\n  Matthias Rüster (GFZ)\n  Hannes Fuchs (GFZ)\n-->\n\n# GeoPeril\n\nThis project is a prototype implementation of an early warning system for\ntsunamis, including:\n\n* harvesting of earthquake events from catalogs or APIs\n* automatic execution of simulations for events with given thresholds\n* remote execution of simulations with EasyWave as the simulation processing backend\n* queuing of simulation processes with support for multiple remote processing servers\n* individual accounts with different permission levels\n* modifying earthquake parameters or creating fictional earthquakes to simulate a scenario\n\n## Development environment\n\nTo start up a development environment use the `docker-compose-dev.yml` file:\n\n```shell\ncd docker\ndocker-compose -f docker-compose-dev.yml up --build\n```\n\nYou can then visit `http://localhost:8080` to see the frontend.\nDefault login credentials for an administrative account are `admin`/`admin` and\nfor a less privileged user `test`/`test`.\n\nChanges for the source code of the frontend component are then hot reloaded and\nwill be rebuild on the fly.\n\nData for world seas were downloaded from:\nhttps://catalog.data.gov/dataset/world-water-body-limits-detailed-2017mar30\n\n## License\n\nCopyright © 2021 Helmholtz Centre Potsdam - GFZ German Research Centre for Geosciences, Germany (https://www.gfz-potsdam.de)\n\nThis work is licensed under the following license(s):\n* Software files are licensed under [Apache-2.0](LICENSES/Apache-2.0.txt)\n* Everything else is licensed under [Apache-2.0](LICENSES/Apache-2.0.txt)\n\nPlease see the individual files for more accurate information.\n\n> **Hint:** We provided the copyright and license information in accordance to the [REUSE Specification 3.0](https://reuse.software/spec/).\n\n## FAQ\n\n### apt error: not signed on build\n\nThe following error may appear:\n\n```bash\nW: GPG error: http://security.debian.org/debian-security buster/updates InRelease: At least one invalid signature was encountered.\nE: The repository 'http://security.debian.org/debian-security buster/updates InRelease' is not signed.\nW: GPG error: http://deb.debian.org/debian buster InRelease: At least one invalid signature was encountered.\nE: The repository 'http://deb.debian.org/debian buster InRelease' is not signed.\nW: GPG error: http://deb.debian.org/debian buster-updates InRelease: At least one invalid signature was encountered.\nE: The repository 'http://deb.debian.org/debian buster-updates InRelease' is not signed.\nERROR: Service '...' failed to build : ... \n```\n\nThis could happen if you have an older base image cached. To solve this remove\nthe local images with: `docker image prune -a`\n\n**NOTE:** This deletes all images on your machine. Save any image you can not\ndownload from a registry!\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/trimmomatic",
            "repo_link": "https://github.com/usadellab/Trimmomatic",
            "content": {
                "codemeta": "",
                "readme": "# Trimmomatic\n# Note \nwhile the software is licensed under the GPL, the adapter sequences are *not* included in the GPL part, but owned by and used with permission of Illumina. Oligonucleotide sequences © 2023 Illumina, Inc. All rights reserved.\n# Quick start\n## Installation\nThe easiest option is to download a binary release zip, and unpack it somewhere convenient. You'll need to modify the example command lines below to reference the trimmomatic JAR file and the location of the adapter fasta files. \n\n## Build from Source\nThe current version can be built by cloning the repository, change into the top level directory and build using 'ant'.\n\nTo build from a source release, download the source zip or tar.gz, unpack it, change into top level directory (Trimmomatic-x.xx), and build using 'ant'. \n\n## Paired End:\n\nWith most new data sets you can use gentle quality trimming and adapter clipping.\n\nYou often don't need leading and traling clipping. Also in general setting the keepBothReads to True can be useful when working with paired end data, you will keep even redunfant information but this likely makes your pipelines more manageable. Note the additional :2 in front of the True (for keepBothReads) - this is the minimum adapter length in palindrome mode, you can even set this to 1. (Default is a very conservative 8)\n\nIf you have questions please don't hesitate to contact us, this is not necessarily one size fits all. (e.g. RNAseq expression analysis vs DNA assembly).\n\njava -jar trimmomatic-0.39.jar PE input_forward.fq.gz input_reverse.fq.gz output_forward_paired.fq.gz output_forward_unpaired.fq.gz output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:True LEADING:3 TRAILING:3 MINLEN:36\n \n\nfor reference only (less sensitive for adapters)\n\njava -jar trimmomatic-0.35.jar PE -phred33 input_forward.fq.gz input_reverse.fq.gz output_forward_paired.fq.gz output_forward_unpaired.fq.gz output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n\nThis will perform the following:\n\n* Remove adapters (ILLUMINACLIP:TruSeq3-PE.fa:2:30:10)\n* Remove leading low quality or N bases (below quality 3) (LEADING:3)\n* Remove trailing low quality or N bases (below quality 3) (TRAILING:3)\n* Scan the read with a 4-base wide sliding window, cutting when the average quality per base drops below 15 (SLIDINGWINDOW:4:15)\n* Drop reads below the 36 bases long (MINLEN:36)\n\n## Single End:\n\njava -jar trimmomatic-0.35.jar SE -phred33 input.fq.gz output.fq.gz ILLUMINACLIP:TruSeq3-SE:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36\n\nThis will perform the same steps, using the single-ended adapter file\n\n \n# Description\n\nTrimmomatic performs a variety of useful trimming tasks for illumina paired-end and single ended data.The selection of trimming steps and their associated parameters are supplied on the command line.\n\nThe current trimming steps are:\n\n* ILLUMINACLIP: Cut adapter and other illumina-specific sequences from the read.\n* SLIDINGWINDOW: Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.\n* LEADING: Cut bases off the start of a read, if below a threshold quality\n* TRAILING: Cut bases off the end of a read, if below a threshold quality\n* CROP: Cut the read to a specified length\n* HEADCROP: Cut the specified number of bases from the start of the read\n* MINLEN: Drop the read if it is below a specified length\n* TOPHRED33: Convert quality scores to Phred-33\n* TOPHRED64: Convert quality scores to Phred-64\n\nIt works with FASTQ (using phred + 33 or phred + 64 quality scores, depending on the Illumina pipeline used), either uncompressed or gzipp'ed FASTQ. Use of gzip format is determined based on the .gz extension.\n\nFor single-ended data, one input and one output file are specified, plus the processing steps. For paired-end data, two input files are specified, and 4 output files, 2 for the 'paired' output where both reads survived the processing, and 2 for corresponding 'unpaired' output where a read survived, but the partner read did not.\n \n# Running Trimmomatic\n\nSince version 0.27, trimmomatic can be executed using -jar. The 'old' method, using the explicit class, continues to work.\nPaired End Mode:\n\njava -jar <path to trimmomatic.jar> PE [-threads <threads] [-phred33 | -phred64] [-trimlog <logFile>] <input 1> <input 2> <paired output 1> <unpaired output 1> <paired output 2> <unpaired output 2> <step 1> ...\n\nor\n\njava -classpath <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticPE [-threads <threads>] [-phred33 | -phred64] [-trimlog <logFile>] <input 1> <input 2> <paired output 1> <unpaired output 1> <paired output 2> <unpaired output 2> <step 1> ...\nSingle End Mode:\n\njava -jar <path to trimmomatic jar> SE [-threads <threads>] [-phred33 | -phred64] [-trimlog <logFile>] <input> <output> <step 1> ...\n\nor\n\njava -classpath <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticSE [-threads <threads>] [-phred33 | -phred64] [-trimlog <logFile>] <input> <output> <step 1> ...\n\nIf no quality score is specified, phred-64 is the default. This will be changed to an 'autodetected' quality score in a future version.\n\nSpecifying a trimlog file creates a log of all read trimmings, indicating the following details:\n\n* the read name\n* the surviving sequence length\n* the location of the first surviving base, aka. the amount trimmed from the start\n* the location of the last surviving base in the original read\n* the amount trimmed from the end\n\nMultiple steps can be specified as required, by using additional arguments at the end.\n\nMost steps take one or more settings, delimited by ':' (a colon)\n\nStep options:\n\n* ILLUMINACLIP:&lt;fastaWithAdaptersEtc>:&lt;seed mismatches>:&lt;palindrome clip threshold>:&lt;simple clip threshold>\n    * fastaWithAdaptersEtc: specifies the path to a fasta file containing all the adapters, PCR sequences etc. The naming of the various sequences within this file determines how they are used. See below.\n    * seedMismatches: specifies the maximum mismatch count which will still allow a full match to be performed\n    * palindromeClipThreshold: specifies how accurate the match between the two 'adapter ligated' reads must be for PE palindrome read alignment.\n    * simpleClipThreshold: specifies how accurate the match between any adapter etc. sequence must be against a read.\n \n* SLIDINGWINDOW:&lt;windowSize>:&lt;requiredQuality>\n    * windowSize: specifies the number of bases to average across\n    * requiredQuality: specifies the average quality required.\n\n* LEADING:&lt;quality>\n    * quality: Specifies the minimum quality required to keep a base.\n\n* TRAILING:&lt;quality>\n    * quality: Specifies the minimum quality required to keep a base.\n\n* CROP:&lt;length>\n    * length: The number of bases to keep, from the start of the read.\n\n*   HEADCROP:&lt;length>\n    * length: The number of bases to remove from the start of the read.\n\n* MINLEN:&lt;length>\n    * length: Specifies the minimum length of reads to be kept.\n\n# Trimming Order\n\nTrimming occurs in the order which the steps are specified on the command line. It is recommended in most cases that adapter clipping, if required, is done as early as possible.\n \n# The Adapter Fasta\n\nIllumina adapter and other technical sequences are copyrighted by Illumina,but we have been granted permission to distribute them with Trimmomatic. Suggested adapter sequences are provided for TruSeq2 (as used in GAII machines) and TruSeq3 (as used by HiSeq and MiSeq machines), for both single-end and paired-end mode. These sequences have not been extensively tested, and depending on specific issues which may occur in library preparation, other sequences may work better for a given dataset.\n\nTo make a custom version of fasta, you must first understand how it will be used. Trimmomatic uses two strategies for adapter trimming: Palindrome and Simple\n\nWith 'simple' trimming, each adapter sequence is tested against the reads, and if a sufficiently accurate match is detected, the read is clipped appropriately.\n\n'Palindrome' trimming is specifically designed for the case of 'reading through' a short fragment into the adapter sequence on the other end. In this approach, the appropriate adapter sequences are 'in silico ligated' onto the start of the reads, and the combined adapter+read sequences, forward and reverse are aligned. If they align in a manner which indicates 'read-through', the forward read is clipped and the reverse read dropped (since it contains no new data).\n\nNaming of the sequences indicates how they should be used. For 'Palindrome' clipping, the sequence names should both start with 'Prefix', and end in '/1' for the forward adapter and '/2' for the reverse adapter. All other sequences are checked using 'simple' mode. Sequences with names ending in '/1' or '/2' will be checked only against the forward or reverse read. Sequences not ending in '/1' or '/2' will be checked against both the forward and reverse read. If you want to check for the reverse-complement of a specific sequence, you need to specifically include the reverse-complemented form of the sequence as well, with another name.\n\nThe thresholds used are a simplified log-likelihood approach. Each matching base adds just over 0.6, while each mismatch reduces the alignment score by Q/10. Therefore, a perfect match of a 12 base sequence will score just over 7, while 25 bases are needed to score 15. As such we recommend values between 7 - 15 for this parameter. For palindromic matches, a longer alignment is possible - therefore this threshold can be higher, in the range of 30. The 'seed mismatch' parameter is used to make alignments more efficient, specifying the maximum base mismatch count in the 'seed' (16 bases). Typical values here are 1 or 2.\n \n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/trixiparticlesjl",
            "repo_link": "https://github.com/trixi-framework/TrixiParticles.jl",
            "content": {
                "codemeta": "",
                "readme": "# TrixiParticles.jl\n\n[![Docs-stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://trixi-framework.github.io/TrixiParticles.jl/stable)\n[![Docs-dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://trixi-framework.github.io/TrixiParticles.jl/dev)\n[![Slack](https://img.shields.io/badge/chat-slack-e01e5a)](https://join.slack.com/t/trixi-framework/shared_invite/zt-sgkc6ppw-6OXJqZAD5SPjBYqLd8MU~g)\n[![Youtube](https://img.shields.io/youtube/channel/views/UCpd92vU2HjjTPup-AIN0pkg?style=social)](https://www.youtube.com/@trixi-framework)\n[![CI](https://github.com/trixi-framework/TrixiParticles.jl/actions/workflows/ci.yml/badge.svg)](https://github.com/trixi-framework/TrixiParticles.jl/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/github/trixi-framework/TrixiParticles.jl/branch/main/graph/badge.svg?token=RDZXYbij0b)](https://codecov.io/github/trixi-framework/TrixiParticles.jl)\n[![SciML Code Style](https://img.shields.io/static/v1?label=code%20style&message=SciML&color=9558b2&labelColor=389826)](https://github.com/SciML/SciMLStyle)\n[![License: MIT](https://img.shields.io/badge/License-MIT-success.svg)](https://opensource.org/licenses/MIT)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10797541.svg)](https://zenodo.org/doi/10.5281/zenodo.10797541)\n\n<p align=\"center\">\n  <img src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/10238714/479ff0c6-3c65-44fe-b3e0-2ed653e7e3a5\" alt=\"TrixiP_logo\" width=\"40%\"/>\n</p>\n\n**TrixiParticles.jl** is a high-performance numerical simulation framework for particle-based methods, focused on the simulation of complex multiphysics problems, and written in [Julia](https://julialang.org).\n\nTrixiParticles.jl focuses on the following use cases:\n- Accurate and efficient physics-based modelling of complex multiphysics problems.\n- Development of new particle-based methods and models.\n- Easy setup of accessible simulations for educational purposes, including student projects, coursework, and thesis work.\n\nIt offers intuitive configuration, robust pre- and post-processing, and vendor-agnostic GPU-support based on the Julia package [KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl). \n\n[![YouTube](https://github.com/user-attachments/assets/dc2be627-a799-4bfd-9226-2077f737c4b0)](https://www.youtube.com/watch?v=V7FWl4YumcA&t=4667s)\n\n## Features\n- Incompressible Navier-Stokes\n  - Methods: Weakly Compressible Smoothed Particle Hydrodynamics (WCSPH), Entropically Damped Artificial Compressibility (EDAC)\n  - Models: Surface Tension, Open Boundaries\n- Solid-body mechanics\n  - Methods:  Total Lagrangian SPH (TLSPH), Discrete Element Method (DEM)\n- Fluid-Structure Interaction\n- Particle sampling of complex geometries from `.stl` and `.asc` files.\n- Output formats:\n  - VTK\n\n## Examples\nWe provide several example simulation setups in the `examples` folder (which can be accessed from Julia via `examples_dir()`).\n\n<table align=\"center\" border=\"0\">\n  <tr>\n  </tr>\n  <tr>\n    <td align=\"center\">\n      <img src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/10238714/683e9363-5705-49cc-9a5c-3b47d73ea4b8\" style=\"width: 80% !important;\"/><br><figcaption>2D Dam Break</figcaption>\n    </td>\n    <td align=\"center\">\n      <img src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/10238714/c10faddf-0400-47c9-b225-f5d286a8ecb8\" style=\"width: 80% !important;\"/><br><figcaption>Moving Wall</figcaption>\n    </td>\n  </tr>\n  <tr>\n  </tr>\n  <tr>\n    <td align=\"center\">\n      <img src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/10238714/e05ace63-e330-441a-a391-eda3d2764074\" style=\"width: 80% !important;\"/><br><figcaption>Oscillating Beam</figcaption>\n    </td>\n    <td align=\"center\">\n      <img src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/10238714/ada0d554-e0ba-44ed-923d-2b77ef252258\" style=\"width: 80% !important;\"/><br><figcaption>Dam Break with Elastic Plate</figcaption>\n    </td>\n  </tr>\n</table>\n\n\n## Installation\nIf you have not yet installed Julia, please [follow the instructions for your\noperating system](https://julialang.org/downloads/platform/). TrixiParticles.jl works\nwith Julia v1.9 and newer. We recommend using the latest stable release of Julia.\n\n### For users\nTrixiParticles.jl is a registered Julia package.\nYou can install TrixiParticles.jl,\n[OrdinaryDiffEq.jl](https://github.com/SciML/OrdinaryDiffEq.jl) (used for time integration)\nand [Plots.jl](https://github.com/JuliaPlots/Plots.jl) by executing the following commands\nin the Julia REPL:\n```julia\njulia> using Pkg\n\njulia> Pkg.add([\"TrixiParticles\", \"OrdinaryDiffEq\", \"Plots\"])\n```\n\n### For developers\nIf you plan on editing TrixiParticles.jl itself, you can download TrixiParticles.jl\nto a local folder and use the code from the cloned directory:\n```bash\ngit clone git@github.com:trixi-framework/TrixiParticles.jl.git\ncd TrixiParticles.jl\nmkdir run\njulia --project=run -e 'using Pkg; Pkg.develop(PackageSpec(path=\".\"))' # Add TrixiParticles.jl to `run` project\njulia --project=run -e 'using Pkg; Pkg.add([\"OrdinaryDiffEq\", \"Plots\"])' # Add additional packages\n```\n\nIf you installed TrixiParticles.jl this way, you always have to start Julia with the\n`--project` flag set to your `run` directory, e.g.,\n```bash\njulia --project=run\n```\nfrom the TrixiParticles.jl root directory.\nFurther details can be found in the [documentation](https://trixi-framework.github.io/TrixiParticles.jl/stable).\n\n## Usage\n\nIn the Julia REPL, first load the package TrixiParticles.jl.\n```jldoctest getting_started\njulia> using TrixiParticles\n```\n\nThen start the simulation by executing\n```jldoctest getting_started; filter = r\".*\"s\njulia> trixi_include(joinpath(examples_dir(), \"fluid\", \"hydrostatic_water_column_2d.jl\"))\n```\n\nThis will open a new window with a 2D visualization of the final solution:\n<img src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/44124897/95821154-577d-4323-ba57-16ef02ea24e0\" width=\"400\">\n\nFurther details can be found in the [documentation](https://trixi-framework.github.io/TrixiParticles.jl/stable).\n\n## Documentation\n\nYou can find the documentation for the latest release\n[here](https://trixi-framework.github.io/TrixiParticles.jl/stable).\n\n## Publications\n\n## Cite Us\n\nIf you use TrixiParticles.jl in your own research or write a paper using results obtained\nwith the help of TrixiParticles.jl, please cite it as\n```bibtex\n@misc{trixiparticles,\n  title={{T}rixi{P}articles.jl: {P}article-based multiphysics simulations in {J}ulia},\n  author={Erik Faulhaber and Niklas Neher and Sven Berger and\n          Michael Schlottke-Lakemper and Gregor Gassner},\n  year={2024},\n  howpublished={\\url{https://github.com/trixi-framework/TrixiParticles.jl}},\n  doi={10.5281/zenodo.10797541}\n}\n```\n\n## Authors\nErik Faulhaber (University of Cologne) and Niklas Neher (HLRS) implemented the foundations\nfor TrixiParticles.jl and are principal developers along with Sven Berger (hereon).\nThe project was started by Michael Schlottke-Lakemper (University of Augsburg)\nand Gregor Gassner (University of Cologne), who provide scientific direction and technical advice.\nThe full list of contributors can be found in [AUTHORS.md](AUTHORS.md).\n\n## License and contributing\nTrixiParticles.jl is licensed under the MIT license (see [LICENSE.md](LICENSE.md)). Since TrixiParticles.jl is\nan open-source project, we are very happy to accept contributions from the\ncommunity. Please refer to [CONTRIBUTING.md](CONTRIBUTING.md) for more details.\nNote that we strive to be a friendly, inclusive open-source community and ask all members\nof our community to adhere to our [`CODE_OF_CONDUCT.md`](CODE_OF_CONDUCT.md).\nTo get in touch with the developers,\n[join us on Slack](https://join.slack.com/t/trixi-framework/shared_invite/zt-sgkc6ppw-6OXJqZAD5SPjBYqLd8MU~g)\nor [create an issue](https://github.com/trixi-framework/TrixiParticles.jl/issues/new).\n\n## Acknowledgments\n<p align=\"center\">\n  <img align=\"middle\" src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/44124897/05132bf1-180f-4228-b30a-37dfb6e36ed5\" width=20%/>&nbsp;&nbsp;&nbsp;\n  <img align=\"middle\" src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/44124897/ae2a91d1-7c10-4e0f-8b92-6ed1c43ddc28\" width=20%/>&nbsp;&nbsp;&nbsp;\n</p>\n\nThe project has benefited from funding from [hereon](https://www.hereon.de/) and [HiRSE](https://www.helmholtz-hirse.de/).\n\n",
                "dependencies": "name = \"TrixiParticles\"\nuuid = \"66699cd8-9c01-4e9d-a059-b96c86d16b3a\"\nauthors = [\"erik.faulhaber <44124897+efaulhaber@users.noreply.github.com>\"]\nversion = \"0.2.4-dev\"\n\n[deps]\nAdapt = \"79e6a3ab-5dfb-504d-930d-738a2a938a0e\"\nCSV = \"336ed68f-0bac-5ca0-87d4-7b16caf5d00b\"\nDataFrames = \"a93c6f00-e57d-5684-b7b6-d8193f3e46c0\"\nDates = \"ade2ca70-3891-5945-98fb-dc099432e06a\"\nDelimitedFiles = \"8bb1440f-4735-579b-a4ab-409b98df4dab\"\nDiffEqCallbacks = \"459566f4-90b8-5000-8ac3-15dfb0a30def\"\nFastPow = \"c0e83750-1142-43a8-81cf-6c956b72b4d1\"\nFileIO = \"5789e2e9-d7fb-5bc7-8068-2c6fae9b9549\"\nForwardDiff = \"f6369f11-7733-5829-9624-2563aa707210\"\nGPUArraysCore = \"46192b85-c4d5-4398-a991-12ede77f4527\"\nJSON = \"682c06a0-de6a-54ab-a142-c8b1cf79cde6\"\nKernelAbstractions = \"63c18a36-062a-441e-b654-da1e3ab1ce7c\"\nLinearAlgebra = \"37e2e46d-f89d-539d-b4ee-838fcccc9c8e\"\nMuladdMacro = \"46d2c3a1-f734-5fdb-9937-b9b9aeba4221\"\nPointNeighbors = \"1c4d5385-0a27-49de-8e2c-43b175c8985c\"\nPolyester = \"f517fe37-dbe3-4b94-8317-1923a5111588\"\nPrintf = \"de0858da-6303-5e67-8744-51eddeeeb8d7\"\nRandom = \"9a3f8284-a2c9-5f02-9a11-845980a1fd5c\"\nRecipesBase = \"3cdcf5f2-1ef4-517c-9805-6587b60abb01\"\nReexport = \"189a3867-3050-52da-a836-e630ba90ab69\"\nSciMLBase = \"0bca4576-84f4-4d90-8ffe-ffa030f20462\"\nStaticArrays = \"90137ffa-7385-5640-81b9-e52037218182\"\nStrideArrays = \"d1fa6d79-ef01-42a6-86c9-f7c551f8593b\"\nTimerOutputs = \"a759f4b9-e2f1-59dc-863e-4aeb61b1ea8f\"\nTrixiBase = \"9a0f1c46-06d5-4909-a5a3-ce25d3fa3284\"\nWriteVTK = \"64499a7a-5c06-52f2-abe2-ccb03c286192\"\n\n[compat]\nAdapt = \"3, 4\"\nCSV = \"0.10\"\nDataFrames = \"1.6\"\nDelimitedFiles = \"1\"\nDiffEqCallbacks = \"2, 3, 4\"\nFastPow = \"0.1\"\nFileIO = \"1\"\nForwardDiff = \"0.10\"\nGPUArraysCore = \"0.1, 0.2\"\nJSON = \"0.21\"\nKernelAbstractions = \"0.9\"\nMuladdMacro = \"0.2\"\nPointNeighbors = \"0.4.2\"\nPolyester = \"0.7.10\"\nRecipesBase = \"1\"\nReexport = \"1\"\nSciMLBase = \"2\"\nStaticArrays = \"1\"\nStrideArrays = \"0.1\"\nTimerOutputs = \"0.5.25\"\nTrixiBase = \"0.1.3\"\nWriteVTK = \"1\"\njulia = \"1.9\"\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tsmp",
            "repo_link": "https://github.com/HPSCTerrSys/TSMP",
            "content": {
                "codemeta": "",
                "readme": "\n# Terrestrial System Modeling Platform - TSMP\n\n[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/HPSCTerrSys/TSMP/RenderMasterSphinxDocumentation.yml?label=documentation)](https://hpscterrsys.github.io/TSMP/index.html)\n[![Latest release](https://img.shields.io/github/v/tag/HPSCTerrSys/TSMP.svg?color=brightgreen&label=latest%20release&sort=semver)](https://github.com/HPSCTerrSys/TSMP/tags) \n[![GitHub last commit](https://img.shields.io/github/last-commit/HPSCTerrSys/TSMP)](https://github.com/HPSCTerrSys/TSMP/commits/master)\n[![Twitter Follow](https://img.shields.io/twitter/follow/HPSCTerrSys?style=social)](https://twitter.com/HPSCTerrSys)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8283715.svg)](https://doi.org/10.5281/zenodo.8283715)\n\n\n## Introduction \n\nThe Terrestrial System Modeling Platform (TSMP or TerrSysMP, https://www.terrsysmp.org) is an open source scale-consistent, highly modular, massively parallel regional Earth system model. TSMP essentially consists of an interface which couples dedicated versions of the Consortium for Small-scale Modeling ([COSMO](http://www.cosmo-model.org)) or ICOsahedral Nonhydrostatic ([ICON](https://code.mpimet.mpg.de/projects/iconpublic)) atmospheric model in NWP or climate mode, the Community Land Model ([CLM](http://www.cesm.ucar.edu/models/clm/)), and the hydrologic model [ParFlow](https://www.parflow.org) through the [OASIS3](https://oasis.cerfacs.fr/en/)-[MCT](https://www.mcs.anl.gov/research/projects/mct/) coupler.\n\nTSMP allows for a physically-based representation of transport processes of mass, energy and momentum and interactions between the different compartments of the geo-ecosystem across scales, explicitly reproducing feedbacks in the hydrological cycle from the groundwater into the atmosphere.\n\nTSMP is extensively used for idealized and real data process and sensitivity studies in water cycle research, for climate change simulations, data assimilation studies including reanalyses, as well as experimental real time forecasting and monitoring simulations, ranging from individual catchments to continental model domains. TSMP runs on notebooks as well on latest supercomputers using a range of compilers.\n\nTSMP development has been driven by groups within the [Center for High-Performance Scientific Computing in Terrestrial Systems](http://www.hpsc-terrsys.de) (HPSC-TerrSys), as part of the [Geoverbund ABC/J](http://www.geoverbund-abcj.de/geoverbund/EN/Home/home_node.html), the geoscientific network of the University of Cologne, Bonn University, RWTH Aachen University, and the Research Centre Jülich. The current team is anchored in Jülich and Bonn in Germany.\n\n**Visit**\n\n**https://www.terrsysmp.org**\n\n**for information on the features of TSMP, ongoing developments, citation, usage examples, links to documentation, the team, contact information and publications.**\n\n## Quick Start on Linux\n\nPlease see [getting started section](https://hpscterrsys.github.io/TSMP/content/gettingstarted.html) for guided steps on how the model can be setup and configured for *one* specific experiment, which we use as one of the default test cases. To get an overview on possible TSMP applications refer to the [TSMP website](https://www.terrsysmp.org) and the [TSMP documention](https://hpscterrsys.github.io/TSMP/index.html).\n\n## TSMP version history\nThe model components used in TSMP are OASIS3-MCT v2, COSMO v5.01, CLM v3.5, ParFlow 3.2 for TSMP versions v1.2.1, v1.2.2 and v1.2.3, ParFlow 3.9 for version v1.3.3 and ParFlow 3.12 for version v1.4.0. TSMP supports ParFlow 3.7 onwards from version v1.3.3 onward. \n\n## Citing TSMP\n\nIf you use TSMP in a publication, please cite the these papers that describe the model's basic functionalities:\n\n* Shrestha, P., Sulis, M., Masbou, M., Kollet, S., and Simmer, C. (2014). A Scale-Consistent Terrestrial Systems Modeling Platform Based on COSMO, CLM, and ParFlow. Monthly Weather Review, 142(9), 3466–3483. doi:[10.1175/MWR-D-14-00029.1](https://dx.doi.org/10.1175/MWR-D-14-00029.1).\n* Gasper, F., Goergen, K., Kollet, S., Shrestha, P., Sulis, M., Rihani, J., and Geimer, M. (2014). Implementation and scaling of the fully coupled Terrestrial Systems Modeling Platform (TerrSysMP) in a massively parallel supercomputing environment &ndash; a case study on JUQUEEN (IBM Blue Gene/Q). Geoscientific Model Development, 7(5), 2531-2543. doi:[10.5194/gmd-7-2531-2014](https://dx.doi.org/10.5194/gmd-7-2531-2014).\n\n## License\nTSMP is open source software and is licensed under the [MIT-License](https://github.com/HPSCTerrSys/TSMP/blob/master/LICENSE).\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ukis-csmask",
            "repo_link": "https://github.com/dlr-eoc/ukis-csmask",
            "content": {
                "codemeta": "",
                "readme": "# [![UKIS](img/ukis-logo.png)](https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-5413/10560_read-21914/) ukis-csmask\n\n![ukis-csmask](https://github.com/dlr-eoc/ukis-csmask/workflows/ukis-csmask/badge.svg)\n[![codecov](https://codecov.io/gh/dlr-eoc/ukis-csmask/branch/main/graph/badge.svg)](https://codecov.io/gh/dlr-eoc/ukis-csmask)\n![Upload Python Package](https://github.com/dlr-eoc/ukis-csmask/workflows/Upload%20Python%20Package/badge.svg)\n[![PyPI version](https://img.shields.io/pypi/v/ukis-csmask)](https://pypi.python.org/pypi/ukis-csmask/)\n[![GitHub license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)\n[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://black.readthedocs.io/en/stable/)\n[![DOI](https://zenodo.org/badge/328616234.svg)](https://zenodo.org/badge/latestdoi/328616234)\n\nUKIS Cloud Shadow MASK (ukis-csmask) package masks clouds and cloud shadows in Sentinel-2, Landsat-9, Landsat-8, Landsat-7 and Landsat-5 images. Masking is performed with a pre-trained convolution neural network. It is fast and works directly on Level-1C data (no atmospheric correction required). Images just need to be in Top Of Atmosphere (TOA) reflectance and include at least the \"blue\", \"green\", \"red\" and \"nir\" spectral bands. Best performance (in terms of accuracy and speed) is achieved when images also include \"swir16\" and \"swir22\" spectral bands and are resampled to approximately 30 m spatial resolution.\n\nThis [publication](https://doi.org/10.1016/j.rse.2019.05.022) provides further insight into the underlying algorithm and compares it to the widely used [Fmask](http://www.pythonfmask.org/en/latest/) algorithm across a heterogeneous test dataset.\n\n> Wieland, M.; Li, Y.; Martinis, S. Multi-sensor cloud and cloud shadow segmentation with a convolutional\nneural network. *Remote Sensing of Environment*, 2019, 230, 1-12. [https://doi.org/10.1016/j.rse.2019.05.022](https://doi.org/10.1016/j.rse.2019.05.022)\n\nThis [publication](https://doi.org/10.5194/isprs-archives-XLIII-B3-2022-217-2022) introduces the Python package, performs additional evaluation on recent cloud and cloud shadow benchmark datasets and tests the applicability of ukis-csmask on Landsat-9 imagery.\n\n> Wieland, M.; Fichtner, F.; Martinis, S. UKIS-CSMASK: A Python package for multi-sensor cloud and cloud shadow segmentation. *Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.*, 2022, XLIII-B3-2022, 217–222. [https://doi.org/10.5194/isprs-archives-XLIII-B3-2022-217-2022](https://doi.org/10.5194/isprs-archives-XLIII-B3-2022-217-2022)\n\nIf you use ukis-csmask in your work, please consider citing one of the above publications.\n\n![Examples](img/examples.png)\n\n## Example (Sentinel 2)\nHere's an example on how to compute a cloud and cloud shadow mask from an image. Please note that here we use [ukis-pysat](https://github.com/dlr-eoc/ukis-pysat) for convencience image handling, but you can also work directly with [numpy](https://numpy.org/) arrays.\n\n````python\nfrom ukis_csmask.mask import CSmask\nfrom ukis_pysat.raster import Image, Platform\n\n# read Level-1C image from file, convert digital numbers to TOA reflectance\n# and make sure resolution is 30 m to get best performance\nimg = Image(data=\"sentinel2.tif\", dimorder=\"last\")\nimg.dn2toa(platform=Platform.Sentinel2)\nimg.warp(\n    resampling_method=0,\n    resolution=30,\n    dst_crs=img.dataset.crs\n)\n\n# compute cloud and cloud shadow mask\n# NOTE: band_order must match the order of bands in the input image. it does not have to be in this explicit order.\n# make sure to use these six spectral bands to get best performance\ncsmask = CSmask(\n    img=img.arr,\n    band_order=[\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"],\n    nodata_value=0,\n)\n\n# access cloud and cloud shadow mask\ncsmask_csm = csmask.csm\n\n# access valid mask\ncsmask_valid = csmask.valid\n\n# convert results to UKIS-pysat Image\ncsmask_csm = Image(csmask.csm, transform=img.dataset.transform, crs=img.dataset.crs, dimorder=\"last\")\ncsmask_valid = Image(csmask.valid, transform=img.dataset.transform, crs=img.dataset.crs, dimorder=\"last\")\n\n# write results back to file\ncsmask_csm.write_to_file(\"sentinel2_csm.tif\", dtype=\"uint8\", compress=\"PACKBITS\")\ncsmask_valid.write_to_file(\"sentinel2_valid.tif\", dtype=\"uint8\", compress=\"PACKBITS\", kwargs={\"nbits\":2})\n````\n\n## Example (Landsat 8)\nHere's a similar example based on Landsat 8.\n\n````python\nimport rasterio\nimport numpy as np\nfrom ukis_csmask.mask import CSmask\nfrom ukis_pysat.raster import Image, Platform\n\n# set Landsat 8 source path and prefix (example)\ndata_path = \"/your_data_path/\"\nL8_file_prefix = \"LC08_L1TP_191015_20210428_20210507_02_T1\"\n\ndata_path = data_path+L8_file_prefix+\"/\"\nmtl_file  = data_path+L8_file_prefix+\"_MTL.txt\"\n\n# stack [B2:'Blue', B3:'Green', B4:'Red', B5:'NIR', B6:'SWIR1', B7:'SWIR2'] as numpy array\nL8_band_files  = [data_path+L8_file_prefix+'_B'+ x + '.TIF' for x in [str(x+2) for x in range(6)]]\n\n# >> adopted from https://gis.stackexchange.com/questions/223910/using-rasterio-or-gdal-to-stack-multiple-bands-without-using-subprocess-commands\n# read metadata of first file\nwith rasterio.open(L8_band_files[0]) as src0:\n    meta = src0.meta\n# update meta to reflect the number of layers\nmeta.update(count = len(L8_band_files))\n# read each layer and append it to numpy array\nL8_bands = []\nfor id, layer in enumerate(L8_band_files, start=1):\n    with rasterio.open(layer) as src1:\n        L8_bands.append(src1.read(1))\nL8_bands = np.stack(L8_bands,axis=2)\n# <<\n\nimg = Image(data=L8_bands, crs = meta['crs'], transform = meta['transform'], dimorder=\"last\")\n\nimg.dn2toa(\n        platform=Platform.Landsat8,\n        mtl_file=mtl_file,\n        wavelengths = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"]\n)\n# >> proceed by analogy with Sentinel 2 example\n````\n\n## Installation\nThe easiest way to install ukis-csmask is through pip. To install ukis-csmask with [default CPU provider](https://onnxruntime.ai/docs/execution-providers/) run the following.\n\n```shell\npip install ukis-csmask[cpu]\n```\n\nTo install ukis-csmask with [OpenVino support](https://onnxruntime.ai/docs/execution-providers/OpenVINO-ExecutionProvider.html) for enhanced CPU inference run the following instead.\n\n```shell\npip install ukis-csmask[openvino]\n```\n\nTo install ukis-csmask with [GPU support](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html) run the following instead. This requires that you have a GPU with CUDA runtime libraries installed on the system.\n\n```shell\npip install ukis-csmask[gpu]\n```\n\nukis-csmask depends on [onnxruntime](https://onnxruntime.ai/). For a list of additional dependencies check the [requirements](https://github.com/dlr-eoc/ukis-csmask/blob/main/requirements.txt).\n\n## Contributors\nThe UKIS team creates and adapts libraries which simplify the usage of satellite data. Our team includes (in alphabetical order):\n* Boehnke, Christian\n* Fichtner, Florian\n* Mandery, Nico\n* Martinis, Sandro\n* Riedlinger, Torsten\n* Wieland, Marc\n\nGerman Aerospace Center (DLR)\n\n## Licenses\nThis software is licensed under the [Apache 2.0 License](https://github.com/dlr-eoc/ukis-csmask/blob/main/LICENSE).\n\nCopyright (c) 2020 German Aerospace Center (DLR) * German Remote Sensing Data Center * Department: Geo-Risks and Civil Security\n\n## Changelog\nSee [changelog](https://github.com/dlr-eoc/ukis-csmask/blob/main/CHANGELOG.rst).\n\n## Contributing\nThe UKIS team welcomes contributions from the community.\nFor more detailed information, see our guide on [contributing](https://github.com/dlr-eoc/ukis-csmask/blob/main/CONTRIBUTING.md) if you're interested in getting involved.\n\n## What is UKIS?\nThe DLR project Environmental and Crisis Information System (the German abbreviation is UKIS, standing for [Umwelt- und Kriseninformationssysteme](https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-5413/10560_read-21914/) aims at harmonizing the development of information systems at the German Remote Sensing Data Center (DFD) and setting up a framework of modularized and generalized software components.\n\nUKIS is intended to ease and standardize the process of setting up specific information systems and thus bridging the gap from EO product generation and information fusion to the delivery of products and information to end users.\n\nFurthermore the intention is to save and broaden know-how that was and is invested and earned in the development of information systems and components in several ongoing and future DFD projects.\n\n",
                "dependencies": "numpy\r\nscipy\r\n\n#!/usr/bin/env python3\r\nimport codecs\r\nimport os\r\n\r\nfrom setuptools import setup, find_packages\r\n\r\n\r\nwith open(r\"README.md\", encoding=\"utf8\") as f:\r\n    long_description = f.read()\r\n\r\n\r\ndef read(rel_path):\r\n    here = os.path.abspath(os.path.dirname(__file__))\r\n    with codecs.open(os.path.join(here, rel_path), \"r\") as fp:\r\n        return fp.read()\r\n\r\n\r\ndef get_version(rel_path):\r\n    for line in read(rel_path).splitlines():\r\n        if line.startswith(\"__version__\"):\r\n            delim = '\"' if '\"' in line else \"'\"\r\n            return line.split(delim)[1]\r\n    else:\r\n        raise RuntimeError(\"Unable to find version string.\")\r\n\r\n\r\nsetup(\r\n    name=\"ukis-csmask\",\r\n    version=get_version(os.path.join(\"ukis_csmask\", \"__init__.py\")),\r\n    url=\"https://github.com/dlr-eoc/ukis-csmask\",\r\n    author=\"German Aerospace Center (DLR)\",\r\n    author_email=\"ukis-helpdesk@dlr.de\",\r\n    license=\"Apache 2.0\",\r\n    description=\"masks clouds and cloud shadows in Sentinel-2, Landsat-8, Landsat-7 and Landsat-5 images\",\r\n    zip_safe=False,\r\n    packages=find_packages(),\r\n    install_requires=open(\"requirements.txt\").read().splitlines(),\r\n    extras_require={\r\n        \"cpu\": [\r\n            \"onnxruntime\",\r\n        ],\r\n        \"gpu\": [\r\n            \"onnxruntime-gpu\",\r\n        ],\r\n        \"openvino\": [\r\n            \"onnxruntime-openvino\",\r\n        ],\r\n        \"dev\": [\r\n            \"pytest\",\r\n        ],\r\n    },\r\n    classifiers=[\r\n        \"Programming Language :: Python :: 3\",\r\n        \"Operating System :: OS Independent\",\r\n        \"Development Status :: 4 - Beta\",\r\n        \"License :: OSI Approved :: Apache Software License\",\r\n        \"Topic :: Scientific/Engineering :: GIS\",\r\n    ],\r\n    python_requires=\">=3.8\",\r\n    long_description=long_description,\r\n    long_description_content_type=\"text/markdown\",\r\n    include_package_data=True,\r\n)\r\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ultimodel",
            "repo_link": "https://github.com/DLR-VF/ULTImodel",
            "content": {
                "codemeta": "",
                "readme": "# ULTImodel\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/DLR-VF/ULTImodel/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/ultimodel.svg)](https://pypi.python.org/pypi/ultimodel)\n[![Documentation Status](https://readthedocs.org/projects/ultimodel/badge/?version=latest)](https://ultimodel.readthedocs.io/en/latest/?badge=latest)\n[![Cite-us](https://img.shields.io/badge/doi-10.5281%2Fzenodo.7826486-blue)](https://doi.org/10.5281/zenodo.7826486)\n \n**ULTImodel** &mdash; A universal transport distribution model written in Python.\n\n## Description\n**ULTImodel** is a distribution model that helps to spatially distribute road-based transport for countries, including border-crossing travel. It is set up using open data like [OSM](https://openstreetmap.org).\nThe software includes modules for network generation, trip generation and trip distribution based on two main inputs:\n\n* Georeferenced traffic analysis zones (TAZ) for the respective region\n* Target value for national transport volume (i.e. person-kilometres or tonne-kilometres)\n\n![Prim_Sec](ultimodel-mkdocs/docs/images/readme_visual_fr.png \"Results of distribution and secondary model\")\n\n## Installation\n\nThe __current version__ is [ultimodel-1.0.0](https://github.com/DLR-VF/ULTImodel/releases/tag/1.0.0).\n\nYou may __install ULTImodel__ by executing the following\n\n__pip__\n```console\npython -m pip install ultimodel\n```\n\nYou may __download a copy or fork the code__ at [ULTImodel&apos;s github page]([link-to-github](https://github.com/DLR-VF/ULTImodel)).\n\nBesides, you may __download the current release__ here:\n\n* [ultimodel-1.0.1.zip](https://github.com/DLR-VF/ULTImodel/archive/refs/tags/1.0.1.zip)\n* [ultimodel-1.0.1.tar.gz](https://github.com/DLR-VF/ULTImodel/archive/refs/tags/1.0.1.tar.gz)\n\n\n## Usage\nExamples of using **ULTImodel** can be found in the [tutorials repository](https://github.com/DLR-VF/ULTImodel-tutorials).\n\nAdditional documentation can be found at <https://ultimodel.readthedocs.io/>.\n\n\n## Authors and acknowledgment\n**ULTImodel** was developed by Nina Thomsen.\n\nWe want to thank the following persons for the help during **ULTImodel's** development: Lars Hedemann, Simon Metzler, Gernot Liedtke, Christian Winkler, Tudor Mocanu, and Daniel Krajzewicz.\n\n## License\n**ULTImodel** is licensed under the [MIT license](https://github.com/DLR-VF/ULTImodel/blob/master/LICENSE).\n\n## Links\nPlease find further information on the web:\n\n* The complete documentation is located at <https://ultimodel.readthedocs.io/>\n* The github repository is located at <https://github.com/DLR-VF/ULTImodel>\n* The issue tracker is located at <https://github.com/DLR-VF/ULTImodel/issues>\n\n## Legal\n\nPlease find the legal information here: <https://github.com/DLR-VF/ULTImodel/blob/master/ultimodel-mkdocs/docs/legal.md>\n\n\n\n\n",
                "dependencies": "pandas\nnumpy\ngeopandas\nshapely\nnetworkx\nosmnx\ntqdm\nrequests\nscikit-learn\nscipy\njinja2==3.0.3\nmkdocs\nmkdocs-material\nmkdocstrings-python\nmkdocstrings\n\n# =========================================================\n# setup.py\n# @author Nina Thomsen\n# @date 20.03.2023\n# @copyright Institut fuer Verkehrsforschung,\n#            Deutsches Zentrum fuer Luft- und Raumfahrt\n# @brief setup module for ULTIMO\n# =========================================================\n\nimport setuptools\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\n'''\nimport required packages from requirements file\nwith open(\"requirements.txt\") as f:\n    INSTALL_REQUIRES = [line.strip() for line in f.readlines()]'''\n\nsetuptools.setup(\n    name='ultimodel',\n    version='1.0.1',\n    author='German Aerospace Center - DLR (Nina Thomsen)',\n    author_email='nina.thomsen@dlr.de',\n    description='Universal transport distribution model',\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url='https://github.com/DLR-VF/ULTImodel',\n    project_urls = {\n        \"Documentation\": 'https://ultimodel.readthedocs.io/',\n        \"Source\": 'https://github.com/DLR-VF/ULTImodel',\n        \"Bug Tracker\": \"https://github.com/DLR-VF/ULTImodel/issues \"\n    },\n    license='MIT',\n    packages=['ultimodel'],\n    install_requires=['pandas', 'numpy', 'geopandas', 'shapely', 'osmnx', 'networkx', 'tqdm', 'requests', 'scikit-learn', 'scipy']\n)\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ultramassexplorer-ume",
            "repo_link": "https://gitlab.awi.de/bkoch/ume",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/unicore",
            "repo_link": "https://github.com/UNICORE-EU/",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/uqtestfuns",
            "repo_link": "https://github.com/casus/uqtestfuns",
            "content": {
                "codemeta": "",
                "readme": "# UQTestFuns\n[![JOSS](https://img.shields.io/badge/JOSS-10.21105/joss.05671-brightgreen?style=flat-square)](https://doi.org/10.21105/joss.05671)\n[![DOI](http://img.shields.io/badge/DOI-10.5281/zenodo.14181563-blue.svg?style=flat-square)](https://doi.org/10.5281/zenodo.14181563)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)\n[![Python 3.7](https://img.shields.io/badge/python-3.7-blue.svg?style=flat-square)](https://www.python.org/downloads/release/python-370/)\n[![License](https://img.shields.io/github/license/damar-wicaksono/uqtestfuns?style=flat-square)](https://choosealicense.com/licenses/mit/)\n[![PyPI](https://img.shields.io/pypi/v/uqtestfuns?style=flat-square)](https://pypi.org/project/uqtestfuns/)\n\n|                                  Branches                                  | Status                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n|:--------------------------------------------------------------------------:|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [`main`](https://github.com/damar-wicaksono/uqtestfuns/tree/main) (stable) | ![build](https://img.shields.io/github/actions/workflow/status/damar-wicaksono/uqtestfuns/main.yml?branch=main&style=flat-square) [![codecov](https://img.shields.io/codecov/c/github/damar-wicaksono/uqtestfuns/main?logo=CodeCov&style=flat-square&token=Y6YQEPJ1TT)](https://app.codecov.io/gh/damar-wicaksono/uqtestfuns/tree/main) [![Docs](https://readthedocs.org/projects/uqtestfuns/badge/?version=stable&style=flat-square)](https://uqtestfuns.readthedocs.io/en/stable/?badge=stable) |\n|  [`dev`](https://github.com/damar-wicaksono/uqtestfuns/tree/dev) (latest)  | ![build](https://img.shields.io/github/actions/workflow/status/damar-wicaksono/uqtestfuns/main.yml?branch=dev&style=flat-square) [![codecov](https://img.shields.io/codecov/c/github/damar-wicaksono/uqtestfuns/dev?logo=CodeCov&style=flat-square&token=Y6YQEPJ1TT)](https://app.codecov.io/gh/damar-wicaksono/uqtestfuns/tree/dev) [![Docs](https://readthedocs.org/projects/uqtestfuns/badge/?version=latest&style=flat-square)](https://uqtestfuns.readthedocs.io/en/latest/?badge=latest)    |\n\n<!--One paragraph description-->\nUQTestFuns is an open-source Python3 library of test functions commonly used\nwithin the applied uncertainty quantification (UQ) community.\nSpecifically, the package provides:\n\n- an implementation _with minimal dependencies_ (i.e., NumPy and SciPy) and\n  _a common interface_ of many test functions available in the UQ literature\n- a _single entry point_ collecting test functions _and_ their probabilistic\n  input specifications in a single Python package\n- an _opportunity for an open-source contribution_, supporting\n  the implementation of new test functions or posting reference results.\n\nIn short, UQTestFuns is an homage\nto the [Virtual Library of Simulation Experiments (VLSE)](https://www.sfu.ca/~ssurjano/).\n\n## Usage\n\nUQTestFuns includes several commonly used test functions in the UQ community.\nTo list the available functions:\n\n```python-repl\n>>> import uqtestfuns as uqtf\n>>> uqtf.list_functions()\n+-------+-------------------------------+-----------+------------+----------+---------------+--------------------------------+\n|  No.  |          Constructor          |  # Input  |  # Output  |  Param.  |  Application  | Description                    |\n+=======+===============================+===========+============+==========+===============+================================+\n|   1   |           Ackley()            |     M     |     1      |   True   | optimization, | Optimization test function     |\n|       |                               |           |            |          | metamodeling  | from Ackley (1987)             |\n+-------+-------------------------------+-----------+------------+----------+---------------+--------------------------------+\n|   2   |        Alemazkoor20D()        |    20     |     1      |  False   | metamodeling  | High-dimensional low-degree    |\n|       |                               |           |            |          |               | polynomial from Alemazkoor &   |\n|       |                               |           |            |          |               | Meidani (2018)                 |\n+-------+-------------------------------+-----------+------------+----------+---------------+--------------------------------+\n|   3   |        Alemazkoor2D()         |     2     |     1      |  False   | metamodeling  | Low-dimensional high-degree    |\n|       |                               |           |            |          |               | polynomial from Alemazkoor &   |\n|       |                               |           |            |          |               | Meidani (2018)                 |\n+-------+-------------------------------+-----------+------------+----------+---------------+--------------------------------+\n|   4   |          Borehole()           |     8     |     1      |  False   | metamodeling, | Borehole function from Harper  |\n|       |                               |           |            |          |  sensitivity  | and Gupta (1983)               |\n+-------+-------------------------------+-----------+------------+----------+---------------+--------------------------------+\n...\n```\n\nConsider the Borehole function, a test function commonly used for metamodeling\nand sensitivity analysis purposes; to create an instance of this test function:\n\n```python-repl\n>>> my_testfun = uqtf.Borehole()\n>>> print(my_testfun)\nFunction ID      : Borehole\nInput Dimension  : 8 (fixed)\nOutput Dimension : 1\nParameterized    : False\nDescription      : Borehole function from Harper and Gupta (1983)\nApplications     : metamodeling, sensitivity\n```\n\nThe probabilistic input specification of this test function is built-in:\n\n```python-repl\n>>> print(my_testfun.prob_input)\nFunction ID     : Borehole\nInput ID        : Harper1983\nInput Dimension : 8\nDescription     : Probabilistic input model of the Borehole model from\n                  Harper and Gupta (1983)\nMarginals       :\n\n No.    Name    Distribution        Parameters                          Description\n-----  ------  --------------  ---------------------  -----------------------------------------------\n  1      rw        normal      [0.1       0.0161812]            radius of the borehole [m]\n  2      r       lognormal        [7.71   1.0056]                 radius of influence [m]\n  3      Tu       uniform        [ 63070. 115600.]      transmissivity of upper aquifer [m^2/year]\n  4      Hu       uniform          [ 990. 1100.]         potentiometric head of upper aquifer [m]\n  5      Tl       uniform          [ 63.1 116. ]        transmissivity of lower aquifer [m^2/year]\n  6      Hl       uniform           [700. 820.]          potentiometric head of lower aquifer [m]\n  7      L        uniform          [1120. 1680.]                length of the borehole [m]\n  8      Kw       uniform         [ 9985. 12045.]     hydraulic conductivity of the borehole [m/year]\n\nCopulas         : Independence\n```\n\nA sample of input values can be generated from the input model:\n\n```python-repl\n>>> xx = my_testfun.prob_input.get_sample(10)\narray([[8.40623544e-02, 2.43926544e+03, 8.12290909e+04, 1.06612711e+03,\n        7.24216436e+01, 7.78916695e+02, 1.13125867e+03, 1.02170796e+04],\n       [1.27235295e-01, 3.28026293e+03, 6.36463631e+04, 1.05132831e+03,\n        6.81653728e+01, 8.17868370e+02, 1.16603931e+03, 1.09370944e+04],\n       [8.72711602e-02, 7.22496512e+02, 9.18506063e+04, 1.06436843e+03,\n        6.44306474e+01, 7.74700231e+02, 1.46266808e+03, 1.12531788e+04],\n       [1.22301709e-01, 2.29922122e+02, 8.00390345e+04, 1.05290108e+03,\n        1.10852262e+02, 7.94709283e+02, 1.28026313e+03, 1.01879077e+04],\n...\n```\n\n...and used to evaluate the test function:\n\n```python-repl\n>>> yy = my_testfun(xx)\narray([ 57.32635774, 110.12229548,  53.10585812,  96.15822154,\n        58.51714875,  89.40068404,  52.61710076,  61.47419171,\n        64.18005235,  79.00454634])\n```\n\n## Installation\n\nYou can obtain UQTestFuns directly from PyPI using `pip`:\n\n```bash\n$ pip install uqtestfuns\n```\n\nAlternatively, you can also install the latest version from the source:\n\n```bash\npip install git+https://github.com/damar-wicaksono/uqtestfuns.git\n```\n\n> **NOTE**: UQTestFuns is currently work in progress,\n> therefore interfaces are subject to change.\n\nIt's a good idea to install the package in an isolated virtual environment.\n\n## Getting help\n\n<!--Getting help-->\nFor a getting-started guide on UQTestFuns,\nplease refer to the [Documentation](https://uqtestfuns.readthedocs.io/en/latest/).\nThe documentation also includes details on each of the available test functions.\n\nFor any other questions related to the package,\npost your questions on the GitHub Issue page.\n\n## Package development and contribution\n\n<!--Package Development-->\nUQTestFuns is under ongoing development;\nany contribution to the code (for example, a new test function)\nand the documentation (including new reference results) are welcomed!\n\nPlease consider the [Contribution Guidelines](CONTRIBUTING.MD) first,\nbefore making a pull request. \n\n## Citing UQTestFuns\n\nIf you use this package in your research or projects, please consider citing\nboth the associated paper and the Zenodo archive (for the specific version\nused).\n\n### Citing the paper (JOSS)\n\nThe citation of the paper associated with this package is:\n\n```bibtex\n@article{Wicaksono2023,\n  author    = {Wicaksono, Damar and Hecht, Michael},\n  title     = {{UQTestFuns}: A {Python3} library of uncertainty quantification ({UQ}) test functions},\n  journal   = {Journal of Open Source Software},\n  year      = {2023},\n  volume    = {8},\n  number    = {90},\n  doi       = {10.21105/joss.05671},\n}\n```\n\n### Citing a specific version (Zenodo)\n\nTo ensure reproducibility, cite the exact version of the package you used.\nEach release is archived on Zenodo with a unique DOI; find and use the DOI\nfor the version you used at [Zenodo].\n\nThe citation for the current public version is:\n\n```bibtex\n@software{UQTestFuns_0_5_0,\n  author       = {Wicaksono, Damar and Hecht, Michael},\n  title        = {{UQTestFuns: A Python3 Library of Uncertainty Quantification (UQ) Test Functions}},\n  month        = nov,\n  year         = 2024,\n  publisher    = {Zenodo},\n  version      = {v0.5.0},\n  doi          = {10.5281/zenodo.14181563},\n  url          = {https://doi.org/10.5281/zenodo.14181563}\n}\n```\n\n## Credits and contributors\n\n<!--Credits and contributors-->\nThis work was partly funded\nby the [Center for Advanced Systems Understanding (CASUS)](https://www.casus.science/)\nwhich is financed by Germany's Federal Ministry of Education and Research (BMBF)\nand by the Saxony Ministry for Science, Culture and Tourism (SMWK)\nwith tax funds on the basis of the budget approved\nby the Saxony State Parliament.\n\nUQTestFuns is currently maintained by:\n\n- Damar Wicaksono ([HZDR/CASUS](https://www.casus.science/))\n\nunder the Mathematical Foundations of Complex System Science Group\nled by Michael Hecht ([HZDR/CASUS](https://www.casus.science/)) at CASUS.\n\n## License\n\n<!--License-->\nUQTestFuns is released under the [MIT License](LICENSE).\n\n[Zenodo]: https://zenodo.org/search?q=parent.id%3A7701903&f=allversions%3Atrue&l=list&p=1&s=10&sort=version\n\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.black]\nline-length = 79\ntarget-version = [\"py37\", \"py38\", \"py39\", \"py310\"]\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/urbem-urban-emission-downscaling-for-air-quality-modeling",
            "repo_link": "https://github.com/martinottopaul/UrbEm",
            "content": {
                "codemeta": "",
                "readme": "# UrbEm v1.0.0\nThe Urban Emission downscaling model (UrbEm)for air quality modeling\n\n0. Introduction\n\nThe use of regional emission inventories can be challenging for urban-scale AQ applications and air quality management in cities. Nevertheless, their exploitation through disaggregation by utilizing spatial proxies is a credible solution for European cities that lack bottom-up emission inventories. \n\nTo this end, we developed the UrbEm approach, which enables in a modular manner downscaling of gridded regional emissions with specific spatial proxies based on a variety of open access, robust, sustainable and frequently updated sources. UrbEm can be applied to any urban area in Europe and provides methodological homogeneity between different cities.\n\nTo demonstrate the general applicability and performance of the developed method and tool, we introduced the method, and compared the spatial distribution of uniformly disaggregated regional emissions with emissions downscaled with the UrbEm approach for the differing cities of Athens and Hamburg (manuscript submitted for publication, pre-print accessible on request). \n\nThe UrbEm downscaling approach is completely free of cost and open source. Its application is realized in (1) a series of R scripts and (2) as Pyhton script. Both applications rely on e.g. CAMS-REG emission inventories as well as a set (maps) of spatial proxies, which need to be downloaded before using UrbEm.\n\n1. Access necessary input data\n\n1.1. Emission datasets\n\nUrbEm v1.0 is configured to read CAMS-REG-AP v3.1 regional emissions. After registration, these can be downloaded free of cost at: https://eccad.aeris-data.fr\nAfter registration in the \"Access Data\" section, the CAMS-REG-AP dataset should be selected and downloaded for all pollutants and sectors.\n\nThe second emission database applied is the European Pollutant Release and Transfer Register E-PRTR, which can be downloaded without registration at: https://www.eea.europa.eu/data-and-maps/data/member-states-reporting-art-7-under-the-european-pollutant-release-and-transfer-register-e-prtr-regulation-23/european-pollutant-release-and-transfer-register-e-prtr-data-base\n\nBoth datasets should be placed in separate folders.\n\n1.2. Spatial proxies\nBesides a collection of spatial proxies (download here: https://doi.org/10.5281/zenodo.5508739), which have been specifically prepared for application in UrbEm, the Global Human Settlement Layer (download here: https://ghsl.jrc.ec.europa.eu/ghs_pop2019.php) Population density product \"GHS_POP_E2015_GLOBE_R2019A_4326_30SS_V1_0\" needs to be downloaded.\n\nMake sure all proxies are placed in one folder.\n\n\n2. Apply UrbEm v1.0\n\nBoth solutions (R and Python) are configured to \n(1) read CAMS-REG-AP v3.1 and E-PRTR emission input files, \n(2) downscale these gridded and point emissions with the downloaded set of spatial proxies,\n(3) to arrive at annual total emissions for a selected year and a selected urban domain,\n(4) and write these as area, line and/or point source emission files,\n(5) in a *.csv file format for the EPISODE-CityChem preprocessor UECT (Karl et al. 2019).\n\nAlthough UrbEm v1.0 delivers only UECT/EPISODE-CityChem file format as output, it is generally possible to change to the desired output format by code modification. Nevertheless, we promote to use the EPISODE-CityChem air quality model for urban-scales due to its efficiency, performance and ongoing development. EPISODE-CityChem can be downloaded free of cost at https://doi.org/10.5281/zenodo.1116173.\n\n2.1. UrbEm Rscripts\n\nThe UrbEm v1.0 Rscripts are separated in three main scripts:\n1_UrbEm_pointsources_v1.R\n2_UrbEm_areasources_v2.R\n3_UrbEm_linesources_v3.\n\nThese scripts need to be run sequentially to create point, area and line emission files. \n\nBefore running the scripts, make sure the R libraries raster, sf, rgdal, osmdata and lwgeom are installed in your R environment. \n\nAddtionally the following auxiliary functions (scripts distributed with UrbEm v1.0) are necessary and will be sourced at the beginning of some main scripts:\nareasources_e-prtr_pointsource_correction.R\nareasources_to_osm_linesources.R\nproxy_distribution.R\nproxy_preparation.R\n\nWhile there are no changes in the auxiliary scripts necessary to run UrbEm, there need to be changes made in the main scripts. Each of the main scripts has an input section at the beginning, which needs to be adjusted, for e.g.:\n- setting input folders of emission files and proxies\n- setting output folders and output text strings\n- setting a domain definition\n- setting downscaling options\n\nThe input section of each main script, as well as the code itself delivers a documentation of each step made in the script.\n\nFeel free to adjust the code for your purposes.\n\n\n\n2.2. UrbEm Python scripts\n\nThe UrbEm v1.0 Python scripts are separated in two main scripts: \n- 1_UrbEM_proxies_v1.py \n- 2_UrbEM_emissions_v1.py\n\nThese scripts need to be run sequentially to 1. create proxy, point, area and line emission files.\n\nBefore running the scripts, make sure the python libraries pandas, numpy, gdal, geopandas, os, sys, fnmatch, inspect, rasterio, rasterio.mask, earthpy.spatial, shapely.geometry, earthpy, fiona, osgeo, gc, geotable, pyproj, shapely, time, shutil, OSMPythonTools.nominatim, OSMPythonTools.overpass, OSMPythonTools.data, collections and shapefile are installed in your Python v3 environment.\n\nSpatial datasets: In order to be able to run the python scripts the user should also download:\n- Population density data (Global dataset/ 2015 / WGS84 / 30 arcsec): https://ghsl.jrc.ec.europa.eu/download.php?ds=pop \n- CORINE raster and GDB files: https://land.copernicus.eu/pan-european/corine-land-cover/clc2018 \n- E-PRTR kmz data: \nhttps://www.eea.europa.eu/data-and-maps/data/member-states-reporting-art-7-under-the-european-pollutant-release-and-transfer-register-e-prtr-regulation-23/e-prtr-facilities-kmz-format/eprtr_facilities_v9.kmz \n- E-PRTR csv data: \nhttps://www.eea.europa.eu/data-and-maps/data/member-states-reporting-art-7-under-the-european-pollutant-release-and-transfer-register-e-prtr-regulation-23/european-pollutant-release-and-transfer-register-e-prtr-data-base/eprtr_v9_csv.zip \n- Urban Center data: https://ghsl.jrc.ec.europa.eu/ghs_stat_ucdb2015mt_r2019a.php \n- Eurostat countries: https://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/countries \n- Shipping Routes\n\nEach of the main scripts has an input section at the beginning, which needs to be adjusted, for e.g.:\n- setting input folders of emission files and proxies\n- setting a domain definition\n- setting downscaling options\n\nThe input section of each script, as well as the code itself delivers a documentation of each step made in the script.\n\nFeel free to adjust the code for your purposes.\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/urmoac",
            "repo_link": "https://github.com/DLR-VF/UrMoAC",
            "content": {
                "codemeta": "",
                "readme": "# UrMoAC\n\n# ![logo.png](https://raw.githubusercontent.com/DLR-VF/UrMoAC/master/logo.png) UrMoAC\n[![License: EPL2](https://img.shields.io/badge/license-EPL2-green)](https://github.com/DLR-VF/UrMoAC/blob/master/LICENSE.md)\n[![DOI](https://img.shields.io/badge/doi-10.5281%2Fzenodo.13234444-blue)](https://doi.org/10.5281/zenodo.13234444)\n[![Documentation Status](https://readthedocs.org/projects/urmoac/badge/?version=latest)](https://urmoac.readthedocs.io/en/latest/?badge=latest)\n![Build Status](https://github.com/DLR-VF/UrMoAC/actions/workflows/maven_build.yml/badge.svg)\n\n&ldquo;Urban Mobility Accessibility Computer&rdquo; or &ldquo;UrMoAC&rdquo; is a tool for computing accessibility measures, supporting aggregation, variable limits, and intermodal paths. It is a scientific tool.\n\nThis version of the documentation describes the current development version. You should use one of the available [releases](https://github.com/DLR-VF/UrMoAC/releases). The according documentation can be found at [readthedocs](http://urmoac.readthedocs.io) or within the release itself.\n\nWhat the tool basically does is to load a set of origin locations and a set of destination locations as well as a road network and optionally a description of the public transport offer. Then, it iterates over all loaded origins and computes the respective accessibility measure for each of them by routing to all destinations within the defined limit. Optionally, areas by which the origins and destinations shall be aggregated may be loaded.\n\nSome features:\n\n* input is read from databases or files;\n* variable origins / destinations;\n* variable aggregation options;\n* weights for origins and destinations;\n* flexible limits for search: max. time, max. distance, max. number, max. seen value, nearest only;\n* support for different transport modes, as well as intermodal accessibilities;\n* GTFS-based public transport accessibility computation;\n* possibility to read time-dependent travel times (for motorised individual traffic);\n* support for data preparation and visualisation.\n\n## Documentation\n\nThe complete documentation is located at <http://urmoac.readthedocs.io>. It should cover different versions.\n\nWhen using one of the releases, you should consult the included documentation as the information below describes the current state of the development.\n\nPlease consult the section *Links* below for further information sources.\n\n## Installation\n\n**UrMoAC** is written in the [Java](https://www.java.com/) programming language. You need [Java](https://www.java.com/) to run it. The easiest way to install it is to download the .jar-file from the latest [release](https://github.com/DLR-VF/UrMoAC/releases). Further possibilities to run it are given at [Installation](https://github.com/DLR-VF/UrMoAC/blob/master/docs/mkdocs/Installation.md).\n\n## Usage examples\n\nA most basic call may look as following:\n\n```console\njava -jar UrMoAC.jar --from origins.csv --to destinations.csv --net network.csv --od-output nm_output.csv --mode bike --time 0 --epsg 0\n```\n\nWhich would compute the accessibility of the destinations stored in ```destinations.csv``` starting at the origins stored in ```origins.csv``` along the road network stored in ```network.csv``` for the transport mode bike. Information about the used file formats are given at [Input Data Formats](https://github.com/DLR-VF/UrMoAC/blob/master/docs/mkdocs/InputDataFormats.md).\n\n## License\n\n**UrMoAC** is licensed under the [Eclipse Public License 2.0](LICENSE.md).\n\n**When using it, please cite it as:**\n\nDaniel Krajzewicz, Dirk Heinrichs and Rita Cyganski (2017) [_Intermodal Contour Accessibility Measures Computation Using the 'UrMo Accessibility Computer'_](https://elib.dlr.de/118235/). International Journal On Advances in Systems and Measurements, 10 (3&4), Seiten 111-123. IARIA.\n\nAnd / or use the DOI: [![DOI](https://img.shields.io/badge/doi-10.5281%2Fzenodo.13234444-blue)](https://doi.org/10.5281/zenodo.13234444) (v0.8.2)\n\n## Support and Contribution\n\n**UrMoAC** is under active development and we are happy about any interaction with users or dvelopers.\n\n## Authors\n\n**UrMoAC** has been developed at the [Institute of Transport Research](http://www.dlr.de/vf) of the [German Aerospace Center](http://www.dlr.de).\n\n## Links\n\nYou may find further information about **UrMoAC** at the following pages:\n* a complete documentation is located at <http://urmoac.readthedocs.io>;\n* the recent as well as the previous releases can be found at <https://github.com/DLR-VF/UrMoAC/releases>;\n* the source code repository is located at <https://github.com/DLR-VF/UrMoAC>;\n* the issue tracker is located at <https://github.com/DLR-VF/UrMoAC/issues>;\n* you may start a discussion or join an existing one at <https://github.com/DLR-VF/UrMoAC/discussions>.\n\n## Legal\n\nPlease find additional legal information at [Legal](https://github.com/DLR-VF/UrMoAC/blob/master/docs/mkdocs/Legal.md).\n\n",
                "dependencies": "<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>de.dlr.ivf.urmo</groupId>\n    <artifactId>UrMoAC</artifactId>\n    <version>0.8.2</version>\n    <packaging>jar</packaging>\n\t<properties>\n\t\t<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n\t\t<maven.compiler.release>17</maven.compiler.release>\n\t</properties>\n\n    <build>\n        <defaultGoal>install</defaultGoal>\n        <sourceDirectory>src</sourceDirectory>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.6.0</version>\n\t\t\t\t<configuration>\n     \t\t\t\t<source>1.17</source>\n     \t\t\t\t<target>1.17</target>\n\t\t\t\t</configuration>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-shade-plugin</artifactId>\n                <version>3.2.1</version>\n                <executions>\n                    <execution>\n                        <goals>\n                            <goal>shade</goal>\n                        </goals>\n                        <configuration>\n                            <shadedArtifactAttached>true</shadedArtifactAttached>\n                            <transformers>\n                                <transformer implementation=\n                                                     \"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\n                                    <mainClass>de.dlr.ivf.urmo.UrMoAccessibilityComputer</mainClass>\n                                </transformer>\n                                <!-- transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/ -->\n                            </transformers>\n                            <filters>\n                                <filter>\n                                    <artifact>*:*</artifact>\n                                    <excludes>\n                                        <exclude>META-INF/*.SF</exclude>\n                                        <exclude>META-INF/*.DSA</exclude>\n                                        <exclude>META-INF/*.RSA</exclude>\n                                    </excludes>\n                                </filter>\n                            </filters>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-jar-plugin</artifactId>\n                <version>3.1.0</version>\n                <configuration>\n                    <archive>\n                        <manifest>\n                            <addClasspath>true</addClasspath>\n                            <classpathPrefix>lib/</classpathPrefix>\n                            <mainClass>de.dlr.ivf.urmo.UrMoAccessibilityComputer</mainClass>\n                        </manifest>\n                    </archive>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n    <organization>\n        <name>German Aerospace Center (DLR) - Institute of Transport Research</name>\n        <url>https://www.dlr.de/vf/en</url>\n    </organization>\n    <name>UrMoAC</name>\n    <url>https://github.com/DLR-VF/UrMoAC</url>\n    <description>A tool for computing accessibility measures, supporting aggregation, variable limits, and\n        intermodality.\n    </description>\n    <repositories>\n        <repository>\n            <id>osgeo-alt</id>\n            <url>https://repo.osgeo.org/repository/release/</url>\n        </repository>\n        <repository>\n            <id>geotoolkit</id>\n            <url>https://maven.geotoolkit.org</url>\n        </repository>\n        <repository>\n            <id>Atlassian 3rdParty Repository</id>\n            <name>Atlassian 3rdParty Repository</name>\n            <url>https://packages.atlassian.com/maven-3rdparty/</url>\n        </repository>\n        <repository>\n            <id>Clojars</id>\n            <name>Clojars Repository</name>\n            <url>http://clojars.org/repo/</url>\n        </repository>\n    </repositories>\n\n    <dependencies>\n        <dependency>\n            <groupId>com.opencsv</groupId>\n            <artifactId>opencsv</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>javax.measure</groupId>\n            <artifactId>jsr-275</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>javax.media</groupId>\n            <artifactId>jai_core</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>javax.vecmath</groupId>\n            <artifactId>vecmath</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.jdom</groupId>\n            <artifactId>jdom2</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>jgridshift</groupId>\n            <artifactId>jgridshift</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.logging.log4j</groupId>\n            <artifactId>log4j-core</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.geotools</groupId>\n            <artifactId>gt-api</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.geotools</groupId>\n            <artifactId>gt-data</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.geotools</groupId>\n            <artifactId>gt-epsg-extension</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.geotools</groupId>\n            <artifactId>gt-epsg-hsql</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.geotools</groupId>\n            <artifactId>gt-geopkg</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.geotools</groupId>\n            <artifactId>gt-main</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.geotools</groupId>\n            <artifactId>gt-metadata</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.geotools</groupId>\n            <artifactId>gt-opengis</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.geotools</groupId>\n            <artifactId>gt-referencing</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.postgresql</groupId>\n            <artifactId>postgresql</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.postgis</groupId>\n            <artifactId>postgis-stubs</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.postgis</groupId>\n            <artifactId>postgis-jdbc</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-api</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-simple</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.xerial</groupId>\n            <artifactId>sqlite-jdbc</artifactId>\n        </dependency>\n    </dependencies>\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>com.opencsv</groupId>\n                <artifactId>opencsv</artifactId>\n                <version>5.8</version>\n            </dependency>\n            <dependency>\n                <groupId>commons-pool</groupId>\n                <artifactId>commons-pool</artifactId>\n                <version>1.5.4</version>\n            </dependency>\n            <dependency>\n                <groupId>javax.media</groupId>\n                <artifactId>jai_core</artifactId>\n                <version>1.1.3</version>\n            </dependency>\n            <dependency>\n                <groupId>javax.media</groupId>\n                <artifactId>javax.media</artifactId>\n                <version>1.1.3</version>\n            </dependency>\n            <dependency>\n                <groupId>javax.vecmath</groupId>\n                <artifactId>vecmath</artifactId>\n                <version>1.5.2</version>\n            </dependency>\n            <dependency>\n                <groupId>org.jdom</groupId>\n                <artifactId>jdom2</artifactId>\n                <version>2.0.6.1</version>\n            </dependency>\n            <dependency>\n                <groupId>jgridshift</groupId>\n                <artifactId>jgridshift</artifactId>\n                <version>1.0</version>\n            </dependency>\n            <dependency>\n                <groupId>org.apache.logging.log4j</groupId>\n                <artifactId>log4j-core</artifactId>\n                <version>2.17.1</version>\n            </dependency>\n            <dependency>\n                <groupId>javax.measure</groupId>\n                <artifactId>jsr-275</artifactId>\n                <version>1.0.0</version>\n            </dependency>\n            <dependency>\n                <groupId>org.geotools</groupId>\n                <artifactId>gt-api</artifactId>\n                <version>20.5</version>\n            </dependency>\n            <dependency>\n                <groupId>org.geotools</groupId>\n                <artifactId>gt-data</artifactId>\n                <version>20.5</version>\n            </dependency>\n            <dependency>\n                <groupId>org.geotools</groupId>\n                <artifactId>gt-epsg-hsql</artifactId>\n                <version>25.2</version>\n            </dependency>\n            <dependency>\n                <groupId>org.geotools</groupId>\n                <artifactId>gt-epsg-extension</artifactId>\n                <version>25.2</version>\n            </dependency>\n            <dependency>\n                <groupId>org.geotools</groupId>\n                <artifactId>gt-geopkg</artifactId>\n                <version>25.2</version>\n            </dependency>\n            <dependency>\n                <groupId>org.geotools</groupId>\n                <artifactId>gt-main</artifactId>\n                <version>25.2</version>\n            </dependency>\n            <dependency>\n                <groupId>org.geotools</groupId>\n                <artifactId>gt-metadata</artifactId>\n                <version>25.2</version>\n            </dependency>\n            <dependency>\n                <groupId>org.geotools</groupId>\n                <artifactId>gt-opengis</artifactId>\n                <version>25.2</version>\n            </dependency>\n            <dependency>\n                <groupId>org.geotools</groupId>\n                <artifactId>gt-referencing</artifactId>\n                <version>25.2</version>\n            </dependency>\n            <dependency>\n                <groupId>org.locationtech.jts</groupId>\n                <artifactId>jts</artifactId>\n                <version>1.18.2</version>\n            </dependency>\n            <dependency>\n                <groupId>org.slf4j</groupId>\n                <artifactId>slf4j-api</artifactId>\n                <version>1.6.3</version>\n            </dependency>\n            <dependency>\n                <groupId>org.slf4j</groupId>\n                <artifactId>slf4j-simple</artifactId>\n                <version>1.6.3</version>\n            </dependency>\n            <dependency>\n                <groupId>org.postgresql</groupId>\n                <artifactId>postgresql</artifactId>\n                <version>42.4.4</version>\n            </dependency>\n            <dependency>\n                <groupId>org.postgis</groupId>\n                <artifactId>postgis-stubs</artifactId>\n                <version>1.3.3</version>\n            </dependency>\n            <dependency>\n                <groupId>org.postgis</groupId>\n                <artifactId>postgis-jdbc</artifactId>\n                <version>1.3.3</version>\n            </dependency>\n            <dependency>\n                <groupId>org.postgis</groupId>\n                <artifactId>postgis</artifactId>\n                <version>1.3.3</version>\n            </dependency>\n            <dependency>\n                <groupId>org.postgis</groupId>\n                <artifactId>postgis-main</artifactId>\n                <version>1.3.3</version>\n            </dependency>\n            <dependency>\n                <groupId>org.xerial</groupId>\n                <artifactId>sqlite-jdbc</artifactId>\n                <version>3.41.2.2</version>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n</project>\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/utile-oxy",
            "repo_link": "https://github.com/andyco98/UTILE-Oxy/",
            "content": {
                "codemeta": "",
                "readme": "# *UTILE-Oxy* - Deep Learning to Automate Video Analysis of Bubble Dynamics in Proton Exchange Membrane Electrolyzers\n\n![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/workflow.png)\n\n\nWe present  an automated workflow using deep learning for the analysis of videos containing oxygen bubbles in PEM electrolyzers by 1. preparing an annotated dataset and training models in order to conduct semantic segmentation of bubbles and 2. automating the extraction of bubble properties for further distribution analysis.\n\nThe publication [UTILE-Oxy - Deep Learning to Automate Video Analysis of Bubble Dynamics in Proton Exchange Membrane Electrolyzers](https://pubs.rsc.org/en/content/articlelanding/2024/cp/d3cp05869g) is available in an open access fashion on the journal PCCP for further information!\n\n\n## Description\nThis project focuses on the deep learning-based automatic analysis of polymer electrolyte membrane water electrolyzers (PEMWE) oxygen evolution videos. \nThis repository contains the Python implementation of the UTILE-Oxy software for automatic video analysis, feature extraction, and plotting.\n\nThe models we present in this work are trained on a specific use-case scenario of interest in oxygen bubble evolution videos of transparent cells. It is possible to fine-tune, re-train or employ another model suitable for your individual case if your data has a strong visual deviation from the presented data here, which was recorded and shown as follows:\n\n![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/figexperiment.png)\n\n## Model's benchmark\nIn our study, we trained several models to compare their prediction performance on unseen data. We trained specifically three different models on the same dataset composed by :\n- Standard U-Net 2D\n- U-Net 2D with a ResNeXt 101 backbone \n- Attention U-Net\n\nWe obtained the following performance results:\n\n| Model                           | Precision [%] | Recall [%] | F1-Score [%] |\n|---------------------------------|----------------|------------|--------------|\n| U-Net 2D                        | 81             | 89         | 85           |\n| U-Net with ResNeXt101 backbone  | 95             | 78         | 86           |\n| Attention U-Net                 | 95             | 75         | 84           |\n\n\nSince the F1-Scores are similar a visual inspection was carried out to find the best-performing model :\n\n![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/benchmark.png)\n\nBut even clearer is the visual comparison of the running videos:\n\n![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/video_results.gif)\n\n## Extracted features\n\n### Time-resolved bubble ratio computation and bubble coverage distribution\n\n![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/timeresolved.png)\n\n### Bubble position probability density map\n\n![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/heatmaps.png)\n\n### Individual bubble shape analysis\n\n![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/individualcorrect.png)\n\n## Installation\nIn order to run the actual version of the code, the following steps need to be done:\n- Clone the repository\n- Create a new environment using Anaconda using Python 3.8 or superior\n- Pip install the jupyter notebook library\n\n    ```\n    pip install notebook\n    ```\n- From your Anaconda console open jupyter notebook (just tip \"jupyter notebook\" and a window will pop up)\n- Open the /UTILE-Oxy/UTILE-Oxy_prediction.ipynb file from the jupyter notebook directory\n- Further instructions on how to use the tool are attached to the code with examples in the juypter notebook\n\n## Dependencies\nThe following libraries are needed to run the program:\n\n  ```\n   pip install opencv-python, numpy, patchify, pillow, segmentation_models, keras, tensorflow, matplotlib, scikit-learn, pandas\n\n   ```\n### Notes\n\nThe datasets used for training and the trained model are available at Zenodo: https://doi.org/10.5281/zenodo.10184579.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/varfish",
            "repo_link": "https://github.com/bihealth/varfish-server",
            "content": {
                "codemeta": "",
                "readme": "[![Documentation Status](https://readthedocs.org/projects/varfish-server/badge/?version=latest)](https://varfish-server.readthedocs.io/en/latest/?badge=latest)\n[![Code Coverage](https://codecov.io/gh/varfish-org/varfish-server/branch/main/graph/badge.svg?token=5ZACSH5MZZ)](https://codecov.io/gh/varfish-org/varfish-server)\n[![image](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n\n# VarFish\n\n**Comprehensive DNA variant analysis for diagnostics and research.**\n\nThis is the repository for the web server component.\n\nHoltgrewe, M.; Stolpe, O.; Nieminen, M.; Mundlos, S.; Knaus, A.; Kornak, U.; Seelow, D.; Segebrecht, L.; Spielmann, M.; Fischer-Zirnsak, B.; Boschann, F.; Scholl, U.; Ehmke, N.; Beule, D. *VarFish: Comprehensive DNA Variant Analysis for Diagnostics and Research*. Nucleic Acids Research 2020, gkaa241. <https://doi.org/10.1093/nar/gkaa241>.\n\n## Getting Started\n\n- [VarFish Homepage](https://www.cubi.bihealth.org/software/varfish/)\n- [Manual](https://varfish-server.readthedocs.io/en/latest/)\n  - [Installation Instructions](https://varfish-server.readthedocs.io/en/latest/admin_install.html).\n- [Docker Compose Installer](https://github.com/varfish-org/varfish-docker-compose#run-varfish-server-using-docker-compose).\n\n## VarFish Repositories\n\n- [varfish-server](https://github.com/varfish-org/varfish-server) The VarFish Server is the web frontend used by the end users / data analysts.\n- [varfish-annotator](https://github.com/varfish-org/varfish-annotator) The VarFish Annotator is a command line utility used for annotating VCF files and converting them to files that can be imported into VarFish Server.\n- [varfish-cli](https://github.com/varfish-org/varfish-cli) The VarFish Command Line Interface allows to import data through the VarFish REST API.\n- [varfish-db-downloader](https://github.com/varfish-org/varfish-db-downloader) The VarFish DB Downloader is a command line tool for downloading the background database.\n- [varfish-docker-compose](https://github.com/varfish-org/varfish-docker-compose) Quickly get started running a VarFish server by using Docker Compose. We provide a prebuilt data set with some already imported data.\n\n## At a Glance\n\n- License: MIT\n- Dependencies / Tech Stack\n  - Python \\>=3.8\n  - Django 3\n  - PostgreSQL \\>=12\n\nGitHub is used for public issue tracking. Currently, development happens\non internal infrastructure.\n\n## VarFish Component Compatibility Table\n\nThe following combinations have been validated / are supported to work.\n\n| VarFish Server | VarFish CLI | VarFish Annotator |\n| -------------- | ----------- | ----------------- |\n| v1.2.2         | v0.3.0      | v0.21             |\n| v1.2.1         | v0.3.0      | v0.21             |\n| v1.2.0         | v0.3.0      | v0.21             |\n\n## VarFish Data Release Compatibility Table\n\nThe following combinations have been validated / are supported to work.\n\n| VarFish Server | Data Release | VarFish DB Downloader |\n| -------------- | ------------ | --------------------- |\n| v1.2.2         | 20210728c    | v0.3.\\*               |\n| v1.2.1         | 20210728     | v0.3.\\*               |\n| v1.2.1         | 20210728b    | v0.3.\\*               |\n| v1.2.0         | 20210728     | v0.3.\\*               |\n| v1.2.0         | 20210728b    | v0.3.\\*               |\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/vasca",
            "repo_link": "https://github.com/rbuehler/vasca",
            "content": {
                "codemeta": "",
                "readme": "\n![VASCA icon](docs/images/VASCA_icon.png)\n[![🧪 pytest](https://github.com/rbuehler/vasca/actions/workflows/ci.yml/badge.svg)](https://github.com/rbuehler/vasca/actions/workflows/ci.yml)\n[![📚 docs](https://github.com/rbuehler/vasca/actions/workflows/docs.yml/badge.svg)](https://rbuehler.github.io/vasca/)\n[![🚀 pypi](https://github.com/rbuehler/vasca/actions/workflows/pypi.yml/badge.svg)](https://pypi.org/project/vasca/)\n\n\n# Variable Source Cluster Analysis (VASCA)\n\n1. [Motivation](#motivation)\n2. [Pipeline Overview](#pipeline-overview)\n3. [Key Features](#key-features)\n4. [Proof-of-Principle Study](#proof-of-principle-study)\n5. [Documentation and Installation](#documentation-and-installation)\n6. [Getting Started](docs/getting_started.md#getting-started)\n\n## Motivation\nVASCA (Italian for \"bathtub\" 🛁) is a high-performance software package developed to\naddress the challenges of time-domain astronomy, especially given the increasing volume\nof data from large-scale surveys such as [ZTF](https://en.wikipedia.org/wiki/Zwicky_Transient_Facility),\n[LSST](https://en.wikipedia.org/wiki/Vera_C._Rubin_Observatory), and [ULTRASAT](https://www.weizmann.ac.il/ultrasat/).\nDesigned to analyze time-variable cosmic sources like active galactic nuclei, stars, and\ntransient events, VASCA provides a modular, scalable solution for integrating data from\nmultiple instruments and conducting a cohesive analysis.\n\n## Pipeline Overview\n\nThe VASCA analysis pipeline consists of three primary steps:\n1. **Spatial Clustering**: Associate detections from repeated observations to unique\ncosmic sources using [mean-shift](https://en.wikipedia.org/wiki/Mean_shift) clustering.\n2. **Statistical Variability Detection**: Identify time-variable sources by testing flux\nvariations against a constant hypothesis at a 5-σ significance level.\n3. **Source Classification**: Classify detected sources, including cross-matching with\nexternal catalogs (e.g., SIMBAD, Gaia).\n\nThe main output of the pipeline is a catalog of time-variable cosmic\nsources, including detailed classifications and cross-matches with existing astronomical\ndatabases.\n\n## Key Features\n\n- **Simplicity and Modularity**: The software uses a hierarchical data model and modular\nprocessing to ensure scalability and ease of use. It supports data from multiple\ninstruments seamlessly.\n- **Proven Algorithms**: VASCA relies on established algorithms and statistical methods,\nensuring robustness and reducing the maintenance burden.\n- **Focus on Specific Use Case**: Optimized for analyzing time-domain astronomical data,\nVASCA keeps complexity low, simplifying auditing and debugging.\n- **Standards Compliance**: Outputs are designed for publication readiness by adhering to\nIAU and CDS standards, using widely-accepted, non-proprietary data formats. \n- **Customization and Extensibility**: VASCA allows flexible configuration, making it\nadaptable to different datasets and instrument-specific requirements.\n\n## Proof-of-Principle Study\n\nVASCA was applied to a proof-of-principle study  using the Galaxy Evolution Explorer\n(GALEX) archive (2003-2013). This study produced a catalog of over 4,000 UV-variable\nsources, revealing UV variability across all classes of stars. Notably, a massive,\npulsating white dwarf exhibited unique long-term variability in the UV. The full article\nincluding a description of VASCA's pipeline can be found here:\n[The time-variable ultraviolet sky: Active galactic nuclei, stars, and white dwarfs](https://ui.adsabs.harvard.edu/abs/2024A%26A...687A.313B/abstract).\n\n## Documentation and Installation\n\nVASCA is distributed as an open-source package. Comprehensive documentation is available\n[here](https://rbuehler.github.io/vasca/), including example notebooks and an API reference to help users get started.\nFor quick installation, VASCA can be installed via [PyPI](https://pypi.org/project/vasca/) using:\n```shell\npip install vasca\n```\nFor more info see the [installation guide](docs/getting_started.md#installation).\n",
                "dependencies": "[build-system]\nrequires = [\"setuptools\", \"setuptools-scm\", \"wheel\", \"build\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"vasca\"\ndynamic = [\"version\"]\ndescription = \"Ultraviolet Variability Analysis is an astronomy pipeline for time-variable sources.\"\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\nlicense = {file = \"LICENSE\"}\nkeywords = [\"astronomy\", \"ultraviolet\", \"pipeline\"]\nauthors = [\n    {name = \"Julian Schliwinski\", email = \"julian.schliwinski@desy.de\"},\n    {name = \"Rolf Bühler\", email = \"rolf.buehler@desy.de\"},\n]\ndependencies = [\n  \"astropy ~= 5.3\",\n  \"astroquery ~= 0.4.6\",\n  \"coverage ~= 7.2.7\",\n  \"healpy ~= 1.17\",\n  \"loguru ~= 0.7.0\",\n  \"matplotlib ~= 3.7.1\",\n  \"numpy ~= 1.24.3\",\n  \"pandas ~= 2.0.2\",\n  \"pytest ~= 7.3.1\",\n  \"pytest-cov ~= 4.1.0\",\n  \"python-dotenv ~= 1.0.0\",\n  \"PyYAML ~= 6.0\",\n  \"scikit-learn ~= 1.3.0\",\n  \"setuptools ~= 67.8.0\",\n  \"setuptools_scm\",\n  \"regions ~= 0.7\",\n  \"pyyaml-include ~= 1.3\",\n  \"ipywidgets ~= 8.0.6\",\n  \"ipympl ~= 0.9.3\",\n  \"jupyterlab ~= 4.0.2\",\n  \"sphinx ~= 7.2.6\",\n  \"myst-nb\",\n  \"furo\",\n  \"sphinx_autodoc2\",\n  \"sphinx_copybutton\",\n  \"sphinx-tippy\",\n  \"jupytext\",\n  \"itables\",\n  \n]\n\n[project.urls]\nhomepage = \"https://schliwiju.github.io/vasca-mirror/\"\ndocumentation = \"https://schliwiju.github.io/vasca-mirror/\"\nrepository = \"https://github.com/rbuehler/vasca\"\nchangelog = \"https://tbd.desy.de\"\n\n[project.scripts]\nvasca_pipe = \"vasca.vasca_pipe:run_from_file\"\n\n[tool.setuptools]\n\n[tool.setuptools_scm]\nwrite_to = \"vasca/_version.py\"\n\n[tool.pytest.ini_options]\n# Pytest settings (https://docs.pytest.org/en/6.2.x/reference.html#configuration-options)\nminversion = \"6.0\"\naddopts = \"-v\"\ntestpaths = [\"vasca/test\"]\nlog_cli = true\n\n[tool.isort]\n# Black-Isort compatibility (https://black.readthedocs.io/en/stable/guides/using_black_with_other_tools.html#isort)\n# could be replaced by 'profile = \"black\"' for isort versions >=5.0.0\nmulti_line_output = 3\ninclude_trailing_comma = true\nforce_grid_wrap = 0\nuse_parentheses = true\nensure_newline_before_comments = true\nline_length = 88\n\n[tool.ruff]\n# Exclude a variety of commonly ignored directories.\nexclude = [\n    \".bzr\",\n    \".direnv\",\n    \".eggs\",\n    \".git\",\n    \".git-rewrite\",\n    \".hg\",\n    \".mypy_cache\",\n    \".nox\",\n    \".pants.d\",\n    \".pytype\",\n    \".ruff_cache\",\n    \".svn\",\n    \".tox\",\n    \".venv\",\n    \"__pypackages__\",\n    \"_build\",\n    \"buck-out\",\n    \"build\",\n    \"dist\",\n    \"node_modules\",\n    \"venv\",\n]\n\n# Same as Black.\nline-length = 88\nindent-width = 4\n\n# Assume Python 3.11\ntarget-version = \"py311\"\n\n[tool.ruff.lint]\n# Ruff rules docs: https://docs.astral.sh/ruff/rules/\n# Selection below from: https://codebase.helmholtz.cloud/hifis/cloud/access-layer/portal/-/blob/4f9600fb942da5eefc2c0d88f70b120ef2b72206/pyproject.toml#L67\nselect = [\n  \"F\",\n  \"E\",\n  \"W\",\n  \"I\",\n  \"N\",\n  \"UP\",\n  \"ANN\",\n  \"BLE\",\n  \"FBT\",\n  \"B\",\n  \"A\",\n  \"C4\",\n  \"DTZ\",\n  \"T10\",\n  \"T20\",\n  \"DJ\",\n  \"EM\",\n  \"EXE\",\n  \"ISC\",\n  \"ICN\",\n  \"G\",\n  \"INP\",\n  \"PIE\",\n  \"PYI\",\n  \"PT\",\n  \"Q\",\n  \"RSE\",\n  \"RET\",\n  \"SLF\",\n  \"SIM\",\n  \"TID\",\n  \"TCH\",\n  \"INT\",\n  \"ARG\",\n  \"PTH\",\n  \"ERA\",\n  \"PD\",\n  \"PGH\",\n  \"PL\",\n  \"TRY\",\n  \"NPY\",\n  \"NPY201\",\n  \"RUF\"\n  ]\n\nignore = [\"ANN101\"]\n\n# Allow fix for all enabled rules (when `--fix`) is provided.\nfixable = [\"ALL\"]\nunfixable = []\n\n# Allow unused variables when underscore-prefixed.\ndummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"numpy\"\n\n[tool.ruff.format]\n# Like Black, use double quotes for strings.\nquote-style = \"double\"\n\n# Like Black, indent with spaces, rather than tabs.\nindent-style = \"space\"\n\n# Like Black, respect magic trailing commas.\nskip-magic-trailing-comma = false\n\n# Like Black, automatically detect the appropriate line ending.\nline-ending = \"auto\"\n\n[tool.jupytext]\nformats = \"ipynb,py:percent\"\nhide_notebook_metadata=true \n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/velocityconversion",
            "repo_link": "https://github.com/cmeessen/VelocityConversion",
            "content": {
                "codemeta": "",
                "readme": "# VelocityConversion\n\n[![DOI](https://zenodo.org/badge/87794116.svg)](https://zenodo.org/badge/latestdoi/87794116) [![PyPI version](https://badge.fury.io/py/velocityconversion.svg)](https://badge.fury.io/py/velocityconversion)\n\n- [VelocityConversion](#velocityconversion)\n  - [Introduction](#introduction)\n  - [Getting started](#getting-started)\n    - [Use the latest version not on PyPI](#use-the-latest-version-not-on-pypi)\n  - [Usage as command line tool](#usage-as-command-line-tool)\n  - [Usage as a Python module](#usage-as-a-python-module)\n  - [Modifying physical properties of the minerals](#modifying-physical-properties-of-the-minerals)\n  - [Contributing](#contributing)\n  - [Citing](#citing)\n  - [References](#references)\n  - [Licence](#licence)\n\n## Introduction\n\nThis code is a python implementation of the p- and s-wave velocity to density\nconversion approach after Goes et al. (2000). The implementation was optimised\nfor regular 3D grids using lookup tables instead of Newton iterations.\n\nGoes et al. (2000) regard the expansion coefficient as temperature dependent\nusing the relation by Saxena and Shen (1992). In `VelocityConversion`, the user\ncan additionally choose between a constant expansion coefficient or a pressure-\nand temperature dependent coefficient that was derived from Hacker and Abers\n(2004).\n\nFor detailed information on the physics behind the approach have a look at the\noriginal paper by Goes et al. (2000).\n\n## Getting started\n\n`VelocityConversion` requires Python 3 and numpy. Install `numpy` and\n`VelocityConversion` by running\n\n```bash\npip install numpy velocityconversion\n```\n\nTo uninstall `VelocityConversion`, run\n\n```bash\npip uninstall velocityconversion\n```\n\n### Use the latest version not on PyPI\n\nIf you want to use the very latest version, or want to\n[contribute](#contributing), clone the repository to you local hard drive:\n\n```bash\ngit clone https://github.com/cmeessen/VelocityConversion.git\n```\n\nor, if you haven an [SSH key](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent)\nassociated to your account:\n\n```bash\ngit clone git@github.com:cmeessen/VelocityConversion.git\n```\n\nTo check whether everything is working run the tests\n\n```bash\npython test.py\n```\n\nIf the output looks like this, everything is working fine:\n\n```\ntest_vp_AlphaConst (__main__.TestVelocityConversion) ... ok\ntest_vs_AlphaConst (__main__.TestVelocityConversion) ... ok\ntest_vs_AlphaPT (__main__.TestVelocityConversion) ... ok\ntest_vs_AlphaT (__main__.TestVelocityConversion) ... ok\n\n----------------------------------------------------------------------\nRan 4 tests in 1.633s\n\nOK\n```\n\n## Usage as command line tool\n\nIn order to use the code as command line tool, add the `./Examples` directory\nto your `PATH`, e.g. in your bash profile:\n\n```bash\nexport PATH=/path/to/VelocityConversion/Examples:$PATH\n```\n\nAlternatively you can move the bash script\n[VelocityConversion](./Examples/VelocityConversion) to a place that is within\nyour `PATH`. Now the bash script `VelocityConversion` can be executed:\n\n```\nVelocityConversion\n\nUsage: VelocityConversion FileIn -type <P|S> [optional args]\n    Optional arguments:\n        -AlphaT\n        -AlphaPT\n        -dT <val>\n        -comp <Filename>\n        -h | --help\n        -NN\n        -out <FileOut>\n        -scaleV <value>\n        -setQ <1|2>\n        -v | -verbose\n        -XFe <val>\n        --version\n```\n\nThe steps to prepare a conversion are\n\n- definition of mantle rock composition in a `*.csv` file using the mineral\n  terminology of [MinDB.csv](./VelocityConversion/MinDB.csv)\n- provide a velocity distribution on a regular 3D grid where columns are `x y z\n  v`\n- run `VelocityConversion` specifying the velocity type with `-type P` or\n  `-type S`\n\nWorking examples for the usage as command line tool are provided in the script\n[RunExamples.sh](./Examples/RunExamples.sh).\n\n## Usage as a Python module\n\nVelocityConversion can also be imported as a Python module. Therefore, navigate\nto the folder that contains your clone of the repository (and\n[setup.py](./setup.py)) and execute\n\n```bash\npip install -e .\n```\n\nNow, the module can be imported to Python:\n\n```python\nfrom VelocityConversion import MantleConversion\nMC = MantleConversion()\n```\n\nA short working example for a conversion is:\n\n```python\nfrom VelocityConversion import MantleConversion\nMC = MantleConversion()\nMC.LoadFile(\"./Examples/VsSL2013.dat\")\nMC.SetVelType(\"S\")\nMC.DefaultMineralogy()\nMC.FillTables()\nMC.CalcPT()\nMC.SaveFile(\"./Examples/VsSL2013_out.dat\")\n```\n\nFor a more complete documentation on how to use `VelocityConversion` as a Python\nmodule please visit the\n[documentation](https://cmeessen.github.io/VelocityConversion/).\n\n## Modifying physical properties of the minerals\n\nThe database that contains the physical properties of the individual mineral\nphases is stored in [MinDB.csv](./VelocityConversion/MinDB.csv).\nMineral parameters can be edited, or new minerals added. A new mineral phase\nshould then be referred to in the code or the assemblage file using the name\nthat was assigned in the `phase` column of `MinDB.csv`.\n\n## Contributing\n\nPlease see [CONTRIBUTING.md](./CONTRIBUTING.md) if you want to contribute to\n`VelocityConversion`.\n\n## Citing\n\nIf you use this code, please consider citing it as\n\n> Meeßen, Christian (2019): \"VelocityConversion (v1.1.2)\". Zenodo,\n> http://doi.org/10.5281/zenodo.5897455.\n\nor refer to [CITATION.cff](./CITATION.cff).\n\n## References\n\nBerckhemer, H., W. Kampfmann, E. Aulbach, and H. Schmeling. “Shear Modulus and\nQ of Forsterite and Dunite near Partial Melting from Forced-Oscillation\nExperiments.” Physics of the Earth and Planetary Interiors, Special Issue\nProperties of Materials at High Pressures and High Temperatures, 29, no. 1\n(July 1, 1982): 30–41. doi:10.1016/0031-9201(82)90135-2.\n\nGoes, S., R. Govers, and P. Vacher. “Shallow Mantle Temperatures under Europe\nfrom P and S Wave Tomography.” Journal of Geophysical Research 105, no. 11\n(2000): 153–11. doi:10.1029/1999jb900300.\n\nHacker, Bradley R., and Geoffrey A. Abers. “Subduction Factory 3: An Excel\nWorksheet and Macro for Calculating the Densities, Seismic Wave Speeds, and H2O\nContents of Minerals and Rocks at Pressure and Temperature.” Geochemistry,\nGeophysics, Geosystems 5, no. 1 (January 1, 2004): Q01005.\ndoi:10.1029/2003GC000614.\n\nKennett, B. L. N., E. R. Engdahl, and R. Buland. “Constraints on Seismic\nVelocities in the Earth from Traveltimes.” Geophysical Journal International\n122, no. 1 (July 1, 1995): 108–24. doi:10.1111/j.1365-246X.1995.tb03540.x.\n\nSaxena, Surendra K., and Guoyin Shen. “Assessed Data on Heat Capacity, Thermal\nExpansion, and Compressibility for Some Oxides and Silicates.” Journal of\nGeophysical Research: Solid Earth 97, no. B13 (Dezember 1992): 19813–25.\ndoi:10.1029/92JB01555.\n\nSchaeffer, A. J., and S. Lebedev. “Global Shear Speed Structure of the Upper\nMantle and Transition Zone.” Geophysical Journal International 194, no. 1 (July\n1, 2013): 417–49. doi:10.1093/gji/ggt095.\n\nSobolev, Stephan V., Hermann Zeyen, Gerald Stoll, Friederike Werling, Rainer\nAltherr, and Karl Fuchs. “Upper Mantle Temperatures from Teleseismic Tomography\nof French Massif Central Including Effects of Composition, Mineral Reactions,\nAnharmonicity, Anelasticity and Partial Melt.” Earth and Planetary Science\nLetters 139, no. 1–2 (März 1996): 147–63. doi:10.1016/0012-821X(95)00238-8.\n\n## Licence\n\nLicence: GNU General Public Licence, Version 3, 29 June 2007\n\nCopyright (2017): Christian Meeßen, Potsdam, Germany\n\nVelocityConversion is free software: you can redistribute it and/or modify it\nunder the terms of the GNU General Public License as published by the Free\nSoftware Foundation, either version 3 of the License, or (at your option) any\nlater version. VelocityConversion is distributed in the hope that it will be\nuseful, but WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public\nLicense for more details. You should have received a cop y of the GNU General\nPublic License along with this program. If not, see\nhttp://www.gnu.org/licenses/.\n\n",
                "dependencies": "[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\nnumpy = \"*\"\n\n[dev-packages]\nflake8 = \"*\"\npydocstyle = \"*\"\nipython = \"*\"\npep8 = \"*\"\nsphinx = \"*\"\nsphinx-rtd-theme = \"*\"\ndoc8 = \"*\"\ncoverage = \"*\"\nmatplotlib = \"*\"\nm2r2 = \"*\"\npycodestyle = \"*\"\n\n[requires]\npython_version = \"3.9\"\n\nfrom setuptools import setup\nfrom setuptools import find_packages\nfrom pkg_resources import resource_filename\nfrom VelocityConversion import __version__ as VERSION\nimport versioneer\n\nVERSION = versioneer.get_version()\n\n# METADATA\nNAME = 'velocityconversion'\nMODULE = 'VelocityConversion'\nAUTHOR = 'Christian Meeßen'\nAUTHOR_EMAIL = 'christian.meessen@gfz-potsdam.de'\nURL = 'https://github.com/cmeessen/VelocityConversion'\nDESCRIPTION = 'Conversion of seismic velocities to temperature and density'\ntry:\n    with open(resource_filename(MODULE, '../README.md'), 'r') as fh:\n        LONG_DESCRIPTION = fh.read()\nexcept ImportError:\n    with open('README.md') as fh:\n        LONG_DESCRIPTION = fh.read()\nLONG_DESCRIPTION_TYPE = 'text/markdown'\n\nPACKAGES = [MODULE]\nPACKAGE_DIR = {MODULE: MODULE}\nPACKAGE_DATA = {MODULE: ['*.csv']}\n\nCLASSIFIERS = [\n    'Natural Language :: English',\n    'Programming Language :: Python :: 3',\n    'Programming Language :: Python :: 3.7',\n    'Programming Language :: Python :: 3.8',\n    'Programming Language :: Python :: 3.9',\n    'License :: OSI Approved :: GNU General Public License v3 (GPLv3)',\n    'Operating System :: OS Independent',\n    'Topic :: Scientific/Engineering :: Physics',\n]\n\n# DEPENDENCIES\nINSTALL_REQUIRES = [\n    'numpy',\n]\n\nif __name__ == '__main__':\n    setup(\n        name=NAME,\n        version=VERSION,\n        author=AUTHOR,\n        author_email=AUTHOR_EMAIL,\n        description=DESCRIPTION,\n        long_description=LONG_DESCRIPTION,\n        long_description_content_type=LONG_DESCRIPTION_TYPE,\n        url=URL,\n        packages=PACKAGES,\n        package_dir=PACKAGE_DIR,\n        package_data=PACKAGE_DATA,\n        use_package_data=True,\n        classifiers=CLASSIFIERS,\n        install_requires=INSTALL_REQUIRES,\n    )\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/vencopy",
            "repo_link": "https://gitlab.com/dlr-ve/esy/vencopy/vencopy",
            "content": {
                "codemeta": "",
                "readme": "# Welcome to venco.py!\n\n- Authors: Niklas Wulff, Fabia Miorelli\n- Contact: vencopy@dlr.de\n\n# Contents\n\n- [Description](#description)\n- [Installation](#installation)\n- [Codestyle](#codestyle)\n- [Documentation](#documentation)\n- [Useful Links](#useful-links)\n- [Want to contribute?](#want-to-contribute)\n\n## Description\n\nA data processing tool estimating hourly electric demand and flexibility profiles for future \nelectric vehicle fleets. Profiles are targeted to be scalable for the use in large-scale\nenergy system models. \n\n## Installation\n\nDepending on if you want to use venco.py or if you want to contribute, there are\ntwo different installation procedures described in venco.py's documentation:\n\n[I want to apply the tool](https://dlr-ve.gitlab.io/esy/vencopy/vencopy/gettingstarted/installation.html#installation-for-users)\n\n[I want to contribute to the codebase, the documentation or the tutorials](https://dlr-ve.gitlab.io/esy/vencopy/vencopy/gettingstarted/installation.html#installation-for-developers)\n\nIn order to start using venco.py, check out our [tutorials](https://dlr-ve.gitlab.io/esy/vencopy/vencopy/gettingstarted/start.html). For this you won't need any additional data.\n\nTo run venco.py in full mode, you will need the data set Mobilität in Deutschland (German for \"mobility in Germany\"). You\ncan request it here from the clearingboard transport: https://daten.clearingstelle-verkehr.de/order-form.html \nAlternatively you can use venco.py with any National Travel Survey or mobility pattern dataset.\n\n\n## Codestyle\n\nWe use PEP-8, with the exception of UpperCamelCase for class names.\n\n## Documentation\n\nThe documentation can be found here: https://dlr-ve.gitlab.io/esy/vencopy/vencopy/\nTo be able to build the documentation locally on your machine you should additionally install the following three packages in your vencopy environment : sphinx, sphinx_rtd_theme and rst2pdf.\nAfter that you can build the documentation locally from a conda bash with the following command:\n\n```python\nsphinx-build -b html ./docs/ ./build/\n```\n\n## Useful Links\n\n- Documentation: https://dlr-ve.gitlab.io/esy/vencopy/vencopy/\n- Source code: https://gitlab.com/dlr-ve/esy/vencopy/vencopy\n- PyPI release: https://pypi.org/project/vencopy/\n- Licence: https://opensource.org/licenses/BSD-3-Clause\n\n## Want to contribute?\n\nPlease read our contribute section in the documentation and reach out to Fabia\n(fabia.miorelli@dlr.de). If you experience difficulties on set up or have other technical questions, join our\n[gitter community](https://gitter.im/vencopy/community)\n\n",
                "dependencies": "[project]\nname = \"vencopy\"\nversion = \"1.0.4\"\ndescription = \"Vehicle Energy Consumption in Python: A tool to simulate load flexibility of electric vehicle fleets.\"\nauthors = [\n    { name = \"Fabia Miorelli\", email = \"fabia.miorelli@dlr.de\" },\n    { name = \"Niklas Wulff\", email = \"niklas.wulff@dlr.de\" }\n]\nreadme = \"README.md\"\nlicense-expression = \"BSD-3-Clause\"\n\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Intended Audience :: Science/Research\",\n    \"License :: OSI Approved :: BSD License\",\n    \"Operating System :: Microsoft :: Windows\",\n    \"Operating System :: Unix\",\n    \"Programming Language :: Python\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering\",\n]\n\nrequires-python = \">=3.9\"\n\n\ndependencies = [\n    \"pandas\",\n    \"click\",\n    \"pyyaml\",\n    \"scipy\",\n    \"matplotlib\",\n]\n\n[project.urls]\nHomepage = 'https://gitlab.com/dlr-ve/esy/vencopy/vencopy'\nDocumentation = 'https://dlr-ve.gitlab.io/esy/vencopy/vencopy/index.html'\nRepository = 'https://gitlab.com/dlr-ve/esy/vencopy/vencopy.git'\nGitter = 'https://gitter.im/vencopy/community'\n\n\n[project.optional-dependencies]\ndocs = [\n    \"sphinx\", \n    \"sphinx-rtd-theme\", \n    \"rst2pdf\"\n    ]\n\ntest = [\"pytest\", \"coverage\"]\n\ndev = [\n    \"black\",\n    ]\n\n[build-system]\nrequires = [\"hatchling>=1.26.1\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.pytest.ini_options]\npython_files = [\"test_*.py\", \"*_test.py\", \"tests.py\"]\n\n[tool.coverage.run]\nbranch = true\nsource = [\"vencopy\"]\n\n[tool.coverage.report]\nshow_missing = true\nprecision = 2\n\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/vinos",
            "repo_link": "https://codebase.helmholtz.cloud/mussel/netlogo-northsea-species.git",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/vitess",
            "repo_link": "https://iffgit.fz-juelich.de/vitess/vitess/-/releases",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/vitruvius",
            "repo_link": "https://github.com/vitruv-tools/Vitruv",
            "content": {
                "codemeta": "",
                "readme": "# Vitruv\n[![GitHub Action CI](https://github.com/vitruv-tools/Vitruv/actions/workflows/ci.yml/badge.svg)](https://github.com/vitruv-tools/Vitruv/actions/workflows/ci.yml)\n[![Latest Release](https://img.shields.io/github/release/vitruv-tools/Vitruv.svg)](https://github.com/vitruv-tools/Vitruv/releases/latest)\n[![Issues](https://img.shields.io/github/issues/vitruv-tools/Vitruv.svg)](https://github.com/vitruv-tools/Vitruv/issues)\n[![License](https://img.shields.io/github/license/vitruv-tools/Vitruv.svg)](https://raw.githubusercontent.com/vitruv-tools/Vitruv/main/LICENSE)\n\n[Vitruvius](https://vitruv.tools) is a framework for view-based (software) development.\nIt assumes different models to be used for describing a system, which are automatically kept consistent by the framework executing (semi-)automated rules that preserve consistency.\nThese models are modified only via views, which are projections from the underlying models.\nFor general information on Vitruvius, see our [GitHub Organisation](https://github.com/vitruv-tools) and our [Wiki](https://github.com/vitruv-tools/.github/wiki).\n\nThis project contains the central Vitruvius framework, providing the definition of a V-SUM (Virtual Single Underlying Model) containing development artifacts to be kept consistent and to be accessed and modified via views.\nIn the implementation, a V-SUM is called `VirtualModel`, which is instantiated with a set of `ChangePropagationSpecifications` (no matter whether they are developed with the [Vitruv-DSLs](https://github.com/vitruv-tools/Vitruv-DSLs) or just as an implementation of the interface defined in the [Vitruv-Change](https://github.com/vitruv-tools/Vitruv-Change) repository).\nThe `VirtualModel` then provides functionality to derive and modify views and to propagate the changes in these views back to the `VirtualModel`, which then executes the `ChangePropagationSpecifications` to preserve consistency.\n\n## Framework-internal Dependencies\n\nThis project depends on the following other projects from the Vitruvius framework:\n- [Vitruv-Change](https://github.com/vitruv-tools/Vitruv-Change)\n\n## Module Overview\n\n| Name         | Description                                                                                  |\n|--------------|----------------------------------------------------------------------------------------------|\n| views        | Definition of view types on the underlying models.                                           |\n| vsum         | Definition of V-SUMs with consistency preservation rules between meta-models and view types. |\n| remote       | Client-server infrastructure for working with V-SUMs.                                        |\n| applications | Definition of and registry for V-SUMs.                                                       |\n| *testutils*  | *Utilities for testing in Vitruvius or V-SUM projects.*                                      |\n\n",
                "dependencies": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n\n  <!-- Build Parent -->\n  <parent>\n    <groupId>tools.vitruv</groupId>\n    <artifactId>parent</artifactId>\n    <version>3.0.6</version>\n  </parent>\n\n  <!-- Project Information -->\n  <artifactId>tools.vitruv.framework</artifactId>\n  <version>3.2.0-SNAPSHOT</version>\n  <packaging>pom</packaging>\n\n  <name>Vitruv Framework</name>\n  <description>The Vitruv framework providing the definition of views and v-sums.</description>\n  <url>https://github.com/vitruv-tools/Vitruv</url>\n\n  <!-- Organizational Information -->\n  <licenses>\n    <license>\n      <name>Eclipse Public License - v 1.0</name>\n      <url>https://www.eclipse.org/org/documents/epl-v10.php</url>\n    </license>\n  </licenses>\n\n  <scm>\n    <connection>scm:git:git://github.com/vitruv-tools/Vitruv.git</connection>\n    <developerConnection>scm:git:https://github.com/vitruv-tools/Vitruv.git</developerConnection>\n    <url>https://github.com/vitruv-tools/Vitruv/tree/main</url>\n  </scm>\n\n  <!-- Modules -->\n  <modules>\n    <module>views</module>\n    <module>vsum</module>\n    <module>remote</module>\n    <module>testutils</module>\n    <module>applications</module>\n  </modules>\n\n  <properties>\n    <!-- Additional Repositories -->\n    <repo.emf-compare.version>3.3</repo.emf-compare.version>\n    <repo.sdq-commons.version>2.2.0</repo.sdq-commons.version>\n    <repo.xannotations.version>1.6.0</repo.xannotations.version>\n    <!-- SonarQube configuration -->\n    <sonar.host.url>https://sonarcloud.io</sonar.host.url>\n    <sonar.organization>vitruv-tools</sonar.organization>\n    <sonar.projectKey>vitruv-tools_Vitruv</sonar.projectKey>\n  </properties>\n\n  <build>\n    <plugins>\n      <plugin>\n        <groupId>org.openntf.maven</groupId>\n        <artifactId>p2-layout-resolver</artifactId>\n      </plugin>\n    </plugins>\n  </build>\n\n  <repositories>\n    <!-- Maven Central should have priority -->\n    <repository>\n      <id>central</id>\n      <name>Maven Central</name>\n      <url>https://repo1.maven.org/maven2/</url>\n      <snapshots>\n        <enabled>false</enabled>\n      </snapshots>\n    </repository>\n\n    <!-- for p2 dependencies, `groupId` specifies the repository -->\n    <repository>\n      <id>emf-compare</id>\n      <name>EMF Compare</name>\n      <layout>p2</layout>\n      <url>https://download.eclipse.org/modeling/emf/compare/updates/releases/${repo.emf-compare.version}</url>\n    </repository>\n    <repository>\n      <id>sdq-commons</id>\n      <name>SDQ Commons</name>\n      <url>https://kit-sdq.github.io/updatesite/release/commons/${repo.sdq-commons.version}</url>\n      <layout>p2</layout>\n    </repository>\n    <repository>\n      <id>xannotations</id>\n      <name>XAnnotations</name>\n      <layout>p2</layout>\n      <url>https://kit-sdq.github.io/updatesite/release/xannotations/${repo.xannotations.version}</url>\n    </repository>\n  </repositories>\n\n  <!-- Dependency Management -->\n  <dependencyManagement>\n    <dependencies>\n      <!-- Vitruvius dependencies -->\n      <dependency>\n        <groupId>tools.vitruv</groupId>\n        <artifactId>tools.vitruv.change.atomic</artifactId>\n        <version>3.1.0</version>\n      </dependency>\n      <dependency>\n        <groupId>tools.vitruv</groupId>\n        <artifactId>tools.vitruv.change.correspondence</artifactId>\n        <version>3.1.0</version>\n      </dependency>\n      <dependency>\n        <groupId>tools.vitruv</groupId>\n        <artifactId>tools.vitruv.change.composite</artifactId>\n        <version>3.1.0</version>\n      </dependency>\n      <dependency>\n        <groupId>tools.vitruv</groupId>\n        <artifactId>tools.vitruv.change.interaction</artifactId>\n        <version>3.1.0</version>\n      </dependency>\n      <dependency>\n        <groupId>tools.vitruv</groupId>\n        <artifactId>tools.vitruv.change.interaction.model</artifactId>\n        <version>3.1.0</version>\n      </dependency>\n      <dependency>\n        <groupId>tools.vitruv</groupId>\n        <artifactId>tools.vitruv.change.propagation</artifactId>\n        <version>3.1.0</version>\n      </dependency>\n      <dependency>\n        <groupId>tools.vitruv</groupId>\n        <artifactId>tools.vitruv.change.testutils.core</artifactId>\n        <version>3.1.0</version>\n      </dependency>\n      <dependency>\n        <groupId>tools.vitruv</groupId>\n        <artifactId>tools.vitruv.change.testutils.integration</artifactId>\n        <version>3.1.0</version>\n      </dependency>\n      <dependency>\n        <groupId>tools.vitruv</groupId>\n        <artifactId>tools.vitruv.change.testutils.metamodels</artifactId>\n        <version>3.1.0</version>\n      </dependency>\n      <dependency>\n        <groupId>tools.vitruv</groupId>\n        <artifactId>tools.vitruv.change.utils</artifactId>\n        <version>3.1.0</version>\n      </dependency>\n\n      <!-- External dependencies -->\n      <dependency>\n        <groupId>com.fasterxml.jackson.core</groupId>\n        <artifactId>jackson-databind</artifactId>\n        <version>2.18.2</version>\n      </dependency>\n      <dependency>\n        <groupId>com.fasterxml.jackson.core</groupId>\n        <artifactId>jackson-core</artifactId>\n        <version>2.18.2</version>\n      </dependency>\n      <dependency>\n        <groupId>com.google.guava</groupId>\n        <artifactId>guava</artifactId>\n        <version>33.3.1-jre</version>\n      </dependency>\n      <dependency>\n        <groupId>io.micrometer</groupId>\n        <artifactId>micrometer-core</artifactId>\n        <version>1.14.1</version>\n      </dependency>\n      <dependency>\n        <groupId>emf-compare</groupId>\n        <artifactId>org.eclipse.emf.compare</artifactId>\n        <version>3.5.3.202212280858</version>\n      </dependency>\n      <dependency>\n        <groupId>log4j</groupId>\n        <artifactId>log4j</artifactId>\n        <version>1.2.17</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.emf</groupId>\n        <artifactId>org.eclipse.emf.common</artifactId>\n        <version>2.40.0</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.emf</groupId>\n        <artifactId>org.eclipse.emf.ecore</artifactId>\n        <version>2.38.0</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.emf</groupId>\n        <artifactId>org.eclipse.emf.ecore.xmi</artifactId>\n        <version>2.38.0</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.emfcloud</groupId>\n        <artifactId>emfjson-jackson</artifactId>\n        <version>2.2.0</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.platform</groupId>\n        <artifactId>org.eclipse.core.runtime</artifactId>\n        <version>3.31.100</version>\n      </dependency>\n      <!-- required to mitigate Eclipse dependency signing problems -->\n      <dependency>\n        <groupId>org.eclipse.platform</groupId>\n        <artifactId>org.eclipse.equinox.common</artifactId>\n        <version>3.19.100</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.platform</groupId>\n        <artifactId>org.eclipse.equinox.registry</artifactId>\n        <version>3.12.100</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.xtend</groupId>\n        <artifactId>org.eclipse.xtend.lib</artifactId>\n        <version>2.37.0</version>\n      </dependency>\n      <dependency>\n        <groupId>org.eclipse.xtext</groupId>\n        <artifactId>org.eclipse.xtext.xbase.lib</artifactId>\n        <version>2.37.0</version>\n      </dependency>\n      <dependency>\n        <groupId>org.hamcrest</groupId>\n        <artifactId>hamcrest</artifactId>\n        <version>3.0</version>\n      </dependency>\n      <dependency>\n        <groupId>org.junit.jupiter</groupId>\n        <artifactId>junit-jupiter-api</artifactId>\n        <version>5.11.3</version>\n      </dependency>\n      <dependency>\n        <groupId>org.junit.jupiter</groupId>\n        <artifactId>junit-jupiter-params</artifactId>\n        <version>5.11.3</version>\n      </dependency>\n      <dependency>\n        <groupId>org.junit.platform</groupId>\n        <artifactId>junit-platform-commons</artifactId>\n        <version>1.11.3</version>\n      </dependency>\n      <dependency>\n        <groupId>org.junit.platform</groupId>\n        <artifactId>junit-platform-launcher</artifactId>\n        <version>1.11.3</version>\n      </dependency>\n      <dependency>\n        <groupId>sdq-commons</groupId>\n        <artifactId>edu.kit.ipd.sdq.commons.util.emf</artifactId>\n        <version>2.3.0.202304271319</version>\n        <exclusions>\n          <!-- exclude unnecessary transitive dependencies from sdq-commons p2 repository -->\n          <exclusion>\n            <groupId>*</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>sdq-commons</groupId>\n        <artifactId>edu.kit.ipd.sdq.commons.util.java</artifactId>\n        <version>2.3.0.202304271319</version>\n        <exclusions>\n          <!-- exclude unnecessary transitive dependencies from sdq-commons p2 repository -->\n          <exclusion>\n            <groupId>*</groupId>\n            <artifactId>*</artifactId>\n          </exclusion>\n        </exclusions>\n      </dependency>\n      <dependency>\n        <groupId>xannotations</groupId>\n        <artifactId>edu.kit.ipd.sdq.activextendannotations</artifactId>\n        <version>1.6.0</version>\n      </dependency>\n      <dependency>\n        <groupId>org.mockito</groupId>\n        <artifactId>mockito-core</artifactId>\n        <version>5.14.2</version>\n      </dependency>\n    </dependencies>\n  </dependencyManagement>\n\n</project>\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/voltron",
            "repo_link": "https://github.com/BIMSBbioinfo/VoltRon",
            "content": {
                "codemeta": "",
                "readme": "![](https://bimsbstatic.mdc-berlin.de/landthaler/VoltRon/Package/images/voltron_framework_box_io.png)\n\n<br>\n\n**Website and Tutorials**: <a href=\"https://bioinformatics.mdc-berlin.de/VoltRon\">https://bioinformatics.mdc-berlin.de/VoltRon</a>\n\n**VoltRon**  is a spatial omic analysis toolbox for multi-omics integration using spatial image registration. VoltRon is also capable of analyzing multiple types of spatially-aware data modalities.\n   \n   <ul class=\"maintext2\">\n    <li style=\"padding-bottom: 10px\">\n      <strong> Unique data structure </strong> of VoltRon allows users to seamlessly define tissue blocks, layers and multiple assay types in one R object.\n    </li>\n    <li style=\"padding-bottom: 10px\">\n      <strong> End-to-end downstream data analysis </strong> for distinct spatial biology technologies are supported. VoltRon visualizes and analyzes regions of interests (ROIs), spots, cells, molecules and tiles **(under development)**.\n    </li>\n    <li style=\"padding-bottom: 10px\">\n      <strong> Automated Image Registration </strong> incorporates <a href=\"https://opencv.org/\">OpenCV</a> (fully embedded into the package using <a href=\"https://www.rcpp.org/\">Rcpp</a>) to detect common features across images and achieves registration. Users may interact with built-in mini shiny apps to change alignment parameters and validate alignment accuracy.\n    </li>\n    <li style=\"padding-bottom: 10px\">\n      <strong> Manual Image Registration </strong> helps users to select common features across spatial datasets using reference images stored in VoltRon objects. In case automated image registration doesn't work, you can still align images by manually picking landmark points.\n    </li>\n    <li style=\"padding-bottom: 10px\">\n    <p style=\"padding-bottom: 3px\"> <strong> Spatially Aware Analysis </strong> allows detecting spatial patterns across cells, spots, molecules and other entities. </p>\n    <ul class=\"maintext3\">\n      <li style=\"padding-bottom: 10px padding-top: 12px\">\n      <strong>(Niche Clustering: Spots)</strong> VoltRon allows integration to single cell RNA datasets using <a href=\"https://satijalab.org/seurat/\">Seurat</a>, <a href=\"https://www.bioconductor.org/packages/release/bioc/vignettes/SingleCellExperiment/inst/doc/intro.html\">SingleCellExperiment</a> and <a href=\"https://github.com/dmcable/spacexr\">spacexr</a> for spot deconvolution. Estimated cell type abundances are then used to cluster spots into groups of cell type niches which are defined as spots with distinct composition of cell types.\n      </li>\n      <li style=\"padding-bottom: 2px\">\n      <strong>(Niche Clustering: Cells)</strong> VoltRon creates spatial neighborhoods around cells to cluster local cellular compositions around all cells which in turn informs users on cell types that are likely within proximity to each other.\n      </li>\n      <li style=\"padding-bottom: 10px\">\n      <strong>(Hot Spot Detection)</strong> VoltRon detects region of locally spatial patterns of cells/molecules/spots that are abundant in biological events and/or features.\n      </li>\n    </ul>  \n    </li>\n    <li style=\"padding-bottom: 10px\">\n    <p> <strong> Support for Big Data </strong> for VoltRon objects enables storing large feature data matrices and large microscopic images of tissues on disk without overloading memory, thus allowing analysis on large datasets with ease. VoltRon stores large images as pyramid structures to speed up visualization and data retrieval. </p>\n    </li>\n    <li style=\"padding-bottom: 10px\">\n    <p> <strong> Interoperability across R/Python frameworks </strong> allows users to convert VoltRon objects to a large number of objects used by other spatial omic platforms such as Seurat, Squidpy (AnnData), SpatialExperiment (BioConductor) and Giotto. </p>\n    </li>\n  </ul>\n\n## Staying up-to-date\n\nTo ask questions please use VoltRon discussion forum on google groups.\n\n- https://groups.google.com/forum/#!forum/voltron_discussion\n\n## Installation\n\nInstall from the GitHub repository using devtools (with R version 4.3.0 or higher):\n\n``` r\nif (!require(\"devtools\", quietly = TRUE))\n    install.packages(\"devtools\")\ndevtools::install_github(\"BIMSBbioinfo/VoltRon\")\n```\n\nDepending on the number of required dependencies, installation may be completed under a minute or may take a few minutes. \n\nOn **Windows** and **MacOS**, OpenCV will be downloaded automatically upon installation. However, [Rtools](https://cran.r-project.org/bin/windows/Rtools/rtools43/rtools.html) may be required to be downloaded too, hence this may take some time!\n\nOn **Ubuntu** we provide a set of instructions that may help users to build OpenCV with necessary headers [here](https://github.com/BIMSBbioinfo/VoltRon/blob/main/inst/extdata/install_ubuntu.md).\n\nOn **Fedora** you may need [`opencv-devel`](https://src.fedoraproject.org/rpms/opencv):\n\n```sh\nyum install opencv-devel\n```\n\n## Tutorials\n\nPlease see the [Explore](https://artur-man.github.io/VoltRon/tutorials.html) section in the VoltRon website for tutorials, example scripts and analysis found in the [preprint](https://www.biorxiv.org/content/10.1101/2023.12.15.571667v1). Tutorials include links for accessing necessary data to run scripts across all tutorials. \n\n## References\n\nManukyan, A., Bahry, E., Wyler, E., Becher, E., Pascual-Reguant, A., Plumbom, I., ... & Akalin, A. (2023). [VoltRon: A Spatial Omics Analysis Platform for Multi-Resolution and Multi-omics Integration using Image Registration](https://www.biorxiv.org/content/10.1101/2023.12.15.571667v1). bioRxiv, 2023-12.\n\n\n",
                "dependencies": "Package: VoltRon\nType: Package\nTitle: VoltRon for Spatial Data Integration and Analysis\nVersion: 0.2.0\nDepends: R (>= 4.3.0)\nAuthor: Artür Manukyan, Ella Bahry, Raj Prateek Rai, Wei-Che Ko, Deborah Schmidt, Markus Landthaler, Altuna Akalin\nMaintainer: Artür Manukyan <artur-man@hotmail.com>\nDescription: VoltRon is a novel spatial omic analysis toolbox for multi-omics integration using spatial image registration. VoltRon is capable of analyzing multiple types and modalities of spatially-aware datasets. VoltRon visualizes and analyzes regions of interests (ROIs), spots, cells and even molecules.\nLicense: MIT + file LICENSE\nSystemRequirements: OpenCV 4.8 (or higher): libopencv-dev (Debian, Ubuntu) or opencv-devel (Fedora)\nEncoding: UTF-8\nRoxygenNote: 7.3.2\nbiocViews:\nImports: methods, S4Arrays, grDevices, data.table, EBImage, RcppAnnoy, RANN,\n         Matrix, dplyr, ggplot2, ggrepel, igraph, irlba, rjson, magick, ids, \n         sp, reshape2, rlang, ggpubr, shiny, shinyjs, stringr, uwot, RCDT, \n         basilisk, reticulate\nLinkingTo: Rcpp, RcppArmadillo (>= 0.4)\nCollate: \n    'RcppExports.R'\n    'annotation.R'\n    'zzz.R'\n    'assay.R'\n    'auxiliary.R'\n    'generics.R'\n    'clustering.R'\n    'conversion.R'\n    'data.R'\n    'deconvolution.R'\n    'differentialexpression.R'\n    'image.R'\n    'import.R'\n    'integration.R'\n    'interactive.R'\n    'io.R'\n    'metadata.R'\n    'objects.R'\n    'processing.R'\n    'registration.R'\n    'sample.R'\n    'spatial.R'\n    'visualization.R'\nSuggests:\n    SpatialExperiment,\n    SingleCellExperiment,\n    Seurat,\n    SeuratObject,\n    DESeq2,\n    MuSiC,\n    spacexr,\n    RBioFormats,\n    XML,\n    ComplexHeatmap,\n    xlsx,\n    tiledb,\n    tiledbsc,\n    arrow,\n    vitessceR,\n    testthat (>= 3.0.0),\n    geojsonR,\n    circlize,\n    rstudioapi,\n    ggforce,\n    anndataR,\n    anndata,\n    DelayedArray,\n    hdf5r,\n    ImageArray,\n    BPCells,\n    HDF5DataFrame,\n    ZarrDataFrame\nConfig/testthat/edition: 3\nLazyData: true\nLazyDataCompression: gzip\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/weskit",
            "repo_link": "https://gitlab.com/one-touch-pipeline/weskit",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/wombat",
            "repo_link": "https://git.gsi.de/phelix/lv/wombat_ce",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/wps-command-line-tool-repository",
            "repo_link": "https://github.com/riesgos/gfz-command-line-tool-repository",
            "content": {
                "codemeta": "",
                "readme": "# gfz-riesgos-wps-repository\n\n[![pipeline status](https://gitext.gfz-potsdam.de/riesgos/gfz-riesgos-wps-repository/badges/master/pipeline.svg)](https://gitext.gfz-potsdam.de/riesgos/gfz-riesgos-wps-repository/commits/master)\n\n## Description\n\nThis is the java source code for the wps-repository for the riesgos project.\n\nIt aims to be an framework for easy integration of command line programs\nas web processing services and provide a bunch of services within the scope\nof the [RIESGOS project](http://www.riesgos.de/en/). This focus here is\nmainly on those processes provided by the [GFZ](https://www.gfz-potsdam.de/en/home/).\n\n## How it works\n\nThe processes that are integrated here are command line programs.\nMost processes integrated so far use python3 but any executable command line\nprogram can be integrated.\n\nEach process must be wrapped in a docker image to provide fully independent\nexecution of the processes (also in case of some hard coded temporary files)\nand to manage the dependencies (programs, libraries, python packages,\ninternal configuration files, ...).\n\nFor each processes a json configuration file must be provided, so that\nthe basic process skeleton - which is the same for all processes -\nknows how to provide the input data, how to\nstart the process and how to read the output of the programs. It is\nalso used to specify the way of error handling in the process skeleton.\n\nFor more information about dockerfiles you can take a look at\nthe [official docker documentation](https://docs.docker.com/engine/reference/builder/).\nThe role of docker for the overall framework here is explained on [its\nown documentation page](doc/RoleOfDocker.md).\n\nThe json configuration is explained in more detail\n[here](doc/JsonConfigurationExplaned.md).\n\n## Requirements\n\nAll of the code here runs on top of the WPS Server provided by\n[52° North](https://github.com/52North/WPS).\n\nFor other details please refer to the [installation guide](doc/Installationguide.md).\n\n## Currently implemented processes\n\nPlease refer to the following [sub page](doc/IncludedProcesses.md)\nfor an overview of the\nprocesses that are already on board.\n\nAdditionally to the main processes there are also some [format conversion\nprocesses](doc/FormatConversionProcesses.md) in the repository.\n\n## How to add a service\n\nIf you want to know how to add your own service, we provide a\nstep-by-step guide to add a service [here](doc/HowToAddOwnProcess.md).\n\n",
                "dependencies": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n\n    <!-- This pom.xml is mostly the same as\n        https://github.com/riesgos/52north-wps-osmtovector-process/blob/master/pom.xml\n\n        to ensure most possible compatibility\n    -->\n    <groupId>org.n52.gfz.riesgos.repository</groupId>\n    <artifactId>gfz-riesgos-wps</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <wps.version>4.0.0-beta.10</wps.version>\n        <java-version>1.8</java-version>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    </properties>\n\n    <repositories>\n        <repository>\n            <id>n52-releases</id>\n            <name>52n Releases</name>\n            <url>http://52north.org/maven/repo/releases</url>\n            <releases>\n                <enabled>true</enabled>\n            </releases>\n            <snapshots>\n                <enabled>false</enabled>\n            </snapshots>\n        </repository>\n        <repository>\n            <id>geotools</id>\n            <name>Geotools Repo</name>\n            <url>http://download.osgeo.org/webdav/geotools</url>\n            <releases>\n                <enabled>true</enabled>\n            </releases>\n            <snapshots>\n                <enabled>true</enabled>\n            </snapshots>\n        </repository>\n\n        <!-- as the originally boundless geo repo is not available anymore,\n        we need to try out some change like the one suggested here:\n        https://github.com/geotools/geotools/wiki/Change-from-webdav-to-nexus-repository\n        -->\n        <repository>\n            <snapshots>\n                <enabled>false</enabled>\n            </snapshots>\n            <id>osgeo-release</id>\n            <name>Open Source Geospatial Foundation Repository</name>\n            <url>https://repo.osgeo.org/repository/release/</url>\n        </repository>\n        <repository>\n            <snapshots>\n                <enabled>true</enabled>\n            </snapshots>\n            <id>osgeo-snapshot</id>\n            <name>Open Source Geospatial Foundation Build Repository</name>\n            <url>https://repo.osgeo.org/repository/snapshot/</url>\n        </repository>\n    </repositories>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.n52.wps</groupId>\n            <artifactId>52n-wps-commons</artifactId>\n            <version>${wps.version}</version>\n            <exclusions>\n                <exclusion>\n                    <groupId>org.apache.xmlbeans</groupId>\n                    <artifactId>xmlbeans</artifactId>\n                </exclusion>\n            </exclusions>\n        </dependency>\n        <dependency>\n            <groupId>org.n52.wps</groupId>\n            <artifactId>52n-wps-commons</artifactId>\n            <version>${wps.version}</version>\n            <classifier>tests</classifier>\n            <scope>test</scope>\n            <exclusions>\n                <exclusion>\n                    <groupId>org.apache.xmlbeans</groupId>\n                    <artifactId>xmlbeans</artifactId>\n                </exclusion>\n            </exclusions>\n        </dependency>\n        <dependency>\n            <groupId>org.n52.wps</groupId>\n            <artifactId>52n-wps-algorithm</artifactId>\n            <version>${wps.version}</version>\n            <exclusions>\n                <exclusion>\n                    <groupId>org.apache.xmlbeans</groupId>\n                    <artifactId>xmlbeans</artifactId>\n                </exclusion>\n            </exclusions>\n        </dependency>\n        <dependency>\n            <groupId>org.n52.wps</groupId>\n            <artifactId>52n-wps-io-geotools</artifactId>\n            <version>${wps.version}</version>\n            <exclusions>\n                <exclusion>\n                    <groupId>org.apache.xmlbeans</groupId>\n                    <artifactId>xmlbeans</artifactId>\n                </exclusion>\n                <exclusion>\n                    <groupId>commons-httpclient</groupId>\n                    <artifactId>commons-httpclient</artifactId>\n                </exclusion>\n                <exclusion>\n                    <groupId>commons-io</groupId>\n                    <artifactId>commons-io</artifactId>\n                </exclusion>\n                <exclusion>\n                    <groupId>com.vividsolutions</groupId>\n                    <artifactId>jts</artifactId>\n                </exclusion>\n            </exclusions>\n        </dependency>\n        <dependency>\n            <groupId>org.n52.wps</groupId>\n            <artifactId>52n-wps-io</artifactId>\n            <version>${wps.version}</version>\n            <scope>provided</scope>\n            <exclusions>\n                <exclusion>\n                    <groupId>org.apache.xmlbeans</groupId>\n                    <artifactId>xmlbeans</artifactId>\n                </exclusion>\n                <exclusion>\n                    <groupId>commons-io</groupId>\n                    <artifactId>commons-io</artifactId>\n                </exclusion>\n            </exclusions>\n        </dependency>\n        <dependency>\n            <groupId>org.n52.wps</groupId>\n            <artifactId>52n-wps-io-impl</artifactId>\n            <version>${wps.version}</version>\n            <exclusions>\n                <exclusion>\n                    <groupId>org.apache.xmlbeans</groupId>\n                    <artifactId>xmlbeans</artifactId>\n                </exclusion>\n                <exclusion>\n                    <groupId>commons-io</groupId>\n                    <artifactId>commons-io</artifactId>\n                </exclusion>\n            </exclusions>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.commons</groupId>\n            <artifactId>commons-compress</artifactId>\n            <version>1.19</version>\n        </dependency>\n        <dependency>\n            <groupId>commons-lang</groupId>\n            <artifactId>commons-lang</artifactId>\n            <version>2.5</version>\n        </dependency>\n        <dependency>\n            <groupId>commons-io</groupId>\n            <artifactId>commons-io</artifactId>\n            <version>2.0</version>\n        </dependency>\n        <dependency>\n            <groupId>commons-httpclient</groupId>\n            <artifactId>commons-httpclient</artifactId>\n            <version>3.1</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.xmlbeans</groupId>\n            <artifactId>xmlbeans</artifactId>\n            <version>2.6.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.ant</groupId>\n            <artifactId>ant</artifactId>\n            <version>1.10.8</version>\n        </dependency>\n        <dependency>\n            <groupId>org.geotools</groupId>\n            <artifactId>gt-process-raster</artifactId>\n            <version>13.5</version>\n        </dependency>\n        <dependency>\n            <groupId>com.google.guava</groupId>\n            <artifactId>guava</artifactId>\n            <version>28.0-jre</version>\n        </dependency>\n\n        <!-- if you need the interpolation function (kriging)\n        add this dependency and insert it to\n\n        <dependency>\n            <groupId>com.github.haifengl</groupId>\n            <artifactId>smile-interpolation</artifactId>\n            <version>1.5.3</version>\n        </dependency>\n        -->\n        <!-- just for stand alone testing;\n        should later be removed\n        -->\n        <!--<dependency>\n            <groupId>org.n52.wps</groupId>\n            <artifactId>52n-wps-io</artifactId>\n            <version>4.0.0-beta.7-SNAPSHOT</version>\n        </dependency>-->\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.13.1</version>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>org.powermock</groupId>\n            <artifactId>powermock-module-junit4</artifactId>\n            <version>2.0.2</version>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>org.powermock</groupId>\n            <artifactId>powermock-api-mockito2</artifactId>\n            <version>2.0.2</version>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n    <build>\n        <pluginManagement>\n            <plugins>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-compiler-plugin</artifactId>\n                    <version>3.1</version>\n                    <configuration>\n                        <source>${java-version}</source>\n                        <target>${java-version}</target>\n                        <compilerArgument>-Xlint:all</compilerArgument>\n                        <showWarnings>true</showWarnings>\n                        <showDeprecation>true</showDeprecation>\n                    </configuration>\n                </plugin>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-assembly-plugin</artifactId>\n                    <version>2.2.2</version>\n                </plugin>\n            </plugins>\n        </pluginManagement>\n        <resources>\n            <resource>\n                <directory>src/main/resources</directory>\n                <filtering>true</filtering>\n                <includes>\n                    <include>**/*</include>\n                </includes>\n            </resource>\n        </resources>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <configuration>\n                            <descriptors>\n                                <descriptor>src/main/config/assemble.xml</descriptor>\n                            </descriptors>\n                        </configuration>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                        <phase>install</phase>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-checkstyle-plugin</artifactId>\n                <version>3.1.0</version>\n                <configuration>\n                    <configLocation>sun_checks.xml</configLocation>\n                    <encoding>UTF-8</encoding>\n                    <consoleOutput>true</consoleOutput>\n                    <failsOnError>true</failsOnError>\n                    <linkXRef>false</linkXRef>\n                    <suppressionsLocation>checkstyle-suppressions.xml</suppressionsLocation>\n                    <suppressionsFileExpression>checkstyle.suppressions.file</suppressionsFileExpression>\n                </configuration>\n            </plugin>\n            <plugin>\n                <groupId>pl.project13.maven</groupId>\n                <artifactId>git-commit-id-plugin</artifactId>\n                <version>3.0.0</version>\n                <executions>\n                    <execution>\n                        <goals>\n                            <goal>revision</goal>\n                        </goals>\n                        <phase>validate</phase>\n                    </execution>\n                </executions>\n                <configuration>\n                    <commitIdGenerationMode>flat</commitIdGenerationMode>\n                    <gitDescribe>\n                        <skip>true</skip>\n                    </gitDescribe>\n                </configuration>\n            </plugin>\n            <plugin>\n                <groupId>org.codehaus.mojo</groupId>\n                <artifactId>templating-maven-plugin</artifactId>\n                <version>1.0.0</version>\n                <executions>\n                    <execution>\n                        <id>filter-src</id>\n                        <goals>\n                            <goal>filter-sources</goal>\n                        </goals>\n                    </execution>\n                    <execution>\n                        <id>filter-test-src</id>\n                        <goals>\n                            <goal>filter-test-sources</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/wrainfo",
            "repo_link": "https://git.gfz-potsdam.de/fernlab/products/furuno/wrainfo",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/xcascade",
            "repo_link": "",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/xdibias",
            "repo_link": "https://gitlab.dlr.de/xdibias/xdibias",
            "content": {}
        },
        {
            "software_organization": "https://helmholtz.software/software/xraypac",
            "repo_link": "https://gitlab.desy.de/cdt/xraypac",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        }
    ]
}