{
    "repositories_with_content": [
        {
            "software_organization": "https://helmholtz.software/software/2x2-3x3-and-nxn-space-filling-curves",
            "repo_link": "https://github.com/jokergoo/sfcurve",
            "content": {
                "codemeta": "",
                "readme": "# sfcurve: 2x2, 3x3 and nxn Space-Filling Curves [![CRAN](https://www.r-pkg.org/badges/version/sfcurve)](https://cran.r-project.org/web/packages/sfcurve/index.html) [![CRAN](https://cranlogs.r-pkg.org/badges/grand-total/sfcurve)](https://cran.r-project.org/web/packages/sfcurve/index.html) ![](https://github.com/user-attachments/assets/7e0e14e7-1300-421f-8ffe-113b80caee97) This package provides a way to encode all possible forms of 2x2 and 3x3 space-filling curves. For example, the following eight forms correspond to the 2x2 curve on level 3 and with `R(0)` (bottom-in right-out base pattern with rotation of 0 degree) as the seed. <img src=\"https://github.com/user-attachments/assets/82b56013-8e9e-45f6-b77a-0875769c6369\" width=700 /> It also supports nxn curves expanded from any valid level-1 unit. ## Install ```r install.packages(\"sfcurve\") ``` or the devel version: ```r devtools::install_github(\"jokergoo/sfcurve\") ``` ## Usage Hilbert curve (2x2): ``` > sfc_2x2(\"I\", \"111\") An sfc_2x2 object. Increase mode: 2 x 2 Level: 3 Expansion rule: 2x2 A sequence of 64 base patterns. R(0)L(270)L(0)R(90) I(0)R(0)R(270)L(180) L(270)R(0)R(270)I(180) R(180)L(90)L(180)I(270) .... other 4 lines .... I(90)L(90)L(180)R(270) I(180)R(180)R(90)L(0) L(90)R(180)R(90)I(0) R(0)L(270)L(0)R(90) Seed: A sequence of 1 base pattern. I(0) ``` Peano curve (3x3): ``` > sfc_3x3_peano(\"I\", \"111\") An sfc_3x3_peano object. Increase mode: 3 x 3 Level: 3 Expansion rule: 3x3 Peano A sequence of 729 base patterns. I(0)J(0)R(0)R(270) I(180)L(180)L(270)J(0) I(0)J(0)I(0)L(0) L(90)J(180)R(180)R(90) .... other 88 lines .... I(0)J(0)R(0)R(270) I(180)L(180)L(270)J(0) I(0) Seed: A sequence of 1 base pattern. I(0) ``` Meander curve (3x3): ``` > sfc_3x3_meander(\"I\", \"111\") An sfc_3x3_meander object. Increase mode: 3 x 3 Level: 3 Expansion rule: 3x3 Meander A sequence of 729 base patterns. R(0)I(270)L(270)I(0) L(0)L(90)R(180)R(90) I(0)R(0)I(270)L(270) I(0)L(0)L(90)R(180) .... other 88 lines .... R(0)I(270)L(270)I(0) L(0)L(90)R(180)R(90) I(0) Seed: A sequence of 1 base pattern. I(0) ``` It also allows using a sequence as the seed: ``` p = sfc_seed(\"LLLILILIILIILIIILIIILIIII\") p2 = sfc_2x2(p, \"1111\") plot(p2) ``` <img src=\"https://github.com/user-attachments/assets/f1144f7f-282f-4988-aafd-9f712dd3ed2d\" width=500 /> For more comprehensive introduction of the theory and the package, please refer to the vignettes. ## License MIT @ Zuguang Gu\n",
                "dependencies": "Package: sfcurve Type: Package Title: 2x2, 3x3 and Nxn Space-Filling Curves Version: 1.0.1 Date: 2024-09-10 Authors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"), comment = c('ORCID'=\"0000-0002-7395-8709\")) Depends: R (>= 4.0.0) Imports: grid, Rcpp, methods, colorRamp2 Suggests: rmarkdown, knitr, rgl, testthat, ComplexHeatmap, igraph, digest VignetteBuilder: knitr Description: Implementation of all possible forms of 2x2 and 3x3 space-filling curves, i.e., the generalized forms of the Hilbert curve <https://en.wikipedia.org/wiki/Hilbert_curve>, the Peano curve <https://en.wikipedia.org/wiki/Peano_curve> and the Peano curve in the meander type (Figure 5 in <https://eudml.org/doc/141086>). It can generates nxn curves expanded from any specific level-1 units. It also implements the H-curve and the three-dimensional Hilbert curve. URL: https://github.com/jokergoo/sfcurve, https://jokergoo.github.io/sfcurve/ License: MIT + file LICENSE LinkingTo: Rcpp Encoding: UTF-8 Roxygen: list(markdown = TRUE) RoxygenNote: 7.3.1\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ai4hpc",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/CoE-RAISE/FZJ/ai4hpc/ai4hpc",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/aiida-kkr",
            "repo_link": "https://github.com/JuDFTteam/aiida-kkr",
            "content": {
                "codemeta": "",
                "readme": "[![aiida-core](https://img.shields.io/badge/AiiDA->=v2.0.0,<3.0.0-1f425f.svg?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAACMAAAAhCAYAAABTERJSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAFhgAABYYBG6Yz4AAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAAUbSURBVFiFzZhrbFRVEMd%2Fc%2B5uu6UUbIFC%2FUAUVEQCLbQJBIiBDyiImJiIhmohYNCkqJAQxASLF8tDgYRHBLXRhIcKNtFEhVDgAxBJqgmVh4JEKg3EIn2QYqBlt917xg%2BFss%2ByaDHOtzsz5z%2B%2FuZl7ztmF%2F5HJvxVQN6cPYX8%2FPLnOmsvNAvqfwuib%2FbNIk9cQeQnLcKRL5xLIV%2Fic9eJeunjPYbRs4FjQSpTB3aS1IpRKeeOOewajy%2FKKEO8Q0DuVdKy8IqsbPulxGHUfCBBu%2BwUYGuFuBTK7wQnht6PEbf4tlRomVRjCbXNjQEB0AyrFQOL5ENIJm7dTLZE6DPJCnEtFZVXDLny%2B4Sjv0PmmYu1ZdUek9RiMgoDmJ8V0L7XJqsZ3UW8YsBOwEeHeeFce7jEYXBy0m9m4BbXqSj2%2Bxnkg26MCVrN6DEZcwggtd8pTFx%2Fh3B9B50YLaFOPwXQKUt0tBLegtSomfBlfY13PwijbEnhztGzgJsK5h9W9qeWwBqjvyhB2iBs1Qz0AU974DciRGO8CVN8AJhAeMAdA3KbrKEtvxhsI%2B9emWiJlGBEU680Cfk%2BSsVqXZvcFYGXjF8ABVJ%2BTNfVXehyms1zzn1gmIOxLEB6E31%2FWBe5rnCarmo7elf7dJEeaLh80GasliI5F6Q9cAz1GY1OJVNDxTzQTw7iY%2FHEZRQY7xqJ9RU2LFe%2FYqakdP911ha0XhjjiTVAkDwgatWfCGeYocx8M3glG8g8EXhSrLrHnEFJ5Ymow%2FkhIYv6ttYUW1iFmEqqxdVoUs9FmsDYSqmtmJh3Cl1%2BVtl2s7owDUdocR5bceiyoSivGTT5vzpbzL1uoBpmcAAQgW7ArnKD9ng9rc%2BNgrobSNwpSkkhcRN%2BvmXLjIsDovYHHEfmsYFygPAnIDEQrQPzJYCOaLHLUfIt7Oq0LJn9fxkSgNCb1qEIQ5UKgT%2Fs6gJmVOOroJhQBXVqw118QtWLdyUxEP45sUpSzqP7RDdFYMyB9UReMiF1MzPwoUqHt8hjGFFeP5wZAbZ%2F0%2BcAtAAcji6LeSq%2FMYiAvSsdw3GtrfVSVFUBbIhwRWYR7yOcr%2FBi%2FB1MSJZ16JlgH1AGM3EO2QnmMyrSbTSiACgFBv4yCUapZkt9qwWVL7aeOyHvArJjm8%2Fz9BhdI4XcZgz2%2FvRALosjsk1ODOyMcJn9%2FYI6IrkS5vxMGdUwou2YKfyVqJpn5t9aNs3gbQMbdbkxnGdsr4bTHm2AxWo9yNZK4PXR3uzhAh%2BM0AZejnCrGdy0UvJxl0oMKgWSLR%2B1LH2aE9ViejiFs%2BXn6bTjng3MlIhJ1I1TkuLdg6OcAbD7Xx%2Bc3y9TrWAiSHqVkbZ2v9ilCo6s4AjwZCzFyD9mOL305nV9aonvsQeT2L0gVk4OwOJqXXVRW7naaxswDKVdlYLyMXAnntteYmws2xcVVZzq%2BtHPAooQggmJkc6TLSusOiL4RKgwzzYU1iFQgiUBA1H7E8yPau%2BZl9P7AblVNebtHqTgxLfRqrNvZWjsHZFuqMqKcDWdlFjF7UGvX8Jn24DyEAykJwNcdg0OvJ4p5pQ9tV6SMlP4A0PNh8aYze1ArROyUNTNouy8tNF3Rt0CSXb6bRFl4%2FIfQzNMjaE9WwpYOWQnOdEF%2BTdJNO0iFh7%2BI0kfORzQZb6P2kymS9oTxzBiM9rUqLWr1WE5G6ODhycQd%2FUnNVeMbcH68hYkGycNoUNWc8fxaxfwhDbHpfwM5oeTY7rUX8QAAAABJRU5ErkJggg%3D%3D)](https://www.aiida.net/) [![Documentation Status](https://readthedocs.org/projects/aiida-kkr/badge/?version=latest)](https://aiida-kkr.readthedocs.io/en/latest/?badge=latest) [![Build status](https://github.com/JuDFTteam/aiida-kkr/actions/workflows/ci.yml/badge.svg)](https://github.com/JuDFTteam/aiida-kkr/actions) [![codecov](https://codecov.io/gh/JuDFTteam/aiida-kkr/branch/develop/graph/badge.svg)](https://codecov.io/gh/JuDFTteam/aiida-kkr) [![MIT license](http://img.shields.io/badge/license-MIT-brightgreen.svg)](http://opensource.org/licenses/MIT) [![GitHub version](https://badge.fury.io/gh/JuDFTteam%2Faiida-kkr.svg)](https://badge.fury.io/gh/JuDFTteam%2Faiida-kkr) [![PyPI version](https://badge.fury.io/py/aiida-kkr.svg)](https://badge.fury.io/py/aiida-kkr) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3628250.svg)](https://doi.org/10.5281/zenodo.3628250) # aiida-kkr [AiiDA](https://aiida.net) plugin for the [Jülich KKR codes](https://jukkr.fz-juelich.de) plus workflows and utility. ## Features * KKR calculations for bulk and interfaces * treatment of alloys using VCA or CPA * self-consistency, DOS and bandstructure calculations * extraction of magnetic exchange coupling parameters (*J_ij*, *D_ij*) * impurity embedding solving the Dyson equation ## How to cite If you use this plugin please cite: > [Rüßmann, P., Bertoldo, F. & Blügel, S. The AiiDA-KKR plugin and its application to high-throughput impurity embedding into a topological insulator. *npj Comput Mater* **7**, 13 (2021). https://doi.org/10.1038/s41524-020-00482-5](https://doi.org/10.1038/s41524-020-00482-5) The ArXiv preprint can be found here: > [Philipp Rüßmann, Fabian Bertoldo and Stefan Blügel, *The AiiDA-KKR plugin and its application to high-throughput impurity embedding into a topological insulator*, arXiv:2003.08315 [cond-mat.mtrl-sci] (2020)](https://arxiv.org/abs/2003.08315) # Installation ```shell $ pip install aiida-kkr # install latest version of aiida-kkr (published on pypi.org) $ reentry scan -r aiida # update entry points, needed in order to find kkr.* entrypoints in aiida # setupt aiida if this was not done already: $ verdi quicksetup # better to set up a new profile $ verdi calculation plugins # should now show kkr.* entrypoints ``` To install the developer version download the repository and install the downloaded version (see `setup.json` for a list of optional packages that are installed with the extras given in `[]`) ```shell $ git clone https://github.com/JuDFTteam/aiida-kkr.git $ pip install -e aiida-kkr[testing,devtools,docs] $ reentry scan -r aiida ``` ## Remarks about dependencies and extras - The `aiida-kkr` plugin uses the `ase` and `pymatgen` packages for structure conversions. - For `aiida-core>=1.5,<1.6` make sure to use the requirements specified in `requirements_aiida-core_1.5.txt` (use `pip install -r requirements_aiida-core_1.5.txt aiida-kkr` for the installation to overwrite the aiida-core dependency). - Other extras that can be optionally installed with `aiida-kkr` are * `pre-commit` which installes the pre-commit hooks that allow style (`yapf`) and static code checking (`pylint`) * `testing` which installs `pytest` and all extension used in the tests * `docs` which installs `Sphinx` to build the documentation * `devtools` which installs tools that might be helpful during development # Usage and Documentation * see http://aiida-kkr.readthedocs.io for user's and developer's guides and API reference * check out http://judft.de and https://jukkr.fz-juelich.de for information of the KKR codes used by the plugin # Contributing Thank you for your interest in contributing to aiida-kkr. Check out our [contributing guide](CONTRIBUTING.md) for some information. # Releasing new versions To create a new release follow these steps: - finish your development and merge it into the `develop` branch - update documentation - update / fix tests - bump version numbers (in files `aiida_kkr/__init__.py`, `pyproject.toml`, `.bumpversion.cfg`) - merge changes from `develop` back into `master` and create a tag for the new version number (this triggers publication to pypi)\n",
                "dependencies": "[build-system] requires = [ \"setuptools>=61.2\",] build-backend = \"setuptools.build_meta\" [project] name = \"aiida-kkr\" version = \"2.3.1\" description = \"AiiDA plugin for the JuKKR codes\" classifiers = [ \"License :: OSI Approved :: MIT License\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.7\", \"Programming Language :: Python :: 3.8\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Development Status :: 4 - Beta\", \"Environment :: Plugins\", \"Intended Audience :: Science/Research\", \"Topic :: Scientific/Engineering :: Physics\", \"Natural Language :: English\", \"Framework :: AiiDA\", ] dependencies = [ \"aiida-core >= 2.0.0,<3.0.0\", \"masci-tools >= 0.4.8.dev5,<1.0.0\", \"seekpath >= 1.9.2\", \"ase\", \"pymatgen\", ] license = {file = \"LICENSE.txt\"} keywords = [ \"material science\", \"aiida\", \"dft\", \"all-electron\", \"kkr\", ] [[project.authors]] name = \"Philipp Ruessmann\" email = \"p.ruessmann@fz-juelich.de\" [[project.authors]] name = \"Jens Broeder\" email = \"j.broeder@fz-juelich.de\" [[project.authors]] name = \"Fabian Bertoldo\" email = \"f.bertoldo@fz-juelich.de\" [project.readme] file = \"README.md\" content-type = \"text/markdown\" [project.urls] Homepage = \"https://github.com/JuDFTteam/aiida-kkr\" Download = \"https://github.com/JuDFTteam/aiida-kkr\" Documentation = \"https://aiida-kkr.readthedocs.io\" [project.optional-dependencies] pre-commit = [ \"pre-commit >= 4.0.1\", \"yapf >= 0.43.0\", \"pylint == 1.9.4; python_version < '3.0'\", \"pylint >= 3.3.1; python_version >= '3.0'\", ] testing = [ \"pgtest >= 1.3.0\", \"pytest-xdist\", \"pytest-cov >= 2.5.0\", \"pytest-mpl >= 0.10\", \"pytest-timeout >= 1.3.3\", \"pytest-regressions >= 1.0\", \"MarkupSafe < 3.1.0\", \"aiida-test-cache\" ] docs = [ \"Sphinx >= 1.8.2\", \"sphinx_rtd_theme >= 0.4.2\", ] devtools = [ \"bump2version >= 0.5.10\", ] widgets = [ \"ase_notebook\", ] [project.scripts] aiida-kkr = \"aiida_kkr.cmdline:cmd_root\" [tool.setuptools] include-package-data = false [project.entry-points.\"aiida.calculations\"] \"kkr.voro\" = \"aiida_kkr.calculations.voro:VoronoiCalculation\" \"kkr.kkr\" = \"aiida_kkr.calculations.kkr:KkrCalculation\" \"kkr.kkrimp\" = \"aiida_kkr.calculations.kkrimp:KkrimpCalculation\" \"kkr.kkrnano\" = \"aiida_kkr.calculations.kkrnano:KKRnanoCalculation\" \"kkr.kkrimporter\" = \"aiida_kkr.calculations.kkrimporter:KkrImporterCalculation\" [project.entry-points.\"aiida.parsers\"] \"kkr.voroparser\" = \"aiida_kkr.parsers.voro:VoronoiParser\" \"kkr.kkrparser\" = \"aiida_kkr.parsers.kkr:KkrParser\" \"kkr.kkrimpparser\" = \"aiida_kkr.parsers.kkrimp:KkrimpParser\" \"kkr.kkrnanoparser\" = \"aiida_kkr.parsers.kkrnano:KKRnanoParser\" \"kkr.kkrimporterparser\" = \"aiida_kkr.parsers.kkrimporter:KkrImporterParser\" [project.entry-points.\"aiida.data\"] \"kkr.strucwithpot\" = \"aiida_kkr.data.strucwithpot:StrucWithPotData\" [project.entry-points.\"aiida.workflows\"] \"kkr.scf\" = \"aiida_kkr.workflows.kkr_scf:kkr_scf_wc\" \"kkr.dos\" = \"aiida_kkr.workflows.dos:kkr_dos_wc\" \"kkr.bs\" = \"aiida_kkr.workflows.bs:kkr_bs_wc\" \"kkr.eos\" = \"aiida_kkr.workflows.eos:kkr_eos_wc\" \"kkr.startpot\" = \"aiida_kkr.workflows.voro_start:kkr_startpot_wc\" \"kkr.gf_writeout\" = \"aiida_kkr.workflows.gf_writeout:kkr_flex_wc\" \"kkr.imp\" = \"aiida_kkr.workflows.kkr_imp:kkr_imp_wc\" \"kkr.imp_sub\" = \"aiida_kkr.workflows.kkr_imp_sub:kkr_imp_sub_wc\" \"kkr.imp_dos\" = \"aiida_kkr.workflows.kkr_imp_dos:kkr_imp_dos_wc\" \"kkr.imp_BdG\" = \"aiida_kkr.workflows.imp_BdG:kkrimp_BdG_wc\" \"kkr.decimation\" = \"aiida_kkr.workflows._decimation:kkr_decimation_wc\" \"kkr.jij\" = \"aiida_kkr.workflows.jijs:kkr_jij_wc\" \"kkr.combine_imp\" = \"aiida_kkr.workflows._combine_imps:combine_imps_wc\" \"kkr.STM\" = \"aiida_kkr.workflows.kkr_STM:kkr_STM_wc\" [tool.setuptools.packages.find] namespaces = false include = [\"aiida_kkr*\"] exclude = [\"docs*\", \"tests*\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/aiida-spirit",
            "repo_link": "https://github.com/JuDFTteam/aiida-spirit",
            "content": {
                "codemeta": "",
                "readme": "[![Build Status](https://github.com/JuDFTteam/aiida-spirit/workflows/ci/badge.svg?branch=master)](https://github.com/JuDFTteam/aiida-spirit/actions) [![Coverage Status](https://codecov.io/gh/JuDFTteam/aiida-spirit/branch/main/graph/badge.svg?token=F7ISM4558S)](https://codecov.io/gh/JuDFTteam/aiida-spirit) [![Docs status](https://readthedocs.org/projects/aiida-spirit/badge)](http://aiida-spirit.readthedocs.io/) [![PyPI version](https://badge.fury.io/py/aiida-spirit.svg)](https://badge.fury.io/py/aiida-spirit) [![DOI](https://zenodo.org/badge/364820045.svg)](https://zenodo.org/badge/latestdoi/364820045) # aiida-spirit AiiDA plugin for the [spirit code](http://spirit-code.github.io/) ## Installation ```shell pip install aiida-spirit # install aiida-spirit from pypi verdi quicksetup # better to set up a new profile verdi plugin list aiida.calculations # should now show your calclulation plugins ``` ## Usage Here goes a complete example of how to submit a test calculation using this plugin. A quick demo of how to submit a calculation (the spirit python API needs to be installed for this to work: `pip install spirit`): ```shell verdi daemon start # make sure the daemon is running cd examples ./example_LLG.py # run test calculation verdi process list -a # check record of calculation ``` ## Development ```shell git clone https://github.com/JuDFTteam/aiida-spirit . cd aiida-spirit pip install -e .[pre-commit,testing] # install extra dependencies pre-commit install # install pre-commit hooks pytest -v # discover and run all tests ``` Note that `pytest -v` will create a test database and profile which requires to find the `pg_ctl` command. If `pg_ctl` is not found you need to nake sure that postgres is installed and then add the localtion of `pg_ctl` to the `PATH`: ``` # add postgres path for pg_ctl to PATH # this is an example for Postgres 9.6 installed on a mac PATH=\"/Applications/Postgres.app/Contents/Versions/9.6/bin/:$PATH\" export PATH ``` ## Citation If you use AiiDA-Spirit please cite the method paper - P. Rüßmann, J. Ribas Sobreviela, M. Sallermann, M. Hoffmann, F. Rhiem, and S. Blügel, *The AiiDA-Spirit Plugin for Automated Spin-Dynamics Simulations and Multi-Scale Modeling Based on First-Principles Calculations*, Front. Mater. **9**, 825043 (2022). [doi: 10.3389/fmats.2022.825043](https://doi.org/10.3389/fmats.2022.825043), and the latest code release - P. Rüßmann, J. Ribas Sobreviela, M. Sallermann, M. Hoffmann, F. Rhiem, and S. Blügel. JuDFTteam/aiida-spirit. Zenodo. [doi: 10.5281/zenodo.8070770](https://doi.org/10.5281/zenodo.8070770). ## License The AiiDA-Spirit code is under the [MIT license](LICENSE). ## Contact p.ruessmann@fz-juelich.de\n",
                "dependencies": "[tool.pylint.format] max-line-length = 125 [tool.pylint.messages_control] disable = [ \"too-many-ancestors\", \"invalid-name\", ] [tool.pytest.ini_options] python_files = \"test_*.py example_*.py\" filterwarnings = [ \"ignore::DeprecationWarning:aiida:\", \"ignore::DeprecationWarning:plumpy:\", \"ignore::DeprecationWarning:django:\", \"ignore::DeprecationWarning:yaml:\", ]\n# -*- coding: utf-8 -*- \"\"\"Setup script for aiida-spirit\"\"\" import json from setuptools import setup, find_packages if __name__ == '__main__': # Provide static information in setup.json # such that it can be discovered automatically with open('setup.json', 'r') as info: kwargs = json.load(info) setup( packages=find_packages(include=['aiida_spirit', 'aiida_spirit.*']), # this doesn't work when placed in setup.json (something to do with str type) package_data={ '': ['*'], }, long_description=open('README.md').read(), long_description_content_type='text/markdown', **kwargs)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/album",
            "repo_link": "https://gitlab.com/album-app/album",
            "content": {
                "codemeta": "",
                "readme": "# album Introduction: [https://album.solutions/](https://album.solutions/) Documentation: [https://docs.album.solutions/](https://docs.album.solutions/) ## Citation Albrecht, J.P.\\*, Schmidt, D.\\*, and Harrington, K., 2021. Album: a framework for scientific data processing with software solutions of heterogeneous tools. arXiv preprint arXiv:2110.00601. https://arxiv.org/abs/2110.00601 ## Developers - Kyle Harrington, Max Delbrueck Center for Molecular Medicine in the Helmholtz Association - Jan Philipp Albrecht, Max Delbrueck Center for Molecular Medicine in the Helmholtz Association - Deborah Schmidt, Max Delbrueck Center for Molecular Medicine in the Helmholtz Association\n",
                "dependencies": "[build-system] requires = [ \"setuptools>=42\", \"wheel\" ] build-backend = \"setuptools.build_meta\"\nfrom setuptools import setup setup()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/alice2modelica",
            "repo_link": "https://jugit.fz-juelich.de/iek-10/public/tools/alice2modelica",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/allpix-squared",
            "repo_link": "https://gitlab.cern.ch/allpix-squared/allpix-squared",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/alpaca",
            "repo_link": "https://github.com/INM-6/alpaca",
            "content": {
                "codemeta": "",
                "readme": "# alpaca ## Automated Lightweight Provenance Capture [![tests](https://github.com/INM-6/alpaca/actions/workflows/CI.yml/badge.svg)](https://github.com/INM-6/alpaca/actions/workflows/CI.yml) [![Documentation Status](https://readthedocs.org/projects/alpaca-prov/badge/?version=latest)](https://alpaca-prov.readthedocs.io/en/latest/?badge=latest) Alpaca is a Python package for the capture of provenance information during the execution of Python scripts that process data. Alpaca provides a simple API for recording the details of the functions being executed, the data flow, and a description of parameters used. This is accomplished with minimal code instrumentation and user intervention. Provenance information is structured and serialized according to a model based on the [W3C PROV format](https://www.w3.org/TR/prov-overview). ## Table of contents - [Prerequisites](#prerequisites) - [Installation](#installation) - [Documentation](#documentation) - [How to run](#how-to-run) - [Collaborators](#collaborators) - [How to contribute](#how-to-contribute) - [Get support](#get-support) - [Acknowledgments](#acknowledgments) - [License](#license) - [Copyright](#copyright) ## Prerequisites ### Requirements Alpaca requires Python 3.8 or higher and the packages specified in [requirements.txt](requirements/requirements.txt). ## Installation Use the package manager [pip](https://pip.pypa.io/en/stable/) to install Alpaca. Package on [pypi](https://pypi.org/) ```bash pip install alpaca-prov ``` More detailed instructions on how to setup conda environments and additional install options can be checked in the [Installation](doc/install.rst) page. ## Documentation See [https://alpaca-prov.readthedocs.io/en/latest/](https://alpaca-prov.readthedocs.io/en/latest/). ## How to run Examples showing how to use Alpaca can be found in the [examples](examples/) folder. Detailed instructions on how to set up and run are [here](doc/examples.rst). ## Colaborators All the contributors to the development of Alpaca can be found in the [Authors and contributors](doc/authors.rst) page. ## How to contribute If you want to suggest new features, changes, or make a contribution, please first open an issue on the [project page on GitHub](https://github.com/INM-6/alpaca/issues) to discuss your idea. Pull requests are welcome. Any contribution should also be covered by appropriate unit tests in the [tests](alpaca/test) folder. ## Get support If you experience any issue or wish to report a bug, please open an issue on the [project page on GitHub](https://github.com/INM-6/alpaca/issues). ## Acknowledgments See [acknowledgments](doc/acknowledgments.rst). ## License BSD 3-Clause License, see [LICENSE.txt](LICENSE.txt) for details. ## Copyright :copyright: 2022-2023, Forschungszentrum Jülich GmbH, INM-6, IAS-6. All rights reserved.\n",
                "dependencies": "import os from setuptools import setup with open(os.path.join(os.path.dirname(__file__), \"alpaca\", \"VERSION\")) as version_file: version = version_file.read().strip() with open(\"README.md\") as f: long_description = f.read() with open('requirements/requirements.txt') as fp: install_requires = fp.read() setup( name=\"alpaca-prov\", version=version, packages=['alpaca', 'alpaca.utils', 'alpaca.serialization', 'alpaca.ontology', 'alpaca.code_analysis'], include_package_data=True, install_requires=install_requires, author=\"Alpaca authors and contributors\", author_email=\"\", description=\"Alpaca is a Python package for the capture of provenance \" \"information during the execution of Python scripts that \" \"process data.\", long_description=long_description, long_description_content_type=\"text/markdown\", license=\"BSD\", url='https://github.com/INM-6/alpaca', classifiers=[ 'Development Status :: 4 - Beta', 'Intended Audience :: Science/Research', 'License :: OSI Approved :: BSD License', 'Natural Language :: English', 'Operating System :: OS Independent', 'Programming Language :: Python :: 3', 'Topic :: Scientific/Engineering'] )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/alpaka",
            "repo_link": "https://github.com/alpaka-group/alpaka",
            "content": {
                "codemeta": "",
                "readme": "**alpaka** - Abstraction Library for Parallel Kernel Acceleration ================================================================= [![Continuous Integration](https://github.com/alpaka-group/alpaka/workflows/Continuous%20Integration/badge.svg)](https://github.com/alpaka-group/alpaka/actions?query=workflow%3A%22Continuous+Integration%22) [![Documentation Status](https://readthedocs.org/projects/alpaka/badge/?version=latest)](https://alpaka.readthedocs.io) [![Doxygen](https://img.shields.io/badge/API-Doxygen-blue.svg)](https://alpaka-group.github.io/alpaka) [![Language](https://img.shields.io/badge/language-C%2B%2B17-orange.svg)](https://isocpp.org/) [![Platforms](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20mac-lightgrey.svg)](https://github.com/alpaka-group/alpaka) [![License](https://img.shields.io/badge/license-MPL--2.0-blue.svg)](https://www.mozilla.org/en-US/MPL/2.0/) ![alpaka](docs/logo/alpaka_401x135.png) The **alpaka** library is a header-only C++20 abstraction library for accelerator development. Its aim is to provide performance portability across accelerators through the abstraction (not hiding!) of the underlying levels of parallelism. It is platform independent and supports the concurrent and cooperative use of multiple devices such as the hosts CPU (x86, ARM, RISC-V and Power 8+) and GPU accelerators from different vendors (NVIDIA, AMD and Intel). A multitude of accelerator back-end variants using CUDA, HIP, SYCL, OpenMP 2.0+, std::thread and also serial execution is provided and can be selected depending on the device. Only one implementation of the user kernel is required by representing them as function objects with a special interface. There is no need to write special CUDA, HIP, SYCL, OpenMP or custom threading code. Accelerator back-ends can be mixed and synchronized via compute device queue. The decision which accelerator back-end executes which kernel can be made at runtime. The abstraction used is very similar to the CUDA grid-blocks-threads domain decomposition strategy. Algorithms that should be parallelized have to be divided into a multi-dimensional grid consisting of small uniform work items. These functions are called kernels and are executed in parallel threads. The threads in the grid are organized in blocks. All threads in a block are executed in parallel and can interact via fast shared memory and low level synchronization methods. Blocks are executed independently and can not interact in any way. The block execution order is unspecified and depends on the accelerator in use. By using this abstraction the execution can be optimally adapted to the available hardware. Software License ---------------- **alpaka** is licensed under **MPL-2.0**. Documentation ------------- The alpaka documentation can be found in the [online manual](https://alpaka.readthedocs.io). The documentation files in [`.rst` (reStructuredText)](https://www.sphinx-doc.org/en/stable/rest.html) format are located in the `docs` subfolder of this repository. The [source code documentation](https://alpaka-group.github.io/alpaka/) is generated with [doxygen](http://www.doxygen.org). Accelerator Back-ends --------------------- | Accelerator Back-end | Lib/API | Devices | Execution strategy grid-blocks | Execution strategy block-threads | |------------------------|---------------------------------------------------------|----------------------------|------------------------------------|--------------------------------------| | Serial | n/a | Host CPU (single core) | sequential | sequential (only 1 thread per block) | | OpenMP 2.0+ blocks | OpenMP 2.0+ | Host CPU (multi core) | parallel (preemptive multitasking) | sequential (only 1 thread per block) | | OpenMP 2.0+ threads | OpenMP 2.0+ | Host CPU (multi core) | sequential | parallel (preemptive multitasking) | | std::thread | std::thread | Host CPU (multi core) | sequential | parallel (preemptive multitasking) | | TBB | TBB 2.2+ | Host CPU (multi core) | parallel (preemptive multitasking) | sequential (only 1 thread per block) | | CUDA | CUDA 12.0+ | NVIDIA GPUs | parallel (undefined) | parallel (lock-step within warps) | | HIP(clang) | [HIP 6.0+](https://github.com/ROCm-Developer-Tools/HIP) | AMD GPUs | parallel (undefined) | parallel (lock-step within warps) | | SYCL(oneAPI) | oneAPI 2024.2+ | CPUs, Intel GPUs and FPGAs | parallel (undefined) | parallel (lock-step within warps) | Supported Compilers ------------------- This library uses C++20 (or newer when available). | Accelerator Back-end | gcc 11.1 (Linux) | gcc 12.3 (Linux) | gcc 13.1 (Linux) | clang 14 (Linux) | clang 15 (Linux) | clang 16 (Linux) | clang 17 (Linux) | clang 18 (Linux) | clang 19 (Linux) | icpx 2025.0 (Linux) | Xcode 15.4 / 16.1 (macOS) | Visual Studio 2022 (Windows) | |----------------------|--------------------------------|---------------------------------------|---------------------------------------|--------------------------------|--------------------------------|--------------------------------|---------------------------------------|---------------------------------------|--------------------|-------------------------|---------------------------|------------------------------| | Serial | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | | OpenMP 2.0+ blocks | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: [^1] | :white_check_mark: | :white_check_mark: | | OpenMP 2.0+ threads | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: [^1] | :white_check_mark: | :white_check_mark: | | std::thread | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | | TBB | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | | CUDA (nvcc) | :white_check_mark: (CUDA 12.0) | :white_check_mark: (CUDA 12.0 - 12.5) | :white_check_mark: (CUDA 12.4 - 12.5) | :white_check_mark: (CUDA 12.0) | :white_check_mark: (CUDA 12.2) | :white_check_mark: (CUDA 12.3) | :white_check_mark: (CUDA 12.4 - 12.5) | :white_check_mark: (CUDA 12.4 - 12.5) | :x: | :x: | - | :x: | | CUDA (clang) | - | - | - | :x: | :x: | :x: | :x: | :x: | :x: | :x: | - | - | | HIP (clang) | - | - | - | :x: | :x: | :x: | :white_check_mark: (HIP 6.0 - 6.1) | :white_check_mark: (HIP 6.2) | :x: | :x: | - | - | | SYCL | :x: | :x: | :x: | :x: | :x: | :x: | :x: | :x: | :x: | :white_check_mark: [^2] | - | :x: | Other compilers or combinations marked with :x: in the table above may work but are not tested in CI and are therefore not explicitly supported. [^1]: Due to an [LLVM bug](https://github.com/llvm/llvm-project/issues/58491) in debug mode only release builds are supported. [^2]: Currently, the unit tests are compiled but not executed. Dependencies ------------ [Boost](https://boost.org/) 1.74.0+ is the only mandatory external dependency. The **alpaka** library itself just requires header-only libraries. However some of the accelerator back-end implementations require different boost libraries to be built. When an accelerator back-end using *CUDA* is enabled, version *11.2* (with nvcc as CUDA compiler) or version *11.2* (with clang as CUDA compiler) of the *CUDA SDK* is the minimum requirement. *NOTE*: When using clang as a native *CUDA* compiler, the *CUDA accelerator back-end* can not be enabled together with any *OpenMP accelerator back-end* because this combination is currently unsupported. *NOTE*: Separable compilation is disabled by default and can be enabled via the CMake flag `CMAKE_CUDA_SEPARABLE_COMPILATION`. When an accelerator back-end using *OpenMP* is enabled, the compiler and the platform have to support the corresponding minimum *OpenMP* version. When an accelerator back-end using *TBB* is enabled, the compiler and the platform have to support the corresponding minimum *TBB* version. Usage ----- The library is header only so nothing has to be built. CMake 3.22+ is required to provide the correct defines and include paths. Just call `alpaka_add_executable` instead of `add_executable` and the difficulties of the CUDA nvcc compiler in handling `.cu` and `.cpp` files are automatically taken care of. Source files do not need any special file ending. Examples of how to utilize alpaka within CMake can be found in the `example` folder. The whole alpaka library can be included with: `#include <alpaka/alpaka.hpp>` Code that is not intended to be utilized by the user is hidden in the `detail` namespace. Furthermore, for a CUDA-like experience when adopting alpaka we provide the library [*cupla*](https://github.com/alpaka-group/cupla). It enables a simple and straightforward way of porting existing CUDA applications to alpaka and thus to a variety of accelerators. ### Single header The CI creates a single-header version of alpaka on each commit, which you can find on the [single-header branch](https://github.com/alpaka-group/alpaka/tree/single-header). This is especially useful, if you would like to play with alpaka on [Compiler explorer](https://godbolt.org/z/hzPnhnna9). Just include alpaka like ```c++ #include <https://raw.githubusercontent.com/alpaka-group/alpaka/single-header/include/alpaka/alpaka.hpp> ``` and enable the desired backend on the compiler's command line using the corresponding macro, e.g. via `-DALPAKA_ACC_CPU_B_SEQ_T_SEQ_ENABLED`. Introduction ------------ For a quick introduction, feel free to playback the recording of our presentation at [GTC 2016](https://www.nvidia.com/gtc/): - E. Zenker, R. Widera, G. Juckeland et al., *Porting the Plasma Simulation PIConGPU to Heterogeneous Architectures with Alpaka*, [video link (39 min)](http://on-demand.gputechconf.com/gtc/2016/video/S6298.html), [slides (PDF)](https://on-demand.gputechconf.com/gtc/2016/presentation/s6298-erik-zenker-porting-the-plasma.pdf), [DOI:10.5281/zenodo.6336086](https://doi.org/10.5281/zenodo.6336086) Citing alpaka ------------- Currently all authors of **alpaka** are scientists or connected with research. For us to justify the importance and impact of our work, please consider citing us accordingly in your derived work and publications: ```latex % Peer-Reviewed Publication %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % % Peer reviewed and accepted publication in % \"2nd International Workshop on Performance Portable % Programming Models for Accelerators (P^3MA)\" % colocated with the % \"2017 ISC High Performance Conference\" % in Frankfurt, Germany @inproceedings{MathesP3MA2017, author = {{Matthes}, A. and {Widera}, R. and {Zenker}, E. and {Worpitz}, B. and {Huebl}, A. and {Bussmann}, M.}, title = {Tuning and optimization for a variety of many-core architectures without changing a single line of implementation code using the Alpaka library}, archivePrefix = \"arXiv\", eprint = {1706.10086}, keywords = {Computer Science - Distributed, Parallel, and Cluster Computing}, day = {30}, month = {Jun}, year = {2017}, url = {https://arxiv.org/abs/1706.10086}, } % Peer-Reviewed Publication %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % % Peer reviewed and accepted publication in % \"The Sixth International Workshop on % Accelerators and Hybrid Exascale Systems (AsHES)\" % at the % \"30th IEEE International Parallel and Distributed % Processing Symposium\" in Chicago, IL, USA @inproceedings{ZenkerAsHES2016, author = {Erik Zenker and Benjamin Worpitz and Ren{\\'{e}} Widera and Axel Huebl and Guido Juckeland and Andreas Kn{\\\"{u}}pfer and Wolfgang E. Nagel and Michael Bussmann}, title = {Alpaka - An Abstraction Library for Parallel Kernel Acceleration}, archivePrefix = \"arXiv\", eprint = {1602.08477}, keywords = {Computer science;CUDA;Mathematical Software;nVidia;OpenMP;Package; performance portability;Portability;Tesla K20;Tesla K80}, day = {23}, month = {May}, year = {2016}, publisher = {IEEE Computer Society}, url = {http://arxiv.org/abs/1602.08477}, } % Original Work: Benjamin Worpitz' Master Thesis %%%%%%%%%% % @MasterThesis{Worpitz2015, author = {Benjamin Worpitz}, title = {Investigating performance portability of a highly scalable particle-in-cell simulation code on various multi-core architectures}, school = {{Technische Universit{\\\"{a}}t Dresden}}, month = {Sep}, year = {2015}, type = {Master Thesis}, doi = {10.5281/zenodo.49768}, url = {http://dx.doi.org/10.5281/zenodo.49768} } ``` Contributing ------------ Rules for contributions can be found in [CONTRIBUTING.md](CONTRIBUTING.md). Any pull request will be reviewed by a [maintainer](https://github.com/orgs/alpaka-group/teams/alpaka-maintainers). Thanks to all [active and former contributors](.zenodo.json).\n",
                "dependencies": "# # Copyright 2023 Benjamin Worpitz, Jan Stephan, Bernhard Manfred Gruber # SPDX-License-Identifier: MPL-2.0 # ################################################################################ # Required CMake version cmake_minimum_required(VERSION 3.25) cmake_policy(SET CMP0091 NEW) include(CMakePrintHelpers) #------------------------------------------------------------------------------- # Find alpaka version. file(STRINGS \"${CMAKE_CURRENT_LIST_DIR}/include/alpaka/version.hpp\" alpaka_VERSION_MAJOR_HPP REGEX \"#define ALPAKA_VERSION_MAJOR \") file(STRINGS \"${CMAKE_CURRENT_LIST_DIR}/include/alpaka/version.hpp\" alpaka_VERSION_MINOR_HPP REGEX \"#define ALPAKA_VERSION_MINOR \") file(STRINGS \"${CMAKE_CURRENT_LIST_DIR}/include/alpaka/version.hpp\" alpaka_VERSION_PATCH_HPP REGEX \"#define ALPAKA_VERSION_PATCH \") string(REGEX MATCH \"([0-9]+)\" alpaka_VERSION_MAJOR ${alpaka_VERSION_MAJOR_HPP}) string(REGEX MATCH \"([0-9]+)\" alpaka_VERSION_MINOR ${alpaka_VERSION_MINOR_HPP}) string(REGEX MATCH \"([0-9]+)\" alpaka_VERSION_PATCH ${alpaka_VERSION_PATCH_HPP}) set(PACKAGE_VERSION \"${alpaka_VERSION_MAJOR}.${alpaka_VERSION_MINOR}.${alpaka_VERSION_PATCH}\") project(alpaka VERSION ${alpaka_VERSION_MAJOR}.${alpaka_VERSION_MINOR}.${alpaka_VERSION_PATCH} DESCRIPTION \"The alpaka library is a header-only C++20 abstraction library for accelerator development.\" HOMEPAGE_URL \"https://github.com/alpaka-group/alpaka\" LANGUAGES CXX) set_property(GLOBAL PROPERTY USE_FOLDERS ON) ################################################################################ # Options and Variants option(alpaka_BUILD_EXAMPLES \"Build the examples\" OFF) option(alpaka_BUILD_BENCHMARKS \"Build the benchmarks\" OFF) # Enable the test infrastructure only if alpaka is the top-level project if(CMAKE_PROJECT_NAME STREQUAL PROJECT_NAME) option(alpaka_ENABLE_WERROR \"Treat all warnings as errors.\" OFF) option(BUILD_TESTING \"Build the testing tree.\" OFF) include(CTest) endif() option(alpaka_INSTALL_TEST_HEADER \"Install headers of the namespace alpaka::test. Attention, headers are not designed for production code, see documentation.\" OFF) include(CMakeDependentOption) cmake_dependent_option(alpaka_CHECK_HEADERS \"Check all alpaka headers as part of the tests whether they can be compiled standalone.\" OFF BUILD_TESTING OFF) cmake_dependent_option(alpaka_USE_INTERNAL_CATCH2 \"Use internally shipped Catch2\" ON \"BUILD_TESTING OR alpaka_BUILD_BENCHMARKS\" OFF) ################################################################################ # Internal variables. # Set found to true initially and set it to false if a required dependency is missing. set(_alpaka_FOUND TRUE) # This file's directory. set(_alpaka_ROOT_DIR ${CMAKE_CURRENT_LIST_DIR}) # Normalize the path (e.g. remove ../) get_filename_component(_alpaka_ROOT_DIR ${_alpaka_ROOT_DIR} ABSOLUTE) # Compiler feature tests. set(_alpaka_FEATURE_TESTS_DIR \"${_alpaka_ROOT_DIR}/cmake/tests\") # Add common functions. set(_alpaka_COMMON_FILE \"${_alpaka_ROOT_DIR}/cmake/common.cmake\") include(${_alpaka_COMMON_FILE}) # Add alpaka_ADD_EXECUTABLE function. set(_alpaka_ADD_EXECUTABLE_FILE \"${_alpaka_ROOT_DIR}/cmake/addExecutable.cmake\") include(${_alpaka_ADD_EXECUTABLE_FILE}) # Add alpaka_ADD_LIBRARY function. set(_alpaka_ADD_LIBRARY_FILE \"${_alpaka_ROOT_DIR}/cmake/addLibrary.cmake\") include(${_alpaka_ADD_LIBRARY_FILE}) # Set include directories set(_alpaka_INCLUDE_DIRECTORY \"${_alpaka_ROOT_DIR}/include\") set(_alpaka_SUFFIXED_INCLUDE_DIR \"${_alpaka_INCLUDE_DIRECTORY}/alpaka\") # the sequential accelerator is required for the tests and examples if(alpaka_BUILD_EXAMPLES OR alpaka_BUILD_BENCHMARKS OR BUILD_TESTING) if (NOT (alpaka_ACC_GPU_CUDA_ONLY_MODE OR alpaka_ACC_GPU_HIP_ONLY_MODE)) if (NOT DEFINED alpaka_ACC_CPU_B_SEQ_T_SEQ_ENABLE) option(alpaka_ACC_CPU_B_SEQ_T_SEQ_ENABLE \"enable alpaka serial accelerator\" ON) elseif(NOT alpaka_ACC_CPU_B_SEQ_T_SEQ_ENABLE) set(alpaka_ACC_CPU_B_SEQ_T_SEQ_ENABLE ON CACHE BOOL \"enable alpaka serial accelerator\" FORCE) endif() else() #CUDA or HIP-only mode message(WARNING \"CUDA/HIP-only mode enabled: not enabling alpaka serial accelerator (required for examples and tests)\") endif() endif() include(${_alpaka_ROOT_DIR}/cmake/alpakaCommon.cmake) # Add all the source and include files in all recursive subdirectories and group them accordingly. append_recursive_files_add_to_src_group(\"${_alpaka_SUFFIXED_INCLUDE_DIR}\" \"${_alpaka_SUFFIXED_INCLUDE_DIR}\" \"hpp\" _alpaka_FILES_HEADER) append_recursive_files_add_to_src_group(\"${_alpaka_SUFFIXED_INCLUDE_DIR}\" \"${_alpaka_SUFFIXED_INCLUDE_DIR}\" \"h\" _alpaka_FILES_HEADER) # remove headers of the folder alpaka/test, if alpaka_INSTALL_TEST_HEADER is disabled if(NOT alpaka_INSTALL_TEST_HEADER) list(FILTER _alpaka_FILES_HEADER EXCLUDE REGEX \"(.*)/alpaka/test/(.*)\") endif() append_recursive_files_add_to_src_group(\"${_alpaka_ROOT_DIR}/script\" \"${_alpaka_ROOT_DIR}\" \"sh\" _alpaka_FILES_SCRIPT) set_source_files_properties(${_alpaka_FILES_SCRIPT} PROPERTIES HEADER_FILE_ONLY TRUE) append_recursive_files_add_to_src_group(\"${_alpaka_ROOT_DIR}/cmake\" \"${_alpaka_ROOT_DIR}\" \"cmake\" _alpaka_FILES_CMAKE) list(APPEND _alpaka_FILES_CMAKE \"${_alpaka_ROOT_DIR}/cmake/alpakaConfig.cmake.in\" \"${_alpaka_ROOT_DIR}/CMakeLists.txt\") set_source_files_properties(${_alpaka_FILES_CMAKE} PROPERTIES HEADER_FILE_ONLY TRUE) append_recursive_files_add_to_src_group(\"${_alpaka_ROOT_DIR}/docs/markdown\" \"${_alpaka_ROOT_DIR}\" \"md\" _alpaka_FILES_DOC) set_source_files_properties(${_alpaka_FILES_DOC} PROPERTIES HEADER_FILE_ONLY TRUE) append_recursive_files_add_to_src_group(\"${_alpaka_ROOT_DIR}/.github\" \"${_alpaka_ROOT_DIR}\" \"yml\" _alpaka_FILES_OTHER) list(APPEND _alpaka_FILES_OTHER \"${_alpaka_ROOT_DIR}/.clang-format\" \"${_alpaka_ROOT_DIR}/.gitignore\" \"${_alpaka_ROOT_DIR}/.zenodo.json\" \"${_alpaka_ROOT_DIR}/LICENSE\" \"${_alpaka_ROOT_DIR}/README.md\") set_source_files_properties(${_alpaka_FILES_OTHER} PROPERTIES HEADER_FILE_ONLY TRUE) if(TARGET alpaka) # HACK: Workaround for the limitation that files added to INTERFACE targets (target_sources) can not be marked as PUBLIC or PRIVATE but only as INTERFACE. # Therefore those files will be added to projects \"linking\" to the INTERFACE library, but are not added to the project itself within an IDE. add_custom_target(\"alpakaIde\" SOURCES ${_alpaka_FILES_HEADER} ${_alpaka_FILES_SCRIPT} ${_alpaka_FILES_CMAKE} ${_alpaka_FILES_DOC} ${_alpaka_FILES_OTHER}) endif() ################################################################################ # Export NVCC/HIPCC flags to parent scope if alpaka is used as a CMake # subdirectory. # # These flags are set in cmake/alpakaCommon.cmake but are visible in this scope # since alpakaCommon.cmake is included. if(NOT CMAKE_PROJECT_NAME STREQUAL PROJECT_NAME) if(alpaka_ACC_GPU_HIP_ENABLE) # export HIPCC flags to parent scope in case alpaka is another project's subdirectory set(HIP_HIPCC_FLAGS ${HIP_HIPCC_FLAGS} PARENT_SCOPE) set(HIP_NVCC_FLAGS ${HIP_NVCC_FLAGS} PARENT_SCOPE) set(HIP_VERBOSE_BUILD ${HIP_VERBOSE_BUILD} PARENT_SCOPE) endif() endif() ################################################################################ # Add subdirectories add_subdirectory(thirdParty) if(alpaka_BUILD_EXAMPLES) add_subdirectory(\"example/\") endif() if(alpaka_BUILD_BENCHMARKS) add_subdirectory(\"benchmarks/\") endif() # Only build the tests if alpaka is the top-level project and BUILD_TESTING is ON if(CMAKE_PROJECT_NAME STREQUAL PROJECT_NAME AND BUILD_TESTING) add_subdirectory(\"test/\") endif() ################################################################################ # Installation. include(CMakePackageConfigHelpers) include(GNUInstallDirs) set(_alpaka_INSTALL_CMAKEDIR \"${CMAKE_INSTALL_LIBDIR}/cmake/alpaka\") install(TARGETS alpaka ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}) write_basic_package_version_file( \"alpakaConfigVersion.cmake\" VERSION ${PROJECT_VERSION} COMPATIBILITY SameMajorVersion) configure_package_config_file( \"${_alpaka_ROOT_DIR}/cmake/alpakaConfig.cmake.in\" \"${PROJECT_BINARY_DIR}/alpakaConfig.cmake\" INSTALL_DESTINATION \"${_alpaka_INSTALL_CMAKEDIR}\") install(FILES \"${PROJECT_BINARY_DIR}/alpakaConfig.cmake\" \"${PROJECT_BINARY_DIR}/alpakaConfigVersion.cmake\" DESTINATION \"${_alpaka_INSTALL_CMAKEDIR}\") if(alpaka_INSTALL_TEST_HEADER) install(DIRECTORY \"${_alpaka_SUFFIXED_INCLUDE_DIR}\" DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}\") else() install(DIRECTORY \"${_alpaka_SUFFIXED_INCLUDE_DIR}\" DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}\" PATTERN \"test\" EXCLUDE) endif() install(FILES \"${_alpaka_ROOT_DIR}/cmake/addExecutable.cmake\" \"${_alpaka_ROOT_DIR}/cmake/addLibrary.cmake\" \"${_alpaka_ROOT_DIR}/cmake/alpakaCommon.cmake\" \"${_alpaka_ROOT_DIR}/cmake/common.cmake\" DESTINATION \"${_alpaka_INSTALL_CMAKEDIR}\") install(DIRECTORY \"${_alpaka_ROOT_DIR}/cmake/tests\" DESTINATION \"${_alpaka_INSTALL_CMAKEDIR}\") install(TARGETS alpaka EXPORT alpakaTargets) install(EXPORT alpakaTargets DESTINATION \"${_alpaka_INSTALL_CMAKEDIR}\" NAMESPACE alpaka::)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/amiris",
            "repo_link": "https://gitlab.com/dlr-ve/esy/amiris/amiris",
            "content": {
                "codemeta": "",
                "readme": "<!-- SPDX-FileCopyrightText: 2025 German Aerospace Center <amiris@dlr.de> SPDX-License-Identifier: Apache-2.0 --> ![AMIRIS_Logo](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/wikis/uploads/AMIRIS_LogoWTitle.png) ## _Simulate electricity markets emerging from interactions of producers, consumers, and flexibilities_ [![Pipeline status](https://gitlab.com/dlr-ve/esy/amiris/amiris/badges/main/pipeline.svg)](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/commits/main) [![REUSE status](https://api.reuse.software/badge/gitlab.com/dlr-ve/esy/amiris/amiris/)](https://api.reuse.software/info/gitlab.com/dlr-ve/esy/amiris/amiris/) [![JOSS paper](https://joss.theoj.org/papers/10.21105/joss.05041/status.svg)](https://joss.theoj.org/papers/10.21105/joss.05041) [![Zenodo](https://img.shields.io/badge/Research-Zenodo-blue)](https://zenodo.org/communities/amiris) [![Last Commit](https://img.shields.io/gitlab/last-commit/dlr-ve/esy/amiris/amiris)](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/commits/dev) AMIRIS is an agent-based simulation model of electricity markets and their actors. Check out its [full documentation](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/wikis/home). **[Get started right away](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/wikis/Get-Started)**! ## What is AMIRIS? AMIRIS enables you to assess the interplay between actors, market designs, and policy instruments for diverse electricity market scenarios. You can freely select and combine scopes in AMIRIS due to its versatile implementation of agents. For example, you may model large-scale transnational energy systems and small-scale decisions of individual households at the same time. Whatever you do: AMIRIS will deliver results quickly thanks to the powerful framework [FAME](https://gitlab.com/fame-framework/wiki/-/wikis/home). The typical **runtime is less than a minute** when simulating a market zone in hourly resolution for one year. The figure below illustrates the available prototypical agents as well as associated flows of information, energy, and money. ![AMIRIS Model Schema](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/wikis/uploads/AMIRIS_ModellSchema_CCBY4.png) ## Who is AMIRIS for? AMIRIS is intended for informed users in the energy domain, such as researchers, companies, and students. Although not strictly necessary, [Basic Knowledge of Electricity Markets](https://en.wikipedia.org/wiki/Electricity_market#Wholesale_electricity_market) is helpful. ## Applications The development of AMIRIS started in 2008. Since then, AMIRIS has seen many different applications. Selected recent applications: - In [TradeRES](https://traderes.eu/) AMIRIS was used to analyse cost recovery of renewables in ~100% renewable electricity systems under different **Energy Policy Instruments**. [Paper](https://doi.org/10.1109/EEM60825.2024.10608886) - Intertwined dynamics between **Energy Community Markets** and national electricity markets were analysed with AMIRIS. [Paper](https://doi.org/10.1016/j.egyr.2024.06.052) - Interactions of **Household Flexibility** (heat pumps, electric vehicles, and electricity storage units) with the German energy system were modelled with AMIRIS. [Paper](https://elib.dlr.de/207802) - In project VERMEER **Impacts of Cold Dunkelflaute** events on the European electricity system were investigated using **Market Coupling** in AMIRIS. [Paper](https://zenodo.org/records/10561382) [Report](https://elib.dlr.de/196641/) - AMIRIS was used to assess the **Economic Potential of Large Flexibility Providers** in future electricity market scenarios. [Paper](https://doi.org/10.1016/j.est.2024.110959) - **Monetary Saving Potentials of Load Shifting** were analysed using AMIRIS. [Dissertation](https://depositonce.tu-berlin.de/items/4a364bac-9e97-4d35-8eb6-645824cfc02d) - A wide range of scenarios for **Future Electricity Markets** was explored using an AMIRIS [scenario generator](https://doi.org/10.5281/zenodo.8382789). [Paper](https://doi.org/10.1016/j.egyr.2024.11.013) ## Non-Applications AMIRIS follows an explorative approach: It is well suited to investigate the effects arising from the interactions of energy system actors under a given set of assumptions. While individual actors can optimise their decisions, AMIRIS **does not optimise the energy system** as a whole. Further, AMIRIS **does not enforce system-wide constraints** like a carbon emission cap. Thus, we recommend using optimisation-based tools like [REMix](https://gitlab.com/dlr-ve/esy/remix/framework), [PyPSA](https://pypsa.org/) or [oemof](https://oemof.org/) to answer questions like \"What is the cheapest electricity system given a carbon emission cap of X?\". ## Community AMIRIS is mainly developed by the German Aerospace Center, Institute of Networked Energy Systems. We provide multi-level support for AMIRIS users as listed on our dedicated [Support Page](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/wikis/Community/Support). **We welcome all contributions**: bug reports, feature requests, documentation enhancements, and code. Please see our [Contribution Guidelines](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/wikis/Community/Contribute). ## Citing AMIRIS If you use AMIRIS in an academic context please cite [doi: 10.21105/joss.05041](https://doi.org/10.21105/joss.05041). In other contexts, please include a link to our [Gitlab repository](https://gitlab.com/dlr-ve/esy/amiris/amiris). ``` @article{schimeczek2023, title = {{AMIRIS}: {Agent}-based {Market} model for the {Investigation} of {Renewable} and {Integrated} energy {Systems}}, volume = {8}, doi = {10.21105/joss.05041}, number = {84}, journal = {Journal of Open Source Software}, author = {Schimeczek, Christoph and Nienhaus, Kristina and Frey, Ulrich and Sperber, Evelyn and Sarfarazi, Seyedfarzad and Nitsch, Felix and Kochems, Johannes and Ghazi, A. Achraf El}, year = {2023}, pages = {5041}, } ``` ## Acknowledgements The development of AMIRIS was funded by the German Aerospace Center, the German Federal Ministry for Economic Affairs and Climate Action, the German Federal Ministry of Education and Research, and the German Federal Ministry for the Environment, Nature Conservation and Nuclear Safety. It has also received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 864276. An extended list of third-party funded research projects can be found on the [AMIRIS Home Page](https://helmholtz.software/software/amiris). We express our gratitude to all [contributors](CONTRIBUTING.md#list-of-contributors). ## What next? * [Install and run AMIRIS](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/wikis/Get-Started) * [Read previous publications & material](https://zenodo.org/communities/amiris) * [Ask questions](https://forum.openmod.org/tag/amiris) * [Contribute an issue or code](https://gitlab.com/dlr-ve/esy/amiris/amiris/-/wikis/Community/Contribute)\n",
                "dependencies": "<!-- SPDX-FileCopyrightText: 2023 German Aerospace Center <amiris@dlr.de> SPDX-License-Identifier: Apache-2.0 --> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> \t<modelVersion>4.0.0</modelVersion> \t<groupId>de.dlr.gitlab.amiris</groupId> \t<artifactId>amiris-core</artifactId> \t<version>3.5.0-alpha1</version> \t<dependencies> \t\t<dependency> \t\t\t<groupId>de.dlr.gitlab.fame</groupId> \t\t\t<artifactId>core</artifactId> \t\t\t<version>2.0.3</version> \t\t</dependency> \t\t<dependency> \t\t\t<groupId>org.apache.commons</groupId> \t\t\t<artifactId>commons-math3</artifactId> \t\t\t<version>3.6.1</version> \t\t</dependency> \t\t<dependency> \t\t\t<groupId>org.slf4j</groupId> \t\t\t<artifactId>slf4j-log4j12</artifactId> \t\t\t<version>1.7.36</version> \t\t\t<scope>runtime</scope> \t\t</dependency> \t\t<dependency> \t\t\t<groupId>org.apache.maven.plugins</groupId> \t\t\t<artifactId>maven-assembly-plugin</artifactId> \t\t\t<version>3.7.1</version> \t\t\t<type>maven-plugin</type> \t\t</dependency> \t\t<dependency> \t\t\t<groupId>org.junit.jupiter</groupId> \t\t\t<artifactId>junit-jupiter-api</artifactId> \t\t\t<version>5.12.2</version> \t\t\t<scope>test</scope> \t\t</dependency> \t\t<dependency> \t\t\t<groupId>org.junit.jupiter</groupId> \t\t\t<artifactId>junit-jupiter-params</artifactId> \t\t\t<version>5.12.2</version> \t\t\t<scope>test</scope> \t\t</dependency> \t\t<dependency> \t\t\t<groupId>org.mockito</groupId> \t\t\t<artifactId>mockito-core</artifactId> \t\t\t<version>5.16.0</version> \t\t\t<scope>test</scope> \t\t</dependency> \t\t<dependency> \t\t\t<groupId>org.hamcrest</groupId> \t\t\t<artifactId>hamcrest</artifactId> \t\t\t<version>2.2</version> \t\t\t<scope>test</scope> \t\t</dependency> <dependency> <groupId>io.github.hakky54</groupId> <artifactId>logcaptor</artifactId> <version>2.7.11</version> <scope>test</scope> </dependency> \t\t<!-- Begin: Required for util.UrlModelService --> \t\t<dependency> \t\t\t<groupId>org.json</groupId> \t\t\t<artifactId>json</artifactId> \t\t\t<version>20240303</version> \t\t</dependency> \t\t<dependency> \t\t\t<groupId>com.jayway.jsonpath</groupId> \t\t\t<artifactId>json-path</artifactId> \t\t\t<version>2.9.0</version> \t\t</dependency> \t\t<dependency> \t\t\t<groupId>com.fasterxml.jackson.core</groupId> \t\t\t<artifactId>jackson-databind</artifactId> \t\t\t<version>2.17.1</version> \t\t</dependency> \t\t<!-- End: Required for util.UrlModelService --> \t</dependencies> \t<build> \t\t<sourceDirectory>src/main/java/</sourceDirectory> \t\t<finalName>amiris</finalName> \t\t<plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>3.5.2</version> <configuration> <classpathDependencyExcludes> <classpathDependencyExclude>org.slf4j:slf4j-reload4j</classpathDependencyExclude> </classpathDependencyExcludes> </configuration> </plugin> <plugin> \t\t\t\t<groupId>org.codehaus.mojo</groupId> \t\t\t\t<artifactId>license-maven-plugin</artifactId> \t\t\t\t<version>2.4.0</version> \t\t\t\t<executions> \t\t\t\t\t<execution> \t\t\t\t\t\t<id>download-licenses</id> \t\t\t\t\t\t<goals> \t\t\t\t\t\t\t<goal>download-licenses</goal> \t\t\t\t\t\t</goals> \t\t\t\t\t\t<configuration> \t\t\t\t\t\t\t<excludedScopes>test</excludedScopes> \t\t\t\t\t\t\t<licensesOutputDirectory>${project.build.directory}/generated-resources/licenses</licensesOutputDirectory> \t\t\t\t\t\t\t<licensesOutputFile>${project.build.directory}/generated-resources/licenses.xml</licensesOutputFile> \t\t\t\t\t\t\t<cleanLicensesOutputDirectory>true</cleanLicensesOutputDirectory> \t\t\t\t\t\t</configuration> \t\t\t\t\t</execution> \t\t\t\t</executions> \t\t\t</plugin> \t\t\t<plugin> \t\t\t\t<groupId>org.apache.maven.plugins</groupId> \t\t\t\t<artifactId>maven-assembly-plugin</artifactId> \t\t\t\t<version>3.7.1</version> \t\t\t\t<configuration> \t\t\t\t <descriptors> \t\t\t\t <descriptor>src/assembly/full-jar-with-licenses.xml</descriptor> \t\t\t\t </descriptors> \t\t\t\t\t<finalName>${project.artifactId}_${project.version}</finalName> \t\t\t\t\t<archive> \t\t\t\t\t\t<manifest> \t\t\t\t\t\t\t<mainClass>de.dlr.gitlab.fame.setup.FameRunner</mainClass> \t\t\t\t\t\t</manifest> \t\t\t\t\t</archive> \t\t\t\t</configuration> \t\t\t\t<executions> \t\t\t\t\t<execution> \t\t\t\t\t\t<id>make-assembly</id> \t\t\t\t\t\t<phase>package</phase> \t\t\t\t\t\t<goals> \t\t\t\t\t\t\t<goal>single</goal> \t\t\t\t\t\t</goals> \t\t\t\t\t</execution> \t\t\t\t</executions> \t\t\t</plugin> \t\t\t<plugin> \t\t\t\t<groupId>org.jacoco</groupId> \t\t\t\t<artifactId>jacoco-maven-plugin</artifactId> \t\t\t\t<version>0.8.12</version> \t\t\t\t<executions> \t\t\t\t\t<execution> \t\t\t\t\t\t<id>pre-unit-test</id> \t\t\t\t\t\t<goals> \t\t\t\t\t\t\t<goal>prepare-agent</goal> \t\t\t\t\t\t</goals> \t\t\t\t\t</execution> \t\t\t\t\t<execution> \t\t\t\t\t\t<id>post-unit-test</id> \t\t\t\t\t\t<phase>test</phase> \t\t\t\t\t\t<goals> \t\t\t\t\t\t\t<goal>report</goal> \t\t\t\t\t\t</goals> \t\t\t\t\t</execution> \t\t\t\t</executions> \t\t\t</plugin> \t\t\t<plugin> \t\t\t\t<groupId>org.apache.maven.plugins</groupId> \t\t\t\t<artifactId>maven-compiler-plugin</artifactId> \t\t\t\t<version>3.13.0</version> \t\t\t\t<configuration> \t\t\t\t\t<release>11</release> \t\t\t\t</configuration> \t\t\t</plugin> \t\t\t \t\t\t<plugin> \t\t\t\t<groupId>com.diffplug.spotless</groupId> \t\t\t\t<artifactId>spotless-maven-plugin</artifactId> \t\t\t\t<version>2.44.3</version> \t\t\t\t<configuration> \t\t\t\t\t<java> \t\t\t\t\t <eclipse> \t\t\t\t\t <version>4.26</version> \t\t\t\t\t <file>${project.basedir}/src/test/resources/CodeStyle.xml</file> \t\t\t\t\t </eclipse> \t\t\t\t\t\t<licenseHeader> \t\t\t\t\t\t\t<file>${project.basedir}/src/main/resources/license_header.txt</file> \t\t\t\t\t\t</licenseHeader> \t\t\t\t\t</java> \t\t\t\t</configuration> \t\t\t\t<executions> \t\t\t\t\t<execution> \t\t\t\t\t\t<goals> \t\t\t\t\t\t\t<goal>check</goal> \t\t\t\t\t\t</goals> \t\t\t\t\t</execution> \t\t\t\t</executions> \t\t\t</plugin> \t\t</plugins> \t</build> \t<properties> \t\t<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> \t\t<maven.compiler.release>11</maven.compiler.release> \t</properties> </project>\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/anndata",
            "repo_link": "https://github.com/scverse/anndata",
            "content": {
                "codemeta": "",
                "readme": "[![Tests](https://github.com/scverse/anndata/actions/workflows/test-cpu.yml/badge.svg)](https://github.com/scverse/anndata/actions) [![Conda](https://img.shields.io/conda/vn/conda-forge/anndata.svg)](https://anaconda.org/conda-forge/anndata) [![Coverage](https://codecov.io/gh/scverse/anndata/branch/main/graph/badge.svg?token=IN1mJN1Wi8)](https://codecov.io/gh/scverse/anndata) [![Docs](https://readthedocs.com/projects/icb-anndata/badge/?version=latest)](https://anndata.readthedocs.io) [![PyPI](https://img.shields.io/pypi/v/anndata.svg)](https://pypi.org/project/anndata) [![Downloads](https://static.pepy.tech/badge/anndata/month)](https://pepy.tech/project/anndata) [![Downloads](https://static.pepy.tech/badge/anndata)](https://pepy.tech/project/anndata) [![Stars](https://img.shields.io/github/stars/scverse/anndata?style=flat&logo=github&color=yellow)](https://github.com/scverse/anndata/stargazers) [![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](http://numfocus.org) <img src=\"https://raw.githubusercontent.com/scverse/anndata/main/docs/_static/img/anndata_schema.svg\" class=\"dark-light\" align=\"right\" width=\"350\" alt=\"image\" /> # anndata - Annotated data anndata is a Python package for handling annotated data matrices in memory and on disk, positioned between pandas and xarray. anndata offers a broad range of computationally efficient features including, among others, sparse data support, lazy operations, and a PyTorch interface. - Discuss development on [GitHub](https://github.com/scverse/anndata). - Read the [documentation](https://anndata.readthedocs.io). - Ask questions on the [scverse Discourse](https://discourse.scverse.org). - Install via `pip install anndata` or `conda install anndata -c conda-forge`. - See [Scanpy's documentation](https://scanpy.readthedocs.io/) for usage related to single cell data. anndata was initially built for Scanpy. [//]: # (numfocus-fiscal-sponsor-attribution) anndata is part of the scverse® project ([website](https://scverse.org), [governance](https://scverse.org/about/roles)) and is fiscally sponsored by [NumFOCUS](https://numfocus.org/). If you like scverse® and want to support our mission, please consider making a tax-deductible [donation](https://numfocus.org/donate-to-scverse) to help the project pay for developer time, professional services, travel, workshops, and a variety of other needs. <div align=\"center\"> <a href=\"https://numfocus.org/project/scverse\"> <img src=\"https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png\" width=\"200\" > </a> </div> ## Public API Our public API is documented in the [API section][] of these docs. We cannot guarantee the stability of our internal APIs, whether it's the location of a function, its arguments, or something else. In other words, we do not officially support (or encourage users to do) something like `from anndata._core import AnnData` as `_core` is both not documented and contains a [leading underscore][]. However, we are aware that [many users do use these internal APIs][] and thus encourage them to [open an issue][] or migrate to the public API. That is, if something is missing from our public API as documented, for example a feature you wish to be exported publicly, please open an issue. [api section]: https://anndata.readthedocs.io/en/stable/api.html [leading underscore]: https://peps.python.org/pep-0008/#public-and-internal-interfaces [many users do use these internal APIs]: https://github.com/search?q=%22anndata._io%22&type=code [open an issue]: https://github.com/scverse/anndata/issues/new/choose ## Citation If you use `anndata` in your work, please cite the `anndata` publication as follows: > **anndata: Annotated data** > > Isaac Virshup, Sergei Rybakov, Fabian J. Theis, Philipp Angerer, F. Alexander Wolf > > _JOSS_ 2024 Sep 16. doi: [10.21105/joss.04371](https://doi.org/10.21105/joss.04371). You can cite the scverse publication as follows: > **The scverse project provides a computational ecosystem for single-cell omics data analysis** > > Isaac Virshup, Danila Bredikhin, Lukas Heumos, Giovanni Palla, Gregor Sturm, Adam Gayoso, Ilia Kats, Mikaela Koutrouli, Scverse Community, Bonnie Berger, Dana Pe'er, Aviv Regev, Sarah A. Teichmann, Francesca Finotello, F. Alexander Wolf, Nir Yosef, Oliver Stegle & Fabian J. Theis > > _Nat Biotechnol._ 2023 Apr 10. doi: [10.1038/s41587-023-01733-8](https://doi.org/10.1038/s41587-023-01733-8).\n",
                "dependencies": "[build-system] build-backend = \"hatchling.build\" requires = [ \"hatchling\", \"hatch-vcs\" ] [project] name = \"anndata\" description = \"Annotated data.\" requires-python = \">=3.11\" license = \"BSD-3-Clause\" authors = [ { name = \"Philipp Angerer\" }, { name = \"Alex Wolf\" }, { name = \"Isaac Virshup\" }, { name = \"Sergei Rybakov\" }, ] maintainers = [ { name = \"Isaac Virshup\", email = \"ivirshup@gmail.com\" }, { name = \"Philipp Angerer\", email = \"philipp.angerer@helmholtz-munich.de\" }, { name = \"Ilan Gold\", email = \"ilan.gold@helmholtz-munich.de\" }, ] readme = \"README.md\" classifiers = [ \"Environment :: Console\", \"Framework :: Jupyter\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", \"Natural Language :: English\", \"Operating System :: MacOS :: MacOS X\", \"Operating System :: Microsoft :: Windows\", \"Operating System :: POSIX :: Linux\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Programming Language :: Python :: 3.13\", \"Topic :: Scientific/Engineering :: Bio-Informatics\", \"Topic :: Scientific/Engineering :: Visualization\", ] dependencies = [ # pandas 2.1.0rc0 has pandas/issues/54622 \"pandas >=2.0.0, !=2.1.0rc0, !=2.1.2\", \"numpy>=1.25\", # https://github.com/scverse/anndata/issues/1434 \"scipy >1.11\", \"h5py>=3.8\", \"natsort\", \"packaging>=24.2\", # array-api-compat 1.5 has https://github.com/scverse/anndata/issues/1410 \"array_api_compat>1.4,!=1.5\", \"legacy-api-wrap\", \"zarr >=2.18.7, !=3.0.0, !=3.0.1, !=3.0.2, !=3.0.3, <3.0.7\", ] dynamic = [ \"version\" ] [project.urls] Documentation = \"https://anndata.readthedocs.io/\" Source = \"https://github.com/scverse/anndata\" Home-page = \"https://github.com/scverse/anndata\" [project.optional-dependencies] dev = [ # runtime dev version generation \"hatch-vcs\", \"anndata[dev-doc,test]\", ] doc = [ \"sphinx>=8.2.1\", \"sphinx-book-theme>=1.1.0\", \"sphinx-autodoc-typehints>=2.2.0\", \"sphinx-issues>=5.0.1\", \"sphinx-copybutton\", \"sphinx-toolbox>=3.8.0\", \"sphinxext.opengraph\", \"myst-nb\", \"scanpydoc[theme,typehints] >=0.15.3\", \"awkward>=2.3\", \"IPython\", # For syntax highlighting in notebooks \"myst_parser\", \"sphinx_design>=0.5.0\", # for unreleased changes \"anndata[dev-doc,dask]\", \"awkward>=2.3\", ] dev-doc = [ \"towncrier>=24.8.0\" ] # release notes tool test-full = [ \"anndata[test,lazy]\" ] test = [ \"loompy>=3.0.5\", \"pytest>=8.2,<8.3.4\", \"pytest-cov\", \"pytest-randomly\", \"pytest-memray\", \"pytest-mock\", \"pytest-xdist[psutil]\", \"filelock\", \"matplotlib\", \"scikit-learn\", \"openpyxl\", \"joblib\", \"boltons\", \"scanpy>=1.10\", \"httpx\", # For data downloading \"dask[distributed]\", \"awkward>=2.3\", \"pyarrow\", \"anndata[dask]\", ] gpu = [ \"cupy\" ] cu12 = [ \"cupy-cuda12x\" ] cu11 = [ \"cupy-cuda11x\" ] # requests and aiohttp needed for zarr remote data lazy = [ \"xarray>=2024.06.0\", \"aiohttp\", \"requests\", \"anndata[dask]\" ] # https://github.com/dask/dask/issues/11290 # https://github.com/dask/dask/issues/11752 dask = [ \"dask[array]>=2023.5.1,!=2024.8.*,!=2024.9.*,<2025.2.0\" ] [tool.hatch.version] source = \"vcs\" raw-options.version_scheme = \"release-branch-semver\" [tool.hatch.build.targets.wheel] packages = [ \"src/anndata\", \"src/testing\" ] [tool.coverage.run] data_file = \"test-data/coverage\" source_pkgs = [ \"anndata\" ] omit = [ \"src/anndata/_version.py\", \"**/test_*.py\" ] concurrency = [ \"multiprocessing\" ] parallel = \"true\" [tool.coverage.xml] output = \"test-data/coverage.xml\" [tool.coverage.paths] source = [ \"./src\", \"**/site-packages\" ] [tool.coverage.report] exclude_also = [ \"if TYPE_CHECKING:\", ] [tool.pytest.ini_options] addopts = [ \"--import-mode=importlib\", \"--strict-markers\", \"--doctest-modules\", \"--pyargs\", \"-ptesting.anndata._pytest\", \"--dist=loadgroup\", ] filterwarnings = [ \"ignore::anndata._warnings.OldFormatWarning\", \"ignore::anndata._warnings.ExperimentalFeatureWarning\", ] # When `--strict-warnings` is used, all warnings are treated as errors, except those: filterwarnings_when_strict = [ \"default::anndata._warnings.ImplicitModificationWarning\", \"default:Transforming to str index:UserWarning\", \"default:(Observation|Variable) names are not unique. To make them unique:UserWarning\", \"default::scipy.sparse.SparseEfficiencyWarning\", \"default::dask.array.core.PerformanceWarning\", \"default:anndata will no longer support zarr v2:DeprecationWarning\", \"default:The codec `vlen-utf8:UserWarning\", \"default:The dtype `StringDType():UserWarning\", \"default:Consolidated metadata is:UserWarning\", ] python_files = \"test_*.py\" testpaths = [ \"anndata\", # docstrings (module name due to --pyargs) \"./tests\", # unit tests \"./ci/scripts\", # CI script tests \"./docs/concatenation.rst\", # further doctests ] # For some reason this effects how logging is shown when tests are run xfail_strict = true markers = [ \"gpu: mark test to run on GPU\" ] [tool.ruff] src = [ \"src\" ] [tool.ruff.format] docstring-code-format = true [tool.ruff.lint] select = [ \"B\", # Likely bugs and design issues \"BLE\", # Blind except \"C4\", # Comprehensions \"E\", # Error detected by Pycodestyle \"EM\", # Traceback-friendly error messages \"F\", # Errors detected by Pyflakes \"FBT\", # Boolean positional arguments \"I\", # isort \"ICN\", # Follow import conventions \"ISC\", # Implicit string concatenation \"PERF\", # Performance \"PIE\", # Syntax simplifications \"PTH\", # Pathlib instead of os.path \"PT\", # Pytest conventions \"PL\", # Pylint \"PYI\", # Typing \"RUF\", # Unused noqa \"SIM\", # Code simplifications \"TC\", # manage type checking blocks \"TID\", # Banned imports \"UP\", # pyupgrade \"W\", # Warning detected by Pycodestyle ] external = [ \"PLR0917\" ] # preview rule ignore = [ \"C408\", # dict() syntax is preferable for dicts used as kwargs \"E501\", # line too long -> we accept long comment lines; formatter gets rid of long code lines \"E731\", # Do not assign a lambda expression, use a def -> AnnData allows lambda expression assignments, \"E741\", # allow I, O, l as variable names -> I is the identity matrix, i, j, k, l is reasonable indexing notation \"TID252\", # We use relative imports from parent modules \"PLR2004\", # “2” is often not too “magic” a number \"PLW2901\", # Shadowing loop variables isn’t a big deal ] allowed-confusables = [ \"×\", \"’\", \"–\", \"α\" ] [tool.ruff.lint.per-file-ignores] # E721 comparing types, but we specifically are checking that we aren't getting subtypes (views) \"tests/test_readwrite.py\" = [ \"E721\" ] # PLR0913, PLR0917: tests can use a lot of “arguments” that are actually fixtures \"tests/**/*.py\" = [ \"PLR0913\", \"PLR0917\" ] [tool.ruff.lint.isort] known-first-party = [ \"anndata\" ] required-imports = [ \"from __future__ import annotations\" ] [tool.ruff.lint.flake8-bugbear] extend-immutable-calls = [ \"slice\" ] [tool.ruff.lint.flake8-tidy-imports.banned-api] \"subprocess.call\".msg = \"Use `subprocess.run([…])` instead\" \"subprocess.check_call\".msg = \"Use `subprocess.run([…], check=True)` instead\" \"subprocess.check_output\".msg = \"Use `subprocess.run([…], check=True, capture_output=True)` instead\" \"legacy_api_wrap.legacy_api\".msg = \"Use anndata.compat.old_positionals instead\" [tool.ruff.lint.flake8-type-checking] exempt-modules = [ ] strict = true [tool.ruff.lint.pylint] max-args = 7 max-positional-args = 5 [tool.codespell] skip = \".git,*.pdf,*.svg\" ignore-words-list = \"theis,coo,homogenous\" [tool.towncrier] package = \"anndata\" directory = \"docs/release-notes\" filename = \"docs/release-notes/{version}.md\" single_file = false package_dir = \"src\" issue_format = \"{{pr}}`{issue}`\" title_format = \"(v{version})=\\n### {version} {{small}}`{project_date}`\" fragment.bugfix.name = \"Bug fixes\" fragment.doc.name = \"Documentation\" fragment.feature.name = \"Features\" fragment.misc.name = \"Miscellaneous improvements\" fragment.performance.name = \"Performance\" fragment.breaking.name = \"Breaking changes\" fragment.dev.name = \"Development Process\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ansible-collection-toolkit",
            "repo_link": "https://github.com/hifis-net/ansible-collection-toolkit",
            "content": {
                "codemeta": "",
                "readme": "<!-- SPDX-FileCopyrightText: Helmholtz Centre for Environmental Research (UFZ) SPDX-FileCopyrightText: Helmholtz-Zentrum Dresden-Rossendorf (HZDR) SPDX-License-Identifier: Apache-2.0 --> # Ansible Collection - hifis.toolkit [![Latest release](https://img.shields.io/github/v/release/hifis-net/ansible-collection-toolkit)](https://github.com/hifis-net/ansible-collection-toolkit/releases) [![hifis.gitlab_runner](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/gitlab_runner.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/gitlab_runner.yml) [![hifis.gitlab](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/gitlab.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/gitlab.yml) [![hifis.haproxy](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/haproxy.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/haproxy.yml) [![hifis.keepalived](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/keepalived.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/keepalived.yml) [![hifis.netplan](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/netplan.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/netplan.yml) [![hifis.redis](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/redis.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/redis.yml) [![hifis.ssh_keys](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/ssh_keys.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/ssh_keys.yml) [![hifis.unattended_upgrades](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/unattended_upgrades.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/unattended_upgrades.yml) [![hifis.zammad](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/zammad.yml/badge.svg)](https://github.com/hifis-net/ansible-collection-toolkit/actions/workflows/zammad.yml) [![DOI](https://zenodo.org/badge/495697576.svg)](https://zenodo.org/doi/10.5281/zenodo.11147483) This collection provides production-ready Ansible roles used for providing services used in research and by research software engineers, but not exclusively. The following use cases are supported: * **DevOps platform:** * [GitLab](roles/gitlab) * deploy [GitLab-Runner](roles/gitlab_runner) with a focus, but not limited, on Openstack autoscaling * [Redis](roles/redis) * **Help desk:** * [Zammad](roles/zammad) * **High Availability (HA) / Load Balancing:** * [HAProxy](roles/haproxy) * [Keepalived](roles/keepalived) * **OS-related:** * [unattended-upgrades](roles/unattended_upgrades) * [netplan](roles/netplan) * distribute authorized [SSH keys](roles/ssh_keys) to users ## Looking for the unattended_upgrades role? You can now find it under [roles/unattended_upgrades](roles/unattended_upgrades). We moved our existing Ansible roles into a single collection to deduplicate code and have a common test suite for all roles. We decided to reuse the unattended_upgrades repository as a collection repo as it is our most popular role. ## Minimum required Ansible-version * Ansible >= 2.16 ## Installation Install the collection via ansible-galaxy: ```shell ansible-galaxy collection install hifis.toolkit ``` ## Contributing See [CONTRIBUTING.md](CONTRIBUTING.md). ## License Apache-2.0 ## Author This collection is maintained by [HIFIS Software Services](https://hifis.net/).\n",
                "dependencies": "# SPDX-FileCopyrightText: Helmholtz Centre for Environmental Research (UFZ) # SPDX-FileCopyrightText: Helmholtz-Zentrum Dresden-Rossendorf (HZDR) # # SPDX-License-Identifier: Apache-2.0 [project] name = \"ansible-collection-toolkit\" version = \"5.3.0\" description = \"This collection provides production-ready Ansible roles used for providing services used in research and by research software engineers, but not exclusively.\" readme = \"README.md\" requires-python = \">=3.11\" dependencies = [ \"ansible>=11.3.0\", ] [dependency-groups] dev = [ \"ansible-lint>=25.1.3\", \"molecule>=25.3.1\", \"molecule-plugins[podman]>=23.7.0\", \"netaddr>=1.3.0\", \"reuse>=5.0.2\", \"yamllint>=1.35.1\", ]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/anvio",
            "repo_link": "https://github.com/merenlab/anvio",
            "content": {
                "codemeta": "",
                "readme": "<p align=\"center\"><img src=\"https://github.com/merenlab/anvio/raw/master/anvio/data/interactive/images/logo-fancy.png\" height=\"256\" /></p> [![Daily Component Tests and Migrations](https://github.com/merenlab/anvio/actions/workflows/daily-component-tests-and-migrations.yaml/badge.svg)](https://github.com/merenlab/anvio/actions/workflows/daily-component-tests-and-migrations.yaml) ### Releases Github [releases page](https://github.com/merenlab/anvio/releases) lists all the stable releases of anvi'o. ### Installation and tutorials The [anvi'o project page](https://anvio.org) gives access to installation manuals, user tutorials, and other sweets. ### Help on anvi'o programs and artifacts [The anvi'o help pages](https://anvio.org/help) describe individual anvi'o programs as well as artifacts they consume or produce. ### Coding style considerations Please see [relevant discussions](https://github.com/merenlab/anvio/issues?q=label%3A%22coding+style%22+). ### Community chat Click [this link](https://discord.gg/C6He6mSNY4) to join the anvi'o Discord channel. ### Others on anvi'o Read our [user testimonials](http://merenlab.org/2017/07/12/testimonials/).\n",
                "dependencies": "numpy==1.24.1 scipy bottle pysam ete3 scikit-learn==1.2.2 django requests mistune six matplotlib==3.5.1 statsmodels colored illumina-utils tabulate rich-argparse numba paste pyani psutil pandas==1.4.4 snakemake multiprocess plotext networkx pulp==2.7.0 biopython reportlab pymupdf\nimport os import sys import glob init_py_path = os.path.normpath(os.path.dirname(os.path.abspath(__file__))) + '/anvio/__init__.py' version_string = [l.strip() for l in open(init_py_path).readlines() if l.strip().startswith('anvio_version')][0] anvio_version = version_string.split('=')[1].strip().strip(\"'\").strip('\"') requirements = [req.strip() for req in open('requirements.txt', 'r').readlines() if not req.startswith('#')] try: if sys.version_info.major != 3: sys.stderr.write(\"Your active Python major version ('%d') is not compatible with what anvi'o expects :/ We recently switched to Python 3.\\n\" % sys.version_info.major) sys.exit(-1) except Exception: sys.stderr.write(\"(anvi'o failed to learn about your Python version, but it will pretend as if nothing happened)\\n\\n\") from setuptools import setup, find_packages os.chdir(os.path.normpath(os.path.join(os.path.abspath(__file__), os.pardir))) with open(os.path.join(os.path.dirname(__file__), 'README.md')) as readme: README = readme.read() setup( name = \"anvio\", version = anvio_version, scripts = [script for script in glob.glob('bin/*') + glob.glob('sandbox/*') if not script.endswith('-OBSOLETE')], include_package_data = True, packages = find_packages(), install_requires = requirements, author = \"anvi'o Authors\", author_email = \"a.murat.eren@gmail.com\", description = \"An interactive analysis and visualization platform for 'omics data. See https://merenlab.org/projects/anvio for more information\", license = \"GPLv3+\", keywords = \"metagenomics metatranscriptomics microbiology shotgun genomics MBL pipeline sequencing bam visualization SNP SNV\", url = \"https://merenlab.org/projects/anvio/\", classifiers=[ 'Development Status :: 5 - Production/Stable', 'Environment :: Console', 'Environment :: Web Environment', 'Intended Audience :: Science/Research', 'License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)', 'Natural Language :: English', 'Operating System :: MacOS', 'Operating System :: POSIX', 'Programming Language :: Python :: 3 :: Only', 'Programming Language :: JavaScript', 'Programming Language :: C', 'Topic :: Scientific/Engineering', ], )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/arbor",
            "repo_link": "https://github.com/arbor-sim/arbor/",
            "content": {
                "codemeta": "",
                "readme": "[![ci](https://github.com/arbor-sim/arbor/actions/workflows/test-matrix.yml/badge.svg)](https://github.com/arbor-sim/arbor/actions/workflows/test-matrix.yml) [![spack](https://github.com/arbor-sim/arbor/actions/workflows/test-spack.yml/badge.svg)](https://github.com/arbor-sim/arbor/actions/workflows/test-spack.yml) [![pip](https://github.com/arbor-sim/arbor/actions/workflows/test-pip.yml/badge.svg)](https://github.com/arbor-sim/arbor/actions/workflows/test-pip.yml) [![pythonwheels](https://github.com/arbor-sim/arbor/actions/workflows/build-pip-wheels.yml/badge.svg)](https://github.com/arbor-sim/arbor/actions/workflows/build-pip-wheels.yml) [![gitpod](https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/arbor-sim/arbor) [![docs](https://readthedocs.org/projects/arbor/badge/?version=latest)](https://docs.arbor-sim.org/en/latest/) [![gitter](https://badges.gitter.im/arbor-sim/community.svg)](https://gitter.im/arbor-sim/community) [![CodeQL](https://github.com/arbor-sim/arbor/actions/workflows/codeql.yml/badge.svg?branch=master)](https://github.com/arbor-sim/arbor/actions/workflows/codeql.yml) # Arbor Library [Arbor](https://arbor-sim.org) is a library for implementing performance portable network simulations of multi-compartment neuron models. An installation guide and library documentation are available online at [docs.arbor-sim.org](http://docs.arbor-sim.org). [Submit a ticket](https://github.com/arbor-sim/arbor/issues) or [join Gitter](https://gitter.im/arbor-sim/community) or [Matrix](https://matrix.to/#/#arbor-sim_community:gitter.im) if you have any questions or need help. ### Citing Arbor The Arbor introductory paper and entry on Zenodo can be cited, see [CITATION.bib](CITATION.bib). Please refer to [our documentation](https://docs.arbor-sim.org/en/latest/index.html#citing-arbor) for more information.\n",
                "dependencies": "cmake_minimum_required(VERSION 3.19) include(CMakeDependentOption) include(CheckIPOSupported) # Usually we don't want to hear those set(CMAKE_SUPPRESS_DEVELOPER_WARNINGS ON CACHE INTERNAL \"\" FORCE) # Make CUDA support throw errors if architectures remain unclear cmake_policy(SET CMP0104 NEW) # Ensure CMake is aware of the policies for modern RPATH behavior cmake_policy(SET CMP0072 NEW) # Set release as the default build type (CMake default is debug.) if (NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE release CACHE STRING \"Choose the type of build.\" FORCE) # Set the possible values of build type for cmake-gui set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"debug\" \"release\") endif() set(CPM_USE_LOCAL_PACKAGES ON) include(cmake/CPM.cmake) file(READ VERSION FULL_VERSION_STRING) string(STRIP \"${FULL_VERSION_STRING}\" FULL_VERSION_STRING) string(REGEX MATCH \"^[0-9]+(\\\\.[0-9]+)?(\\\\.[0-9]+)?(\\\\.[0-9]+)?\" numeric_version \"${FULL_VERSION_STRING}\") project(arbor VERSION ${numeric_version}) enable_language(CXX) include(GNUInstallDirs) include(CheckCXXCompilerFlag) # Effectively adds '-fpic' flag to CXX_FLAGS. Needed for dynamic catalogues. set(CMAKE_POSITION_INDEPENDENT_CODE ON) # Have LTO where possible, ie add -flto check_ipo_supported(RESULT HAVE_LTO OUTPUT ERR_LTO) if(NOT DEFINED CMAKE_INTERPROCEDURAL_OPTIMIZATION) if(HAVE_LTO) message (VERBOSE \"LTO support found, enabling\") set(CMAKE_INTERPROCEDURAL_OPTIMIZATION TRUE) else() message(STATUS \"No LTO: ${ERR_LTO}\") endif() endif() # Use pybind11-stubgen to make type stubs. cmake_dependent_option(ARB_BUILD_PYTHON_STUBS \"Use pybind11-stubgen to build type stubs.\" ON \"ARB_WITH_PYTHON\" OFF) # Turn on this option to force the compilers to produce color output when output is # redirected from the terminal (e.g. when using ninja or a pager). option(ARBDEV_COLOR \"Always produce ANSI-colored output (GNU/Clang only).\" OFF) mark_as_advanced(FORCE ARBDEV_COLOR) #---------------------------------------------------------- # Configure-time build options for Arbor: #---------------------------------------------------------- # Specify target architecture. check_cxx_compiler_flag(\"-march=native\" CXX_HAS_NATIVE) if(CXX_HAS_NATIVE) set(ARB_DEFAULT_ARCH \"native\") else() set(ARB_DEFAULT_ARCH \"none\") endif() set(ARB_ARCH ${ARB_DEFAULT_ARCH} CACHE STRING \"Target architecture for arbor libraries\") option(BUILD_SHARED_LIBS \"Build using shared libraries\" OFF) # Perform explicit vectorization? option(ARB_VECTORIZE \"use explicit SIMD code in generated mechanisms\" OFF) # Support for Thread pinning option(ARB_USE_HWLOC \"request support for thread pinning via HWLOC\" OFF) mark_as_advanced(ARB_USE_HWLOC) # Build tests and benchmarks, docs option(BUILD_TESTING \"build tests and benchmarks\" ON) option(BUILD_DOCUMENTATION \"build documentation\" ON) # Use externally built modcc? set(ARB_MODCC \"\" CACHE STRING \"path to external modcc NMODL compiler\") mark_as_advanced(FORCE ARB_MODCC) # Use libunwind to generate stack traces on errors? option(ARB_BACKTRACE \"Enable stacktraces on assertion and exceptions (requires Boost).\" OFF) mark_as_advanced(FORCE ARB_BACKTRACE) # Specify GPU build type set(ARB_GPU \"none\" CACHE STRING \"GPU backend and compiler configuration\") set_property(CACHE PROPERTY STRINGS \"none\" \"cuda\" \"cuda-clang\" \"hip\") if(NOT ARB_GPU STREQUAL \"none\") set(ARB_USE_GPU_DEP ON) endif() cmake_dependent_option(ARB_USE_GPU_RNG \"Use GPU generated random numbers (only cuda, not bitwise equal to CPU version)\" OFF \"ARB_USE_GPU_DEP\" OFF) # Optional additional CXX Flags used for all code that will run on the target # CPU architecture. Recorded in installed target, for downstream dependencies # to use. # Useful, for example, when a user wants to compile with target-specific # optimization flag.spr set(ARB_CXX_FLAGS_TARGET \"\" CACHE STRING \"Optional additional flags for compilation\") mark_as_advanced(FORCE ARB_CXX_FLAGS_TARGET) #---------------------------------------------------------- # Debug support #---------------------------------------------------------- # Print builtin catalogue configuration while building option(ARB_CAT_VERBOSE \"Print catalogue build information\" OFF) mark_as_advanced(ARB_CAT_VERBOSE) #---------------------------------------------------------- # Configure-time features for Arbor: #---------------------------------------------------------- option(ARB_WITH_MPI \"build with MPI support\" OFF) option(ARB_WITH_PROFILING \"enable Tracy profiling\" OFF) cmake_dependent_option(ARB_WITH_STACK_PROFILING \"enable stack collection in profiling\" OFF \"ARB_WITH_PROFILING\" OFF) cmake_dependent_option(ARB_WITH_MEMORY_PROFILING \"enable memory in profiling\" OFF \"ARB_WITH_PROFILING\" OFF) mark_as_advanced(FORCE ARB_WITH_STACK_PROFILING ARB_WITH_MEMORY_PROFILING) option(ARB_WITH_ASSERTIONS \"enable arb_assert() assertions in code\" OFF) #---------------------------------------------------------- # Python front end for Arbor: #---------------------------------------------------------- option(ARB_WITH_PYTHON \"enable Python front end\" OFF) #---------------------------------------------------------- # Global CMake configuration #---------------------------------------------------------- # Include own CMake modules in search path, load common modules. set(CMAKE_MODULE_PATH \"${PROJECT_SOURCE_DIR}/cmake\") include(GitSubmodule) # required for check_git_submodule include(ErrorTarget) # reguired for add_error_target # Add CUDA as a language if GPU support requested. (This has to be set early so # as to enable CUDA tests in generator expressions.) if(ARB_GPU STREQUAL \"cuda\") include(FindCUDAToolkit) set(ARB_WITH_NVCC TRUE) # CMake 3.18 and later set the default CUDA architecture for # each target according to CMAKE_CUDA_ARCHITECTURES. # This fixes nvcc picking up a wrong host compiler for linking, causing # issues with outdated libraries, eg libstdc++ and std::filesystem. Must # happen before all calls to enable_language(CUDA) set(CMAKE_CUDA_HOST_COMPILER ${CMAKE_CXX_COMPILER}) enable_language(CUDA) find_package(CUDAToolkit) if(${CUDAToolkit_VERSION_MAJOR} GREATER_EQUAL 12) if(NOT DEFINED CMAKE_CUDA_ARCHITECTURES) # Pascal, Volta, Ampere, Hopper set(CMAKE_CUDA_ARCHITECTURES 60 70 80 90) endif() else() message(FATAL_ERROR \"Need at least CUDA 12, got ${CUDAToolkit_VERSION_MAJOR}\") endif() # We _still_ need this otherwise CUDA symbols will not be exported # from libarbor.a leading to linker errors when link external clients. # Unit tests are NOT external enough. Re-review this somewhere in the # future. find_package(CUDA ${CUDAToolkit_VERSION_MAJOR} REQUIRED) elseif(ARB_GPU STREQUAL \"cuda-clang\") include(FindCUDAToolkit) if(NOT DEFINED CMAKE_CUDA_ARCHITECTURES) set(CMAKE_CUDA_ARCHITECTURES 60 70 80 90) endif() set(ARB_WITH_CUDA_CLANG TRUE) enable_language(CUDA) elseif(ARB_GPU STREQUAL \"hip\") set(ARB_WITH_HIP_CLANG TRUE) # Specify AMD architecture using a (user provided) list. # Note: CMake native HIP architectures are introduced with version 3.21. set(ARB_HIP_ARCHITECTURES gfx906 gfx900 CACHE STRING \"AMD offload architectures (semicolon separated)\") endif() if(ARB_WITH_NVCC OR ARB_WITH_CUDA_CLANG OR ARB_WITH_HIP_CLANG) set(ARB_WITH_GPU TRUE) endif() # Build paths. set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib) set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib) set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin) # Generate a .json file with full compilation command for each file. set(CMAKE_EXPORT_COMPILE_COMMANDS \"YES\") # Compiler options common to library, examples, tests, etc. include(\"CompilerOptions\") check_supported_cxx() add_compile_options(\"$<$<COMPILE_LANGUAGE:CXX>:${CXXOPT_WALL}>\") set(CMAKE_CXX_STANDARD 20) set(CMAKE_CUDA_STANDARD 20) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(CMAKE_CXX_EXTENSIONS OFF) mark_as_advanced(FORCE CMAKE_OSX_ARCHITECTURES CMAKE_OSX_DEPLOYMENT_TARGET CMAKE_OSX_SYSROOT) #---------------------------------------------------------- # Set up flags and dependencies: #---------------------------------------------------------- # Note: any target dependency of arbor needs to be explicitly added # to the 'export set', even the private ones, and this must be done # in the same CMakeLists.txt in which the target is defined. # Data and internal scripts go here set(ARB_INSTALL_DATADIR ${CMAKE_INSTALL_DATAROOTDIR}/arbor) # Interface library `arbor-config-defs` collects configure-time defines # for arbor, arborenv, arborio, of the form ARB_HAVE_XXX. These # defines should _not_ be used in any installed public headers. add_library(arbor-config-defs INTERFACE) install(TARGETS arbor-config-defs EXPORT arbor-targets) # Interface library `arbor-private-deps` collects dependencies, options etc. # for the arbor library. add_library(arbor-private-deps INTERFACE) target_link_libraries(arbor-private-deps INTERFACE arbor-config-defs ext-random123 ${CMAKE_DL_LIBS}) install(TARGETS arbor-private-deps EXPORT arbor-targets) # Interface library `arborenv-private-deps` collects dependencies, options etc. # for the arborenv library. add_library(arborenv-private-deps INTERFACE) target_link_libraries(arborenv-private-deps INTERFACE arbor-config-defs) install(TARGETS arborenv-private-deps EXPORT arbor-targets) # Interface library `arborio-private-deps` collects dependencies, options etc. # for the arborio library. add_library(arborio-private-deps INTERFACE) target_link_libraries(arborio-private-deps INTERFACE arbor-config-defs) install(TARGETS arborio-private-deps EXPORT arbor-targets) # Interface library `arbor-public-deps` collects requirements for the # users of the arbor library (e.g. mpi) that will become part # of arbor's PUBLIC interface. add_library(arbor-public-deps INTERFACE) install(TARGETS arbor-public-deps EXPORT arbor-targets) # Interface library `arborio-public-deps` collects requirements for the # users of the arborio library (e.g. xml libs) that will become part # of arborio's PUBLIC interface. add_library(arborio-public-deps INTERFACE) install(TARGETS arborio-public-deps EXPORT arborio-targets) # Add scripts and supporting CMake for setting up external catalogues install(PROGRAMS scripts/arbor-build-catalogue DESTINATION ${CMAKE_INSTALL_BINDIR}) install(FILES mechanisms/BuildModules.cmake DESTINATION ${ARB_INSTALL_DATADIR}) # Add all dependencies. # Keep track of packages we need to add to the generated CMake config # file for arbor. set(arbor_export_dependencies) # First make ourselves less chatty set(_saved_CMAKE_MESSAGE_LOG_LEVEL ${CMAKE_MESSAGE_LOG_LEVEL}) set(CMAKE_MESSAGE_LOG_LEVEL STATUS) # in the event we can find hwloc, just add it find_package(hwloc QUIET) add_library(ext-hwloc INTERFACE) if(hwloc_FOUND) # We'd like to use the package syntax, here, yet if we do, we'd need to # provide the find script to the system. target_link_directories(ext-hwloc INTERFACE ${hwloc_LIBRARY_DIRS}) target_link_libraries(ext-hwloc INTERFACE ${hwloc_LIBRARY}) target_include_directories(ext-hwloc INTERFACE ${hwloc_INCLUDE_DIR}) target_compile_definitions(ext-hwloc INTERFACE ARB_HAVE_HWLOC) target_link_libraries(arbor-private-deps INTERFACE ext-hwloc) else() if(ARB_USE_HWLOC) message(SEND_ERROR \"Requested support for hwloc, but CMake couldn't find it.\") endif() endif() install(TARGETS ext-hwloc EXPORT arbor-targets) CPMFindPackage(NAME json GITHUB_REPOSITORY nlohmann/json VERSION 3.12.0 OPTIONS \"CMAKE_SUPPRESS_DEVELOPER_WARNINGS ON\") install(TARGETS nlohmann_json EXPORT arbor-targets) add_library(ext-random123 INTERFACE) CPMFindPackage(NAME random123 DOWNLOAD_ONLY YES GITHUB_REPOSITORY DEShawResearch/random123 VERSION 1.14.0) if(random123_ADDED) target_include_directories(ext-random123 INTERFACE $<BUILD_INTERFACE:${random123_SOURCE_DIR}/include>) else() target_include_directories(ext-random123 INTERFACE ${RANDOM123_INCLUDE_DIR}) endif() install(TARGETS ext-random123 EXPORT arbor-targets) if (ARB_WITH_PYTHON) CPMFindPackage(NAME pybind11 GITHUB_REPOSITORY pybind/pybind11 VERSION 2.13.6 OPTIONS \"PYBIND11_CPP_STANDARD -std=c++20\") # required for find_python_module include(FindPythonModule) endif() CPMFindPackage(NAME pugixml GITHUB_REPOSITORY zeux/pugixml VERSION 1.13 DOWNLOAD_ONLY YES) add_library(ext-pugixml INTERFACE) if(pugixml_ADDED) target_compile_definitions(ext-pugixml INTERFACE PUGIXML_HEADER_ONLY) target_include_directories(ext-pugixml INTERFACE $<BUILD_INTERFACE:${pugixml_SOURCE_DIR}/src>) else() list(APPEND arbor_export_dependencies pugixml) target_link_libraries(ext-pugixml INTERFACE pugixml::pugixml) endif() install(TARGETS ext-pugixml EXPORT arbor-targets) CPMFindPackage(NAME fmt GITHUB_REPOSITORY fmtlib/fmt VERSION 10.0.0 GIT_TAG 10.0.0) add_library(ext-gtest INTERFACE) add_library(ext-bench INTERFACE) if (BUILD_TESTING) CPMFindPackage(NAME benchmark GITHUB_REPOSITORY google/benchmark VERSION 1.8.3 OPTIONS \"BENCHMARK_ENABLE_TESTING OFF\" \"CMAKE_BUILD_TYPE release\" \"BUILD_SHARED_LIBS OFF\") CPMFindPackage(NAME googletest GITHUB_REPOSITORY google/googletest GIT_TAG release-1.12.1 VERSION 1.12.1 OPTIONS \"INSTALL_GTEST OFF\" \"BUILD_GMOCK OFF\") if(benchmark_ADDED) target_link_libraries(ext-bench INTERFACE benchmark) else() target_link_libraries(ext-bench INTERFACE benchmark::benchmark) endif() if(googletest_ADDED) target_link_libraries(ext-gtest INTERFACE ) else() target_link_libraries(ext-gtest INTERFACE gtest gtest_main) endif() endif() CPMFindPackage(NAME units GITHUB_REPOSITORY llnl/units VERSION 0.13.1 OPTIONS \"UNITS_PROJECT_NAME units\" \"SKBUILD OFF\" \"UNITS_INSTALL ON\" \"BUILD_SHARED_LIBS OFF\" \"UNITS_BUILD_STATIC_LIBRARY ON\" \"UNITS_BUILD_SHARED_LIBRARY OFF\" \"UNITS_BUILD_OBJECT_LIBRARY OFF\" \"UNITS_ENABLE_TESTS OFF\" \"UNITS_BUILD_CONVERTER_APP OFF\" \"UNITS_BUILD_WEBSERVER OFF\") target_link_libraries(arbor-public-deps INTERFACE units::units) if(units_ADDED) install(TARGETS units compile_flags_target EXPORT arbor-targets) endif() list(APPEND arbor_export_dependencies units) CPMFindPackage(NAME tinyopt GITHUB_REPOSITORY halfflat/tinyopt GIT_TAG 7e6d707d49c6cb4be27ebd253856be65293288df DOWNLOAD_ONLY YES) add_library(ext-tinyopt INTERFACE) if(tinyopt_ADDED) target_include_directories(ext-tinyopt INTERFACE $<BUILD_INTERFACE:${tinyopt_SOURCE_DIR}/include>) else() message(FATAL_ERROR \"Could not obtain tinyopt.\") endif() # hide all internal vars mark_as_advanced(FORCE benchmark_DIR BENCHMARK_BUILD_32_BITS BENCHMARK_DOWNLOAD_DEPENDENCIES BENCHMARK_ENABLE_ASSEMBLY_TESTS BENCHMARK_ENABLE_DOXYGEN BENCHMARK_ENABLE_EXCEPTIONS BENCHMARK_ENABLE_GTEST_TESTS BENCHMARK_ENABLE_INSTALL BENCHMARK_ENABLE_LIBPFM BENCHMARK_ENABLE_LTO BENCHMARK_ENABLE_WERROR BENCHMARK_FORCE_WERROR BENCHMARK_INSTALL_DOCS BENCHMARK_USE_BUNDLED_GTEST BENCHMARK_USE_LIBCXX) mark_as_advanced(FORCE googletest_DIR BUILD_GMOCK) mark_as_advanced(FORCE json_DIR JSON_CI JSON_BuildTests JSON_Diagnostics JSON_DisableEnumSerialization JSON_GlobalUDLs JSON_ImplicitConversions JSON_Install JSON_LegacyDiscardedValueComparison JSON_MultipleHeaders JSON_SystemInclude) mark_as_advanced(FORCE RANDOM123_INCLUDE_DIR) mark_as_advanced(FORCE pybind11_DIR PYBIND11_PYTHONLIBS_OVERWRITE PYBIND11_PYTHON_VERSION PYBIND11_FINDPYTHON PYBIND11_INSTALL PYBIND11_INTERNALS_VERSION PYBIND11_NOPYTHON PYBIND11_SIMPLE_GIL_MANAGEMENT PYBIND11_TEST) mark_as_advanced(FORCE pugixml_DIR) mark_as_advanced(FORCE fmt_DIR) mark_as_advanced(FORCE units_DIR UNITS_BUILD_OBJECT_LIBRARY UNITS_BUILD_SHARED_LIBRARY UNITS_HEADER_ONLY UNITS_NAMESPACE UNITS_BUILD_FUZZ_TARGETS UNITS_ENABLE_TESTS) mark_as_advanced(FORCE tinyopt_DIR) mark_as_advanced(FORCE CXXFEATURECHECK_DEBUG) mark_as_advanced(FORCE CPM_DONT_CREATE_PACKAGE_LOCK CPM_DONT_UPDATE_MODULE_PATH CPM_DOWNLOAD_ALL CPM_INCLUDE_ALL_IN_PACKAGE_LOCK CPM_LOCAL_PACKAGES_ONLY CPM_SOURCE_CACHE CPM_USE_NAMED_CACHE_DIRECTORIES) mark_as_advanced(FORCE FETCHCONTENT_BASE_DIR FETCHCONTENT_FULLY_DISCONNECTED FETCHCONTENT_QUIET FETCHCONTENT_SOURCE_DIR_BENCHMARK FETCHCONTENT_SOURCE_DIR_GOOGLETEST FETCHCONTENT_SOURCE_DIR_JSON FETCHCONTENT_SOURCE_DIR_PYBIND11 FETCHCONTENT_SOURCE_DIR_RANDOM123 FETCHCONTENT_SOURCE_DIR_TINYOPT FETCHCONTENT_SOURCE_DIR_UNITS FETCHCONTENT_UPDATES_DISCONNECTED FETCHCONTENT_UPDATES_DISCONNECTED_BENCHMARK FETCHCONTENT_UPDATES_DISCONNECTED_GOOGLETEST FETCHCONTENT_UPDATES_DISCONNECTED_JSON FETCHCONTENT_UPDATES_DISCONNECTED_PYBIND11 FETCHCONTENT_UPDATES_DISCONNECTED_RANDOM123 FETCHCONTENT_UPDATES_DISCONNECTED_TINYOPT FETCHCONTENT_UPDATES_DISCONNECTED_UNITS) # Restore chattyness set(CMAKE_MESSAGE_LOG_LEVEL ${_saved_CMAKE_MESSAGE_LOG_LEVEL}) # Keep track of which 'components' of arbor are included (this is # currently just 'MPI' support and 'neuroml' for NeuroML support in # libarborio.) set(arbor_supported_components) # Target microarchitecture for building arbor libraries, tests and examples #--------------------------------------------------------------------------- # Set the full set of target flags in ARB_CXX_FLAGS_TARGET_FULL, which # will include target-specific -march flags if ARB_ARCH is not \"none\". if(ARB_ARCH STREQUAL \"none\") set(ARB_CXX_FLAGS_TARGET_FULL ${ARB_CXX_FLAGS_TARGET}) set(ARB_CXX_FLAGS_TARGET_FULL_CPU ${ARB_CXX_FLAGS_TARGET}) else() set_arch_target(ARB_CXXOPT_ARCH_CPU ARB_CXXOPT_ARCH ${ARB_ARCH}) set(ARB_CXX_FLAGS_TARGET_FULL ${ARB_CXX_FLAGS_TARGET} ${ARB_CXXOPT_ARCH}) set(ARB_CXX_FLAGS_TARGET_FULL_CPU ${ARB_CXX_FLAGS_TARGET} ${ARB_CXXOPT_ARCH_CPU}) endif() # Add SVE compiler flags if detected/desired set(ARB_SVE_WIDTH \"auto\" CACHE STRING \"Default SVE vector length in bits. Default: auto (detection during configure time).\") mark_as_advanced(ARB_SVE_WIDTH) if (ARB_VECTORIZE) if (ARB_SVE_WIDTH STREQUAL \"auto\") get_sve_length(ARB_HAS_SVE ARB_SVE_BITS) if (ARB_HAS_SVE) message(STATUS \"SVE detected with vector size = ${ARB_SVE_BITS} bits\") set(ARB_CXX_SVE_FLAGS \" -msve-vector-bits=${ARB_SVE_BITS}\") else() message(STATUS \"NO SVE detected\") set(ARB_CXX_SVE_FLAGS \"\") endif() else() set(ARB_SVE_BITS ${ARB_SVE_WIDTH}) set(ARB_CXX_SVE_FLAGS \" -msve-vector-bits=${ARB_SVE_BITS}\") endif() list(APPEND ARB_CXX_FLAGS_TARGET_FULL \"$<$<BUILD_INTERFACE:$<COMPILE_LANGUAGE:CXX>>:${ARB_CXX_SVE_FLAGS}>\") endif() # Compile with `-fvisibility=hidden` to ensure that the symbols of the generated # arbor static libraries are hidden from the dynamic symbol tables of any shared # libraries that link against them. list(APPEND ARB_CXX_FLAGS_TARGET_FULL \"$<$<BUILD_INTERFACE:$<COMPILE_LANGUAGE:CXX>>:-fvisibility=hidden>\" \"$<$<BUILD_INTERFACE:$<COMPILE_LANGUAGE:CUDA>>:-Xcompiler=-fvisibility=hidden>\") separate_arguments(ARB_CXX_FLAGS_TARGET_FULL) target_compile_options(arbor-private-deps INTERFACE ${ARB_CXX_FLAGS_TARGET_FULL}) target_compile_options(arborenv-private-deps INTERFACE ${ARB_CXX_FLAGS_TARGET_FULL}) target_compile_options(arborio-private-deps INTERFACE ${ARB_CXX_FLAGS_TARGET_FULL}) # Profiling and test features #----------------------------- if(ARB_WITH_PROFILING) target_compile_definitions(arbor-config-defs INTERFACE ARB_HAVE_PROFILING) endif() if(ARB_WITH_ASSERTIONS) target_compile_definitions(arbor-config-defs INTERFACE ARB_HAVE_ASSERTIONS) endif() # Python bindings #---------------------------------------------------------- # The minimum version of Python supported by Arbor. set(arb_py_version 3.10.0) if(DEFINED PYTHON_EXECUTABLE) set(Python3_EXECUTABLE ${PYTHON_EXECUTABLE}) endif() if(ARB_WITH_PYTHON) if(DEFINED ENV{CIBUILDWHEEL} AND (UNIX AND NOT APPLE)) find_package(Python3 ${arb_py_version} COMPONENTS Interpreter Development.Module REQUIRED) else() find_package(Python3 ${arb_py_version} COMPONENTS Interpreter Development REQUIRED) endif() else() # If not building the Python module, the interpreter is still required # to build some targets, e.g. when building the documentation. find_package(Python3 ${arb_py_version} COMPONENTS Interpreter) endif() if(${Python3_FOUND}) set(PYTHON_EXECUTABLE \"${Python3_EXECUTABLE}\") message(VERBOSE \"PYTHON_EXECUTABLE: ${PYTHON_EXECUTABLE}\") endif() # Threading model #----------------- find_package(Threads REQUIRED) target_link_libraries(arbor-private-deps INTERFACE Threads::Threads) list(APPEND arbor_export_dependencies \"Threads\") # MPI support #------------------- if(ARB_WITH_MPI) find_package(MPI REQUIRED CXX) target_compile_definitions(arbor-config-defs INTERFACE ARB_HAVE_MPI) # CMake 3.9 does not allow us to add definitions to an import target. so # wrap MPI::MPI_CXX in an interface library 'mpi-wrap' instead. add_library(mpi-wrap INTERFACE) target_link_libraries(mpi-wrap INTERFACE MPI::MPI_CXX) target_compile_definitions(mpi-wrap INTERFACE MPICH_SKIP_MPICXX=1 OMPI_SKIP_MPICXX=1) target_link_libraries(arbor-public-deps INTERFACE mpi-wrap) install(TARGETS mpi-wrap EXPORT arbor-targets) list(APPEND arbor_export_dependencies \"MPI\\;COMPONENTS\\;CXX\") list(APPEND arbor_supported_components \"MPI\") endif() # CUDA support #-------------- if(ARB_WITH_GPU) if(ARB_WITH_NVCC OR ARB_WITH_CUDA_CLANG) target_include_directories(arborenv-private-deps INTERFACE ${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES}) add_compile_options( \"$<$<COMPILE_LANGUAGE:CUDA>:-Xcudafe=--diag_suppress=integer_sign_change>\" \"$<$<COMPILE_LANGUAGE:CUDA>:-Xcudafe=--diag_suppress=unsigned_compare_with_zero>\") endif() if(ARB_WITH_NVCC) target_compile_definitions(arbor-private-deps INTERFACE ARB_CUDA) target_compile_definitions(arborenv-private-deps INTERFACE ARB_CUDA) elseif(ARB_WITH_CUDA_CLANG) # Transform cuda archtitecture list into clang cuda flags list(TRANSFORM CMAKE_CUDA_ARCHITECTURES PREPEND \"--cuda-gpu-arch=sm_\" OUTPUT_VARIABLE TMP) string(REPLACE \";\" \" \" CUDA_ARCH_STR \"${TMP}\") set(clang_options_ -DARB_CUDA -xcuda ${CUDA_ARCH_STR} --cuda-path=${CUDA_TOOLKIT_ROOT_DIR}) target_compile_options(arbor-private-deps INTERFACE $<$<COMPILE_LANGUAGE:CXX>:${clang_options_}>) target_compile_options(arborenv-private-deps INTERFACE $<$<COMPILE_LANGUAGE:CXX>:${clang_options_}>) elseif(ARB_WITH_HIP_CLANG) # Transform hip archtitecture list into clang hip flags list(TRANSFORM ARB_HIP_ARCHITECTURES PREPEND \"--offload-arch=\" OUTPUT_VARIABLE TMP) string(REPLACE \";\" \" \" HIP_ARCH_STR \"${TMP}\") set(clang_options_ -DARB_HIP -xhip ${HIP_ARCH_STR}) target_compile_options(arbor-private-deps INTERFACE $<$<COMPILE_LANGUAGE:CXX>:${clang_options_}>) target_compile_options(arborenv-private-deps INTERFACE $<$<COMPILE_LANGUAGE:CXX>:${clang_options_}>) endif() endif() # Use boost::stacktrace if requested for pretty printing stack traces #-------------------------------------------------------------------- if (ARB_BACKTRACE) find_package(Boost REQUIRED COMPONENTS stacktrace_basic stacktrace_addr2line) target_link_libraries(arbor-private-deps INTERFACE Boost::stacktrace_basic Boost::stacktrace_addr2line ${CMAKE_DL_LIBS}) target_compile_definitions(arbor-private-deps INTERFACE WITH_BACKTRACE) endif() # Build modcc flags #------------------------------------------------ if(ARB_MODCC) find_program(modcc NAMES ${ARB_MODCC} NO_CMAKE_PATH NO_CMAKE_ENVIRONMENT_PATH NO_CMAKE_SYSTEM_PATH REQUIRED) if(NOT modcc) message(FATAL_ERROR \"Unable to find modcc executable.\") endif() set(ARB_WITH_EXTERNAL_MODCC TRUE) else() set(modcc $<TARGET_FILE:modcc>) set(ARB_WITH_EXTERNAL_MODCC FALSE) endif() set(ARB_MODCC_FLAGS) if(ARB_VECTORIZE) list(APPEND ARB_MODCC_FLAGS \"--simd\") endif() # Random number creation # ----------------------------------------------- if(ARB_USE_GPU_RNG AND (ARB_WITH_NVCC OR ARB_WITH_CUDA_CLANG)) set(ARB_USE_GPU_RNG_IMPL TRUE) else() set(ARB_USE_GPU_RNG_IMPL FALSE) endif() #---------------------------------------------------------- # Set up install paths, permissions. #---------------------------------------------------------- # Set up install paths according to GNU conventions. # # GNUInstallDirs picks (e.g.) `lib64` for the library install path on some # systems where this is definitely not correct (e.g. Arch Linux). If there # are cases where `lib` is inappropriate, we will have to incorporate special # case behaviour here. if(NOT CMAKE_INSTALL_LIBDIR) set(CMAKE_INSTALL_LIBDIR lib) endif() include(GNUInstallDirs) # Implicitly created directories require permissions to be set explicitly # via this CMake variable. # # Note that this has no effect until CMake version 3.11. set(CMAKE_INSTALL_DEFAULT_DIRECTORY_PERMISSIONS OWNER_READ OWNER_WRITE OWNER_EXECUTE GROUP_READ GROUP_EXECUTE WORLD_READ WORLD_EXECUTE) # CMake versions 3.11 and 3.12 ignore this variable for directories # implicitly created by install(DIRECTORY ...), which for us corresponds # to our doc and include directories. Work-around by trying to install # a non-existant file to these locations. foreach(directory \"${CMAKE_INSTALL_DOCDIR}\" \"${CMAKE_INSTALL_INCLUDEDIR}\") install(FILES _no_such_file_ OPTIONAL DESTINATION \"${directory}\") endforeach() #---------------------------------------------------------- # Configure targets in sub-directories. #---------------------------------------------------------- # arbor-public-headers: add_subdirectory(arbor/include) # arbor-sup: add_subdirectory(sup) # modcc, libmodcc: add_subdirectory(modcc) # arbor, arbor-private-headers: add_subdirectory(arbor) # arborenv, arborenv-public-headers: add_subdirectory(arborenv) # arborio, arborio-public-headers: add_subdirectory(arborio) # unit, unit-mpi, unit-local, unit-modcc if (BUILD_TESTING) add_subdirectory(test) endif() # self contained examples: add_subdirectory(example) # html: if (BUILD_DOCUMENTATION) add_subdirectory(doc) endif() # python interface: if(ARB_WITH_PYTHON) add_subdirectory(python) endif() #---------------------------------------------------------- # Generate CMake config/version files for install. #---------------------------------------------------------- # Note: each dependency for the arbor library target, private or otherwise, # needs to add itself to the arbor-exports EXPORT target in the subdirectory # in which they are defined, or none of this will work. set(cmake_config_dir \"${CMAKE_INSTALL_LIBDIR}/cmake/arbor\") install(EXPORT arbor-targets NAMESPACE arbor:: DESTINATION \"${cmake_config_dir}\") include(CMakePackageConfigHelpers) write_basic_package_version_file( \"${CMAKE_CURRENT_BINARY_DIR}/arbor-config-version.cmake\" COMPATIBILITY SameMajorVersion) # Template file will use contents of arbor_export_dependencies to include the # required `find_dependency` statements, and arbor_supported_components will # be used to check feature support. # # To avoid CMake users of the installed arbor library conditionally requiring # that they add CUDA to their project language, explicitly munge the import # language and library dependencies on the installed target if ARB_WITH_GPU # is set, via the variables arbor_override_import_lang and arbor_add_import_libs. # arbor_build_config records our build type in a way compatible with the # generated export cmake files. set(arbor_build_config NOCONFIG) if(CMAKE_BUILD_TYPE) string(TOUPPER \"${CMAKE_BUILD_TYPE}\" arbor_build_config) endif() set(arbor_override_import_lang) set(arbor_add_import_libs) set(arborenv_add_import_libs) set(arborio_add_import_libs) if(ARB_WITH_GPU) set(arbor_override_import_lang CXX) set(arbor_add_import_libs ${CUDA_LIBRARIES}) set(arborenv_add_import_libs ${CUDA_LIBRARIES}) endif() # (We remove old generated one so that the generation happens every time we run cmake.) file(REMOVE \"${CMAKE_CURRENT_BINARY_DIR}/arbor-config.cmake\") configure_file( \"${CMAKE_CURRENT_SOURCE_DIR}/cmake/arbor-config.cmake.in\" \"${CMAKE_CURRENT_BINARY_DIR}/arbor-config.cmake\" @ONLY) install( FILES \"${CMAKE_CURRENT_BINARY_DIR}/arbor-config.cmake\" \"${CMAKE_CURRENT_BINARY_DIR}/arbor-config-version.cmake\" DESTINATION \"${cmake_config_dir}\") add_subdirectory(lmorpho)\n[project] name = \"arbor\" dynamic = [\"version\"] readme = {file = \"README.md\", content-type = \"text/markdown\"} license = \"BSD-3-Clause\" license-files = [\"LICENSE\"] description = \"High performance simulation of networks of multicompartment neurons.\" requires-python = \">=3.10\" keywords = [\"simulator\", \"neuroscience\", \"morphological detail\", \"HPC\", \"GPU\", \"C++\"] authors = [ {name = \"Arbor Dev Team\", email = \"contact@arbor-sim.org\"} ] maintainers = [ {name = \"Arbor Dev Team\", email = \"contact@arbor-sim.org\"} ] classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Intended Audience :: Science/Research\", \"Programming Language :: Python\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Programming Language :: Python :: 3.13\", \"Programming Language :: C++\" ] dependencies = [ \"numpy\" ] [project.entry-points.\"cmake.root\"] arbor = \"arbor\" [project.scripts] modcc = \"arbor:modcc\" arbor-build-catalogue = \"arbor:build_catalogue\" [tool.scikit-build] cmake.args = [ \"-DARB_WITH_PYTHON=ON\", ] sdist.include = [\"ext/*/.git\"] wheel.install-dir = \"arbor\" wheel.packages = [] [tool.scikit-build.metadata.version] provider = \"scikit_build_core.metadata.regex\" input = \"VERSION\" regex = \"(?P<value>\\\\d+\\\\.\\\\d+\\\\.\\\\d+(-.+)?)\" [tool.ruff] exclude = [ \".direnv\", \".eggs\", \".git\", \".ipynb_checkpoints\", \"_deps\", \".ruff_cache\", \".venv\", \".vscode\", \"build\", \"dist\", \"ext\", \"install-*\", \"doc/scripts/inputs.py\", \"doc/scripts/make_images.py\", \".*\", \"spack/package.py\"] line-length = 88 indent-width = 4 target-version = \"py312\" [tool.ruff.lint] ignore = [ # for black \"E203\", \"E231\", # zealous line lengths \"E501\", # ambiguous varnames I ./. l etc \"E741\", # Ruff doesn't like this rule \"ISC001\", # old school zip \"B905\"] select = [\"C\", \"E\", \"F\", \"W\", \"B\"] # Allow fix for all enabled rules (when `--fix`) is provided. fixable = [\"ALL\"] unfixable = [] mccabe.max-complexity = 15 # Allow unused variables when underscore-prefixed. dummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\" [tool.ruff.lint.per-file-ignores] \"python/example/brunel/analysis.py\" = [\"F405\", \"F403\"] \"python/example/brunel/arbor_brunel.py\" = [\"F405\", \"F403\"] \"python/example/brunel/nest_brunel.py\" = [\"F405\", \"F403\"] [tool.ruff.format] quote-style = \"double\" indent-style = \"space\" skip-magic-trailing-comma = false line-ending = \"auto\" # Disable auto-formatting of code examples in docstrings. Markdown, # reStructuredText code/literal blocks and doctests are all supported. docstring-code-format = false docstring-code-line-length = \"dynamic\" [project.urls] homepage = \"https://arbor-sim.org\" documentation = \"https://docs.arbor-sim.org\" repository = \"https://github.com/arbor-sim/arbor\" changelog = \"https://github.com/arbor-sim/arbor/releases\" [build-system] requires = [ \"scikit-build-core\", \"numpy\", \"pybind11-stubgen\", ] build-backend = \"scikit_build_core.build\" [tool.cibuildwheel] build-frontend = \"build\" build = [\"*linux*\",\"*macosx*\"] manylinux-x86_64-image = \"manylinux_2_28\" manylinux-aarch64-image = \"manylinux_2_28\" skip = [\"cp36*\", \"cp37*\", \"cp38*\", \"cp39*\", \"*musllinux*\", \"pp*\"] test-command = \"python -m unittest discover -v -s {project}/python\" dependency-versions = \"latest\" [tool.cibuildwheel.macos] archs = [\"arm64\"] environment = { MACOSX_DEPLOYMENT_TARGET = \"14.0\" } [tool.cibuildwheel.linux] archs = [\"x86_64\"] [[tool.cibuildwheel.overrides]] select = \"*-musllinux*\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ardoco",
            "repo_link": "https://github.com/ArDoCo/Core",
            "content": {
                "codemeta": "",
                "readme": "# ArDoCo Core [![Maven Verify](https://github.com/ArDoCo/Core/actions/workflows/verify.yml/badge.svg)](https://github.com/ArDoCo/Core/actions/workflows/verify.yml) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/io.github.ardoco.core/parent/badge.svg)](https://maven-badges.herokuapp.com/maven-central/io.github.ardoco.core/parent) [![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=ArDoCo_Core&metric=alert_status)](https://sonarcloud.io/dashboard?id=ArDoCo_Core) [![Latest Release](https://img.shields.io/github/release/ArDoCo/Core.svg)](https://github.com/ArDoCo/Core/releases/latest) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7274034.svg)](https://doi.org/10.5281/zenodo.7274034) The goal of the ArDoCo project is to connect architecture documentation and models with Traceability Link Recovery (TLR) while identifying missing or deviating elements (inconsistencies). An element can be any representable item of the model, like a component or a relation. To do so, we first create trace links and then make use of them and other information to identify inconsistencies. ArDoCo is actively developed by researchers of the _[Modelling for Continuous Software Engineering (MCSE) group](https://mcse.kastel.kit.edu)_ of _[KASTEL - Institute of Information Security and Dependability](https://kastel.kit.edu)_ at the [KIT](https://www.kit.edu). This **Core** repository contains the framework and core definitions for the other approaches. As such, there is the definition of our pipeline and the data handling as well as the definitions for the various pipeline steps, inputs, outputs, etc. For more information about the setup, the project structure, or the architecture, please have a look at the [Wiki](https://github.com/ArDoCo/Core/wiki). ## Maven ```xml <dependencies> <dependency> <groupId>io.github.ardoco.core</groupId> <artifactId>framework</artifactId> <!-- or any other subproject --> <version>VERSION</version> </dependency> </dependencies> ``` For snapshot releases, make sure to add the following repository ```xml <repositories> <repository> <releases> <enabled>false</enabled> </releases> <snapshots> <enabled>true</enabled> </snapshots> <id>mavenSnapshot</id> <url>https://s01.oss.sonatype.org/content/repositories/snapshots</url> </repository> </repositories> ``` ## Relevant repositories The following is an excerpt of repositories that use this framework and implement the different approaches and pipelines of ArDoCo: * [ArDoCo/TLR](https://github.com/ArDoCo/TLR): implementing different traceability link recovery approaches * [ArDoCo/InconsistencyDetection](https://github.com/ArDoCo/InconsistencyDetection): implementing inconsistency detection approaches * [ArDoCo/LiSSA](https://github.com/ArDoCo/LiSSA): implementing processing of sketches and diagrams for, e.g., TLR\n",
                "dependencies": "<?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>io.github.ardoco.core</groupId> <artifactId>parent</artifactId> <version>${revision}</version> <packaging>pom</packaging> <name>ArDoCo - The Consistency Analyzer: Core Framework</name> <description>The goal of this project is to connect architecture documentation and models while identifying missing or deviating elements (inconsistencies). An element can be any representable item of the model, like a component or a relation. To do so, we first create trace links and then make use of them and other information to identify inconsistencies. ArDoCo is actively developed by researchers of the Modelling for Continuous Software Engineering (MCSE) group of KASTEL - Institute of Information Security and Dependability at the KIT.</description> <url>https://github.com/ArDoCo/Core</url> <licenses> <license> <name>MIT License</name> <url>https://www.opensource.org/licenses/mit-license.php</url> </license> </licenses> <developers> <developer> <id>Hossiphi</id> <name>Sophie Corallo</name> <email>sophie.corallo@kit.edu</email> <url>https://mcse.kastel.kit.edu/staff_sophie_corallo.php</url> <organization>KASTEL</organization> <organizationUrl>https://mcse.kastel.kit.edu/</organizationUrl> <timezone>GMT+1</timezone> </developer> <developer> <id>dfuchss</id> <name>Dominik Fuchss</name> <email>dominik.fuchss@kit.edu</email> <url>https://mcse.kastel.kit.edu/staff_dominik_fuchss.php</url> <organization>KASTEL</organization> <organizationUrl>https://mcse.kastel.kit.edu/</organizationUrl> <timezone>GMT+1</timezone> </developer> <developer> <id>Gram21</id> <name>Jan Keim</name> <email>jan.keim@kit.edu</email> <url>https://mcse.kastel.kit.edu/staff_Keim_Jan.php</url> <organization>KASTEL</organization> <organizationUrl>https://mcse.kastel.kit.edu/</organizationUrl> <timezone>GMT+1</timezone> </developer> </developers> <scm> <connection>scm:git:git://github.com/ArDoCo/Core.git</connection> <developerConnection>scm:git:ssh://github.com:ArDoCo/Core.git</developerConnection> <tag>HEAD</tag> <url>http://github.com/ArDoCo/Core/tree/main</url> </scm> <issueManagement> <system>GitHub Issues</system> <url>https://github.com/ArDoCo/Core/issues</url> </issueManagement> <distributionManagement> <snapshotRepository> <id>ossrh</id> <url>https://s01.oss.sonatype.org/content/repositories/snapshots</url> </snapshotRepository> </distributionManagement> <properties> <revision>2.0.0-SNAPSHOT</revision> <ardoco.version>${revision}</ardoco.version> <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding> <project.source.encoding>UTF-8</project.source.encoding> <java.version>21</java.version> <maven.compiler.source>${java.version}</maven.compiler.source> <maven.compiler.target>${java.version}</maven.compiler.target> <maven.compiler.release>${java.version}</maven.compiler.release> <!-- Plugin Versions --> <slf4j.version>2.0.14</slf4j.version> <spotless.version>2.43.0</spotless.version> <junit.version>5.11.0</junit.version> <eclipse-collections.version>12.0.0.M3</eclipse-collections.version> <jackson.version>2.17.2</jackson.version> <javaparser.version>3.25.8</javaparser.version> <error-prone.version>2.31.0</error-prone.version> <mockito.version>5.2.0</mockito.version> <maven-jar-plugin.version>3.3.0</maven-jar-plugin.version> <jgrapht.version>1.5.2</jgrapht.version> <sonar.projectKey>ArDoCo_Core</sonar.projectKey> <sonar.moduleKey>${project.groupId}:${project.artifactId}</sonar.moduleKey> <sonar.organization>ardoco</sonar.organization> <sonar.host.url>https://sonarcloud.io</sonar.host.url> <sonar.coverage.jacoco.xmlReportPaths>${project.basedir}/../${aggregate.report.dir}, ${project.basedir}/../../${aggregate.report.dir}</sonar.coverage.jacoco.xmlReportPaths> <aggregate.report.dir>report/target/site/jacoco-aggregate/jacoco.xml</aggregate.report.dir> <argLine>-Xmx4g -Xss256m</argLine> <stanford.corenlp.version>4.5.6</stanford.corenlp.version> <doclint.options>all,-missing</doclint.options> </properties> <dependencyManagement> <dependencies> <dependency> <groupId>com.fasterxml.jackson.core</groupId> <artifactId>jackson-annotations</artifactId> <version>${jackson.version}</version> </dependency> <dependency> <groupId>com.fasterxml.jackson.core</groupId> <artifactId>jackson-core</artifactId> <version>${jackson.version}</version> </dependency> <dependency> <groupId>com.fasterxml.jackson.core</groupId> <artifactId>jackson-databind</artifactId> <version>${jackson.version}</version> </dependency> <dependency> <groupId>com.fasterxml.jackson.datatype</groupId> <artifactId>jackson-datatype-jdk8</artifactId> <version>${jackson.version}</version> </dependency> <!-- Kotlin --> <dependency> <groupId>com.fasterxml.jackson.module</groupId> <artifactId>jackson-module-kotlin</artifactId> <version>${jackson.version}</version> </dependency> <dependency> <groupId>com.tngtech.archunit</groupId> <artifactId>archunit-junit5</artifactId> <version>1.3.0</version> </dependency> <dependency> <groupId>commons-io</groupId> <artifactId>commons-io</artifactId> <version>2.16.1</version> </dependency> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-text</artifactId> <version>1.12.0</version> </dependency> <dependency> <groupId>org.apache.httpcomponents.client5</groupId> <artifactId>httpclient5</artifactId> <version>5.3.1</version> </dependency> <dependency> <groupId>org.assertj</groupId> <artifactId>assertj-core</artifactId> <version>3.26.3</version> <scope>test</scope> </dependency> <dependency> <groupId>org.eclipse.collections</groupId> <artifactId>eclipse-collections</artifactId> <version>${eclipse-collections.version}</version> </dependency> <dependency> <groupId>org.eclipse.collections</groupId> <artifactId>eclipse-collections-api</artifactId> <version>${eclipse-collections.version}</version> </dependency> <dependency> <groupId>org.eclipse.jgit</groupId> <artifactId>org.eclipse.jgit</artifactId> <version>6.9.0.202403050737-r</version> </dependency> <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-core</artifactId> <version>${mockito.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-inline</artifactId> <version>${mockito.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-junit-jupiter</artifactId> <version>${mockito.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.reflections</groupId> <artifactId>reflections</artifactId> <version>0.10.2</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>log4j-over-slf4j</artifactId> <version>${slf4j.version}</version> </dependency> <!-- Testing --> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-simple</artifactId> <version>${slf4j.version}</version> </dependency> </dependencies> </dependencyManagement> <dependencies> <dependency> <groupId>com.google.errorprone</groupId> <artifactId>error_prone_core</artifactId> <version>${error-prone.version}</version> </dependency> <dependency> <groupId>org.junit.jupiter</groupId> <artifactId>junit-jupiter-api</artifactId> <version>${junit.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.junit.jupiter</groupId> <artifactId>junit-jupiter-engine</artifactId> <version>${junit.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.junit.jupiter</groupId> <artifactId>junit-jupiter-params</artifactId> <version>${junit.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.junit.vintage</groupId> <artifactId>junit-vintage-engine</artifactId> <version>${junit.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version>${slf4j.version}</version> </dependency> </dependencies> <repositories> <repository> <id>mavenCentral</id> <url>https://repo1.maven.org/maven2/</url> </repository> <repository> <releases> <enabled>false</enabled> </releases> <snapshots> <enabled>true</enabled> </snapshots> <id>mavenSnapshot</id> <url>https://s01.oss.sonatype.org/content/repositories/snapshots</url> </repository> </repositories> <build> <pluginManagement> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-assembly-plugin</artifactId> <version>3.7.1</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.13.0</version> <configuration> <release>${java.version}</release> <source>${java.version}</source> <target>${java.version}</target> <encoding>UTF-8</encoding> <fork>true</fork> <compilerArgs> <arg>-XDcompilePolicy=simple</arg> <arg>-Xplugin:ErrorProne</arg> <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED</arg> <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED</arg> <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.main=ALL-UNNAMED</arg> <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.model=ALL-UNNAMED</arg> <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED</arg> <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.processing=ALL-UNNAMED</arg> <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED</arg> <arg>-J--add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED</arg> <arg>-J--add-opens=jdk.compiler/com.sun.tools.javac.code=ALL-UNNAMED</arg> <arg>-J--add-opens=jdk.compiler/com.sun.tools.javac.comp=ALL-UNNAMED</arg> </compilerArgs> <meminitial>128m</meminitial> <maxmem>512m</maxmem> <annotationProcessorPaths> <path> <groupId>com.google.errorprone</groupId> <artifactId>error_prone_core</artifactId> <version>${error-prone.version}</version> </path> </annotationProcessorPaths> </configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-failsafe-plugin</artifactId> <version>3.2.5</version> <executions> <execution> <goals> <goal>integration-test</goal> <goal>verify</goal> </goals> <phase>integration-test</phase> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-gpg-plugin</artifactId> <version>3.2.4</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-install-plugin</artifactId> <version>3.1.2</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-jar-plugin</artifactId> <version>3.4.2</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>3.2.5</version> </plugin> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>flatten-maven-plugin</artifactId> <version>1.6.0</version> <configuration> <updatePomFile>true</updatePomFile> <flattenMode>resolveCiFriendliesOnly</flattenMode> <pomElements> <dependencyManagement>expand</dependencyManagement> <dependencies>expand</dependencies> </pomElements> </configuration> <executions> <execution> <id>flatten.clean</id> <goals> <goal>clean</goal> </goals> <phase>clean</phase> </execution> <execution> <id>flatten</id> <goals> <goal>flatten</goal> </goals> <phase>process-resources</phase> </execution> </executions> </plugin> <plugin> <groupId>org.sonatype.plugins</groupId> <artifactId>nexus-staging-maven-plugin</artifactId> <version>1.7.0</version> <extensions>true</extensions> <configuration> <serverId>ossrh</serverId> <nexusUrl>https://s01.oss.sonatype.org/</nexusUrl> <autoReleaseAfterClose>true</autoReleaseAfterClose> </configuration> </plugin> </plugins> </pluginManagement> <plugins> <plugin> <groupId>com.diffplug.spotless</groupId> <artifactId>spotless-maven-plugin</artifactId> <version>${spotless.version}</version> <configuration> <formats> <format> <includes> <include>*.md</include> <include>.gitignore</include> </includes> <trimTrailingWhitespace /> <endWithNewline /> <indent> <tabs>true</tabs> <spacesPerTab>4</spacesPerTab> </indent> </format> </formats> <!-- define a language-specific format --> <java> <eclipse> <!--suppress UnresolvedMavenProperty --> <file>${maven.multiModuleProjectDirectory}/formatter.xml</file> </eclipse> <removeUnusedImports /> <licenseHeader> <!--suppress UnresolvedMavenProperty --> <file>${maven.multiModuleProjectDirectory}/license-header</file> </licenseHeader> <importOrder> <!--suppress UnresolvedMavenProperty --> <file>${maven.multiModuleProjectDirectory}/spotless.importorder</file> </importOrder> </java> <pom> <sortPom> <encoding>UTF-8</encoding> <keepBlankLines>true</keepBlankLines> <indentBlankLines>false</indentBlankLines> <nrOfIndentSpace>2</nrOfIndentSpace> <expandEmptyElements>false</expandEmptyElements> <spaceBeforeCloseEmptyElement>true</spaceBeforeCloseEmptyElement> <sortDependencies>groupId,artifactId</sortDependencies> <sortDependencyExclusions>groupId,artifactId</sortDependencyExclusions> <sortDependencyManagement>groupId,artifactId</sortDependencyManagement> <sortPlugins>groupId,artifactId</sortPlugins> <sortProperties>false</sortProperties> <sortModules>true</sortModules> <sortExecutions>true</sortExecutions> <predefinedSortOrder>recommended_2008_06</predefinedSortOrder> </sortPom> </pom> <ratchetFrom>origin/main</ratchetFrom> </configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-javadoc-plugin</artifactId> <version>3.7.0</version> <executions> <execution> <id>attach-javadocs</id> <goals> <goal>jar</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-source-plugin</artifactId> <version>3.3.1</version> <executions> <execution> <id>attach-sources</id> <goals> <goal>jar-no-fork</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>flatten-maven-plugin</artifactId> <version>1.6.0</version> <configuration> <updatePomFile>true</updatePomFile> <flattenMode>resolveCiFriendliesOnly</flattenMode> </configuration> <executions> <execution> <id>flatten.clean</id> <goals> <goal>clean</goal> </goals> <phase>clean</phase> </execution> <execution> <id>flatten</id> <goals> <goal>flatten</goal> </goals> <phase>process-resources</phase> </execution> </executions> </plugin> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>versions-maven-plugin</artifactId> <version>2.16.2</version> <configuration> <ruleSet> <rule> <groupId>*</groupId> <ignoreVersion> <type>regex</type> <version>.+-(alpha|Alpha|beta|Beta|RC).*</version> </ignoreVersion> </rule> </ruleSet> </configuration> </plugin> <plugin> <groupId>org.jacoco</groupId> <artifactId>jacoco-maven-plugin</artifactId> <version>0.8.12</version> <executions> <execution> <id>prepare-agent</id> <goals> <goal>prepare-agent</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>org.sonatype.plugins</groupId> <artifactId>nexus-staging-maven-plugin</artifactId> <configuration> <serverId>ossrh</serverId> <nexusUrl>https://s01.oss.sonatype.org/</nexusUrl> <autoReleaseAfterClose>true</autoReleaseAfterClose> </configuration> </plugin> </plugins> </build> <profiles> <profile> <id>deployment</id> <activation> <activeByDefault>false</activeByDefault> </activation> <modules> <module>framework</module> <module>pipeline-core</module> </modules> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <executions> <execution> <id>default-testCompile</id> <goals> <goal>testCompile</goal> </goals> <phase>test-compile</phase> <configuration> <skip>true</skip> </configuration> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-gpg-plugin</artifactId> <executions> <execution> <id>sign-artifacts</id> <goals> <goal>sign</goal> </goals> <phase>verify</phase> <configuration> <keyname>2673EE7DF64D33426A93D642E88F0DA2FB06A126</keyname> </configuration> </execution> </executions> </plugin> </plugins> </build> </profile> <profile> <id>complete</id> <activation> <activeByDefault>true</activeByDefault> </activation> <modules> <module>framework</module> <module>pipeline-core</module> <module>report</module> </modules> </profile> </profiles> </project>\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/arosics",
            "repo_link": "https://git.gfz-potsdam.de/danschef/arosics",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/atomec",
            "repo_link": "https://github.com/atomec-project/atoMEC",
            "content": {
                "codemeta": "",
                "readme": "![image](https://github.com/atomec-project/atoMEC/blob/develop/docs/source/img/logos/atoMEC_horizontal2.png) # atoMEC: Average-Atom Code for Matter under Extreme Conditions [![docs](https://github.com/atomec-project/atoMEC/actions/workflows/gh-pages.yml/badge.svg)](https://github.com/atomec-project/atoMEC/actions/workflows/gh-pages.yml) [![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/release/python-3100/) [![image](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause) [![codecov](https://codecov.io/gh/atomec-project/atoMEC/branch/develop/graph/badge.svg?token=V66CJJ3KPI)](https://codecov.io/gh/atomec-project/atoMEC) [![CodeFactor](https://www.codefactor.io/repository/github/atomec-project/atomec/badge)](https://www.codefactor.io/repository/github/atomec-project/atomec) atoMEC is a python-based average-atom code for simulations of high energy density phenomena such as in warm dense matter. It is designed as an open-source and modular python package. atoMEC uses Kohn-Sham density functional theory, in combination with an average-atom approximation, to solve the electronic structure problem for single-element materials at finite temperature. More information on the average-atom methodology and Kohn-Sham density functional theory can be found (for example) in this [paper](https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.4.023055) and references therein. This repository is structured as follows: ``` ├── atoMEC : source code ├── docs : sphinx documentation ├── examples : simple examples to get you started with the package └── tests : CI tests ``` ## Installation The latest stable release of `atoMEC` can be installed via `pip`. It is first necessary to install the `libxc` package from a tarball source, because it currently has no official wheels distribution on PyPI. This step takes some time. ```sh $ pip install https://gitlab.com/libxc/libxc/-/archive/6.2.2/libxc-6.2.2.tar.gz $ pip install atoMEC ``` Note that atoMEC does not (yet) support Windows installation (please see the section below on supported operating systems). Read on for instructions on how to install `atoMEC` from source, using the recommended `pipenv` installation route. ### Installation via `pipenv` First, clone the atoMEC repository and ``cd`` into the main directory. * It is recommended to install atoMEC inside a virtual environment. Below, we detail how to achive this with [pipenv](https://pypi.org/project/pipenv/). This route is recommended because `pipenv` automatically creates a virtual environment and manages dependencies. Note that `pyblibxc` is automatically installed in this case, so there is no need to install it separately. 1. First, install `pipenv` if it is not already installed, for example via `pip install pipenv` (or see [pipenv](https://pypi.org/project/pipenv/) for installation instructions) 2. Next, install `atoMEC`'s dependencies with `pipenv install` (use `--dev` option to install the test dependencies in the same environment) 3. Use `pipenv shell` to activate the virtual environment 4. Install atoMEC with `pip install atoMEC` (for developers: `pip install -e .`) 5. Now run scripts from inside the `atoMEC` virtual environment, e.g. `python examples/simple.py` * Run the tests (see Testing section below) and report any failures (for example by raising an issue). ### Supported operating systems * **Linux and macOS**: atoMEC has been installed on various linux distributions and macOS, and is expected to work for most distributions and versions * **Windows**: atoMEC does **not** support Windows installation. This is due to the dependency on `pylibxc` which currently lacks Windows support. We are looking into ways to make the dependency on `pylibxc` optional, in order to allow installation on Windows. However, this is not currently a priority. ### Supported Python versions * atoMEC has been tested and is expected to work for all Python versions >= 3.8 and <= 3.12 * atoMEC does not work for Python <= 3.7 * Until 09.10.2023 (release 1.4.0), all development and CI testing was done with Python 3.8. As of this date, development and CI testing is done with Python 3.12. * Python 3.12 is therefore the recommended version for atoMEC >= 1.4.0, since this is used for the current testing and development environment ## Running You can familiarize yourself with the usage of this package by running the example scripts in `examples/`. ## Contributing to atoMEC We welcome your contributions, please adhere to the following guidelines when contributing to the code: * In general, contributors should develop on branches based off of `develop` and merge requests should be to `develop` * Please choose a descriptive branch name * Merges from `develop` to `master` will be done after prior consultation of the core development team * Merges from `develop` to `master` are only done for code releases. This way we always have a clean `master` that reflects the current release * Code should be formatted using [black](https://pypi.org/project/black/) style ## Testing * First, install the test requirements (if not already installed in the virtual env with `pipenv install --dev`): ```sh # activate environment first (optional) $ pipenv shell # install atoMEC as editable project in current directory (for developers) $ pip install -e .[tests] # alternatively install package from PyPI with test dependencies $ pip install atoMEC[tests] ``` * To run the tests: ```sh $ pytest --cov=atoMEC --random-order tests/ ``` ### Build documentation locally (for developers) Install the prerequisites: ```sh $ pip install -r docs/requirements.txt ``` 1. Change into `docs/` folder. 2. Run `make apidocs`. 3. Run `make html`. This creates a `_build` folder inside `docs`. You may also want to use `make html SPHINXOPTS=\"-W\"` sometimes. This treats warnings as errors and stops the output at first occurrence of an error (useful for debugging rST syntax). 4. Open `docs/_build/html/index.html`. 5. `make clean` if required (e.g. after fixing errors) and building again. ## Developers ### Scientific Supervision - Attila Cangi ([Center for Advanced Systems Understanding](https://www.casus.science/)) - Eli Kraisler ([Hebrew University of Jerusalem](https://en.huji.ac.il/en)) ### Core Developers and Maintainers - Tim Callow ([Center for Advanced Systems Understanding](https://www.casus.science/)) - Daniel Kotik ([Center for Advanced Systems Understanding](https://www.casus.science/)) ### Contributions (alphabetical) - Nathan Rahat ([Hebrew University of Jerusalem](https://en.huji.ac.il/en)) - Ekaterina Tsvetoslavova Stankulova ([Center for Advanced Systems Understanding](https://www.casus.science/)) ## Citing atoMEC If you use code from this repository in a published work, please cite 1. T. J. Callow, D. Kotik, E. Kraisler, and A. Cangi, \"atoMEC: An open-source average-atom Python code\", _Proceedings of the 21st Python in Science Conference_, edited by Meghann Agarwal, Chris Calloway, Dillon Niederhut, and David Shupe (2022), pp. 31 - 39 2. The DOI corresponding to the specific version of atoMEC that you used (DOIs are listed at [Zenodo.org](https://doi.org/10.5281/zenodo.5205718))\n",
                "dependencies": "[[source]] url = \"https://pypi.python.org/simple\" verify_ssl = true name = \"pypi\" [packages] numpy = \">=1.20.3\" scipy = \">=1.6.3\" mendeleev = \">=0.7.0\" tabulate = \">=0.8.9\" joblib = \">=1.0.1\" pylibxc = {file = \"https://gitlab.com/libxc/libxc/-/archive/6.2.2/libxc-6.2.2.tar.gz\"} [requires] python_version = \"3.12\" [dev-packages] pytest = \">=7.1.3\" pytest-cov = \">=4.0.0\" pytest-random-order = \">=1.0.4\" pytest-lazy-fixture = \">=0.6.3\" flake8 = \">=6.1.0\" pydocstyle = \">=6.3.0\" black = \">=23.11.0\"\n[build-system] requires = [ \"setuptools>=53\", \"wheel\" ] build-backend = \"setuptools.build_meta\"\nnumpy>=1.20.3 scipy>=1.6.3 mendeleev>=0.7.0 tabulate>=0.8.9 joblib>=1.0.1\nfrom setuptools import setup, find_packages with open(\"README.md\") as f: readme = f.read() with open(\"LICENSE\") as f: license = f.read() extras = { \"dev\": [\"bump2version\"], \"docs\": open(\"docs/requirements.txt\").read().splitlines(), \"tests\": open(\"tests/requirements.txt\").read().splitlines(), } setup( name=\"atoMEC\", version=\"1.4.0\", description=\"KS-DFT average-atom code\", long_description=readme, long_description_content_type=\"text/markdown\", author=\"Tim Callow et al.\", author_email=\"t.callow@hzdr.de\", url=\"https://github.com/atomec-project/atoMEC\", license=license, packages=find_packages(exclude=(\"tests\", \"docs\", \"examples\")), install_requires=open(\"requirements.txt\").read().splitlines(), extras_require=extras, python_requires=\">=3.6\", )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/autogaita",
            "repo_link": "https://github.com/mahan-hosseini/AutoGaitA",
            "content": {
                "codemeta": "",
                "readme": "![AutoGaitA](https://github.com/mahan-hosseini/AutoGaitA/blob/main/autogaita/resources/logo.png?raw=true) ![Repository Active](https://www.repostatus.org/badges/latest/active.svg) [![Test AutoGaitA](https://github.com/mahan-hosseini/AutoGaitA/actions/workflows/autogaita_test_and_black.yml/badge.svg)](https://github.com/mahan-hosseini/AutoGaitA/actions/workflows/autogaita_test_and_black.yml) ![Python](https://img.shields.io/badge/python-v3.10+-blue.svg) [![PyPI - Version](https://img.shields.io/pypi/v/autogaita)](https://pypi.org/project/autogaita/) ![license: GPL v3](https://img.shields.io/badge/license-GPLv3-blue.svg) [![paper: biorxiv](https://img.shields.io/badge/paper-biorxiv-blue)](https://doi.org/10.1101/2024.04.14.589409) ![Black](https://img.shields.io/badge/code%20style-black-000000.svg) [![X URL](https://img.shields.io/twitter/url?url=https%3A%2F%2Fx.com%2Fautogaita&style=social&label=updates)](https://x.com/autogaita) # Automated Gait Analysis in Python 🐸 - AutoGaitA simplifies, accelerates, and standardises gait analyses (as well as the analysis of other rhythmic behaviours) after body posture tracking in 2D with [DeepLabCut](https://github.com/DeepLabCut/DeepLabCut) and [SLEAP](https://github.com/talmolab/sleap) or marker-based as well as marker-less (such as [Simi Motion](http://www.simi.com/en/products/movement-analysis/simi-motion-2d3d.html?type=rss%2F)) methods for obtaining 3D coordinates. - AutoGaitA's first-level tools provide a wide range of automated kinematic analyses for each input video and AutoGaitA Group allows the comparison of up to six groups. - AutoGaitA enables comparisons to be made across experimental conditions, species, disease states or genotypes. - Despite being developed with gait data, AutoGaitA can be utilised for the analysis of any motor behaviour. ## Getting Started ***Note!** [Our documentation](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?usp=sharing) provides step-by-step walkthroughs of how to install autogaita for **[Windows](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?tab=t.0#heading=h.28j6wu2vamre)** and **[Mac](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?tab=t.0#heading=h.ljmdh7hfayyx)*** It is strongly recommended that a separate virtual environment for AutoGaitA is created (note that the approach below creates the virtual environment to your current directory): - Create the virtual environment: - `python -m venv env_gaita` - After creation, activate the virtual environment via: - *Windows:* `env_gaita\\Scripts\\activate` - *Mac:* `source env_gaita/bin/activate` - Once activated, install AutoGaitA in the virtual environment via pip: `pip install autogaita`. - Access the main user interface via: `python -m autogaita`. - To update to the latest release (see the *Releases* panel on the right for the latest release) activate the virtual environment and: `pip install autogaita -U`. ## Demo Video *Check out the video below for a demonstration of AutoGaitA's main workflow!* <p><a href=\"https://youtu.be/_HIZVuUzpzk?feature=shared\"> <img src=\"https://github.com/mahan-hosseini/AutoGaitA/blob/main/autogaita/resources/pic_to_demo_for_repo.png\" width=\"550\"> ## Tutorials & Examples ### Walkthrough Tutorial Videos **[The AutoGaitA YouTube Channel](https://youtube.com/playlist?list=PLCn5T7K_H8K56NIcEsfDK664OP7cN_Bad&si=mV5p2--nYvbofkPh) provides tutorials for file preparation and instructions on how to use AutoGaitA. This includes in-depth explanations of all details, (main & advanced) configurations, possibilities, and outputs.** *Please note that tutorial videos might not always reflect the most up-to-date version of our toolbox, especially in the beginning when things are regularly changing. We will make sure to record new videos whenever there are major changes though. Last tutorial-update was with v0.4.0. (August 2024).* ### Example Data We provide an example dataset in the **example data** folder of this repository, with a set of mice walking over differently wide beams and both the beam as well as body coordinates being tracked with DLC. Note that this dataset was used in our tutorial videos introducing *AutoGaitA DLC*, *AutoGaitA Group* and in our video explaining file preparation for *AutoGaitA DLC*. We further provide a **group** folder there that can be used alongside the *AutoGaitA Group* tutorial to confirm that users generate the same set of results following our instructions. ### Annotation Table Examples and Templates Annotation Table example and template files for *AutoGaitA DLC* and *AutoGaitA Universal 3D* can be found in the [**annotation tables**](https://github.com/mahan-hosseini/AutoGaitA/tree/main/annotation%20tables) folder of this repository. Users are advised to read the **General Recommendations** section of that folder, use the template to enter their data's timestamp information and to then compare the resulting table with our example to check formatting. Users working with ImageJ/FIJI are encouraged to check out the [AnnotationTable-Plugin](https://github.com/luca-flemming/AnnotationTable-Plugin) developed by our contributor Luca Flemming. ## Documentation **[The AutoGaitA Documentation](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?usp=sharing) provides complete guidelines on installation, file preparation, AutoGaitA GUIs, using AutoGaitA via the command line, installing FFmpeg for rotating 3D PCA videos, lists known issues and FAQ.** ## Two important options ### Custom joints & angles **We strongly advise** users to pay attention to the *custom joints and angles* windows of AutoGaitA's first level toolboxes. Please see the relevant links below. These windows allow users to customise which columns of their data should be analysed and how angles should be computed. By default, *AutoGaitA DLC* and *AutoGaitA Universal 3D* implement standard values for mouse and human locomotion, respectively. If your analysis deviates from these standards (e.g. by focussing on another behaviour or a different species) **you must change these values!** **Find out more about *AutoGaitA's custom joints and angles:*** - [YouTube - AutoGaitA DLC Advanced Configuration](https://youtu.be/MP9g9kXRE_Q?feature=shared) - [YouTube - AutoGaitA Universal 3D (prev. called Simi)](https://youtu.be/rTG-Fc9XI9g?feature=shared) - [Documentation - AutoGaitA DLC](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?tab=t.0#heading=h.20bg7b7ymt0b) - [Documentation - AutoGaitA Universal 3D](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?tab=t.0#heading=h.uz61bpmua7qz) ### Bin number of step cycle normalisation An important step in AutoGaitA is normalising step cycles (or instances of other behaviours) to a uniform length before calculating the video-level average. This uniform length is called *bin number*, must be set by users and defaults to a value of 25. Step cycles are normalised via averaging temporally adjacent data points if their original length was larger than the bin number and repeating values if they were shorter originally. Examples are provided here: - [Documentation/AutoGaitA DLC/Main Configuration/Option #6](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?tab=t.0#heading=h.bboivsfqr2lz). **We strongly advise** users to think carefully about an appropriate bin number for their datasets. The correct value varies and depends strongly on the studied species, behaviour and the frame rate of cameras. ## 2D Kinematics in AutoGaitA Universal 3D **We strongly advise** AutoGaitA Universal 3D users to carefully consider how our toolbox computes kinematics in 2D, i.e., angles along the y/z-plane and velocities along the y-dimensions. For more information, please see the corresponding **[important note on 2D kinematics in our documentation](https://docs.google.com/document/d/1iQxSwqBW3VdIXHm-AtV4TGlgpJPDldogVx6qzscsGxA/edit?tab=t.0#heading=h.xc5ome7hzfid)**. ## Analysing other behaviours - AutoCyclA 🚴 Even though AutoGaitA's main focus is to automate and standardise gait analyses, our toolbox can be used to automate the analyses of any rhythmic behaviour of interest. For a proof-of-principle demonstration and an introduction of the general workflow of such analyses, see **[AutoCyclA - Automated Cycling Analysis with AutoGaitA.](https://github.com/mahan-hosseini/AutoGaitA/tree/main/autocycla)** ## Updating AutoGaitA It is strongly recommended that AutoGaitA is kept up to date since new features and important bugfixes are provided regularly. AutoGaitA's cfg files and dictionaries sometimes change as a result, which means that previously generated first-level *Results* folders cannot always be analysed with AutoGaitA Group after an update. In such cases, it is recommended to re-run first-level analyses. We document each version's cfg-changes in [AutoGaitA Releases](https://github.com/mahan-hosseini/AutoGaitA/releases), which is particularly relevant for users wrapping custom scripts around AutoGaitA's functions. ## Reference If you use this code or data please [cite our preprint](https://www.biorxiv.org/content/10.1101/2024.04.14.589409v1). ## License AutoGaitA is licensed under [GPL v3.0](https://github.com/mahan-hosseini/AutoGaitA/blob/main/LICENSE) and Forschungszentrum Jülich GmbH holds all copyrights. The AutoGaitA software is provided without warranty of any kind, express or implied, including, but not limited to, the implied warranty of fitness for a particular purpose. ## Authors [Mahan Hosseini](https://github.com/mahan-hosseini) ## Contributors [Luca Flemming](https://github.com/luca-flemming) - Undergraduate Student [Nicholas del Grosso](https://github.com/nickdelgrosso) - RSE Advisor ## Contributing If you would like to contribute to the AutoGaitA toolbox, feel free to open a pull request or contact us at autogaita@fz-juelich.de! We are looking forward to your input and ideas 😊 ## Archive We have archived the resources of outdated AutoGaitA versions here: - v0.4.1 - [Documentation](https://docs.google.com/document/d/1Y4wrrsjs0ybLDKPzE2LAatqPDq9jtwjIuk4M0jRZ3wE/edit?usp=sharing) - v0.3.1 - [YouTube Tutorials](https://youtube.com/playlist?list=PLCn5T7K_H8K776DLuXKoPsUpI6Yb0NU33&si=7ZAAvcrPxR7WsB8a) & [Documentation](https://docs.google.com/document/d/11mJd7jUHk7joQ0BdZT98CJRrIANdyosMQMJGFtp6yR4/edit?usp=sharing)\n",
                "dependencies": "# import from setuptools import setup, find_packages import platform # list of across-platform dependencies install_requires = [ \"customtkinter>=5.2\", \"pandas>=2.0\", \"numpy>=1.24\", \"seaborn>=0.13\", \"matplotlib>=3.7\", \"scikit-learn>=1.2\", \"pingouin>=0.5\", \"scipy>=1.11\", \"ffmpeg-python>=0.2\", \"openpyxl>=3.1\", \"pillow>=10.3\", \"h5py>=3.11\", ] # add platform-specific dependencies if platform.system() == \"Darwin\": install_requires.append(\"pyobjc\") # call setup function setup( name=\"autogaita\", python_requires=\">=3.10\", version=\"1.3.0rc\", # rc == release candidate (before release is finished) author=\"Mahan Hosseini\", description=\"Automatic Gait Analysis in Python. A toolbox to streamline and standardise the analysis of kinematics across species after ML-based body posture tracking. Despite being optimised for gait analyses, AutoGaitA has the potential to be used for any kind of kinematic analysis.\", packages=find_packages(), include_package_data=True, package_data={\"\": [\"*.txt\", \"*.rst\", \"*.png\", \"*.icns\", \"*.ico\", \"*.json\"]}, install_requires=install_requires, extras_require={\"dev\": [\"pytest\", \"hypothesis\"]}, license=\"GPLv3\", url=\"https://github.com/mahan-hosseini/AutoGaitA/\", )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/autopq",
            "repo_link": "https://github.com/SMEISEN/AutoPQ",
            "content": {
                "codemeta": "",
                "readme": "# AutoPQ: Automated point forecast-based quantile forecasts AutoPQ addresses three challenges: - Many state-of-the-art forecasting methods are still point forecasts and remain unused for probabilistic forecasts - According to the no-free-lunch theorem, no forecasting method exists that excels in all forecasting tasks - Smart grid applications typically require forecasts with customized probabilistic characteristics ## Methodology The underlying idea of AutoPQ is to generate a probabilistic forecast based on an arbitrary point forecast using a conditional Invertible Neural Network (cINN) and to make corresponding design decisions automatically, aiming to increase the probabilistic performance. To account for different computing systems and performance requirements, two variants are available: AutoPQ-default suitable for standard computing systems achieving competitive forecasting performance, and AutoPQ-advanced requiring High-Performance Computing (HPC) systems to further increase forecasting performance for smart grid applications with high decision costs. ![concept_pipeline_github](https://github.com/SMEISEN/AutoPQ/assets/33990691/40344260-77ee-4515-9964-16875b9383d7) ## Installation To install this project, perform the following steps. 1) Clone the project 2) Open a terminal of the virtual environment where you want to use the project 3) cd AutoPQ 4) pip install . or pip install -e . if you want to install the project editable. ## How to use Exemplary evaluations using AutoPQ are given in the examples folder. ### Hyperparameter optimization - The default configuration optimizes the sampling hyperparameter $\\lambda_\\text{q}$ for generating samples in the latent space of the cINN. - The advanced configuration simultaneously optimizes the point forecasting method's hyperparameters $\\boldsymbol{\\lambda_\\text{p}}$ and the sampling hyperparameter $\\lambda_\\text{q}$. ### Evaluation types The evaluation trains the models using the training data sub-set, optimizes hyperparameters based on the validation data sub-set, and makes probabilistic forecasts for the test data sub-set. ## Citation If you use this method please cite the corresponding papers: > Kaleb Phipps, Stefan Meisenbacher, Benedikt Heidrich, Marian Turowski, Ralf Mikut, and Veit Hagenmeyer. 2023. Loss-customised probabilistic energy time series forecasts using automated hyperparameter optimisation. In Proceedings of the 14th ACM International Conference on Future Energy Systems (e-Energy '23), Association for Computing Machinery, New York, NY, USA, 271-286. [https://doi.org/10.1145/3575813.3595204](https://doi.org/10.1145/3575813.3595204) > Stefan Meisenbacher et al. 2024. AutoPQ: Automated point forecast-based quantile forecasts. In preparation. ## Funding This project is funded by the Helmholtz Association under the Program \"Energy System Design\" and the Helmholtz Association's Initiative and Networking Fund through Helmholtz AI. ## References The cINN is based on: > B. Heidrich, M. Turowski, K. Phipps, K. Schmieder, W. Süß, R. Mikut, and V. Hagenmeyer, \"Controlling non-stationarity and periodicities in time series generation using conditional invertible neural networks\", Applied Intelligence, vol. 53, no. 8, pp. 8826-8843, 2023. Generating probabilistic forecasts by sampling in the cINN's latent space is based on: > K. Phipps, B. Heidrich, M. Turowski, M. Wittig, R. Mikut, and V. Hagenmeyer, \"Generating probabilistic forecasts from arbitrary point forecasts using a conditional invertible neural network\", Applied Intelligence, 2024. Optimization of the sampling hyperparameter is performed using [Hyperopt](https://github.com/hyperopt/hyperopt) > J. Bergstra, D. Yamins, and D. Cox, \"Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures\", in Proceedings of the 30th International Conference on Machine Learning, ser. ICML '13, Proceedings of Machine Learning Research, PMLR, 2013, pp. 115-123. Optimization of the point forecasting method's hyperparameters is performed using [Propulate](https://github.com/Helmholtz-AI-Energy/propulate) > O. Taubert, M. Weiel, D. Coquelin, A. Farshian, C. Debus, A. Schug, A. Streit, and M. Götz, \"Massively parallel genetic optimization through asynchronous propagation of populations\", in High Performance Computing, A. Bhatele, J. Hammond, M. Baboulin, and C. Kruse, Eds., Cham, Switzerland: Springer Nature, 2023, pp. 106-124. The Load-BW data is taken from the Open Power System Data (OPSD) portal: > F. Wiese et al., \"Open Power System Data: Frictionless data for electricity system modelling\", Applied Energy, vol. 236, pp. 401-409, 2019. The Load-GCP data set is taken from the UCI Machine Learning Repository: > A. Trindade, Electricity load diagrams 2011-2014, UCI Machine Learning Repository, 2015. The Mobility data set is taken from the UCI Machine Learning Repository: > H. Fanaee-T, Bike sharing dataset, UCI Machine Learning Repository, 2013. The Price, PV, and WP data are from the price, solar power, and wind power forecasting tracks of the Global Energy Forecasting Competition (GEFCom) 2014: > T. Hong, P. Pinson, S. Fan, H. Zareipour, A. Troccoli, and R. J. Hyndman, \"Probabilistic energy forecasting: Global energy forecasting competition 2014 and beyond\", International Journal of Forecasting, vol. 32, no. 3, pp. 896-913, 2016.\n",
                "dependencies": "git+https://github.com/SMEISEN/pyWATTS.git@453133590beb64866ec7576fa58bebaf38a84ed5#egg=pywatts git+https://github.com/SMEISEN/propulate.git@main#egg=propulate git+https://github.com/SMEISEN/msvr@master#egg=msvr git+https://github.com/SMEISEN/sktime@main#egg=sktime pytorch-forecasting==0.10.3 pytorch-lightning==1.9.0 properscoring==0.1 FrEIA==0.2 gluonts==0.12.2 tensorflow>=2 numpy<1.24 xgboost==1.7.3 scikit-learn==1.1.3 hyperopt==0.2.7 pmdarima==2.0.3 tbats==1.1.3 pvlib==0.9.3 --extra-index-url https://download.pytorch.org/whl/cu116 torch==1.13.1+cu116\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/autopv",
            "repo_link": "https://github.com/SMEISEN/AutoPV",
            "content": {
                "codemeta": "",
                "readme": "# AutoPV: Automated photovoltaic forecasts with limited information using an ensemble of pre-trained models AutoPV addresses three challenges: - Missing information about the PV mounting configuration (tilt and azimuth angles, mixed-oriented configurations) - Missing or limited training data for PV model design (cold-start problem) - Adaption to drifting PV power generation capabilities during operation (e.g. age-related degradation, soiling, maintenance) ## Methodology The underlying idea of AutoPV is to describe the arbitrary mounting configuration of a new PV plant as a convex linear combination of outputs from a sufficiently diverse ensemble pool of PV models of the same region. AutoPV incorporates three steps: i) create the ensemble model pool, ii) form the ensemble output by an optimally weighted sum of the scaled model outputs in the pool, and iii) rescale the ensemble output with the new PV plant's peak power rating. ![pipeline](https://github.com/SMEISEN/AutoPV/assets/33990691/56363d4b-5418-427b-b723-bf14255804ce) ## Installation To install this project, perform the following steps. 1) Clone the project 2) Open a terminal of the virtual environment where you want to use the project 3) cd AuroPV 4) pip install . or pip install -e . if you want to install the project editable. ## How to use Exemplary evaluations using AutoPV are given in the examples folder. ### Model pools - The default model pool is based on physical-inspired modeling and uses 12 models (tilt: 15°, 45°, 75°, azimuth: 0°, 90°, 180°, 270°). The default model pool is suitable for situations where no data of nearby PV plants are available. - The nearby plants model pool uses machine learning-based modeling to create the models using data from nearby PV plants. The nearby plants model pool can represent shading if it is present in the nearby PV plants. ### Evaluation types - The offline evaluation optimizes the ensemble weights using the entire training data and does not adapt the weights over time. - The online evaluation cyclically adapts the weights based on the testing data (cold-start) and does not require training data. ## Citation If you use this method please cite the corresponding paper: > Stefan Meisenbacher, Benedikt Heidrich, Tim Martin, Ralf Mikut, and Veit Hagenmeyer. 2023. AutoPV: Automated photovoltaic forecasts with limited information using an ensemble of pre-trained models. In Proceedings of the Fourteenth ACM International Conference on Future Energy Systems (e-Energy '23). Association for Computing Machinery, New York, NY, USA, 386-414. https://doi.org/10.1145/3575813.3597348 ## Funding This project is funded by the Helmholtz Association under the Program \"Energy System Design\" and the Helmholtz Association's Initiative and Networking Fund through Helmholtz AI. ## References The example data includes weather measurements from DWD: > Deutscher Wetterdienst. 2023. Historical 10-minute station observations of solar incoming radiation, longwave downward radiation, pressure, air temperature, and mean wind speed for Germany. https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes\n",
                "dependencies": "-e git+https://github.com/SMEISEN/pyWATTS.git@453133590beb64866ec7576fa58bebaf38a84ed5#egg=pywatts hyperopt=0.2.7 ray=2.0.1 pvlib=0.9.3\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/autowp",
            "repo_link": "https://github.com/SMEISEN/AutoWP",
            "content": {
                "codemeta": "",
                "readme": "# AutoWP: Automated wind power forecasts with limited computing resources using an ensemble of diverse wind power curves AutoWP addresses two challenges: - Achieving good accuracy in Wind Power (WP) forecasting with low computational effort - Handling regular or irregular interventions in the WP generation capabilities ## Methodology The underlying idea of AutoWP is to represent a new WP turbine as a convex linear combination of WP curves from a sufficiently diverse ensemble. The method consists of three steps: i) create the ensemble of normalized WP curves, ii) form the normalized ensemble WP curve by the optimally weighted sum of the WP curves in the ensemble, and iii) re-scale the ensemble WP curve with the new WP turbine's peak power rating. ![autowp_pipeline](https://github.com/SMEISEN/AutoWP/assets/33990691/46b4f23c-4a8f-423e-8e20-a24e2b02ff5d) ## Installation To install this project, perform the following steps. 1) Clone the project 2) Open a terminal of the virtual environment where you want to use the project 3) cd AutoWP 4) pip install . or pip install -e . if you want to install the project editable. ## How to use Exemplary evaluations using AutoWP are given in the examples folder. ### Model pool The model pool is based on a selection of 10 WP curves from the [windpowerlib](https://github.com/wind-python/windpowerlib). The selection reduces redundancy and preserves diversity. Since these WP curves provided by turbine Original Equipment Manufacturers (OEMs) have the hub height as reference height, height correction of the wind speed forecast based on the wind profile power law is used. ### Evaluation types The offline evaluation optimizes the ensemble weights using the entire training data and does not adapt the weights over time. ## Citation If you use this method please cite the corresponding paper: > S. Meisenbacher et al, \"AutoWP: Automated wind power forecasts with limited computing resources using an ensemble of diverse wind power curves\", 2024, in preparation. ## Funding This project is funded by the Helmholtz Association under the Program \"Energy System Design\" and the Helmholtz Association?s Initiative and Networking Fund through Helmholtz AI. ## References The example data is from the wind power forecasting track of the Global Energy Forecasting Competition (GEFCom) 2014: > T. Hong, P. Pinson, S. Fan, H. Zareipour, A. Troccoli, and R. J. Hyndman, \"Probabilistic energy forecasting: Global energy forecasting competition 2014 and beyond\", International Journal of Forecasting, vol. 32, no. 3, pp. 896-913, 2016.\n",
                "dependencies": "git+https://github.com/SMEISEN/pyWATTS.git@453133590beb64866ec7576fa58bebaf38a84ed5#egg=pywatts windpowerlib==0.2.1\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/aviator",
            "repo_link": "https://github.com/CCB-SB/Aviator",
            "content": {
                "codemeta": "",
                "readme": "# Aviator ### [https://ccb-compute2.cs.uni-saarland.de/aviator](https://ccb-compute2.cs.uni-saarland.de/aviator) Aviator is a web-server monitoring the availability of other published web-servers. It allows researchers to monitor their own tools or to asses if a tool they would like to access is temporarily or permanently offline. Aviator is composed of two modules: ### - [Tool List](https://ccb-compute2.cs.uni-saarland.de/aviator/tools): web-servers collected automatically from literature ### - [Aviator-enabled](https://ccb-compute2.cs.uni-saarland.de/aviator/aviator-enabled): web-servers manually added by their authors The web-server URL or an API endpoint provided by the authors are queried twice per day. In addition to providing an availability overview we provide the possibility for authors to be notified if their webserver is offline for an unexpected period of time. To add your published web-server to Aviator a simple [API endpoint](https://ccb-compute2.cs.uni-saarland.de/aviator/aviator-enable) and a [registration](https://ccb-compute2.cs.uni-saarland.de/aviator/register) is needed. ## License [MIT © CCB-SB](/LICENSE)\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/base-repo",
            "repo_link": "https://github.com/kit-data-manager/base-repo",
            "content": {
                "codemeta": "",
                "readme": "# KIT Data Manager - Base Repository Service [![Build Status](https://github.com/kit-data-manager/base-repo/actions/workflows/gradle.yml/badge.svg)](https://github.com/kit-data-manager/base-repo/actions/workflows/gradle.yml) [![Codecov](https://codecov.io/gh/kit-data-manager/base-repo/branch/master/graph/badge.svg)](https://codecov.io/gh/kit-data-manager/base-repo) ![License](https://img.shields.io/github/license/kit-data-manager/base-repo.svg) [![SQAaaS badge shields.io](https://img.shields.io/badge/Docker-white?logo=docker)](https://github.com/kit-data-manager/base-repo/pkgs/container/base-repo) [![SQAaaS badge shields.io](https://img.shields.io/badge/sqaaas%20software-silver-lightgrey)](https://api.eu.badgr.io/public/assertions/onNKx_lhTn68bPKnMAg-eQ \"SQAaaS silver badge achieved\") [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7660036.svg)](https://doi.org/10.5281/zenodo.7660036) This project contains the repository service microservice for the KIT DM infrastructure. The service provides data resource management, e.g. register DataCite-oriented metadata and upload/download content to data resources. ## How to build In order to build this microservice you'll need: * Java SE Development Kit 17 or higher After obtaining the sources change to the folder where the sources are located perform the following steps: ```bash user@localhost:/home/user/base-repo$ ./gradlew -Dprofile=minimal build Running gradle version: 7.4.2 Building base-repo version: 1.5.5 JDK version: 11 Using minimal profile for building base-repoo <-------------> 0% EXECUTING [0s] [...] user@localhost:/home/user/base-repo$ ``` The Gradle wrapper will now take care of downloading the configured version of Gradle, checking out all required libraries, build these libraries and finally build the base-repo microservice itself. As a result, a fat jar containing the entire service is created at 'build/libs/base-repo.jar'. ## How to start ### Prerequisites * PostgreSQL 9.1 or higher * RabbitMQ 3.7.3 or higher (in case you want to use the messaging feature, which is recommended) * Elastic 8.X or higher (in case you want to use the search feature) ### Setup Before you are able to start the repository microservice, you have provide a configuration file according to your local setup. Therefor, copy the file 'config/application-default.properties' to your project folder, rename it to 'application.properties' and customize it as required. Special attentioned should be payed to the datasource url as well as to the repository base path. Also, the property 'repo.messaging.enabled' should be changed to 'true' in case you want to use the messaging feature of the repository. As soon as you finished modifying 'application.properties', you may start the repository microservice by executing the following command inside the project folder, e.g. where the service has been built before: ```bash user@localhost:/home/user/base-repo$ ./build/libs/base-repo.jar . ____ _ __ _ _ /\\\\ / ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.7.5) [...] 1970-01-01 00:00:00.000 INFO 56918 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path '' ``` If your 'application.properties' is not located inside the project folder you can provide it using the command line argument --spring.config.location=<PATH_TO_APPLICATION.PROPERTIES> As soon as the microservice is started, you can browse to <http://localhost:8090/swagger-ui.html> in order to see available RESTful endpoints and their documentation. ### Enhanced Startup At certain points, base-repo offers and will offer extension points allowing to add custom features that are not part of the default distribution, e.g. custom message handlers. If you are familiar with software development, it might be no big deal to include an additional dependency to 'build.gradle' of base-repo. However, in some cases this might not be desirable or possible. Therefor, base-repo allows to place additional libraries required at runtime in a separate folder which is then loaded as soon as the microservice starts and made available using the dependency injection feature of Spring Boot. In order to tell Spring Boot where to look for additional libraries, you have to define an environment variable JAVA_OPTS looking as follows: ```bash export JAVA_OPTS=\"-cp .:./config:./base-repo.jar -Dloader.path=./base-repo.jar,./lib/,.\" ``` The first part '-cp' has to contain three elements divided by ':': 1. The configuration folder where your application.properties is located (this element can be omitted, if application.properties is located in the current folder), 2. the current folder, 3. and the microservice jar file. The second part '-Dloader.path' basically contains the same information as '-cp' but with the difference, that the config folder is not required, whereas the folder containing all additional libraries has to be provided, in our case it's './lib'. Please keep in mind that all arguments shown in the example assume, that you are in the same folder where your microservice jar file is located and that you start the service by calling './base-repo.jar'. If your microservice jar is located elsewhere, you should consider to provide absolute paths for all arguments above. In case you want to choose a different folder for placing your additional libraries, you have to rename it in JAVA_OPTS accordingly. What you now have to do before you start the microservice is to place additional jar files (and required dependencies!) in the 'lib' folder. At the next startup, the new functionality should be available. ## More Information * [Getting Started & Documentation](https://kit-data-manager.github.io/webpage/base-repo/index.html) * [API documentation](https://kit-data-manager.github.io/webpage/base-repo/documentation/api-docs.html) * [Docker container](https://github.com/kit-data-manager/base-repo/pkgs/container/base-repo%2Fbase-repo) * [Information about the DataCite metadata schema](https://schema.datacite.org/) ## License The KIT Data Manager is licensed under the Apache License, Version 2.0.\n",
                "dependencies": "plugins { id 'org.springframework.boot' version '3.4.2' id 'io.spring.dependency-management' version '1.1.7' id 'io.freefair.lombok' version '8.12' id 'io.freefair.maven-publish-java' version '8.12' id 'org.owasp.dependencycheck' version '12.0.1' id 'org.asciidoctor.jvm.convert' version '4.0.4' id 'net.researchgate.release' version '3.1.0' id 'com.gorylenko.gradle-git-properties' version '2.4.2' id 'java' id 'jacoco' } jar { archiveBaseName = 'base-repo' // version is defined in file 'gradle.properties' archiveVersion = System.getenv('version') } repositories { mavenLocal() mavenCentral() } //configurations { // all*.exclude module : 'spring-boot-starter-logging' //} ext { set('javersVersion', \"7.7.0\") set('springBootVersion', \"3.2.1\") set('springDocVersion', \"2.8.4\") set('keycloakVersion', \"19.0.0\") // directory for generated code snippets during tests snippetsDir = file(\"build/generated-snippets\") } println \"Running gradle version: $gradle.gradleVersion\" println \"Building ${name} version: ${version}\" println \"JDK version: ${JavaVersion.current()}\" sourceCompatibility = JavaVersion.VERSION_17 targetCompatibility = JavaVersion.VERSION_17 if (System.getProperty('profile') == 'minimal') { println 'Using minimal profile for building ' + project.getName() apply from: 'gradle/profile-minimal.gradle' } else { println 'Using default profile executing all tests for building ' + project.getName() apply from: 'gradle/profile-complete.gradle' } dependencies { // boot starter implementation \"org.springframework.boot:spring-boot-starter-validation\" implementation \"org.springframework.boot:spring-boot-starter-data-jpa\" implementation \"org.springframework.boot:spring-boot-starter-data-rest\" implementation \"org.springframework.boot:spring-boot-starter-mail\" implementation \"org.springframework.boot:spring-boot-starter-security\" implementation \"org.springframework.boot:spring-boot-starter-actuator\" implementation 'org.springframework.data:spring-data-elasticsearch:5.4.2' implementation \"org.springframework:spring-messaging:6.2.2\" // cloud support implementation \"org.springframework.cloud:spring-cloud-starter-config:4.2.0\" implementation \"org.springframework.cloud:spring-cloud-starter-netflix-eureka-client:4.2.0\" implementation \"org.springframework.cloud:spring-cloud-gateway-mvc:4.2.0\" implementation 'de.codecentric:spring-boot-admin-starter-client:3.4.1' // springdoc implementation \"org.springdoc:springdoc-openapi-starter-webmvc-ui:${springDocVersion}\" implementation \"org.springdoc:springdoc-openapi-starter-common:${springDocVersion}\" implementation \"org.springdoc:springdoc-openapi-starter-webmvc-api:${springDocVersion}\" implementation \"edu.kit.datamanager:repo-core:1.2.5\" implementation \"edu.kit.datamanager:service-base:1.3.3\" //implementation \"com.github.victools:jsonschema-generator:4.23.0\" //Keycloak // implementation \"org.keycloak:keycloak-spring-boot-starter:${keycloakVersion}\" implementation \"com.nimbusds:nimbus-jose-jwt:10.0.1\" // implementation \"io.jsonwebtoken:jjwt-api:0.11.5\" //implementation \"io.jsonwebtoken:jjwt-impl:0.11.5\" //implementation \"io.jsonwebtoken:jjwt-jackson:0.11.5\" implementation \"org.javers:javers-core:${javersVersion}\" implementation \"com.github.fge:json-patch:1.9\" implementation \"com.bazaarvoice.jolt:jolt-core:0.1.7\" implementation \"com.bazaarvoice.jolt:json-utils:0.1.8\" // implementation \"javax.xml.bind:jaxb-api:2.3.1\" runtimeOnly \"org.apache.httpcomponents:httpclient:4.5.14\" // driver for postgres implementation \"org.postgresql:postgresql:42.7.5\" //driver for h2 implementation \"com.h2database:h2:2.3.232\" testImplementation \"org.springframework.restdocs:spring-restdocs-mockmvc:3.0.3\" testImplementation \"org.springframework.boot:spring-boot-starter-test\" testImplementation \"org.springframework:spring-test\" testImplementation \"org.springframework.security:spring-security-test\" //Java 11 Support testImplementation \"org.mockito:mockito-inline:5.2.0\" testImplementation \"junit:junit:4.13.2\" } if (project.hasProperty('release')) { println 'Using \\'release\\' profile for building ' + project.getName() apply from: 'gradle/profile-deploy.gradle' } test { testLogging { outputs.upToDateWhen {false} showStandardStreams = true } environment \"spring.config.location\", \"classpath:/test-config/\" } tasks.withType(Test) { testLogging { events 'started', 'passed' } } springBoot { buildInfo() } gitProperties { failOnNoGitDirectory = false } bootJar { println 'Create bootable jar...' archiveFileName = \"${archiveBaseName.get()}.${archiveExtension.get()}\" duplicatesStrategy = DuplicatesStrategy.EXCLUDE manifest { attributes 'Main-Class': 'org.springframework.boot.loader.launch.PropertiesLauncher' } launchScript() } jacoco { toolVersion = \"0.8.12\" } // task for printing project name. task printProjectName { doLast { println \"${project.name}\" } }\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/basic",
            "repo_link": "https://github.com/marrlab/BaSiC",
            "content": {
                "codemeta": "",
                "readme": "# BaSiC Matlab code accompanying **A BaSiC Tool for Background and Shading Correction of Optical Microscopy Images** by Tingying Peng, Kurt Thorn, Timm Schroeder, Lichao Wang, Fabian J Theis, Carsten Marr\\*, Nassir Navab\\*, Nature Communication 8:14836 (2017). [doi: 10.1038/ncomms14836](http://www.nature.com/articles/ncomms14836). BaSiC is licensed under [Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License](https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode) It is free for academic use and please contact us for any commercial use. A new Python implementation of BaSiC algorithm is [BaSiCPy](https://github.com/peng-lab/BaSiCPy), with documentation on [Pypi](https://basicpy.readthedocs.io/en/latest/). ## Usage ![BaSiC corrects both spatial uneven illumination of microscopy images and temporal background bleaching for time-lapse movies.](images/usage.png) ## Demo Download demo data examples from [Dropbox](https://www.dropbox.com/s/plznvzdjglrse3h/Demoexamples.zip?dl=0) and run matlab files under example folder. ## ImageJ/Fiji Plugin BaSiC is also available as a ImageJ/Fiji Plugin. ### Installation instruction Note: If you do not have Fiji installed on your computer, you can download it from [Fiji website](http://fiji.sc/). ### Install via Fiji Updater 1. Start Fiji and run the updater (\"Help->Update Fiji\") 2. Select the \"Manage Update Sites\" button at the bottom-left of the updater window 3. Scroll the list of available update sites to find \"BaSiC\" (Note: If you cannot find \"BaSiC\" in the list, select \"Add Update Sites\", Change the name field from default \"New\" to \"BaSiC\", set the URL field to http://sites.imagej.net/BaSiC/) 4. Check the box at the left of \"BaSiC\" 5. Select \"Close\" 6. Select \"Apply Changes\" 7. Restart Fiji. BaSiC should appear in the Plugins menu. From now on, running the Fiji updater will also check for BaSiC updates, and install them if they are available. ### Install manually Please download [BaSiC Plugin](https://github.com/QSCD/BaSiC/blob/master/BaSiCPlugin.zip) from this repository. 1. Copy \"BaSiC_.jar\" to the \"$FIJIROOT/plugins\" folder of your Fiji/ImageJ installation. 2. Copy all dependent jar files in the \"Dependent\" folder to your Fiji/ImageJ \"$FIJIROOT/jars\" directory. ### Troubleshooting If you get the error message \"java.lang.NoSuchMethodError: edu.emory.mathcs.utils.ConcurrencyUtils.submit\" make sure that in your Fiji/ImageJ \"$FIJIROOT/jars\" directory, there is only one version of each jar from the \"Dependent\" folder. Particularly, delete jtransforms-2.4.jar and replace it with our jtransform.jar. ## Issues If you have any issues concerning BaSiC, please report them in the [Issues](https://github.com/QSCD/BaSiC/issues) section of this GitHub repository and we will try to find a solution. ## BaSiCPy Python version of BaSiC implementation (https://github.com/peng-lab/BaSiCPy)\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/basicpy",
            "repo_link": "https://github.com/peng-lab/BaSiCPy",
            "content": {
                "codemeta": "",
                "readme": "# BaSiCPy A python package for background and shading correction of optical microscopy images [![PyPI](https://img.shields.io/pypi/v/basicpy.svg)](https://pypi.org/project/basicpy) [![Status](https://img.shields.io/pypi/status/basicpy.svg)](https://pypi.org/project/basicpy/) [![Python Version](https://img.shields.io/pypi/pyversions/basicpy.svg)](https://python.org) [![License](https://img.shields.io/pypi/l/basicpy)](https://github.com/peng-lab/BaSiCPy/blob/main/LICENSE) [![Tests](https://github.com/peng-lab/basicpy/workflows/CI/badge.svg)](https://github.com/peng-lab/basicpy/actions?workflow=CI) [![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit) [![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![Read the Docs](https://img.shields.io/readthedocs/basicpy/latest.svg?label=Read%20the%20Docs)](https://basicpy.readthedocs.io/) <!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section --> [![All Contributors](https://img.shields.io/badge/all_contributors-5-orange.svg?style=flat-square)](#contributors-) <!-- ALL-CONTRIBUTORS-BADGE:END --> BaSiCPy is a python package for background and shading correction of optical microscopy images. It is developed based on the Matlab version of [BaSiC](https://github.com/marrlab/BaSiC) tool with major improvements in the algorithm. Reference: - BaSiCPy: A robust and scalable shadow correction tool for optical microscopy images (in prep.) - A BaSiC Tool for Background and Shading Correction of Optical Microscopy Images by Tingying Peng, Kurt Thorn, Timm Schroeder, Lichao Wang, Fabian J Theis, Carsten Marr\\*, Nassir Navab\\*, Nature Communication 8:14836 (2017). [doi: 10.1038/ncomms14836](http://www.nature.com/articles/ncomms14836). ## Simple examples |Notebook|Description|Colab Link| | :------------------------: |:---------------:| :---------------------------------------------------: | | [timelapse_brightfield](https://github.com/peng-lab/BaSiCPy/tree/dev/docs/notebooks/timelapse_brightfield.ipynb)| 100 continuous brightfield frames of a time-lapse movie of differentiating mouse hematopoietic stem cells. | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/peng-lab/BaSiCPy/blob/dev/docs/notebooks/timelapse_brightfield.ipynb) | | [timelapse_nanog](https://github.com/peng-lab/BaSiCPy/tree/dev/docs/notebooks/timelapse_nanog.ipynb)| 189 continuous fluorescence frames of a time-lapse movie of differentiating mouse embryonic stem cells, which move much more slower compared to the fast moving hematopoietic stem cells, resulting in a much larger correlation between frames. Note that in this challenging case, the automatic parameters are no longer optimal, so we use the manual parameter setting (larger smooth regularization on both flat-field and dark-field) to improve BaSiC's performance. | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/peng-lab/BaSiCPy/blob/dev/docs/notebooks/timelapse_nanog.ipynb) | | [WSI_brain](https://github.com/peng-lab/BaSiCPy/tree/dev/docs/notebooks/WSI_brain.ipynb)| you can stitch image tiles together to view the effect of shading correction | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/peng-lab/BaSiCPy/blob/dev/docs/notebooks/WSI_brain.ipynb) | You can also find examples of running the package at [notebooks folder](https://github.com/peng-lab/BaSiCPy/tree/dev/docs/notebooks). Data used in the examples and a description can be downloaded from [Zenodo](https://doi.org/10.5281/zenodo.6334809). --- ## Usage See [Read the Docs](https://basicpy.readthedocs.io/en/latest/) for the detailed usage. ## Installation ### For Mac (Intel chip), Linux or WSL2 users Install from PyPI ```console pip install basicpy ``` or install the latest development version ```console git clone https://github.com/peng-lab/BaSiCPy.git cd BaSiCPy pip install . ``` ### For Mac users with M1 / M2 chip BaSiCPy requires [`jax`](https://github.com/google/jax/), which has potential build issue with M1 chips. One easiest solution is using [Miniforge](https://github.com/conda-forge/miniforge) as explained [here](https://github.com/google/jax/issues/5501). In the Miniforge environment, please try the following: ```bash conda install -c conda-forge jax jaxlib pip install basicpy ``` ### For Windows users BaSiCPy requires [`jax`](https://github.com/google/jax/) which does not support Windows officially. However, thanks to [cloudhan/jax-windows-builder](https://github.com/cloudhan/jax-windows-builder), we can install BaSiCPy as follows: ```bash pip install \"jax[cpu]==0.4.11\" -f https://whls.blob.core.windows.net/unstable/index.html --use-deprecated legacy-resolver pip install ml-dtypes==0.2.0 pip install basicpy ``` One may need to add ```python import jax jax.config.update('jax_platform_name', 'cpu') ``` at the top of the script to ensure that JAX uses CPU. For details and latest updates, see [this issue](https://github.com/google/jax/issues/438). ### Install with dev dependencies ```console git clone https://github.com/peng-lab/BaSiCPy.git cd BaSiCPy python -m venv venv source venv/bin/activate pip install -e '.[dev]' ``` ## Development ### bump2version This repository uses [bump2version](https://github.com/c4urself/bump2version) to manage dependencies. New releases are pushed to PyPi in the CI pipeline when a new version is committed with a version tag and pushed to the repo. The development flow should use the following process: 1. New features and bug fixes should be pushed to `dev` 2. When tests have passed a new development version is ready to be release, use `bump2version major|minor|patch`. This will commit and create a new version tag with the `-dev` suffix. 3. Additional fixes/features can be added to the current development release by using `bump2version build`. 4. Once the new bugs/features have been tested and a main release is ready, use `bump2version release` to remove the `-dev` suffix. After creating a new tagged version, push to Github and the version will be built and pushed to PyPi. ### All-contributors This repository uses [All Contributors](https://allcontributors.org/) to manage the contributor list. Please execute the following to add/update contributors. ```bash yarn yarn all-contributors add username contribution yarn all-contributors generate # to reflect the changes to README.md ``` For the possible contribution types, see the [All Contributors documentation](https://allcontributors.org/docs/en/emoji-key). ## Contributors ### Current version <!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --> <!-- prettier-ignore-start --> <!-- markdownlint-disable --> <table> <tr> <td align=\"center\"><a href=\"https://github.com/Nicholas-Schaub\"><img src=\"https://avatars.githubusercontent.com/u/15925882?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Nicholas-Schaub</b></sub></a><br /><a href=\"#projectManagement-Nicholas-Schaub\" title=\"Project Management\">📆</a> <a href=\"https://github.com/peng-lab/BaSiCPy/pulls?q=is%3Apr+reviewed-by%3ANicholas-Schaub\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#infra-Nicholas-Schaub\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=Nicholas-Schaub\" title=\"Tests\">⚠️</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=Nicholas-Schaub\" title=\"Code\">💻</a> <a href=\"#ideas-Nicholas-Schaub\" title=\"Ideas, Planning, & Feedback\">🤔</a></td> <td align=\"center\"><a href=\"https://github.com/tdmorello\"><img src=\"https://avatars.githubusercontent.com/u/34800427?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Tim Morello</b></sub></a><br /><a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=tdmorello\" title=\"Code\">💻</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=tdmorello\" title=\"Documentation\">📖</a> <a href=\"https://github.com/peng-lab/BaSiCPy/pulls?q=is%3Apr+reviewed-by%3Atdmorello\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=tdmorello\" title=\"Tests\">⚠️</a> <a href=\"#ideas-tdmorello\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"#infra-tdmorello\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a></td> <td align=\"center\"><a href=\"https://github.com/tying84\"><img src=\"https://avatars.githubusercontent.com/u/11461947?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Tingying Peng</b></sub></a><br /><a href=\"#data-tying84\" title=\"Data\">🔣</a> <a href=\"#financial-tying84\" title=\"Financial\">💵</a> <a href=\"#projectManagement-tying84\" title=\"Project Management\">📆</a> <a href=\"#talk-tying84\" title=\"Talks\">📢</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=tying84\" title=\"Code\">💻</a></td> <td align=\"center\"><a href=\"https://github.com/yfukai\"><img src=\"https://avatars.githubusercontent.com/u/5919272?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Yohsuke T. Fukai</b></sub></a><br /><a href=\"#research-yfukai\" title=\"Research\">🔬</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=yfukai\" title=\"Code\">💻</a> <a href=\"#ideas-yfukai\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"https://github.com/peng-lab/BaSiCPy/pulls?q=is%3Apr+reviewed-by%3Ayfukai\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=yfukai\" title=\"Tests\">⚠️</a> <a href=\"#question-yfukai\" title=\"Answering Questions\">💬</a> <a href=\"#infra-yfukai\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a></td> <td align=\"center\"><a href=\"https://github.com/YuLiu-web\"><img src=\"https://avatars.githubusercontent.com/u/70626217?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>YuLiu-web</b></sub></a><br /><a href=\"https://github.com/peng-lab/BaSiCPy/commits?author=YuLiu-web\" title=\"Documentation\">📖</a> <a href=\"#userTesting-YuLiu-web\" title=\"User Testing\">📓</a></td> </tr> </table> <!-- markdownlint-restore --> <!-- prettier-ignore-end --> <!-- ALL-CONTRIBUTORS-LIST:END --> For details on the contribution roles, see the [documentation](https://basicpy.readthedocs.io/en/latest/contributors.html). ### Old version (`f3fcf19`), used as the reference implementation to check the approximate algorithm - Lorenz Lamm (@LorenzLamm) - Mohammad Mirkazemi (@Mirkazemi)\n",
                "dependencies": "{\"devDependencies\":{\"all-contributors-cli\":\"^6.20.0\"}}\n[build-system] requires = [ \"setuptools>=42\", \"wheel\" ] build-backend = \"setuptools.build_meta\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/bending-stiffness",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Bending-Stiffness",
            "content": {
                "codemeta": "",
                "readme": "![Java](https://img.shields.io/badge/java-%23ED8B00.svg?style=plastic&logo=openjdk&logoColor=white) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/bendingStiffness?style=plastic) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808969.svg)](https://doi.org/10.5281/zenodo.12808969) [![SWH](https://archive.softwareheritage.org/badge/swh:1:dir:65f4716d51672926f9ae328ea314d969e37534c6/)](https://archive.softwareheritage.org/swh:1:dir:65f4716d51672926f9ae328ea314d969e37534c6;origin=https://github.com/Ramy-Badr-Ahmed/bendingStiffness;visit=swh:1:snp:cf3a5710e567c74b08a7144be79796fb78e9743c;anchor=swh:1:rev:ae6455bbac2db3f8838eb0d69b5ba09e5f50d06e) ### Summary A java code analysing the Bending Stiffness of Actin Filament Experiment. - Reads coordinates from `snake.txt` of a measured filament (an example is in the `images` directory) - Calculates the mean squared displacement (MSD) and contour length from data - Error calculation in persistence length and contour length are saved in `mathematica` directory. - (optional) Modify `config.properties` as needed. ### Running the Demo 1. Place your `snake.txt` and `elongation.txt` files in the `data` directory (an example exists). 2. Compile and run the `Snake` class. ```sh javac -d out src/Snake.java java -cp out Snake\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/beos",
            "repo_link": "https://gitlab.com/dlr-sy/beos",
            "content": {
                "codemeta": "",
                "readme": "[![doi](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.13175694-red.svg)](https://zenodo.org/records/13175694) [![doc](https://img.shields.io/static/v1?label=Pages&message=Reference%20Guide&color=blue&style=flat&logo=gitlab)](https://dlr-sy.gitlab.io/beos) [![PyPi](https://img.shields.io/pypi/v/beos?label=PyPi)](https://pypi.org/project/beos/) [![pipeline status](https://gitlab.com/dlr-sy/beos/badges/master/pipeline.svg)]() # BEOS BEOS is a legacy Fortran-based buckling tool. It is compiled for Python using [f2py](https://numpy.org/doc/stable/f2py). > Installation from source requires an active Fortran compiler (ifort, gfortran). ## Downloading Use GIT to get the latest code base. From the command line, use ``` git clone https://gitlab.dlr.de/fa_sw/beos beos ``` If you check out the repository for the first time, you have to initialize all submodule dependencies first. Execute the following from within the repository. ``` git submodule update --init --recursive ``` To update all refererenced submodules to the latest production level, use ``` git submodule foreach --recursive 'git pull origin $(git config -f $toplevel/.gitmodules submodule.$name.branch || echo master)' ``` ## Installation BEOS can be installed from source using [poetry](https://python-poetry.org). If you don't have [poetry](https://python-poetry.org) installed, run ``` pip install poetry --pre --upgrade ``` to install the latest version of [poetry](https://python-poetry.org) within your python environment. Use ``` poetry update ``` to update all dependencies in the lock file or directly execute ``` poetry install ``` to install all dependencies from the lock file. Last, you should be able to import BEOS as a python package. ```python import beos ``` ## Example Please refer to the linked [repository](https://gitlab.com/dlr-sy/beos) for specific application examples. ## Contact * [Marc Garbade](mailto:marc.garbade@dlr.de) ## Support * [List of Contributors](CONTRIBUTING.md)\n",
                "dependencies": "# TOML file to create BEOS # # @note: TOML file # Created on 13.12.2022 # # @version: 1.0 # ---------------------------------------------------------------------------------------------- # @requires: # - # # @change: # - # # @author: garb_ma [DLR-FA,STM Braunschweig] # ---------------------------------------------------------------------------------------------- [build-system] requires = [\"poetry-core>=1.0\",\"packaging\"] build-backend = \"poetry.core.masonry.api\" [tool.poetry] name = \"beos\" version = \"1.3.0\" description = \"Calculation of the buckling behavior of composite shells\" authors = [\"Freund, Sebastian <sebastian.freund@dlr.de>\"] maintainers = [\"Garbade, Marc <marc.garbade@dlr.de>\"] license = \"MIT\" packages = [{include=\"**/*\", from=\"bin\"}] exclude = [\"bin/_*\", \"bin/**/.*\"] repository = \"https://gitlab.com/dlr-sy/beos\" documentation = \"https://gitlab.com/dlr-sy/beos/-/blob/master/README.md\" keywords = [\"analysis\",\"buckling\",\"composite\"] readme = \"README.md\" classifiers = [ \"Development Status :: 7 - Inactive\", \"Topic :: Scientific/Engineering\", \"Programming Language :: Python :: 2\", \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: MIT License\", \"Operating System :: OS Independent\" ] [tool.poetry.urls] Changelog = \"https://gitlab.com/dlr-sy/beos/-/blob/master/CHANGELOG.md\" [[tool.poetry.source]] name = \"dlr-pypi\" url = \"https://pypi.python.org/simple\" priority = \"primary\" [[tool.poetry.source]] name = \"dlr-sy\" url = \"https://gitlab.dlr.de/api/v4/groups/541/-/packages/pypi/simple\" priority = \"supplemental\" [tool.poetry.build] script = \"config/build.py\" generate-setup-file = false [tool.poetry.dependencies] python = \"~2.7 || ^3.5\" numpy = [{version = \"^1.16\", python = \"~2.7 || ~3.5\"}, {version = \"^1.18\", python = \"~3.6\"}, {version = \"^1.21\", python = \"~3.7\"}, {version = \"^1.22\", python = \"~3.8\"}, {version = \">=1.22,<2\", python = \"^3.9\"}] # All optional dependencies fa-pyutils = [{version = \"*\", python = \"^3.7\", optional = true}] PyQt5-Qt5 = [{version = \"5.15.2\", python = \"^3.7\", optional = true}] PyQt5-sip = [{version = \"~12.12\", python = \"~3.7\", optional = true}, {version = \">=12.13\", python = \"^3.8\", optional = true}] vampire = [{version = \">=0.2.5\", python = \"^3.7\", source=\"dlr-sy\", optional = true}] # All mandatory development dependencies [tool.poetry.group.dev.dependencies] delis = [{git = \"https://gitlab.dlr.de/fa_sw/stmlab/plugins/delis.git\", develop=true, python=\"^3.7\"}] mkl = [{version = \"~2022.2\", platform = \"win32\"}] mkl-devel = [{version = \"~2022.2\", platform = \"win32\"}] tbb = [{version = \"~2021.7\", platform = \"win32\"}] pyx-core = [{version = \">=1.17\", python = \"~2.7 || ^3.5,<3.7\"}, {git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake.git\", develop=true, python=\"^3.7\"}] [tool.poetry.group.lock.dependencies] matplotlib = [{version = \"^2\", python = \"~2.7 || ~3.5\"}, {version = \"^3\", python = \"~3.6\"}, {version = \"~3.5\", python = \"~3.7\"}, \t\t\t {version = \">=3.6\", python = \"^3.8\"}] setuptools = [{version = \"^39.0\", python = \"~2.7\"}, {version = \"^49.0,\", python = \"~3.5\"}, {version = \"^58.0,\", python = \"~3.6\"}, {version = \"^64.0\", python = \"~3.7\"}, {version = \">=64.0\", python = \"^3.8\"}] [tool.poetry.extras] devel = [\"fa-pyutils\",\"vampire\",\"PyQt5-Qt5\",\"PyQt5-sip\"] [tool.pytest.ini_options] addopts = \"-ra -v -m ''\" markers = [\"long: Marks all long running tests\"] [tool.pyxmake.doxygen] name = \"BEOS\" title = [\"BEOS\", \"BEOS Reference Guide\"] source = \"src\" files = [\"beos_data\",\"beos_tools\",\"beos_main\"] output = \"doc/beos_core\" [tool.pyxmake.coverage] name = \"beos\" source = \"bin\" include = [\"example/beos_delis.py\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/bifurcations-discrete-maps",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Bifurcations-Discrete-Maps",
            "content": {
                "codemeta": "",
                "readme": "![Python](https://img.shields.io/badge/Python-3670A0?style=plastic&logo=python&logoColor=ffdd54) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/Bifurcations?style=plastic) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13276873.svg)](https://doi.org/10.5281/zenodo.13276873) # Bifurcation Diagrams for Discrete-Time Maps This repository provides Python implementations for generating interactive bifurcation diagrams and analysing chaotic behavior in discrete-time dynamical systems. The following maps are currently included: - Logistic Map - Tent Map - Sine Map ### Overview Each map is implemented to analyse its bifurcation diagram and Lyapunov exponent, which are key to understanding the dynamics and chaos within these systems. ### Installation 1) Create and source virtual environment: ```shell python -m venv env source env/bin/activate # On Windows use `env\\Scripts\\activate` ``` 2) Install the dependencies: ```shell pip install -r requirements.txt ``` ### Example Usage To analyse a specific map, run the corresponding script in the Maps directory. You can tweak the map parameters as needed. Interactive plots will be generated and saved as offline HTML files within each map's directory. >[!Note] > Some plots have been uploaded to the `Maps` directories for reference. ##### Scripts ```shell python Maps/LogisticMap/main.py python Maps/TentMap/main.py python Maps/SineMap/main.py ``` ##### Example (Logistic Map): ```python from logistic import LogisticMap from utils.plotting import saveInteractivePlot import datetime # Parameters for logistic map params = { 'paramMin': 3.57, # bifurcation parameter 'paramMax': 4.0, 'stepSize': 1e-3, 'numTransient': 300, 'numPlotPoints': 300, 'numIterationsLyapunov': 200 } logisticMap = LogisticMap(**params) # Generate and save bifurcation diagram rhoValues, xValues, hoverText = logisticMap.generateBifurcationData() timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") plotParams = { 'title': 'Logistic Map Bifurcation Diagram', 'xAxisTitle': 'Bifurcation Parameter (rho)', 'yAxisTitle': 'System States x(n)', 'fileName': f'logisticMap_bifurcation_{timestamp}.html', 'markerSize': 0.05, 'opacity': 0.6, } saveInteractivePlot(rhoValues, xValues, hoverText, **plotParams) # Generate and save Lyapunov exponent plot rhoValuesLyapunov, lyapunovExponents, lyapunovHoverText = logisticMap.generateLyapunovData() plotParams = { 'title': 'Logistic Map Lyapunov Exponent', 'xAxisTitle': 'Bifurcation Parameter (rho)', 'yAxisTitle': 'Lyapunov Exponent', 'fileName': f'logisticMap_lyapunov_exponent_{timestamp}.html', 'mode': 'lines' } saveInteractivePlot(rhoValuesLyapunov, lyapunovExponents, lyapunovHoverText, **plotParams) ```\n",
                "dependencies": "plotly numpy matplotlib\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/bioautoml",
            "repo_link": "https://github.com/Bonidia/BioAutoML",
            "content": {
                "codemeta": "",
                "readme": "![Python](https://img.shields.io/badge/python-v3.7-blue) ![Dependencies](https://img.shields.io/badge/dependencies-up%20to%20date-brightgreen.svg) ![Contributions welcome](https://img.shields.io/badge/contributions-welcome-orange.svg) ![Status](https://img.shields.io/badge/status-up-brightgreen) <h1 align=\"center\"> <img src=\"https://github.com/Bonidia/BioAutoML/blob/main/img/BioAutoML.png\" alt=\"BioAutoML\" width=\"400\"> </h1> <h4 align=\"center\">BioAutoML: Automated Feature Engineering and Metalearning for Classification of Biological Sequences</h4> <h4 align=\"center\">Democratizing Machine Learning in Life Sciences</h4> <p align=\"center\"> <a href=\"https://github.com/Bonidia/BioAutoML/\">Home</a> • <a href=\"https://bonidia.github.io/BioAutoML/\">Documentation</a> • <a href=\"#installing-dependencies-and-package\">Installing</a> • <a href=\"#how-to-use\">How To Use</a> • <a href=\"#citation\">Citation</a> </p> <h1 align=\"center\"></h1> ## Update news!!! **New Website: [[https://bonidia.github.io/BioAutoML-WP/](https://bonidia.github.io/BioAutoML-WP/)]** **New Version - Protein:** BioAutoML + iFeature[[Ref](https://github.com/Superzchen/iFeature)]- Access on [[https://bonidia.github.io/BioAutoML/](https://bonidia.github.io/BioAutoML/)] **New Version - Protein:** Access on [[https://bonidia.github.io/BioAutoML/](https://bonidia.github.io/BioAutoML/)] **Published Paper:** Access on [[https://doi.org/10.1093/bib/bbac218](https://doi.org/10.1093/bib/bbac218)] ## Awards ⭐ Latin America Research Awards (LARA), Google, 2021. Project: BioAutoML: Automated Feature Engineering for Classification of Biological Sequences (24 awarded projects, from a base of 700 submissions). Elected by LARA-Google among the 24 most promising ideas in Latin America - 2021 - [[Link](https://blog.google/intl/pt-br/novidades/iniciativas/conheca-os-vencedores-do-premio-lara-2021-o-programa-de-bolsas-de-pesquisa-do-google/)] [[Link](http://www.saocarlos.usp.br/programa-de-bolsas-do-google-premia-trabalhos-orientados-pelo-cemeai/)]. ⭐ Finalist Project (Top 15 of 82), Falling Walls Lab Brazil 2022, DWIH São Paulo, Falling Walls Foundation, DAAD The German Center for Science and Innovation [[Link](https://www.youtube.com/watch?v=H5C_UIgVeQM)]. ⭐ Helmholtz Visiting Researcher Grant/Award - Helmholtz Information & Data Science Academy (HIDA), 2023. Project Title: BioAutoML-Fast: End-to-End Multi-Threaded Machine Learning Package for Life Sciences [[Link](https://bonidia.github.io/website/Certificate%20HVRG_Bonidia-1.pdf)]. ⭐ FEMS Research & Training Grant/Award - Federation of European Microbiological Societies (FEMS), 2023 (€: 5.000,00) [[Link](https://ibbsonline.org/wp-content/uploads/2023/10/IBBS-NL_Sep-2023-Issue.pdf)]. ⭐ BioAutoML - Top 10 Finalist - Santander X Brazil Award - Selected among the top 10 university projects (from over 200 entries) in Brazil in the national innovation competition promoted by Banco Santander. ⭐ BioAutoML received an honorable mention from the Young Bioinformatics Award 2024, being chosen among the best theses in Bioinformatics and Computational Biology in Brazil, 2024. ⭐ BioAutoML received third place in the ARTUR ZIVIANI THESIS AWARD (SBCAS), being chosen among the best theses in computing applied to health in Brazil, 2024. ## Abstract Recent technological advances allowed an exponential expansion of biological sequence data and the extraction of meaningful information through Machine Learning (ML) algorithms. This knowledge improved the understanding of the mechanisms related to several fatal diseases, e.g., Cancer and COVID-19, helping to develop innovative solutions, such as CRISPR-based gene editing, coronavirus vaccine, and precision medicine. These advances benefit our society and economy, directly impacting people's lives in various areas, such as health care, drug discovery, forensic analysis, and food processing. Nevertheless, ML-based approaches to biological data require representative, quantitative, and informative features. Many ML algorithms can handle only numerical data, so sequences need to be translated into a numerical feature vector. This process, known as feature extraction, is a fundamental step for elaborating high-quality ML-based models in bioinformatics, by allowing the feature engineering stage, with the design and selection of suitable features. Feature engineering, ML algorithm selection, and hyperparameter tuning are often manual and time-consuming processes, requiring extensive domain knowledge. To deal with this problem, we present a new package, BioAutoML. BioAutoML automatically runs an end-to-end ML pipeline, extracting numerical and informative features from biological sequence databases, using the MathFeature package, and automating the feature selection, ML algorithm(s) recommendation and tuning of the selected algorithm(s) hyperparameters, using Automated ML (AutoML). BioAutoML has two components, divided into four modules, (1) automated feature engineering (feature extraction and selection modules) and (2) Metalearning (algorithm recommendation and hyper-parameter tuning modules). We experimentally evaluate BioAutoML in two different scenarios: (i) prediction of the three main classes of ncRNAs and (ii) prediction of the seven categories of ncRNAs in bacteria, including housekeeping and regulatory types. To assess BioAutoML predictive performance, it is experimentally compared with two other AutoML tools (RECIPE and TPOT). According to the experimental results, BioAutoML can accelerate new studies, reducing the cost of feature engineering processing and either keeping or improving predictive performance. * First study to propose an automated feature engineering and metalearning pipeline for ncRNA sequences in bacteria; * BioAutoML can be applied in multi-class and binary problems; * BioAutoML can be used in other DNA/RNA sequences scenarios; * BioAutoML can accelerate new studies, reducing the feature engineering time-consuming stage and improving the design and performance of ML pipelines in bioinformatics; * BioAutoML does not require specialist human assistance. <h1 align=\"center\"> <img src=\"https://github.com/Bonidia/BioAutoML/blob/main/img/bio-v2-1.png\" alt=\"BioAutoML\" width=\"1000\"> </h1> <h1 align=\"center\"> <img src=\"https://github.com/Bonidia/BioAutoML/blob/main/img/bio-v4-1.png\" alt=\"BioAutoML\" width=\"900\"> </h1> ## Authors * Robson Parmezan Bonidia, Anderson Paulo Avila Santos, Breno Lívio Silva de Almeida, Peter F. Stadler, Ulisses Nunes da Rocha, Danilo Sipoli Sanches, and André Carlos Ponce de Leon Ferreira de Carvalho. * **Correspondence:** bonidia@usp.br, andre@icmc.usp.br, ulisses.rocha@ufz.de ## Publication Robson P Bonidia, Anderson P Avila Santos, Breno L S de Almeida, Peter F Stadler, Ulisses N da Rocha, Danilo S Sanches, André C P L F de Carvalho, BioAutoML: automated feature engineering and metalearning to predict noncoding RNAs in bacteria, Briefings in Bioinformatics, 2022, bbac218, [[DOI](https://doi.org/10.1093/bib/bbac218)]. ## Installing dependencies and package ## Conda - Terminal Installing BioAutoML using miniconda, e.g.: ```sh $ git clone https://github.com/Bonidia/BioAutoML.git BioAutoML $ cd BioAutoML $ git submodule init $ git submodule update ``` **1 - Install Miniconda:** ```sh See documentation: https://docs.conda.io/en/latest/miniconda.html $ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh $ chmod +x Miniconda3-latest-Linux-x86_64.sh $ ./Miniconda3-latest-Linux-x86_64.sh $ export PATH=~/miniconda3/bin:$PATH ``` **2 - Create environment:** ```sh conda env create -f BioAutoML-env.yml -n bioautoml ``` **3 - Activate environment:** ```sh conda activate bioautoml ``` **4 - You can deactivate the environment, using:** ```sh conda deactivate ``` ## How to use See our [documentation](https://bonidia.github.io/BioAutoML/). ## Citation If you use this code in a scientific publication, we would appreciate citations to the following paper: Robson P Bonidia, Anderson P Avila Santos, Breno L S de Almeida, Peter F Stadler, Ulisses N da Rocha, Danilo S Sanches, André C P L F de Carvalho, BioAutoML: automated feature engineering and metalearning to predict noncoding RNAs in bacteria, Briefings in Bioinformatics, 2022, bbac218, [[DOI](https://doi.org/10.1093/bib/bbac218)]. ```sh @article{10.1093/bib/bbac218, author = {Bonidia, Robson P and Santos, Anderson P Avila and de Almeida, Breno L S and Stadler, Peter F and da Rocha, Ulisses N and Sanches, Danilo S and de Carvalho, André C P L F}, title = \"{BioAutoML: automated feature engineering and metalearning to predict noncoding RNAs in bacteria}\", journal = {Briefings in Bioinformatics}, year = {2022}, month = {06}, issn = {1477-4054}, doi = {10.1093/bib/bbac218}, url = {https://doi.org/10.1093/bib/bbac218}, note = {bbac218}, } ```\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/blenderproc",
            "repo_link": "https://github.com/DLR-RM/BlenderProc",
            "content": {
                "codemeta": "",
                "readme": "# BlenderProc2 [![Documentation](https://img.shields.io/badge/documentation-passing-brightgreen.svg)](https://dlr-rm.github.io/BlenderProc/) [![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DLR-RM/BlenderProc/blob/main/examples/basics/basic/basic_example.ipynb) [![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0) <p align=\"center\"> <img src=\"https://user-images.githubusercontent.com/6104887/137109535-275a2aa3-f5fd-4173-9d16-a9a9b86f66e7.gif\" alt=\"Front readme image\" width=100%> </p> A procedural Blender pipeline for photorealistic rendering. [Documentation](https://dlr-rm.github.io/BlenderProc) | [Tutorials](#tutorials) | [Examples](#examples) | [ArXiv paper](https://arxiv.org/abs/1911.01911) | [Workshop paper](https://sim2real.github.io/assets/papers/2020/denninger.pdf) | [JOSS article](https://joss.theoj.org/papers/10.21105/joss.04901) ## Features * Loading: `*.obj`, `*.ply`, `*.blend`, `*.fbx`, BOP, ShapeNet, Haven, 3D-FRONT, etc. * Objects: Set or sample object poses, apply physics and collision checking. * Materials: Set or sample physically-based materials and textures * Lighting: Set or sample lights, automatic lighting of 3D-FRONT scenes. * Cameras: Set, sample or load camera poses from file. * Rendering: RGB, stereo, depth, normal and segmentation images/sequences. * Writing: .hdf5 containers, COCO & BOP annotations. ## Installation ### Via pip The simplest way to install blenderproc is via pip: ```bash pip install blenderproc ``` ### Via git Alternatively, if you need to make changes to blenderproc or you want to make use of the most recent version on the main-branch, clone the repository: ```bash git clone https://github.com/DLR-RM/BlenderProc ``` To still make use of the blenderproc command and therefore use blenderproc anywhere on your system, make a local pip installation: ```bash cd BlenderProc pip install -e . ``` ## Usage BlenderProc has to be run inside the blender python environment, as only there we can access the blender API. Therefore, instead of running your script with the usual python interpreter, the command line interface of BlenderProc has to be used. ```bash blenderproc run <your_python_script> ``` In general, one run of your script first loads or constructs a 3D scene, then sets some camera poses inside this scene and renders different types of images (RGB, distance, semantic segmentation, etc.) for each of those camera poses. Usually, you will run your script multiple times, each time producing a new scene and rendering e.g. 5-20 images from it. With a little more experience, it is also possible to change scenes during a single script call, read [here](docs/tutorials/key_frames.md#render-multiple-times) how this is done. ## Quickstart You can test your BlenderProc pip installation by running ```bash blenderproc quickstart ``` This is an alias to `blenderproc run quickstart.py` where `quickstart.py` is: ```python import blenderproc as bproc import numpy as np bproc.init() # Create a simple object: obj = bproc.object.create_primitive(\"MONKEY\") # Create a point light next to it light = bproc.types.Light() light.set_location([2, -2, 0]) light.set_energy(300) # Set the camera to be in front of the object cam_pose = bproc.math.build_transformation_mat([0, -5, 0], [np.pi / 2, 0, 0]) bproc.camera.add_camera_pose(cam_pose) # Render the scene data = bproc.renderer.render() # Write the rendering into an hdf5 file bproc.writer.write_hdf5(\"output/\", data) ``` BlenderProc creates the specified scene and renders the image into `output/0.hdf5`. To visualize that image, simply call: ```bash blenderproc vis hdf5 output/0.hdf5 ``` Thats it! You rendered your first image with BlenderProc! ### Debugging in the Blender GUI To understand what is actually going on, BlenderProc has the great feature of visualizing everything inside the blender UI. To do so, simply call your script with the `debug` instead of `run` subcommand: ```bash blenderproc debug quickstart.py ``` *Make sure that `quickstart.py` actually exists in your working directory.* Now the Blender UI opens up, the scripting tab is selected and the correct script is loaded. To start the BlenderProc pipeline, one now just has to press `Run BlenderProc` (see red circle in image). As in the normal mode, print statements are still printed to the terminal. <p align=\"center\"> <img src=\"images/debug.jpg\" alt=\"Front readme image\" width=500> </p> The pipeline can be run multiple times, as in the beginning of each run the scene is cleared. ### Breakpoint-Debugging in IDEs As blenderproc runs in blenders separate python environment, debugging your blenderproc script cannot be done in the same way as with any other python script. Therefore, remote debugging is necessary, which is explained for vscode and PyCharm in the following: #### Debugging with vscode First, install the `debugpy` package in blenders python environment. ``` blenderproc pip install debugpy ``` Now add the following configuration to your vscode [launch.json](https://code.visualstudio.com/docs/python/debugging#_initialize-configurations). ```json { \"name\": \"Attach\", \"type\": \"python\", \"request\": \"attach\", \"connect\": { \"host\": \"localhost\", \"port\": 5678 } } ``` Finally, add the following lines to the top (after the imports) of your blenderproc script which you want to debug. ```python import debugpy debugpy.listen(5678) debugpy.wait_for_client() ``` Now run your blenderproc script as usual via the CLI and then start the added \"Attach\" configuration in vscode. You are now able to add breakpoints and go through the execution step by step. #### Debugging with PyCharm Professional In Pycharm, go to `Edit configurations...` and create a [new configuration](https://www.jetbrains.com/help/pycharm/remote-debugging-with-product.html#remote-debug-config) based on `Python Debug Server`. The configuration will show you, specifically for your version, which pip package to install and which code to add into the script. The following assumes Pycharm 2021.3: First, install the `pydevd-pycharm` package in blenders python environment. ``` blenderproc pip install pydevd-pycharm~=212.5457.59 ``` Now, add the following code to the top (after the imports) of your blenderproc script which you want to debug. ```python import pydevd_pycharm pydevd_pycharm.settrace('localhost', port=12345, stdoutToServer=True, stderrToServer=True) ``` Then, first run your `Python Debug Server` configuration in PyCharm and then run your blenderproc script as usual via the CLI. PyCharm should then go in debug mode, blocking the next code line. You are now able to add breakpoints and go through the execution step by step. ## What to do next? As you now ran your first BlenderProc script, your ready to learn the basics: ### Tutorials Read through the tutorials, to get to know with the basic principles of how BlenderProc is used: 1. [Loading and manipulating objects](docs/tutorials/loader.md) 2. [Configuring the camera](docs/tutorials/camera.md) 3. [Rendering the scene](docs/tutorials/renderer.md) 4. [Writing the results to file](docs/tutorials/writer.md) 5. [How key frames work](docs/tutorials/key_frames.md) 6. [Positioning objects via the physics simulator](docs/tutorials/physics.md) ### Examples We provide a lot of [examples](examples/README.md) which explain all features in detail and should help you understand how BlenderProc works. Exploring our examples is the best way to learn about what you can do with BlenderProc. We also provide support for some datasets. * [Basic scene](examples/basics/basic/README.md): Basic example, this is the ideal place to start for beginners * [Camera sampling](examples/basics/camera_sampling/README.md): Sampling of different camera positions inside of a shape with constraints for the rotation. * [Object manipulation](examples/basics/entity_manipulation/README.md): Changing various parameters of objects. * [Material manipulation](examples/basics/material_manipulation/README.md): Material selecting and manipulation. * [Physics positioning](examples/basics/physics_positioning/README.md): Enabling simple simulated physical interactions between objects in the scene. * [Semantic segmentation](examples/basics/semantic_segmentation/README.md): Generating semantic segmentation labels for a given scene. * [BOP Challenge](README_BlenderProc4BOP.md): Generate the pose-annotated data used at the BOP Challenge 2020 * [COCO annotations](examples/advanced/coco_annotations/README.md): Write COCO annotations to a .json file for selected objects in the scene. and much more, see our [examples](examples/README.md) for more details. ## Contributions Found a bug? help us by reporting it. Want a new feature in the next BlenderProc release? Create an issue. Made something useful or fixed a bug? Start a PR. Check the [contributions guidelines](CONTRIBUTING.md). ## Change log See our [change log](change_log.md). ## Citation If you use BlenderProc in a research project, please cite as follows: ``` @article{Denninger2023, doi = {10.21105/joss.04901}, url = {https://doi.org/10.21105/joss.04901}, year = {2023}, publisher = {The Open Journal}, volume = {8}, number = {82}, pages = {4901}, author = {Maximilian Denninger and Dominik Winkelbauer and Martin Sundermeyer and Wout Boerdijk and Markus Knauer and Klaus H. Strobl and Matthias Humt and Rudolph Triebel}, title = {BlenderProc2: A Procedural Pipeline for Photorealistic Rendering}, journal = {Journal of Open Source Software} } ``` --- <div align=\"center\"> <a href=\"https://www.dlr.de/EN/Home/home_node.html\"><img src=\"images/logo.svg\" hspace=\"3%\" vspace=\"60px\"></a> </div>\n",
                "dependencies": "from setuptools import setup, find_packages import os # Extract version from blenderproc/version.py here = os.path.abspath(os.path.dirname(__file__)) version = {} with open(os.path.join(here, \"blenderproc\", \"version.py\")) as fp: exec(fp.read(), version) with open(os.path.join(here, \"README.md\")) as fp: long_description = fp.read() setup(name='blenderproc', version=version['__version__'], url='https://github.com/DLR-RM/BlenderProc', author='Maximilian Denninger, Dominik Winkelbauer, Martin Sundermeyer', maintainer='Dominik Winkelbauer', packages=find_packages(exclude=['docs', 'examples', 'external', 'images', 'resources', 'scripts', 'tests']), include_package_data=True, entry_points={ 'console_scripts': ['blenderproc=blenderproc.command_line:cli'], }, install_requires=[\"setuptools\", \"pyyaml\", \"requests\", \"matplotlib\", \"numpy\", \"Pillow\", \"h5py\", \"progressbar\"], long_description=long_description, long_description_content_type='text/markdown' )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/boxbeam",
            "repo_link": "https://gitlab.com/dlr-sy/boxbeam",
            "content": {
                "codemeta": "",
                "readme": "[![doi](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.12795533-red.svg)](https://zenodo.org/records/12795533) [![doc](https://img.shields.io/static/v1?label=Pages&message=Reference%20Guide&color=blue&style=flat&logo=gitlab)](https://dlr-sy.gitlab.io/boxbeam) [![PyPi](https://img.shields.io/pypi/v/boxbeam?label=PyPi)](https://pypi.org/project/boxbeam/) [![pipeline status](https://gitlab.com/dlr-sy/boxbeam/badges/master/pipeline.svg)]() # BoxBeam BoxBeam is a legacy Fortran-based beam calculation tool. It is compiled for Python using [f2py](https://numpy.org/doc/stable/f2py). > Installation from source requires an active Fortran compiler (ifort, gfortran). ## Downloading Use GIT to get the latest code base. From the command line, use ``` git clone https://gitlab.dlr.de/fa_sw/boxbeam boxbeam ``` If you check out the repository for the first time, you have to initialize all submodule dependencies first. Execute the following from within the repository. ``` git submodule update --init --recursive ``` To update all refererenced submodules to the latest production level, use ``` git submodule foreach --recursive 'git pull origin $(git config -f $toplevel/.gitmodules submodule.$name.branch || echo master)' ``` ## Installation BoxBeam can be installed from source using [poetry](https://python-poetry.org). If you don't have [poetry](https://python-poetry.org) installed, run ``` pip install poetry --pre --upgrade ``` to install the latest version of [poetry](https://python-poetry.org) within your python environment. Use ``` poetry update ``` to update all dependencies in the lock file or directly execute ``` poetry install ``` to install all dependencies from the lock file. Last, you should be able to import BoxBeam as a python package. ```python import boxbeam ``` ## Example Copy and paste the following text into a new python script to verify the local installation ```python import os, sys import boxbeam as bbeam from itertools import count, zip_longest from operator import itemgetter from collections import OrderedDict import numpy as np def toolInitiate(directory): #---INITIATE BOXBEAM VARIABLES bbeam.boxbeam.initialize() extendedLogFile = False newPath = directory#+'\\\\TestProfile.out' #setting control parameters for BOXBEAM bbeam.steuer.druck = extendedLogFile # extended log file bbeam.steuer.nurqer = False bbeam.steuer.klog = 11 bbeam.steuer.kraeft = False # calculate nodal forces (utilization for FE applications) #---CHANGE THE NAME OF THE OUTPUT FILE TO THE ACTUAL CROSS SECTION NAME if extendedLogFile: for ffile, bbeamPathVar in zip([\"boxbeam_results.out\", \"boxbeam_test.out\"], [\"csinfopath\", \"vbinfopath\"]): fullPathFfile = os.path.abspath(os.path.join(newPath, ffile)) # 240 is the length of the character variable reserved # within FORTRAN to store the directory name if len(fullPathFfile) > 240: raise Exception(\"Path length of file %s to long!\" % fullPathFfile) pathList = [\"\"] * 240 pathList[: len(fullPathFfile)] = list(fullPathFfile) if bbeamPathVar == \"csinfopath\": self.bbeam.path.csinfopath = np.array(pathList,dtype=\"object\") else: self.bbeam.path.vbinfopath = np.array(pathList,dtype=\"object\") def toolCalculate(capCount, webCount, cellCount, yi, zi, yk0, zk0, yklk, zklk, ig1, ig2, iak, ia, webExtensionalStiffness, webShearStiffness, webRefPlaneDist, webThickness,webDensity): #---ASSIGN GURT DATA bbeam.gurt.ig = capCount #---ASSIGN GURT COORDINATES variableList = np.zeros(bbeam.restr.maxgu-bbeam.gurt.ig).tolist() bbeam.gurt.yi = yi + variableList bbeam.gurt.zi = zi + variableList #---ASSIGN GURT MATERIAL INFORMATION WITHIN BOXBEAM bbeam.gurt.bi = np.zeros(bbeam.restr.maxgu) bbeam.gurt.myi = np.zeros(bbeam.restr.maxgu) #---ASSIGN BOXBEAM WAND DATA bbeam.wand.kw = webCount #---ASSIGN WAND COORDINATES variableList = np.zeros(bbeam.restr.maxwa-len(yk0)).tolist() bbeam.wand.yk0 = yk0 + variableList bbeam.wand.yklk = yklk + variableList bbeam.wand.zk0 = zk0 + variableList bbeam.wand.zklk = zklk + variableList #---ASSIGN WAND MATERIAL INFORMATION WITHIN BOXBEAM variableList = np.zeros(bbeam.restr.maxwa-bbeam.wand.kw).tolist() bbeam.wand.it = (5*np.ones(bbeam.wand.kw)).tolist()+variableList bbeam.wand.bk = webExtensionalStiffness+variableList bbeam.wand.gk = webShearStiffness+variableList bbeam.wand.rhok = webDensity+variableList bbeam.wand.e = webRefPlaneDist+variableList #---ASSIGN WAND TOPOLOGY WITHIN BOXBEAM bbeam.wand.th = webThickness+variableList bbeam.wand.ig1 = ig1+variableList bbeam.wand.ig2 = ig2+variableList #---ASSIGN BOXBEAM ZELLE DATA bbeam.zelle.az = cellCount variableList = np.zeros(bbeam.restr.maxze-bbeam.zelle.az).tolist() bbeam.zelle.iak = iak+variableList iaArrayTransposed = np.zeros((bbeam.restr.maxze, bbeam.restr.maxgu)) for cellNumber in range(int(bbeam.restr.maxze)): if cellNumber < cellCount: iaArrayTransposed[cellNumber, :len(ia[cellNumber])] += ia[cellNumber] bbeam.zelle.ia = iaArrayTransposed.T #---ASSIGN BOXBEAM UNIFY LOADS for attrName, load in zip_longest([\"qqx\",\"qqy\",\"qqz\",\"qmx\",\"qmy\",\"qmz\"], reactionForces): setattr(bbeam.spanug, attrName, load) #---EXECUTE BOXBEAM FOR CALCULATING THE CROSS SECTION PARAMETERS bbeam.boxbeam.getequivalentxsection() crossSectionParamNames = ['YS','ZS','YT','ZT','YMST','ZMST','YMSTAC','ZMSTAC','BX', 'ALPHA','DYYSTE','DZZSTE','DYY','DZZ','DZY','DT','M','IYYS','IZZS','IZYS','ITT'] crossSectionParameters = {} for param in crossSectionParamNames: crossSectionParameters[param] = float(getattr(bbeam.quer, param.lower())) effectiveProps = OrderedDict([ ('EA',crossSectionParameters['BX'] ), ('EIxx',crossSectionParameters['DYY'] ), ('EIyy',crossSectionParameters['DZZ'] ), ('GJ',crossSectionParameters['DT'] ), ('YS',crossSectionParameters['YS'] ), ('ZS',crossSectionParameters['ZS'] ), ]) if __name__ == '__main__': #Specify folder where output files are to be stored runDir = os.path.join(os.getcwd(),\"boxbeam\") try: os.makedirs(runDir) except WindowsError: pass #Tool specific limitations #maxCaps = 22 #variable specifying the maximum number of caps within a BoxBeam cross section - defined in bbeam.pyd #maxWebs = 31 #variable specifying the maximum number of walls within a BoxBeam cross section - defined in bbeam.pyd #maxCells = 10 #variable specifying the maximum number of cells within a BoxBeam cross section - defined in bbeam.pyd #------------------------------------------------------------------------------------------------------------------------ # Initiation of boxbeam #------------------------------------------------------------------------------------------------------------------------ toolInitiate(runDir) #------------------------------------------------------------------------------------------------------------------------ # Input for profile #------------------------------------------------------------------------------------------------------------------------ calcGeometry = False # If true the material data is set in a fashion, that the geometric properties (e.g. area moments of inertia) can be calculated. thickness1 = 1.25 thickness2 = .375 if calcGeometry: extensionalStiffness1 = 1.*thickness1 extensionalStiffness2 = 1.*thickness2 shearStiffness1 = 1.*thickness1 shearStiffness2 = 1.*thickness2 density1 = 1.*thickness1 density2 = 1.*thickness2 bbeam.steuer.nurqer = True #\"qqx\",\"qqy\",\"qqz\",\"qmx\",\"qmy\",\"qmz\" reactionForces = [0., 0., 0., 0., 0., 0.] else: extensionalStiffness1 = 7.3335e4*thickness1 extensionalStiffness2 = 3.2232e4*thickness2 shearStiffness1 = 1.7327e4*thickness1 shearStiffness2 = 2.5012e4*thickness2 density1 = 0.00158*thickness1 density2 = 0.00158*thickness2 #\"qqx\",\"qqy\",\"qqz\",\"qmx\",\"qmy\",\"qmz\" reactionForces = [0., 0., 500., 0., -62500., 0.] #---RETRIEVING POINT LOCATIONS AND TOPOLOGY yi,zi,yk0,zk0 = [],[],[],[] yklk, zklk, ig1, ig2 = [],[],[],[] iak = [] ia = [] webExtensionalStiffness = [] webShearStiffness, webRefPlaneDist = [], [] webThickness, webDensity = [],[] capExtensionalStiffness = [] capMass = [] #definition of simple profile capCount = 8 webCount = 9 cellCount = 2 yi = [55., 55., -225., 13., 13., 75., -75., 75.] zi = [12., -12., 0., 18., -18., 0., 16., 16.] yk0 = [55., 75., 55., 13., -75., -225., -75., 13., 13., 13.] zk0 = [12., 0., -12., -18., -16., 0., 16., 18., 18., 18.] yklk = [75., 55., 13., -75., -225., -75., 13., 55., 13., 13.] zklk = [0., -12., -18., -16., 0., 16., 18., 12., -18., -18.] ig1 = [1, 6, 2, 5, 8, 3, 7, 4, 4] ig2 = [6, 2, 5, 8, 3, 7, 4, 1, 5] iak = [5, 5] ia = [[1, 6, 2, 5, 4], [5, 8, 3, 7, 4]] webExtensionalStiffness = [extensionalStiffness1]*(webCount-1)+[extensionalStiffness2] webShearStiffness = [shearStiffness1]*(webCount-1)+[shearStiffness2] webRefPlaneDist = [thickness1/2.]*(webCount-1)+[thickness2/2.] webThickness = [thickness1]*(webCount-1)+[thickness2] webDensity = [density1]*(webCount-1)+[density2] #------------------------------------------------------------------------------------------------------------------------ # Assigning variables of boxbeam # Executing boxbeam # Retrieving results from boxbeam #------------------------------------------------------------------------------------------------------------------------ toolCalculate( capCount, webCount, cellCount, yi, zi, yk0, zk0, yklk, zklk, ig1, ig2, iak, ia, webExtensionalStiffness, webShearStiffness, webRefPlaneDist, webThickness,webDensity ) ``` ## Contact * [Marc Garbade](mailto:marc.garbade@dlr.de) ## Support * [List of Contributors](CONTRIBUTING.md)\n",
                "dependencies": "# TOML file to create Boxbeam # # @note: TOML file # Created on 13.12.2022 # # @version: 1.0 # ---------------------------------------------------------------------------------------------- # @requires: # - # # @change: # - # # @author: garb_ma [DLR-SY,STM Braunschweig] # ---------------------------------------------------------------------------------------------- [build-system] requires = [\"poetry-core>=1.0\",\"packaging\"] build-backend = \"poetry.core.masonry.api\" [tool.poetry] name = \"boxbeam\" version = \"1.3.0.post1\" description = \"Calculation of effective cross-sectional properties of composite beams\" authors = [\"Heinecke, Falk <falk.heinecke@volkswagen.de>\"] maintainers = [\"Garbade, Marc <marc.garbade@dlr.de>\", \"Schuster, Andreas <andreas.schuster@dlr.de>\"] license = \"MIT\" packages = [{include=\"**/*\", from=\"bin\"}] exclude = [\"bin/_*\", \"bin/**/.*\"] repository = \"https://gitlab.com/dlr-sy/boxbeam\" documentation = \"https://gitlab.com/dlr-sy/boxbeam/-/blob/master/README.md\" keywords = [\"analysis\",\"beam\",\"composite\"] readme = \"README.md\" classifiers = [ \"Development Status :: 7 - Inactive\", \"Topic :: Scientific/Engineering\", \"Programming Language :: Python :: 2\", \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: MIT License\", \"Operating System :: OS Independent\" ] [tool.poetry.urls] Changelog = \"https://gitlab.com/dlr-sy/boxbeam/-/blob/master/CHANGELOG.md\" [[tool.poetry.source]] name = \"dlr-pypi\" url = \"https://pypi.python.org/simple\" priority = \"primary\" [[tool.poetry.source]] name = \"dlr-sy\" url = \"https://gitlab.dlr.de/api/v4/groups/541/-/packages/pypi/simple\" priority = \"supplemental\" [tool.poetry.build] script = \"config/build.py\" generate-setup-file = false [tool.poetry.dependencies] python = \"~2.7 || ^3.5\" numpy = [{version = \"^1.16\", python = \"~2.7 || ~3.5\"}, {version = \"^1.18\", python = \"~3.6\"}, {version = \"^1.21\", python = \"~3.7\"}, {version = \"^1.22\", python = \"~3.8\"}, {version = \">=1.22,<2\", python = \"^3.9\"}] # All optional dependencies fa-pyutils = [{version = \"*\", python = \"^3.7\", optional = true}] PyQt5-Qt5 = [{version = \"5.15.2\", python = \"^3.7\", optional = true}] PyQt5-sip = [{version = \"~12.12\", python = \"~3.7\", optional = true}, {version = \">=12.13\", python = \"^3.8\", optional = true}] vampire = [{version = \">=0.2.5\", python = \"^3.7\", source=\"dlr-sy\", optional = true}] # All mandatory development dependencies [tool.poetry.group.dev.dependencies] delis = [{git = \"https://gitlab.dlr.de/fa_sw/stmlab/plugins/delis.git\", develop=true, python=\"^3.7\"}] mkl = [{version = \"~2022.2\", platform = \"win32\"}] mkl-devel = [{version = \"~2022.2\", platform = \"win32\"}] tbb = [{version = \"~2021.7\", platform = \"win32\"}] pyx-core = [{version = \">=1.18\", python = \"~2.7 || ^3.5,<3.7\"}, {git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake.git\", develop=true, python=\"^3.7\"}] \t\t\t [tool.poetry.group.lock.dependencies] matplotlib = [{version = \"^2\", python = \"~2.7 || ~3.5\"}, {version = \"^3\", python = \"~3.6\"}, {version = \"~3.5\", python = \"~3.7\"}, \t\t\t {version = \">=3.6\", python = \"^3.8\"}] setuptools = [{version = \"^39.0\", python = \"~2.7\"}, {version = \"^49.0,\", python = \"~3.5\"}, {version = \"^58.0,\", python = \"~3.6\"}, {version = \"^64.0\", python = \"~3.7\"}, {version = \">=64.0\", python = \"^3.8\"}] [tool.poetry.extras] devel = [\"fa-pyutils\",\"vampire\",\"PyQt5-Qt5\",\"PyQt5-sip\"] [tool.pyxmake.doxygen] name = \"BoxBeam\" title = [\"BoxBeam\", \"BoxBeam Reference Guide\"] source = \"src\" output = \"doc/box_core\" [tool.pyxmake.coverage] name = \"boxbeam\" source = \"bin\" include = [\"example/box_kernel.py\", \"example/box_delis.py\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/brainprint",
            "repo_link": "https://github.com/Deep-MI/brainprint",
            "content": {
                "codemeta": "",
                "readme": "[![PyPI version](https://badge.fury.io/py/brainprint.svg)](https://pypi.org/project/brainprint/) # BrainPrint This is the `brainprint` python package, a derivative of the original [BrainPrint-legacy](https://github.com/Deep-MI/BrainPrint-legacy) scripts, with the primary goal to provide a Python-only version, to integrate the [LaPy](https://github.com/Deep-MI/LaPy) package, and to remove dependencies on third-party software (shapeDNA-* binaries, gmsh, meshfix). As a result, some functionality of the original BrainPrint-legacy scripts is no longer maintained (currently no support of tetrahedral meshes and no support of cortical parcellations or label files). ## Installation Use the following code to install the latest release into your local Python package directory: `python3 -m pip install brainprint` This will also install the necessary dependencies, e.g. the [LaPy](https://github.com/Deep-MI/LaPy) package. You may need to add your local Python package directory to your $PATH in order to run the scripts. ## Usage ### Command Line Interface (CLI) Once installed, the package provides a `brainprint` executable which can be run from the command line. The `brainprint` CLI enables per-subject computation of the individual brainprint descriptors. Its usage and options are summarized below; detailed info is available by calling the script without any arguments from the command line. ```sh brainprint --sdir <directory> --sid <SubjectID> [--num <num>] [--evec] [--skipcortex] [--norm <surface|volume|geometry|none> ] [--reweight] [--asymmetry] [--outdir <directory>] [--help] [--more-help] Options: --help Show this help message and exit --more-help Show extensive help message and exit Required options: --sid <SubjectID> Subject ID (FreeSurfer-processed directory inside the subjects directory) --sdir <directory> FreeSurfer subjects directory Processing directives: --num <num> Number of eigenvalues/vectors to compute (default: 50) --evec Switch on eigenvector computation (default: off) --skipcortex Skip cortical surfaces (default: off) --norm <surface|volume|geometry|none> Switch on eigenvalue normalization; will be either surface, volume, or determined by the geometry of the object. Use \"none\" or leave out entirely to skip normalization. --reweight Switch on eigenvalue reweighting (default: off) --asymmetry Perform left-right asymmetry calculation (default: off) --cholmod Switch on use of (faster) Cholesky decomposition instead of (slower) LU decomposition (default: off). May require manual install of scikit-sparse package. Output parameters: --outdir=OUTDIR Output directory (default: <sdir>/<sid>/brainprint) --keep-temp Whether to keep the temporary files directory or not by default False ``` ### Python Package `brainprint` can also be run within a pure Python environment, i.e. installed and imported as a Python package. E.g.: ```python >>> from brainprint import Brainprint >>> subjects_dir = \"/path/to/freesurfer/subjects_dir/\" >>> subject_id = \"42\" >>> bp = Brainprint(subjects_dir=subjects_dir, asymmetry=True, keep_eigenvectors=True) >>> results = bp.run(subject_id=subject_id) >>> results {\"eigenvalues\": PosixPath(\"/path/to/freesurfer/subjects_dir/subject_id/brainprint/subject_id.brainprint.csv\"), \"eigenvectors\": PosixPath(\"/path/to/freesurfer/subjects_dir/subject_id/brainprint/eigenvectors\"), \"distances\": PosixPath(\"/path/to/freesurfer/subjects_dir/subject_id/brainprint/subject_id.brainprint.asymmetry.csv\")} ``` ## Output The script will create an output directory that contains a CSV table with values (in that order) for the area, volume, and first n eigenvalues per each FreeSurfer structure. An additional output file will be created if the asymmetry calculation is performed and/or for the eigenvectors (CLI `--evecs` flag or `keep_eigenvectors` on class initialization). ## Changes Since version 0.5.0, some changes break compatibility with earlier versions (0.4.0 and lower) as well as the [original BrainPrint](https://github.com/Deep-MI/BrainPrint-legacy). These changes include: - for the creation of surfaces from voxel-based segmentations, we have replaced FreeSurfer's marching cube algorithm by scikit-image's marching cube algorithm. Similarly, other FreeSurfer binaries have been replaced by custom Python functions. As a result, a parallel FreeSurfer installation is no longer a requirement for running the brainprint software. - we have changed / removed the following composite structures from the brainprint shape descriptor: the left and right *striatum* (composite of caudate, putamen, and nucleus accumbens) and the left and right *ventricles* (composite of lateral, inferior lateral, 3rd ventricle, choroid plexus, and CSF) have been removed; the left and right *cerebellum-white-matter* and *cerebellum-cortex* have been merged into left and right *cerebellum*. As a result of these changes, numerical values for the brainprint shape descriptor that are obtained from version 0.5.0 and higher are expected to differ from earlier versions when applied to the same data, but should remain highly correlated with earlier results. There are some changes in version 0.4.0 (and lower) in functionality in comparison to the original [BrainPrint](https://github.com/Deep-MI/BrainPrint-legacy) scripts: - currently no support for tetrahedral meshes - currently no support for analyses of cortical parcellation or label files - no more Python 2.x compatibility ## API Documentation The API Documentation can be found at https://deep-mi.org/BrainPrint . ## References If you use this software for a publication please cite: [1] BrainPrint: a discriminative characterization of brain morphology. Wachinger C, Golland P, Kremen W, Fischl B, Reuter M. Neuroimage. 2015;109:232-48. http://dx.doi.org/10.1016/j.neuroimage.2015.01.032 http://www.ncbi.nlm.nih.gov/pubmed/25613439 [2] Laplace-Beltrami spectra as 'Shape-DNA' of surfaces and solids. Reuter M, Wolter F-E, Peinecke N Computer-Aided Design. 2006;38:342-366. http://dx.doi.org/10.1016/j.cad.2005.10.011\n",
                "dependencies": "[build-system] requires = ['setuptools >= 61.0.0'] build-backend = 'setuptools.build_meta' [project] name = 'brainprint' version = '0.5.0-dev' description = 'A package to compute BrainPrint (shape descriptors) from FastSurfer/FreeSurfer MRI segmentations' readme = 'README.md' license = {file = 'LICENSE'} requires-python = '>=3.9' authors = [ {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'}, {name = 'Kersten Diers', email = 'kersten.diers@dzne.de'}, ] maintainers = [ {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'}, ] keywords = [ 'neuroscience', 'sMRI', 'FreeSurfer' ] classifiers = [ 'Operating System :: Microsoft :: Windows', 'Operating System :: Unix', 'Operating System :: MacOS', 'Programming Language :: Python :: 3 :: Only', 'Programming Language :: Python :: 3.9', 'Programming Language :: Python :: 3.10', 'Programming Language :: Python :: 3.11', 'Programming Language :: Python :: 3.12', 'Natural Language :: English', 'License :: OSI Approved :: MIT License', 'Intended Audience :: Science/Research', ] dependencies = [ 'numpy>=1.21', 'scipy!=1.13.0', 'pandas', 'lapy >= 1.1.1', 'psutil', 'nibabel', 'scikit-image', ] [project.optional-dependencies] build = [ 'build', 'twine', ] doc = [ 'furo!=2023.8.17', 'matplotlib', 'memory-profiler', 'numpydoc', 'sphinx!=7.2.*', 'sphinxcontrib-bibtex', 'sphinx-copybutton', 'sphinx-design', 'sphinx-gallery', 'sphinx-issues', 'pypandoc', 'nbsphinx', 'IPython', # For syntax highlighting in notebooks 'ipykernel', ] style = [ 'bibclean', 'codespell', 'pydocstyle[toml]', 'ruff', ] test = [ 'pytest', 'pytest-cov', 'pytest-timeout', ] all = [ 'brainprint[build]', 'brainprint[doc]', 'brainprint[style]', 'brainprint[test]', ] full = [ 'brainprint[all]', ] [project.urls] homepage = 'https://github.com/Deep-MI/BrainPrint' documentation = 'https://github.com/Deep-MI/BrainPrint' source = 'https://github.com/Deep-MI/BrainPrint' tracker = 'https://github.com/Deep-MI/BrainPrint/issues' [project.scripts] brainprint = 'brainprint.cli:main' brainprint-sys_info = 'brainprint.commands.sys_info:run' [tool.setuptools] include-package-data = false [tool.setuptools.packages.find] include = ['brainprint*'] exclude = ['brainprint*tests'] [tool.pydocstyle] convention = 'numpy' ignore-decorators = '(copy_doc|property|.*setter|.*getter|pyqtSlot|Slot)' match = '^(?!setup|__init__|test_).*\\.py' match-dir = '^brainprint.*' add_ignore = 'D100,D104,D107' [tool.ruff] line-length = 88 extend-exclude = [ \"doc\", \"setup.py\", ] [tool.ruff.lint] # https://docs.astral.sh/ruff/linter/#rule-selection select = [ \"E\", # pycodestyle \"F\", # Pyflakes \"UP\", # pyupgrade \"B\", # flake8-bugbear \"I\", # isort # \"SIM\", # flake8-simplify ] [tool.ruff.lint.per-file-ignores] \"__init__.py\" = [\"F401\"] [tool.pytest.ini_options] minversion = '6.0' addopts = '--durations 20 --junit-xml=junit-results.xml --verbose' filterwarnings = [] [tool.coverage.run] branch = true cover_pylib = false omit = [ '**/__init__.py', '**/brainprint/_version.py', '**/brainprint/commands/*', '**/tests/**', ] [tool.coverage.report] exclude_lines = [ 'pragma: no cover', 'if __name__ == .__main__.:', ] precision = 2\nfrom setuptools import setup setup()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cadet",
            "repo_link": "https://github.com/cadet/CADET-Core",
            "content": {
                "codemeta": "",
                "readme": "CADET-Core ========== .. image:: https://img.shields.io/github/release/cadet/cadet-core.svg :target: https://github.com/cadet/cadet-core/releases .. image:: https://github.com/cadet/cadet-core/actions/workflows/ci.yml/badge.svg?branch=master :target: https://github.com/cadet/cadet-core/actions/workflows/ci.yml?query=branch%3Amaster .. image:: https://anaconda.org/conda-forge/cadet/badges/downloads.svg :target: https://anaconda.org/conda-forge/cadet .. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.8179015.svg :target: https://doi.org/10.5281/zenodo.8179015 .. image:: https://img.shields.io/badge/JuRSE_Code_Pick-Oct_2024-blue.svg :target: https://www.fz-juelich.de/en/rse/community-initiatives/jurse-code-of-the-month/october-2024 - **Website (including documentation):** https://cadet.github.io - **Forum:** https://forum.cadet-web.de - **Source:** https://github.com/cadet/cadet-core - **Bug reports:** https://github.com/cadet/cadet-core/issues - **Demo:** https://www.cadet-web.de - **Newsletter:** https://cadet-web.de/newsletter/ Installation ------------ CADET-Core can be installed via conda from the ``conda-forge`` channel. ``conda install -c conda-forge cadet`` This requires a working `conda installation <https://github.com/conda-forge/miniforge>`_. `Additional information <https://cadet.github.io/master/getting_started/installation>`_ and a `tutorial <https://cadet.github.io/master/getting_started/tutorials/breakthrough>`_ are available to guide you through the installation and the first steps of using CADET. Citing ------------ The development of CADET-Core has been a collaborative effort, with multiple dedicated individuals contributing their expertise to create a powerful and versatile open-source software tool. Countless hours of hard work have been invested to provide the scientific community with a valuable resource. As an open-source project, CADET-Core relies on the support and recognition from users and researchers to thrive. Therefore, we kindly ask that any publications or projects leveraging the capabilities of CADET-Core acknowledge its creators and their contributions by citing an adequate selection of our publications. **General:** - Leweke, S.; von Lieres, E.: `Chromatography Analysis and Design Toolkit (CADET) <https://doi.org/10.1016/j.compchemeng.2018.02.025>`_, Computers and Chemical Engineering **113** (2018), 274-294. - von Lieres, E.; Andersson, J.: `A fast and accurate solver for the general rate model of column liquid chromatography <https://doi.org/10.1016/j.compchemeng.2010.03.008>`_, Computers and Chemical Engineering **34,8** (2010), 1180-1191. **Major extensions:** - Breuer, J. M.; Leweke, S.; Schmölder, J.; Gassner, G.; von Lieres, E.: `Spatial discontinuous Galerkin spectral element method for a family of chromatography models in CADET <https://doi.org/10.1016/j.compchemeng.2023.108340>`_, Computers and Chemical Engineering **177** (2023), 108340. - Zhang, W.; Przybycien T., Breuer J. M. , Leweke S. , von Lieres E.: `Solving crystallization/precipitation population balance models in CADET, part II: Size-based Smoluchowski coagulation and fragmentation equations in batch and continuous modes <https://doi.org/10.1016/j.compchemeng.2024.108860>`_, Computers and Chemical Engineering **192** (2025), 108860. - Zhang, W.; Przybycien T., Schmölder J. , Leweke S. , von Lieres E.: `Solving crystallization/precipitation population balance models in CADET, part I: Nucleation growth and growth rate dispersion in batch and continuous modes on nonuniform grids <https://doi.org/10.1016/j.compchemeng.2024.108612>`_, Computers and Chemical Engineering **183** (2024), 108612. - Püttmann, A.; Schnittert, S.; Naumann, U.; von Lieres, E.: `Fast and accurate parameter sensitivities for the general rate model of column liquid chromatography <http://dx.doi.org/10.1016/j.compchemeng.2013.04.021>`_, Computers and Chemical Engineering **56** (2013), 46-57. Additionally, to ensure reproducibility of your work, we recommend citing the zenodo doi corresponding to the specific CADET-Core release that you used. For a comprehensive list and guidance on citing CADET-Core publications, please refer to the publications section of the `documentation <https://cadet.github.io/master/publications.html>`_. Ongoing Development ------------------- We do our best to provide you with a stable API. However, CADET-Core is actively developed and breaking changes can sometimes be unavoidable. For non-developers, it is recommended to upgrade from release to release instead of always working with the most recent commit. Bugs ---- Please report any bugs that you find `here <https://github.com/cadet/cadet-core/issues>`_. Or, even better, fork the repository on `GitHub <https://github.com/cadet/cadet-core>`_ and create a pull request (PR) with the fix. Donations --------- `Donations <https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=FCQ2M89558ZAG>`_ for helping to host, maintain, and further develop the CADET-Core project are highly appreciated. Copyright and License Notice ---------------------------- Copyright (C) 2008-present: The CADET-Core Authors (see `AUTHORS.md <https://github.com/cadet/cadet-core/blob/master/AUTHORS.md>`_). This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version (see `LICENSE.txt <https://github.com/cadet/cadet-core/blob/master/LICENSE.txt>`_). This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program, see below. If not, see <https://www.gnu.org/licenses/>. Except as contained in this notice, the name of a copyright holder shall not be used in advertising or otherwise to promote the sale, use, or other dealings in this Software without prior written authorization of the copyright holder. Acknowledgments --------------- Please refer to the `list of contributors <https://github.com/cadet/cadet-core/blob/master/AUTHORS.md>`_ who helped building and funding this project.\n",
                "dependencies": "# ============================================================================= # CADET # # Copyright © 2008-present: The CADET-Core Authors # Please see the AUTHORS.md file. # # All rights reserved. This program and the accompanying materials # are made available under the terms of the GNU Public License v3.0 (or, at # your option, any later version) which accompanies this distribution, and # is available at http://www.gnu.org/licenses/gpl.html # ============================================================================= # Require a fairly new cmake version cmake_minimum_required(VERSION 3.12) # Prohibit in-source build set(CMAKE_DISABLE_SOURCE_CHANGES ON) set(CMAKE_DISABLE_IN_SOURCE_BUILD ON) if (\"${CMAKE_SOURCE_DIR}\" STREQUAL \"${CMAKE_BINARY_DIR}\") message(FATAL_ERROR \"In-source build prohibited.\") endif() # Set module path in order to use custom CMake modules set(CMAKE_MODULE_PATH \"${CMAKE_SOURCE_DIR}/cmake/Modules\") find_package(Git) # Write the current version number to variable if (GIT_FOUND) if (EXISTS \"${CMAKE_SOURCE_DIR}/.git\") execute_process(COMMAND ${GIT_EXECUTABLE} describe --abbrev=0 HEAD WORKING_DIRECTORY ${CMAKE_SOURCE_DIR} OUTPUT_VARIABLE CADET_VERSION OUTPUT_STRIP_TRAILING_WHITESPACE) if (NOT \"${CADET_VERSION}\" STREQUAL \"\") message(STATUS \"Get version from git\") # Remove first character (\"v\") string(LENGTH \"${CADET_VERSION}\" CADET_VERSION_STRLEN) math(EXPR CADET_VERSION_STRLEN \"${CADET_VERSION_STRLEN}-1\") string(SUBSTRING \"${CADET_VERSION}\" 1 ${CADET_VERSION_STRLEN} CADET_VERSION) endif() endif() endif() # In case of missing tags, default to versions.txt file if (\"${CADET_VERSION}\" STREQUAL \"\") message(STATUS \"Get version from file\") file(STRINGS \"${CMAKE_SOURCE_DIR}/version.txt\" CADET_VERSION) endif() message(STATUS \"CADET version: ${CADET_VERSION}\") # Get current commit hash from git if (GIT_FOUND) include(GetGitRevisionDescription) get_git_head_revision(GIT_REFSPEC GIT_SHA1) endif() if (NOT DEFINED GIT_SHA1) set(GIT_SHA1 \"NO-COMMIT-HASH\") set(GIT_REFSPEC \"NO-REFSPEC\") endif() message(STATUS \"Current git HEAD: ${GIT_REFSPEC} SHA1 ${GIT_SHA1}\") # Name of the current project project(CadetFramework VERSION ${CADET_VERSION} DESCRIPTION \"Liquid column chromatography simulator\" HOMEPAGE_URL \"https://github.com/cadet/cadet-core\" LANGUAGES CXX C) # Always use '-fPIC'/'-fPIE' option set(CMAKE_POSITION_INDEPENDENT_CODE ON) # Hide symbols by default set(CMAKE_C_VISIBILITY_PRESET hidden) set(CMAKE_CXX_VISIBILITY_PRESET hidden) set(CMAKE_VISIBILITY_INLINES_HIDDEN ON) # Enable folders for IDEs set_property(GLOBAL PROPERTY USE_FOLDERS ON) # --------------------------------------------------- # Other configuration options # --------------------------------------------------- # Option that allows users to build release or debug version if (NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"Choose the type of build, options are: None Debug Release RelWithDebInfo MinSizeRel\" FORCE) message(STATUS \"Build type: ${CMAKE_BUILD_TYPE} (default)\") endif() # Default IPO setting: OFF for Debug, ON for all other build types set(DEFAULT_IPO_ENABLED ON) if (CMAKE_BUILD_TYPE STREQUAL \"Debug\") set(DEFAULT_IPO_ENABLED OFF) endif() include(FeatureSummary) option(ENABLE_LOGGING \"Enables logging\" ON) add_feature_info(ENABLE_LOGGING ENABLE_LOGGING \"Enables logging\") option(ENABLE_BENCHMARK \"Enables benchmark mode (fine-grained timing)\" OFF) add_feature_info(ENABLE_BENCHMARK ENABLE_BENCHMARK \"Enables benchmark mode (fine-grained timing)\") option(ENABLE_PLATFORM_TIMER \"Use a platform-dependent timer\" OFF) add_feature_info(ENABLE_PLATFORM_TIMER ENABLE_PLATFORM_TIMER \"Use a platform-dependent timer\") option(ENABLE_THREADING \"Use multi-threading\" OFF) add_feature_info(ENABLE_THREADING ENABLE_THREADING \"Use multi-threading\") option(ENABLE_DEBUG_THREADING \"Use multi-threading in debug builds\" OFF) add_feature_info(ENABLE_DEBUG_THREADING ENABLE_DEBUG_THREADING \"Use multi-threading in debug builds\") option(ENABLE_2D_MODELS \"Build 2D models (e.g., 2D general rate model, multichannel transport)\" ON) add_feature_info(ENABLE_2D_MODELS ENABLE_2D_MODELS \"Build 2D models (e.g., 2D general rate model, multichannel transport)\") option(ENABLE_DG \"Build DG variants of models\" ON) add_feature_info(ENABLE_DG ENABLE_DG \"Build DG variants of models\") option(ENABLE_SUNDIALS_OPENMP \"Prefer OpenMP vector implementation of SUNDIALS if available (for large problems)\" OFF) add_feature_info(ENABLE_SUNDIALS_OPENMP ENABLE_SUNDIALS_OPENMP \"Prefer OpenMP vector implementation of SUNDIALS if available (for large problems)\") option(ENABLE_ANALYTIC_JACOBIAN_CHECK \"Enable verification of analytical Jacobian by AD\" OFF) add_feature_info(ENABLE_ANALYTIC_JACOBIAN_CHECK ENABLE_ANALYTIC_JACOBIAN_CHECK \"Enable verification of analytical Jacobian by AD\") set(ADLIB \"sfad\" CACHE STRING \"Selects the AD library, options are 'sfad', 'setfad'\") string(TOLOWER ${ADLIB} ADLIB) set(NUM_MAX_AD_DIRS 80 CACHE STRING \"Sets the maximum number of AutoDiff directions\") option(ENABLE_CADET_CLI \"Build CADET command line interface\" ON) add_feature_info(ENABLE_CADET_CLI ENABLE_CADET_CLI \"Build CADET command line interface\") option(ENABLE_CADET_TOOLS \"Build CADET tools\" ON) add_feature_info(ENABLE_CADET_TOOLS ENABLE_CADET_TOOLS \"Build CADET tools\") option(ENABLE_TESTS \"Build CADET tests\" OFF) add_feature_info(ENABLE_TESTS ENABLE_TESTS \"Build CADET tests\") option(ENABLE_PACKAGED_SUNDIALS \"Use packaged SUNDIALS code\" ON) add_feature_info(ENABLE_PACKAGED_SUNDIALS ENABLE_PACKAGED_SUNDIALS \"Use packaged SUNDIALS code\") option(ENABLE_IPO \"Enable interprocedural optimization if compiler supports it\" ${DEFAULT_IPO_ENABLED}) add_feature_info(ENABLE_IPO ENABLE_IPO \"Enable interprocedural optimization if compiler supports it\") option(ENABLE_ASAN \"Enable address sanitizer (clang and gcc)\" OFF) add_feature_info(ENABLE_ASAN ENABLE_ASAN \"Enable address sanitizer (clang and gcc)\") option(ENABLE_UBSAN \"Enable undefined behavior sanitizer (clang and gcc)\" OFF) add_feature_info(ENABLE_UBSAN ENABLE_UBSAN \"Enable undefined behavior sanitizer (clang and gcc)\") option(ENABLE_STATIC_LINK_DEPS \"Prefer static over dynamic linking of dependencies\" OFF) add_feature_info(ENABLE_STATIC_LINK_DEPS ENABLE_STATIC_LINK_DEPS \"Prefer static over dynamic linking of dependencies\") option(ENABLE_STATIC_LINK_LAPACK \"Prefer static over dynamic linking of LAPACK and BLAS\" OFF) add_feature_info(ENABLE_STATIC_LINK_LAPACK ENABLE_STATIC_LINK_LAPACK \"Prefer static over dynamic linking of LAPACK and BLAS\") option(ENABLE_STATIC_LINK_CLI \"Prefer static over dynamic linking for CADET CLI\" OFF) add_feature_info(ENABLE_STATIC_LINK_CLI ENABLE_STATIC_LINK_CLI \"Prefer static over dynamic linking for CADET CLI\") option(CMAKE_INSTALL_RPATH_USE_LINK_PATH \"Add paths to linker search and installed rpath\" ON) add_feature_info(CMAKE_INSTALL_RPATH_USE_LINK_PATH CMAKE_INSTALL_RPATH_USE_LINK_PATH \"Add paths to linker search and installed rpath\") # Hande RPATH on OSX when not installing to a system directory, see # https://groups.google.com/d/msg/fenics-dev/KSCrob4M_1M/zsJwdN-SCAAJ # and https://cmake.org/Wiki/CMake_RPATH_handling#Always_full_RPATH if (UNIX) # The RPATH to be used when installing, but only if it's not a system directory set(CMAKE_INSTALL_RPATH \"${CMAKE_INSTALL_PREFIX}/lib\") list(FIND CMAKE_PLATFORM_IMPLICIT_LINK_DIRECTORIES \"${CMAKE_INSTALL_PREFIX}/lib\" isSystemDir) if (\"${isSystemDir}\" STREQUAL \"-1\") set(CMAKE_INSTALL_RPATH \"${CMAKE_INSTALL_PREFIX}/lib\") endif() endif() # --------------------------------------------------- # Check build environment # --------------------------------------------------- include(WriteCompilerDetectionHeader) set(TBB_TARGET \"\") if (ENABLE_THREADING) set(TBB_PREFER_STATIC_LIBS ${ENABLE_STATIC_LINK_DEPS}) set(TBB_USE_DEBUG_BUILD OFF) find_package(TBB COMPONENTS tbb OPTIONAL_COMPONENTS tbb_preview) set_package_properties(TBB PROPERTIES TYPE RECOMMENDED PURPOSE \"Accelerates computation via multi-threading\" ) set(CADET_PARALLEL_FLAG \"\") if (TBB_FOUND) # Use tbb_preview instead of tbb if it has been found if (TBB_tbb_preview_FOUND) set(TBB_TARGET \"TBB::TBBpreview\") else() set(TBB_TARGET \"TBB::TBB\") endif() if (CMAKE_BUILD_TYPE STREQUAL \"Debug\") if (ENABLE_DEBUG_THREADING) set(CADET_PARALLEL_FLAG \"CADET_PARALLELIZE\") endif() else() set(CADET_PARALLEL_FLAG \"CADET_PARALLELIZE\") endif() get_target_property(TBB_IFACE_COMP_DEF ${TBB_TARGET} INTERFACE_COMPILE_DEFINITIONS) if (TBB_IFACE_COMP_DEF) list(APPEND TBB_IFACE_COMP_DEF ${CADET_PARALLEL_FLAG}) else() set(TBB_IFACE_COMP_DEF ${CADET_PARALLEL_FLAG}) endif() if (TBB_IFACE_COMP_DEF) set_target_properties(${TBB_TARGET} PROPERTIES INTERFACE_COMPILE_DEFINITIONS \"${TBB_IFACE_COMP_DEF}\") endif() unset(TBB_IFACE_COMP_DEF) if (TBB_INTERFACE_VERSION GREATER_EQUAL 11004) # Use global_control instead of task_scheduler_init get_target_property(TBB_IFACE_COMP_DEF ${TBB_TARGET} INTERFACE_COMPILE_DEFINITIONS) set_target_properties(${TBB_TARGET} PROPERTIES INTERFACE_COMPILE_DEFINITIONS \"${TBB_IFACE_COMP_DEF}\") if (TBB_IFACE_COMP_DEF) list(APPEND TBB_IFACE_COMP_DEF \"CADET_TBB_GLOBALCTRL\") else() set(TBB_IFACE_COMP_DEF \"CADET_TBB_GLOBALCTRL\") endif() if (TBB_IFACE_COMP_DEF) set_target_properties(${TBB_TARGET} PROPERTIES INTERFACE_COMPILE_DEFINITIONS \"${TBB_IFACE_COMP_DEF}\") endif() unset(TBB_IFACE_COMP_DEF) endif() endif() endif() set(BLA_STATIC ${ENABLE_STATIC_LINK_LAPACK}) find_package(LAPACK) set_package_properties(LAPACK PROPERTIES TYPE RECOMMENDED PURPOSE \"Solution of dense linear systems\" ) if (NOT ENABLE_PACKAGED_SUNDIALS) # SUNDIALS_ROOT environment variable can be used to find SUNDIALS package set(SUNDIALS_PREFER_STATIC_LIBRARIES ${ENABLE_STATIC_LINK_DEPS}) find_package(SUNDIALS REQUIRED COMPONENTS sundials_idas sundials_nvecserial OPTIONAL_COMPONENTS sundials_nvecopenmp) set_package_properties(SUNDIALS PROPERTIES TYPE REQUIRED PURPOSE \"Time integration\" ) # Check whether OpenMP is available in SUNDIAL'S NVECTOR module set(SUNDIALS_NVEC_TARGET \"SUNDIALS::sundials_nvecserial\") if (SUNDIALS_sundials_nvecopenmp_LIBRARY AND ENABLE_SUNDIALS_OPENMP) # Prefer OpenMP over serial version set(SUNDIALS_NVEC_TARGET \"SUNDIALS::sundials_nvecopenmp\") get_target_property(SUNDIALS_IFACE_COMP_DEF SUNDIALS::sundials_nvecopenmp INTERFACE_COMPILE_DEFINITIONS) if (SUNDIALS_IFACE_COMP_DEF) list(APPEND SUNDIALS_IFACE_COMP_DEF \"CADET_SUNDIALS_OPENMP\") else() set(SUNDIALS_IFACE_COMP_DEF \"CADET_SUNDIALS_OPENMP\") endif() set_target_properties(SUNDIALS::sundials_nvecopenmp PROPERTIES INTERFACE_COMPILE_DEFINITIONS \"${SUNDIALS_IFACE_COMP_DEF}\") unset(SUNDIALS_IFACE_COMP_DEF) endif() # Determine SUNDIALS interface version if (SUNDIALS_FOUND) get_target_property(SUNDIALS_IFACE_COMP_DEF SUNDIALS::sundials_idas INTERFACE_COMPILE_DEFINITIONS) if (SUNDIALS_IFACE_COMP_DEF) list(APPEND SUNDIALS_IFACE_COMP_DEF \"CADET_SUNDIALS_IFACE=${SUNDIALS_VERSION_MAJOR}\") else() set(SUNDIALS_IFACE_COMP_DEF \"CADET_SUNDIALS_IFACE=${SUNDIALS_VERSION_MAJOR}\") endif() set_target_properties(SUNDIALS::sundials_idas PROPERTIES INTERFACE_COMPILE_DEFINITIONS \"${SUNDIALS_IFACE_COMP_DEF}\") unset(SUNDIALS_IFACE_COMP_DEF) endif() else() set(SUNDIALS_FOUND TRUE) set(SUNDIALS_VERSION \"3.2.1\") set(SUNDIALS_NVEC_TARGET \"SUNDIALS::sundials_nvecserial\") add_subdirectory(ThirdParty/sundials) add_library(SUNDIALS::sundials_idas INTERFACE IMPORTED) target_link_libraries(SUNDIALS::sundials_idas INTERFACE sundials_idas_static) target_include_directories(SUNDIALS::sundials_idas INTERFACE ThirdParty/sundials/include ThirdParty/sundials/src \"${CMAKE_BINARY_DIR}/ThirdParty/sundials/include\") set_target_properties(SUNDIALS::sundials_idas PROPERTIES INTERFACE_COMPILE_DEFINITIONS \"CADET_SUNDIALS_IFACE=3\") add_library(SUNDIALS::sundials_nvecserial INTERFACE IMPORTED) target_include_directories(SUNDIALS::sundials_nvecserial INTERFACE ThirdParty/sundials/include \"${CMAKE_BINARY_DIR}/ThirdParty/sundials/include\") target_link_libraries(SUNDIALS::sundials_nvecserial INTERFACE sundials_nvecserial_static) endif() if (ENABLE_CADET_TOOLS OR ENABLE_CADET_CLI) set(HDF5_USE_STATIC_LIBRARIES ${ENABLE_STATIC_LINK_DEPS}) find_package(HDF5 COMPONENTS C) set_package_properties(HDF5 PROPERTIES DESCRIPTION \"Hierarchical Data Format 5 (HDF5)\" URL \"https://www.hdfgroup.org/HDF5\" TYPE RECOMMENDED PURPOSE \"File IO\" ) if (HDF5_FOUND) # Create custom HDF5 target if CMake's FindHDF5 is too old if (NOT TARGET HDF5::HDF5) list(LENGTH HDF5_C_LIBRARIES HDF5_C_LEN) if (HDF5_C_LEN GREATER 1) list(GET HDF5_C_LIBRARIES 0 HDF5_MAIN_LIBRARY) set(HDF5_SUPPORT_LIBRARIES ${HDF5_C_LIBRARIES}) list(REMOVE_AT HDF5_SUPPORT_LIBRARIES 0) else() set(HDF5_MAIN_LIBRARY ${HDF5_C_LIBRARIES}) set(HDF5_SUPPORT_LIBRARIES) endif() add_library(HDF5::HDF5 UNKNOWN IMPORTED) set_target_properties(HDF5::HDF5 PROPERTIES IMPORTED_LOCATION ${HDF5_MAIN_LIBRARY} INTERFACE_INCLUDE_DIRECTORIES \"${HDF5_C_INCLUDE_DIRS}\" # INTERFACE_COMPILE_DEFINITIONS ${HDF5_C_DEFINITIONS} ) if (HDF5_SUPPORT_LIBRARIES) target_link_libraries(HDF5::HDF5 INTERFACE ${HDF5_SUPPORT_LIBRARIES}) endif() unset(HDF5_SUPPORT_LIBRARIES) unset(HDF5_MAIN_LIBRARY) unset(HDF5_C_LEN) endif() # Make sure HDF5_LIBRARY_DIRS is defined if ((NOT DEFINED HDF5_LIBRARY_DIRS) OR (NOT HDF5_LIBRARY_DIRS) OR (\"${HDF5_LIBRARY_DIRS}\" STREQUAL \"\")) list(GET HDF5_LIBRARIES 0 HDF5_LIB_TEMP) get_filename_component(HDF5_LIBRARY_DIRS ${HDF5_LIB_TEMP} DIRECTORY) unset(HDF5_LIB_TEMP) endif() # Check if we need additional libraries for linking (i.e., zlib, szip) include(${CMAKE_ROOT}/Modules/CheckCXXSourceCompiles.cmake) include(${CMAKE_ROOT}/Modules/CMakePushCheckState.cmake) cmake_push_check_state(RESET) # Set libs and includes set(CMAKE_REQUIRED_LIBRARIES ${HDF5_LIBRARIES}) set(CMAKE_REQUIRED_INCLUDES ${HDF5_INCLUDE_DIRS}) CHECK_CXX_SOURCE_COMPILES(\"#include <hdf5.h>\\nint main(int argc, char** argv){\\n H5Zfilter_avail(H5Z_FILTER_SZIP);\\nH5Zfilter_avail(H5Z_FILTER_DEFLATE);\\nreturn 0;\\n}\\n\" HDF5_DONT_NEED_ZLIBS) # Reset libs and includes cmake_pop_check_state() # Find szip and zlib libs if we need them if (NOT HDF5_DONT_NEED_ZLIBS) # Prefer static libs if enabled set(_HDF5_ORIG_CMAKE_FIND_LIBRARY_SUFFIXES ${CMAKE_FIND_LIBRARY_SUFFIXES}) if(ENABLE_STATIC_LINK_DEPS) if(WIN32) set(CMAKE_FIND_LIBRARY_SUFFIXES .lib ${CMAKE_FIND_LIBRARY_SUFFIXES}) else() set(CMAKE_FIND_LIBRARY_SUFFIXES .a ${CMAKE_FIND_LIBRARY_SUFFIXES}) endif() endif() find_library(HDF5_SZLIB NAMES libszip szip PATHS ${HDF5_LIBRARY_DIRS}) find_library(HDF5_ZLIB NAMES libzlib zlib PATHS ${HDF5_LIBRARY_DIRS}) if (HDF5_SZLIB) list(APPEND HDF5_LIBRARIES ${HDF5_SZLIB}) add_library(HDF5::SZLIB UNKNOWN IMPORTED) set_target_properties(HDF5::SZLIB PROPERTIES IMPORTED_LOCATION ${HDF5_SZLIB}) target_link_libraries(HDF5::HDF5 INTERFACE HDF5::SZLIB) endif() if (HDF5_ZLIB) list(APPEND HDF5_LIBRARIES ${HDF5_ZLIB}) add_library(HDF5::ZLIB UNKNOWN IMPORTED) set_target_properties(HDF5::ZLIB PROPERTIES IMPORTED_LOCATION ${HDF5_ZLIB}) target_link_libraries(HDF5::HDF5 INTERFACE HDF5::ZLIB) endif() unset(HDF5_SZLIB) unset(HDF5_ZLIB) set(CMAKE_FIND_LIBRARY_SUFFIXES ${_HDF5_ORIG_CMAKE_FIND_LIBRARY_SUFFIXES}) unset(_HDF5_ORIG_CMAKE_FIND_LIBRARY_SUFFIXES) endif() endif() endif() if (ENABLE_2D_MODELS) set(SUPERLU_PREFER_STATIC_LIBS ${ENABLE_STATIC_LINK_DEPS}) find_package(SuperLU) set_package_properties(SuperLU PROPERTIES TYPE RECOMMENDED PURPOSE \"Sparse matrix solver\" ) set(UMFPACK_PREFER_STATIC_LIBS ${ENABLE_STATIC_LINK_DEPS}) find_package(UMFPACK) set_package_properties(UMFPACK PROPERTIES TYPE RECOMMENDED PURPOSE \"Sparse matrix solver\" ) endif() set(EIGEN_TARGET \"\") if (ENABLE_DG) find_package(Eigen3 3.4 REQUIRED NO_MODULE) # Disable DG if Eigen is not present if (NOT TARGET Eigen3::Eigen) message(STATUS \"Disabling DG support because Eigen3 could not be found\") set(ENABLE_DG OFF) else() set(EIGEN_TARGET \"Eigen3::Eigen\") endif() endif() set(IPO_AVAILABLE OFF) if (ENABLE_IPO) include(CheckIPOSupported) check_ipo_supported(RESULT IPO_RESULT OUTPUT IPO_OUT LANGUAGES CXX) if (IPO_RESULT) set(IPO_AVAILABLE ON) set(CMAKE_INTERPROCEDURAL_OPTIMIZATION ON) else() message(WARNING \"IPO is not supported: ${IPO_OUT}\") endif() unset(IPO_RESULT) unset(IPO_OUT) endif() # --------------------------------------------------- # Add selected modules to the build system and add the targets to the list of all targets # --------------------------------------------------- add_library(CADET::CompileOptions INTERFACE IMPORTED) target_compile_features(CADET::CompileOptions INTERFACE cxx_std_23) set(CMAKE_CXX_EXTENSIONS OFF) if (WIN32) target_compile_definitions(CADET::CompileOptions INTERFACE NOMINMAX) endif() if (CMAKE_BUILD_TYPE STREQUAL \"Debug\") target_compile_definitions(CADET::CompileOptions INTERFACE CADET_LOGLEVEL_MIN=Trace DEBUG _DEBUG) else() target_compile_definitions(CADET::CompileOptions INTERFACE CADET_LOGLEVEL_MIN=Warning NDEBUG) endif() if (ENABLE_BENCHMARK) target_compile_definitions(CADET::CompileOptions INTERFACE CADET_BENCHMARK_MODE) endif() if (ENABLE_PLATFORM_TIMER) target_compile_definitions(CADET::CompileOptions INTERFACE CADET_USE_PLATFORM_TIMER) if ((NOT APPLE) AND (NOT WIN32)) target_link_libraries(CADET::CompileOptions INTERFACE rt) endif() endif() if (CMAKE_BUILD_TYPE STREQUAL \"Debug\") target_compile_options(CADET::CompileOptions INTERFACE $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>: -Wall -pedantic-errors -Wextra -Wno-unused-parameter -Wno-unused-function> #-Wconversion -Wsign-conversion $<$<CXX_COMPILER_ID:MSVC>: /W4 /wd4100 /bigobj /MP> ) else() target_compile_options(CADET::CompileOptions INTERFACE $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>: -Wall -pedantic-errors -Wextra -Wno-unused-parameter -Wno-unused-function> #-Wconversion -Wsign-conversion $<$<CXX_COMPILER_ID:MSVC>: /W4 /wd4100 /MP> ) endif() if (ENABLE_ASAN) target_compile_options(CADET::CompileOptions INTERFACE $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fsanitize=address>) target_link_options(CADET::CompileOptions INTERFACE $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fsanitize=address>) endif() if (ENABLE_UBSAN) target_compile_options(CADET::CompileOptions INTERFACE $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fsanitize=undefined>) target_link_options(CADET::CompileOptions INTERFACE $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fsanitize=undefined>) endif() add_library(CADET::AD INTERFACE IMPORTED) if (ADLIB STREQUAL \"sfad\") message(STATUS \"AD library: SFAD\") target_compile_definitions(CADET::AD INTERFACE ACTIVE_SFAD) target_include_directories(CADET::AD INTERFACE \"${CMAKE_SOURCE_DIR}/include/ad\") elseif (ADLIB STREQUAL \"setfad\") message(STATUS \"AD library: SETFAD\") target_compile_definitions(CADET::AD INTERFACE ACTIVE_SETFAD) target_include_directories(CADET::AD INTERFACE \"${CMAKE_SOURCE_DIR}/include/ad\") else() message(FATAL_ERROR \"Unkown AD library ${ADLIB} (options are 'sfad', 'setfad')\") endif() # Build tools add_subdirectory(src/build-tools) # CADET library add_subdirectory(src/libcadet) if (ENABLE_CADET_CLI) add_subdirectory(src/cadet-cli) endif() if (ENABLE_CADET_TOOLS) add_subdirectory(src/tools) endif() if (ENABLE_TESTS) add_subdirectory(test) # Add \"make check\" target include(ProcessorCount) ProcessorCount(NPROC) add_custom_target(check COMMAND test/testRunner -d yes --tbbthreads ${NPROC}) endif() # --------------------------------------------------- # Set properties, definitions, install target etc. # --------------------------------------------------- # Packaging support include(CPack) set(CPACK_PACKAGE_VENDOR \"CADET\") set(CPACK_PACKAGE_DESCRIPTION_SUMMARY ${PROJECT_DESCRIPTION}) set(CPACK_PACKAGE_VERSION_MAJOR ${PROJECT_VERSION_MAJOR}) set(CPACK_PACKAGE_VERSION_MINOR ${PROJECT_VERSION_MINOR}) set(CPACK_PACKAGE_VERSION_PATCH ${PROJECT_VERSION_PATCH}) set(CPACK_STRIP_FILES ON) # Combine LICENSE.txt and ThirdParty-LICENSES.txt file(WRITE \"${CMAKE_BINARY_DIR}/LICENSE.txt\" \"####################################################################################\\n\") file(APPEND \"${CMAKE_BINARY_DIR}/LICENSE.txt\" \"## CADET ##\\n\") file(APPEND \"${CMAKE_BINARY_DIR}/LICENSE.txt\" \"####################################################################################\\n\") file(READ \"${CMAKE_SOURCE_DIR}/LICENSE.txt\" LICENSE_TEXT) file(APPEND \"${CMAKE_BINARY_DIR}/LICENSE.txt\" \"\\n${LICENSE_TEXT}\") file(READ \"${CMAKE_SOURCE_DIR}/ThirdParty-LICENSES.txt\" LICENSE_THIRDPARTY) file(APPEND \"${CMAKE_BINARY_DIR}/LICENSE.txt\" \"\\n\\n${LICENSE_THIRDPARTY}\") set(CPACK_RESOURCE_FILE_LICENSE \"${CMAKE_BINARY_DIR}/LICENSE.txt\") set(CPACK_RESOURCE_FILE_README \"${CMAKE_SOURCE_DIR}/README.md\") set(CPACK_SOURCE_IGNORE_FILES /.git /\\\\\\\\.DS_Store ) message(\"\") message(\"--------------------------- Feature Summary ---------------------------\") feature_summary(WHAT ALL) # Summary message(\"\") message(\"------------------------------- Summary -------------------------------\") message(\"C++ compiler name: ${CMAKE_CXX_COMPILER_ID} at ${CMAKE_CXX_COMPILER}\") message(\"Build type: ${CMAKE_BUILD_TYPE}\") message(\"Source dir: ${CMAKE_SOURCE_DIR}\") message(\"Binary dir: ${CMAKE_BINARY_DIR}\") message(\"Install dir: ${CMAKE_INSTALL_PREFIX}\") message(\"C Flags: ${CMAKE_C_FLAGS}\") message(\"C++ Flags: ${CMAKE_CXX_FLAGS}\") message(\"IPO enabled: ${IPO_AVAILABLE}\") message(\"------------------------------- Modules -------------------------------\") message(\"CADET-CLI: ${ENABLE_CADET_CLI}\") message(\"Tools: ${ENABLE_CADET_TOOLS}\") message(\"Tests: ${ENABLE_TESTS}\") message(\"------------------------------- Options -------------------------------\") message(\"Logging: ${ENABLE_LOGGING}\") message(\"Benchmark mode: ${ENABLE_BENCHMARK}\") message(\"Platform-dependent timer: ${ENABLE_PLATFORM_TIMER}\") message(\"AD library: ${ADLIB}\") message(\"2D Models: ${ENABLE_2D_MODELS}\") message(\"Check analytic Jacobian: ${ENABLE_ANALYTIC_JACOBIAN_CHECK}\") message(\"----------------------------- Dependencies ----------------------------\") message(\"Found BLAS: ${BLAS_FOUND}\") if (BLAS_FOUND) message(\" Linker flags ${BLAS_LINKER_FLAGS}\") message(\" Libs ${BLAS_LIBRARIES}\") message(\" Underscore suffix ${BLAS_UNDERSCORE_SUFFIX}\") endif() message(\"Found LAPACK: ${LAPACK_FOUND}\") if (LAPACK_FOUND) message(\" Linker flags ${LAPACK_LINKER_FLAGS}\") message(\" Libs ${LAPACK_LIBRARIES}\") endif() message(\"Found TBB: ${TBB_FOUND}\") if (TBB_FOUND) message(\" Version ${TBB_VERSION} (Interface ${TBB_INTERFACE_VERSION})\") message(\" Include ${TBB_INCLUDE_DIRS}\") message(\" Definitions ${TBB_DEFINITIONS}\") message(\" Libs ${TBB_LIBRARIES}\") endif() if (ENABLE_PACKAGED_SUNDIALS) message(\"Found SUNDIALS: ${SUNDIALS_FOUND}\") message(\" Version ${SUNDIALS_VERSION}\") message(\" Packaged\") else() message(\"Found SUNDIALS: ${SUNDIALS_FOUND}\") if (SUNDIALS_FOUND) message(\" Version ${SUNDIALS_VERSION}\") message(\" Includes ${SUNDIALS_INCLUDE_DIRS}\") message(\" Libs ${SUNDIALS_LIBRARIES}\") endif() endif() if (ENABLE_2D_MODELS) message(\"Found SuperLU: ${SUPERLU_FOUND}\") if (SUPERLU_FOUND) message(\" Version ${SUPERLU_VERSION}\") message(\" Includes ${SUPERLU_INCLUDE_DIRS}\") message(\" Libs ${SUPERLU_LIBRARIES}\") message(\" Integer type ${SUPERLU_INT_TYPE}\") endif() message(\"Found UMFPACK: ${UMFPACK_FOUND}\") if (UMFPACK_FOUND) message(\" Version ${UMFPACK_VERSION}\") message(\" Includes ${UMFPACK_INCLUDE_DIRS}\") message(\" Libs ${UMFPACK_LIBRARIES}\") endif() endif() if (ENABLE_DG) message(\"Found Eigen3: ${Eigen3_FOUND}\") if (TARGET Eigen3::Eigen) message(\" Version ${Eigen3_VERSION}\") message(\" Includes ${Eigen3_INCLUDE_DIRS}\") endif() endif() message(\"Found HDF5: ${HDF5_FOUND}\") if (HDF5_FOUND) message(\" Version ${HDF5_VERSION}\") message(\" Includes ${HDF5_INCLUDE_DIRS}\") message(\" Libs ${HDF5_LIBRARIES}\") message(\" Defs ${HDF5_C_DEFINITIONS}\") endif() message(\"-----------------------------------------------------------------------\") message(\"\")\n{\"dependencies\":[\"hdf5\",\"suitesparse\",\"superlu\",\"eigen3\"]}\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/calibr8",
            "repo_link": "https://github.com/JuBiotech/calibr8",
            "content": {
                "codemeta": "",
                "readme": "[![PyPI version](https://img.shields.io/pypi/v/calibr8)](https://pypi.org/project/calibr8) [![pipeline](https://github.com/jubiotech/calibr8/workflows/pipeline/badge.svg)](https://github.com/jubiotech/calibr8/actions) [![coverage](https://codecov.io/gh/jubiotech/calibr8/branch/master/graph/badge.svg)](https://codecov.io/gh/jubiotech/calibr8) [![documentation](https://readthedocs.org/projects/calibr8/badge/?version=latest)](https://calibr8.readthedocs.io/en/latest/?badge=latest) [![DOI](https://zenodo.org/badge/306862348.svg)](https://zenodo.org/badge/latestdoi/306862348) # `calibr8` This package provides templates and functions for performing likelihood-based calibration modeling. To see implementation examples & excercises, you can go to [notebooks/](notebooks). # Installation `calibr8` is released on [PyPI](https://pypi.org/project/calibr8/): ``` pip install calibr8 ``` # Documentation Read the package documentation [here](https://calibr8.readthedocs.io/en/latest/?badge=latest). # Usage and Citing `calibr8` is licensed under the [GNU Affero General Public License v3.0](https://github.com/JuBiotech/calibr8/blob/master/LICENSE). When using `calibr8` in your work, please cite the [Helleckes & Osthege et al. (2022) paper](https://doi.org/10.1371/journal.pcbi.1009223) __and__ the [corresponding software version](https://doi.org/10.5281/zenodo.4127012). Note that the paper is a shared first co-authorship, which can be indicated by <sup>1</sup> in the bibliography. ```bibtex @article{calibr8Paper, doi = {10.1371/journal.pcbi.1009223}, author = {Helleckes$^1$, Laura Marie and Osthege$^1$, Michael and Wiechert, Wolfgang and von Lieres, Eric and Oldiges, Marco}, journal = {PLOS Computational Biology}, publisher = {Public Library of Science}, title = {Bayesian and calibration, process modeling and uncertainty quantification in biotechnology}, year = {2022}, month = {03}, volume = {18}, url = {https://doi.org/10.1371/journal.pcbi.1009223}, pages = {1-46}, number = {3} } @software{calibr8version, author = {Michael Osthege and Laura Helleckes}, title = {JuBiotech/calibr8: v6.5.2}, month = mar, year = 2022, publisher = {Zenodo}, version = {v6.5.2}, doi = {10.5281/zenodo.4127012}, url = {https://doi.org/10.5281/zenodo.4127012} } ``` Head over to Zenodo to [generate a BibTeX citation](https://doi.org/10.5281/zenodo.4127012) for the latest release.\n",
                "dependencies": "# inspired by https://hynek.me/articles/sharing-your-labor-of-love-pypi-quick-and-dirty/ [build-system] requires = [\"setuptools\", \"wheel\"] build-backend = \"setuptools.build_meta\" [tool.black] line-length = 110 [tool.isort] profile = \"black\" [tool.mypy] ignore_missing_imports = true exclude = [ 'test_.*?\\.py$', ]\nmatplotlib numpy scipy>1.6.0 typing_extensions\n# Copyright 2021 Forschungszentrum Jülich GmbH # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU Affero General Public License as published # by the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Affero General Public License for more details. # # You should have received a copy of the GNU Affero General Public License # along with this program. If not, see <https://www.gnu.org/licenses/>. import os import pathlib import re import setuptools __packagename__ = \"calibr8\" ROOT = pathlib.Path(__file__).parent def package_files(directory): assert os.path.exists(directory) fp_typed = pathlib.Path(ROOT, __packagename__, \"py.typed\") fp_typed.touch() paths = [str(fp_typed.absolute())] for (path, directories, filenames) in os.walk(directory): for filename in filenames: paths.append(os.path.join(\"..\", path, filename)) return paths def get_version(): VERSIONFILE = pathlib.Path(pathlib.Path(__file__).parent, __packagename__, \"core.py\") initfile_lines = open(VERSIONFILE, \"rt\").readlines() VSRE = r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\" for line in initfile_lines: mo = re.search(VSRE, line, re.M) if mo: return mo.group(1) raise RuntimeError(\"Unable to find version string in %s.\" % (VERSIONFILE,)) __version__ = get_version() setuptools.setup( name=__packagename__, packages=setuptools.find_packages(), version=__version__, description=\"Toolbox for non-linear calibration modeling.\", long_description=open(pathlib.Path(ROOT, \"README.md\")).read(), long_description_content_type=\"text/markdown\", url=\"https://github.com/JuBiotech/calibr8\", author=\"Laura Marie Helleckes, Michael Osthege\", author_email=\"l.helleckes@fz-juelich.de, m.osthege@fz-juelich.de\", license=\"GNU Affero General Public License v3\", classifiers=[ \"Programming Language :: Python\", \"Operating System :: OS Independent\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"License :: OSI Approved :: GNU Affero General Public License v3\", \"Intended Audience :: Science/Research\", \"Topic :: Scientific/Engineering\", \"Topic :: Scientific/Engineering :: Mathematics\", ], install_requires=[open(pathlib.Path(ROOT, \"requirements.txt\")).readlines()], package_data={ \"calibr8\": package_files(str(pathlib.Path(pathlib.Path(__file__).parent, \"calibr8\").absolute())) }, include_package_data=True, python_requires=\">=3.10\", )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cat4kit",
            "repo_link": "https://codebase.helmholtz.cloud/cat4kit/cat4kit-docker",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/celldetection",
            "repo_link": "https://github.com/FZJ-INM1-BDA/celldetection",
            "content": {
                "codemeta": "",
                "readme": "# Cell Detection [![Downloads](https://static.pepy.tech/badge/celldetection?l)](https://pepy.tech/project/celldetection) [![Test](https://github.com/FZJ-INM1-BDA/celldetection/workflows/Test/badge.svg)](https://github.com/FZJ-INM1-BDA/celldetection/actions?query=workflow%3ATest) [![PyPI](https://img.shields.io/pypi/v/celldetection?l)](https://pypi.org/project/celldetection/) [![Documentation Status](https://readthedocs.org/projects/celldetection/badge/?version=latest)](https://celldetection.readthedocs.io/en/latest/?badge=latest) [![DOI](https://zenodo.org/badge/349111085.svg)](https://zenodo.org/badge/latestdoi/349111085) ## ⭐ Showcase ###### NeurIPS 22 Cell Segmentation Competition ![neurips22](https://raw.githubusercontent.com/FZJ-INM1-BDA/celldetection/main/assets/neurips-cellseg-demo.png \"NeurIPS 22 Cell Segmentation Competition - Find more information here: https://neurips.cc/Conferences/2022/CompetitionTrack\") *https://openreview.net/forum?id=YtgRjBw-7GJ* ###### Nuclei of U2OS cells in a chemical screen ![bbbc039](https://raw.githubusercontent.com/FZJ-INM1-BDA/celldetection/main/assets/bbbc039-cpn-u22-demo.png \"BBBC039 demo with CpnU22 - Find the dataset here: https://bbbc.broadinstitute.org/BBBC039\") *https://bbbc.broadinstitute.org/BBBC039 (CC0)* ###### P. vivax (malaria) infected human blood ![bbbc041](https://raw.githubusercontent.com/FZJ-INM1-BDA/celldetection/main/assets/bbbc041-cpn-u22-demo.png \"BBBC041 demo with CpnU22 - Find the dataset here: https://bbbc.broadinstitute.org/BBBC041\") *https://bbbc.broadinstitute.org/BBBC041 (CC BY-NC-SA 3.0)* ## 🛠 Install Make sure you have [PyTorch](https://pytorch.org/get-started/locally/) installed. ### PyPI ``` pip install -U celldetection ``` ### GitHub ``` pip install git+https://github.com/FZJ-INM1-BDA/celldetection.git ``` ## 💾 Trained models ```python model = cd.fetch_model(model_name, check_hash=True) ``` | model name | training data | link | |---------------------------------------------|----------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------:| | `ginoro_CpnResNeXt101UNet-fbe875f1a3e5ce2c` | BBBC039, BBBC038, Omnipose, Cellpose, Sartorius - Cell Instance Segmentation, Livecell, NeurIPS 22 CellSeg Challenge | [🔗](https://celldetection.org/torch/models/ginoro_CpnResNeXt101UNet-fbe875f1a3e5ce2c.pt) | <details> <summary style=\"font-weight: bold; color: #888888\">Run a demo with a pretrained model</summary> ```python import torch, cv2, celldetection as cd from skimage.data import coins from matplotlib import pyplot as plt # Load pretrained model device = 'cuda' if torch.cuda.is_available() else 'cpu' model = cd.fetch_model('ginoro_CpnResNeXt101UNet-fbe875f1a3e5ce2c', check_hash=True).to(device) model.eval() # Load input img = coins() img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) print(img.dtype, img.shape, (img.min(), img.max())) # Run model with torch.no_grad(): x = cd.to_tensor(img, transpose=True, device=device, dtype=torch.float32) x = x / 255 # ensure 0..1 range x = x[None] # add batch dimension: Tensor[3, h, w] -> Tensor[1, 3, h, w] y = model(x) # Show results for each batch item contours = y['contours'] for n in range(len(x)): cd.imshow_row(x[n], x[n], figsize=(16, 9), titles=('input', 'contours')) cd.plot_contours(contours[n]) plt.show() ``` </details> ## 🔬 Architectures ```python import celldetection as cd ``` <details> <summary style=\"font-weight: bold; color: #888888\">Contour Proposal Networks</summary> - [`cd.models.CPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CPN) - [`cd.models.CpnU22`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnU22) - [`cd.models.CPNCore`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CPNCore) - [`cd.models.CpnResUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResUNet) - [`cd.models.CpnSlimU22`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnSlimU22) - [`cd.models.CpnWideU22`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnWideU22) - [`cd.models.CpnResNet18FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet18FPN) - [`cd.models.CpnResNet34FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet34FPN) - [`cd.models.CpnResNet50FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet50FPN) - [`cd.models.CpnResNeXt50FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNeXt50FPN) - [`cd.models.CpnResNet101FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet101FPN) - [`cd.models.CpnResNet152FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet152FPN) - [`cd.models.CpnResNet18UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet18UNet) - [`cd.models.CpnResNet34UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet34UNet) - [`cd.models.CpnResNet50UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet50UNet) - [`cd.models.CpnResNeXt101FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNeXt101FPN) - [`cd.models.CpnResNeXt152FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNeXt152FPN) - [`cd.models.CpnResNeXt50UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNeXt50UNet) - [`cd.models.CpnResNet101UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet101UNet) - [`cd.models.CpnResNet152UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNet152UNet) - [`cd.models.CpnResNeXt101UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNeXt101UNet) - [`cd.models.CpnResNeXt152UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnResNeXt152UNet) - [`cd.models.CpnWideResNet50FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnWideResNet50FPN) - [`cd.models.CpnWideResNet101FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnWideResNet101FPN) - [`cd.models.CpnMobileNetV3LargeFPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnMobileNetV3LargeFPN) - [`cd.models.CpnMobileNetV3SmallFPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnMobileNetV3SmallFPN) </details> <details> <summary style=\"font-weight: bold; color: #888888\">PyTorch Image Models (timm)</summary> Also have a look at [Timm Documentation](https://huggingface.co/docs/timm/index). ```python import timm timm.list_models(filter='*') # explore available models ``` - [`cd.models.CpnTimmMaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnTimmMaNet) - [`cd.models.CpnTimmUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnTimmUNet) - [`cd.models.TimmEncoder`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.timmodels.TimmEncoder) - [`cd.models.TimmFPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.TimmFPN) - [`cd.models.TimmMaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.manet.TimmMaNet) - [`cd.models.TimmUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.TimmUNet) </details> <details> <summary style=\"font-weight: bold; color: #888888\">Segmentation Models PyTorch (smp)</summary> ```python import segmentation_models_pytorch as smp smp.encoders.get_encoder_names() # explore available models ``` ```python encoder = cd.models.SmpEncoder(encoder_name='mit_b5', pretrained='imagenet') ``` Find a list of [Smp Encoders](https://smp.readthedocs.io/en/latest/encoders.html) in the `smp` documentation. - [`cd.models.CpnSmpMaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnSmpMaNet) - [`cd.models.CpnSmpUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.cpn.CpnSmpUNet) - [`cd.models.SmpEncoder`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.smp.SmpEncoder) - [`cd.models.SmpFPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.SmpFPN) - [`cd.models.SmpMaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.manet.SmpMaNet) - [`cd.models.SmpUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.SmpUNet) </details> <details> <summary style=\"font-weight: bold; color: #888888\">U-Nets</summary> ```python # U-Nets are available in 2D and 3D import celldetection as cd model = cd.models.ResNeXt50UNet(in_channels=3, out_channels=1, nd=3) ``` - [`cd.models.U22`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.U22) - [`cd.models.U17`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.U17) - [`cd.models.U12`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.U12) - [`cd.models.UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.UNet) - [`cd.models.WideU22`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.WideU22) - [`cd.models.SlimU22`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.SlimU22) - [`cd.models.ResUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResUNet) - [`cd.models.UNetEncoder`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.UNetEncoder) - [`cd.models.ResNet50UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNet50UNet) - [`cd.models.ResNet18UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNet18UNet) - [`cd.models.ResNet34UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNet34UNet) - [`cd.models.ResNet152UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNet152UNet) - [`cd.models.ResNet101UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNet101UNet) - [`cd.models.ResNeXt50UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNeXt50UNet) - [`cd.models.ResNeXt152UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNeXt152UNet) - [`cd.models.ResNeXt101UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.ResNeXt101UNet) - [`cd.models.WideResNet50UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.WideResNet50UNet) - [`cd.models.WideResNet101UNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.WideResNet101UNet) - [`cd.models.MobileNetV3SmallUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.MobileNetV3SmallUNet) - [`cd.models.MobileNetV3LargeUNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.unet.MobileNetV3LargeUNet) </details> <details> <summary style=\"font-weight: bold; color: #888888\">MA-Nets</summary> ```python # Many MA-Nets are available in 2D and 3D import celldetection as cd encoder = cd.models.ConvNeXtSmall(in_channels=3, nd=3) model = cd.models.MaNet(encoder, out_channels=1, nd=3) ``` - [`cd.models.MaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.manet.MaNet) - [`cd.models.SmpMaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.manet.SmpMaNet) - [`cd.models.TimmMaNet`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.manet.TimmMaNet) </details> <details> <summary style=\"font-weight: bold; color: #888888\">Feature Pyramid Networks</summary> - [`cd.models.FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.FPN) - [`cd.models.ResNet18FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNet18FPN) - [`cd.models.ResNet34FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNet34FPN) - [`cd.models.ResNet50FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNet50FPN) - [`cd.models.ResNeXt50FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNeXt50FPN) - [`cd.models.ResNet101FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNet101FPN) - [`cd.models.ResNet152FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNet152FPN) - [`cd.models.ResNeXt101FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNeXt101FPN) - [`cd.models.ResNeXt152FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.ResNeXt152FPN) - [`cd.models.WideResNet50FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.WideResNet50FPN) - [`cd.models.WideResNet101FPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.WideResNet101FPN) - [`cd.models.MobileNetV3LargeFPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.MobileNetV3LargeFPN) - [`cd.models.MobileNetV3SmallFPN`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.fpn.MobileNetV3SmallFPN) </details> <details> <summary style=\"font-weight: bold; color: #888888\">ConvNeXt Networks</summary> ```python # ConvNeXt Networks are available in 2D and 3D import celldetection as cd model = cd.models.ConvNeXtSmall(in_channels=3, nd=3) ``` - [`cd.models.ConvNeXt`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.convnext.MaNet) - [`cd.models.ConvNeXtTiny`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.convnext.ConvNeXtTiny) - [`cd.models.ConvNeXtSmall`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.convnext.ConvNeXtSmall) - [`cd.models.ConvNeXtBase`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.convnext.ConvNeXtBase) - [`cd.models.ConvNeXtLarge`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.convnext.ConvNeXtLarge) </details> <details> <summary style=\"font-weight: bold; color: #888888\">Residual Networks</summary> ```python # Residual Networks are available in 2D and 3D import celldetection as cd model = cd.models.ResNet50(in_channels=3, nd=3) ``` - [`cd.models.ResNet18`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNet18) - [`cd.models.ResNet34`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNet34) - [`cd.models.ResNet50`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNet50) - [`cd.models.ResNet101`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNet101) - [`cd.models.ResNet152`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNet152) - [`cd.models.WideResNet50_2`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.WideResNet50_2) - [`cd.models.ResNeXt50_32x4d`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNeXt50_32x4d) - [`cd.models.WideResNet101_2`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.WideResNet101_2) - [`cd.models.ResNeXt101_32x8d`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNeXt101_32x8d) - [`cd.models.ResNeXt152_32x8d`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.resnet.ResNeXt152_32x8d) </details> <details> <summary style=\"font-weight: bold; color: #888888\">Mobile Networks</summary> - [`cd.models.MobileNetV3Large`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.mobilenetv3.MobileNetV3Large) - [`cd.models.MobileNetV3Small`](https://docs.celldetection.org/en/latest/celldetection.models.html#celldetection.models.mobilenetv3.MobileNetV3Small) </details> ## 🐳 Docker Find us on Docker Hub: https://hub.docker.com/r/ericup/celldetection You can pull the latest version of `celldetection` via: ``` docker pull ericup/celldetection:latest ``` <details> <summary style=\"font-weight: bold; color: #888888\">CPN inference via Docker with GPU</summary> ``` docker run --rm \\ -v $PWD/docker/outputs:/outputs/ \\ -v $PWD/docker/inputs/:/inputs/ \\ -v $PWD/docker/models/:/models/ \\ --gpus=\"device=0\" \\ celldetection:latest /bin/bash -c \\ \"python cpn_inference.py --tile_size=1024 --stride=768 --precision=32-true\" ``` </details> <details> <summary style=\"font-weight: bold; color: #888888\">CPN inference via Docker with CPU</summary> ``` docker run --rm \\ -v $PWD/docker/outputs:/outputs/ \\ -v $PWD/docker/inputs/:/inputs/ \\ -v $PWD/docker/models/:/models/ \\ celldetection:latest /bin/bash -c \\ \"python cpn_inference.py --tile_size=1024 --stride=768 --precision=32-true --accelerator=cpu\" ``` </details> ### Apptainer You can also pull our Docker images for the use with [Apptainer](https://apptainer.org/) (formerly [Singularity](https://github.com/apptainer/singularity)) with this command: ``` apptainer pull --dir . --disable-cache docker://ericup/celldetection:latest ``` ## 🤗 Hugging Face Spaces Find us on Hugging Face and upload your own images for segmentation: https://huggingface.co/spaces/ericup/celldetection There's also an API (Python & JavaScript), allowing you to utilize community GPUs (currently Nvidia A100) remotely! <details> <summary style=\"font-weight: bold; color: #888888\">Hugging Face API</summary> ### Python ```python from gradio_client import Client # Define inputs (local filename or URL) inputs = 'https://raw.githubusercontent.com/scikit-image/scikit-image/main/skimage/data/coins.png' # Set up client client = Client(\"ericup/celldetection\") # Predict overlay_filename, img_filename, h5_filename, csv_filename = client.predict( inputs, # str: Local filepath or URL of your input image # Model name 'ginoro_CpnResNeXt101UNet-fbe875f1a3e5ce2c', # Custom Score Threshold (numeric value between 0 and 1) False, .9, # bool: Whether to use custom setting; float: Custom setting # Custom NMS Threshold False, .3142, # bool: Whether to use custom setting; float: Custom setting # Custom Number of Sample Points False, 128, # bool: Whether to use custom setting; int: Custom setting # Overlapping objects True, # bool: Whether to allow overlapping objects # API name (keep as is) api_name=\"/predict\" ) # Example usage: Code below only shows how to use the results from matplotlib import pyplot as plt import celldetection as cd import pandas as pd # Read results from local temporary files img = imread(img_filename) overlay = imread(overlay_filename) # random colors per instance; transparent overlap properties = pd.read_csv(csv_filename) contours, scores, label_image = cd.from_h5(h5_filename, 'contours', 'scores', 'labels') # Optionally display overlay cd.imshow_row(img, img, figsize=(16, 9)) cd.imshow(overlay) plt.show() # Optionally display contours with text cd.imshow_row(img, img, figsize=(16, 9)) cd.plot_contours(contours, texts=['score: %d%%\\narea: %d' % s for s in zip((scores * 100).round(), properties.area)]) plt.show() ``` ### Javascript ```javascript import { client } from \"@gradio/client\"; const response_0 = await fetch(\"https://raw.githubusercontent.com/scikit-image/scikit-image/main/skimage/data/coins.png\"); const exampleImage = await response_0.blob(); const app = await client(\"ericup/celldetection\"); const result = await app.predict(\"/predict\", [ exampleImage, // blob: Your input image // Model name (hosted model or URL) \"ginoro_CpnResNeXt101UNet-fbe875f1a3e5ce2c\", // Custom Score Threshold (numeric value between 0 and 1) false, .9, // bool: Whether to use custom setting; float: Custom setting // Custom NMS Threshold false, .3142, // bool: Whether to use custom setting; float: Custom setting // Custom Number of Sample Points false, 128, // bool: Whether to use custom setting; int: Custom setting // Overlapping objects true, // bool: Whether to allow overlapping objects // API name (keep as is) api_name=\"/predict\" ]); ``` </details> ## 🧑‍💻 Napari Plugin Find our Napari Plugin here: https://github.com/FZJ-INM1-BDA/celldetection-napari </br> Find out more about Napari here: https://napari.org ![bbbc039](https://raw.githubusercontent.com/FZJ-INM1-BDA/celldetection-napari/main/assets/coins-demo.png \"Napari Plugin\") You can install it via pip: ``` pip install git+https://github.com/FZJ-INM1-BDA/celldetection-napari.git ``` ## 🏆 Awards - [NeurIPS 2022 Cell Segmentation Challenge](https://neurips22-cellseg.grand-challenge.org/): Winner Finalist Award ## 📝 Citing If you find this work useful, please consider giving a **star** ⭐️ and **citation**: ``` @article{UPSCHULTE2022102371, title = {Contour proposal networks for biomedical instance segmentation}, journal = {Medical Image Analysis}, volume = {77}, pages = {102371}, year = {2022}, issn = {1361-8415}, doi = {https://doi.org/10.1016/j.media.2022.102371}, url = {https://www.sciencedirect.com/science/article/pii/S136184152200024X}, author = {Eric Upschulte and Stefan Harmeling and Katrin Amunts and Timo Dickscheid}, keywords = {Cell detection, Cell segmentation, Object detection, CPN}, } ``` ## 🔗 Links - [Article (sciencedirect)](https://www.sciencedirect.com/science/article/pii/S136184152200024X \"Contour Proposal Networks for Biomedical Instance Segmentation\") - [PDF (sciencedirect)](https://www.sciencedirect.com/science/article/pii/S136184152200024X/pdfft \"Contour Proposal Networks for Biomedical Instance Segmentation\") - [PyPI](https://pypi.org/project/celldetection/ \"CellDetection\") - [Documentation](https://docs.celldetection.org \"Documentation\") ## 🧑‍🔬 Thanks! [![Stargazers repo roster for @FZJ-INM1-BDA/celldetection](http://reporoster.com/stars/FZJ-INM1-BDA/celldetection)](https://github.com/FZJ-INM1-BDA/celldetection/stargazers) [![Forkers repo roster for @FZJ-INM1-BDA/celldetection](http://reporoster.com/forks/FZJ-INM1-BDA/celldetection)](https://github.com/FZJ-INM1-BDA/celldetection/network/members)\n",
                "dependencies": "numpy matplotlib scipy scikit-image opencv-python torch>=2.0.0 torchvision tensorboard seaborn tqdm h5py pillow nvidia-ml-py albumentations>=1.3.0 timm segmentation-models-pytorch pandas pytorch_lightning pyyaml\nfrom setuptools import setup from os.path import join, dirname, abspath def read_utf8(*args): with open(join(*args), encoding=\"utf-8\") as f: return f.read() directory, m = dirname(abspath(__file__)), {} exec(read_utf8(directory, 'celldetection', '__meta__.py'), m) requirements = read_utf8(directory, 'requirements.txt').strip().split(\"\\n\") long_description = read_utf8(directory, 'README.md') setup( author=m['__author__'], author_email=m['__email__'], name=m['__title__'], version=m['__version__'], description=m['__summary__'], long_description=long_description, long_description_content_type='text/markdown', url=m['__url__'], packages=['celldetection', 'celldetection.data', 'celldetection.callbacks', 'celldetection.optim', 'celldetection.data.datasets', 'celldetection.models', 'celldetection.mpi', 'celldetection.ops', 'celldetection.util', 'celldetection.visualization', 'celldetection_scripts'], package_data={'': ['LICENSE', 'requirements.txt', 'README.md']}, include_package_data=True, install_requires=requirements, license=m['__license__'], keywords=['cell', 'detection', 'object', 'segmentation', 'pytorch', 'cpn', 'contour', 'proposal', 'network', 'deep', 'learning', 'unet', 'fzj', 'julich', 'juelich', 'ai'], classifiers=[ 'Topic :: Scientific/Engineering :: Artificial Intelligence', 'Topic :: Scientific/Engineering :: Medical Science Apps.', 'License :: OSI Approved :: Apache Software License', 'Operating System :: OS Independent' ], entry_points={ 'console_scripts': [ 'cd-inference-cpn=celldetection_scripts.cpn_inference:main', # 'cd-train=celldetection_scripts.train:main' ] } )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cellrank",
            "repo_link": "https://github.com/theislab/cellrank",
            "content": {
                "codemeta": "",
                "readme": "|PyPI| |Downloads| |CI| |Docs| |Codecov| |Discourse| CellRank 2: Unified fate mapping in multiview single-cell data ============================================================== .. image:: docs/_static/img/light_mode_overview.png#gh-light-mode-only :width: 600px :align: center :class: only-light .. image:: docs/_static/img/dark_mode_overview.png#gh-dark-mode-only :width: 600px :align: center **CellRank** is a modular framework to study cellular dynamics based on Markov state modeling of multi-view single-cell data. See our `documentation`_, and the `CellRank 1`_ and `CellRank 2 manuscript`_ to learn more. .. important:: Please refer to `our citation guide <https://github.com/theislab/cellrank/blob/main/docs/about/cite.rst>`_ to cite our software correctly. CellRank scales to large cell numbers, is fully compatible with the `scverse`_ ecosystem, and easy to use. In the backend, it is powered by `pyGPCCA`_ (`Reuter et al. (2018)`_). Feel free to open an `issue`_ if you encounter a bug, need our help or just want to make a comment/suggestion. CellRank's key applications --------------------------- - Estimate differentiation direction based on a varied number of biological priors, including RNA velocity (`La Manno et al. (2018)`_, `Bergen et al. (2020)`_), any pseudotime or developmental potential, experimental time points, metabolic labels, and more. - Compute initial, terminal and intermediate macrostates. - Infer fate probabilities and driver genes. - Visualize and cluster gene expression trends. - ... and much more, check out our `documentation`_. .. |PyPI| image:: https://img.shields.io/pypi/v/cellrank.svg :target: https://pypi.org/project/cellrank :alt: PyPI .. |Downloads| image:: https://static.pepy.tech/badge/cellrank :target: https://pepy.tech/project/cellrank :alt: Downloads .. |Discourse| image:: https://img.shields.io/discourse/posts?color=yellow&logo=discourse&server=https%3A%2F%2Fdiscourse.scverse.org :target: https://discourse.scverse.org/c/ecosystem/cellrank/ :alt: Discourse .. |CI| image:: https://img.shields.io/github/actions/workflow/status/theislab/cellrank/test.yml?branch=main :target: https://github.com/theislab/cellrank/actions :alt: CI .. |Docs| image:: https://img.shields.io/readthedocs/cellrank :target: https://cellrank.readthedocs.io/ :alt: Documentation .. |Codecov| image:: https://codecov.io/gh/theislab/cellrank/branch/main/graph/badge.svg :target: https://codecov.io/gh/theislab/cellrank :alt: Coverage .. _La Manno et al. (2018): https://doi.org/10.1038/s41586-018-0414-6 .. _Bergen et al. (2020): https://doi.org/10.1038/s41587-020-0591-3 .. _Reuter et al. (2018): https://doi.org/10.1021/acs.jctc.8b00079 .. _scverse: https://scverse.org/ .. _pyGPCCA: https://github.com/msmdev/pyGPCCA .. _CellRank 1: https://www.nature.com/articles/s41592-021-01346-6 .. _CellRank 2 manuscript: https://doi.org/10.1038/s41592-024-02303-9 .. _documentation: https://cellrank.org .. _issue: https://github.com/theislab/cellrank/issues/new/choose\n",
                "dependencies": "[build-system] requires = [\"setuptools>=61\", \"setuptools-scm[toml]>=6.2\"] build-backend = \"setuptools.build_meta\" [project] name = \"cellrank\" dynamic = [\"version\"] description = \"CellRank: dynamics from multi-view single-cell data\" readme = \"README.rst\" requires-python = \">=3.9\" license = \"BSD-3-Clause\" license-files = [\"LICENSE\"] classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", \"Natural Language :: English\", \"Operating System :: POSIX :: Linux\", \"Operating System :: MacOS :: MacOS X\", \"Operating System :: Microsoft :: Windows\", \"Typing :: Typed\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Topic :: Scientific/Engineering :: Bio-Informatics\", \"Topic :: Scientific/Engineering :: Mathematics\", \"Topic :: Scientific/Engineering :: Visualization\", ] keywords = [ \"single-cell\", \"bio-informatics\", \"RNA velocity\", \"Markov chain\", \"GPCCA\", ] authors = [ {name = \"Marius Lange\"}, {name = \"Michal Klein\"}, {name = \"Philipp Weiler\"}, ] maintainers = [ {name = \"Michal Klein\", email = \"info@cellrank.org\"} ] dependencies = [ \"anndata>=0.9\", \"docrep>=0.3.0\", \"joblib>=0.13.1\", \"matplotlib>=3.5.0\", \"networkx>=2.2\", \"numba>=0.51.0,!=0.57.0\", \"numpy>=1.22.0\", \"pandas>=1.5.0\", \"pygam>=0.8.0\", \"pygpcca>=1.0.4\", \"scanpy>=1.7.2\", \"scikit-learn>=0.24.0\", \"scipy>=1.12.0\", \"scvelo>=0.2.5\", \"seaborn>=0.10.0\", \"wrapt>=1.12.1\", ] [project.optional-dependencies] dev = [ \"pre-commit>=3.0.0\", \"tox>=4\", ] test = [ \"pytest>=8\", \"pytest-mock>=3.5.0\", \"pytest-cov>=4\", \"pytest-xdist\", \"coverage[toml]>=7\", \"zarr<3\", \"igraph\", \"leidenalg\", \"Pillow\", \"jax\", ] docs = [ \"sphinx>=5.1.1\", \"furo>=2022.09.29\", \"myst-nb>=0.17.1\", \"sphinx-tippy>=0.4.1\", \"sphinx-autodoc-typehints>=1.10.3\", \"sphinx_copybutton>=0.5.0\", \"sphinx_design>=0.3.0\", \"sphinxcontrib-bibtex>=2.3.0\", \"sphinxcontrib-spelling>=7.6.2\", ] [project.urls] Homepage = \"https://github.com/theislab/cellrank\" Download = \"https://cellrank.readthedocs.io/en/latest/installation.html\" \"Bug Tracker\" = \"https://github.com/theislab/cellrank/issues\" Documentation = \"https://cellrank.readthedocs.io\" \"Source Code\" = \"https://github.com/theislab/cellrank\" [tool.setuptools] package-dir = {\"\" = \"src\"} include-package-data = true [tool.setuptools_scm] [tool.ruff] target-version = \"py39\" line-length = 120 [tool.ruff.lint] exclude = [ \".eggs\", \".git\", \".ruff_cache\", \".tox\", \"__pypackages__\", \"_build\", \"build\", \"dist\", ] ignore = [ \"PT011\", # TODO/ # Do not implicitly `return None` in function able to return non-`None` value \"RET502\", # Missing explicit `return` at the end of function able to return non-`None` value \"RET503\", # Do not assign a lambda expression, use a def -> lambda expression assignments are convenient \"E731\", # allow I, O, l as variable names -> I is the identity matrix, i, j, k, l is reasonable indexing notation \"E741\", # Missing docstring in public package \"D104\", # Missing docstring in public module \"D100\", # Missing docstring in __init__ \"D107\", # Missing docstring in magic method \"D105\", ] select = [ \"D\", # flake8-docstrings \"E\", # pycodestyle \"F\", # pyflakes \"W\", # pycodestyle \"Q\", # flake8-quotes \"SIM\", # flake8-simplify \"NPY\", # NumPy-specific rules \"PT\", # flake8-pytest-style \"TID\", # flake8-tidy-imports \"B\", # flake8-bugbear \"UP\", # pyupgrade \"C4\", # flake8-comprehensions \"BLE\", # flake8-blind-except \"T20\", # flake8-print \"RET\", # flake8-raise ] unfixable = [\"B\", \"C4\", \"BLE\", \"T20\", \"RET\"] [tool.ruff.lint.per-file-ignores] \"tests/*\" = [\"D\"] \"*/__init__.py\" = [\"F401\"] \"docs/*\" = [\"D\"] [tool.ruff.lint.pydocstyle] convention = \"numpy\" [tool.ruff.lint.flake8-tidy-imports] ban-relative-imports = \"all\" [tool.ruff.lint.flake8-quotes] inline-quotes = \"double\" [tool.black] line-length = 120 target-version = ['py39'] include = '\\.pyi?$' [tool.isort] profile = \"black\" include_trailing_comma = true sections = [\"FUTURE\", \"STDLIB\", \"THIRDPARTY\", \"GENERIC\", \"NUMERIC\", \"PLOTTING\", \"BIO\", \"FIRSTPARTY\", \"LOCALFOLDER\"] # also contains what we import in notebooks known_generic = [\"wrapt\", \"joblib\"] known_numeric = [\"numpy\", \"numba\", \"scipy\", \"jax\", \"pandas\", \"sklearn\", \"networkx\", \"statsmodels\"] known_bio = [\"anndata\", \"scanpy\"] known_plotting = [\"IPython\", \"matplotlib\", \"mpl_toolkits\", \"seaborn\"] [tool.pytest.ini_options] testpaths = \"tests\" xfail_strict = true [tool.coverage.run] branch = true parallel = true source = [\"src/\"] omit = [ \"*/__init__.py\", ] [tool.coverage.report] exclude_lines = [ '\\#.*pragma:\\s*no.?cover', \"^if __name__ == .__main__.:$\", '^\\s*raise AssertionError\\b', '^\\s*raise NotImplementedError\\b', '^\\s*return NotImplemented\\b', ] precision = 2 show_missing = true skip_empty = true sort = \"Miss\" [tool.rstcheck] ignore_directives = [ \"toctree\", \"currentmodule\", \"autosummary\", \"module\", \"automodule\", \"autoclass\", \"bibliography\", \"grid\", ] ignore_roles = [ \"mod\", \"class\", \"attr\", \"func\", \"meth\", \"doc\", \"cite\", ] [tool.doc8] max_line_length = 120 ignore-path = \"docs/release/**.rst\" [tool.tox] legacy_tox_ini = \"\"\" [tox] # TODO(michalk8): upgrade to `tox>=4.0` once `tox-conda` supports it requires = tox-conda isolated_build = true envlist = lint-code,py{3.9,3.10,3.11,3.12,3.13}-{slepc,noslepc} skip_missing_interpreters = true [testenv] conda_deps = py: r-mgcv py: rpy2 slepc: mpi4py slepc: petsc4py slepc: slepc4py conda_channels= conda-forge extras = test passenv = PYTEST_* CI commands = python -m pytest {tty:--color=yes} {posargs: \\ --cov={envsitepackagesdir}{/}cellrank --cov-config={toxinidir}{/}pyproject.toml \\ --no-cov-on-fail --cov-report=xml --cov-report=term-missing:skip-covered} [testenv:lint-code] description = Lint the code. deps = pre-commit>=3.0.0 skip_install = true commands = pre-commit run --all-files --show-diff-on-failure [testenv:lint-docs] description = Lint the documentation. deps = extras = docs ignore_errors = true allowlist_externals = make passenv = PYENCHANT_LIBRARY_PATH setenv = SPHINXOPTS = -W -q --keep-going changedir = {toxinidir}{/}docs commands = # make linkcheck {posargs} # make spelling {posargs} [testenv:clean-docs] description = Remove the documentation. deps = skip_install = true changedir = {toxinidir}{/}docs allowlist_externals = make commands = make clean [testenv:build-docs] description = Build the documentation. deps = extras = docs allowlist_externals = make changedir = {toxinidir}{/}docs commands = make html {posargs} commands_post = python -c 'import pathlib; print(\"Documentation is under:\", pathlib.Path(\"{toxinidir}\") / \"docs\" / \"_build\" / \"html\" / \"index.html\")' [testenv:build-package] description = Build the package. deps = build twine allowlist_externals = rm commands = rm -rf {toxinidir}{/}dist python -m build --sdist --wheel --outdir {toxinidir}{/}dist{/} {posargs:} python -m twine check {toxinidir}{/}dist{/}* commands_post = python -c 'import pathlib; print(f\"Package is under:\", pathlib.Path(\"{toxinidir}\") / \"dist\")' [testenv:format-references] description = Format references.bib. deps = skip_install = true allowlist_externals = biber commands = biber --tool --output_file={toxinidir}{/}docs{/}references.bib --nolog \\ --output_align --output_indent=2 --output_fieldcase=lower \\ --output_legacy_dates --output-field-replace=journaltitle:journal,thesis:phdthesis,institution:school \\ {toxinidir}{/}docs{/}references.bib \"\"\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/chase",
            "repo_link": "https://github.com/ChASE-library/ChASE",
            "content": {
                "codemeta": "",
                "readme": "[![License](https://img.shields.io/github/license/ChASE-library/ChASE)](https://github.com/ChASE-library/ChASE/blob/master/LICENSE) [![DOI](https://zenodo.org/badge/349075288.svg)](https://zenodo.org/badge/latestdoi/349075288) [![Latest Version](https://img.shields.io/github/v/release/ChASE-library/ChASE)](https://github.com/ChASE-library/ChASE/releases/latest) [![DOI](https://img.shields.io/badge/DOI-10.1145%2F3313828%20-orange)](https://doi.org/10.1145/3313828) [![DOI](https://img.shields.io/badge/DOI-10.1002%2Fcpe.3394%20-orange)](https://doi.org/10.1002/cpe.3394) [![coverage](https://gitlab.jsc.fz-juelich.de/chase/chase-library/ChASE/badges/master/coverage.svg?job=coverage)](https://gitlab.jsc.fz-juelich.de/chase/chase-library/ChASE/badges/master/coverage.svg) ![Coverage](https://chase-library.github.io/ChASE/_images/coverage/coverage.svg) <img src=\"docs/images/ChASE_Logo_RGB.png\" alt=\"Matrix Generation Pattern\" style=\"zoom:40%;\" /> # ChASE: a Chebyshev Accelerated Subspace Eigensolver for Dense Eigenproblems The **Ch**ebyshev **A**ccelerated **S**ubspace **E**igensolver (ChASE) is a modern and scalable library based on subspace iteration with polynomial acceleration to solve dense Hermitian (Symmetric) algebraic eigenvalue problems, especially solving dense Hermitian eigenproblems arragend in a sequence. Novel to ChASE is the computation of the spectral estimates that enter in the filter and an optimization of the polynomial degree that further reduces the necessary floating-point operations. ChASE is written in C++ using the modern software engineering concepts that favor a simple integration in application codes and a straightforward portability over heterogeneous platforms. When solving sequences of Hermitian eigenproblems for a portion of their extremal spectrum, ChASE greatly benefits from the sequence's spectral properties and outperforms direct solvers in many scenarios. The library ships with two distinct parallelization schemes, supports execution over distributed GPUs, and is easily extensible to other parallel computing architectures. ## Use Case and Features - **Real and Complex:** ChASE is templated for real and complex numbers. So it can be used to solve *real symmetric* eigenproblems as well as *complex Hermitian* ones. - **Eigespectrum:** ChASE algorithm is designed to solve for the *extremal portion* of the eigenspectrum of matrix `A`. The library is particularly efficient when no more than `20%` of the extremal portion of the eigenspectrum is sought after. For larger fractions the subspace iteration algorithm may struggle to be competitive. Converge could become an issue for fractions close to or larger than `50%`. - **Type of Problem:** ChASE can currently handle only standard eigenvalue problems. - **Sequences:** ChASE is particularly efficient when dealing with *sequences of eigenvalue problems*, where the eigenvectors solving for one problem can be use as input to accelerate the solution of the next one. - **Vectors input:** Since it is based on subspace iteration, ChASE can receive as input a matrix of vector equal to the number of desired eigenvalues. ChASE can experience substantial speed-ups when this input matrix contains some information about the sought after eigenvectors. - **Degree optimization:** For a fixed accuracy level, ChASE can optimize the degree of the Chebyshev polynomial filter so as to minimize the number of FLOPs necessary to reach convergence. - **Precision:** ChASE is also templated to work in *Single Precision* (SP) or *Double Precision* (DP). ## Builds of ChASE ChASE supports different builds for different systems with different architectures: - **Shared memory build:** This is the simplest configuration and should be exclusively selected when ChASE is used on only one computing node or on a single GPU. - **MPI+Threads build:** On multi-core homogeneous CPU clusters, ChASE is best used in its pure MPI build. In this configuration, ChASE is typically used with one MPI rank per NUMA domain and as many threads as number of available cores per NUMA domain. - **Multi-GPU build:** ChASE can be configured to take advantage of GPUs on heterogeneous computing clusters. Currently we support the use of one GPU per MPI rank. Multiple-GPU per computing node can be used when MPI rank number per node equals to the GPU number per node. - **NCCL Backend:** by default, ChASE uses **[NCCL](https://developer.nvidia.com/nccl)** as backend for the collective communications across different GPUs. - **CUDA-Aware MPI Backend**: alternatively, CUDA-Aware MPI can be used for the communications. ## Supported Data types ChASE supports different data types: - **Shared memory build** requires dense matrices to be column major. - **Distributed-memory build** support two types of data distribution of matrix `A` across 2D MPI/GPU grid: - **Block Distribution**: each MPI rank of 2D grid is assigned a block of dense matrix **A**. - **Block-Cyclic Distribution**: an distribution scheme for implementation of dense matrix computations on distributed-memory machines, to improve the load balance of matrix computation if the amount of work differs for different entries of a matrix. For more details, please refer to [Netlib](https://www.netlib.org/scalapack/slug/node75.html) . ## Quick Start ### Installing Dependencies ```bash #Linux Operating System sudo apt-get install cmake #install CMake sudo apt-get install build-essential #install GNU Compiler sudo apt-get install libopenblas-dev #install BLAS and LAPACK sudo apt-get install libopenmpi-dev #install MPI #Apple Mac Operating System sudo port install cmake #install CMake sudo port install gcc10 #install GNU Compiler sudo port select --set gcc mp-gcc10 #Set installed GCC as C compiler sudo port install OpenBLAS +native #install BLAS and LAPACK sudo port install openmpi #install MPI sudo port select --set mpi openmpi-mp-fortran #Set installed MPI as MPI compiler ``` ### Cloning ChASE source code ```bash git clone https://github.com/ChASE-library/ChASE #cloning the ChASE repository git checkout v1.0.0 #it is recommended to check out the latest stable tag. ``` ### Building and Installing the ChASE library ```bash cd ChASE/ mkdir build cd build/ cmake .. -DCMAKE_INSTALL_PREFIX=${ChASEROOT} make install ``` More details about the installation on both local machine and clusters, please refer to [User Documentation](https://chase-library.github.io/ChASE/quick-start.html) (⚠️**To be updated**). <!-- a normal html comment ## Documentation The documentation of ChASE is available [online](https://chase-library.github.io/ChASE/index.html). Compiling the documentation in local requires enable `-DBUILD_WITH_DOCS=ON` flag when compiling ChASE library: ```bash cmake .. -DBUILD_WITH_DOCS=ON ``` --> ## Examples Multiple examples are provided, which helps user get familiar with ChASE. **Build ChASE with Examples** requires enable `-DCHASE_BUILD_WITH_EXAMPLES=ON` flag when compiling ChASE library: ```bash cmake .. -DCHASE_BUILD_WITH_EXAMPLES=ON ``` **5 examples are available** in folder [examples](https://github.com/ChASE-library/ChASE/tree/master/examples): 0. The example [0_hello_world](https://github.com/ChASE-library/ChASE/tree/master/examples/0_hello_world) constructs a simple Clement matrix and find a given number of its eigenpairs. 1. The example [1_sequence_eigenproblems](https://github.com/ChASE-library/ChASE/tree/master/examples/1_sequence_eigenproblems) illustrates how ChASE can be used to solve a sequence of eigenproblems. (⚠️**To be included**). 2. The example [2_input_output](https://github.com/ChASE-library/ChASE/tree/master/examples/2_input_output) provides the configuration of parameters of ChASE from command line (supported by Boost); the parallel I/O which loads the local matrices into the computing nodes in parallel. 3. The example [3_installation](https://github.com/ChASE-library/ChASE/tree/master/examples/3_installation) shows the way to link ChASE to other applications. 4. The example [4_interface](https://github.com/ChASE-library/ChASE/tree/master/examples/4_interface) shows examples to use the C and Fortran interfaces of ChASE. ## Developers ### Main developers - Edoardo Di Napoli - Algorithm design and development - Xinzhe Wu - Algorithm development, advanced parallel (MPI and GPU) implementation and optimization, developer documentation ### Current contributors - Clément Richefort - Integration of ChASE into [YAMBO](https://www.yambo-code.eu/) code. - Davor Davidović - Advanced parallel GPU implementation and optimization - Nenad Mijić - ARM-based implementation and optimization, CholeskyQR, unitests, parallel IO ### Past contributors - Xiao Zhang - Integration of ChASE into Jena BSE code - Miriam Hinzen, Daniel Wortmann - Integration of ChASE into FLEUR code - Sebastian Achilles - Library benchmarking on parallel platforms, documentation - Jan Winkelmann - DoS algorithm development and advanced `C++` implementation - Paul Springer - Advanced GPU implementation - Marija Kranjcevic - OpenMP `C++` implementation - Josip Zubrinic - Early GPU algorithm development and implementation - Jens Rene Suckert - Lanczos algorithm and GPU implementation - Mario Berljafa - Early `C` and `MPI` implementation using the Elemental library ## Contribution This Github repository mirrors the principal Gitlab repository hosted at the Juelich Supercomputing Centre. There are two main ways you can contribute: 1. you can fork the open source ChASE repository on Github (https://github.com/ChASE-library/ChASE). Modify the source code (and relative inlined documentation, if necessary) and then submit a pull request. If you have not contributed to the ChASE library before, we will ask you to agree to a Collaboration Agreement (CLA) before the pull request can be approved. Currentlly there is no automatic mechanism to sign such an agreement and we need you to download the file CLA.pdf (that is part of the repository), print it, sign it, scan it and send it back to chase@fz-juelich.de. Upon reception of your signed CLA, your pull request will be reviewed and then eventually approved. 2. Alternatively, if you want to contribute as a developer stably integrated into this project please contact us at chase@fz-juelich.de with a motivated request of collaboration. We will consider your request and get in touch with you to evaluate if and how to give you access directly to the Gitlab repository where the major developments of this software is carried out. An automatic process to approve a pull request and sign a CLA is under development and will soon substitute option 1. In the meantime, we ask you for your patience and understanding in having to follow such a time consuming procedure. ## How to Cite the Code The main reference of ChASE is [1] while [2] provides some early results on scalability and usage on sequences of eigenproblems generated by Materials Science applications. [3] and [5] provides the distributed-memory multi-GPU implementation and performance analysis. - [1] J. Winkelmann, P. Springer, and E. Di Napoli. *ChASE: a Chebyshev Accelerated Subspace iteration Eigensolver for sequences of Hermitian eigenvalue problems.* ACM Transaction on Mathematical Software, **45** Num.2, Art.21, (2019). [DOI:10.1145/3313828](https://doi.org/10.1145/3313828) , [[arXiv:1805.10121](https://arxiv.org/abs/1805.10121/) ] - [2] M. Berljafa, D. Wortmann, and E. Di Napoli. *An Optimized and Scalable Eigensolver for Sequences of Eigenvalue Problems.* Concurrency & Computation: Practice and Experience **27** (2015), pp. 905-922. [DOI:10.1002/cpe.3394](https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.3394) , [[arXiv:1404.4161](https://arxiv.org/abs/1404.4161) ]. - [3] X. Wu, D. Davidović, S. Achilles,E. Di Napoli. *ChASE: a distributed hybrid CPU-GPU eigensolver for large-scale hermitian eigenvalue problems.* Proceedings of the Platform for Advanced Scientific Computing Conference (PASC22). [DOI:10.1145/3539781.3539792](https://dl.acm.org/doi/10.1145/3539781.3539792) , [[arXiv:2205.02491](https://arxiv.org/pdf/2205.02491/) ]. - [4] X. Wu, E. Di Napoli. *Advancing the distributed Multi-GPU ChASE library through algorithm optimization and NCCL library.* Proceedings of the SC'23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (pp. 1688-1696). [DOI:10.1145/3624062.3624249](https://dl.acm.org/doi/abs/10.1145/3624062.3624249), [[arXiv:2309.15595](https://arxiv.org/pdf/2309.15595)]. ## Copyright and License [3-Clause BSD License (BSD License 2.0)](https://github.com/ChASE-library/ChASE/blob/master/LICENSE) <!-- @Edo, add CLA here -->\n",
                "dependencies": "# -*- Mode: cmake -*- cmake_minimum_required( VERSION 3.8 ) project( ChASE LANGUAGES C CXX Fortran VERSION 1.6.1 ) set(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} \"${CMAKE_CURRENT_SOURCE_DIR}/cmake/modules\") # ## algorithm ## set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(CMAKE_CXX_EXTENSIONS OFF) add_library(chase_algorithm INTERFACE) include(GNUInstallDirs) target_include_directories( chase_algorithm INTERFACE \"$<BUILD_INTERFACE:${CMAKE_CURRENT_LIST_DIR}>\" $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}> # <prefix>/include/mylib ) target_compile_features(chase_algorithm INTERFACE cxx_auto_type) option( CHASE_OUTPUT \"ChASE will provide output at each iteration\") # Add an option to enable/disable OpenMP support option(CHASE_ENABLE_OPENMP \"Enable OpenMP support\" ON) option(CHASE_ENABLE_MIXED_PRECISION \"Enable mixed precision support\" OFF) option(CHASE_ENABLE_MPI_IO \"Enable MPI IO to read Hamiltonian matrix from local\" OFF) option(CHASE_USE_NVTX \"Enable NVTX for profiling\" OFF) option(CHASE_BUILD_WITH_EXAMPLES \"Build the examples\" OFF) option(CHASE_BUILD_WITH_DOCS \"Build the docs\" OFF) if( CHASE_OUTPUT ) target_compile_definitions( chase_algorithm INTERFACE \"-DCHASE_OUTPUT\" ) endif() # Find OpenMP package if the option is enabled # Find OpenMP package if the option is enabled if(ENABLE_OPENMP) find_package(OpenMP REQUIRED) if(OpenMP_CXX_FOUND) message(STATUS \"OpenMP found, enabling OpenMP support\") # Add OpenMP compiler flags globally add_compile_options(${OpenMP_CXX_FLAGS}) # Link OpenMP libraries globally set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}\") set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${OpenMP_EXE_LINKER_FLAGS}\") else() message(WARNING \"OpenMP not found, building without OpenMP support\") endif() endif() # Add the flag to the preprocessor if the option is enabled if (CHASE_ENABLE_MIXED_PRECISION) add_definitions(-DENABLE_MIXED_PRECISION) message(STATUS \"Mixed precision support enabled.\") else() message(STATUS \"Mixed precision support disabled.\") endif() if(CHASE_USE_NVTX) target_compile_definitions( chase_algorithm INTERFACE \"-DUSE_NVTX\" ) endif() add_subdirectory(external) add_subdirectory(grid) add_subdirectory(linalg) add_subdirectory(Impl) add_subdirectory(interface) if(CHASE_BUILD_WITH_EXAMPLES) add_subdirectory(examples) endif() add_executable( \"chase_driver\" tests/noinput.cpp ) target_link_libraries(chase_driver chase_cpu) if(TARGET chase_gpu) add_executable( \"chase_driver_gpu\" tests/noinput.cpp ) target_link_libraries(chase_driver_gpu chase_gpu) endif() add_executable( \"quasi_chase_driver\" tests/quasi_noinput.cpp ) target_link_libraries(quasi_chase_driver chase_cpu) if(TARGET chase_gpu) add_executable( \"quasi_chase_driver_gpu\" tests/quasi_noinput.cpp ) target_link_libraries(quasi_chase_driver_gpu chase_gpu) endif() if(TARGET pchase_cpu) add_executable( \"dist_quasi_chase_driver\" tests/dist_quasi_noinput.cpp ) target_link_libraries(dist_quasi_chase_driver pchase_cpu) endif() option(ENABLE_TESTS \"Enable unit tests.\" OFF) if(ENABLE_TESTS) MESSAGE(\"Test enabled. Finding GoogleTests.\") include(${CMAKE_SOURCE_DIR}/cmake/external/Gtest/FetchGtest.cmake) include(CTest) set(MPI_RUN mpirun CACHE STRING \"MPI runner (mpirun/srun)\") set(MPI_RUN_ARGS CACHE STRING \"\") set(MPI_TEST ON CACHE BOOL \"Run test with mpi\") add_subdirectory(tests) endif() # Documentation if(CHASE_BUILD_WITH_DOCS) message(STATUS \"Building Documentation of ChASE\") add_subdirectory(\"docs\") endif() #get_target_property(interface_defs mpi_grid_nccl INTERFACE_COMPILE_DEFINITIONS) #message(\"INTERFACE_COMPILE_DEFINITIONS: ${interface_defs}\") # List all dependencies (linked targets) #get_target_property(linked_libraries pchase_gpu INTERFACE_LINK_LIBRARIES) #message(\"linked_libraries: ${linked_libraries}\") # Prepare a list to store definitions #set(all_inherited_defs \"\") #foreach(dep ${linked_libraries}) # Check if the dependency is a valid target # if(TARGET ${dep}) # Get INTERFACE_COMPILE_DEFINITIONS from the dependency # get_target_property(dep_defs ${dep} INTERFACE_COMPILE_DEFINITIONS) # Append to the list # if(dep_defs) # list(APPEND all_inherited_defs ${dep_defs}) # endif() # endif() #endforeach() # Remove duplicates (optional) #list(REMOVE_DUPLICATES all_inherited_defs) # Print all inherited definitions #message(\"Inherited compile definitions: ${all_inherited_defs}\") #get_target_property(linked_libraries blaspp INTERFACE_LINK_LIBRARIES) #message(\"Libraries linked by blaspp: ${linked_libraries}\") #get_target_property(blas_library ${linked_libraries} IMPORTED_LOCATION) #message(\"BLAS library path: ${blas_library}\") ##installation install( TARGETS chase_algorithm EXPORT chase_targets LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} INCLUDES DESTINATION ${CMAKE_INSTALL_INCLUDEDIR} ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} ) install(DIRECTORY algorithm DESTINATION ${CMAKE_INSTALL_INCLUDEDIR} FILES_MATCHING PATTERN \"*.hpp\" PATTERN \"*.inc\" ) install(EXPORT chase_targets NAMESPACE ChASE:: EXPORT_LINK_INTERFACE_LIBRARIES DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME} FILE chaseTargets.cmake) include(CMakePackageConfigHelpers) set(IS_ENABLE_MPI OFF) set(IS_ENABLE_SCALAPACK OFF) set(IS_ENABLE_NCCL OFF) if(TARGET pchase_cpu) set(IS_ENABLE_MPI ON) endif() if(TARGET scalapackpp) set(IS_ENABLE_SCALAPACK ON) endif() if(TARGET linalg_internal_nccl) set(IS_ENABLE_NCCL ON) endif() # Convert booleans to \"ON\"/\"OFF\" strings for the config file set(IS_ENABLE_MPI_STRING ${IS_ENABLE_MPI}) set(IS_ENABLE_SCALAPACK_STRING ${IS_ENABLE_SCALAPACK}) set(IS_ENABLE_NCCL_STRING ${IS_ENABLE_NCCL}) configure_package_config_file( \"${CMAKE_CURRENT_SOURCE_DIR}/cmake/Config.cmake.in\" \"${CMAKE_CURRENT_BINARY_DIR}/chase-config.cmake\" INSTALL_DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/ChASE ) install( FILES \"${CMAKE_CURRENT_BINARY_DIR}/chase-config.cmake\" DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/ChASE ) install(DIRECTORY ${CMAKE_SOURCE_DIR}/cmake/modules DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/ChASE)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cheetah",
            "repo_link": "https://github.com/desy-ml/cheetah.git",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/chemotion-eln",
            "repo_link": "https://github.com/ComPlat/chemotion_ELN",
            "content": {
                "codemeta": "",
                "readme": "# Chemotion [![Badge DOI]][DOI] An **Electronic Lab Notebook** for chemists! --- **⸢ [Installation] ⸥ ⸢ [Documentation] ⸥ ⸢ [Changelog] ⸥** --- ## Tests ![Badge CI] --- ## Acknowledgments This project has been funded by the **[DFG]**. [![DFG Logo]][DFG] Funded by the [Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)](https://www.dfg.de/) under the [National Research Data Infrastructure - NFDI4Chem](https://nfdi4chem.de/) - Projektnummer **441958208** since 2020. --- ## License **Copyright © `2015` - `2023` [Nicole Jung]** <br> of the **[Karlsruhe Institute of Technology]**. > This program is free software: > > You can redistribute it and / or modify it under the terms <br> > of the GNU Affero General Public License as published by <br> > the Free Software Foundation, either version 3 of the <br> > License, or (at your option) any later version. > > This program is distributed in the hope that it will be useful, but <br> > WITHOUT ANY WARRANTY; without even the implied warranty <br> > of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. > > See the GNU Affero General Public License for more details. > > You should have received a copy of the GNU Affero<br> > General Public License along with this program. > > If not, see <https://www.gnu.org/licenses/>. <!-----------------------------------------------------------------------------> [Installation]: https://www.chemotion.net/docs/eln/install_configure [Documentation]: https://www.chemotion.net/docs/eln [Changelog]: CHANGELOG.md [DFG]: https://www.dfg.de/en/ [DFG Logo]: https://chemotion.net/img/logos/DFG_logo.png [Nicole Jung]: mailto:nicole.jung@kit.edu [Karlsruhe Institute of Technology]: https://www.kit.edu/english/ [DOI]: https://doi.org/10.5281/zenodo.1054134 [Badge CI]: https://github.com/ComPlat/chemotion_ELN/actions/workflows/ci.yml/badge.svg?branch=main [Badge DOI]: https://zenodo.org/badge/DOI/10.5281/zenodo.1054134.svg\n",
                "dependencies": "# frozen_string_literal: true source 'https://rubygems.org' gem 'aasm' gem 'activejob-status' gem 'activerecord-nulldb-adapter' gem 'ancestry' gem 'api-pagination' gem 'caxlsx' gem 'backup' gem 'barby' gem 'bcrypt_pbkdf' gem 'bibtex-ruby' gem 'bootsnap' gem 'bootstrap', '~> 5.3' gem 'charlock_holmes' gem 'closure_tree' gem 'countries' gem 'delayed_cron_job' gem 'delayed_job_active_record' gem 'devise' gem 'dotenv-rails', require: 'dotenv/rails-now' gem 'ed25519' gem 'faker', require: false gem 'faraday' gem 'faraday-follow_redirects' gem 'faraday-multipart' gem 'font-awesome-rails' gem 'fugit' gem 'fun_sftp', git: 'https://github.com/fl9/fun_sftp.git', branch: 'allow-port-option' gem 'fx' gem 'grape' gem 'grape-entity' gem 'grape-kaminari' gem 'grape-swagger' gem 'grape-swagger-entity' gem 'grape-swagger-rails' gem 'graphql', '< 2.2' gem 'haml-rails' gem 'hashie-forbidden_attributes' gem 'httparty' gem 'icalendar' gem 'image_processing', '~> 1.8' gem 'inchi-gem', '1.06.1', git: 'https://github.com/ComPlat/inchi-gem.git', branch: 'main' gem 'jquery-rails' # must be in, otherwise the views lack jquery, even though the gem is supplied by ketcher-rails gem 'jwt' gem 'kaminari' gem 'kaminari-grape' # gem 'ketcherails', git: 'https://github.com/complat/ketcher-rails.git', branch: 'upgrade-to-rails-6' gem 'ketcherails', git: 'https://github.com/complat/ketcher-rails.git', ref: 'd4ae864a0e2d9e853eac8e4fc4ce7e3ab8174f80' gem 'labimotion', '2.0.0' gem 'logidze' gem 'mimemagic', '0.3.10' gem 'mime-types' # locked to enforce latest version of net-scp. without lock net-ssh would be updated first which locks # out newer net-scp versions gem 'net-scp', '3.0.0' gem 'net-sftp' gem 'net-ssh' gem 'nokogiri' gem 'omniauth', '~> 1.9.1' gem 'omniauth-github', '~> 1.4.0' gem 'omniauth-oauth2', '~> 1.7', '>= 1.7.2' gem 'omniauth_openid_connect' gem 'omniauth-orcid', git: 'https://github.com/datacite/omniauth-orcid' gem 'omniauth-shibboleth' gem 'chemical_elements' gem 'openbabel', '2.4.90.3', git: 'https://github.com/ptrxyz/openbabel-gem.git', branch: 'ptrxyz-ctime-fix' gem 'pandoc-ruby' gem 'paranoia', '2.6.0' gem 'pg' gem 'pg_search' gem 'prawn' gem 'prawn-svg' gem 'puma', '< 6.0.0' gem 'pundit' gem 'rack' gem 'rack-cors', require: 'rack/cors' gem 'rails', '~> 6.1.7.7' gem 'rinchi-gem', '1.0.2', git: 'https://github.com/ComPlat/rinchi-gem.git', branch: 'main' gem 'rmagick' gem 'roo' gem 'rqrcode' # required for Barby to work but not listed as its dependency -_- gem 'rtf' gem 'ruby-geometry', require: 'geometry' gem 'ruby-mailchecker' gem 'ruby-ole' gem 'sablon', git: 'https://github.com/ComPlat/sablon' gem 'sassc-rails' gem 'scenic' gem 'schmooze' gem 'semacode', git: 'https://github.com/toretore/semacode.git', branch: 'master' # required for Barby but not listed... gem 'shakapacker', '8.0.2' gem 'sentry-delayed_job' gem 'sentry-rails' gem 'sentry-ruby' gem 'shrine', '~> 3.0' gem 'sys-filesystem' gem 'thor' gem 'thumbnailer', git: 'https://github.com/merlin-p/thumbnailer.git' gem 'turbo-sprockets-rails4' gem 'tzinfo-data' gem 'whenever', require: false gem 'yaml_db' group :development do gem 'better_errors' # allows to debug exception on backend from browser gem 'fast_stack' # For Ruby MRI 2.0 gem 'flamegraph' gem 'memory_profiler' # gem 'rack-mini-profiler', git: 'https://github.com/MiniProfiler/rack-mini-profiler' gem 'stackprof' # For Ruby MRI 2.1+ gem 'web-console' end group :vscode do gem 'debase' gem 'ruby-debug-ide' gem 'solargraph' end group :development, :test do gem 'annotate' gem 'awesome_print' gem 'binding_of_caller' gem 'bullet' gem 'byebug' gem 'chronic' gem 'listen' gem 'meta_request' gem 'pry-byebug' gem 'pry-rails' gem 'rubocop', require: false gem 'rubocop-performance', require: false gem 'rubocop-rails', require: false gem 'rubocop-rspec', require: false gem 'rspec' gem 'rspec-rails' gem 'ruby_jard' gem 'spring' end group :test do gem 'capybara' gem 'cypress-on-rails' gem 'database_cleaner' gem 'database_cleaner-active_record' gem 'factory_bot_rails' gem 'launchy' gem 'rspec-repeat' gem 'shoulda-matchers' gem 'simplecov', require: false gem 'simplecov-lcov', require: false gem 'webdrivers' gem 'webmock' end\n{\"name\":\"chemotion\",\"repository\":{},\"dependencies\":{\"@babel/core\":\"7\",\"@babel/plugin-transform-class-properties\":\"^7\",\"@babel/plugin-transform-modules-commonjs\":\"^7.13.8\",\"@babel/plugin-transform-object-rest-spread\":\"^7\",\"@babel/plugin-transform-runtime\":\"7\",\"@babel/polyfill\":\"^7.10.4\",\"@babel/preset-env\":\"7\",\"@babel/preset-react\":\"^7.26.3\",\"@babel/runtime\":\"7\",\"@citation-js/plugin-isbn\":\"0.3.0\",\"@complat/chem-spectra-client\":\"1.3.5\",\"@complat/chemotion-converter-client\":\"~0.13.2\",\"@complat/react-spectra-editor\":\"1.3.5\",\"@novnc/novnc\":\"~1.5.0\",\"@rails/ujs\":\"^6.1.3-1\",\"@sentry/react\":\"^7.73.0\",\"@sentry/tracing\":\"^7.16.0\",\"@sentry/webpack-plugin\":\"^2.22.4\",\"@types/babel__core\":\"7\",\"@types/webpack\":\"5\",\"acorn\":\"^5.7.0\",\"ag-grid-community\":\"~32.1\",\"ag-grid-react\":\"~32.1\",\"alt\":\"0.18.6\",\"alt-utils\":\"2.0.0\",\"antd\":\"^5.17.2\",\"aviator\":\"v0.6.1\",\"base-64\":\"^0.1.0\",\"chem-generic-ui\":\"2.0.0\",\"citation-js\":\"0.6.8\",\"classnames\":\"^2.2.5\",\"commonmark\":\"^0.28.1\",\"compression-webpack-plugin\":\"9\",\"create-react-class\":\"^15.6.3\",\"d3\":\"^3.5.15\",\"d3-selection\":\"^3.0.0\",\"deep-equal\":\"1.0.1\",\"dnd-multi-backend\":\"^9.0.0\",\"es6-promise-debounce\":\"^1.0.1\",\"html2pdf.js\":\"^0.10.1\",\"humps\":\"^2.0.1\",\"immutable\":\"^4.0.0-rc.12\",\"jcampconverter\":\"^2.11.0\",\"js-yaml\":\"^3.13.1\",\"jsdom\":\"^22.1.0\",\"lodash\":\"^4.17.20\",\"mime-types\":\"^2.1.35\",\"mobx\":\"^6.6.0\",\"mobx-react\":\"^7.5.0\",\"mobx-state-tree\":\"^5.1.5\",\"moment\":\"^2.29.4\",\"moment-precise-range-plugin\":\"^1.2.4\",\"npm\":\"^8.11.0\",\"numeral\":\"^1.5.3\",\"path\":\"^0.12.7\",\"pdf-lib\":\"^1.17.1\",\"prop-types\":\"15.6.2\",\"quagga\":\"^0.11.5\",\"querystring-es3\":\"^0.2.1\",\"quill\":\"^1.3.7\",\"quill-delta\":\"3.4.3\",\"quill-delta-to-html\":\"0.8.2\",\"quill-delta-to-plaintext\":\"^1.0.0\",\"raw-loader\":\"^4.0.2\",\"react\":\"^17.0.2\",\"react-barcode\":\"^1.1.0\",\"react-big-calendar\":\"^1.5.1\",\"react-bootstrap\":\"~2.10.2\",\"react-color\":\"^2.17.3\",\"react-contextmenu\":\"^2.14.0\",\"react-cookie\":\"^0.4.8\",\"react-datepicker\":\"~7.3.0\",\"react-datetime-picker\":\"^4.1.1\",\"react-dnd\":\"^14.0.3\",\"react-dnd-html5-backend\":\"^14.0.3\",\"react-dnd-touch-backend\":\"^16.0.1\",\"react-dom\":\"^17.0.2\",\"react-draggable\":\"^4.4.3\",\"react-dropzone\":\"^3.6.0\",\"react-html-id\":\"^0.1.5\",\"react-inlinesvg\":\"0.8.4\",\"react-input-autosize\":\"1.1.0\",\"react-json-editor-ajrm\":\"^2.5.10\",\"react-markdown\":\"^6.0.2\",\"react-molviewer\":\"^1.1.3\",\"react-notification-system\":\"^0.2.7\",\"react-papaparse\":\"^3.17.2\",\"react-pdf\":\"^7.7.3\",\"react-qr-reader\":\"^2.1.0\",\"react-select\":\"^5.8.1\",\"react-svg-file-zoom-pan\":\"0.1.5\",\"react-svg-file-zoom-pan-latest\":\"npm:@complat/react-svg-file-zoom-pan@1.1.3\",\"react-ui-tree\":\"3.1.0\",\"react-vis\":\"1.12.1\",\"reactflow\":\"^11.7.2\",\"redux\":\"^4.1.0\",\"sha256\":\"^0.2.0\",\"shakapacker\":\"8.0.2\",\"spark-md5\":\"^3.0.1\",\"svgedit\":\"^7.3.0\",\"terser-webpack-plugin\":\"5\",\"uglify-js\":\"~3.10.3\",\"uglifyify\":\"^5.0.2\",\"util\":\"^0.12.4\",\"uuid\":\"^3.3.2\",\"webpack\":\"5\",\"webpack-assets-manifest\":\"5\",\"webpack-cli\":\"4\",\"webpack-merge\":\"5\",\"whatwg-fetch\":\"^3.6.2\",\"yarn\":\"^1.22.19\"},\"devDependencies\":{\"@babel/eslint-parser\":\"^7.26.8\",\"@babel/register\":\"^7.25.9\",\"@eflexsystems/factory-bot\":\"^9.0.0\",\"@pmmmwh/react-refresh-webpack-plugin\":\"^0.5.5\",\"@wojtekmaj/enzyme-adapter-react-17\":\"^0.8.0\",\"babel-loader\":\"8\",\"cypress\":\"^12.3.0\",\"enzyme\":\"^3.7.0\",\"eslint\":\"^8.20.0\",\"eslint-config-airbnb\":\"^19.0.4\",\"eslint-filtered-fix\":\"^0.3.0\",\"eslint-plugin-import\":\"^2.26.0\",\"eslint-plugin-jsx-a11y\":\"^6.6.1\",\"eslint-plugin-no-relative-import-paths\":\"^1.4.0\",\"eslint-plugin-react\":\"^7.30.1\",\"expect\":\"^24.7.1\",\"jsdom-global\":\"^3.0.2\",\"mocha\":\"^11\",\"nyc\":\"^15.1.0\",\"process\":\"^0.11.10\",\"react-refresh\":\"^0.12.0\",\"sinon\":\"^15.2.0\",\"webpack-dev-server\":\"4\"},\"scripts\":{\"postinstall\":\"./package_postinstall.sh\",\"test\":\"NODE_PATH=./spec/javascripts:./app/javascript yarn mocha --exit --require './spec/javascripts/setup.js' './spec/javascripts/**/*.spec.js'\",\"coverage\":\"yarn nyc npm test;yarn nyc report --reporter=html\",\"update-browserslist-db\":\"node_modules/.bin/update-browserslist-db\"},\"nyc\":{\"exclude\":[\"**/*.spec.js\",\"spec/javascripts/fixture/*\",\"spec/javascripts/helper/*\"]},\"license\":\"MIT\",\"engines\":{\"node\":\"^22.14\"},\"browserslist\":[\"defaults\"],\"babel\":{\"presets\":[\"./node_modules/shakapacker/package/babel/preset.js\",\"@babel/preset-react\"],\"plugins\":[\"@babel/plugin-transform-arrow-functions\",\"@babel/plugin-transform-async-to-generator\",\"@babel/plugin-transform-class-properties\",\"@babel/plugin-transform-classes\",\"@babel/plugin-transform-private-methods\"]},\"packageManager\":\"yarn@1.22.22\",\"private\":true,\"version\":\"0.1.0\"}\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/climate-index-collection",
            "repo_link": "https://github.com/MarcoLandtHayen/climate_index_collection",
            "content": {
                "codemeta": "",
                "readme": "# Climate Index Collection [![Build Status](https://github.com/MarcoLandtHayen/climate_index_collection/workflows/Tests/badge.svg)](https://github.com/MarcoLandtHayen/climate_index_collection/actions) [![codecov](https://codecov.io/gh/MarcoLandtHayen/climate_index_collection/branch/main/graph/badge.svg)](https://codecov.io/gh/MarcoLandtHayen/climate_index_collection) [![License:MIT](https://img.shields.io/badge/License-MIT-lightgray.svg?style=flt-square)](https://opensource.org/licenses/MIT) [![Docker Image Version (latest by date)](https://img.shields.io/docker/v/mlandthayen/climate_index_collection?label=DockerHub)](https://hub.docker.com/r/mlandthayen/climate_index_collection/tags) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7779883.svg)](https://doi.org/10.5281/zenodo.7779883) Collection of climate indices derived from climate model outputs. ## Quickstart: Using the climate indices The resulting climate-index time series are published to Zenodo under the DOI [10.5281/zenodo.7779883](https://doi.org/10.5281/zenodo.7779883) and you can obtain the index timeseries by manually downloading the file `climate_indices.csv` from this dataset. You can also use [pooch](https://www.fatiando.org/pooch/latest/) to obtain the published index time series programmatically: ```python import pooch climate_indices_file = pooch.retrieve( url=\"doi:10.5281/zenodo.7779883/climate_indices.csv\", known_hash=None, ) ``` With `climate_indices_file` containing the path to the CSV file either resulting from the code above or from manually setting it to the location of the manually downloaded data, we recommend using [Pandas](https://pandas.pydata.org/docs/) for reading the data: ```python import pandas as pd climate_indices = pd.read_csv(climate_indices_file) ``` This results in a dataframe with the following structure: ```python print(climate_indices) ``` ``` model year month index long_name value 0 FOCI 1 1 SAM_ZM southern_annular_mode_zonal_mean -0.295492 1 FOCI 1 2 SAM_ZM southern_annular_mode_zonal_mean 0.530890 2 FOCI 1 3 SAM_ZM southern_annular_mode_zonal_mean 1.684005 3 FOCI 1 4 SAM_ZM southern_annular_mode_zonal_mean 1.409169 4 FOCI 1 5 SAM_ZM southern_annular_mode_zonal_mean 0.984511 ... ... ... ... ... ... ... 695647 CESM 999 8 NP north_pacific -0.210202 695648 CESM 999 9 NP north_pacific 0.206541 695649 CESM 999 10 NP north_pacific -0.331067 695650 CESM 999 11 NP north_pacific 0.487844 695651 CESM 999 12 NP north_pacific -0.782657 [695652 rows x 6 columns] ``` To apply statistics or to plot all indices, you can apply standard modifications provided by Pandas. Calculating, e.g., the standard deviation of all indices amounts to ```python print(climate_indices.groupby([\"model\", \"index\"])[[\"value\"]].std()) ``` ``` model index CESM AMO 0.109084 ENSO_12 0.603481 ENSO_3 0.881938 ENSO_34 0.933544 ENSO_4 0.909434 NAO_PC 1.000042 NAO_ST 1.554596 NP 0.569459 PDO_PC 1.000042 ... ... ... FOCI AMO 0.128715 ENSO_12 0.342368 ENSO_3 0.600242 ENSO_34 0.759405 ENSO_4 0.923854 NAO_PC 1.000042 NAO_ST 1.459676 NP 0.644014 PDO_PC 1.000042 ... ... ... Name: value, dtype: float64 ``` ## Quickstart: Reproducing the dataset The Python package in this repository can be installed using [`pip`](https://pip.pypa.io/en/stable/getting-started/#install-a-package-from-github): ```shell $ python -m pip install git+https://github.com/MarcoLandtHayen/climate_index_collection.git@v2023.03.29.1 ``` The data from which the indices have been calculated are published under the DOI [10.5281/zenodo.7060385](https://doi.org/10.5281/zenodo.7060385). After downloading the data to, e.g., `./cicmod_data/`, you can run the command line version of this package by ```shell $ climate_index_collection_run --input-path ./cicmod_data/ --output-path . ``` which will create a file `climate_indices.csv`. Please see either `$ climate_index_collection_run --help` on the command line, or the tutorial notebook in [notebooks/Tutorial.ipynb](notebooks/Tutorial.ipynb) for more details. ## Development For now, we're developing in the Pangeo notebook container. More details: https://github.com/pangeo-data/pangeo-docker-images To start a JupyterLab within this container, run ```shell $ docker pull pangeo/pangeo-notebook:2022.07.27 $ docker run -p 8888:8888 --rm -it -v $PWD:/work -w /work pangeo/pangeo-notebook:2022.07.27 jupyter lab --ip=0.0.0.0 ``` and open the URL starting on `http://127.0.0.1...`. Then, open a Terminal within JupyterLab and run ```shell $ python -m pip install -e . ``` to have a local editable installation of the package. ## Container Image There's a container image: https://hub.docker.com/r/mlandthayen/climate_index_collection ### Use with Docker You can use it wherever Docker is installed by running: ```shell $ docker pull mlandthayen/climate_index_collection:<tag> $ docker run --rm -v $PWD:/work -w /work mlandthayen/climate_index_collection:<tag> climate_index_collection_run --help ``` Here, `<tag>` can either be `latest` or a more specific tag. ### Use with Singularity You can use it wherever Singularity is installed by essentially running: ```shell $ singularity pull --disable-cache --dir \"${PWD}\" docker://mlandthayen/climate_index_collection:<tag> $ singularity run climate_index_collection_<tag>.sif climate_index_collection_run --help ``` Here, `<tag>` can either be `latest` or a more specific tag. _Note_ that for NESH, it's currently necessary to - specify the version of singularity to use, and - to make sure to bind mount various parts of the file system explicitly. So the full call on NESH would look like: ```shell $ module load singularity/3.5.2 $ singularity pull --disable-cache --dir \"${PWD}\" docker://mlandthayen/climate_index_collection:<tag> $ singularity run -B /sfs -B /gxfs_work1 -B ${PWD}:/work --pwd /work climate_index_collection_<tag>.sif climate_index_collection_run --help ``` ## Release Procedure A release will contain the specific version of the package (taken care of automatically) and the CSV file created with the full data. 1. _**Draft a release:**_ Go to https://github.com/MarcoLandtHayen/climate_index_collection/releases/new and draft a new release (don't publish yet). 2. _**Prepeare data:**_ For the commit in `main` for which the release is planned, pull the container on NESH (see above) and run: ``` $ singularity run -B /sfs -B /gxfs_work1 -B ${PWD}:/work --pwd /work climate_index_collection_<tag>.sif climate_index_collection_run --input-path <path_to_full_data> ``` 3. _**Attach data:**_ Attach the CSV file to the drafted release. 4. _**Publish:**_ By clicking on the `Publish release` button. -------- <p><small>Project based on the <a target=\"_blank\" href=\"https://github.com/jbusecke/cookiecutter-science-project\">cookiecutter science project template</a>.</small></p>\n",
                "dependencies": "[build-system] requires = [\"setuptools>=42\", \"wheel\", \"setuptools_scm[toml]>=3.4\"] build-backend = \"setuptools.build_meta\" [tool.setuptools_scm] [tool.interrogate] ignore-init-method = true ignore-init-module = false ignore-magic = false ignore-semiprivate = true ignore-private = true ignore-property-decorators = true ignore-module = false fail-under = 95 exclude = [\"setup.py\", \"docs\", \"tests\"] verbose = 1 quiet = false color = true [tool.isort] known_third_party = [\"cftime\", \"click\", \"numpy\", \"pandas\", \"pkg_resources\", \"pytest\", \"scipy\", \"setuptools\", \"shapely\", \"xarray\", \"xhistogram\"] [tool.pytest.ini_options] minversion = \"6.0\" addopts = \"-v\" # only test the root level, otherwise it picks up the tests of the project template testpaths = [ \"tests\", ]\nfrom setuptools import setup setup( use_scm_version={ \"write_to\": \"climate_index_collection/_version.py\", \"write_to_template\": '__version__ = \"{version}\"', \"tag_regex\": r\"^(?P<prefix>v)?(?P<version>[^\\+]+)(?P<suffix>.*)?$\", \"local_scheme\": \"node-and-date\", }, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cimpredict",
            "repo_link": "https://github.com/tonitacker/CIMPredict",
            "content": {
                "codemeta": "",
                "readme": "# CIMPredict ## Description: CIMPredict is used to generate Fracture Risk Values in % for A timeframe of two years upon consultation. The CIM-blood value and the patients age are used to calculate a Fracture Risk based on a model trained by study data that include CIM-Values and information about fractures that occured within two years. ## Features: CIMPredict offers a numerical risk in percent of a patient suffering an osteoporotic fracture within the next two years ## Installation: 1. Download the ZIP-File found in the releases tab and extract it to your preferred folder 2. Start CIMPredict.exe Do not move the EXE-File out of the directory in comes in. You can create a shortcut to the exe if you wish. The functionality of the software relies on the data in the directory. The software checks for new versions of the dependencies, thus it may take a few minutes upon first start for the main window to appear. Do not interrupt this process ## Usage: Enter the CIM-Value using a decimal point, not a comma into the dedicated field. Similarly, provide a valid age (between 51 and 90 years). The \"Berechnen\"-button is made accessible once all values are provided. Click it to retrieve the fracture risk value. ## License This project is licensed under the GNU General Public License v3.0 - see the [LICENSE](./.LICENSE) file for details. ### Contact For further information or inquiries, please contact: - **Name:** Anton Krackhardt - **E-Mail:** antonkrackhardt444@gmail.com\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/circlize",
            "repo_link": "https://github.com/jokergoo/circlize",
            "content": {
                "codemeta": "",
                "readme": "# circlize: circular visualization in R <a href=\"https://jokergoo.github.io/circlize_book/book/\"><img src=\"https://jokergoo.github.io/circlize_book/book/images/circlize_cover.jpg\" width=240 align=\"right\" ></a> [![R-CMD-check](https://github.com/jokergoo/circlize/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/circlize/actions) [![CRAN](https://www.r-pkg.org/badges/version/circlize)](https://cran.r-project.org/web/packages/circlize/index.html) [![CRAN](https://cranlogs.r-pkg.org/badges/grand-total/circlize)](https://cran.r-project.org/web/packages/circlize/index.html) [![Codecov test coverage](https://codecov.io/gh/jokergoo/circlize/branch/master/graph/badge.svg)](https://codecov.io/gh/jokergoo/circlize?branch=master) Circular layout is an efficient way for the visualization of huge amounts of information. Here the circlize package provides an implementation of circular layout generation in R as well as an enhancement of available software. The flexibility of this package is based on the usage of low-level graphics functions such that self-defined high-level graphics can be easily implemented by users for specific purposes. Together with the seamless connection between the powerful computational and visual environment in R, circlize gives users more convenience and freedom to design figures for better understanding complex patterns behind multi-dimensional data. ## Citation Zuguang Gu, et al., circlize Implements and enhances circular visualization in R. Bioinformatics (Oxford, England) 2014. [PubMed](https://www.ncbi.nlm.nih.gov/pubmed/24930139) ## Documentation The full documentations are available at https://jokergoo.github.io/circlize_book/book/ and the online website is at https://jokergoo.github.io/circlize/. ## Blog posts There are the following blog posts focusing on specific topics. - [Make circular heatmaps](https://jokergoo.github.io/2020/05/21/make-circular-heatmaps/) - [Multiple-group Chord diagram](https://jokergoo.github.io/2020/06/08/multiple-group-chord-diagram/) - [Changes in circlize 0.4.10](https://jokergoo.github.io/2020/06/14/changes-in-circlize-0.4.10/) - [Reverse x-axes in the circular plot](https://jokergoo.github.io/2020/08/17/reverse-x-axes-in-the-circular-plot/) ## Examples See https://jokergoo.github.io/circlize_examples/. <img width=\"700\" alt=\"circlize_example\" src=\"https://jokergoo.github.io/circlize_book/book/images/ciclize_examples.jpg\"> ## Install The package can be installed from CRAN: ```r install.packages(\"circlize\") ``` or directly from GitHub: ```r devtools::install_github(\"jokergoo/circlize\") ``` ## Basic design Since most of the figures are composed of points, lines and polygons, we just need to implement functions for drawing points, lines and polygons, then the plots will not be restricted in any specific types. Current there are following low-level graphic functions: - `circos.points()` - `circos.lines()` - `circos.segments()` - `circos.rect()` - `circos.polygon()` - `circos.text()` - `circos.axis()` - `circos.raster()` - `circos.arrow()` - `circos.raster()` - `circos.barplot()` - `circos.boxplot()` - `circos.link()`, This maybe the unique feature for circos layout to represent relationships between elements. For drawing points, lines and text through the whole track (among several sectors), the following functions are available: - `circos.trackPoints()` - `circos.trackLines()` - `circos.trackText()` Draw circular heatmaps - `circos.heatmap()` Functions to arrange the circular layout: - `circos.track()` - `circos.update()` - `circos.nested()` - `circos.par()` - `circos.info()` - `circos.clear()` Theoretically, you are able to draw most kinds of circular plots by the above functions. For specific use in Genomics, we also implement functions which add graphics in genome scale. Functions to initialize circular plot with genomic coordinates: - `circos.initializeWithIdeogram()` - `circos.genomicInitialize()` Functions to arrange genomic circular layout: - `circos.genomicTrack()` Functions to add basic graphics in genomic scale: - `circos.genomicPoints()` - `circos.genomicLines()` - `circos.genomicText()` - `circos.genomicRect()` - `circos.genomicLink()` Functions with specific purpose: - `circos.genomicIdeogram()` - `circos.genomicHeatmap()` - `circos.genomicLabels()` - `circos.genomicDensity()` - `circos.genomicRainfall()` Finally, function that draws Chord diagram: - `chordDiagram()` ## License MIT @ Zuguang Gu\n",
                "dependencies": "Package: circlize Type: Package Title: Circular Visualization Version: 0.4.16 Date: 2022-11-24 Authors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"), comment = c('ORCID'=\"0000-0002-7395-8709\")) Depends: R (>= 3.0.0), graphics Imports: GlobalOptions (>= 0.1.2), shape, grDevices, utils, stats, colorspace, methods, grid Suggests: knitr, dendextend (>= 1.0.1), ComplexHeatmap (>= 2.0.0), gridBase, png, markdown, bezier, covr, rmarkdown VignetteBuilder: knitr Description: Circular layout is an efficient way for the visualization of huge amounts of information. Here this package provides an implementation of circular layout generation in R as well as an enhancement of available software. The flexibility of the package is based on the usage of low-level graphics functions such that self-defined high-level graphics can be easily implemented by users for specific purposes. Together with the seamless connection between the powerful computational and visual environment in R, it gives users more convenience and freedom to design figures for better understanding complex patterns behind multiple dimensional data. The package is described in Gu et al. 2014 <doi:10.1093/bioinformatics/btu393>. URL: https://github.com/jokergoo/circlize, https://jokergoo.github.io/circlize_book/book/ License: MIT + file LICENSE\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/citation-file-format",
            "repo_link": "https://github.com/citation-file-format/citation-file-format",
            "content": {
                "codemeta": "",
                "readme": "# Citation File Format [![Build Status](https://github.com/citation-file-format/citation-file-format/workflows/testing/badge.svg?branch=main)](https://github.com/citation-file-format/citation-file-format/actions/workflows/testing.yml?query=branch%3Amain) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1003149.svg)](https://doi.org/10.5281/zenodo.1003149) [![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/) [![Project homepage](https://img.shields.io/badge/Project%20homepage-citation--file--format.github.io-ff0080)](https://citation-file-format.github.io) The Citation File Format lets you provide citation metadata for software or datasets in plaintext files that are easy to read by both humans and machines. ## Structure You can specify citation metadata for your software (or dataset) in a file named `CITATION.cff`. This is what a typical `CITATION.cff` file may look like for research software: ```yaml cff-version: 1.2.0 message: If you use this software, please cite it using these metadata. title: My Research Software abstract: This is my awesome research software. It does many things. authors: - family-names: Druskat given-names: Stephan orcid: \"https://orcid.org/1234-5678-9101-1121\" - name: \"The Research Software project\" version: 0.11.2 date-released: \"2021-07-18\" identifiers: - description: This is the collection of archived snapshots of all versions of My Research Software type: doi value: \"10.5281/zenodo.123456\" - description: This is the archived snapshot of version 0.11.2 of My Research Software type: doi value: \"10.5281/zenodo.123457\" license: Apache-2.0 repository-code: \"https://github.com/citation-file-format/my-research-software\" ``` In addition, the Citation File Format allows you to - provide references to works that your software or dataset builds on ([see here for more info](schema-guide.md#referencing-other-work)); - ask people to cite a different, related work instead of the software or dataset itself ([see here for more info](schema-guide.md#credit-redirection)). ## Format specifications :books: **You can find the complete format specifications in the [Guide to the Citation File Format schema](schema-guide.md).** ## Why should I add a `CITATION.cff` file to my repository? :bulb: When you do this, great things may happen: 1. Users of your software can easily cite it using the metadata from `CITATION.cff`! 2. If your repository is hosted on GitHub, they will [show the citation information in the sidebar](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-citation-files), which makes it easy for visitors to cite your software or dataset correctly. 3. When you publish your software on Zenodo via the [GitHub-Zenodo integration](https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content), they will use the metadata from your `CITATION.cff` file. 4. People can import the correct reference to your software into the [Zotero](https://www.zotero.org) reference manager via a [browser plugin](https://www.zotero.org/download/). ## Creation :heavy_plus_sign: To create a `CITATION.cff` file, you can - use the [**cffinit** website](https://citation-file-format.github.io/cff-initializer-javascript/#/), - copy and paste the [example snippet](#structure), and adapt it to your needs, or - create a new file called `CITATION.cff` using the *Add file* button on GitHub, and use the template they provide. ## Validation :heavy_check_mark: You can validate your `CITATION.cff` file on the command line with the [`cffconvert` Python package](https://pypi.org/project/cffconvert/): ```shell # Install cffconvert with pip in user space python3 -m pip install --user cffconvert # Validate your CFF file cffconvert --validate ``` If you get a Traceback with error messages, look for the relevant validation error and fix it. If the output is very long, it may help if you search it for lines starting with `jsonschema.exceptions.ValidationError`. If you prefer to use Docker, you can use the [`cffconvert` Docker image](https://hub.docker.com/r/citationcff/cffconvert): ```bash cd <directory-containing-your-CITATION.cff> docker run --rm -v ${PWD}:/app citationcff/cffconvert --validate ``` <!-- Later, this should link to tutorials --> ## Tools to work with `CITATION.cff` files :wrench: There is tooling available to work with `CITATION.cff` files to do different things: create new files, edit existing files, validate existing files, convert files from the Citation File Format into another format. The following table gives an overview of the tools that we know about. If there is a tool missing from this table, please [open a new issue](https://github.com/citation-file-format/citation-file-format/issues/new/choose) and let us know. | | Creation | Editing/Updating | Validation | Conversion | | -------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | Command line | | | • [cffconvert](#validation-heavy_check_mark) | • [cffconvert](https://pypi.org/project/cffconvert/)<br> • [bibtex-to-cff](https://github.com/monperrus/bibtexbrowser/)<br>• [cff-from-621](https://pypi.org/project/cff-from-621/)<br>• [openCARP-CI](https://git.opencarp.org/openCARP/openCARP-CI/-/tree/master/#create_cff) | | GitHub Actions | | | [cff-validator](https://github.com/marketplace/actions/cff-validator) | • [cffconvert](https://github.com/marketplace/actions/cffconvert)<br>• [codemeta2cff](https://github.com/caltechlibrary/codemeta2cff) | | GitHub Bot | | | [#238](https://github.com/citation-file-format/citation-file-format/issues/238) | | | Docker | | | [cffconvert Docker image](#validation-heavy_check_mark) | [cffconvert Docker image](https://hub.docker.com/r/citationcff/cffconvert) | | Go | | | | • [datatools/codemeta2cff](https://github.com/caltechlibrary/datatools/) | | Haskell | | • [cffreference](https://github.com/kevinmatthes/cffreference) | | | | Java | • [CFF Maven plugin](https://github.com/hexatomic/cff-maven-plugin) | • [CFF Maven plugin](https://github.com/hexatomic/cff-maven-plugin) | | • [CFF Maven plugin](https://github.com/hexatomic/cff-maven-plugin) | | JavaScript | | | | • [Citation.js](https://citation.js.org/) [plugin](https://www.npmjs.com/package/@citation-js/plugin-software-formats) | | Julia | | | • [Bibliography.jl](https://github.com/Humans-of-Julia/Bibliography.jl) | • [Bibliography.jl](https://github.com/Humans-of-Julia/Bibliography.jl) | | PHP | | | | • [bibtex-to-cff](https://github.com/monperrus/bibtexbrowser/) | | Python | | • [cff2toml](https://github.com/willynilly/cff2toml)<br> • [doi2cff](https://github.com/citation-file-format/doi2cff)<br> • [updateCitation](https://github.com/hunterhogan/updateCitation) | • [cffconvert](#validation-heavy_check_mark) | • [cff-from-621](https://pypi.org/project/cff-from-621/)<br>• [cff2toml](https://github.com/willynilly/cff2toml)<br>• [cffconvert](https://github.com/citation-file-format/cff-converter-python)<br>• [doi2cff](https://github.com/citation-file-format/doi2cff)<br>• [openCARP-CI](https://git.opencarp.org/openCARP/openCARP-CI/-/tree/master/#create_cff)<br>• [py_bibtex_to_cff_converter](https://github.com/vdplasthijs/py_bibtex_to_cff_converter) | | R | | | • [cffr](https://CRAN.R-project.org/package=cffr) | • [citation](https://cran.r-project.org/web/packages/citation/)<br>• [r2cff](https://github.com/ocbe-uio/RCFF)<br>• [handlr](https://github.com/ropensci/handlr)<br>• [cffr](https://CRAN.R-project.org/package=cffr) | | Ruby | • [ruby-cff](https://github.com/citation-file-format/ruby-cff) | • [ruby-cff](https://github.com/citation-file-format/ruby-cff) | • [ruby-cff](https://github.com/citation-file-format/ruby-cff) | • [ruby-cff](https://github.com/citation-file-format/ruby-cff) | | Rust | • [Aeruginous](https://github.com/kevinmatthes/aeruginous-rs) | • [Aeruginous](https://github.com/kevinmatthes/aeruginous-rs) | | • [citeworks](https://github.com/passcod/citeworks) | | TypeScript | | | | [#28](https://github.com/citation-file-format/citation-file-format/issues/28#issuecomment-892105342) | | Website | • [cffinit](https://citation-file-format.github.io/cff-initializer-javascript/) | | | | ## Maintainers :nerd_face: The Citation File Format schema is maintained by - Stephan Druskat ([@sdruskat](https://github.com/sdruskat/)) - Jurriaan H. Spaaks ([@jspaaks](https://github.com/jspaaks/)) ## Contributing :handshake: The Citation File Format is a collaborative project and we welcome suggestions and contributions. We hope one of the invitations below works for you, but if not, please let us know! :running: **I'm busy, I only have 1 minute** - Tell a friend about the Citation File Format, or tweet about it! - Give the project a star :star:! :hourglass_flowing_sand: **I've got 10 minutes - tell me what I should do** - Create a `CITATION.cff` file for your repository. - Suggest ideas for how you would like to use the Citation File Format, or for an improvement to the format or its tooling. - If you know how to validate `CITATION.cff` files, help someone with a validation problem and look at the [issues labeled ![GitHub labels](https://img.shields.io/github/labels/citation-file-format/citation-file-format/validation)](https://github.com/citation-file-format/citation-file-format/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+label%3Avalidation) :computer: **I've got a few hours to work on this** - Help create tooling for the community by looking at the [issues labeled ![GitHub labels](https://img.shields.io/github/labels/citation-file-format/citation-file-format/tooling)](https://github.com/citation-file-format/citation-file-format/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+label%3Atooling) :tada: **I want to help grow the community** - Write a blog post or news item for your own community. - Organise a hack event or workshop to help others use or improve the Citation File Format. Please read the more detailed [contributing guidelines](CONTRIBUTING.md) and [open a GitHub issue](https://github.com/citation-file-format/citation-file-format/issues) to suggest a new idea or let us know about bugs. Please put up pull requests for changes to the format and schema against the `develop` branch! ## License :balance_scale: Copyright © 2016 - 2023. The Citation File Format Contributors This work is licensed under a [Creative Commons Attribution 4.0 International (CC-BY-4.0)](https://creativecommons.org/licenses/by/4.0/legalcode) license. ## Acknowledgments :pray: **We'd like to thank everyone who has contributed to the Citation File Format!** They are listed in the [`CITATION.cff`](CITATION.cff) file for this repository. Please open an issue if you find that you are missing from the file. We gratefully acknowledge support from: - The [Institute for Software Technology](https://www.dlr.de/en/sc) of the [German Aerospace Center (DLR)](https://www.dlr.de/en/) - The [Netherlands eScience Center](https://www.esciencecenter.nl) - The [Software Sustainability Institute](https://software.ac.uk/)\n",
                "dependencies": "cffconvert pytest jsonschema ruamel.yaml bump2version\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/citychem",
            "repo_link": "https://github.com/matthkarl/citychem",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/climsight",
            "repo_link": "https://github.com/CliDyn/climsight",
            "content": {
                "codemeta": "",
                "readme": "# ClimSight ClimSight is an advanced tool that integrates Large Language Models (LLMs) with climate data to provide localized climate insights for decision-making. ClimSight transforms complex climate data into actionable insights for agriculture, urban planning, disaster management, and policy development. The target audience includes researchers, providers of climate services, policymakers, agricultural planners, urban developers, and other stakeholders who require detailed climate information to support decision-making. ClimSight is designed to democratize access to climate data, empowering users with insights relevant to their specific contexts. ![screencast](https://github.com/koldunovn/climsight/assets/3407313/bf7cd327-c8a9-4a09-bfb5-778269fcd15c) ClimSight distinguishes itself through several key advancements: - **Integration of LLMs**: ClimSight leverages state-of-the-art LLMs to interpret complex climate-related queries, synthesizing information from diverse data sources. - **Multi-Source Data Integration**: Unlike conventional systems that rely solely on structured climate data, ClimSight integrates information from multiple sources. - **Evidence-Based Approach**: ClimSight ensures contextually accurate answers by retrieving relevant knowledge from scientific reports, IPCC documents, and geographical databases. - **Modular Architecture**: Specialized components handle distinct tasks, such as data retrieval, contextual understanding, and result synthesis, leading to more accurate outputs. - **Real-World Applications**: ClimSight is validated through practical examples, such as assessing climate risks for specific agricultural activities and urban planning scenarios. ## Installation Options You can use ClimSight in three ways: 1. Run a pre-built Docker container (simplest approach) 2. Build and run a Docker container from source 3. Install the Python package (via pip or conda/mamba) Using ClimSight requires an OpenAI API key unless using the `skipLLMCall` mode for testing. The API key is only needed when running the application, not during installation. ## 1. Running with Docker (Pre-built Container) The simplest way to get started is with our pre-built Docker container: ```bash # Make sure your OpenAI API key is set as an environment variable export OPENAI_API_KEY=\"your-api-key-here\" # Pull and run the container docker pull koldunovn/climsight:stable docker run -p 8501:8501 -e OPENAI_API_KEY=$OPENAI_API_KEY koldunovn/climsight:stable ``` Then open `http://localhost:8501/` in your browser. ## 2. Building and Running from Source with Docker If you prefer to build from the latest source: ```bash # Clone the repository git clone https://github.com/CliDyn/climsight.git cd climsight # Download required data python download_data.py # Build and run the container docker build -t climsight . docker run -p 8501:8501 -e OPENAI_API_KEY=$OPENAI_API_KEY climsight ``` Visit `http://localhost:8501/` in your browser once the container is running. For testing without OpenAI API calls: ```bash docker run -p 8501:8501 -e STREAMLIT_ARGS=\"skipLLMCall\" climsight ``` ## 3. Python Package Installation ### Option A: Building from source with conda/mamba ```bash # Clone the repository git clone https://github.com/CliDyn/climsight.git cd climsight # Create and activate the environment mamba env create -f environment.yml conda activate climsight # Download required data python download_data.py ``` ### Option B: Using pip It's recommended to create a virtual environment to avoid dependency conflicts: ```bash # Option 1: Install from source git clone https://github.com/CliDyn/climsight.git cd climsight # Create and activate a virtual environment python -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate # Install ClimSight pip install -e . python download_data.py ``` Or if you prefer to set up without cloning the repository: ```bash # Option 2: Install from PyPI # Create and activate a virtual environment python -m venv climsight_env source climsight_env/bin/activate # On Windows: climsight_env\\Scripts\\activate # Install the package pip install climsight # Create a directory for data mkdir -p climsight cd climsight # Download necessary configuration files wget https://raw.githubusercontent.com/CliDyn/climsight/main/data_sources.yml wget https://raw.githubusercontent.com/CliDyn/climsight/main/download_data.py wget https://raw.githubusercontent.com/CliDyn/climsight/main/config.yml # Download the required data (about 8 GB) python download_data.py ``` ## Configuration ClimSight will automatically use a `config.yml` file from the current directory. You can modify this file to customize settings: ```yaml # Key settings you can modify in config.yml: # - LLM model (gpt-4, ...) # - Climate data sources # - RAG database configuration # - Agent parameters ``` ## Running ClimSight ### If installed with conda/mamba from source: ```bash # Run from the repository root streamlit run src/climsight/climsight.py ``` ### If installed with pip: ```bash # Make sure you're in the directory with your data and config climsight ``` You can optionally set your OpenAI API key as an environment variable: ```bash export OPENAI_API_KEY=\"your-api-key-here\" ``` Otherwise, you can enter your API key directly in the browser interface when prompted. ### Testing without an OpenAI API key: ```bash # From source: streamlit run src/climsight/climsight.py skipLLMCall # Or if installed with pip: climsight skipLLMCall ``` The application will open in your browser automatically. Just type your climate-related questions and press \"Generate\" to get insights. <img width=\"800\" alt=\"ClimSight Interface\" src=\"https://github.com/koldunovn/climsight/assets/3407313/569a4c38-a601-4014-b10d-bd34c59b91bb\"> ## Citation If you use or refer to ClimSight in your work, please cite: Koldunov, N., Jung, T. Local climate services for all, courtesy of large language models. _Commun Earth Environ_ **5**, 13 (2024). https://doi.org/10.1038/s43247-023-01199-1\n",
                "dependencies": "[project] name = \"climsight\" version = \"1.0.0\" description = \"A tool that combines LLMs with climate data to provide localized insights for decision-making in agriculture, urban planning, disaster management, and policy development.\" readme = \"README.md\" authors = [ {name = \"kuivi\"}, {name = \"AntoniaJost\"}, {name = \"koldunovn\"}, {name = \"dmpantiu\"}, {name = \"boryasbora\"} ] license = {text = \"BSD-3-Clause\"} keywords = [\"climate\", \"llm\", \"climate-assessment\", \"rag\", \"decision-support\"] dependencies = [ \"streamlit\", \"xarray\", \"geopy\", \"geopandas\", \"pyproj\", \"requests\", \"requests-mock\", \"pandas\", \"folium\", \"langchain\", \"streamlit-folium\", \"netcdf4\", \"dask\", \"pip\", \"osmnx\", \"matplotlib\", \"openai\", \"langchain-community\", \"langchain-openai\", \"langchain-chroma\", \"langchain-core\", \"pydantic\", \"langgraph\", \"bs4\", \"wikipedia\", \"scipy\", \"pyproj\" ] [build-system] requires = [\"setuptools\"] build-backend = \"setuptools.build_meta\" [tool.setuptools] package-dir = {\"\" = \"src\"} [tool.setuptools.packages.find] where = [\"src\"] #find = {} # Scan the project directory with the default parameters [project.scripts] climsight = \"climsight.launch:launch_streamlit\" [project.urls] \"Source\" = \"https://github.com/CliDyn/climsight\" \"Citation\" = \"https://doi.org/10.1038/s43247-023-01199-1\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cnpypp",
            "repo_link": "https://gitlab.iap.kit.edu/mreininghaus/cnpypp/",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cola",
            "repo_link": "https://github.com/jokergoo/cola",
            "content": {
                "codemeta": "",
                "readme": "# cola: A General Framework for Consensus Partitioning <img src=\"https://user-images.githubusercontent.com/449218/54158555-03e3af80-444b-11e9-9773-070823101263.png\" width=250 align=\"right\" style=\"border:4px solid black;\" /> [![R-CMD-check](https://github.com/jokergoo/cola/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/cola/actions) [ ![bioc](https://bioconductor.org/shields/downloads/devel/cola.svg) ](http://bioconductor.org/packages/stats/bioc/cola) [ ![bioc](http://bioconductor.org//shields/lastcommit/devel/bioc/cola.svg) ](http://bioconductor.org/checkResults/devel/bioc-LATEST/cola/) ## Citation Zuguang Gu, et al., cola: an R/Bioconductor package for consensus partitioning through a general framework, Nucleic Acids Research, 2021. https://doi.org/10.1093/nar/gkaa1146 Zuguang Gu, et al., Improve consensus partitioning via a hierarchical procedure. Briefings in bioinformatics 2022. https://doi.org/10.1093/bib/bbac048 ## Install *cola* is available on [Bioconductor](http://bioconductor.org/packages/devel/bioc/html/cola.html), you can install it by: ```r if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"cola\") ``` The latest version can be installed directly from GitHub: ```r library(devtools) install_github(\"jokergoo/cola\") ``` ## Methods The **cola** supports two types of consensus partitioning. ### Standard consensus partitioning #### Features 1. It modularizes the consensus clustering processes that various methods can be easily integrated in different steps of the analysis. 2. It provides rich visualizations for intepreting the results. 3. It allows running multiple methods at the same time and provides functionalities to compare results in a straightforward way. 4. It provides a new method to extract features which are more efficient to separate subgroups. 5. It generates detailed HTML reports for the complete analysis. #### Workflow <img width=\"700\" src=\"https://user-images.githubusercontent.com/449218/52628723-86af3400-2eb8-11e9-968d-b7f47a408818.png\" /> The steps of consensus partitioning is: 1. Clean the input matrix. The processing are: adjusting outliers, imputing missing values and removing rows with very small variance. This step is optional. 2. Extract subset of rows with highest scores. Here \"scores\" are calculated by a certain method. For gene expression analysis or methylation data analysis, $n$ rows with highest variance are used in most cases, where the \"method\", or let's call it **\"the top-value method\"** is the variance (by `var()` or `sd()`). Note the choice of \"the top-value method\" can be general. It can be e.g. MAD (median absolute deviation) or any user-defined method. 3. Scale the rows in the sub-matrix (e.g. gene expression) or not (e.g. methylation data). This step is optional. 4. Randomly sample a subset of rows from the sub-matrix with probability $p$ and perform partition on the columns of the matrix by a certain partition method, with trying different numbers of subgroups. 5. Repeat step 4 several times and collect all the partitions. 6. Perform consensus partitioning analysis and determine the best number of subgroups which gives the most stable subgrouping. 7. Apply statistical tests to find rows that show significant difference between the predicted subgroups. E.g. to extract subgroup specific genes. 8. If rows in the matrix can be associated to genes, downstream analysis such as function enrichment analysis can be performed. #### Usage Three lines of code to perfrom *cola* analysis: ```r mat = adjust_matrix(mat) # optional rl = run_all_consensus_partition_methods( mat, top_value_method = c(\"SD\", \"MAD\", ...), partition_method = c(\"hclust\", \"kmeans\", ...), cores = ...) cola_report(rl, output_dir = ...) ``` #### Plots Following plots compare consensus heatmaps with k = 4 under all combinations of methods. <img src=\"https://user-images.githubusercontent.com/449218/52631118-3a66f280-2ebe-11e9-8dea-0172d9beab91.png\" /> ### Hierarchical consensus partitioning #### Features 1. It can detect subgroups which show major differences and also moderate differences. 2. It can detect subgroups with large sizes as well as with tiny sizes. 3. It generates detailed HTML reports for the complete analysis. #### Hierarchical Consensus Partitioning <img src=\"https://user-images.githubusercontent.com/449218/126491482-31a9496f-cc4d-4c4f-80b7-7b752d8d8d06.png\" width=\"400\" /> #### Usage Three lines of code to perfrom hierarchical consensus partitioning analysis: ```r mat = adjust_matrix(mat) # optional rh = hierarchical_partition(mat, mc.cores = ...) cola_report(rh, output_dir = ...) ``` #### Plots Following figure shows the hierarchy of the subgroups. <img src=\"https://user-images.githubusercontent.com/449218/100014572-d7b2c280-2dd6-11eb-9265-a84d324122f2.png\" width=\"300\" /> Following figure shows the signature genes. <img src=\"https://user-images.githubusercontent.com/449218/100014657-f913ae80-2dd6-11eb-9bf7-53f733e9f8f0.png\" width=\"600\" /> ## License MIT @ Zuguang Gu\n",
                "dependencies": "Package: cola Type: Package Title: A Framework for Consensus Partitioning Version: 2.13.1 Date: 2025-02-06 Authors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"), comment = c('ORCID'=\"0000-0002-7395-8709\")) Depends: R (>= 4.0.0) Imports: grDevices, graphics, grid, stats, utils, ComplexHeatmap (>= 2.5.4), matrixStats (>= 1.2.0), GetoptLong, circlize (>= 0.4.7), GlobalOptions (>= 0.1.0), clue, parallel, RColorBrewer, cluster, skmeans, png, mclust, crayon, methods, xml2, microbenchmark, httr, knitr (>= 1.4.0), markdown (>= 1.6), digest, impute, brew, Rcpp (>= 0.11.0), BiocGenerics, eulerr, foreach, doParallel, doRNG, irlba Suggests: genefilter, mvtnorm, testthat (>= 0.3), samr, pamr, kohonen, NMF, WGCNA, Rtsne, umap, clusterProfiler, ReactomePA, DOSE, AnnotationDbi, gplots, hu6800.db, BiocManager, data.tree, dendextend, Polychrome, rmarkdown, simplifyEnrichment, cowplot, flexclust, randomForest, e1071 Description: Subgroup classification is a basic task in genomic data analysis, especially for gene expression and DNA methylation data analysis. It can also be used to test the agreement to known clinical annotations, or to test whether there exist significant batch effects. The cola package provides a general framework for subgroup classification by consensus partitioning. It has the following features: 1. It modularizes the consensus partitioning processes that various methods can be easily integrated. 2. It provides rich visualizations for interpreting the results. 3. It allows running multiple methods at the same time and provides functionalities to straightforward compare results. 4. It provides a new method to extract features which are more efficient to separate subgroups. 5. It automatically generates detailed reports for the complete analysis. 6. It allows applying consensus partitioning in a hierarchical manner. URL: https://github.com/jokergoo/cola, https://jokergoo.github.io/cola_collection/ VignetteBuilder: knitr biocViews: Clustering, GeneExpression, Classification, Software License: MIT + file LICENSE LinkingTo: Rcpp\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/comando",
            "repo_link": "https://jugit.fz-juelich.de/iek-10/public/optimization/comando",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/comola",
            "repo_link": "https://github.com/Helmholtz-UFZ/CoMOLA",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/complexheatmap",
            "repo_link": "https://github.com/jokergoo/ComplexHeatmap",
            "content": {
                "codemeta": "",
                "readme": "# Make Complex Heatmaps <a href=\"https://jokergoo.github.io/ComplexHeatmap-reference/book/\"><img src=\"https://jokergoo.github.io/ComplexHeatmap-reference/book/complexheatmap-cover.jpg\" width=240 align=\"right\" style=\"border:2px solid black;\" ></a> [![R-CMD-check](https://github.com/jokergoo/ComplexHeatmap/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/ComplexHeatmap/actions) [![codecov](https://img.shields.io/codecov/c/github/jokergoo/ComplexHeatmap.svg)](https://codecov.io/github/jokergoo/ComplexHeatmap) [![bioc](http://www.bioconductor.org/shields/downloads/devel/ComplexHeatmap.svg)](https://bioconductor.org/packages/stats/bioc/ComplexHeatmap/) [![bioc](http://www.bioconductor.org/shields/years-in-bioc/ComplexHeatmap.svg)](http://bioconductor.org/packages/devel/bioc/html/ComplexHeatmap.html) <img src=\"http://jokergoo.github.io/complexheatmap_logo.svg\" width=\"550\"> Complex heatmaps are efficient to visualize associations between different sources of data sets and reveal potential patterns. Here the **ComplexHeatmap** package provides a highly flexible way to arrange multiple heatmaps and supports various annotation graphics. The [**InteractiveComplexHeatmap**](https://github.com/jokergoo/InteractiveComplexHeatmap) package can directly export static complex heatmaps into an interactive Shiny app. Have a try! ## Citation Zuguang Gu, et al., [Complex heatmaps reveal patterns and correlations in multidimensional genomic data](http://bioinformatics.oxfordjournals.org/content/early/2016/05/20/bioinformatics.btw313.abstract), Bioinformatics, 2016. Zuguang Gu. [Complex Heatmap Visualization](https://doi.org/10.1002/imt2.43), iMeta, 2022. ## Install `ComplexHeatmap` is available on [Bioconductor](http://www.bioconductor.org/packages/devel/bioc/html/ComplexHeatmap.html), you can install it by: ```r if (!requireNamespace(\"BiocManager\", quietly=TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"ComplexHeatmap\") ``` If you want the latest version, install it directly from GitHub: ```r library(devtools) install_github(\"jokergoo/ComplexHeatmap\") ``` ## Usage Make a single heatmap: ```r Heatmap(mat, ...) ``` A single Heatmap with column annotations: ```r ha = HeatmapAnnotation(df = anno1, anno_fun = anno2, ...) Heatmap(mat, ..., top_annotation = ha) ``` Make a list of heatmaps: ```r Heatmap(mat1, ...) + Heatmap(mat2, ...) ``` Make a list of heatmaps and row annotations: ```r ha = HeatmapAnnotation(df = anno1, anno_fun = anno2, ..., which = \"row\") Heatmap(mat1, ...) + Heatmap(mat2, ...) + ha ``` ## Documentation The full documentations are available at https://jokergoo.github.io/ComplexHeatmap-reference/book/ and the website is at https://jokergoo.github.io/ComplexHeatmap. ## Blog posts There are following blog posts focusing on specific topics: - [Make 3D heatmap](https://jokergoo.github.io/2021/03/24/3d-heatmap/) - [Translate from pheatmap to ComplexHeatmap](https://jokergoo.github.io/2020/05/06/translate-from-pheatmap-to-complexheatmap/) - [Set cell width/height in the heatmap](https://jokergoo.github.io/2020/05/11/set-cell-width/height-in-the-heatmap/) - [Interactive ComplexHeatmap](https://jokergoo.github.io/2020/05/15/interactive-complexheatmap/) - [Word cloud as heatmap annotation](https://jokergoo.github.io/2020/05/31/word-cloud-as-heatmap-annotation/) - [Which heatmap function is faster?](https://jokergoo.github.io/2020/06/19/which-heatmap-function-is-faster/) - [Rasterization in ComplexHeatmap](https://jokergoo.github.io/2020/06/30/rasterization-in-complexheatmap/) - [Block annotation over several slices](https://jokergoo.github.io/2020/07/06/block-annotation-over-several-slices/) - [Integrate ComplexHeatmap with cowplot package](https://jokergoo.github.io/2020/07/14/integrate-complexheatmap-with-cowplot-package/) ## Examples ### Visualize Methylation Profile with Complex Annotations ![complexheatmap_example4](https://user-images.githubusercontent.com/449218/47718635-2ec22980-dc49-11e8-9f01-37becb19e0d5.png) ### Correlations between methylation, expression and other genomic features ![complexheatmap_example3](https://user-images.githubusercontent.com/449218/47718636-2ec22980-dc49-11e8-8db0-1659c27dcf40.png) ### Visualize Cell Heterogeneity from Single Cell RNASeq ![complexheatmap_example2](https://user-images.githubusercontent.com/449218/47718637-2ec22980-dc49-11e8-925e-955c16cfa982.png) ### Making Enhanced OncoPrint ![complexheatmap_example1](https://user-images.githubusercontent.com/449218/47718638-2ec22980-dc49-11e8-845e-21e51d3b8e73.png) ### UpSet plot <img src=\"https://user-images.githubusercontent.com/449218/102615477-48c76a80-4136-11eb-98d9-3c528844fbe8.png\" width=500 /> ### 3D heatmap ![image](https://user-images.githubusercontent.com/449218/112284448-8c77c600-8c89-11eb-8d38-c5538900df20.png) ## License MIT @ Zuguang Gu\n",
                "dependencies": "Package: ComplexHeatmap Type: Package Title: Make Complex Heatmaps Version: 2.23.1 Date: 2025-04-01 Authors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"), comment = c('ORCID'=\"0000-0002-7395-8709\")) Depends: R (>= 4.0.0), methods, grid, graphics, stats, grDevices Imports: circlize (>= 0.4.14), GetoptLong, colorspace, clue, RColorBrewer, GlobalOptions (>= 0.1.0), png, digest, IRanges, matrixStats, foreach, doParallel, codetools Suggests: testthat (>= 1.0.0), knitr, markdown, dendsort, jpeg, tiff, fastcluster, EnrichedHeatmap, dendextend (>= 1.0.1), grImport, grImport2, glue, GenomicRanges, gridtext, pheatmap (>= 1.0.12), gridGraphics, gplots, rmarkdown, Cairo, magick VignetteBuilder: knitr Description: Complex heatmaps are efficient to visualize associations between different sources of data sets and reveal potential patterns. Here the ComplexHeatmap package provides a highly flexible way to arrange multiple heatmaps and supports various annotation graphics. biocViews: Software, Visualization, Sequencing URL: https://github.com/jokergoo/ComplexHeatmap, https://jokergoo.github.io/ComplexHeatmap-reference/book/ License: MIT + file LICENSE\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cipm",
            "repo_link": "https://github.com/CIPM-tools/CIPM",
            "content": {
                "codemeta": "",
                "readme": "# Commit-Based Continuous Integration of architectural Performance Models This repository provides the prototypical implementation for the change extraction, change propagation, incremental model update, and adaptive instrumentation of the [CIPM approach](https://sdq.kastel.kit.edu/wiki/CIPM). # Setup This project requires Java 11 for all actions. In particular, if a script is executed in the following, it usually uses Maven to build projects. As a result, the `JAVA_HOME` environment variable must be set, pointing to a JDK 11 (the top-level JDK directory, not the `bin` folder so that Maven can find the Java executable in `%JAVA_HOME%\\bin\\java.exe`). Additionally, any script must be executed from the top-level directory of this repository (and not within the `scripts` directory). When executing one of the scripts, it is possible that the error `Internal error: java.lang.IllegalArgumentException: bundleLocation not found: [home]/.m2/[...]` occurs. In such a case, it can help to delete the file `[home]/.m2/repository/.meta/p2-artifacts.properties` and restart the script. Currently, only Windows is supported. ## Build To build the complete project for the first time, the script `scripts/build.bat` needs to be executed. Subsequent builds can be executed with the script `scripts/rebuild.bat`. If a complete clean build is required, the script `scripts/clean.bat` allows to clean the complete repository with all build artifacts and submodules. Afterward, the script `scripts/build.bat` can be executed again. ## Execute TEAMMATES With the script `scripts/build-and-test.bat`, the project is built. In addition, the TEAMMATES case is executed. For more information, see [the test plugin description](commit-based-cipm/tests/cipm.consistency.vsum.test). ## Simple Setup with Eclipse With the following steps, the project can be setup within Eclipse to view the source code. It is possible to also edit the code, but not to test it. As a reminder, this project requires Java 11. As Eclipse version, the Eclipse Modeling Tools 2022-09 are currently supported. 1. In Eclipse, install the plugins from the CIPM update site. 1. Import any CIPM plugin. ## Full Setup with Eclipse The following steps are necessary to setup the project for development within Eclipse. As a reminder, this project requires Java 11. As Eclipse version, the project currently supports the Eclipse Modeling Tools 2022-09. It requires the installation of Xtext (from the Marketplace), Lombok (from [its update site](https://projectlombok.org/p2)), and Checkstyle (optional, from [update site](https://checkstyle.org/eclipse-cs-update-site)). 1. Execute the `scripts/build.bat` script if it was not executed before. 1. After executing the script, all bundles from `commit-based-cipm/bundles/fi` and `commit-based-cipm/releng-dev` can be imported into the Eclipse instance. The `releng-dev` directory contains the bundle `cipm.consistency.targetplatform` with the `cipm.consistency.targetplatform.target` file. Within Eclipse, open this file and click on `Set as active target platform`. Wait until the target platform is set, loaded, and the plugins are successfully compiled. It is possible that the target platform needs to be reloaded. 1. The project requires a second running Eclipse instance. After all plugins in the first instance has been setup, start the provided `SecondInstance` run configuration. It should start the second instance. In the second instance, import all bundles from `commit-based-cipm/bundles/si` and `commit-based-cipm/tests`. In `cipm.consistency.vsum.test`, the test cases should be executable. ## About the Internal Structure of the Setup The current build process provides a replicable build. Therefore, dependencies are provided via Eclipse P2 Update Sites with fixed versions or via Git submodules. In particular, the submodules include JaMoPP, SoMoX, and a specialized old Vitruv version. As the submodules only contain source code, they need to be compiled after cloning the repository or if they are cleaned. The build process contains necessary steps to build the submodules. The artifacts from the submodules are also packaged into local Eclipse P2 Update Sites. Unfortunately, Maven Tycho, which is internally used to build the artifacts, does not support local Eclipse P2 Update Sites via file paths and requires HTTP or HTTPS paths instead. Thus, a simple update site server, which serves the local Eclipse P2 Update Sites, is started and stopped during the build process. The Reactions language from Vitruv detects the meta-models from Vitruv domains. To find the Vitruv domains, the corresponding lookup mechanisms in Eclipse and in the build process require the domains and domain providers to be built and to be on the classpath. As a consequence, a separation between the bundles is performed. The first half of the bundles (located in `commit-based-cipm/bundles/fi`, also imported into the first Eclipse instance) contain two domains (one for the instrumentation model and one adjusted domain for Java) so that they are built in a first build step and for the second Eclipse instance. In the second build step and in the second Eclipse instance, the built domains can be found by the Reactions language. Notes on generating code for Reactions in the build process: 1. The plugin containing Reactions requires a `.maven_enable_dsls-generation` file in the plugin directory (the `[plugin name]` directory, not `[plugin name]/src`). 2. Furthermore, all classes or methods imported within a Reactiosn file cannot be located in the same plugin as the Reactions. They need to be in separated plugins. # Executing the Pipeline with TeaStore or TEAMMATES The manual to execute the pipeline with TeaStore or TEAMMATES can be found [here](commit-based-cipm/tests/cipm.consistency.vsum.test). Reference PCM repository models for these executions can be found [here](data).\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/copy-paste-imputation",
            "repo_link": "https://github.com/KIT-IAI/CopyPasteImputation",
            "content": {
                "codemeta": "",
                "readme": "# Copy-Paste Imputation (CPI) for Energy Time Series This repository contains the Python implementation of the Copy-Paste Imputation (CPI) method presented in the following paper: >M. Weber, M. Turowski, H. K. Çakmak, R. Mikut, U. Kühnapfel and V. Hagenmeyer, 2021, \"Data-Driven Copy-Paste Imputation for Energy Time Series,\" in IEEE Transactions on Smart Grid, 12, 6, pp. 5409-5419, doi: [10.1109/TSG.2021.3101831](https://doi.org/10.1109/TSG.2021.3101831). ## Installation To install this project, perform the following steps: 1. Clone the project 2. Open a terminal of the virtual environment where you want to use the project 3. `cd` into the cloned directory 4. `pip install .` or `pip install -e .` to install the project editable. * Use `pip install -e .[dev]` to install with development dependencies ## Use from cpiets.cpi import CopyPasteImputation import pandas as pd cpi = CopyPasteImputation() data = pd.read_csv('data.csv') cpi.fit(data) result = cpi.impute() ### Input Data Requirements **Example data:** | time | energy | | ------------------- | ------:| | 2012-01-02 00:15:00 | 11.60 | | 2012-01-02 00:30:00 | 24.87 | | 2012-01-02 00:45:00 | 37.31 | The names of the columns are arbitrary. **Assumptions:** * There are no missing values (nan) at the start or end of the time series. * A day starts with the first value after 0:00 (0:15 in the example above) and ends with 0:00. * The time series starts at the beginning of a day and ends at the end of a day. **Supported time formats:** * %Y-%m-%d %H:%M:%S (2020-01-17 13:37:42) * %d-%b-%Y %H:%M:%S (17-Jan-2020 13:37:42) ## Example In this repository, we included example data derived from the [ElectricityLoadDiagrams20112014](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014) data set. To run the CPI method with simple test data, you can run the example python example/simple_imputation.py and play around with the parameters. ## Funding This project is supported by the Helmholtz Association under the Joint Initiative \"Energy System 2050 - A Contribution of the Research Field Energy\". ## License This code is licensed under the [LGPL-3.0 License](COPYING).\n",
                "dependencies": "import setuptools with open(\"README.md\", \"r\") as fh: long_description = fh.read() setuptools.setup( name=\"cpi-ets\", version=\"0.0.2\", author=\"Moritz Weber\", author_email=\"moritz.weber@kit.edu\", description=\"Copy-Paste Imputation for Energy Time Series\", long_description=long_description, long_description_content_type=\"text/markdown\", url=\"...tbd...\", packages=['cpiets'], classifiers=[ \"Programming Language :: Python :: 3\", ], python_requires='>=3.6', install_requires=['numpy', 'pandas', 'prophet'], extras_require={ 'dev': [ 'pylint', ] }, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/corc",
            "repo_link": "https://github.com/KIT-TVA/CorC",
            "content": {
                "codemeta": "",
                "readme": "# CorC CbC is an approach to create correct programs incrementally. Guided by pre-/postcondition specifications, a program is developed using refinement rules, guaranteeing the implementation is correct. With [CorC](https://github.com/KIT-TVA/CorC/wiki), we implemented a graphical and textual IDE to create programs following the CbC approach. Starting with an abstract specification, CorC supports CbC developers in refining a program by a sequence of refinement steps and in verifying the correctness of these refinement steps using a deductive verifier. Learn more about CorC and the underlying concepts in the [wiki](https://github.com/KIT-TVA/CorC/wiki). # Getting Started ## Java Version Install [**JDK 21**](https://www.oracle.com/java/technologies/downloads/#java21). CorC may not work out of the box with newer versions. ## Eclipse Modelling Tools - Install [Eclipse Modelling Tools (EMT)](https://www.eclipse.org/downloads/packages/release/2024-03/r/eclipse-modeling-tools) (Version 2024-03). - Get the latest release of [Z3](https://github.com/Z3Prover/z3/releases) and add the `*/z3-[cur-version]-[x64/x32]-win/bin` folder to the environment variable [PATH](https://www.wikihow.com/Change-the-PATH-Environment-Variable-on-Windows) ## EMT Plugins - **Graphiti** Install Graphiti using the update site https://download.eclipse.org/graphiti/updates/release/0.18.0 - **FeatureIDE** Available in [Eclipse Marketplace](https://marketplace.eclipse.org/content/featureide) - **Mylyn** Available in [Eclipse Marketplace](https://marketplace.eclipse.org/content/mylyn) (Mylyn 3.23) - **TestNG** Available in [Eclipse Marketplace](https://marketplace.eclipse.org/content/testng-eclipse) ## CorC Setup 1. Clone the repo: ```sh git clone https://github.com/KIT-TVA/CorC.git ``` 2. In EMT, select `Open Projects... -> CorC` and check the boxes for the following packages: - `de.tu-bs.cs.isf.cbc.exceptions` - `de.tu-bs.cs.isf.cbc.model` - `de.tu-bs.cs.isf.cbc.mutation` - `de.tu-bs.cs.isf.cbc.tool` - `de.tu-bs.cs.isf.cbc.util` - `de.tu-bs.cs.isf.cbcclass.tool` - `de.tu-bs.cs.isf.wizards` - `de.tu_bs.cs.isf.cbc.parser` - `de.tu_bs.cs.isf.cbc.statistics` - `de.tu_bs.cs.isf.cbc.statistics.ui` - `de.tu_bs.cs.isf.commands.toolbar` - `de.tu_bs.cs.isf.lattice` - `de.tu_bs.cs.isf.proofrepository` - `de.kit.tva.lost` 3. Open: - `*.cbc.model -> model -> genmodel.genmodel` - `*.cbc.statistics -> model -> cbcstatistics.genmodel` Right click and `Generate Model/Edit/Editor Code` for all files listed. If EMT still displays errors, clean and rebuild all projects as described in the [common issues](#common-issues) section. 4. Select any package and run project as `Eclipse Application`. # Contribution 1. Create a fork. 2. Create a new branch with a name that describes your new feature. 3. Ensure that the command `mvn compile spotless:apply` runs successfully. 4. Start a pull request. ## Formatting We use the default Eclipse formatting style with [spotless](https://github.com/diffplug/spotless/blob/main/plugin-maven/README.md#eclipse-jdt). Run `mvn spotless:apply` to format all src files automatically if needed. # Examples & Case Study Introduction We provide different examples and case studies to explore CorC! ## Examples Create CorC-examples via `File -> New -> Other... -> CorC -> CorC Examples`. ## Case studies The repository you checked out contains various software product line case studies in the folder `CaseStudies`. They can be loaded via `File -> Open project from file system`. ### BankAccount The BankAccount implements basic functions of a bank account such as withdrawals, limits, money transfers and checking the account balance. - **BankAccount** Object-oriented implementation with class structure and CbC-Classes. - **BankAccountOO** Object-oriented implementation with class structure and CbC-Classes. Non-SPL implementation. ### Elevator The Elevator implements basic functions of an elevator such as the movement and entering and leaving of persons. - **Elevator** Object-oriented implementation with class structure and CbC-Classes. ### Email The product line Email implements basic functions of an email system including server- and client-side interactions. - **EmailOO** Object-oriented implementation with class structure and CbC-Classes. Non-SPL implementation. - **EmailFeatureInteraction** Java-Implementation without implementation with CbC. ### IntegerList The IntegerList implements a list of integers with add and sort operations. - **IntegerList** Object-oriented implementation with class structure and CbC-Classes. - **IntegerListOO** Object-oriented implementation with class structure and CbC-Classes. Non-SPL implementation. # Common Issues **Problem:** EMT gets stuck while trying to generate a model. **Solution:** Terminate EMT using any process manager and generate the model again. --- **Problem:** Multiple resolving errors after generating model files. **Solution:** Clean and rebuild all projects via `Project -> Clean...`. --- **Problem:** Cycling depedency issues. **Solution:** Navigate to: `Project -> Properties -> Java Compiler -> Building -> Configure Workspace Settings -> Build path problems -> Circular dependencies` and set the listbox to `Warning`. --- **Problem:** Errors in certain files about undefined methods and classes. **Solution:** Changing the compliance: `Project -> Java Compiler -> JDK Complicance -> Use compliance from execution environment 'JavaSE-16'`. --- **Problem:** Errors involving the message 'Cannot modify resource set without a write transaction'. **Solution:** Delete the folder `.settings` in `org.eclipse.core.runtime` within the current workspace. If that doesn't resolve the issue, delete all `.settings` folders and the `.project` file in the `CorC` folder. --- **Problem:** Some library file or package that is in the git is not shown locally in eclipse and there are errors missing that file **Solution:** Press `F5` when hovering over the parent directory of the missing file. The file should appear.\n",
                "dependencies": "<?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\" xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"> \t<modelVersion>4.0.0</modelVersion> \t<!-- The parent pom.xml and its children were build using the website https://eclipse.dev/Xtext/documentation/350_continuous_integration.html#tycho-build and the associated git repository https://github.com/xtext/maven-xtext-example git@github.com:xtext/maven-xtext-example.git--> \t<groupId>de.tu-bs.cbc</groupId> \t<artifactId>CorC-feat-autobuild_final</artifactId> \t<version>0.0.1-SNAPSHOT</version> \t<packaging>pom</packaging> \t<properties> \t\t<maven.compiler.release>21</maven.compiler.release> \t \t<tycho-version>4.0.10</tycho-version> \t \t<xtext-version>2.31.0</xtext-version> \t <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> \t <eclipse-repo.url>https://download.eclipse.org/releases/latest</eclipse-repo.url> \t <tycho.disableP2Mirrors>true</tycho.disableP2Mirrors> \t</properties> \t<build> \t <pluginManagement> \t \t<plugins> \t\t\t\t\t\t\t<!-- Formatting Plugin --> \t\t\t\t\t\t\t<plugin> \t\t\t\t\t\t\t\t<groupId>com.diffplug.spotless</groupId> \t\t\t\t\t\t\t\t<artifactId>spotless-maven-plugin</artifactId> \t\t\t\t\t\t\t\t<version>2.44.0</version> \t\t\t\t\t\t\t\t<configuration> \t\t\t\t\t\t\t\t\t<java> \t\t\t\t\t\t\t\t\t\t<removeUnusedImports/> \t\t\t\t\t\t\t\t\t\t<eclipse /> \t\t\t\t\t\t\t\t\t\t<includes> \t\t\t\t\t\t\t\t\t\t\t<include>src/**/*.java</include> \t\t\t\t\t\t\t\t\t\t</includes> \t\t\t\t\t\t\t\t\t</java> \t\t\t\t\t\t\t\t</configuration> \t\t\t\t\t\t\t<!-- --> \t\t\t\t\t\t</plugin> \t \t\t<plugin> \t\t <groupId>org.eclipse.tycho</groupId> \t\t <artifactId>tycho-p2-director-plugin</artifactId> \t\t <version>${tycho-version}</version> \t \t</plugin> \t \t<plugin> \t\t <groupId>org.eclipse.tycho.extras</groupId> \t\t <artifactId>tycho-eclipserun-plugin</artifactId> \t\t <version>3.0.0</version> \t \t</plugin> \t \t<!-- xtend-maven-plugin is in pluginManagement instead of in plugins \t\t so that it doesn't run before the exec-maven-plugin's *.mwe2 gen; \t\t this way we can list it after.--> \t\t\t \t\t\t <plugin> \t\t\t \t<groupId>org.eclipse.xtend</groupId> \t\t\t \t<artifactId>xtend-maven-plugin</artifactId> \t\t\t \t<version>2.29.0</version> \t\t\t \t<executions> \t\t\t \t<execution> \t\t\t \t\t<goals> \t\t\t\t\t <goal>compile</goal> \t\t\t\t\t <goal>xtend-install-debug-info</goal> \t\t\t\t\t <!--<goal>testCompile</goal>--> \t\t\t\t\t <goal>xtend-test-install-debug-info</goal> \t\t\t \t\t</goals> \t\t\t \t</execution> \t\t\t \t</executions> \t\t\t \t<configuration> \t\t\t \t<outputDirectory>xtend-gen</outputDirectory> \t\t\t \t</configuration> \t\t\t </plugin> \t\t\t <plugin> \t\t\t\t\t<groupId>org.eclipse.tycho</groupId> \t\t\t\t\t<artifactId>tycho-packaging-plugin</artifactId> \t\t\t\t\t<version>${tycho-version}</version> \t\t\t\t\t<configuration> \t\t\t\t\t\t<skipPomGeneration>true</skipPomGeneration> \t\t\t\t\t</configuration> \t\t\t\t</plugin> \t\t\t\t \t \t</plugins> \t </pluginManagement> \t <plugins> \t \t<plugin> \t\t <groupId>org.eclipse.tycho</groupId> \t\t <artifactId>tycho-maven-plugin</artifactId> \t\t <version>${tycho-version}</version> \t\t <extensions>true</extensions> \t\t\t\t\t\t<executions> <execution> <phase>package</phase> <id>package-feature</id> <configuration> <finalName>${project.artifactId}_${unqualifiedVersion}.${buildQualifier}</finalName> </configuration> </execution> \t\t\t\t\t\t</executions> \t\t\t\t</plugin> \t <!--<plugin> \t <artifactId>maven-failsafe-plugin</artifactId> \t <version>2.22.2</version> \t </plugin>--> \t \t<plugin> \t\t <groupId>org.eclipse.tycho</groupId> \t\t <artifactId>target-platform-configuration</artifactId> \t\t <version>${tycho-version}</version> \t\t <configuration> \t\t <!-- Optional set the Java version you are using--> \t\t <executionEnvironment>JavaSE-21</executionEnvironment> \t\t \t<target> \t\t\t\t\t\t<file> \t\t\t\t\t\t\t${project.basedir}/../targetplatform.target \t\t\t\t\t\t</file> \t\t\t\t\t</target> \t\t \t<environments> \t\t \t<environment> \t\t\t <os>linux</os> \t\t\t <ws>gtk</ws> \t\t\t <arch>x86_64</arch> \t\t \t</environment> \t\t \t<environment> \t\t\t <os>win32</os> \t\t\t <ws>win32</ws> \t\t\t <arch>x86_64</arch> \t\t \t</environment> \t\t \t<environment> \t\t\t \t<os>macosx</os> \t\t\t <ws>cocoa</ws> \t\t\t <arch>x86_64</arch> \t\t \t</environment> \t\t \t</environments> \t \t</configuration> \t \t</plugin> \t </plugins> \t</build> \t<repositories> \t\t<repository> \t \t<id>eclipse-release</id> \t\t <url>${eclipse-repo.url}</url> \t\t <layout>p2</layout> \t \t</repository> \t \t<repository> \t\t <id>graphiti</id> \t\t <url>https://download.eclipse.org/graphiti/updates/0.18.0/</url> \t\t <layout>p2</layout> \t \t</repository> \t \t<repository> \t \t<id>emftext</id> \t\t <url>http://update.emftext.org/release/</url> \t\t <layout>p2</layout> \t \t</repository> \t \t<repository> \t\t <id>featureIDE</id> \t\t <url>http://featureide.cs.ovgu.de/update/v3/</url> \t\t <layout>p2</layout> \t \t</repository> \t \t<repository> \t\t <id>mylyn</id> \t\t <url>https://download.eclipse.org/mylyn/snapshots/weekly/</url> \t\t <layout>p2</layout> \t \t</repository> \t \t<repository> \t\t \t<id>justj</id> \t\t \t<url>https://download.eclipse.org/justj/jres/25/updates/nightly/latest</url> \t\t \t<layout>p2</layout> \t \t</repository> \t \t<repository> \t \t\t<id>testng</id> \t \t\t<url>https://testng.org/testng-eclipse-update-site/</url> \t \t\t<layout>p2</layout> \t\t</repository> \t \t \t\t<repository> \t\t\t<id>codehaus-snapshots</id> \t\t\t<name>disable dead 'Codehaus Snapshots' repository, see https://bugs.eclipse.org/bugs/show_bug.cgi?id=481478</name> \t\t\t<url>http://nexus.codehaus.org/snapshots/</url> \t\t\t<releases> \t\t\t\t<enabled>false</enabled> \t\t\t</releases> \t\t\t<snapshots> \t\t\t\t<enabled>false</enabled> \t\t\t</snapshots> \t\t</repository> \t\t<!-- This must be disabled explicitly, otherwise it is enabled by https://github.com/mojohaus/mojo-parent \t\t\twhich is taken from exec-maven-plugin from at least version 1.6.0 --> \t\t<repository> \t\t\t<id>ossrh-snapshots</id> \t\t\t<name>ossrh-snapshots</name> \t\t\t<releases> \t\t\t\t<enabled>false</enabled> \t\t\t</releases> \t\t\t<snapshots> \t\t\t\t<enabled>false</enabled> \t\t\t</snapshots> \t\t\t<url>http://oss.sonatype.org/content/repositories/snapshots</url> \t\t</repository> \t\t<!-- This is enabled by /org/sonatype/oss/oss-parent/7 used as parent by \t\t\torg/xtext/antlr-generator/3.2.1 --> \t\t<repository> \t\t\t<id>sonatype-nexus-snapshots</id> \t\t\t<name>Sonatype Nexus Snapshots</name> \t\t\t<url>https://oss.sonatype.org/content/repositories/snapshots</url> \t\t\t<releases> \t\t\t\t<enabled>false</enabled> \t\t\t</releases> \t\t\t<snapshots> \t\t\t\t<enabled>false</enabled> \t\t\t</snapshots> \t\t</repository> \t</repositories> \t<pluginRepositories> \t\t<pluginRepository> \t\t\t<id>codehaus-snapshots</id> \t\t\t<name>disable dead 'Codehaus Snapshots' repository, see https://bugs.eclipse.org/bugs/show_bug.cgi?id=481478</name> \t\t\t<url>http://nexus.codehaus.org/snapshots/</url> \t\t\t<releases> \t\t\t\t<enabled>false</enabled> \t\t\t</releases> \t\t\t<snapshots> \t\t\t\t<enabled>false</enabled> \t\t\t</snapshots> \t\t</pluginRepository> \t\t<pluginRepository> \t\t\t<id>ossrh-snapshots</id> \t\t\t<name>ossrh-snapshots</name> \t\t\t<releases> \t\t\t\t<enabled>false</enabled> \t\t\t</releases> \t\t\t<snapshots> \t\t\t\t<enabled>false</enabled> \t\t\t</snapshots> \t\t\t<url>http://oss.sonatype.org/content/repositories/snapshots</url> \t\t</pluginRepository> \t\t<pluginRepository> \t\t\t<id>sonatype-nexus-snapshots</id> \t\t\t<name>Sonatype Nexus Snapshots</name> \t\t\t<url>https://oss.sonatype.org/content/repositories/snapshots</url> \t\t\t<releases> \t\t\t\t<enabled>false</enabled> \t\t\t</releases> \t\t\t<snapshots> \t\t\t\t<enabled>false</enabled> \t\t\t</snapshots> \t\t</pluginRepository> \t</pluginRepositories> \t<modules> \t <module>de.tu-bs.cs.isf.cbc.model</module> \t\t\t<module>de.tu-bs.cs.isf.cbc.mutation</module> \t\t\t<module>de.tu-bs.cs.isf.cbc.exceptions</module> \t <module>de.tu-bs.cs.isf.cbc.tool</module> \t <module>de.tu-bs.cs.isf.cbc.util</module> \t <module>de.tu-bs.cs.isf.cbcclass.tool</module> \t <module>de.tu-bs.cs.isf.wizards</module> \t <module>de.tu_bs.cs.isf.cbc.statistics</module> \t <module>de.tu_bs.cs.isf.cbc.statistics.ui</module> \t <module>de.tu_bs.cs.isf.commands.toolbar</module> \t <module>de.tu_bs.cs.isf.cbc.parser</module> \t <module>de.tu_bs.cs.isf.lattice</module> \t <module>de.kit.tva.lost</module> \t\t\t<module>de.tu-bs.cs.isf.cbc.proofrepository</module> \t</modules> </project>\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cosmoscout-vr",
            "repo_link": "https://github.com/cosmoscout/cosmoscout-vr",
            "content": {
                "codemeta": "",
                "readme": "<!-- SPDX-FileCopyrightText: German Aerospace Center (DLR) <cosmoscout@dlr.de> SPDX-License-Identifier: CC-BY-4.0 --> <p align=\"center\"> <img src =\"resources/logo/large.svg\" /> </p> CosmoScout VR is a modular virtual universe developed at the German Aerospace Center (DLR). It lets you explore, analyze and present huge planetary data sets and large simulation data in real-time. [![Build Status](https://github.com/cosmoscout/cosmoscout-vr/workflows/Build/badge.svg?branch=main)](https://github.com/cosmoscout/cosmoscout-vr/actions) [![REUSE](https://api.reuse.software/badge/github.com/cosmoscout/cosmoscout-vr)](https://api.reuse.software/info/github.com/cosmoscout/cosmoscout-vr) [![Coverage Status](https://coveralls.io/repos/github/cosmoscout/cosmoscout-vr/badge.svg?branch=main)](https://coveralls.io/github/cosmoscout/cosmoscout-vr?branch=main) [![documentation](https://img.shields.io/badge/Docs-online-34D058.svg)](docs/README.md) [![license](https://img.shields.io/badge/License-MIT-purple.svg)](LICENSE.md) [![source loc](https://img.shields.io/badge/LoC-15.9k-green.svg)](tools/cloc.sh) [![plugin loc](https://img.shields.io/badge/LoC_Plugins-25.5k-green.svg)](tools/cloc.sh) [![comments](https://img.shields.io/badge/Comments-8.4k-yellow.svg)](tools/cloc.sh) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3381953.svg)](https://doi.org/10.5281/zenodo.3381953) The software can be build on Linux (gcc or clang) and Windows (msvc). Nearly all dependencies are included as [git submodules](externals), please refer to the [**documentation**](docs) in order to get started. # Features <p align=\"center\"> <img src =\"docs/img/banner-mars.jpg\" /> </p> Below is a rough sketch of the possibilities you have with CosmoScout VR. While this list is far from complete it provides a good overview of the current feature set. You can also read the [**changelog**](docs/changelog.md) to learn what's new in the current version. There is also an [**interesting article in the DLR magazine**](https://dlr.de/dlr/portaldata/1/resources/documents/dlr_magazin_161_EN/DLR-Magazin_161-GB/?page=18), and [**several papers**](docs/citation.md) which provide some insight into the ideas behind CosmoScout VR. - [X] Solar System Simulation - [X] Positioning of celestial bodies and space crafts based on [SPICE](https://naif.jpl.nasa.gov/naif) - [X] Rendering of highly detailed level-of-detail planets based on WebMapServices (with [csp-lod-bodies](plugins/csp-lod-bodies)) - [X] Rendering of configurable atmospheres (Mie- and Rayleigh-scattering) around planets (with [csp-atmospheres](plugins/csp-atmospheres)) - [X] Physically based rendering of 3D satellites (with [csp-satellites](plugins/csp-satellites)) - [X] Rendering of Tycho, Tycho2 and Hipparcos star catalogues (with [csp-stars](plugins/csp-stars)) - [X] Rendering of orbits and trajectories based on SPICE (with [csp-trajectories](plugins/csp-trajectories)) - [X] Rendering of shadows - [X] HDR-Rendering - [x] Flexible User Interface - [X] Completely written in JavaScript with help of the [Chromium Embedded Framework](https://bitbucket.org/chromiumembedded/cef/src) - [X] Main UI can be drawn in the screen- or world-space - [X] Web pages can be placed on planetary surfaces - [X] Interaction works both in VR and on the Desktop - [x] Clear API between C++ and JavaScript - [ ] Cross-Platform - [X] Runs on Linux - [X] Runs on Windows - [ ] Runs on MacOS - [ ] System Architecture - [X] Plugin-based - most functionality is loaded at run-time - [ ] Network synchronization of multiple instances - [x] Hardware device support - CosmoScout VR basically supports everything which is supported by [ViSTA](https://github.com/cosmoscout/vista) and [VRPN](https://github.com/vrpn/vrpn). The devices below are actively supported (or planned to be supported). - [X] Mouse - [X] Keyboard - [X] HTC-Vive - [X] ART-Tracking systems - [X] 3D-Connexion Space Navigator - [X] Multi-screen systems like tiled displays or CAVE's - [X] Multi-screen systems on distributed rendering clusters - [X] Side-by-side stereo systems - [X] Quad-buffer stereo systems - [X] Anaglyph stereo systems - [x] Game Pads like the X-Box controller # Getting Started <p align=\"center\"> <img src =\"docs/img/banner-light-shafts.jpg\" /> </p> :warning: _**Warning:** CosmoScout VR is research software which is still under heavy development and changes on a daily basis. Many features are badly documented, it will crash without warning and may do other unexpected things. We are working hard on improving the user experience - please [report all issues and suggestions](https://github.com/cosmoscout/cosmoscout-vr/issues) you have!_ For each release, [binary packages](https://github.com/cosmoscout/cosmoscout-vr/releases) are automatically created via [Github Actions](https://github.com/cosmoscout/cosmoscout-vr/actions). When started for the very first time, some example datasets will be downloaded from the internet. **This will take some time!** The progress of this operation is shown on the loading screen. If the binary releases do not work for you or you want to test the latest features, you have to compile CosmoScout VR yourself. This is actually quite easy as there are several guides in the **[`docs`](docs)** directory to get you started! # Plugins for CosmoScout VR CosmoScout VR can be extended via plugins. In fact, without any plugins, CosmoScout VR is just a black and empty universe. Here is a list of plugins which are included in this repository. There are also additional plugins which are listed further below. Core Plugins | Description | Screenshot :----|:-----------------|:---------- [csp-anchor-labels](plugins/csp-anchor-labels) | Draws a click-able label at each celestial anchor. When activated, the user automatically travels to the selected body. The size and overlapping-behavior of the labels can be adjusted. | ![screenshot](docs/img/csp-anchor-labels.jpg) [csp-atmospheres](plugins/csp-atmospheres) | Draws atmospheres around celestial bodies. It supports multiple atmospheric models. | ![screenshot](docs/img/csp-atmospheres.jpg) [csp-custom-web-ui](plugins/csp-custom-web-ui) | Allows adding custom HTML-based user interface elements as sidebar-tabs, as floating windows or into free space. | ![screenshot](docs/img/csp-custom-web-ui.jpg) [csp-demo-node-editor](plugins/csp-demo-node-editor) | An example on how to use the `csl-node-editor` plugin library for creating data flow graphs within CosmoScout VR. | ![screenshot](docs/img/csp-demo-node-editor.jpg) [csp-fly-to-locations](plugins/csp-fly-to-locations) | Adds several quick travel targets to the sidebar. It supports shortcuts to celestial bodies and to specific geographic locations on those bodies. | ![screenshot](docs/img/csp-fly-to-locations.jpg) [csp-lod-bodies](plugins/csp-lod-bodies) | Draws level-of-detail planets and moons. This plugin supports the visualization of entire planets in a 1:1 scale. The data is streamed via Web-Map-Services (WMS) over the internet. A dedicated MapServer is required to use this plugin. | ![screenshot](docs/img/csp-lod-bodies.jpg) [csp-measurement-tools](plugins/csp-measurement-tools) | Provides several tools for terrain measurements. Like measurement of distances, height profiles, volumes or areas. | ![screenshot](docs/img/csp-measurement-tools.jpg) [csp-minimap](plugins/csp-minimap) | Displays a configurable 2D-Minimap in the user interface. | ![screenshot](docs/img/csp-minimap.jpg) [csp-recorder](plugins/csp-recorder) | A CosmoScout VR plugin which allows basic capturing of high-quality videos. Requires that `csp-web-api` is enabled. | ![screenshot](docs/img/csp-recorder.jpg) [csp-rings](plugins/csp-rings) | Draws simple rings around celestial bodies. The rings can be configured with an inner and an outer radius and a texture. | ![screenshot](docs/img/csp-rings.jpg) [csp-satellites](plugins/csp-satellites) | Draws GTLF models at positions based on SPICE data. It uses physically based rendering for surface shading. | ![screenshot](docs/img/csp-satellites.jpg) [csp-sharad](plugins/csp-sharad) | Renders radar datasets acquired by the Mars Reconnaissance Orbiter. The SHARAD profiles are rendered inside of Mars, the Martian surface is made translucent in front of the profiles. | ![screenshot](docs/img/csp-sharad.jpg) [csp-simple-bodies](plugins/csp-simple-bodies) | Renders simple spherical celestial bodies. The bodies are drawn as an ellipsoid with an equirectangular texture. | ![screenshot](docs/img/csp-simple-bodies.jpg) [csp-stars](plugins/csp-stars) | Draws 3D-stars loaded from catalogues. For now Tycho, Tycho2 and the Hipparcos catalogue are supported. | ![screenshot](docs/img/csp-stars.jpg) [csp-timings](plugins/csp-timings) | Uses the built-in timer queries of CosmoScout VR to draw on-screen live frame timing statistics. This plugin can also be used to export recorded time series to a CSV file. | ![screenshot](docs/img/csp-timings.jpg) [csp-trajectories](plugins/csp-trajectories) | Draws trajectories of celestial bodies and spacecrafts based on SPICE. The color, length, number of samples and the reference frame can be configured. | ![screenshot](docs/img/csp-trajectories.jpg) [csp-web-api](plugins/csp-web-api) | Allows to control CosmoScout VR via an HTTP protocol. It also allows capturing screenshots over HTTP. | ![screenshot](docs/img/csp-web-api.jpg) [csp-wms-overlays](plugins/csp-wms-overlays) | Overlays time dependent map data from Web-Map-Services (WMS) over bodies rendered by other plugins. | ![screenshot](docs/img/csp-wms-overlays.jpg) Additional Plugins | Description | Screenshot :----|:-----------------|:---------- [csp-gaussian-splatting](https://github.com/cosmoscout/csp-gaussian-splatting) | This plugin uses the code provided for the paper \"3D Gaussian Splatting for Real-Time Radiance Field Rendering\" to visualize radiance fields. | ![screenshot](docs/img/csp-gaussian-splatting.jpg) [csp-user-study](https://github.com/cosmoscout/csp-user-study) |This plugin was used for the user study of the IEEE Aerospace paper \"CosmoScout VR: A Modular 3D Solar System Based on SPICE\". It can be used to record a series of checkpoints which the user has to fly through. | ![screenshot](docs/img/csp-user-study.jpg) ### Credits Some badges in this README.md are from [shields.io](https://shields.io). The documentation of CosmoScout VR also uses icons from [simpleicons.org](https://simpleicons.org/). <p align=\"center\"><img src =\"docs/img/hr.svg\"/></p>\n",
                "dependencies": "# ------------------------------------------------------------------------------------------------ # # This file is part of CosmoScout VR # # ------------------------------------------------------------------------------------------------ # # SPDX-FileCopyrightText: German Aerospace Center (DLR) <cosmoscout@dlr.de> # SPDX-License-Identifier: MIT cmake_minimum_required(VERSION 3.13) project(cosmoscout-vr VERSION 1.10.0) # Use cmake 3.12's <PACKAGE>_ROOT variabled for searching. cmake_policy(SET CMP0074 NEW) # cmake_policy(SET CMP0167 OLD) # Ensure local modules (for dependencies etc.) are found. list(APPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/cmake) # CMAKE_BUILD_TYPE must be set (except for Visual Studio). if(NOT MSVC) if(NOT CMAKE_BUILD_TYPE OR (NOT ${CMAKE_BUILD_TYPE} STREQUAL \"Release\" AND NOT ${CMAKE_BUILD_TYPE} STREQUAL \"Debug\")) set(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"Release or Debug\" FORCE) endif() endif() # Use folders when targeting an IDE set_property(GLOBAL PROPERTY USE_FOLDERS ON) # find dependencies -------------------------------------------------------------------------------- include(GenerateExportHeader) # Boost and OpenGL must be present on your system. All other dependencies are included as submodules # in \"externals/\". Those must be built beforehand, preferably using the scripts \"make_externals.*\". set(Boost_REALPATH ON) set(Boost_USE_MULTITHREADED ON) set(Boost_USE_STATIC_LIBS OFF) find_package(Boost REQUIRED COMPONENTS system chrono date_time filesystem) find_package(OpenGL REQUIRED) # You have to provide the directory where the externals got installed to. The scripts make_*.sh and # make_*.bat set this directory via the command line. set(COSMOSCOUT_EXTERNALS_DIR COSMOSCOUT_EXTERNALS_DIR-NotFound CACHE STRING \"Directory where the externals got installed to.\") # Make sure to use forward slashes only. file(TO_CMAKE_PATH ${COSMOSCOUT_EXTERNALS_DIR} COSMOSCOUT_EXTERNALS_DIR) if (DEFINED ENV{CARES_ROOT_DIR}) SET(CARES_ROOT_DIR \"$ENV{CARES_ROOT_DIR}\") else() SET(CARES_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{CURL_ROOT_DIR}) SET(CURL_ROOT_DIR \"$ENV{CURL_ROOT_DIR}\") else() SET(CURL_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{CURLPP_ROOT_DIR}) SET(CURLPP_ROOT_DIR \"$ENV{CURLPP_ROOT_DIR}\") else() SET(CURLPP_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{SPDLOG_ROOT_DIR}) SET(SPDLOG_ROOT_DIR \"$ENV{SPDLOG_ROOT_DIR}\") else() SET(SPDLOG_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{GLM_ROOT_DIR}) SET(GLM_ROOT_DIR \"$ENV{GLM_ROOT_DIR}\") else() SET(GLM_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{GLI_ROOT_DIR}) SET(GLI_ROOT_DIR \"$ENV{GLI_ROOT_DIR}\") else() SET(GLI_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{DOCTEST_ROOT_DIR}) SET(DOCTEST_ROOT_DIR \"$ENV{DOCTEST_ROOT_DIR}\") else() SET(DOCTEST_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{TINYGLTF_ROOT_DIR}) SET(TINYGLTF_ROOT_DIR \"$ENV{TINYGLTF_ROOT_DIR}\") else() SET(TINYGLTF_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{CSPICE_ROOT_DIR}) SET(CSPICE_ROOT_DIR \"$ENV{CSPICE_ROOT_DIR}\") else() SET(CSPICE_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{CEF_ROOT_DIR}) SET(CEF_ROOT_DIR \"$ENV{CEF_ROOT_DIR}\") else() SET(CEF_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{TIFF_ROOT_DIR}) SET(TIFF_ROOT_DIR \"$ENV{TIFF_ROOT_DIR}\") else() SET(TIFF_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{GLEW_ROOT_DIR}) SET(GLEW_ROOT_DIR \"$ENV{GLEW_ROOT_DIR}\") else() SET(GLEW_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{SDL2_ROOT_DIR}) SET(SDL2_ROOT_DIR \"$ENV{SDL2_ROOT_DIR}\") else() SET(SDL2_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{SDL2_TTF_ROOT_DIR}) SET(SDL2_TTF_ROOT_DIR \"$ENV{SDL2_TTF_ROOT_DIR}\") else() SET(SDL2_TTF_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{OPENSG_ROOT_DIR}) SET(OPENSG_ROOT_DIR \"$ENV{OPENSG_ROOT_DIR}\") else() SET(OPENSG_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{CIVETWEB_ROOT_DIR}) SET(CIVETWEB_ROOT_DIR \"$ENV{CIVETWEB_ROOT_DIR}\") else() SET(CIVETWEB_ROOT_DIR ${COSMOSCOUT_EXTERNALS_DIR}) endif() if (DEFINED ENV{VISTA_CMAKE_CONFIG_DIR}) SET(VistaCoreLibs_DIR \"$ENV{VISTA_CMAKE_CONFIG_DIR}\") else() SET(VistaCoreLibs_DIR ${COSMOSCOUT_EXTERNALS_DIR}/share/VistaCoreLibs/cmake) endif() find_package(GLM REQUIRED) find_package(GLI REQUIRED) find_package(DOCTEST REQUIRED) find_package(TINYGLTF REQUIRED) find_package(CSPICE REQUIRED) find_package(CARES REQUIRED) find_package(CURL REQUIRED) find_package(CURLPP REQUIRED) find_package(SPDLOG REQUIRED) find_package(CEF REQUIRED) find_package(TIFF REQUIRED) find_package(GLEW REQUIRED) find_package(SDL2 REQUIRED) find_package(SDL2_ttf REQUIRED) find_package(OPENSG REQUIRED) find_package(CIVETWEB REQUIRED) find_package(VistaCoreLibs REQUIRED COMPONENTS \"VistaBase\" \"VistaKernel\" \"VistaKernelOpenSGExt\" \"VistaOGLExt\" ) # X11 is used on Linux to set the application window's name and icon. if (UNIX) find_package(X11) endif() # install some files ------------------------------------------------------------------------------- # Copy all third-party libraries to install directory. install( DIRECTORY ${COSMOSCOUT_EXTERNALS_DIR}/lib/ ${COSMOSCOUT_EXTERNALS_DIR}/bin/ DESTINATION \"lib\" FILES_MATCHING PATTERN \"*.so*\" PATTERN \"*.dll*\" ) # Copy boost libraries to install directory. foreach(BOOST_LIB ${Boost_CHRONO_LIBRARY_DEBUG} ${Boost_CHRONO_LIBRARY_RELEASE} ${Boost_DATE_TIME_LIBRARY_DEBUG} ${Boost_DATE_TIME_LIBRARY_RELEASE} ${Boost_FILESYSTEM_LIBRARY_RELEASE} ${Boost_FILESYSTEM_LIBRARY_DEBUG} ${Boost_SYSTEM_LIBRARY_RELEASE} ${Boost_SYSTEM_LIBRARY_DEBUG} ) if(EXISTS \"${BOOST_LIB}\") get_filename_component(LIB_BASE_NAME ${BOOST_LIB} NAME_WE) get_filename_component(LIB_PATH ${BOOST_LIB} PATH) if (WIN32) install(FILES ${LIB_PATH}/${LIB_BASE_NAME}.dll DESTINATION \"lib\") endif() if (UNIX) file(GLOB LIB_FILES ${LIB_PATH}/${LIB_BASE_NAME}.so*) install(FILES ${LIB_FILES} DESTINATION \"lib\") endif() endif() endforeach() # Install documentation directory install(DIRECTORY ${CMAKE_SOURCE_DIR}/docs DESTINATION \".\" ) # Install license files install(FILES ${CMAKE_SOURCE_DIR}/LICENSE-3RD-PARTY.txt ${CMAKE_SOURCE_DIR}/LICENSE.md DESTINATION \"docs\" ) install(DIRECTORY ${CMAKE_SOURCE_DIR}/LICENSES DESTINATION \"docs\" ) # create version header ---------------------------------------------------------------------------- message(STATUS \"Trying to get current git branch and commit...\") # Get the current git branch name. execute_process( COMMAND git rev-parse --abbrev-ref HEAD WORKING_DIRECTORY ${CMAKE_SOURCE_DIR} OUTPUT_VARIABLE GIT_BRANCH OUTPUT_STRIP_TRAILING_WHITESPACE ) # Get the current commit hash. execute_process( COMMAND git log -1 --format=%h WORKING_DIRECTORY ${CMAKE_SOURCE_DIR} OUTPUT_VARIABLE GIT_COMMIT_HASH OUTPUT_STRIP_TRAILING_WHITESPACE ) # Fallback to project version if it failed. if (GIT_BRANCH STREQUAL \"\") message(STATUS \" Failed - falling back to project version: v${PROJECT_VERSION}\") else() message(STATUS \" Success: v${PROJECT_VERSION} (${GIT_BRANCH} @${GIT_COMMIT_HASH})\") endif() configure_file( ${CMAKE_SOURCE_DIR}/src/cs-core/cs-version.hpp.in ${CMAKE_BINARY_DIR}/src/cs-core/cs-version.hpp ) # compiler settings -------------------------------------------------------------------------------- add_definitions( -DBOOST_ALL_DYN_LINK -DGLM_ENABLE_EXPERIMENTAL -DGLM_FORCE_SWIZZLE -DNOMINMAX -DSPDLOG_COMPILED_LIB ) if (MSVC) add_definitions( # For whatever reason ViSTA checks for 'WIN32' instead of '_WIN32' in a lot of places, which # results in Linux code being included on Windows. -DWIN32 ) add_compile_options( $<$<COMPILE_LANGUAGE:CXX>:-experimental:external> $<$<COMPILE_LANGUAGE:CXX>:-external:anglebrackets> $<$<COMPILE_LANGUAGE:CXX>:-external:W0> $<$<COMPILE_LANGUAGE:CXX>:-W3> $<$<COMPILE_LANGUAGE:CXX>:-WX> $<$<COMPILE_LANGUAGE:CXX>:-EHsc> $<$<COMPILE_LANGUAGE:CXX>:-wd4251> # Warns about multiple inheritance problems, which we can't avoid inheriting from ViSTA classes. $<$<COMPILE_LANGUAGE:CXX>:-wd4250> ) else() add_compile_options( $<$<COMPILE_LANGUAGE:CXX>:-Wall> $<$<COMPILE_LANGUAGE:CXX>:-Werror> ) endif() set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) # Enable unit tests option(COSMOSCOUT_UNIT_TESTS \"Enable compilation of tests\" OFF) if (NOT COSMOSCOUT_UNIT_TESTS) add_definitions(-DDOCTEST_CONFIG_DISABLE) endif() # Enable code coverage measurements option(COSMOSCOUT_COVERAGE_INFO \"Run code coverage analytics\" OFF) if(COSMOSCOUT_COVERAGE_INFO AND \"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\") add_definitions(--coverage) add_link_options(--coverage) endif() # subdirectories ----------------------------------------------------------------------------------- include_directories( ${CMAKE_BINARY_DIR}/src/cs-core ${CMAKE_BINARY_DIR}/src/cs-utils ${CMAKE_BINARY_DIR}/src/cs-graphics ${CMAKE_BINARY_DIR}/src/cs-gui ${CMAKE_BINARY_DIR}/src/cs-scene ) add_subdirectory(config) add_subdirectory(resources) add_subdirectory(src) add_subdirectory(plugins) add_subdirectory(tools/eclipse-shadow-generator)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cp2k",
            "repo_link": "https://github.com/cp2k/cp2k",
            "content": {
                "codemeta": "",
                "readme": "# CP2K [![Release Status][release-badge]][release-link] [![Debian Status][debian-badge]][debian-link] [![Fedora Status][fedora-badge]][fedora-link] [![Ubuntu Status][ubuntu-badge]][ubuntu-link] [![Arch Status][arch-badge]][arch-link] [![Homebrew Status][homebrew-badge]][homebrew-link] [![Docker Status][docker-badge]][docker-link] [![Spack Status][spack-badge]][spack-link] [![Conda Status][conda-badge]][conda-link] CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems. CP2K provides a general framework for different modeling methods such as DFT using the mixed Gaussian and plane waves approaches GPW and GAPW. Supported theory levels include DFT, MP2, RPA, GW, tight-binding (xTB, DFTB), semi-empirical methods (AM1, PM3, PM6, RM1, MNDO, ...), and classical force fields (AMBER, CHARMM, ...). CP2K can do simulations of molecular dynamics, metadynamics, Monte Carlo, Ehrenfest dynamics, vibrational analysis, core level spectroscopy, energy minimization, and transition state optimization using NEB or dimer method. CP2K is written in Fortran 2008 and can be run efficiently in parallel using a combination of multi-threading, MPI, and CUDA. ## Downloading CP2K source code To clone the current master (development version): ```shell git clone --recursive https://github.com/cp2k/cp2k.git cp2k ``` Note the `--recursive` flag that is needed because CP2K uses git submodules. To clone a release version v*x.y*: ```shell git clone -b support/vx.y --recursive https://github.com/cp2k/cp2k.git cp2k ``` For more information on downloading CP2K, see [Downloading CP2K](https://www.cp2k.org/download). For help on git, see [Git Tips & Tricks](https://github.com/cp2k/cp2k/wiki/Git-Tips-&-Tricks). ## Install CP2K The easiest way to build CP2K with all of its dependencies is as a [Docker container](./tools/docker/README.md). For building CP2K from scratch see the [installation instructions](./INSTALL.md). ## Links - [CP2K.org](https://www.cp2k.org) for showcases of scientific work, tutorials, exercises, presentation slides, etc. - [The manual](https://manual.cp2k.org/) with descriptions of all the keywords for the CP2K input file - [The dashboard](https://dashboard.cp2k.org) to get an overview of the currently tested architectures - [The Google group](https://groups.google.com/group/cp2k) to get help if you could not find an answer in one of the previous links - [Acknowledgements](https://www.cp2k.org/funding) for list of institutions and grants that help to fund the development of CP2K ## Directory organization - [`arch`](./arch): Collection of definitions for different architectures and compilers - [`benchmarks`](./benchmarks): Inputs for benchmarks - [`data`](./data): Simulation parameters e.g. basis sets and pseudopotentials - [`exts`](./exts): Access to external libraries via GIT submodules - [`src`](./src): The source code - [`tests`](./tests): Inputs for tests and regression tests - [`tools`](./tools): Mixed collection of useful scripts related to cp2k Additional directories created during build process: - `lib`: Libraries built during compilation - `obj`: Objects and other intermediate compilation-time files - `exe`: Where the executables will be located [arch-badge]: https://img.shields.io/aur/version/cp2k [arch-link]: https://aur.archlinux.org/packages/cp2k [conda-badge]: https://img.shields.io/conda/vn/conda-forge/cp2k [conda-link]: https://anaconda.org/conda-forge/cp2k [debian-badge]: https://img.shields.io/debian/v/cp2k [debian-link]: https://packages.debian.org/search?keywords=cp2k [docker-badge]: https://img.shields.io/docker/v/cp2k/cp2k?label=docker [docker-link]: https://hub.docker.com/r/cp2k/cp2k [fedora-badge]: https://img.shields.io/fedora/v/cp2k [fedora-link]: https://src.fedoraproject.org/rpms/cp2k [homebrew-badge]: https://img.shields.io/homebrew/v/cp2k [homebrew-link]: https://formulae.brew.sh/formula/cp2k [release-badge]: https://img.shields.io/github/v/release/cp2k/cp2k [release-link]: https://github.com/cp2k/cp2k/releases [spack-badge]: https://img.shields.io/spack/v/cp2k [spack-link]: https://packages.spack.io/package.html?name=cp2k [ubuntu-badge]: https://img.shields.io/ubuntu/v/cp2k [ubuntu-link]: https://packages.ubuntu.com/search?keywords=cp2k\n",
                "dependencies": "#!-------------------------------------------------------------------------------------------------! #! CP2K: A general program to perform molecular dynamics simulations ! #! Copyright 2000-2025 CP2K developers group <https://cp2k.org> ! #! ! #! SPDX-License-Identifier: GPL-2.0-or-later ! #!-------------------------------------------------------------------------------------------------! cmake_minimum_required(VERSION 3.24) # include our cmake snippets set(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ${CMAKE_CURRENT_SOURCE_DIR}/cmake) # ================================================================================================= # REQUIRE OUT-OF-SOURCE BUILDS file(TO_CMAKE_PATH \"${PROJECT_BINARY_DIR}/CMakeLists.txt\" LOC_PATH) if(EXISTS \"${LOC_PATH}\") message( FATAL_ERROR \"You cannot build in a source directory (or any directory with a CMakeLists.txt file). Please make a build subdirectory.\" ) endif() # ================================================================================================= # PROJECT AND VERSION include(CMakeDependentOption) include(GitSubmodule) include(CustomTargets) cmake_policy(SET CMP0048 NEW) if(POLICY CMP0144) cmake_policy(SET CMP0144 NEW) endif() # !!! Keep version in sync with cp2k_info.F !!! project( cp2k DESCRIPTION \"CP2K\" HOMEPAGE_URL \"https://www.cp2k.org\" VERSION \"2025.1\" LANGUAGES Fortran C CXX) list(APPEND CMAKE_MODULE_PATH \"${PROJECT_SOURCE_DIR}/cmake/modules\") # set language and standard. # # cmake does not provide any mechanism to set the fortran standard. Adding the # `std` option to compiler flags is the only way to control it. So leave them # be. # if(NOT DEFINED CMAKE_CUDA_STANDARD) set(CMAKE_CUDA_STANDARD 11) set(CMAKE_CUDA_STANDARD_REQUIRED ON) endif() if(NOT DEFINED CMAKE_CXX_STANDARD) set(CMAKE_CXX_STANDARD 14) set(CMAKE_CXX_STANDARD_REQUIRED ON) endif() if(NOT DEFINED CMAKE_C_STANDARD) set(CMAKE_C_STANDARD 11) set(CMAKE_C_STANDARD_REQUIRED ON) endif() if(NOT DEFINED CMAKE_HIP_STANDARD) set(CMAKE_HIP_STANDARD 14) set(CMAKE_HIP_STANDARD_REQUIRED ON) endif() # remove NDEBUG flag string(REPLACE \"-DNDEBUG\" \"\" CMAKE_C_FLAGS_RELEASE ${CMAKE_C_FLAGS_RELEASE}) string(REPLACE \"-DNDEBUG\" \"\" CMAKE_CXX_FLAGS_RELEASE ${CMAKE_CXX_FLAGS_RELEASE}) string(REPLACE \"-DNDEBUG\" \"\" CMAKE_Fortran_FLAGS_RELEASE ${CMAKE_Fortran_FLAGS_RELEASE}) string(REPLACE \"-DNDEBUG\" \"\" CMAKE_C_FLAGS_RELWITHDEBINFO ${CMAKE_C_FLAGS_RELWITHDEBINFO}) string(REPLACE \"-DNDEBUG\" \"\" CMAKE_CXX_FLAGS_RELWITHDEBINFO ${CMAKE_CXX_FLAGS_RELWITHDEBINFO}) string(REPLACE \"-DNDEBUG\" \"\" CMAKE_Fortran_FLAGS_RELWITHDEBINFO ${CMAKE_Fortran_FLAGS_RELWITHDEBINFO}) string(REPLACE \"-DNDEBUG\" \"\" CMAKE_C_FLAGS_MINSIZEREL ${CMAKE_C_FLAGS_MINSIZEREL}) string(REPLACE \"-DNDEBUG\" \"\" CMAKE_CXX_FLAGS_MINSIZEREL ${CMAKE_CXX_FLAGS_MINSIZEREL}) string(REPLACE \"-DNDEBUG\" \"\" CMAKE_Fortran_FLAGS_MINSIZEREL ${CMAKE_Fortran_FLAGS_MINSIZEREL}) find_package(PkgConfig) # ############################################################################## # Define the paths for static libraries and executables # ############################################################################## set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${cp2k_BINARY_DIR}/lib CACHE PATH \"Single output directory for building all libraries.\") # Search for rocm in common locations foreach(__var ROCM_ROOT CRAY_ROCM_ROOT ORNL_ROCM_ROOT CRAY_ROCM_PREFIX ROCM_PREFIX CRAY_ROCM_DIR) if($ENV{${__var}}) list(APPEND CMAKE_PREFIX_PATH $ENV{__var}) set(ROCM_PATH $ENV{__var} CACHE PATH \"Path to ROCm installation\") endif() endforeach() set(CMAKE_INSTALL_LIBDIR \"lib\" CACHE PATH \"Default installation directory for libraries\") # ================================================================================================= # OPTIONS option(CMAKE_POSITION_INDEPENDENT_CODE \"Enable position independent code\" ON) option(CP2K_ENABLE_CONSISTENCY_CHECKS \"Check that the list of compiled files and files contained in src match\" OFF) option(CP2K_USE_EVERYTHING \"Enable all dependencies. They can be individually turned off again.\" OFF) option(CP2K_USE_DFTD4 \"Enable DFTD4 support\" ${CP2K_USE_EVERYTHING}) option(CP2K_USE_DEEPMD \"Enable DeePMD\" ${CP2K_USE_EVERYTHING}) option(CP2K_USE_FFTW3 \"Enable fftw3 support\" ${CP2K_USE_EVERYTHING}) option(CP2K_USE_MPI \"Enable MPI support\" ${CP2K_USE_EVERYTHING}) option(CP2K_USE_LIBINT2 \"Enable libint2 support\" ${CP2K_USE_EVERYTHING}) option(CP2K_USE_VORI \"Enable libvori support\" ${CP2K_USE_EVERYTHING}) option(CP2K_USE_SPGLIB \"Enable spglib support\" ${CP2K_USE_EVERYTHING}) option(CP2K_USE_LIBXC \"Enable libxc support\" ${CP2K_USE_EVERYTHING}) option(CP2K_USE_LIBTORCH \"Enable libtorch support\" ${CP2K_USE_EVERYTHING}) option(CP2K_USE_GRPP \"Enable libgrpp support\" ${CP2K_USE_EVERYTHING}) option(CP2K_USE_TREXIO \"Enable trexio support\" ${CP2K_USE_EVERYTHING}) option(CP2K_USE_HDF5 \"Enable HDF5 support\" ${CP2K_USE_EVERYTHING}) option(CP2K_USE_GREENX \"Enable GreenX support\" ${CP2K_USE_EVERYTHING}) option(CP2K_USE_STATIC_BLAS \"Link against static version of BLAS/LAPACK\" OFF) option(BUILD_SHARED_LIBS \"Build cp2k shared library\" ON) option( CP2K_USE_FFTW3_WITH_MKL \"MKL has its own compatible implementation of the fftw library. This option, when ON, will use the separate and original fftw library.\" OFF) # MPI-enabled options cmake_dependent_option(CP2K_USE_COSMA \"Enable COSMA support\" ${CP2K_USE_EVERYTHING} \"CP2K_USE_MPI\" OFF) cmake_dependent_option(CP2K_USE_DLAF \"Enable DLA-Future support\" ${CP2K_USE_EVERYTHING} \"CP2K_USE_MPI\" OFF) cmake_dependent_option(CP2K_USE_ELPA \"Enable ELPA support\" ${CP2K_USE_EVERYTHING} \"CP2K_USE_MPI\" OFF) cmake_dependent_option(CP2K_USE_LIBSMEAGOL \"Enable libSMEAGOL support\" ${CP2K_USE_EVERYTHING} \"CP2K_USE_MPI\" OFF) cmake_dependent_option(CP2K_USE_MPI_F08 \"Enable MPI Fortran 2008 interface\" ${CP2K_USE_EVERYTHING} \"CP2K_USE_MPI\" OFF) cmake_dependent_option(CP2K_USE_PLUMED \"Enable PLUMED2 support\" ${CP2K_USE_EVERYTHING} \"CP2K_USE_MPI\" OFF) cmake_dependent_option(CP2K_USE_SIRIUS \"Enable SIRIUS support\" ${CP2K_USE_EVERYTHING} \"CP2K_USE_MPI\" OFF) cmake_dependent_option(CP2K_USE_SPLA \"Enable SPLA support\" ${CP2K_USE_EVERYTHING} \"CP2K_USE_MPI\" OFF) cmake_dependent_option( CP2K_USE_LIBXSMM \"Enable libxsmm support\" ${CP2K_USE_EVERYTHING} \"NOT CP2K_USE_ACCEL MATCHES \\\"OPENCL\\\"\" ON) cmake_dependent_option(CP2K_USE_LIBVDWXC \"Enable libvdwxc support with SIRIUS\" ${CP2K_USE_EVERYTHING} \"CP2K_USE_SIRIUS\" OFF) cmake_dependent_option( CP2K_DBCSR_USE_CPU_ONLY \"Disable the DBCSR accelerated backend\" OFF \"NOT CP2K_USE_ACCEL MATCHES \\\"OPENCL\\\"\" OFF) cmake_dependent_option( CP2K_ENABLE_DBM_GPU \"Disable the dbm accelerated backend (mostly GPU).\" ON \"CP2K_USE_ACCEL\" OFF) cmake_dependent_option( CP2K_ENABLE_GRID_GPU \"Disable acceleration for grid related functions.\" ON \"CP2K_USE_ACCEL MATCHES \\\"HIP|CUDA\\\"\" OFF) cmake_dependent_option( CP2K_ENABLE_PW_GPU \"Disable the ffts accelerated backend (mostly GPU).\" ON \"CP2K_USE_ACCEL MATCHES \\\"HIP|CUDA\\\"\" OFF) cmake_dependent_option( CP2K_USE_UNIFIED_MEMORY \"Use CPU/GPU unified memory (Mi250x onwards)\" OFF \"CP2K_USE_ACCEL MATCHES \\\"HIP\\\"\" OFF) cmake_dependent_option(CP2K_ENABLE_ELPA_OPENMP_SUPPORT \"Enable ELPA OpenMP support\" ON \"CP2K_USE_ELPA\" OFF) cmake_dependent_option(CP2K_ENABLE_FFTW3_OPENMP_SUPPORT \"Enable FFTW OpenMP support\" ON \"CP2K_USE_FFTW3\" OFF) cmake_dependent_option(CP2K_ENABLE_FFTW3_THREADS_SUPPORT \"Enable FFTW threads support\" OFF \"CP2K_USE_FFTW3\" OFF) cmake_dependent_option( CP2K_USE_CUSOLVER_MP \"Use Nvidia gpu accelerated eigensolver. Only active when CUDA is ON\" OFF \"CP2K_USE_ACCEL MATCHES \\\"CUDA\\\"\" OFF) cmake_dependent_option(CP2K_USE_NVHPC OFF \"Enable Nvidia NVHPC kit\" \"(NOT CP2K_USE_ACCEL MATCHES \\\"CUDA\\\")\" OFF) cmake_dependent_option( CP2K_USE_SPLA_GEMM_OFFLOADING ON \"Enable SPLA dgemm offloading (only valid with gpu support on)\" \"(NOT CP2K_USE_ACCEL MATCHES \\\"NONE\\\") AND (CP2K_USE_SPLA)\" OFF) set(CP2K_BLAS_VENDOR \"auto\" CACHE STRING \"BLAS library for computations on host\") set(CP2K_SCALAPACK_VENDOR_LIST \"MKL\" \"SCI\" \"GENERIC\" \"auto\") set(CP2K_SCALAPACK_VENDOR \"auto\" CACHE STRING \"scalapack vendor/generic backend\") set_property(CACHE CP2K_SCALAPACK_VENDOR PROPERTY STRINGS ${CP2K_SCALAPACK_VENDOR_LIST}) if(DEFINED CP2K_SCALAPACK_VENDOR) if(NOT ${CP2K_SCALAPACK_VENDOR} IN_LIST CP2K_SCALAPACK_VENDOR_LIST) message(FATAL_ERROR \"An invalid ScaLAPACK vendor backend was specified\") endif() endif() set(CP2K_DATA_DIR \"default\" CACHE STRING \"Set the location for cp2k data\") # ############################################################################## # gpu related options # ############################################################################## set(CP2K_SUPPORTED_ACCELERATION_TARGETS CUDA HIP OPENCL NONE) set(CP2K_SUPPORTED_CUDA_ARCHITECTURES K20X K40 K80 P100 V100 A100 H100 A40) set(CP2K_SUPPORTED_HIP_ARCHITECTURES Mi50 Mi100 Mi210 Mi250 Mi300 K20X K40 K80 P100 V100 A100 H100 A40) set(CP2K_WITH_GPU \"NONE\" CACHE STRING \"Set the CUDA GPU architecture if HIP is enabled (default: NONE)\") set_property( CACHE CP2K_WITH_GPU PROPERTY STRINGS ${CP2K_SUPPORTED_CUDA_ARCHITECTURES} ${CP2K_SUPPORTED_HIP_ARCHITECTURES}) set(CP2K_USE_ACCEL \"NONE\" CACHE STRING \"Set hardware acceleration support: CUDA, HIP, OPENCL\") set_property(CACHE CP2K_USE_ACCEL PROPERTY STRINGS ${CP2K_SUPPORTED_ACCELERATION_TARGETS}) # ############################################################################## # specific variables for the regtests. Binaries will be created with an # extension # ############################################################################## set(__cp2k_ext \"\") if(CP2K_USE_MPI) set(__cp2k_ext \"psmp\") else() set(__cp2k_ext \"ssmp\") endif() # we can run the src consistency checks without actually searching for any # dependencies. if(CP2K_ENABLE_CONSISTENCY_CHECKS) add_subdirectory(src) # it is better to simply rm -Rf build but if someone wants to do something # like # # cmake -DCP2K_ENABLE_CONSISTENCY_CHECKS=ON .. cmake .. # # he/she can set(CP2K_ENABLE_CONSISTENCY_CHECKS OFF CACHE BOOL \"\" FORCE) return() endif() set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${cp2k_BINARY_DIR}/bin CACHE PATH \"Single output directory for building all executables.\") # Python # # this module looks preferably for version 3 of Python. If not found, version 2 # is searched. In CMake 3.15, if a python virtual environment is activated, it # will search the virtual environment for a python interpreter before searching # elsewhere in the system. In CMake <3.15, the system is searched before the # virtual environment. if(NOT Python_EXECUTABLE) # If the python interpreter isn't specified as a command line option, look for # it: find_package( Python COMPONENTS Interpreter REQUIRED) endif() # get the git hash Get the latest abbreviated commit hash of the working branch execute_process( COMMAND git log -1 --format=%h WORKING_DIRECTORY ${CMAKE_CURRENT_LIST_DIR} OUTPUT_VARIABLE CP2K_GIT_HASH OUTPUT_STRIP_TRAILING_WHITESPACE) execute_process( COMMAND hostnamectl --transient WORKING_DIRECTORY ${CMAKE_CURRENT_LIST_DIR} OUTPUT_VARIABLE CP2K_HOST_NAME OUTPUT_STRIP_TRAILING_WHITESPACE) add_custom_target( AlwaysCheckGit COMMAND ${CMAKE_COMMAND} -DRUN_CHECK_GIT_VERSION=1 -Dpre_configure_dir=${pre_configure_dir} -Dpost_configure_file=${post_configure_dir} -DGIT_HASH_CACHE=${GIT_HASH_CACHE} -P ${CURRENT_LIST_DIR}/CheckGit.cmake BYPRODUCTS ${post_configure_file}) # MPI if(CP2K_USE_MPI) get_property(REQUIRED_MPI_COMPONENTS GLOBAL PROPERTY ENABLED_LANGUAGES) list(REMOVE_ITEM REQUIRED_MPI_COMPONENTS CUDA) # CUDA does not have an MPI # component if(NOT CMAKE_CROSSCOMPILING) # when cross compiling, assume the users know # what they are doing set(MPI_DETERMINE_LIBRARY_VERSION TRUE) endif() find_package( MPI COMPONENTS ${REQUIRED_MPI_COMPONENTS} REQUIRED) if(NOT MPI_Fortran_HAVE_F90_MODULE) message( FATAL_ERROR \"\\ The listed MPI implementation does not provide the required mpi.mod interface. \\ When using the GNU compiler in combination with Intel MPI, please use the \\ Intel MPI compiler wrappers. Check the INSTALL.md for more information.\") endif() if(\"${MPI_Fortran_LIBRARY_VERSION_STRING}\" MATCHES \"Open MPI v2.1\" OR \"${MPI_Fortran_LIBRARY_VERSION_STRING}\" MATCHES \"Open MPI v3.1\") message( WARNING \"RMA with ${MPI_Fortran_LIBRARY_VERSION_STRING} is not supported due to issues with its implementation.\" \" Please use a newer version of OpenMPI or switch to MPICH if you plan on using MPI-RMA.\" ) endif() endif() # BLAS & LAPACK, PkgConfig find_package(Lapack REQUIRED) # also calls find_package(BLAS) # SMM (Small Matrix-Matrix multiplication) if(CP2K_USE_LIBXSMM) find_package(LibXSMM REQUIRED) message(STATUS \"Using LibXSMM for Small Matrix Multiplication\") endif() # in practice it is always for any decent configuration. But I add a flags to # turn it off if(CP2K_USE_MPI) find_package(SCALAPACK REQUIRED) endif() # CUDA / ROCM easy for cuda a moving target for hip if((CP2K_USE_ACCEL MATCHES CUDA) OR (CP2K_USE_ACCEL MATCHES HIP)) set(CP2K_GPU_ARCH_NUMBER_K20X 35) set(CP2K_GPU_ARCH_NUMBER_K40 35) set(CP2K_GPU_ARCH_NUMBER_K80 37) set(CP2K_GPU_ARCH_NUMBER_P100 60) set(CP2K_GPU_ARCH_NUMBER_V100 70) set(CP2K_GPU_ARCH_NUMBER_A100 80) set(CP2K_GPU_ARCH_NUMBER_H100 90) set(CP2K_GPU_ARCH_NUMBER_A40 86) set(CP2K_GPU_ARCH_NUMBER_Mi50 gfx906) set(CP2K_GPU_ARCH_NUMBER_Mi100 gfx908) set(CP2K_GPU_ARCH_NUMBER_Mi200 gfx90a) set(CP2K_GPU_ARCH_NUMBER_Mi250 gfx90a) set(CP2K_GPU_ARCH_NUMBER_Mi300 gfx942) # CMAKE_HIP_ARCHITECTURES and CMAKE_CUDA_ARCHITECTURES are the prefered # mechanism to set the gpu architecture. We still offer the CP2K_WITH_GPU # option to avoid breaking the ci/cd or any other scripts based on this # option. # check that CMAKE_{HIP|CUDA}_ARCHITECTURES or CP2K_WITH_GPU are given if((NOT DEFINED CMAKE_HIP_ARCHITECTURES) AND (NOT DEFINED CMAKE_CUDA_ARCHITECTURES) AND (CP2K_WITH_GPU MATCHES NONE)) message( FATAL_ERROR \"----------------------------------------------------------------\\n\" \" \\n\" \"CMAKE_{HIP|CUDA}_ARCHITECTURES or CP2K_WITH_GPU should be given.\\n\" \" \\n\" \"----------------------------------------------------------------\\n\") endif() set(_ignore_with_gpu_option false) if((DEFINED CMAKE_HIP_ARCHITECTURES) OR (DEFINED CMAKE_CUDA_ARCHITECTURES)) set(_ignore_with_gpu_option true) endif() if(NOT _ignore_with_gpu_option) message( STATUS \"\\nCP2K_WITH_GPU is deprecated in favor of CMAKE_HIP_ARCHITECTURES or CMAKE_CUDA_ARCHITECTURES\\n\" ) if(CP2K_USE_ACCEL MATCHES CUDA) list(FIND CP2K_SUPPORTED_CUDA_ARCHITECTURES ${CP2K_WITH_GPU} CP2K_GPU_SUPPORTED) if(CP2K_GPU_SUPPORTED EQUAL -1) message( FATAL_ERROR \"GPU architecture (${CP2K_WITH_GPU}) is not supported. Please choose from: ${CP2K_SUPPORTED_CUDA_ARCHITECTURES}\" ) endif() set(CMAKE_CUDA_ARCHITECTURES ${CP2K_GPU_ARCH_NUMBER_${CP2K_WITH_GPU}}) else(CP2K_USE_ACCEL MATCHES HIP) list(FIND CP2K_SUPPORTED_HIP_ARCHITECTURES ${CP2K_WITH_GPU} CP2K_GPU_SUPPORTED) if(CP2K_GPU_SUPPORTED EQUAL -1) message( FATAL_ERROR \"GPU architecture (${CP2K_WITH_GPU}) is not supported. Please choose from: ${CP2K_SUPPORTED_HIP_ARCHITECTURES}\" ) endif() set(CMAKE_HIP_ARCHITECTURES \"${CP2K_GPU_ARCH_NUMBER_${CP2K_WITH_GPU}}\") endif() endif() endif() set(CP2K_USE_HIP OFF) set(CP2K_USE_CUDA OFF) set(CP2K_USE_OPENCL OFF) if(CP2K_USE_ACCEL MATCHES \"CUDA\") option(CP2K_WITH_CUDA_PROFILING \"Enable CUDA profiling\" OFF) # P100 is the default target. # allow for unsupported compilers (gcc/cuda version mismatch) set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -allow-unsupported-compiler\") enable_language(CUDA) if(CP2K_USE_NVHPC) find_package(NVHPC REQUIRED COMPONENTS CUDA MATH HOSTUTILS NCCL) else() find_package(CUDAToolkit REQUIRED) endif() message(\"\\n-----------------------------------------------------------\") message(\"- CUDA -\") message(\"-----------------------------------------------------------\\n\") message(STATUS \"GPU architecture number: ${CMAKE_CUDA_ARCHITECTURES}\") message(STATUS \"GPU profiling enabled: ${CP2K_WITH_CUDA_PROFILING}\") if(WITH_CUDA_PROFILING) find_library( CUDA_NVTOOLSEXT nvToolsExt PATHS ${CMAKE_CUDA_IMPLICIT_LINK_DIRECTORIES} DOC \"Building with CUDA profiling requires the nvToolsExt CUDA library\" REQUIRED) message(STATUS \"Found nvToolsExt: ${CUDA_NVTOOLSEXT}\") endif() set(CP2K_USE_CUDA ON) if(CP2K_USE_CUSOLVER_MP) find_package(CuSolverMP REQUIRED) endif() message(STATUS \"CUDA compiler and libraries found\\n\") elseif(CP2K_USE_ACCEL MATCHES \"HIP\") message(\"\\n------------------------------------------------------------\") message(\"- HIP -\") message(\"------------------------------------------------------------\\n\") message(INFO \"${CMAKE_HIP_ARCHITECTURES}\") enable_language(HIP) if(CMAKE_HIP_PLATFORM MATCHES \"nvidia\") find_package(CUDAToolkit) endif() if(NOT CMAKE_BUILD_TYPE AND (CMAKE_HIP_PLATFORM MATCHES \"amd\")) set(CMAKE_HIP_FLAGS \"-O3\") elseif(CMAKE_BUILD_TYPE STREQUAL \"RelWithDebInfo\") set(CMAKE_HIP_FLAGS \"-O2 -g\") elseif(CMAKE_BUILD_TYPE STREQUAL \"Release\") set(CMAKE_HIP_FLAGS \"-O3\") elseif(CMAKE_BUILD_TYPE STREQUAL \"Debug\") set(CMAKE_HIP_FLAGS \"-O0 -g\") endif() # Find hip find_package(hipfft REQUIRED IMPORTED CONFIG) find_package(hipblas REQUIRED IMPORTED CONFIG) set(CP2K_USE_HIP ON) # use hardware atomic operations on Mi250X. if(NOT CMAKE_HIP_PLATFORM OR (CMAKE_HIP_PLATFORM MATCHES \"amd\")) set(CMAKE_HIP_FLAGS \"${CMAKE_HIP_FLAGS} -munsafe-fp-atomics\") endif() # add the Mi300A parameters when available if(CP2K_USE_UNIFIED_MEMORY) if(CMAKE_HIP_ARCHITECTURES MATCHES \"gfx90a\") set(CMAKE_HIP_ARCHITECTURES \"gfx90a:xnack+\") endif() if(CMAKE_HIP_ARCHITECTURES MATCHES \"gfx942\") set(CMAKE_HIP_ARCHITECTURES \"gfx942:xnack+\") endif() endif() elseif(CP2K_USE_ACCEL MATCHES \"OPENCL\") find_package(OpenCL REQUIRED) set(CP2K_USE_OPENCL ON) endif() message(\"\\n------------------------------------------------------------\") message(\"- OPENMP -\") message(\"------------------------------------------------------------\\n\") # PACKAGE DISCOVERY (compiler configuration can impact package discovery) find_package(OpenMP REQUIRED COMPONENTS Fortran C CXX) find_package(DBCSR 2.6 REQUIRED) # ================================== if(CP2K_USE_ELPA) find_package(Elpa REQUIRED) endif() if(CP2K_USE_LIBXC) find_package(Libxc 7 REQUIRED CONFIG) endif() if(CP2K_USE_HDF5) find_package(HDF5 REQUIRED COMPONENTS C Fortran) endif() if(CP2K_USE_COSMA) find_package(cosma REQUIRED) get_target_property(CP2K_COSMA_INCLUDE_DIRS cosma::cosma INTERFACE_INCLUDE_DIRECTORIES) get_target_property(CP2K_COSMA_LINK_LIBRARIES cosma::cosma INTERFACE_LINK_LIBRARIES) # check that cosma::cosma_pxgemm_cpp and cosma::cosma_prefixed_pxgemm exist if(NOT TARGET cosma::cosma_pxgemm_cpp OR NOT TARGET cosma::cosma_prefixed_pxgemm) message( FATAL_ERROR \" COSMA needs to be build with scalapack offloading support. COSTA_SCALAPACK and COSMA_SCALAPACK should probably be set properly\" ) endif() endif() if(CP2K_USE_VORI) find_package(LibVORI REQUIRED) endif() if(CP2K_USE_DLAF) find_package(DLAFFortran REQUIRED) get_target_property(CP2K_DLAF_INCLUDE_DIRS DLAF::DLAF INTERFACE_INCLUDE_DIRECTORIES) get_target_property(CP2K_DLAF_LINK_LIBRARIES DLAF::dlaf.prop INTERFACE_LINK_LIBRARIES) message(\"${CP2K_DLAF_INCLUDE_DIRS} ${CP2K_DLAF_LINK_LIBRARIES}\") endif() # FFTW3 # we set this variable to ON when we want fftw3 support (with or without MKL). set(CP2K_USE_FFTW3_ OFF) if(CP2K_USE_FFTW3) if(NOT CP2K_BLAS_VENDOR MATCHES \"MKL\" OR CP2K_USE_FFTW3_WITH_MKL) find_package(Fftw REQUIRED) if(CP2K_ENABLE_FFTW3_THREADS_SUPPORT AND CP2K_ENABLE_FFTW3_OPENMP_SUPPORT) message( FATAL_ERROR \"Fftw3 threads and openmp supports can not be used at the same time\") endif() if((CP2K_ENABLE_FFTW3_THREADS_SUPPORT) AND (NOT TARGET cp2k::FFTW3::fftw3_threads)) message( FATAL_ERROR \"fftw3 was compiled without multithreading support (--enable-threads option in the fftw build system).\" ) endif() if((CP2K_ENABLE_FFTW3_OPENMP_SUPPORT) AND (NOT TARGET cp2k::FFTW3::fftw3_omp )) message( FATAL_ERROR \"fftw3 was compiled without openmp support (--enable-openmp option in the fftw build system).\" ) endif() set(CP2K_USE_FFTW3_ ON) else() message(\"-- Using the MKL implementation of FFTW3.\") set(CP2K_USE_FFTW3_MKL_ ON) endif() endif() # libint if(CP2K_USE_LIBINT2) find_package(Libint2 REQUIRED) endif() # spglib if(CP2K_USE_SPGLIB) find_package(Spglib CONFIG REQUIRED) endif() if(CP2K_USE_LIBSMEAGOL) find_package(libsmeagol REQUIRED) endif() if(CP2K_USE_SPLA) find_package(SPLA REQUIRED) get_target_property(SPLA_INCLUDE_DIRS SPLA::spla INTERFACE_INCLUDE_DIRECTORIES) if(NOT SPLA_INCLUDE_DIRS) set(SPLA_INCLUDE_DIRS \"/usr/include;/usr/include/spla\") endif() if(NOT SPLA_GPU_BACKEND AND CP2K_USE_GEMM_OFFLOADING) set(CP2K_USE_GEMM_OFFLOADING OFF) message( FATAL_ERROR \"SPLA should be compiled with GPU support if the gemm offloading is requested. Use -DCP2K_USE_GEMM_OFFLOADING=OFF otherwise\" ) endif() endif() if(CP2K_USE_DFTD4) find_package(dftd4 REQUIRED) get_target_property(CP2K_DFTD4_LINK_LIBRARIES dftd4::dftd4 INTERFACE_LINK_LIBRARIES) get_target_property(CP2K_DFTD4_LIB_LINK_LIBRARIES dftd4::dftd4-lib INTERFACE_LINK_LIBRARIES) get_target_property(CP2K_DFTD4_INCLUDE_DIR dftd4::dftd4-lib INTERFACE_INCLUDE_DIRECTORIES) endif() if(CP2K_USE_DEEPMD) find_package(DeePMD REQUIRED CONFIG) endif() # SIRIUS if(CP2K_USE_SIRIUS) find_package(sirius 7.7.0 REQUIRED) endif() if(CP2K_USE_PLUMED) find_package(Plumed REQUIRED) endif() if(CP2K_USE_LIBTORCH) find_package(Torch REQUIRED) get_target_property(CP2K_TORCH_CXX_STANDARD torch CXX_STANDARD) if(${CP2K_TORCH_CXX_STANDARD} VERSION_GREATER ${CMAKE_CXX_STANDARD}) set(CMAKE_CXX_STANDARD ${CP2K_TORCH_CXX_STANDARD}) message(\"-- Torch setting CMAKE_CXX_STANDARD to ${CP2K_TORCH_CXX_STANDARD}\") endif() endif() if(CP2K_USE_MPI_F08 AND NOT MPI_Fortran_HAVE_F08_MODULE) message( FATAL_ERROR \"The Fortran 2008 interface is not supported by the MPI implementation found by cmake.\" ) endif() if(CP2K_USE_TREXIO) find_package(TrexIO REQUIRED) endif() if(CP2K_USE_GREENX) find_package(greenX REQUIRED CONFIG) get_target_property(CP2K_GREENX_INCLUDE_DIRS greenX::GXCommon INTERFACE_INCLUDE_DIRECTORIES) get_target_property(CP2K_GREENX_LINK_LIBRARIES greenX::GXCommon LOCATION) get_target_property(LIB_GXMiniMax greenX::LibGXMiniMax LOCATION) list(APPEND CP2K_GREENX_LINK_LIBRARIES ${LIB_GXMiniMax}) get_target_property(LIB_GXAC greenX::LibGXAC LOCATION) list(APPEND CP2K_GREENX_LINK_LIBRARIES ${LIB_GXAC}) endif() # OPTION HANDLING # make sure that the default build type is RELEASE set(default_build_type \"Release\") if(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES) message( STATUS \"Setting build type to '${default_build_type}' as none was specified.\") set(CMAKE_BUILD_TYPE \"${default_build_type}\" CACHE STRING \"Choose the type of build, options are: Debug Release Coverage.\" FORCE) # set the possible values of build type for cmake-gui set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"Coverage\") endif() # compiler configuration could have impacted package discovery (above) include(CompilerConfiguration) include(CheckCompilerSupport) include(GNUInstallDirs) # subdirectories add_subdirectory(src) get_target_property(CP2K_LIBS cp2k_link_libs INTERFACE_LINK_LIBRARIES) configure_file(cmake/libcp2k.pc.in libcp2k.pc @ONLY) message( \"\" \"--------------------------------------------------------------------\\n\" \"- -\\n\" \"- Summary of enabled dependencies -\\n\" \"- -\\n\" \"--------------------------------------------------------------------\\n\\n\") message( \" - BLAS AND LAPACK\\n\" \" - vendor: ${CP2K_BLAS_VENDOR}\\n\" \" - include directories: ${CP2K_BLAS_INCLUDE_DIR} ${LAPACK_INCLUDE_DIR}\\n\" \" - libraries: ${CP2K_BLAS_LINK_LIBRARIES} ${CP2K_LAPACK_LINK_LIBRARIES}\\n\\n\" ) if(CP2K_USE_MPI) message(\" - MPI\\n\" # let below line separate \" - include directories: ${MPI_INCLUDE_DIRS}\\n\" \" - libraries: ${MPI_LIBRARIES}\\n\\n\") if(CP2K_USE_MPI_F08) message(\" - MPI_08: ON\\n\") endif() if(MPI_Fortran_HAVE_F08_MODULE AND NOT CP2K_USE_MPI_F08) message( \" - MPI_08 is supposed by MPI but turned off by default.\\n\" \" To use it add -DCP2K_USE_MPI_F08=ON to the cmake command line\\n\\n\") endif() message(\" - SCALAPACK:\\n\" \" - libraries: ${CP2K_SCALAPACK_LINK_LIBRARIES}\\n\\n\") endif() if((CP2K_USE_ACCEL MATCHES \"CUDA\") OR (CP2K_USE_ACCEL MATCHES \"HIP\")) message(\" - Hardware Acceleration:\\n\") if(CP2K_USE_ACCEL MATCHES \"CUDA\") message(\" - CUDA:\\n\" # let below line separate \" - GPU architecture number: ${CMAKE_CUDA_ARCHITECTURES}\\n\" \" - GPU profiling enabled: ${CP2K_WITH_CUDA_PROFILING}\\n\\n\") endif() if(CP2K_USE_ACCEL MATCHES \"HIP\") message( \" - HIP:\\n\" # let below line separate \" - GPU target architecture: ${CP2K_WITH_GPU}\\n\" \" - GPU architecture number: ${CP2K_ACC_ARCH_NUMBER}\\n\" \" - FLAGS: ${CMAKE_HIP_FLAGS}\") endif() message( \" - GPU accelerated modules\\n\" \" - PW module: ${CP2K_ENABLE_PW_GPU}\\n\" \" - GRID module: ${CP2K_ENABLE_GRID_GPU}\\n\" \" - DBM module: ${CP2K_ENABLE_DBM_GPU}\\n\\n\") endif() if(CP2K_USE_CUSOLVER_MP) message( \" - CUSolverMP: \\n\" \" - library: ${CP2K_CUSOLVER_MP_LINK_LIBRARIES} \\n\" \" - include: ${CP2K_CUSOLVER_MP_INCLUDE_DIRS} \\n\" \" - CAL library: ${CP2K_CAL_LINK_LIBRARIES} \\n\" \" - CAL include: ${CP2K_CAL_INCLUDE_DIRS} \\n\" \" - ucc library: ${CP2K_UCC_LINK_LIBRARIES} \\n\" \" - ucx library: ${CP2K_UCX_LINK_LIBRARIES} \\n\" \" - ucc include: ${CP2K_UCC_INCLUDE_DIRS} \\n\") endif() if(CP2K_USE_LIBXC) message( \" - LibXC\\n\" # let below line separate \" - version: ${Libxc_VERSION}\\n\" \" - include directories: ${Libxc_INCLUDE_DIRS}\\n\" \" - libraries: ${Libxc_LIBRARIES}\\n\\n\") endif() if(CP2K_USE_LIBTORCH) message(\" - LIBTORCH\\n\" \" - extra CXX flags: ${TORCH_CXX_FLAGS}\\n\" \" - include directories: ${TORCH_INCLUDE_DIRS}\\n\" \" - libraries: ${TORCH_LIBRARY}\\n\") endif() if(CP2K_USE_HDF5) message( \" - HDF5\\n\" # let below line separate \" - version: ${HDF5_VERSION}\\n\" \" - include directories: ${HDF5_INCLUDE_DIRS}\\n\" \" - libraries: ${HDF5_LIBRARIES}\\n\\n\") endif() if(CP2K_USE_FFTW3) message(\" - FFTW3\\n\" \" - include directories: ${CP2K_FFTW3_INCLUDE_DIRS}\\n\" \" - libraries: ${CP2K_FFTW3_LINK_LIBRARIES}\\n\\n\") endif() if(CP2K_USE_PLUMED) message(\" - PLUMED\\n\" \" - include directories: ${CP2K_PLUMED_INCLUDE_DIRS}\\n\" \" - libraries: ${CP2K_PLUMED_LINK_LIBRARIES}\\n\\n\") endif() if(CP2K_USE_LIBXSMM) message( \" - LIBXSMM\\n\" \" - include directories: ${CP2K_LIBXSMM_INCLUDE_DIRS}\\n\" \" - libraries: ${CP2K_LIBXSMMEXT_LINK_LIBRARIES};${CP2K_LIBXSMMF_LINK_LIBRARIES}\\n\\n\" ) endif() if(CP2K_USE_SPLA) message(\" - SPLA\\n\" # let below line separate \" - include directories: ${SPLA_INCLUDE_DIRS}\\n\" \" - libraries: ${SPLA_LIBRARIES}\\n\\n\") endif() if(CP2K_USE_DFTD4) message( \" - DFTD4\\n\" \" - include directories: ${CP2K_DFTD4_INCLUDE_DIR}\\n\" \" - libraries: ${CP2K_DFTD4_LINK_LIBRARIES};${CP2K_DFTD4_LIB_LINK_LIBRARIES}\\n\\n\" ) endif() if(CP2K_USE_DEEPMD) message(\" - DeePMD\\n\\n\") endif() if(CP2K_USE_LIBSMEAGOL) message(\" - LIBSMEAGOL\\n\" \" - include directories: ${CP2K_LIBSMEAGOL_INCLUDE_DIRS}\\n\" \" - libraries: ${CP2K_LIBSMEAGOL_LINK_LIBRARIES}\\n\\n\") endif() if(CP2K_USE_SIRIUS) message(\" - SIRIUS\") endif() if(CP2K_USE_COSMA) message(\" - COSMA\\n\" # let below line separate \" - include directories: ${CP2K_COSMA_INCLUDE_DIRS}\\n\" \" - libraries: ${CP2K_COSMA_LINK_LIBRARIES}\\n\\n\") endif() if(CP2K_USE_LIBINT2) message(\" - libint2\\n\" \" - include directories: ${CP2K_LIBINT2_INCLUDE_DIRS}\\n\" \" - libraries: ${CP2K_LIBINT2_LINK_LIBRARIES}\\n\\n\") endif() if(CP2K_USE_VORI) message(\" - libvori\\n\" \" - include directories: ${CP2K_LIBVORI_INCLUDE_DIRS}\\n\" \" - libraries: ${CP2K_LIBVORI_LINK_LIBRARIES}\\n\\n\") endif() if(CP2K_USE_ELPA) message(\" - ELPA\\n\" # let below line separate \" - include directories: ${CP2K_ELPA_INCLUDE_DIRS}\\n\" \" - libraries: ${CP2K_ELPA_LINK_LIBRARIES}\\n\\n\") endif() if(CP2K_USE_DLAF) message(\" - DLA-Future\\n\" \" - include directories: ${CP2K_DLAF_INCLUDE_DIRS}\\n\" \" - libraries: ${CP2K_DLAF_LINK_LIBRARIES}\\n\\n\") endif() if(CP2K_USE_GRPP) message(\" - grpp\\n\") endif() if(CP2K_USE_TREXIO) message(\" - trexio\\n\" \" - include directories: ${CP2K_TREXIO_INCLUDE_DIRS}\\n\" \" - libraries: ${CP2K_TREXIO_LINK_LIBRARIES}\\n\\n\") endif() if(CP2K_USE_GREENX) message(\" - GreenX\\n\" \" - include directories: ${CP2K_GREENX_INCLUDE_DIRS}\\n\" \" - libraries: ${CP2K_GREENX_LINK_LIBRARIES}\\n\\n\") endif() message( \"--------------------------------------------------------------------\\n\" \"- -\\n\" \"- List of dependencies not included in this build -\\n\" \"- -\\n\" \"--------------------------------------------------------------------\\n\") if(NOT CP2K_USE_MPI) message(\" - MPI\") endif() if(NOT CP2K_USE_DFTD4) message(\" - DFTD4\") endif() if(NOT CP2K_USE_DEEPMD) message(\" - DeePMD\") endif() if(NOT CP2K_USE_SIRIUS) message(\" - SIRIUS\") endif() if(NOT CP2K_USE_SPGLIB) message(\" - SPGLIB\") endif() if(NOT CP2K_USE_LIBSMEAGOL) message(\" - libSMEAGOL\") endif() if(NOT CP2K_USE_COSMA) message(\" - COSMA\") endif() if(NOT CP2K_USE_SPLA) message(\" - SPLA\") endif() if(NOT CP2K_USE_HDF5) message(\" - HDF5\") endif() if(${CP2K_USE_ACCEL} MATCHES \"NONE\") message(\" - GPU acceleration is disabled\") endif() if(NOT CP2K_USE_ELPA) message(\" - ELPA\") endif() if(NOT CP2K_USE_DLAF) message(\" - DLA-Future\") endif() if(NOT CP2K_USE_PLUMED) message(\" - PLUMED\") endif() if(NOT CP2K_USE_LIBXSMM) message(\" - LIBXSMM\") endif() if(NOT CP2K_USE_LIBINT2) message(\" - LIBINT2\") endif() if(NOT CP2K_USE_LIBXC) message(\" - LIBXC\") endif() if(NOT CP2K_USE_VORI) message(\" - LIBVORI\") endif() if(NOT CP2K_USE_FFTW3) message(\" - FFTW3\") endif() if(NOT CP2K_USE_LIBTORCH) message(\" - libtorch\") endif() if(NOT CP2K_USE_TREXIO) message(\" - trexio\") endif() if(NOT CP2K_USE_GREENX) message(\" - greenx\") endif() if(NOT CP2K_USE_GRPP) message(\" - grpp\") endif() message( \"\\n\\n\" # let below line separate \"To run the regtests you need to run the following commands\\n\" \"\\n\\n cd ..\\n\" # let below line separate \" export CP2K_DATA_DIR=${CMAKE_SOURCE_DIR}/data/\\n\" \" ./tests/do_regtest.py ${cp2k_BINARY_DIR}/bin ${__cp2k_ext}\\n\\n\") # files needed for cmake write_basic_package_version_file( \"${PROJECT_BINARY_DIR}/cp2kConfigVersion.cmake\" VERSION \"${CP2K_VERSION}\" COMPATIBILITY SameMajorVersion) configure_file(\"${PROJECT_SOURCE_DIR}/cmake/cp2kConfig.cmake.in\" \"${PROJECT_BINARY_DIR}/cp2kConfig.cmake\" @ONLY) install(FILES \"${PROJECT_BINARY_DIR}/cp2kConfig.cmake\" \"${PROJECT_BINARY_DIR}/cp2kConfigVersion.cmake\" DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/cp2k\") install(FILES \"${PROJECT_BINARY_DIR}/libcp2k.pc\" DESTINATION \"${CMAKE_INSTALL_LIBDIR}/pkgconfig\") install( DIRECTORY \"${PROJECT_SOURCE_DIR}/cmake/modules\" DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/cp2k\" FILES_MATCHING PATTERN \"*.cmake\")\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cryogrid",
            "repo_link": "https://github.com/CryoGrid/CryoGridCommunity_source",
            "content": {
                "codemeta": "",
                "readme": "# CryoGrid community model This is the community version of *CryoGrid*, a numerical model to investigate land surface processes in the terrestrial cryosphere. This version of *CryoGrid* is implemented in MATLAB. *Note: This is the latest development of the CryoGrid model family. It comprises the functionalities of previous versions including [CryoGrid3](https://github.com/CryoGrid/CryoGrid3), which is no longer encouraged to be used.* ## Documentation The paper [\"The CryoGrid community model - a multi-physics toolbox for climate-driven simulations in the terrestrial cryosphere\"](https://doi.org/10.5194/gmd-16-2607-2023) in published in Geoscientific Model Development and contains a description of the model and instructions to run it (Supplements 1, 3). ## Getting started Both [CryoGridCommunity_source](https://github.com/CryoGrid/CryoGridCommunity_source) and [CryoGridCommunity_run](https://github.com/CryoGrid/CryoGridCommunity_run) are required. See [CryoGridCommunity_run](https://github.com/CryoGrid/CryoGridCommunity_run) for details. An instruction video on downloading the CryoGrid community model and running simple simulations is available here: https://www.youtube.com/watch?v=L1GIurc5_J4&t=372s The parameter files and model forcing data for the simple simulations from the video can be downloaded here: http://files.artek.byg.dtu.dk/files/cryogrid/CryoGridExamples/CryoGrid_simpleExamples.zip ## Get involved There is an [email group](https://groups.google.com/g/cryogrid) for news, (high-level) discussions and invitation to the annual hackathon, as well as a [slack](https://join.slack.com/t/cryogrid/shared_invite/zt-2487oq3o5-ghOQCw2rLimIk13xft27ZQ) for debugging, (low-level) discussions, community building and other things CryoGrid-related!\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/crystfel",
            "repo_link": "https://gitlab.desy.de/thomas.white/crystfel/",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cuas-mpi",
            "repo_link": "https://github.com/tudasc/CUAS-MPI",
            "content": {
                "codemeta": "",
                "readme": "# CUAS-MPI &middot; [![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause) ## What is CUAS-MPI? CUAS-MPI is an MPI parallel implementation of the Confined-Unconfined Aquifer System model ([CUAS18](#ref-CUAS-2018)) for subglacial hydrology. The implementation relies on the parallel data structures and linear system solvers provided by the Portable, Extensible Toolkit for Scientific Computation ([PETSc](https://petsc.org/)). The model uses a one-layer equivalent porous medium (EPM) approach for efficient (channel-like) and inefficient (cavity-like) subglacial water transport. A two-dimensional Darcy-type groundwater flow equation with spatially and temporarily varying hydraulic transmissivity is solved considering confined and unconfined aquifer conditions. ## How to use it? One of the integration tests can be used to generate a simple setup to explore the modelling choices and command line options. The example below needs `ncks` and `ncap2` from the [NCO toolkit](https://nco.sourceforge.net/) to manipulate the NetCDF files. ``` # modifiy according to your installation CUAS_BUILD_DIR=$CUAS_ROOT/CUAS-MPI/cmake-build-debug/ # number of MPI processes NN=4 # # generate simple input file from example integration test # exact=$CUAS_BUILD_DIR/test/cuascore/integration/test-exactSteadySolution mpirun -n $NN $exact 1000.0 101 101 31 86400 out0.nc # convert output to CUAS-MPI input and forcing file format ncks -O -d time,-1 -v topg,bnd_mask,thk out0.nc input.nc ncap2 -O -s \"bmelt=watersource * 365*24*3600\" out0.nc forcing.nc # run a simple experiment # cuas=$CUAS_BUILD_DIR/tools/cuas.exe # set-up the solver TOL=\"-ksp_rtol 1e-7 -ksp_atol 1e-15 -ksp_max_it 10000 -ksp_converged_use_min_initial_residual_norm\" export PETSC_OPTIONS=\"-options_left -ksp_initial_guess_nonzero -pc_type bjacobi -ksp_type gmres $TOL\" # make use of many options for this example mpirun -n $NN $cuas --totaltime '15 days' --dt '1 hour' --saveEvery 1 --verbose --outputSize large \\ --doChannels --Tmax 100 --Tmin 1.e-08 --initialHead Nzero $opts \\ --conductivity 10 --layerThickness 0.1 \\ --flowConstant 3.4e-24 --cavityBeta 5e-4 --basalVelocityIce 1e-6 --supplyMultiplier 1.0 \\ --forcingFile forcing.nc \\ input.nc output.nc ``` ## How to install? ### Requirements - c++ compiler - at least support for c++ 17 - the code is tested using gcc/10.2.0 - MPI - we use OpenMPI - https://www.open-mpi.org/ - version 4.0.x and 4.1.x - PETSc - https://petsc.org/ - we tested version 3.14.6 - with MPI support - netcdf-c - https://www.unidata.ucar.edu/software/netcdf/ - we use version 4.7.4 - with MPI and hdf5 1.8.22 ### Build Starting from the CUAS-MPI directory: ``` cmake -B build -DCMAKE_BUILD_TYPE=Release -DPETSC_DIR=<petsc-root-directory> -DNETCDF_DIR=<netcdf-root-directory> cmake --build build cmake --install build --prefix <prefix> ``` You may want to use legacy cmake calls to generate Makefiles and build CUAS-MPI: ``` mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release -DPETSC_DIR=<petsc-root-directory> -DNETCDF_DIR=<netcdf-root-directory> -DCMAKE_INSTALL_PREFIX=<prefix> make make install ``` CUAS-MPI makes use of the following options: | Option | Default | Description | | --- | :---: |--------------------------------------------------------------------------------------------------------------| | `CUAS_ENABLE_TESTS` | `OFF` | Enables targets building tests. | | `CUAS_ENABLE_DOCS` | `OFF` | Enables targets building documentation. | ## References <table style=\"border:0px\"> <tr> <td valign=\"top\"><a name=\"ref-CUAS-2018\"></a>[CUAS18]</td> <td>Beyer, Sebastian and Kleiner, Thomas and Aizinger, Vadym and Rückamp, Martin and Humbert, Angelika <a href=https://doi.org/10.5194/tc-12-3931-2018> A confined-unconfined aquifer model for subglacial hydrology and its application to the Northeast Greenland Ice Stream</a>. In <i>The Cryosphere</i>, pages 3931-3947, 2018.</td> </tr> <tr> <td valign=\"top\"><a name=\"ref-CUAS-2023\"></a>[CUAS23]</td> <td>Fischler, Yannic and Kleiner, Thomas and Bischof, Christian and Schmiedel, Jeremie and Sayag, Roiy and Emunds, Raban and Oestreich, Lennart Frederik and Humbert, Angelika <a href=https://doi.org/10.5194/gmd-16-5305-2023> A parallel implementation of the confined-unconfined aquifer system model for subglacial hydrology: design, verification, and performance analysis (CUAS-MPI v0.1.0) </a>. In <i>Geoscientific Model Development</i>, pages 5305-5322, 2023.</td> </tr> </table> ## CUAS-MPI Applications - Wolovick, Micheal and Humbert, Angelika and Kleiner, Thomas and Rückamp, Martin [Regularization and L-curves in ice sheet inverse models: a case study in the Filchner-Ronne catchment](https://doi.org/10.5194/tc-17-5027-2023), <i>The Cryosphere</i>, vol. 17, no. 12, pages 5027-5060, 2023.\n",
                "dependencies": "cmake_minimum_required(VERSION 3.16) project( CUAS-MPI VERSION 0.2.0 LANGUAGES CXX) list(APPEND CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake) find_package(PETSc REQUIRED) find_package(NetCDF REQUIRED) set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(CMAKE_EXPORT_COMPILE_COMMANDS ON) include(ToolchainOptions) add_subdirectory(lib) add_subdirectory(tools) # The default options cache option(CUAS_ENABLE_TESTS \"Enables targets building tests.\" OFF) option(CUAS_ENABLE_DOCS \"Enables targets building documentation.\" OFF) if(CUAS_ENABLE_TESTS) message(STATUS \"creating targets to build tests is enabled\") add_subdirectory(test) endif() if(CUAS_ENABLE_DOCS) message(STATUS \"creating targets to build documentation is enabled\") add_subdirectory(docs) endif()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cudamemtest",
            "repo_link": "https://github.com/ComputationalRadiationPhysics/cuda_memtest",
            "content": {
                "codemeta": "",
                "readme": "# cuda_memtest This software tests GPU memory for hardware errors and soft errors using CUDA (or OpenCL). ## Note for this Fork This is a fork of the original, yet long-time unmaintained project at https://sourceforge.net/projects/cudagpumemtest/ . After our fork in 2013 (v1.2.3), we primarily focused on support for newer CUDA versions and support of newer Nvidia hardware. Pull-requests maintaining the OpenCL versions are nevertheless still welcome. ## License Illinois Open Source License University of Illinois/NCSA Open Source License Copyright 2009-2012, University of Illinois. All rights reserved. Copyright 2013-2019, The developers of PIConGPU at Helmholtz-Zentrum Dresden-Rossendorf Developed by: Innovative Systems Lab National Center for Supercomputing Applications http://www.ncsa.uiuc.edu/AboutUs/Directorates/ISL.html Forked and maintained for newer Nvidia GPUs since 2013 by: Axel Huebl and Rene Widera Computational Radiation Physics Group Helmholtz-Zentrum Dresden-Rossendorf https://www.hzdr.de/crp Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution. * Neither the names of the Innovative Systems Lab, the National Center for Supercomputing Applications, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE. ## Compile and Run ### Compile for CUDA Inside the source directory, run: ```bash mkdir build cd build # build for NVIDIA architecture sm_35 cmake -DCMAKE_CUDA_ARCHITECTURES=35 .. make ``` ### Compile for HIP Inside the source directory, run: ```bash mkdir build cd build # build for NVIDIA architecture MI2XX cmake -DCUDA_MEMTEST_BACKEND=hip -DGPU_TARGETS=gfx90a .. make ``` Note: - In CMake, `..` is the path to the source directory. - You can find the architecture for your NVIDIA GPU on [this site](https://developer.nvidia.com/cuda-gpus). We also provide the package `cuda-memtest` in the [Spack package manager](https://spack.io) . ### Run ``` cuda_memtest ``` The default behavior is running the test on all the GPUs available infinitely. There are options to change the default behavior. ``` cuda_memtest --disable_all --enable_test 10 cuda_memtest --stress ``` This runs test 10 (the stress test). `--stress` is equivalent to `--disable_all --enable_test 10 --exit_on_error` ``` cuda_memtest --stress --num_iterations 100 --num_passes 1 ``` This one does a quick sanity check for GPUs with a short run of test 10. More on this later. See help message by ``` cuda_memtest --help ``` ### Sanity Check There is a simple script `sanity_check.sh` in the directory. This script does a quick check if one GPU or all GPUs are in bad health. Example usage: ```bash # copy the cuda_memtest binary first into the same location as this script, e.g. cd .. mv build/cuda_memtest . ``` ``` ./sanity_check.sh 0 //check GPU 0 ./sanity_check.sh 1 //check GPU 1 ./sanity_check.sh //check All GPUs in the system ``` Fork note: We just run the `cuda_memtest` binary directly. Consider this script as a source for inspiration, or so. ### Known Issues * Even if you compile with AMD HIP the tool binary will be named `cuda_memtest`. * If you run on AMD GPUs via HIP the tool will mention everywhere CUDA instead of HIP. * We are **not** maintaining the OpenCL version of this code base. Pull requests restoring and updating the OpenCL capabilities are welcome. ## Test Descriptions ### List of all Tests Running ``` cuda_memtest --list_tests ``` will print out all tests and their short descriptions, as of 6/18/2009, we implemented 11 tests ``` Test0 [Walking 1 bit] Test1 [Own address test] Test2 [Moving inversions, ones&zeros] Test3 [Moving inversions, 8 bit pat] Test4 [Moving inversions, random pattern] Test5 [Block move, 64 moves] Test6 [Moving inversions, 32 bit pat] Test7 [Random number sequence] Test8 [Modulo 20, random pattern] Test9 [Bit fade test] ==disabled by default== Test10 [Memory stress test] ``` ### The General Algorithm First a kernel is launched to write a pattern. Then we exit the kernel so that the memory can be flushed. Then we start a new kernel to read and check if the value matches the pattern. An error is recorded if it does not match for each memory location. In the same kernel, the complement of the pattern is written after the checking. The third kernel is launched to read the value again and checks against the complement of the pattern. ### Detailed Description Test 0 `[Walking 1 bit]` This test changes one bit a time in memory address to see it goes to a different memory location. It is designed to test the address wires. Test 1 `[Own address test]` Each Memory location is filled with its own address. The next kernel checks if the value in each memory location still agrees with the address. Test 2 `[Moving inversions, ones&zeros]` This test uses the moving inversions algorithm with patterns of all ones and zeros. Test 3 `[Moving inversions, 8 bit pat]` This is the same as test 1 but uses a 8 bit wide pattern of \"walking\" ones and zeros. This test will better detect subtle errors in \"wide\" memory chips. Test 4 `[Moving inversions, random pattern]` Test 4 uses the same algorithm as test 1 but the data pattern is a random number and it's complement. This test is particularly effective in finding difficult to detect data sensitive errors. The random number sequence is different with each pass so multiple passes increase effectiveness. Test 5 `[Block move, 64 moves]` This test stresses memory by moving block memories. Memory is initialized with shifting patterns that are inverted every 8 bytes. Then blocks of memory are moved around. After the moves are completed the data patterns are checked. Because the data is checked only after the memory moves are completed it is not possible to know where the error occurred. The addresses reported are only for where the bad pattern was found. Test 6 `[Moving inversions, 32 bit pat]` This is a variation of the moving inversions algorithm that shifts the data pattern left one bit for each successive address. The starting bit position is shifted left for each pass. To use all possible data patterns 32 passes are required. This test is quite effective at detecting data sensitive errors but the execution time is long. Test 7 `[Random number sequence]` This test writes a series of random numbers into memory. A block (1 MB) of memory is initialized with random patterns. These patterns and their complements are used in moving inversions test with rest of memory. Test 8 `[Modulo 20, random pattern]` A random pattern is generated. This pattern is used to set every 20th memory location in memory. The rest of the memory location is set to the complement of the pattern. Repeat this for 20 times and each time the memory location to set the pattern is shifted right. Test 9 `[Bit fade test, 90 min, 2 patterns]` The bit fade test initializes all of memory with a pattern and then sleeps for 90 minutes. Then memory is examined to see if any memory bits have changed. All ones and all zero patterns are used. This test takes 3 hours to complete. The Bit Fade test is disabled by default Test 10 `[memory stress test]` Stress memory as much as we can. A random pattern is generated and a kernel of large grid size and block size is launched to set all memory to the pattern. A new read and write kernel is launched immediately after the previous write kernel to check if there is any errors in memory and set the memory to the complement. This process is repeated for 1000 times for one pattern. The kernel is written as to achieve the maximum bandwidth between the global memory and GPU. This will increase the chance of catching software error. In practice, we found this test quite useful to flush hardware errors as well.\n",
                "dependencies": "################################################################################ # Required cmake version ################################################################################ cmake_minimum_required(VERSION 3.16.0) ################################################################################ # Project ################################################################################ include(CheckLanguage) # check for CUDA/HIP language support project(CUDA_memtest LANGUAGES CXX) if(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT) set(CMAKE_INSTALL_PREFIX \"${CMAKE_BINARY_DIR}\" CACHE PATH \"install prefix\" FORCE) endif(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT) # own modules for find_packages set(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ${CMAKE_CURRENT_SOURCE_DIR}/cmake/) set(CUDA_MEMTEST_BACKEND \"cuda\" CACHE STRING \"Select the backend API used for the test.\") set_property(CACHE CUDA_MEMTEST_BACKEND PROPERTY STRINGS \"cuda;hip\") option(CUDA_MEMTEST_ADD_RPATH \"Add RPATH's to binaries.\" ON) ################################################################################ # CMake policies # # Search in <PackageName>_ROOT: # https://cmake.org/cmake/help/v3.12/policy/CMP0074.html ################################################################################ if(POLICY CMP0074) cmake_policy(SET CMP0074 NEW) endif() if(CUDA_MEMTEST_BACKEND STREQUAL \"cuda\") ################################################################################ # Find CUDA ################################################################################ enable_language(CUDA) # cuda toolkit contains CUDA::nvml find_package(CUDAToolkit) if(SAME_NVCC_FLAGS_IN_SUBPROJECTS) if(CUDA_SHOW_CODELINES) target_compile_options(cuda_memtest PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:--source-in-ptx -lineinfo>) set(CUDA_KEEP_FILES ON CACHE BOOL \"activate keep files\" FORCE) endif(CUDA_SHOW_CODELINES) if(CUDA_SHOW_REGISTER) target_compile_options(cuda_memtest PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:-Xcuda-ptxas=-v>) endif() if(CUDA_KEEP_FILES) target_compile_options(cuda_memtest PRIVATE $<$<COMPILE_LANGUAGE:CUDA>:--keep>) endif() endif() ################################################################################ # Find NVML ################################################################################ set(GPU_DEPLOYMENT_KIT_ROOT_DIR \"$ENV{GDK_ROOT}\") find_package(NVML) else() ################################################################################ # Find HIP ################################################################################ # supported HIP version range check_language(HIP) if(CMAKE_HIP_COMPILER) enable_language(HIP) find_package(hip REQUIRED) set(CUDA_MEMTEST_HIP_MIN_VER 5.2) set(CUDA_MEMTEST_HIP_MAX_VER 6.1) # construct hip version only with major and minor level # cannot use hip_VERSION because of the patch level # 6.0 is smaller than 6.0.1234, so CUDA_MEMTEST_HIP_MAX_VER would have to be defined with a large patch level or # the next minor level, e.g. 6.1, would have to be used. set(_hip_MAJOR_MINOR_VERSION \"${hip_VERSION_MAJOR}.${hip_VERSION_MINOR}\") if(${_hip_MAJOR_MINOR_VERSION} VERSION_LESS ${CUDA_MEMTEST_HIP_MIN_VER} OR ${_hip_MAJOR_MINOR_VERSION} VERSION_GREATER ${CUDA_MEMTEST_HIP_MAX_VER}) message(WARNING \"HIP ${_hip_MAJOR_MINOR_VERSION} is not official supported by cuda_memtest. Supported versions: ${CUDA_MEMTEST_HIP_MIN_VER} - ${CUDA_MEMTEST_HIP_MAX_VER}\") endif() else() message(FATAL_ERROR \"Optional alpaka dependency HIP could not be found!\") endif() endif() ################################################################################ # Find PThreads ################################################################################ if(NOT MSVC) set(THREADS_PREFER_PTHREAD_FLAG TRUE) endif() find_package(Threads REQUIRED) ################################################################################ # Warnings ################################################################################ set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall\") ################################################################################ # Compile & Link cuda_memtest ################################################################################ if(CUDA_MEMTEST_BACKEND STREQUAL \"cuda\") set_source_files_properties(tests.cpp cuda_memtest.cpp misc.cpp PROPERTIES LANGUAGE CUDA) endif() add_executable(cuda_memtest tests.cpp misc.cpp cuda_memtest.cpp ) if(CUDA_MEMTEST_BACKEND STREQUAL \"cuda\") target_link_libraries(cuda_memtest INTERFACE CUDA::cudart) target_link_libraries(cuda_memtest INTERFACE CUDA::cuda_driver) if(TARGET CUDA::nvml) message(STATUS \"nvml found\") target_link_libraries(cuda_memtest PRIVATE CUDA::nvml) target_compile_definitions(cuda_memtest PRIVATE \"ENABLE_NVML=1\") else() target_compile_definitions(cuda_memtest PRIVATE \"ENABLE_NVML=0\") endif() if(NOT CUDA_MEMTEST_RELEASE) set(CMAKE_BUILD_TYPE Debug) endif() else() target_link_libraries(cuda_memtest PRIVATE hip::host) target_link_libraries(cuda_memtest PRIVATE hip::device) target_compile_definitions(cuda_memtest PRIVATE \"$<$<COMPILE_LANGUAGE:CXX>:__HIP_PLATFORM_AMD__>\") target_compile_definitions(cuda_memtest PRIVATE \"ENABLE_NVML=0\") endif() if(NOT MSVC) target_link_libraries(cuda_memtest PRIVATE Threads::Threads) endif() ## annotate with RPATH's if(CUDA_MEMTEST_ADD_RPATH) if(NOT DEFINED CMAKE_INSTALL_RPATH) if(APPLE) set_target_properties(cuda_memtest PROPERTIES INSTALL_RPATH \"@loader_path\") elseif(CMAKE_SYSTEM_NAME MATCHES \"Linux\") set_target_properties(cuda_memtest PROPERTIES INSTALL_RPATH \"$ORIGIN\") endif() endif() if(NOT DEFINED CMAKE_INSTALL_RPATH_USE_LINK_PATH) set_target_properties(cuda_memtest PROPERTIES INSTALL_RPATH_USE_LINK_PATH ON) endif() endif() ################################################################################ # Build type (debug, release) ################################################################################ option(CUDA_MEMTEST_RELEASE \"disable all runtime asserts\" ON) if(CUDA_MEMTEST_RELEASE) target_compile_definitions(cuda_memtest PRIVATE NDEBUG) endif(CUDA_MEMTEST_RELEASE) ################################################################################ # Install cuda_memtest ################################################################################ install(TARGETS cuda_memtest RUNTIME DESTINATION bin)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/cupla",
            "repo_link": "https://github.com/alpaka-group/cupla",
            "content": {
                "codemeta": "",
                "readme": "**cupla** - C++ User interface for the Platform Independent Library alpaka ========================================================================== [![Code Status dev](https://gitlab.com/hzdr/crp/cupla/badges/dev/pipeline.svg?key_text=dev)](https://gitlab.com/hzdr/crp/cupla/pipelines/dev/latest) ![cupla Release](doc/logo/cupla_logo_320x210.png) **cupla** [[qχɑpˈlɑʔ]](https://en.wiktionary.org/wiki/Qapla%27) is a simple user interface for the platform independent parallel kernel acceleration library [**alpaka**](https://github.com/alpaka-group/alpaka). It follows a similar concept as the [NVIDIA® CUDA® API](https://developer.nvidia.com/cuda-zone) by providing a software layer to manage accelerator devices. **alpaka** is used as backend for **cupla**. Please keep in mind that a first, [\"find & replace\"](doc/PortingGuide.md) port from **CUDA to cupla(x86)** will result in rather bad performance. In order to reach decent performance on x86 systems you just need to add the **alpaka** [element level](doc/TuningGuide.md) to your kernels. (*Read as:* add some *tiling* to your CUDA kernels by letting the same thread compute a fixed number of elements (N=4..16) instead of just computing one *element* per thread. Also, make the number of elements in your tiling a *compile-time constant* and your CUDA code (N=1) will just stay with the very same performance while adding single-source performance portability for, e.g., x86 targets). Software License ---------------- **cupla** is licensed under **LGPLv3** or later. For more information see [LICENSE.md](LICENSE.md). Dependencies ------------ - **cmake 3.22.0** or higher (depends on the used alpaka version) - **[alpaka 1.0.0](eba6db5d8efc3c2585470085e76ba3dcab510e49)** or newer - alpaka is loaded as `git subtree` within **cupla**, see [INSTALL.md](INSTALL.md) Usage ----- - See our notes in [INSTALL.md](INSTALL.md). - Checkout the [guide](doc/PortingGuide.md) how to port your project. - Checkout the [tuning guide](doc/TuningGuide.md) for a step further to performance portable code. - Checkout the [interoperability guide](doc/InteroperabilityGuide.md) to learn more on how to use **cupla** with software developed with an **alpaka** compatible interface. [cupla can be used as a header-only library and without the CMake build system](doc/ConfigurationHeader.md) Contributing ------------ Any pull request will be reviewed by a [maintainer](https://github.com/orgs/alpaka-group/teams/alpaka-maintainers). Thanks to all [active and former contributors](.rodare.json). Trademarks Disclaimer --------------------- All product names and trademarks are the property of their respective owners. CUDA® is a trademark of the NVIDIA Corporation.\n",
                "dependencies": "# # Copyright 2016-2021 Rene Widera, Benjamin Worpitz, Simeon Ehrig # # This file is part of cupla. # # cupla is free software: you can redistribute it and/or modify # it under the terms of the GNU Lesser General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # cupla is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU Lesser General Public License for more details. # # You should have received a copy of the GNU Lesser General Public License # along with cupla. # If not, see <http://www.gnu.org/licenses/>. # ################################################################################ # Required cmake version. ################################################################################ cmake_minimum_required(VERSION 3.22.0) ################################################################################ # Setup project information ################################################################################ # Find cupla version. file(STRINGS \"${CMAKE_CURRENT_LIST_DIR}/include/cupla/version.hpp\" CUPLA_VERSION_MAJOR_HPP REGEX \"#define CUPLA_VERSION_MAJOR \") file(STRINGS \"${CMAKE_CURRENT_LIST_DIR}/include/cupla/version.hpp\" CUPLA_VERSION_MINOR_HPP REGEX \"#define CUPLA_VERSION_MINOR \") file(STRINGS \"${CMAKE_CURRENT_LIST_DIR}/include/cupla/version.hpp\" CUPLA_VERSION_PATCH_HPP REGEX \"#define CUPLA_VERSION_PATCH \") string(REGEX MATCH \"([0-9]+)\" CUPLA_VERSION_MAJOR ${CUPLA_VERSION_MAJOR_HPP}) string(REGEX MATCH \"([0-9]+)\" CUPLA_VERSION_MINOR ${CUPLA_VERSION_MINOR_HPP}) string(REGEX MATCH \"([0-9]+)\" CUPLA_VERSION_PATCH ${CUPLA_VERSION_PATCH_HPP}) set(PACKAGE_VERSION \"${CUPLA_VERSION_MAJOR}.${CUPLA_VERSION_MINOR}.${CUPLA_VERSION_PATCH}\") project(cupla VERSION ${CUPLA_VERSION_MAJOR}.${CUPLA_VERSION_MINOR}.${CUPLA_VERSION_PATCH} DESCRIPTION \"cupla is a simple CUDA like user interface for the platform independent parallel kernel acceleration library alpaka.\" HOMEPAGE_URL \"https://github.com/alpaka-group/cupla\" LANGUAGES CXX) include(GNUInstallDirs) ################################################################################ # cupla options ################################################################################ option(CUPLA_STREAM_ASYNC_ENABLE \"Enable asynchronous streams\" ON) option(cupla_BUILD_EXAMPLES \"Build examples\" OFF) ################################################################################ # setup alpaka ################################################################################ # the min and max. supported alpaka version is also copied to the cuplaConfig.cmake set(_CUPLA_MIN_ALPAKA_VERSION 1.0.0) set(_CUPLA_MAX_ALPAKA_VERSION 1.1.0) # do not search for alpaka if it already exists # for example, a project that includes alpaka via add_subdirectory before including cupla via add_subdirectory if(NOT TARGET alpaka::alpaka) # the alpaka provider for the internal alpaka is only available, # if cupla is used via add_subdirectory in another project # or examples are build if(cupla_BUILD_EXAMPLES OR (NOT ${CMAKE_PROJECT_NAME} STREQUAL ${PROJECT_NAME})) set(cupla_ALPAKA_PROVIDER \"internal\" CACHE STRING \"Select which alpaka is used\") set_property(CACHE cupla_ALPAKA_PROVIDER PROPERTY STRINGS \"internal;external\") mark_as_advanced(cupla_ALPAKA_PROVIDER) if(${cupla_ALPAKA_PROVIDER} STREQUAL \"internal\") set(alpaka_BUILD_EXAMPLES OFF) set(BUILD_TESTING OFF) add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/alpaka) else() find_package(alpaka ${_CUPLA_MAX_ALPAKA_VERSION} HINTS $ENV{ALPAKA_ROOT}) if(NOT TARGET alpaka::alpaka) message(STATUS \"Could not find alpaka ${_CUPLA_MAX_ALPAKA_VERSION}. Now searching for alpaka ${_CUPLA_MIN_ALPAKA_VERSION}\") find_package(alpaka ${_CUPLA_MIN_ALPAKA_VERSION} REQUIRED HINTS $ENV{ALPAKA_ROOT}) endif() if(alpaka_VERSION VERSION_GREATER _CUPLA_MAX_ALPAKA_VERSION) message(WARNING \"Unsupported alpaka version ${alpaka_VERSION}. \" \"Supported versions [${_CUPLA_MIN_ALPAKA_VERSION},${_CUPLA_MAX_ALPAKA_VERSION}].\") endif() endif() if(NOT TARGET alpaka::alpaka) message(FATAL_ERROR \"Required cupla dependency alpaka could not be found!\") endif() endif() endif() ################################################################################ # cupla Target. ################################################################################ # create cupla target only if the cupla is used via add_subdirectory # or examples are build # for the explanation please have a look in the cuplaConfig.cmake.in if(cupla_BUILD_EXAMPLES OR (NOT ${CMAKE_PROJECT_NAME} STREQUAL ${PROJECT_NAME})) include(\"${CMAKE_CURRENT_LIST_DIR}/cmake/addExecutable.cmake\") include(\"${CMAKE_CURRENT_LIST_DIR}/cmake/cuplaTargetHelper.cmake\") # export HIP_HIPCC_FLAGS to the parent scope else the variable is not visible # for application files if(HIP_HIPCC_FLAGS) set(HIP_HIPCC_FLAGS ${HIP_HIPCC_FLAGS} PARENT_SCOPE) endif() createCuplaTarget(${PROJECT_NAME} ${PROJECT_SOURCE_DIR}/include # include directory path ${PROJECT_SOURCE_DIR}/src # src directory path ) endif() ################################################################################ # add examples ################################################################################ if(cupla_BUILD_EXAMPLES) add_subdirectory(example/) endif() ################################################################################ # install cupla ################################################################################ if(${CMAKE_PROJECT_NAME} STREQUAL ${PROJECT_NAME}) include(CMakePackageConfigHelpers) set(_CUPLA_INSTALL_CMAKEDIR \"${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\") set(_CUPLA_SOURCE_CMAKEDIR \"${PROJECT_SOURCE_DIR}/cmake/\") write_basic_package_version_file(\"${PROJECT_NAME}ConfigVersion.cmake\" VERSION ${PROJECT_VERSION} COMPATIBILITY SameMajorVersion) configure_package_config_file( \"${_CUPLA_SOURCE_CMAKEDIR}/${PROJECT_NAME}Config.cmake.in\" \"${PROJECT_BINARY_DIR}/${PROJECT_NAME}Config.cmake\" INSTALL_DESTINATION ${_CUPLA_INSTALL_CMAKEDIR} PATH_VARS _CUPLA_MIN_ALPAKA_VERSION _CUPLA_MAX_ALPAKA_VERSION) install(FILES \"${PROJECT_BINARY_DIR}/${PROJECT_NAME}Config.cmake\" \"${PROJECT_BINARY_DIR}/${PROJECT_NAME}ConfigVersion.cmake\" DESTINATION ${_CUPLA_INSTALL_CMAKEDIR}) install(FILES \"${_CUPLA_SOURCE_CMAKEDIR}/addExecutable.cmake\" \"${_CUPLA_SOURCE_CMAKEDIR}/cuplaTargetHelper.cmake\" DESTINATION ${_CUPLA_INSTALL_CMAKEDIR}) install(DIRECTORY ${PROJECT_SOURCE_DIR}/include/ DESTINATION include) # copy source files instead compiled library # this is necessary because some functions use the ACC as a template parameter, # but the ACC is not defined at the install time of cupla install(DIRECTORY ${PROJECT_SOURCE_DIR}/src/ DESTINATION src/cupla) endif()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/damnit",
            "repo_link": "https://github.com/European-XFEL/DAMNIT",
            "content": {
                "codemeta": "",
                "readme": "# DAMNIT [![Documentation Status](https://readthedocs.org/projects/damnit/badge/?version=latest)](https://damnit.readthedocs.io/en/latest/?badge=latest) [![codecov](https://codecov.io/gh/European-XFEL/DAMNIT/graph/badge.svg?token=NGwo3ShLNw)](https://codecov.io/gh/European-XFEL/DAMNIT) DAMNIT is a tool developed at the European XFEL to help users create an automated overview of their experiment. Check out the documentation for more information: https://damnit.rtfd.io\n",
                "dependencies": "[build-system] requires = [\"flit_core >=3.2,<4\"] build-backend = \"flit_core.buildapi\" [project] name = \"damnit\" authors = [ {name = \"Thomas Kluyver\", email = \"thomas.kluyver@xfel.eu\"}, {name = \"Luca Gelisio\", email = \"luca.gelisio@xfel.eu\"}, ] description = \"The Data And Metadata iNspection Interactive Thing\" license = {file = \"LICENSE\"} requires-python = \">=3.10\" classifiers = [\"License :: OSI Approved :: BSD License\"] dynamic = [\"version\"] readme = \"README.md\" dependencies = [ \"h5netcdf>=1.4.1\", \"h5py\", \"orjson\", # used in plotly for faster json serialization \"pandas\", \"plotly\", \"xarray\" ] [project.optional-dependencies] backend = [ \"EXtra-data\", \"ipython\", \"kafka-python-ng\", \"kaleido\", # used in plotly to convert figures to images \"matplotlib\", \"numpy\", \"pyyaml\", \"requests\", \"supervisor\", \"termcolor\" ] gui = [ \"adeqt\", \"fonticon-fontawesome6\", \"mplcursors\", \"mpl-pan-zoom\", \"natsort\", \"openpyxl\", # for spreadsheet export \"PyQt5\", \"PyQtWebEngine\", \"pyflakes\", # for checking context file in editor \"QScintilla==2.13\", \"scikit-learn\", \"superqt\", \"tabulate\", # used in pandas to make markdown tables (for Zulip) ] test = [ \"pillow\", \"pytest\", \"pytest-qt\", \"pytest-cov\", \"pytest-xvfb\", \"pytest-timeout\", \"pytest-venv\", \"testpath\", ] docs = [ \"mkdocs\", \"mkdocs-material\", \"mkdocstrings\", \"mkdocstrings-python\", \"pymdown-extensions\" ] [project.urls] Home = \"https://github.com/European-XFEL/DAMNIT\" [project.scripts] amore-proto = \"damnit.cli:main\" damnit = \"damnit.cli:main\" [tool.pytest.ini_options] timeout = 120 norecursedirs = \"tests/helpers\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/dasf-messaging-python",
            "repo_link": "https://codebase.helmholtz.cloud/dasf/dasf-messaging-python",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/datadesc",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/DataDesc/",
            "content": {
                "codemeta": "",
                "readme": "<a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://raw.githubusercontent.com/OfficialCodexplosive/README_Assets/862a93188b61ab4dd0eebde3ab5daad636e129d5/FJZ_IEK-3_logo.svg\" alt=\"FZJ Logo\" width=\"300px\"></a> # DataDesc The framework surrounding DataDesc, a metadata schema for software documentation with focus on interfaces, comes with a machine-actionable metadata exchange format and a software toolkit supporting the documentation, extraction and publication of software metadata. ## Features DataDesc combines four distinct python software packages: - a form to convert general software metadata into a DataDesc compliant JSON file - a parser for extracting extensive metadata information from python source code to DataDesc compliant JSON - a tool for combining two DataDesc compliant JSON files into a single one - a tool for uploading DataDesc compliant files to the ORKG, GitHub, PyPI, and more ## Installation The framework can be installed directly via git - this will preserve the connection to the GitHub repository: ```bash git clone https://github.com/FZJ-IEK3-VSA/DataDesc ``` Most parts of this framework are out-of-the-box solutions which do not need to be installed. For others, a proper installation and usage instruction is provided alongside the files themselves. ## License MIT License Copyright (c) 2024 Patrick Kuckertz (FZJ/IEK-3), Jan-Maris Göpfert (FZJ/IEK-3), Oliver Karras (TIB), David Neuroth (FZJ/IEK-3), Julian Schönau (FZJ/IEK-3), Rodrigo Pueblas (FZJ/IEK-3), Stephan Ferenz (University of Oldenburg/ Dept. of Computer Science), Felix Engel (TIB), Noah Pflugradt (FZJ/IEK-3), Jann Michael Weinand (FZJ/IEK-3), Leander Kotzur (FZJ/IEK-3), Astrid Nieße (University of Oldenburg/ Dept. of Computer Science), Sören Auer (TIB), Detlef Stolten (FZJ/IEK-3, RWTH) You should have received a copy of the MIT License along with this program. If not, see https://opensource.org/licenses/MIT ## About Us The [Institute of Energy and Climate Research - Techno-economic Systems Analysis (IEK-3)](https://www.fz-juelich.de/en/iek/iek-3) belongs to the [Forschungszentrum Jülich](https://www.fz-juelich.de/en). The department's interdisciplinary research is focusing on energy-related process and systems analyses. Data searches and system simulations are used to determine energy and mass balances, as well as to evaluate performance, emissions and costs of energy systems. The results are used for performing comparative assessment studies between the various systems. The current priorities include the development of energy strategies, in accordance with the German Federal Government's greenhouse gas reduction targets, by designing new infrastructures for sustainable and secure energy supply chains and by conducting cost analysis studies for integrating new technologies into future energy market frameworks. The [Data Science & Digtal Libraries research group](https://www.tib.eu/en/research-development/research-groups-and-labs/data-science-digital-libraries) belonging to the [TIB - Leibniz Information Centre for Science and Technology](https://www.tib.eu/en/) was established in July 2017 by the call of Prof. Dr. Sören Auer, which was jointly conducted with [Leibniz Universität Hannover](https://www.uni-hannover.de/de/). The research group serves TIB's strategic goal of improving access to and work with information, data and knowledge. The overall objective of the research group is to transform the current document-based knowledge communication in the sciences (Scholarly Communication) into knowledge-based communication (\"from papers to knowledge graphs\"). The Digitalized Energy Systems (DES) group is part of the Department of Computer Science of the University of Oldenburg. The group was established by Prof. Astrid Nieße in 2020. The group works in the field of energy informatics and focuses on distributed artificial intelligence, like agent-based systems, strategy learning in energy markets and machine learning approaches for cyber-physical energy systems (CPES). With the high importance of data and open software in this field, research data management and how to integrate both FAIRness and industry involvement in CPES research, is a rising research area in the DES group. ## Acknowledgements The authors would like to thank the Federal Ministry for Economic Affairs and Energy of Germany (BMWi) for supporting this work with a grant for the project LOD-GEOSS (03EI1005B). Furthermore, the authors would like to thank the German Federal Government, the German State Governments, and the Joint Science Conference (GWK) for their funding and support as part of the NFDI4Ing consortium. Funded by the German Research Foundation (DFG) - project number: 442146713. In addition, the work was supported by the Lower Saxony Ministry of Science and Culture within the Lower Saxony ''Vorab'' of the Volkswagen Foundation under Grant 11-76251-13-3/19-ZN3488 (ZLE), and by the Center for Digital Innovation (ZDIN). This work was also supported by the Helmholtz Association under the program \"Energy System Design\". <a href=\"https://www.bmwk.de/Navigation/EN/Home/home.html\"><img src=\"https://www.bmwk.de/SiteGlobals/BMWI/StyleBundles/Bilder/bmwi_logo_en.svg?__blob=normal&v=13\" alt=\"BMWK Logo\" width=\"130px\"></a>\n",
                "dependencies": "# OpenAPI Extension Generator; General Metadata Merger pyyaml # General Metadata Creator jupyter ipywidgets # ORKG Metadata Uploader orkg\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/datalad",
            "repo_link": "https://github.com/datalad/datalad",
            "content": {
                "codemeta": "",
                "readme": "____ _ _ _ | _ \\ __ _ | |_ __ _ | | __ _ __| | | | | | / _` | | __| / _` | | | / _` | / _` | | |_| | | (_| | | |_ | (_| | | |___ | (_| | | (_| | |____/ \\__,_| \\__| \\__,_| |_____| \\__,_| \\__,_| Read me [![DOI](https://joss.theoj.org/papers/10.21105/joss.03262/status.svg)](https://doi.org/10.21105/joss.03262) [![Test Status](https://github.com/datalad/datalad/actions/workflows/test.yml/badge.svg)](https://github.com/datalad/datalad/actions/workflows/test.yml) [![Build status](https://ci.appveyor.com/api/projects/status/github/datalad/datalad?branch=master&svg=true)](https://ci.appveyor.com/project/mih/datalad/branch/master) [![Extensions](https://github.com/datalad/datalad/actions/workflows/test_extensions.yml/badge.svg)](https://github.com/datalad/datalad/actions/workflows/test_extensions.yml) [![Linters](https://github.com/datalad/datalad/actions/workflows/lint.yml/badge.svg)](https://github.com/datalad/datalad/actions/workflows/lint.yml) [![codecov.io](https://codecov.io/github/datalad/datalad/coverage.svg?branch=master)](https://codecov.io/github/datalad/datalad?branch=master) [![Documentation](https://readthedocs.org/projects/datalad/badge/?version=latest)](http://datalad.rtfd.org) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![GitHub release](https://img.shields.io/github/release/datalad/datalad.svg)](https://GitHub.com/datalad/datalad/releases/) [![Supported Python versions](https://img.shields.io/pypi/pyversions/datalad)](https://pypi.org/project/datalad/) [![Testimonials 4](https://img.shields.io/badge/testimonials-4-brightgreen.svg)](https://github.com/datalad/datalad/wiki/Testimonials) [![https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg](https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg)](https://singularity-hub.org/collections/667) [![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](https://github.com/datalad/datalad/blob/master/CODE_OF_CONDUCT.md) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.808846.svg)](https://doi.org/10.5281/zenodo.808846) [![RRID](https://img.shields.io/badge/RRID-SCR__003931-blue)](https://identifiers.org/RRID:SCR_003931) <!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section --> [![All Contributors](https://img.shields.io/badge/all_contributors-52-orange.svg?style=flat-square)](#contributors-) <!-- ALL-CONTRIBUTORS-BADGE:END --> ## Distribution [![Anaconda](https://anaconda.org/conda-forge/datalad/badges/version.svg)](https://anaconda.org/conda-forge/datalad) [![Arch (AUR)](https://repology.org/badge/version-for-repo/aur/datalad.svg?header=Arch%20%28%41%55%52%29)](https://repology.org/project/datalad/versions) [![Debian Stable](https://badges.debian.net/badges/debian/stable/datalad/version.svg)](https://packages.debian.org/stable/datalad) [![Debian Unstable](https://badges.debian.net/badges/debian/unstable/datalad/version.svg)](https://packages.debian.org/unstable/datalad) [![Fedora Rawhide package](https://repology.org/badge/version-for-repo/fedora_rawhide/datalad.svg?header=Fedora%20%28rawhide%29)](https://repology.org/project/datalad/versions) [![Gentoo (::science)](https://repology.org/badge/version-for-repo/gentoo_ovl_science/datalad.svg?header=Gentoo%20%28%3A%3Ascience%29)](https://repology.org/project/datalad/versions) [![PyPI package](https://repology.org/badge/version-for-repo/pypi/datalad.svg?header=PyPI)](https://repology.org/project/datalad/versions) # 10000-ft. overview DataLad's purpose is to make data management and data distribution more accessible. To do so, it stands on the shoulders of [Git] and [Git-annex] to deliver a decentralized system for data exchange. This includes automated ingestion of data from online portals and exposing it in readily usable form as Git(-annex) repositories - or datasets. However, the actual data storage and permission management remains with the original data provider(s). The full documentation is available at http://docs.datalad.org and http://handbook.datalad.org provides a hands-on crash-course on DataLad. # Extensions A number of extensions are available that provide additional functionality for DataLad. Extensions are separate packages that are to be installed in addition to DataLad. In order to install DataLad customized for a particular domain, one can simply install an extension directly, and DataLad itself will be automatically installed with it. An [annotated list of extensions](http://handbook.datalad.org/extension_pkgs.html) is available in the [DataLad handbook](http://handbook.datalad.org). # Support The documentation for this project is found here: http://docs.datalad.org All bugs, concerns, and enhancement requests for this software can be submitted here: https://github.com/datalad/datalad/issues If you have a problem or would like to ask a question about how to use DataLad, please [submit a question to NeuroStars.org](https://neurostars.org/new-topic?body=-%20Please%20describe%20the%20problem.%0A-%20What%20steps%20will%20reproduce%20the%20problem%3F%0A-%20What%20version%20of%20DataLad%20are%20you%20using%20%28run%20%60datalad%20--version%60%29%3F%20On%20what%20operating%20system%20%28consider%20running%20%60datalad%20plugin%20wtf%60%29%3F%0A-%20Please%20provide%20any%20additional%20information%20below.%0A-%20Have%20you%20had%20any%20luck%20using%20DataLad%20before%3F%20%28Sometimes%20we%20get%20tired%20of%20reading%20bug%20reports%20all%20day%20and%20a%20lil'%20positive%20end%20note%20does%20wonders%29&tags=datalad) with a `datalad` tag. NeuroStars.org is a platform similar to StackOverflow but dedicated to neuroinformatics. All previous DataLad questions are available here: http://neurostars.org/tags/datalad/ # Installation ## Debian-based systems On Debian-based systems, we recommend enabling [NeuroDebian], via which we provide recent releases of DataLad. Once enabled, just do: apt-get install datalad ## Gentoo-based systems On Gentoo-based systems (i.e. all systems whose package manager can parse ebuilds as per the [Package Manager Specification]), we recommend [enabling the ::science overlay], via which we provide recent releases of DataLad. Once enabled, just run: emerge datalad ## Other Linux'es via conda conda install -c conda-forge datalad will install the most recently released version, and release candidates are available via conda install -c conda-forge/label/rc datalad ## Other Linux'es, macOS via pip Before you install this package, please make sure that you [install a recent version of git-annex](https://git-annex.branchable.com/install). Afterwards, install the latest version of `datalad` from [PyPI](https://pypi.org/project/datalad). It is recommended to use a dedicated [virtualenv](https://virtualenv.pypa.io): # Create and enter a new virtual environment (optional) virtualenv --python=python3 ~/env/datalad . ~/env/datalad/bin/activate # Install from PyPI pip install datalad By default, installation via pip installs the core functionality of DataLad, allowing for managing datasets etc. Additional installation schemes are available, so you can request enhanced installation via `pip install datalad[SCHEME]`, where `SCHEME` could be: - `tests` to also install dependencies used by DataLad's battery of unit tests - `full` to install all dependencies. More details on installation and initial configuration can be found in the [DataLad Handbook: Installation]. # License MIT/Expat # Contributing See [CONTRIBUTING.md](CONTRIBUTING.md) if you are interested in internals or contributing to the project. ## Acknowledgements The DataLad project received support through the following grants: - US-German collaboration in computational neuroscience (CRCNS) project \"DataGit: converging catalogues, warehouses, and deployment logistics into a federated 'data distribution'\" (Halchenko/Hanke), co-funded by the US National Science Foundation (NSF 1429999) and the German Federal Ministry of Education and Research (BMBF 01GQ1411). - CRCNS US-German Data Sharing \"DataLad - a decentralized system for integrated discovery, management, and publication of digital objects of science\" (Halchenko/Pestilli/Hanke), co-funded by the US National Science Foundation (NSF 1912266) and the German Federal Ministry of Education and Research (BMBF 01GQ1905). - Helmholtz Research Center Jülich, FDM challenge 2022 - German federal state of Saxony-Anhalt and the European Regional Development Fund (ERDF), Project: Center for Behavioral Brain Sciences, Imaging Platform - ReproNim project (NIH 1P41EB019936-01A1). - Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant SFB 1451 ([431549029](https://gepris.dfg.de/gepris/projekt/431549029), INF project) - European Union's Horizon 2020 research and innovation programme under grant agreements: - [Human Brain Project SGA3 (H2020-EU.3.1.5.3, grant no. 945539)](https://cordis.europa.eu/project/id/945539) - [VirtualBrainCloud (H2020-EU.3.1.5.3, grant no. 826421)](https://cordis.europa.eu/project/id/826421) Mac mini instance for development is provided by [MacStadium](https://www.macstadium.com/). ### Contributors ✨ Thanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)): <!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --> <!-- prettier-ignore-start --> <!-- markdownlint-disable --> <table> <tbody> <tr> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/glalteva\"><img src=\"https://avatars2.githubusercontent.com/u/14296143?v=4?s=100\" width=\"100px;\" alt=\"glalteva\"/><br /><sub><b>glalteva</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=glalteva\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/adswa\"><img src=\"https://avatars1.githubusercontent.com/u/29738718?v=4?s=100\" width=\"100px;\" alt=\"adswa\"/><br /><sub><b>adswa</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=adswa\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/chrhaeusler\"><img src=\"https://avatars0.githubusercontent.com/u/8115807?v=4?s=100\" width=\"100px;\" alt=\"chrhaeusler\"/><br /><sub><b>chrhaeusler</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=chrhaeusler\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/soichih\"><img src=\"https://avatars3.githubusercontent.com/u/923896?v=4?s=100\" width=\"100px;\" alt=\"soichih\"/><br /><sub><b>soichih</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=soichih\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mvdoc\"><img src=\"https://avatars1.githubusercontent.com/u/6150554?v=4?s=100\" width=\"100px;\" alt=\"mvdoc\"/><br /><sub><b>mvdoc</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=mvdoc\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mih\"><img src=\"https://avatars1.githubusercontent.com/u/136479?v=4?s=100\" width=\"100px;\" alt=\"mih\"/><br /><sub><b>mih</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=mih\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/yarikoptic\"><img src=\"https://avatars3.githubusercontent.com/u/39889?v=4?s=100\" width=\"100px;\" alt=\"yarikoptic\"/><br /><sub><b>yarikoptic</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=yarikoptic\" title=\"Code\">💻</a></td> </tr> <tr> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/loj\"><img src=\"https://avatars2.githubusercontent.com/u/15157717?v=4?s=100\" width=\"100px;\" alt=\"loj\"/><br /><sub><b>loj</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=loj\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/feilong\"><img src=\"https://avatars2.githubusercontent.com/u/2242261?v=4?s=100\" width=\"100px;\" alt=\"feilong\"/><br /><sub><b>feilong</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=feilong\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jhpoelen\"><img src=\"https://avatars2.githubusercontent.com/u/1084872?v=4?s=100\" width=\"100px;\" alt=\"jhpoelen\"/><br /><sub><b>jhpoelen</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=jhpoelen\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/andycon\"><img src=\"https://avatars1.githubusercontent.com/u/3965889?v=4?s=100\" width=\"100px;\" alt=\"andycon\"/><br /><sub><b>andycon</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=andycon\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nicholsn\"><img src=\"https://avatars3.githubusercontent.com/u/463344?v=4?s=100\" width=\"100px;\" alt=\"nicholsn\"/><br /><sub><b>nicholsn</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=nicholsn\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/adelavega\"><img src=\"https://avatars0.githubusercontent.com/u/2774448?v=4?s=100\" width=\"100px;\" alt=\"adelavega\"/><br /><sub><b>adelavega</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=adelavega\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kskyten\"><img src=\"https://avatars0.githubusercontent.com/u/4163878?v=4?s=100\" width=\"100px;\" alt=\"kskyten\"/><br /><sub><b>kskyten</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=kskyten\" title=\"Code\">💻</a></td> </tr> <tr> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/TheChymera\"><img src=\"https://avatars2.githubusercontent.com/u/950524?v=4?s=100\" width=\"100px;\" alt=\"TheChymera\"/><br /><sub><b>TheChymera</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=TheChymera\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/effigies\"><img src=\"https://avatars0.githubusercontent.com/u/83442?v=4?s=100\" width=\"100px;\" alt=\"effigies\"/><br /><sub><b>effigies</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=effigies\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jgors\"><img src=\"https://avatars1.githubusercontent.com/u/386585?v=4?s=100\" width=\"100px;\" alt=\"jgors\"/><br /><sub><b>jgors</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=jgors\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/debanjum\"><img src=\"https://avatars1.githubusercontent.com/u/6413477?v=4?s=100\" width=\"100px;\" alt=\"debanjum\"/><br /><sub><b>debanjum</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=debanjum\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nellh\"><img src=\"https://avatars3.githubusercontent.com/u/11369795?v=4?s=100\" width=\"100px;\" alt=\"nellh\"/><br /><sub><b>nellh</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=nellh\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/emdupre\"><img src=\"https://avatars3.githubusercontent.com/u/15017191?v=4?s=100\" width=\"100px;\" alt=\"emdupre\"/><br /><sub><b>emdupre</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=emdupre\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/aqw\"><img src=\"https://avatars0.githubusercontent.com/u/765557?v=4?s=100\" width=\"100px;\" alt=\"aqw\"/><br /><sub><b>aqw</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=aqw\" title=\"Code\">💻</a></td> </tr> <tr> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/vsoch\"><img src=\"https://avatars0.githubusercontent.com/u/814322?v=4?s=100\" width=\"100px;\" alt=\"vsoch\"/><br /><sub><b>vsoch</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=vsoch\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kyleam\"><img src=\"https://avatars2.githubusercontent.com/u/1297788?v=4?s=100\" width=\"100px;\" alt=\"kyleam\"/><br /><sub><b>kyleam</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=kyleam\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/driusan\"><img src=\"https://avatars0.githubusercontent.com/u/498329?v=4?s=100\" width=\"100px;\" alt=\"driusan\"/><br /><sub><b>driusan</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=driusan\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/overlake333\"><img src=\"https://avatars1.githubusercontent.com/u/28018084?v=4?s=100\" width=\"100px;\" alt=\"overlake333\"/><br /><sub><b>overlake333</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=overlake333\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/akeshavan\"><img src=\"https://avatars0.githubusercontent.com/u/972008?v=4?s=100\" width=\"100px;\" alt=\"akeshavan\"/><br /><sub><b>akeshavan</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=akeshavan\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jwodder\"><img src=\"https://avatars1.githubusercontent.com/u/98207?v=4?s=100\" width=\"100px;\" alt=\"jwodder\"/><br /><sub><b>jwodder</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=jwodder\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bpoldrack\"><img src=\"https://avatars2.githubusercontent.com/u/10498301?v=4?s=100\" width=\"100px;\" alt=\"bpoldrack\"/><br /><sub><b>bpoldrack</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=bpoldrack\" title=\"Code\">💻</a></td> </tr> <tr> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/yetanothertestuser\"><img src=\"https://avatars0.githubusercontent.com/u/19335420?v=4?s=100\" width=\"100px;\" alt=\"yetanothertestuser\"/><br /><sub><b>yetanothertestuser</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=yetanothertestuser\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/christian-monch\"><img src=\"https://avatars.githubusercontent.com/u/17925232?v=4?s=100\" width=\"100px;\" alt=\"Christian Mönch\"/><br /><sub><b>Christian Mönch</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=christian-monch\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mattcieslak\"><img src=\"https://avatars.githubusercontent.com/u/170026?v=4?s=100\" width=\"100px;\" alt=\"Matt Cieslak\"/><br /><sub><b>Matt Cieslak</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=mattcieslak\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mikapfl\"><img src=\"https://avatars.githubusercontent.com/u/7226087?v=4?s=100\" width=\"100px;\" alt=\"Mika Pflüger\"/><br /><sub><b>Mika Pflüger</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=mikapfl\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://me.ypid.de/\"><img src=\"https://avatars.githubusercontent.com/u/1301158?v=4?s=100\" width=\"100px;\" alt=\"Robin Schneider\"/><br /><sub><b>Robin Schneider</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=ypid\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://orcid.org/0000-0003-4652-3758\"><img src=\"https://avatars.githubusercontent.com/u/7570456?v=4?s=100\" width=\"100px;\" alt=\"Sin Kim\"/><br /><sub><b>Sin Kim</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=kimsin98\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/DisasterMo\"><img src=\"https://avatars.githubusercontent.com/u/49207524?v=4?s=100\" width=\"100px;\" alt=\"Michael Burgardt\"/><br /><sub><b>Michael Burgardt</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=DisasterMo\" title=\"Code\">💻</a></td> </tr> <tr> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://remi-gau.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/6961185?v=4?s=100\" width=\"100px;\" alt=\"Remi Gau\"/><br /><sub><b>Remi Gau</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=Remi-Gau\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mslw\"><img src=\"https://avatars.githubusercontent.com/u/11985212?v=4?s=100\" width=\"100px;\" alt=\"Michał Szczepanik\"/><br /><sub><b>Michał Szczepanik</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=mslw\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bpinsard\"><img src=\"https://avatars.githubusercontent.com/u/1155388?v=4?s=100\" width=\"100px;\" alt=\"Basile\"/><br /><sub><b>Basile</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=bpinsard\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/taylols\"><img src=\"https://avatars.githubusercontent.com/u/28018084?v=4?s=100\" width=\"100px;\" alt=\"Taylor Olson\"/><br /><sub><b>Taylor Olson</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=taylols\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jdkent.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/12564882?v=4?s=100\" width=\"100px;\" alt=\"James Kent\"/><br /><sub><b>James Kent</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=jdkent\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/xgui3783\"><img src=\"https://avatars.githubusercontent.com/u/19381783?v=4?s=100\" width=\"100px;\" alt=\"xgui3783\"/><br /><sub><b>xgui3783</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=xgui3783\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/tstoeter\"><img src=\"https://avatars.githubusercontent.com/u/4901704?v=4?s=100\" width=\"100px;\" alt=\"tstoeter\"/><br /><sub><b>tstoeter</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=tstoeter\" title=\"Code\">💻</a></td> </tr> <tr> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jsheunis.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/10141237?v=4?s=100\" width=\"100px;\" alt=\"Stephan Heunis\"/><br /><sub><b>Stephan Heunis</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=jsheunis\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.mmmccormick.com\"><img src=\"https://avatars.githubusercontent.com/u/25432?v=4?s=100\" width=\"100px;\" alt=\"Matt McCormick\"/><br /><sub><b>Matt McCormick</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=thewtex\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/vickychenglau\"><img src=\"https://avatars.githubusercontent.com/u/22065437?v=4?s=100\" width=\"100px;\" alt=\"Vicky C Lau\"/><br /><sub><b>Vicky C Lau</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=vickychenglau\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://chris-lamb.co.uk\"><img src=\"https://avatars.githubusercontent.com/u/133209?v=4?s=100\" width=\"100px;\" alt=\"Chris Lamb\"/><br /><sub><b>Chris Lamb</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=lamby\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/asmacdo\"><img src=\"https://avatars.githubusercontent.com/u/1028657?v=4?s=100\" width=\"100px;\" alt=\"Austin Macdonald\"/><br /><sub><b>Austin Macdonald</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=asmacdo\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://nobodyinperson.de\"><img src=\"https://avatars.githubusercontent.com/u/19148271?v=4?s=100\" width=\"100px;\" alt=\"Yann Büchau\"/><br /><sub><b>Yann Büchau</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=nobodyinperson\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/matrss\"><img src=\"https://avatars.githubusercontent.com/u/9308656?v=4?s=100\" width=\"100px;\" alt=\"Matthias Riße\"/><br /><sub><b>Matthias Riße</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=matrss\" title=\"Code\">💻</a></td> </tr> <tr> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Aksoo\"><img src=\"https://avatars.githubusercontent.com/u/141905668?v=4?s=100\" width=\"100px;\" alt=\"Aksoo\"/><br /><sub><b>Aksoo</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=Aksoo\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dguibert\"><img src=\"https://avatars.githubusercontent.com/u/1178864?v=4?s=100\" width=\"100px;\" alt=\"David Guibert\"/><br /><sub><b>David Guibert</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=dguibert\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/alliesw\"><img src=\"https://avatars.githubusercontent.com/u/72238329?v=4?s=100\" width=\"100px;\" alt=\"Alex Shields-Weber\"/><br /><sub><b>Alex Shields-Weber</b></sub></a><br /><a href=\"https://github.com/datalad/datalad/commits?author=alliesw\" title=\"Code\">💻</a></td> </tr> </tbody> </table> <!-- markdownlint-restore --> <!-- prettier-ignore-end --> <!-- ALL-CONTRIBUTORS-LIST:END --> [![macstadium](https://uploads-ssl.webflow.com/5ac3c046c82724970fc60918/5c019d917bba312af7553b49_MacStadium-developerlogo.png)](https://www.macstadium.com/) [Git]: https://git-scm.com [Git-annex]: http://git-annex.branchable.com [setup.py]: https://github.com/datalad/datalad/blob/master/setup.py [NeuroDebian]: http://neuro.debian.net [Package Manager Specification]: https://projects.gentoo.org/pms/latest/pms.html [enabling the ::science overlay]: https://github.com/gentoo/sci#manual-install- [DataLad Handbook: Installation]: http://handbook.datalad.org/en/latest/intro/installation.html\n",
                "dependencies": "[build-system] # wheel to get more lightweight (not EASY-INSTALL) entry-points requires = [\"packaging\", \"setuptools>=40.8.0\", \"wheel\"] [tool.isort] force_grid_wrap = 2 include_trailing_comma = true multi_line_output = 3\n# If you want to develop, use requirements-devel.txt # Theoretically we don't want -e here but ATM pip would puke if just .[full] is provided # TODO -- figure it out and/or complain to pip folks # -e .[full] # this one should work but would copy entire . tree so should be ran on a clean copy .[full] # doesn't install datalad itself # file://.#egg=datalad[full]\n#!/usr/bin/env python # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ## # # See COPYING file distributed along with the DataLad package for the # copyright and license terms. # # ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ## import sys from os.path import dirname from os.path import join as opj # This is needed for versioneer to be importable when building with PEP 517. # See <https://github.com/warner/python-versioneer/issues/193> and links # therein for more information. sys.path.append(dirname(__file__)) import versioneer from _datalad_build_support.setup import ( BuildConfigInfo, BuildManPage, datalad_setup, ) requires = { 'core': [ 'platformdirs', 'chardet>=3.0.4', # rarely used but small/omnipresent 'colorama; platform_system==\"Windows\"', 'distro', 'importlib-metadata >=3.6; python_version < \"3.10\"', 'iso8601', 'humanize', 'fasteners>=0.14', 'packaging', 'patool>=1.7', 'tqdm>=4.32.0', 'typing_extensions>=4.0.0; python_version < \"3.11\"', 'annexremote', 'looseversion', ], 'downloaders': [ 'boto3', 'keyring>=20.0,!=23.9.0', 'keyrings.alt', 'msgpack', 'requests>=1.2', ], 'downloaders-extra': [ 'requests_ftp', ], 'publish': [ 'python-gitlab', # required for create-sibling-gitlab ], 'misc': [ 'argcomplete>=1.12.3', # optional CLI completion 'pyperclip', # clipboard manipulations 'python-dateutil', # add support for more date formats to check_dates ], 'tests': [ 'BeautifulSoup4', # VERY weak requirement, still used in one of the tests 'httpretty>=0.9.4', # Introduced py 3.6 support 'mypy', 'pytest>=7.0', # https://github.com/datalad/datalad/issues/7555 'pytest-cov', 'pytest-fail-slow~=0.2', 'types-python-dateutil', 'types-requests', 'vcrpy', ], 'duecredit': [ 'duecredit', # needs >= 0.6.6 to be usable, but should be \"safe\" with prior ones ], } requires['full'] = sum(list(requires.values()), []) # Now add additional ones useful for development requires.update({ 'devel-docs': [ # used for converting README.md -> .rst for long_description 'pypandoc', # Documentation 'sphinx>=4.3.0', 'sphinx-autodoc-typehints', 'sphinx-rtd-theme>=0.5.1', ], 'devel-utils': [ 'asv', # benchmarks 'coverage!=7.6.5', 'gprof2dot', # rendering cProfile output as a graph image 'psutil', 'pytest-xdist', # parallelize pytest runs etc # disable for now, as it pulls in ipython 6, which is PY3 only #'line-profiler', # necessary for accessing SecretStorage keyring (system wide Gnome # keyring) but not installable on travis, IIRC since it needs connectivity # to the dbus whenever installed or smth like that, thus disabled here # but you might need it # 'dbus-python', 'scriv', # changelog ], }) requires['devel'] = sum(list(requires.values()), []) # let's not build manpages and examples automatically (gh-896) # configure additional command for custom build steps #class DataladBuild(build_py): # def run(self): # self.run_command('build_manpage') # self.run_command('build_examples') # build_py.run(self) cmdclass = { 'build_manpage': BuildManPage, # 'build_examples': BuildRSTExamplesFromScripts, 'build_cfginfo': BuildConfigInfo, # 'build_py': DataladBuild } setup_kwargs = {} # normal entrypoints for the rest # a bit of a dance needed, as on windows the situation is different entry_points = { 'console_scripts': [ 'datalad=datalad.cli.main:main', 'git-annex-remote-datalad-archives=datalad.customremotes.archives:main', 'git-annex-remote-datalad=datalad.customremotes.datalad:main', 'git-annex-remote-ria=datalad.customremotes.ria_remote:main', 'git-annex-remote-ora=datalad.distributed.ora_remote:main', 'git-credential-datalad=datalad.local.gitcredential_datalad:git_credential_datalad', ], } setup_kwargs['entry_points'] = entry_points classifiers = [ 'Development Status :: 5 - Production/Stable', 'Environment :: Console', 'Intended Audience :: Developers', 'Intended Audience :: Education', 'Intended Audience :: End Users/Desktop', 'Intended Audience :: Science/Research', 'License :: DFSG approved', 'License :: OSI Approved :: MIT License', 'Natural Language :: English', 'Operating System :: POSIX', 'Programming Language :: Python :: 3 :: Only', 'Programming Language :: Unix Shell', 'Topic :: Communications :: File Sharing', 'Topic :: Education', 'Topic :: Internet', 'Topic :: Other/Nonlisted Topic', 'Topic :: Scientific/Engineering', 'Topic :: Software Development :: Libraries :: Python Modules', 'Topic :: Software Development :: Version Control :: Git', 'Topic :: Utilities', ] setup_kwargs['classifiers'] = classifiers setup_kwargs[\"version\"] = versioneer.get_version() cmdclass.update(versioneer.get_cmdclass()) datalad_setup( 'datalad', description=\"data distribution geared toward scientific datasets\", install_requires= requires['core'] + requires['downloaders'] + requires['publish'], python_requires='>=3.9', project_urls={'Homepage': 'https://www.datalad.org', 'Developer docs': 'https://docs.datalad.org/en/stable', 'User handbook': 'https://handbook.datalad.org', 'Source': 'https://github.com/datalad/datalad', 'Bug Tracker': 'https://github.com/datalad/datalad/issues', 'RRID': 'https://identifiers.org/RRID:SCR_003931'}, extras_require=requires, cmdclass=cmdclass, include_package_data=True, **setup_kwargs )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/datalad-container-extension",
            "repo_link": "https://github.com/datalad/datalad-container",
            "content": {
                "codemeta": "",
                "readme": "____ _ _ _ | _ \\ __ _ | |_ __ _ | | __ _ __| | | | | | / _` || __| / _` || | / _` | / _` | | |_| || (_| || |_ | (_| || |___ | (_| || (_| | |____/ \\__,_| \\__| \\__,_||_____| \\__,_| \\__,_| Container [![Build status](https://ci.appveyor.com/api/projects/status/k4eyq1yygcvwf7wk/branch/master?svg=true)](https://ci.appveyor.com/project/mih/datalad-container/branch/master) [![Travis tests status](https://app.travis-ci.com/datalad/datalad-container.svg?branch=master)](https://app.travis-ci.com/datalad/datalad-container) [![codecov.io](https://codecov.io/github/datalad/datalad-container/coverage.svg?branch=master)](https://codecov.io/github/datalad/datalad-container?branch=master) [![Documentation](https://readthedocs.org/projects/datalad-container/badge/?version=latest)](http://datalad-container.rtfd.org) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![GitHub release](https://img.shields.io/github/release/datalad/datalad-container.svg)](https://GitHub.com/datalad/datalad-container/releases/) [![PyPI version fury.io](https://badge.fury.io/py/datalad-container.svg)](https://pypi.python.org/pypi/datalad-container/) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3368666.svg)](https://doi.org/10.5281/zenodo.3368666) ![Conda](https://anaconda.org/conda-forge/datalad-container/badges/version.svg) This extension enhances DataLad (http://datalad.org) for working with computational containers. Please see the [extension documentation](http://datalad-container.rtfd.org) for a description on additional commands and functionality. For general information on how to use or contribute to DataLad (and this extension), please see the [DataLad website](http://datalad.org) or the [main GitHub project page](http://datalad.org). ## Installation Before you install this package, please make sure that you [install a recent version of git-annex](https://git-annex.branchable.com/install). Afterwards, install the latest version of `datalad-container` from [PyPi](https://pypi.org/project/datalad-container). It is recommended to use a dedicated [virtualenv](https://virtualenv.pypa.io): # create and enter a new virtual environment (optional) virtualenv --system-site-packages --python=python3 ~/env/datalad . ~/env/datalad/bin/activate # install from PyPi pip install datalad_container It is also available for conda package manager from conda-forge: conda install -c conda-forge datalad-container ## Support The documentation of this project is found here: http://docs.datalad.org/projects/container All bugs, concerns and enhancement requests for this software can be submitted here: https://github.com/datalad/datalad-container/issues If you have a problem or would like to ask a question about how to use DataLad, please [submit a question to NeuroStars.org](https://neurostars.org/tags/datalad) with a ``datalad`` tag. NeuroStars.org is a platform similar to StackOverflow but dedicated to neuroinformatics. All previous DataLad questions are available here: http://neurostars.org/tags/datalad/ ## Acknowledgements DataLad development is supported by a US-German collaboration in computational neuroscience (CRCNS) project \"DataGit: converging catalogues, warehouses, and deployment logistics into a federated 'data distribution'\" (Halchenko/Hanke), co-funded by the US National Science Foundation (NSF 1429999) and the German Federal Ministry of Education and Research (BMBF 01GQ1411). Additional support is provided by the German federal state of Saxony-Anhalt and the European Regional Development Fund (ERDF), Project: Center for Behavioral Brain Sciences, Imaging Platform. This work is further facilitated by the ReproNim project (NIH 1P41EB019936-01A1).\n",
                "dependencies": "[build-system] requires = [\"setuptools >= 43.0.0\", \"tomli\", \"wheel\"] [tool.isort] force_grid_wrap = 2 include_trailing_comma = true multi_line_output = 3 combine_as_imports = true [tool.codespell] skip = '.git,*.pdf,*.svg,venvs,versioneer.py,venvs' # DNE - do not exist ignore-words-list = 'dne' [tool.versioneer] # See the docstring in versioneer.py for instructions. Note that you must # re-run 'versioneer.py setup' after changing this section, and commit the # resulting files. VCS = 'git' style = 'pep440' versionfile_source = 'datalad_container/_version.py' versionfile_build = 'datalad_container/_version.py' tag_prefix = '' parentdir_prefix = ''\n# If you want to develop, use requirements-devel.txt # git+https://github.com/datalad/datalad.git\n#!/usr/bin/env python from setuptools import setup import versioneer from _datalad_buildsupport.setup import ( BuildManPage, ) cmdclass = versioneer.get_cmdclass() cmdclass.update(build_manpage=BuildManPage) if __name__ == '__main__': setup(name='datalad_container', version=versioneer.get_version(), cmdclass=cmdclass, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/datalad-next-extension",
            "repo_link": "https://github.com/datalad/datalad-next",
            "content": {
                "codemeta": "",
                "readme": "# DataLad NEXT extension [![All Contributors](https://img.shields.io/github/all-contributors/datalad/datalad-next?color=ee8449&style=flat-square)](#contributors) [![Build status](https://ci.appveyor.com/api/projects/status/dxomp8wysjb7x2os/branch/main?svg=true)](https://ci.appveyor.com/project/mih/datalad-next/branch/main) [![codecov](https://codecov.io/gh/datalad/datalad-next/branch/main/graph/badge.svg?token=2P8rak7lSX)](https://codecov.io/gh/datalad/datalad-next) [![docs](https://github.com/datalad/datalad-next/workflows/docs/badge.svg)](https://github.com/datalad/datalad-next/actions?query=workflow%3Adocs) [![Documentation Status](https://readthedocs.org/projects/datalad-next/badge/?version=latest)](http://docs.datalad.org/projects/next/en/latest/?badge=latest) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![GitHub release](https://img.shields.io/github/release/datalad/datalad-next.svg)](https://GitHub.com/datalad/datalad-next/releases/) [![PyPI version fury.io](https://badge.fury.io/py/datalad-next.svg)](https://pypi.python.org/pypi/datalad-next/) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6833099.svg)](https://doi.org/10.5281/zenodo.6833099) [![Hatch project](https://img.shields.io/badge/%F0%9F%A5%9A-Hatch-4051b5.svg)](https://github.com/pypa/hatch) This DataLad extension can be thought of as a staging area for additional functionality, or for improved performance and user experience. Unlike other topical or more experimental extensions, the focus here is on functionality with broad applicability. This extension is a suitable dependency for other software packages that intend to build on this improved set of functionality. ## Installation ``` # create and enter a new virtual environment (optional) $ virtualenv --python=python3 ~/env/dl-next $ . ~/env/dl-next/bin/activate # install from PyPi $ python -m pip install datalad-next ``` ## How to use Additional commands provided by this extension are immediately available after installation. However, in order to fully benefit from all improvements, the extension has to be enabled for auto-loading by executing: git config --global --add datalad.extensions.load next Doing so will enable the extension to also alter the behavior the core DataLad package and its commands. ## Summary of functionality provided by this extension - A replacement sub-system for credential handling that is able to handle arbitrary properties for annotating a secret, and facilitates determining suitable credentials while minimizing avoidable user interaction, without compromising configurability. A convenience method is provided that implements a standard workflow for obtaining a credential. - A user-facing `credentials` command to set, remove, and query credentials. - The `create-sibling-...` commands for the platforms GitHub, GIN, GOGS, Gitea are equipped with improved credential handling that, for example, only stores entered credentials after they were confirmed to work, or auto-selects the most recently used, matching credentials, when none are specified. - A `create-sibling-webdav` command for hosting datasets on a WebDAV server via a sibling tandem for Git history and file storage. Datasets hosted on WebDAV in this fashion are cloneable with `datalad-clone`. A full annex setup for storing complete datasets with historical file content version, and an additional mode for depositing single-version dataset snapshot are supported. The latter enables convenient collaboration with audiences that are not using DataLad, because all files are browsable via a WebDAV server's point-and-click user interface. - Enhance `datalad-push` to automatically export files to git-annex special remotes configured with `exporttree=yes`. - Speed-up `datalad-push` when processing non-git special remotes. This particularly benefits less efficient hosting scenarios like WebDAV. - Enhance `datalad-siblings enable` (`AnnexRepo.enable_remote()`) to automatically deploy credentials for git-annex special remotes that require them. - `git-remote-datalad-annex` is a Git remote helper to push/fetch to any location accessible by any git-annex special remote. - `git-annex-backend-XDLRA` (originally available from the `mihextras` extension) is a custom external git-annex backend used by `git-remote-datalad-annex`. A base class to facilitate development of external backends in Python is also provided. - Enhance `datalad-configuration` to support getting configuration from \"global\" scope without a dataset being present. - New modular framework for URL operations. This framework directly supports operation on `http(s)`, `ssh`, and `file` URLs, and can be extended with custom functionality for additional protocols or even interaction with specific individual servers. The basic operations `download`, `upload`, `delete`, and `stat` are recognized, and can be implemented. The framework offers uniform progress reporting and simultaneous content has computation. This framework is meant to replace and extend the downloader/provide framework in the DataLad core package. In contrast to its predecessor it is integrated with the new credential framework, and operations beyond downloading. - `git-annex-remote-uncurl` is a special remote that exposes the new URL operations framework via git-annex. It provides flexible means to compose and rewrite URLs (e.g., to compensate for storage infrastructure changes) without having to modify individual URLs recorded in datasets. It enables seamless transitions between any services and protocols supported by the framework. This special remote can replace the `datalad` special remote provided by the DataLad core package. - A `download` command is provided as a front-end for the new modular URL operations framework. - A `python-requests` compatible authentication handler (`DataladAuth`) that interfaces DataLad's credential system. - Boosted throughput of DataLad's `runner` component for command execution. - Substantially more comprehensive replacement for DataLad's `constraints` system for type conversion and parameter validation. - Windows and Mac client support for RIA store access. - A `next-status` command that is A LOT faster than `status`, and offers a `mono` recursion mode that shows modifications of nested dataset hierarchies relative to the state of the root dataset. Requires Git v2.31 (or later). ## Summary of additional features for DataLad extension development - Framework for uniform command parameter validation. Regardless of the used API (Python, CLI, or GUI), command parameters are uniformly validated. This facilitates a stricter separation of parameter specification (and validation) from the actual implementation of a command. The latter can now focus on a command's logic only, while the former enables more uniform and more comprehensive validation and error reporting. Beyond per-parameter validation and type-conversion also inter-parameter dependency validation and value transformations are supported. - Improved composition of importable functionality. Key components for `commands`, `annexremotes`, `datasets` (etc) are collected in topical top-level modules that provide \"all\" necessary pieces in a single place. - `webdav_server` fixture that automatically deploys a local WebDAV server. - Utilities for HTTP handling - `probe_url()` discovers redirects and authentication requirements for an HTTP URL - `get_auth_realm()` returns a label for an authentication realm that can be used to query for matching credentials - Utilities for special remote credential management: - `get_specialremote_credential_properties()` inspects a special remote and returns properties for querying a credential store for matching credentials - `update_specialremote_credential()` updates a credential in a store after successful use - `get_specialremote_credential_envpatch()` returns a suitable environment \"patch\" from a credential for a particular special remote type - Helper for runtime-patching other datalad code (`datalad_next.utils.patch`) - Base class for implementing custom `git-annex` backends. - A set of `pytest` fixtures to: - check that no global configuration side-effects are left behind by a test - check that no secrets are left behind by a test - provide a temporary configuration that is isolated from a user environment and from other tests - provide a temporary secret store that is isolated from a user environment and from other tests - provide a temporary credential manager to perform credential deployment and manipulation isolated from a user environment and from other tests - An `iter_subproc()` helper that enable communication with subprocesses via input/output iterables. - A `shell` context manager that enables interaction with (remote) shells, including support for input/output iterables for each shell-command execution within the context. ## Patching the DataLad core package. Some of the features described above rely on a modification of the DataLad core package itself, rather than coming in the form of additional commands. Loading this extension causes a range of patches to be applied to the `datalad` package to enable them. A comprehensive description of the current set of patch is available at http://docs.datalad.org/projects/next/en/latest/#datalad-patches ## Developing with DataLad NEXT This extension package moves fast in comparison to the core package. Nevertheless, attention is paid to API stability, adequate semantic versioning, and informative changelogs. ### Public vs internal API Anything that can be imported directly from any of the sub-packages in `datalad_next` is considered to be part of the public API. Changes to this API determine the versioning, and development is done with the aim to keep this API as stable as possible. This includes signatures and return value behavior. As an example: `from datalad_next.runners import iter_git_subproc` imports a part of the public API, but `from datalad_next.runners.git import iter_git_subproc` does not. ### Use of the internal API Developers can obviously use parts of the non-public API. However, this should only be done with the understanding that these components may change from one release to another, with no guarantee of transition periods, deprecation warnings, etc. Developers are advised to never reuse any components with names starting with `_` (underscore). Their use should be limited to their individual subpackage. ## Acknowledgements This DataLad extension was developed with funding from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant SFB 1451 ([431549029](https://gepris.dfg.de/gepris/projekt/431549029), INF project). ## Contributors <!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --> <!-- prettier-ignore-start --> <!-- markdownlint-disable --> <table> <tbody> <tr> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://psychoinformatics.de/\"><img src=\"https://avatars.githubusercontent.com/u/136479?v=4?s=100\" width=\"100px;\" alt=\"Michael Hanke\"/><br /><sub><b>Michael Hanke</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/issues?q=author%3Amih\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=mih\" title=\"Code\">💻</a> <a href=\"#content-mih\" title=\"Content\">🖋</a> <a href=\"#design-mih\" title=\"Design\">🎨</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=mih\" title=\"Documentation\">📖</a> <a href=\"#financial-mih\" title=\"Financial\">💵</a> <a href=\"#fundingFinding-mih\" title=\"Funding Finding\">🔍</a> <a href=\"#ideas-mih\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"#infra-mih\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"#maintenance-mih\" title=\"Maintenance\">🚧</a> <a href=\"#mentoring-mih\" title=\"Mentoring\">🧑‍🏫</a> <a href=\"#platform-mih\" title=\"Packaging/porting to new platform\">📦</a> <a href=\"#projectManagement-mih\" title=\"Project Management\">📆</a> <a href=\"https://github.com/datalad/datalad-next/pulls?q=is%3Apr+reviewed-by%3Amih\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#talk-mih\" title=\"Talks\">📢</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=mih\" title=\"Tests\">⚠️</a> <a href=\"#tool-mih\" title=\"Tools\">🔧</a> <a href=\"#userTesting-mih\" title=\"User Testing\">📓</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/catetrai\"><img src=\"https://avatars.githubusercontent.com/u/18424941?v=4?s=100\" width=\"100px;\" alt=\"catetrai\"/><br /><sub><b>catetrai</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/commits?author=catetrai\" title=\"Code\">💻</a> <a href=\"#design-catetrai\" title=\"Design\">🎨</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=catetrai\" title=\"Documentation\">📖</a> <a href=\"#ideas-catetrai\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=catetrai\" title=\"Tests\">⚠️</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/effigies\"><img src=\"https://avatars.githubusercontent.com/u/83442?v=4?s=100\" width=\"100px;\" alt=\"Chris Markiewicz\"/><br /><sub><b>Chris Markiewicz</b></sub></a><br /><a href=\"#maintenance-effigies\" title=\"Maintenance\">🚧</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=effigies\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mslw\"><img src=\"https://avatars.githubusercontent.com/u/11985212?v=4?s=100\" width=\"100px;\" alt=\"Michał Szczepanik\"/><br /><sub><b>Michał Szczepanik</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/issues?q=author%3Amslw\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=mslw\" title=\"Code\">💻</a> <a href=\"#content-mslw\" title=\"Content\">🖋</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=mslw\" title=\"Documentation\">📖</a> <a href=\"#example-mslw\" title=\"Examples\">💡</a> <a href=\"#ideas-mslw\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"#infra-mslw\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"#maintenance-mslw\" title=\"Maintenance\">🚧</a> <a href=\"https://github.com/datalad/datalad-next/pulls?q=is%3Apr+reviewed-by%3Amslw\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#talk-mslw\" title=\"Talks\">📢</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=mslw\" title=\"Tests\">⚠️</a> <a href=\"#tutorial-mslw\" title=\"Tutorials\">✅</a> <a href=\"#userTesting-mslw\" title=\"User Testing\">📓</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jsheunis.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/10141237?v=4?s=100\" width=\"100px;\" alt=\"Stephan Heunis\"/><br /><sub><b>Stephan Heunis</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/issues?q=author%3Ajsheunis\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=jsheunis\" title=\"Code\">💻</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=jsheunis\" title=\"Documentation\">📖</a> <a href=\"#ideas-jsheunis\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"#maintenance-jsheunis\" title=\"Maintenance\">🚧</a> <a href=\"#talk-jsheunis\" title=\"Talks\">📢</a> <a href=\"#userTesting-jsheunis\" title=\"User Testing\">📓</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bpoldrack\"><img src=\"https://avatars.githubusercontent.com/u/10498301?v=4?s=100\" width=\"100px;\" alt=\"Benjamin Poldrack\"/><br /><sub><b>Benjamin Poldrack</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/issues?q=author%3Abpoldrack\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=bpoldrack\" title=\"Code\">💻</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/yarikoptic\"><img src=\"https://avatars.githubusercontent.com/u/39889?v=4?s=100\" width=\"100px;\" alt=\"Yaroslav Halchenko\"/><br /><sub><b>Yaroslav Halchenko</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/issues?q=author%3Ayarikoptic\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=yarikoptic\" title=\"Code\">💻</a> <a href=\"#infra-yarikoptic\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"#maintenance-yarikoptic\" title=\"Maintenance\">🚧</a> <a href=\"#tool-yarikoptic\" title=\"Tools\">🔧</a></td> </tr> <tr> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/christian-monch\"><img src=\"https://avatars.githubusercontent.com/u/17925232?v=4?s=100\" width=\"100px;\" alt=\"Christian Mönch\"/><br /><sub><b>Christian Mönch</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/commits?author=christian-monch\" title=\"Code\">💻</a> <a href=\"#design-christian-monch\" title=\"Design\">🎨</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=christian-monch\" title=\"Documentation\">📖</a> <a href=\"#ideas-christian-monch\" title=\"Ideas, Planning, & Feedback\">🤔</a> <a href=\"https://github.com/datalad/datalad-next/pulls?q=is%3Apr+reviewed-by%3Achristian-monch\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=christian-monch\" title=\"Tests\">⚠️</a> <a href=\"#userTesting-christian-monch\" title=\"User Testing\">📓</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/adswa\"><img src=\"https://avatars.githubusercontent.com/u/29738718?v=4?s=100\" width=\"100px;\" alt=\"Adina Wagner\"/><br /><sub><b>Adina Wagner</b></sub></a><br /><a href=\"#a11y-adswa\" title=\"Accessibility\">️️️️♿️</a> <a href=\"https://github.com/datalad/datalad-next/issues?q=author%3Aadswa\" title=\"Bug reports\">🐛</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=adswa\" title=\"Code\">💻</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=adswa\" title=\"Documentation\">📖</a> <a href=\"#example-adswa\" title=\"Examples\">💡</a> <a href=\"#maintenance-adswa\" title=\"Maintenance\">🚧</a> <a href=\"#projectManagement-adswa\" title=\"Project Management\">📆</a> <a href=\"https://github.com/datalad/datalad-next/pulls?q=is%3Apr+reviewed-by%3Aadswa\" title=\"Reviewed Pull Requests\">👀</a> <a href=\"#talk-adswa\" title=\"Talks\">📢</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=adswa\" title=\"Tests\">⚠️</a> <a href=\"#tutorial-adswa\" title=\"Tutorials\">✅</a> <a href=\"#userTesting-adswa\" title=\"User Testing\">📓</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jwodder\"><img src=\"https://avatars.githubusercontent.com/u/98207?v=4?s=100\" width=\"100px;\" alt=\"John T. Wodder II\"/><br /><sub><b>John T. Wodder II</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/commits?author=jwodder\" title=\"Code\">💻</a> <a href=\"#infra-jwodder\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">🚇</a> <a href=\"https://github.com/datalad/datalad-next/commits?author=jwodder\" title=\"Tests\">⚠️</a></td> <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/candleindark\"><img src=\"https://avatars.githubusercontent.com/u/12135617?v=4?s=100\" width=\"100px;\" alt=\"Isaac To\"/><br /><sub><b>Isaac To</b></sub></a><br /><a href=\"https://github.com/datalad/datalad-next/commits?author=candleindark\" title=\"Code\">💻</a></td> </tr> </tbody> </table> <!-- markdownlint-restore --> <!-- prettier-ignore-end --> <!-- ALL-CONTRIBUTORS-LIST:END -->\n",
                "dependencies": "[build-system] requires = [ \"hatchling\", \"hatch-vcs\", ] build-backend = \"hatchling.build\" [project] name = \"datalad-next\" dynamic = [\"version\"] description = \"What is next in DataLad\" readme = \"README.md\" requires-python = \">= 3.8\" license = \"MIT\" keywords = [ \"datalad\", \"git\", \"git-annex\", ] authors = [ { name = \"The DataLad Team and Contributors\", email = \"team@datalad.org\" }, ] classifiers = [ \"License :: OSI Approved :: MIT License\", \"Development Status :: 4 - Beta\", \"Intended Audience :: Developers\", \"Intended Audience :: End Users/Desktop\", \"Natural Language :: English\", \"Operating System :: OS Independent\", \"Topic :: Software Development\", \"Topic :: Software Development :: Version Control\", \"Topic :: Software Development :: Version Control :: Git\", \"Programming Language :: Python\", \"Programming Language :: Python :: 3\", ] dependencies = [ \"annexremote\", \"datalad >= 0.18.4\", \"datasalad >= 0.2.1\", \"humanize\", \"more-itertools\", ] [project.urls] Homepage = \"https://github.com/datalad/datalad-next\" Documentation = \"https://docs.datalad.org/projects/next/en/latest/\" Issues = \"https://github.com/datalad/datalad-next/issues\" Source = \"https://github.com/datalad/datalad-next\" Changelog = \"https://github.com/datalad/datalad-next/blob/main/CHANGELOG.md\" [project.optional-dependencies] devel = [ \"cheroot\", \"coverage\", \"psutil\", \"pytest\", \"pytest-cov\", \"webdavclient3\", \"wsgidav\", ] httpsupport = [ \"requests\", \"requests_toolbelt\", ] [project.scripts] git-annex-backend-XDLRA = \"datalad_next.annexbackends.xdlra:main\" git-annex-remote-archivist = \"datalad_next.annexremotes.archivist:main\" git-annex-remote-uncurl = \"datalad_next.annexremotes.uncurl:main\" git-remote-datalad-annex = \"datalad_next.gitremotes.datalad_annex:main\" [project.entry-points.\"datalad.extensions\"] next = \"datalad_next:command_suite\" [tool.hatch.version] source = \"vcs\" [tool.hatch.build.hooks.vcs] version-file = \"datalad_next/_version.py\" [tool.hatch.build.targets.sdist] exclude = [ \".github\", \"tools\", \"docs/build\", ] [tool.hatch.envs.hatch-test] default-args = [\"datalad_next\"] extra-dependencies = [ \"pytest\", \"pytest-cov\", \"psutil\", \"webdavclient3\", \"wsgidav\", ] [tool.hatch.envs.hatch-test.env-vars] # load the extension DATALAD_EXTENSIONS_LOAD = \"next\" [tool.hatch.envs.tests] description = \"run tests across Python versions\" template = \"hatch-test\" [[tool.hatch.envs.tests.matrix]] python = [\"3.9\", \"3.10\", \"3.11\", \"3.12\"] [tool.hatch.envs.tests.scripts] run = 'python -m pytest {args}' [tool.hatch.envs.types] description = \"type checking with MyPy\" extra-dependencies = [ \"mypy>=1.0.0\", \"pytest\", ] [tool.hatch.envs.types.scripts] check = [ \"mypy --install-types --non-interactive --python-version 3.8 --pretty --show-error-context {args:datalad_next}\", ] [tool.hatch.envs.docs] description = \"build Sphinx-based docs\" extra-dependencies = [ \"sphinx\", \"sphinx_rtd_theme\", \"pytest\", ] [tool.hatch.envs.docs.scripts] build = [ \"make -C docs html\", ] clean = [ \"rm -rf docs/generated\", \"make -C docs clean\", ] [tool.hatch.envs.cz] description = \"commit compliance, changelog, and release generation\" detached = true extra-dependencies = [ \"commitizen\", ] [tool.hatch.envs.cz.scripts] #check-commits = [ # # check all commit messages since the (before) beginning # \"cz check --rev-range 4b825dc642cb6eb9a060e54bf8d69288fbee4904..HEAD\", #] show-changelog = [ # show the would-be changelog on stdout \"cz changelog --dry-run\", ] bump-version = [ # bump version (also tags) and update changelog \"cz bump --changelog\", ] [tool.hatch.envs.codespell] description = \"spell checking\" detached = true extra-dependencies = [ \"codespell\", ] [tool.hatch.envs.codespell.scripts] check = \"codespell\" fix = \"codespell --write-changes\" [tool.codespell] skip = \".git,build,.*cache,dist\" exclude-file = \".codespell-exclude\" [tool.pytest.ini_options] addopts = \"--strict-markers\" markers = [ # datalad-next custom markers \"skip_if_no_network\", # (implicitly) used markers from datalad-core, which are only declared # in its tox.ini (inaccessible to pytest here) \"fail_slow\", \"githubci_osx\", \"githubci_win\", \"integration\", \"known_failure\", \"known_failure_githubci_osx\", \"known_failure_githubci_win\", \"known_failure_osx\", \"known_failure_windows\", \"network\", \"osx\", \"probe_known_failure\", \"serve_path_via_http\", \"skip_if_adjusted_branch\", \"skip_if_no_network\", \"skip_if_on_windows\", \"skip_if_root\", \"skip_known_failure\", \"skip_nomultiplex_ssh\", \"skip_ssh\", \"skip_wo_symlink_capability\", \"slow\", \"turtle\", \"usecase\", \"windows\", \"with_config\", \"with_fake_cookies_db\", \"with_memory_keyring\", \"with_sameas_remotes\", \"with_testrepos\", \"without_http_proxy\", ] [tool.coverage.run] source_pkgs = [\"datalad_next\"] branch = true parallel = true omit = [ # \"src/datasalad/__about__.py\", ] data_file = \"${COVERAGE_ROOT-.}/.coverage\" [tool.coverage.paths] datalad_next = [\"src/datalad_next\", \"*/datalad_next/src/datalad_next\"] tests = [\"tests\", \"*/datalad_next/*/tests\"] [tool.coverage.report] show_missing = true exclude_lines = [ \"no cov\", \"if __name__ == .__main__.:\", \"if TYPE_CHECKING:\", \"raise NotImplementedError\", ] [tool.ruff] exclude = [ # sphinx \"docs\", ] line-length = 88 indent-width = 4 target-version = \"py38\" [tool.ruff.format] # Prefer single quotes over double quotes. quote-style = \"single\" [tool.ruff.lint.per-file-ignores] \"**/test_*\" = [ # permit assert statements in tests \"S101\", # permit relative import in tests \"TID252\", # permit versatile function names in tests \"N802\", ] # permit relative import in subpackage root \"datalad_next/*/__init__.py\" = [\"TID252\"] [tool.commitizen] name = \"cz_customize\" tag_format = \"$version\" version_scheme = \"pep440\" version_provider = \"scm\" changelog_incremental = true template = \".changelog.md.j2\" gpg_sign = true [tool.commitizen.customize] commit_parser = \"^((?P<change_type>feat|fix|rf|perf|test|doc|BREAKING CHANGE)(?:\\\\((?P<scope>[^()\\r\\n]*)\\\\)|\\\\()?(?P<breaking>!)?|\\\\w+!):\\\\s(?P<message>.*)?(?P<body>.*)?\" change_type_order = [\"BREAKING CHANGE\", \"feat\", \"fix\", \"rf\", \"perf\", \"doc\", \"test\"] changelog_pattern = \"^((BREAKING[\\\\-\\\\ ]CHANGE|\\\\w+)(\\\\(.+\\\\))?!?):\" bump_pattern = \"^((BREAKING[\\\\-\\\\ ]CHANGE|\\\\w+)(\\\\(.+\\\\))?!?):\" schema_pattern = \"(?s)(ci|doc|feat|fix|perf|rf|style|test|chore|revert|bump)(\\\\(\\\\S+\\\\))?!?:( [^\\\\n\\\\r]+)((\\\\n\\\\n.*)|(\\\\s*))?$\" [tool.commitizen.customize.bump_map] \"^\\\\w+!\" = \"MAJOR\" \"^BREAKING\" = \"MAJOR\" \"^feat\" = \"MINOR\" \"^fix\" = \"PATCH\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/datasail",
            "repo_link": "https://github.com/kalininalab/DataSAIL",
            "content": {
                "codemeta": "",
                "readme": "# DataSAIL: Data Splitting Against Information Leaking ![testing](https://github.com/kalininalab/glyles/actions/workflows/test.yaml/badge.svg) [![docs-image](https://readthedocs.org/projects/glyles/badge/?version=latest)](https://datasail.readthedocs.io/en/latest/index.html) [![codecov](https://codecov.io/gh/kalininalab/DataSAIL/branch/main/graph/badge.svg)](https://codecov.io/gh/kalininalab/DataSAIL) [![anaconda](https://anaconda.org/kalininalab/datasail/badges/version.svg)](https://anaconda.org/kalininalab/datasail) [![update](https://anaconda.org/kalininalab/datasail/badges/latest_release_date.svg)](https://anaconda.org/kalininalab/datasail) [![license](https://anaconda.org/kalininalab/datasail/badges/license.svg)](https://anaconda.org/kalininalab/datasail) [![downloads](https://anaconda.org/kalininalab/datasail/badges/downloads.svg)](https://anaconda.org/kalininalab/datasail) ![Python 3](https://img.shields.io/badge/python-3-blue.svg) [![DOI](https://zenodo.org/badge/598109632.svg)](https://doi.org/10.5281/zenodo.13938602) DataSAIL: [![platforms](https://anaconda.org/kalininalab/datasail/badges/platforms.svg)](https://anaconda.org/kalininalab/datasail) DataSAIL-lite: [![platforms](https://anaconda.org/kalininalab/datasail-lite/badges/platforms.svg)](https://anaconda.org/kalininalab/datasail-lite) DataSAIL is a tool that splits data while minimizing Information Leakage. This tool formulates the splitting of a dataset as a constrained minimization problem and computes the assignment of data points to splits while minimizing the objective function that accounts for information leakage. Internally, DataSAIL uses disciplined quasi-convex programming and binary quadratic programs to formulate the optimization task. DataSAIL utilizes solves like [SCIP](https://scipopt.org/), one of the fastest non-commercial solvers for this type of problem, and [MOSEK](https://mosek.com), a commercial solver that distributes free licenses for academic use. There are other options; please check the documentation. Apart from the short overview, you can find a more detailed explanation of the tool on [ReadTheDocs](https://datasail.readthedocs.io/en/latest/index.html). ## Installation DataSAIL is installable from [conda](https://anaconda.org/kalininalab/datasail) using [mamba](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html>). using ````shell mamba create -n sail -c conda-forge -c kalininalab -c bioconda datasail conda activate sail pip install grakel ```` to install it into a new empty environment or ````shell mamba install -c conda-forge -c kalininalab -c bioconda -c mosek datasail pip install grakel ```` to install DataSAIL in an already existing environment. Alternatively, one can install DataSAIL-lite from conda. DataSAIL-lite is a version of DataSAIL that does not install all clustering algorithms as the standard DataSAIL. Installing either package usually takes less than 5 minutes. DataSAIL is available for Python 3.9 to Python 3.12. ## Usage DataSAIL is installed as a command-line tool. So, in the conda environment, DataSAIL has been installed to, you can run ````shell datasail --e-type P --e-data <path_to_fasta> --e-sim mmseqs --output <path_to_output_path> --technique C1e ```` to split a set of proteins that have been clustered using mmseqs. For a full list of arguments, run `datasail -h` and checkout [ReadTheDocs](https://datasail.readthedocs.io/en/latest/index.html). There is a more detailed explanation of the arguments and example notebooks. The runtime largy depends on the number and type of splits to be computed and the size of the dataset. For small datasets (less then 10k samples) DataSAIL finished within minutes. On large datasets (more than 100k samples) it can take several hours to complete. ## When to use DataSAIL and when not to use One can distinguish two main ways to train a machine-learning model on biological data. * Either the model shall be applied to data substantially different from the data to train on. In this case, it is essential to have test cases that correctly model this real-world application scenario by being as dissimilar as possible to the training data. * Or the training dataset already covers the whole space of possible samples shown to the model. DataSAIL is created to compute complex data splits by separating data based on similarities. This makes complex data splits for the first scenario. So, you can use DataSAIL when your model is applied to data different from your training data but not if the data in the application is more or less the same as in the training. ## Citation If you used DataSAIL to split your data, please cite DataSAIL in your publication. ```` @article{joeres2025datasail, title={Data splitting to avoid information leakage with DataSAIL}, author={Joeres, Roman and Blumenthal, David B. and Kalinina, Olga V}, journal={Nature Communications}, volume={16}, pages={3337}, year={2025}, doi={10.1038/s41467-025-58606-8}, } ````\n",
                "dependencies": "from setuptools import setup, find_packages from datasail.version import __version__ with open(\"README.md\", \"r\") as desc_file: long_description = desc_file.read() setup( name=\"DataSAIL\", version=__version__, description=\"Data Splitting Against Information Leaking\", long_description=long_description, long_description_content_type=\"text/markdown\", license='MIT', author=\"Roman Joeres\", maintainer=\"Roman Joeres\", classifiers=[ \"Development Status :: 5 - Production/Stable\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Intended Audience :: Science/Research\", \"Natural Language :: English\", \"Topic :: Scientific/Engineering :: Bio-Informatics\", ], packages=[\"datasail\"], include_package_data=True, # packages=find_packages(), # include_package_data=False, install_requires=[], python_requires=\">=3.9, <3.13.0\", keywords=\"bioinformatics\", )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/dcache",
            "repo_link": "https://github.com/dCache/dcache",
            "content": {
                "codemeta": "",
                "readme": "common: Test commit\nFritz Heiden\n\ndCache ====== <img src=\"dCache.png\" height=\"165\" width=\"200\"> __dCache__ is a system for storing and retrieving huge amounts of data, distributed among a large number of heterogeneous server nodes, under a single virtual filesystem tree with a variety of standard access methods. Depending on the Persistency Model, dCache provides methods for exchanging data with backend (tertiary) Storage Systems as well as space management, pool attraction, dataset replication, hot spot determination and recovery from disk or node failures. Connected to a tertiary storage system, the cache simulates unlimited direct access storage space. Data exchanges to and from the underlying HSM are performed automatically and invisibly to the user. Beside HEP specific protocols, data in dCache can be accessed via __NFSv4.1 (pNFS)__, __FTP__ as well as through __WebDav__. [![DOI](https://zenodo.org/badge/9113580.svg)](https://zenodo.org/badge/latestdoi/9113580) Documentation ============= [The dCache book](docs/TheBook/src/main/markdown/index.md) [User Guide](docs/UserGuide/src/main/markdown/index.md) Getting Started =============== The file [BUILDING.md](BUILDING.md) describes how to compile dCache code and build various packages. The file also describes how to create the __system-test__ deployment, which provides a quick and easy way to get a working dCache. Running system-test requires no special privileges and all the generated files reside within the code-base. There are also packages of stable releases at https://www.dcache.org/downloads/. License ======= The project is licensed under __AGPL v3__. Some parts licensed under __BSD__ and __LGPL__. See the source code for details. For more info, check the official [dCache.ORG](http://www.dcache.org) web page. Contributors ============ dCache is a joint effort between [Deutsches Elektronen-Synchrotron DESY](http://www.desy.de), [Fermi National Accelerator Laboratory](http://www.fnal.gov) and [Nordic DataGrid Facility](http://www.ndgf.org). Contributions are welcome! Please check out our [CONTRIBUTING guide](CONTRIBUTING.md).\n",
                "dependencies": "<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>org.dcache</groupId> <version>11.0.0-SNAPSHOT</version> <artifactId>dcache-parent</artifactId> <packaging>pom</packaging> <name>dCache parent</name> <url>http://www.dcache.org/</url> <licenses> <license> <name>GNU Affero General Public License Version 3</name> <url>http://www.gnu.org/licenses/agpl-3.0.txt</url> <distribution>manual</distribution> </license> <license> <name>GNU Lesser General Public License Version 3</name> <url>http://www.gnu.org/licenses/lgpl-3.0.txt</url> <distribution>manual</distribution> </license> <license> <name>Fermitools Software Legal Information (Modified BSD License)</name> <distribution>manual</distribution> </license> </licenses> <organization> <name>dCache.org</name> <url>http://www.dcache.org/</url> </organization> <ciManagement> <system>jenkins</system> <url>https://ci.dcache.org/job/dCache-master/</url> </ciManagement> <distributionManagement> <downloadUrl>https://download.dcache.org/nexus/content/repositories/releases/</downloadUrl> <repository> <uniqueVersion>false</uniqueVersion> <id>dcache.release.repository</id> <name>dCache.org release repository</name> <url>https://download.dcache.org/nexus/content/repositories/releases/</url> <layout>default</layout> </repository> <snapshotRepository> <uniqueVersion>true</uniqueVersion> <id>dcache.snapshot.repository</id> <name>dCache.org snapshot repository</name> <url>https://download.dcache.org/nexus/content/repositories/snapshots/</url> <layout>default</layout> </snapshotRepository> </distributionManagement> <properties> <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> <java.version>17</java.version> <argLine>--add-modules=java.security.jgss</argLine> <version.slf4j>1.7.32</version.slf4j> <version.milton>2.7.3.0-dcache-1</version.milton> <version.spring>5.3.39</version.spring> <!-- Remember to sync aspectj version in dcache.properties --> <version.aspectj>1.9.21.2</version.aspectj> <version.smc>6.6.0</version.smc> <version.xerces>2.12.0</version.xerces> <version.jetty>9.4.57.v20241219</version.jetty> <version.xrootd4j>4.6.0</version.xrootd4j> <version.jersey>2.41</version.jersey> <version.dcache-view>3.0.0</version.dcache-view> <version.dcache-view-admin>3.0.0</version.dcache-view-admin> <version.netty>4.1.118.Final</version.netty> <version.dcache>${project.version}</version.dcache> <version.swagger-ui>3.23.11</version.swagger-ui> <!-- Remember to update modules/logback-test-config/pom.xml when upgrading logback. --> <version.logback>1.2.13</version.logback> <version.logback.contrib>0.1.5</version.logback.contrib> <version.jackson>2.14.0</version.jackson> <version.jna>5.4.0</version.jna> <jmh.version>1.35</jmh.version> <version.dropwizard>4.1.29</version.dropwizard> <version.nfs4j>0.26.0</version.nfs4j> <version.spring-integration-kafka>5.5.11</version.spring-integration-kafka> <version.spring_kafka>2.9.11</version.spring_kafka> <version.kafka>3.1.0</version.kafka> <bouncycastle.bcprov>bcprov-jdk18on</bouncycastle.bcprov> <bouncycastle.bcpkix>bcpkix-jdk18on</bouncycastle.bcpkix> <bouncycastle.version>1.78</bouncycastle.version> <datanucleus-core.version>6.0.6</datanucleus-core.version> <datanucleus.plugin.version>6.0.0-release</datanucleus.plugin.version> <asm.version>9.5</asm.version> <asm-tools.version>5.0.3</asm-tools.version> </properties> <scm> <url>https://github.com/dCache/dcache</url> <connection>scm:git:https://github.com/dCache/dcache.git</connection> <developerConnection>scm:git:ssh://git@github.com/dCache/dcache.git</developerConnection> <tag>8.2</tag> </scm> <repositories> <repository> <id>dcache.repository</id> <url>https://download.dcache.org/nexus/content/groups/public</url> </repository> </repositories> <pluginRepositories> <pluginRepository> <id>dcache.repository</id> <url>https://download.dcache.org/nexus/content/groups/public</url> </pluginRepository> </pluginRepositories> <dependencyManagement> <dependencies> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version>${version.slf4j}</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>jul-to-slf4j</artifactId> <version>${version.slf4j}</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>jcl-over-slf4j</artifactId> <version>${version.slf4j}</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>log4j-over-slf4j</artifactId> <version>${version.slf4j}</version> </dependency> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> <version>${version.logback}</version> </dependency> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-core</artifactId> <version>${version.logback}</version> </dependency> <dependency> <groupId>org.codehaus.janino</groupId> <artifactId>janino</artifactId> <version>3.0.6</version> </dependency> <dependency> <groupId>org.apache.curator</groupId> <artifactId>curator-recipes</artifactId> <version>5.7.0</version> <exclusions> <exclusion> <groupId>org.slf4j</groupId> <artifactId>slf4j-log4j12</artifactId> </exclusion> <exclusion> <groupId>org.apache.zookeeper</groupId> <artifactId>zookeeper</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.apache.zookeeper</groupId> <artifactId>zookeeper</artifactId> <version>3.8.4</version> <exclusions> <exclusion> <groupId>com.sun.jmx</groupId> <artifactId>jmxri</artifactId> </exclusion> <exclusion> <groupId>com.sun.jdmk</groupId> <artifactId>jmxtools</artifactId> </exclusion> <exclusion> <groupId>javax.jms</groupId> <artifactId>jms</artifactId> </exclusion> <exclusion> <groupId>org.slf4j</groupId> <artifactId>slf4j-log4j12</artifactId> </exclusion> <exclusion> <groupId>log4j</groupId> <artifactId>log4j</artifactId> </exclusion> </exclusions> </dependency> <!-- Required for ZooKeeper 3.6.x https://issues.apache.org/jira/browse/ZOOKEEPER-3647 --> <dependency> <groupId>io.dropwizard.metrics</groupId> <artifactId>metrics-core</artifactId> <version>${version.dropwizard}</version> <exclusions> <exclusion> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>jline</groupId> <artifactId>jline</artifactId> <version>2.14.6</version> </dependency> <dependency> <groupId>com.github.spotbugs</groupId> <artifactId>spotbugs-annotations</artifactId> <version>3.1.12</version> <scope>compile</scope> </dependency> <dependency> <groupId>com.google.code.findbugs</groupId> <artifactId>jsr305</artifactId> <version>2.0.1</version> </dependency> <dependency> <groupId>com.google.guava</groupId> <artifactId>guava</artifactId> <version>32.0.0-jre</version> </dependency> <dependency> <groupId>eu.eu-emi.security</groupId> <artifactId>canl</artifactId> <version>2.8.3</version> </dependency> <dependency> <groupId>org.italiangrid</groupId> <artifactId>voms-api-java</artifactId> <version>3.3.0</version> <exclusions> <exclusion> <groupId>org.bouncycastle</groupId> <artifactId>bcprov-jdk15on</artifactId> </exclusion> <exclusion> <groupId>org.bouncycastle</groupId> <artifactId>bcpkix-jdk15on</artifactId> </exclusion> <exclusion> <groupId>eu.eu-emi.security</groupId> <artifactId>canl</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>gov.fnal</groupId> <artifactId>vox-alldepends</artifactId> <version>1.0</version> </dependency> <dependency> <groupId>gov.fnal</groupId> <artifactId>vox-security</artifactId> <version>1.0</version> </dependency> <dependency> <groupId>net.java.dev.jna</groupId> <artifactId>jna</artifactId> <version>${version.jna}</version> </dependency> <dependency> <groupId>net.java.dev.jna</groupId> <artifactId>jna-platform</artifactId> <version>${version.jna}</version> </dependency> <dependency> <groupId>javax.xml</groupId> <artifactId>jaxrpc-api</artifactId> <version>1.1</version> </dependency> <dependency> <groupId>com.sun.xml.rpc</groupId> <artifactId>jaxrpc-impl</artifactId> <version>1.1.3_01</version> </dependency> <dependency> <groupId>com.sun.xml.rpc</groupId> <artifactId>jaxrpc-spi</artifactId> <version>1.1.3_01</version> </dependency> <dependency> <groupId>axis</groupId> <artifactId>axis</artifactId> <version>1.4</version> <exclusions> <exclusion> <groupId>commons-logging</groupId> <artifactId>commons-logging</artifactId> </exclusion> <!-- 'servlet-api' conflicts with Jetty-supplied 'javax.servlet' jar. --> <exclusion> <groupId>javax.servlet</groupId> <artifactId>servlet-api</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>terapaths</groupId> <artifactId>example-client</artifactId> <version>1.0</version> </dependency> <dependency> <groupId>org.jdom</groupId> <artifactId>jdom</artifactId> <version>1.1</version> </dependency> <dependency> <groupId>com.zaxxer</groupId> <artifactId>HikariCP</artifactId> <version>5.0.1</version> </dependency> <dependency> <groupId>org.postgresql</groupId> <artifactId>postgresql</artifactId> <version>42.3.9</version> </dependency> <dependency> <groupId>io.milton</groupId> <artifactId>milton-server-ce</artifactId> <version>${version.milton}</version> <exclusions> <exclusion> <groupId>commons-logging</groupId> <artifactId>commons-logging</artifactId> </exclusion> <exclusion> <groupId>io.milton</groupId> <artifactId>milton-mail-api</artifactId> </exclusion> <exclusion> <groupId>io.milton</groupId> <artifactId>milton-mail-server</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>io.netty</groupId> <artifactId>netty-codec-http</artifactId> <version>${version.netty}</version> </dependency> <dependency> <groupId>io.netty</groupId> <artifactId>netty-codec-haproxy</artifactId> <version>${version.netty}</version> </dependency> <dependency> <groupId>io.netty</groupId> <artifactId>netty-handler</artifactId> <version>${version.netty}</version> </dependency> <dependency> <groupId>io.netty</groupId> <artifactId>netty-tcnative-boringssl-static</artifactId> <version>2.0.48.Final</version> </dependency> <dependency> <groupId>com.sleepycat</groupId> <artifactId>je</artifactId> <version>7.5.11</version> </dependency> <dependency> <groupId>com.github.parboiled1</groupId> <artifactId>grappa</artifactId> <version>1.0.4</version> <exclusions> <exclusion> <groupId>org.ow2.asm</groupId> <artifactId>asm-debug-all</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.ow2.asm</groupId> <artifactId>asm</artifactId> <version>${asm.version}</version> </dependency> <dependency> <groupId>org.ow2.asm</groupId> <artifactId>asm-tree</artifactId> <version>${asm-tools.version}</version> </dependency> <dependency> <groupId>org.ow2.asm</groupId> <artifactId>asm-analysis</artifactId> <version>${asm-tools.version}</version> </dependency> <dependency> <groupId>org.ow2.asm</groupId> <artifactId>asm-util</artifactId> <version>${asm-tools.version}</version> </dependency> <dependency> <groupId>org.datanucleus</groupId> <artifactId>datanucleus-core</artifactId> <version>${datanucleus-core.version}</version> </dependency> <dependency> <groupId>org.datanucleus</groupId> <artifactId>datanucleus-api-jdo</artifactId> <version>6.0.1</version> </dependency> <dependency> <groupId>org.datanucleus</groupId> <artifactId>datanucleus-jdo-query</artifactId> <version>6.0.1</version> </dependency> <dependency> <groupId>org.datanucleus</groupId> <artifactId>javax.jdo</artifactId> <version>3.2.0-release</version> </dependency> <dependency> <groupId>org.datanucleus</groupId> <artifactId>datanucleus-rdbms</artifactId> <version>${datanucleus-core.version}</version> </dependency> <dependency> <groupId>org.datanucleus</groupId> <artifactId>datanucleus-xml</artifactId> <version>6.0.0-release</version> </dependency> <dependency> <groupId>org.datanucleus</groupId> <artifactId>datanucleus-cache</artifactId> <version>6.0.0-release</version> </dependency> <dependency> <groupId>net.sf.saxon</groupId> <artifactId>saxon-dom</artifactId> <version>8.7</version> </dependency> <dependency> <groupId>org.eclipse.jetty</groupId> <artifactId>jetty-util</artifactId> <version>${version.jetty}</version> </dependency> <dependency> <groupId>org.eclipse.jetty</groupId> <artifactId>jetty-io</artifactId> <version>${version.jetty}</version> </dependency> <dependency> <groupId>org.eclipse.jetty</groupId> <artifactId>jetty-server</artifactId> <version>${version.jetty}</version> </dependency> <dependency> <groupId>org.eclipse.jetty</groupId> <artifactId>jetty-deploy</artifactId> <version>${version.jetty}</version> </dependency> <dependency> <groupId>org.eclipse.jetty</groupId> <artifactId>jetty-webapp</artifactId> <version>${version.jetty}</version> </dependency> <dependency> <groupId>org.eclipse.jetty</groupId> <artifactId>jetty-servlets</artifactId> <version>${version.jetty}</version> </dependency> <dependency> <groupId>org.eclipse.jetty</groupId> <artifactId>jetty-plus</artifactId> <version>${version.jetty}</version> </dependency> <dependency> <groupId>org.eclipse.jetty</groupId> <artifactId>jetty-security</artifactId> <version>${version.jetty}</version> </dependency> <dependency> <groupId>org.eclipse.jetty</groupId> <artifactId>jetty-rewrite</artifactId> <version>${version.jetty}</version> </dependency> <dependency> <groupId>org.eclipse.jetty.toolchain</groupId> <artifactId>jetty-schemas</artifactId> <version>3.1</version> </dependency> <dependency> <groupId>org.glassfish.jersey.containers</groupId> <artifactId>jersey-container-servlet</artifactId> <version>${version.jersey}</version> </dependency> <dependency> <groupId>org.glassfish.jersey.core</groupId> <artifactId>jersey-client</artifactId> <version>${version.jersey}</version> </dependency> <dependency> <groupId>org.glassfish.jersey.media</groupId> <artifactId>jersey-media-json-jackson</artifactId> <version>${version.jersey}</version> </dependency> <dependency> <groupId>org.glassfish.jersey.ext</groupId> <artifactId>jersey-spring5</artifactId> <version>${version.jersey}</version> </dependency> <dependency> <groupId>org.glassfish.jersey.media</groupId> <artifactId>jersey-media-sse</artifactId> <version>${version.jersey}</version> </dependency> <dependency> <groupId>org.antlr</groupId> <artifactId>ST4</artifactId> <version>4.0.8</version> </dependency> \t <dependency> \t <groupId>org.apache.sshd</groupId> \t <artifactId>sshd-core</artifactId> \t <version>2.15.0</version> \t </dependency> <dependency> <groupId>org.liquibase</groupId> <artifactId>liquibase-core</artifactId> <version>4.29.2</version> </dependency> <!-- needed by liquibase cli --> <dependency> <groupId>info.picocli</groupId> <artifactId>picocli</artifactId> <version>4.7.6</version> </dependency> <dependency> <groupId>org.springframework</groupId> <artifactId>spring-beans</artifactId> <version>${version.spring}</version> </dependency> <dependency> <groupId>org.springframework</groupId> <artifactId>spring-context</artifactId> <version>${version.spring}</version> </dependency> <dependency> <groupId>org.springframework</groupId> <artifactId>spring-core</artifactId> <version>${version.spring}</version> <exclusions> <exclusion> <groupId>commons-logging</groupId> <artifactId>commons-logging</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.springframework</groupId> <artifactId>spring-expression</artifactId> <version>${version.spring}</version> </dependency> <dependency> <groupId>org.springframework</groupId> <artifactId>spring-jdbc</artifactId> <version>${version.spring}</version> </dependency> <dependency> <groupId>org.springframework</groupId> <artifactId>spring-orm</artifactId> <version>${version.spring}</version> </dependency> <dependency> <groupId>org.springframework</groupId> <artifactId>spring-web</artifactId> <version>${version.spring}</version> </dependency> <dependency> <groupId>org.springframework</groupId> <artifactId>spring-aspects</artifactId> <version>${version.spring}</version> </dependency> <dependency> <groupId>org.springframework</groupId> <artifactId>spring-tx</artifactId> <version>${version.spring}</version> </dependency> <dependency> <groupId>org.aspectj</groupId> <artifactId>aspectjrt</artifactId> <version>${version.aspectj}</version> </dependency> <dependency> <groupId>org.aspectj</groupId> <artifactId>aspectjweaver</artifactId> <version>${version.aspectj}</version> </dependency> <dependency> <groupId>net.sf.smc</groupId> <artifactId>statemap</artifactId> <version>${version.smc}</version> </dependency> <dependency> <groupId>com.h2database</groupId> <artifactId>h2</artifactId> <version>1.4.199</version> </dependency> <dependency> <!-- https://sourceforge.net/p/hsqldb/bugs/1341/ prevents updating to 2.3.2 --> <groupId>org.hsqldb</groupId> <artifactId>hsqldb</artifactId> <version>2.3.1</version> </dependency> <dependency> <groupId>java.jndi</groupId> <artifactId>nis</artifactId> <version>1.2.1</version> </dependency> <dependency> <groupId>org.json</groupId> <artifactId>json</artifactId> <version>20231013</version> </dependency> <dependency> <groupId>com.thaiopensource</groupId> <artifactId>jing</artifactId> <version>20091111</version> <scope>test</scope> <exclusions> <exclusion> <groupId>xml-apis</groupId> <artifactId>xml-apis</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.dcache</groupId> <artifactId>xrootd4j</artifactId> <version>${version.xrootd4j}</version> </dependency> <dependency> <groupId>org.dcache</groupId> <artifactId>xrootd4j-gsi</artifactId> <version>${version.xrootd4j}</version> <exclusions> <exclusion> <groupId>org.bouncycastle</groupId> <artifactId>bcprov-jdk15on</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.dcache</groupId> <artifactId>xrootd4j-unix</artifactId> <version>${version.xrootd4j}</version> </dependency> <dependency> <groupId>org.dcache</groupId> <artifactId>xrootd4j-scitokens</artifactId> <version>${version.xrootd4j}</version> </dependency> <dependency> <groupId>org.dcache</groupId> <artifactId>xrootd4j-ztn</artifactId> <version>${version.xrootd4j}</version> </dependency> <dependency> <groupId>org.dcache</groupId> <artifactId>xrootd4j-authz-plugin-alice</artifactId> <version>1.2.0</version> <exclusions> <exclusion> <groupId>org.bouncycastle</groupId> <artifactId>bcprov-jdk15on</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.dcache</groupId> <artifactId>dcache-view</artifactId> <version>${version.dcache-view}</version> </dependency> <dependency> <groupId>org.dcache</groupId> <artifactId>dcache-view-admin</artifactId> <version>${version.dcache-view-admin}</version> </dependency> <dependency> <groupId>org.bouncycastle</groupId> <artifactId>${bouncycastle.bcpkix}</artifactId> <version>${bouncycastle.version}</version> </dependency> <dependency> <groupId>org.bouncycastle</groupId> <artifactId>${bouncycastle.bcprov}</artifactId> <version>${bouncycastle.version}</version> </dependency> <dependency> <groupId>xerces</groupId> <artifactId>xercesImpl</artifactId> <version>${version.xerces}</version> </dependency> <dependency> <groupId>com.google.code.gson</groupId> <artifactId>gson</artifactId> <version>2.8.9</version> </dependency> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-compress</artifactId> <version>1.26.0</version> </dependency> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-math3</artifactId> <version>3.6.1</version> </dependency> <dependency> <groupId>org.python</groupId> <artifactId>jython-slim</artifactId> <version>2.7.3</version> <exclusions> <exclusion> <groupId>com.github.jnr</groupId> <artifactId>jffi</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.dcache</groupId> <artifactId>nfs4j-core</artifactId> <version>${version.nfs4j}</version> </dependency> <dependency> <groupId>org.dcache</groupId> <artifactId>rquota</artifactId> <version>${version.nfs4j}</version> </dependency> <dependency> <groupId>com.github.nitram509</groupId> <artifactId>jmacaroons</artifactId> <version>0.3.1</version> </dependency> <dependency> <groupId>org.apache.httpcomponents</groupId> <artifactId>httpclient</artifactId> <version>4.5.13</version> <exclusions> <exclusion> <groupId>commons-logging</groupId> <artifactId>commons-logging</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.apache.httpcomponents</groupId> <artifactId>httpcore</artifactId> <version>4.4.6</version> <exclusions> <exclusion> <groupId>commons-logging</groupId> <artifactId>commons-logging</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>com.google.jimfs</groupId> <artifactId>jimfs</artifactId> <version>1.1</version> </dependency> <dependency> <groupId>org.apache.maven.archetype</groupId> <artifactId>archetype-packaging</artifactId> <version>2.4</version> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.13.1</version> <exclusions> <exclusion> <groupId>org.hamcrest</groupId> <artifactId>hamcrest-core</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.dcache</groupId> <artifactId>ldap4testing</artifactId> <version>1.0</version> </dependency> <!-- used by alarms, needs to be deployed --> <dependency> <groupId>javax.mail</groupId> <artifactId>mail</artifactId> <version>1.4.7</version> </dependency> <dependency> <groupId>org.mongodb</groupId> <artifactId>mongodb-driver-sync</artifactId> <version>4.11.1</version> </dependency> <dependency> <groupId>org.springframework.integration</groupId> <artifactId>spring-integration-kafka</artifactId> <version>${version.spring-integration-kafka}</version> </dependency> <dependency> <groupId>org.springframework.kafka</groupId> <artifactId>spring-kafka</artifactId> <exclusions> <exclusion> <groupId>org.slf4j</groupId> <artifactId>slf4j-log4j12</artifactId> </exclusion> </exclusions> <version>${version.spring_kafka}</version> </dependency> <dependency> <groupId>org.apache.kafka</groupId> <artifactId>kafka_2.12</artifactId> <version>${version.kafka}</version> <exclusions> <exclusion> <groupId>org.apache.zookeeper</groupId> <artifactId>zookeeper</artifactId> </exclusion> <exclusion> <groupId>org.slf4j</groupId> <artifactId>slf4j-log4j12</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.apache.kafka</groupId> <artifactId>kafka-clients</artifactId> <exclusions> <exclusion> <groupId>org.apache.zookeeper</groupId> <artifactId>zookeeper</artifactId> </exclusion> <exclusion> <groupId>org.slf4j</groupId> <artifactId>slf4j-log4j12</artifactId> </exclusion> </exclusions> <version>${version.kafka}</version> </dependency> <dependency> <groupId>com.github.danielwegener</groupId> <artifactId>logback-kafka-appender</artifactId> <version>0.2.0-RC1</version> </dependency> <!-- ch.qos.logback.contrib.jackson.JacksonJsonFormatter --> <dependency> <groupId>ch.qos.logback.contrib</groupId> <artifactId>logback-jackson</artifactId> <version>${version.logback.contrib}</version> </dependency> <!-- ch.qos.logback.contrib.json.classic.JsonLayout --> <dependency> <groupId>ch.qos.logback.contrib</groupId> <artifactId>logback-json-classic</artifactId> <version>${version.logback.contrib}</version> </dependency> <!-- com.fasterxml.jackson.databind.ObjectMapper --> <dependency> <groupId>com.fasterxml.jackson.core</groupId> <artifactId>jackson-databind</artifactId> <version>${version.jackson}</version> </dependency> <dependency> <groupId>com.fasterxml.jackson.core</groupId> <artifactId>jackson-annotations</artifactId> <version>${version.jackson}</version> </dependency> <dependency> <groupId>com.fasterxml.jackson.core</groupId> <artifactId>jackson-core</artifactId> <version>${version.jackson}</version> </dependency> <dependency> <groupId>org.glassfish</groupId> <artifactId>javax.json</artifactId> <version>1.0.4</version> </dependency> <dependency> <groupId>org.webjars</groupId> <artifactId>swagger-ui</artifactId> <version>${version.swagger-ui}</version> </dependency> <dependency> <groupId>io.swagger</groupId> <artifactId>swagger-jersey2-jaxrs</artifactId> <version>1.6.2</version> </dependency> <dependency> <groupId>org.springframework.plugin</groupId> <artifactId>spring-plugin-core</artifactId> <version>1.2.0.RELEASE</version> </dependency> <dependency> <!-- enforce version as javassist used powermock --> <groupId>org.javassist</groupId> <artifactId>javassist</artifactId> <version>3.25.0-GA</version> </dependency> <dependency> <groupId>org.reflections</groupId> <artifactId>reflections</artifactId> <version>0.9.12</version> </dependency> \t <dependency> <groupId>com.github.npathai</groupId> <artifactId>hamcrest-optional</artifactId> <version>2.0.0</version> <exclusions> <exclusion> <groupId>org.hamcrest</groupId> <artifactId>hamcrest-core</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.hamcrest</groupId> <artifactId>hamcrest</artifactId> <version>2.2</version> </dependency> <dependency> <groupId>org.openjdk.jmh</groupId> <artifactId>jmh-core</artifactId> <version>${jmh.version}</version> </dependency> <dependency> <groupId>org.openjdk.jmh</groupId> <artifactId>jmh-generator-annprocess</artifactId> <version>${jmh.version}</version> </dependency> <dependency> <groupId>com.github.erosb</groupId> <artifactId>everit-json-schema</artifactId> <version>1.14.1</version> </dependency> </dependencies> </dependencyManagement> <dependencies> <dependency> <groupId>org.hamcrest</groupId> <artifactId>hamcrest</artifactId> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-api-mockito2</artifactId> <version>2.0.4</version> <scope>test</scope> </dependency> <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-core</artifactId> <version>3.2.4</version> <scope>test</scope> </dependency> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-module-junit4</artifactId> <version>2.0.4</version> <scope>test</scope> </dependency> <dependency> <groupId>org.dcache</groupId> <artifactId>logback-test-config</artifactId> <version>${project.version}</version> <scope>test</scope> </dependency> </dependencies> <build> <pluginManagement> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-site-plugin</artifactId> <version>4.0.0-M5</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> <showDeprecation>true</showDeprecation> <release>${java.version}</release> </configuration> </plugin> <plugin> <!-- https://github.com/mojohaus/aspectj-maven-plugin/pull/45 --> <!-- <groupId>org.codehaus.mojo</groupId> --> <groupId>com.nickwongdev</groupId> <artifactId>aspectj-maven-plugin</artifactId> <version>1.12.6</version> <dependencies> <dependency> <groupId>org.aspectj</groupId> <artifactId>aspectjtools</artifactId> <version>${version.aspectj}</version> </dependency> </dependencies> <configuration> <XterminateAfterCompilation>true</XterminateAfterCompilation> <source>${java.version}</source> <target>${java.version}</target> <complianceLevel>${java.version}</complianceLevel> <XhasMember>true</XhasMember> <sources> <source> <basedir>${project.basedir}/src/main/aspect</basedir> </source> </sources> </configuration> <executions> <execution> <goals> <goal>compile</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-resources-plugin</artifactId> <!-- Cannot update to 3.2.0 or higher due to https://github.com/spring-projects/spring-boot/issues/24346 --> <!-- Not UTF-8 Characters are not being tolerated by the plugin and hence causing the issue --> <version>3.1.0</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-dependency-plugin</artifactId> <version>3.5.0</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-assembly-plugin</artifactId> <version>3.5.0</version> <configuration> <tarLongFileMode>gnu</tarLongFileMode> </configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-war-plugin</artifactId> <version>3.3.2</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-clean-plugin</artifactId> <version>3.2.0</version> <configuration> <fast>true</fast> </configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-jar-plugin</artifactId> <version>3.3.0</version> </plugin> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>buildnumber-maven-plugin</artifactId> <version>3.0.0</version> </plugin> <plugin> <groupId>com.lukegb.mojo</groupId> <artifactId>gitdescribe-maven-plugin</artifactId> <version>3.0</version> </plugin> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>build-helper-maven-plugin</artifactId> <version>3.3.0</version> </plugin> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>exec-maven-plugin</artifactId> <version>3.1.0</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>3.1.2</version> <configuration> <!-- Disable stack-trace trimming as surefire makes bad decisions too often. --> <trimStackTrace>false</trimStackTrace> </configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-release-plugin</artifactId> <version>3.0.0-M7</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-deploy-plugin</artifactId> <version>3.1.0</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-install-plugin</artifactId> <version>3.1.0</version> </plugin> <plugin> <groupId>org.datanucleus</groupId> <artifactId>datanucleus-maven-plugin</artifactId> <version>${datanucleus.plugin.version}</version> <dependencies> <dependency> <groupId>org.datanucleus</groupId> <artifactId>datanucleus-core</artifactId> <version>${datanucleus-core.version}</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>log4j-over-slf4j</artifactId> <version>${version.slf4j}</version> </dependency> <dependency> <groupId>org.dcache</groupId> <artifactId>logback-test-config</artifactId> <version>${project.version}</version> </dependency> </dependencies> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-antrun-plugin</artifactId> <version>3.1.0</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-failsafe-plugin</artifactId> <version>3.0.0-M9</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-pmd-plugin</artifactId> <version>3.20.0</version> <configuration> <sourceEncoding>utf-8</sourceEncoding> <minimumTokens>100</minimumTokens> <targetJdk>${java.version}</targetJdk> <!-- Rely on Jenkins plugin to do source-code cross-referencing --> <linkXRef>false</linkXRef> </configuration> </plugin> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>axistools-maven-plugin</artifactId> <version>1.4</version> </plugin> <plugin> <groupId>io.github.lukasmansour</groupId> <artifactId>patch-maven-plugin</artifactId> <version>1.0</version> </plugin> <plugin> <groupId>org.jacoco</groupId> <artifactId>jacoco-maven-plugin</artifactId> <version>0.8.8</version> </plugin> <plugin> <groupId>com.ruleoftech</groupId> <artifactId>markdown-page-generator-plugin</artifactId> <version>2.4.0</version> </plugin> <plugin> <groupId>org.codehaus.gmaven</groupId> <artifactId>groovy-maven-plugin</artifactId> <version>2.1.1</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-enforcer-plugin</artifactId> <!-- Remember to update version in modules/logback-test-config/pom.xml when upgrading --> <version>3.2.1</version> </plugin> <plugin> <groupId>org.gaul</groupId> <artifactId>modernizer-maven-plugin</artifactId> <version>2.7.0</version> </plugin> <plugin> <groupId>com.github.spotbugs</groupId> <artifactId>spotbugs-maven-plugin</artifactId> <version>4.7.3.0</version> </plugin> <plugin> <groupId>de.chkal.maven</groupId> <artifactId>gitlab-code-quality-plugin</artifactId> <version>1.0.2</version> </plugin> </plugins> </pluginManagement> <plugins> <plugin> <groupId>org.gaul</groupId> <artifactId>modernizer-maven-plugin</artifactId> <configuration> <javaVersion>${java.version}</javaVersion> <failOnViolations>false</failOnViolations> </configuration> <executions> <execution> <id>modernizer</id> <phase>verify</phase> <goals> <goal>modernizer</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-enforcer-plugin</artifactId> <executions> <execution> <id>enforce-maven</id> <goals> <goal>enforce</goal> </goals> <configuration> <rules> <requireMavenVersion> <version>3.5.0</version> </requireMavenVersion> </rules> </configuration> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <configuration> <release>${java.version}</release> </configuration> </plugin> <plugin> <groupId>io.github.git-commit-id</groupId> <artifactId>git-commit-id-maven-plugin</artifactId> <version>5.0.0</version> <executions> <execution> <goals> <goal>revision</goal> </goals> </execution> </executions> <configuration> <dotGitDirectory>${project.basedir}/.git</dotGitDirectory> <skipPoms>false</skipPoms> <injectAllReactorProjects>true</injectAllReactorProjects> <useNativeGit>false</useNativeGit> <gitDescribe> <always>true</always> <dirty>+dirty+${user.name}</dirty> </gitDescribe> </configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-jar-plugin</artifactId> <configuration> <archive> <index>true</index> <manifest> <addDefaultImplementationEntries>true</addDefaultImplementationEntries> </manifest> <manifestEntries> <Build-Time>${maven.build.timestamp}</Build-Time> <Implementation-Build>${git.commit.id.describe}</Implementation-Build> <Implementation-Branch>${git.branch}</Implementation-Branch> </manifestEntries> </archive> </configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <configuration> <includes> <include>**/*Test.class</include> <include>**/*Tests.class</include> </includes> <!-- dCache uses the singleton anti-pattern in way too many places. That unfortunately means we have to accept the overhead of forking each test run. --> <forkCount>1</forkCount> <reuseForks>false</reuseForks> <!-- Make Powermock compatible with JDK 17 --> <argLine> @{argLine} --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/java.util=ALL-UNNAMED --add-opens java.base/java.lang.reflect=ALL-UNNAMED --add-opens java.base/java.time=ALL-UNNAMED --add-opens java.base/java.time.format=ALL-UNNAMED --add-opens java.base/java.util.concurrent=ALL-UNNAMED --add-opens java.base/java.util.stream=ALL-UNNAMED </argLine> </configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-release-plugin</artifactId> <configuration> <autoVersionSubmodules>true</autoVersionSubmodules> <tagNameFormat>@{project.version}</tagNameFormat> </configuration> </plugin> <plugin> <groupId>com.github.spotbugs</groupId> <artifactId>spotbugs-maven-plugin</artifactId> <configuration> <omitVisitors>FindDeadLocalStores,UnreadFields,Naming,FindUncalledPrivateMethods,FormatStringChecker</omitVisitors> </configuration> </plugin> <plugin> <artifactId>maven-failsafe-plugin</artifactId> <executions> <execution> <goals> <goal>integration-test</goal> <goal>verify</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>de.chkal.maven</groupId> <artifactId>gitlab-code-quality-plugin</artifactId> <executions> <execution> <phase>verify</phase> <goals> <goal>generate</goal> </goals> </execution> </executions> </plugin> </plugins> </build> <reporting> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-project-info-reports-plugin</artifactId> <version>3.4.2</version> <reportSets> <reportSet> <reports> <report>cim</report> <report>scm</report> </reports> </reportSet> </reportSets> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-pmd-plugin</artifactId> <version>3.20.0</version> <configuration> <sourceEncoding>utf-8</sourceEncoding> <minimumTokens>100</minimumTokens> <targetJdk>${java.version}</targetJdk> </configuration> </plugin> </plugins> </reporting> <modules> <module>modules/logback-test-config</module> <module>modules/logback-console-config</module> <module>modules/common</module> <module>modules/common-cli</module> <module>modules/common-security</module> <module>modules/cells</module> <module>modules/gplazma2</module> <module>modules/gplazma2-alise</module> <module>modules/gplazma2-fermi</module> <module>modules/gplazma2-grid</module> <module>modules/gplazma2-krb5</module> <module>modules/gplazma2-jaas</module> <module>modules/gplazma2-nis</module> <module>modules/gplazma2-nsswitch</module> <module>modules/gplazma2-banfile</module> <module>modules/gplazma2-voms</module> <module>modules/gplazma2-kpwd</module> <module>modules/gplazma2-ldap</module> <module>modules/gplazma2-htpasswd</module> <module>modules/gplazma2-pyscript</module> <module>modules/gplazma2-oidc</module> <module>modules/gplazma2-oidc-te</module> <module>modules/gplazma2-omnisession</module> <module>modules/gplazma2-multimap</module> <module>modules/gplazma2-roles</module> <module>modules/gplazma2-scitoken</module> <module>modules/dcache-vehicles</module> <module>modules/dcache-nearline-spi</module> <module>modules/dcache</module> <module>modules/dcache-bulk</module> <module>modules/dcache-chimera</module> <module>modules/dcache-ftp</module> <module>modules/dcache-resilience</module> <module>modules/dcache-webdav</module> <module>modules/dcache-xrootd</module> <module>modules/dcache-dcap</module> <module>modules/dcache-gplazma</module> <module>modules/dcache-frontend</module> <module>modules/dcache-nfs</module> <module>modules/dcache-srm</module> <module>modules/dcache-history</module> <module>modules/dcache-info</module> <module>modules/dcache-spacemanager</module> <module>modules/dcache-qos</module> <module>modules/ftp-client</module> <module>modules/srm-common</module> <module>modules/srm-server</module> <module>modules/srm-client</module> <module>modules/acl-vehicles</module> <module>modules/acl</module> <module>modules/chimera</module> <module>modules/missingfiles-semsg</module> <module>modules/benchmarks</module> <module>plugins</module> <module>docs</module> <module>packages</module> <module>archetypes</module> </modules> <profiles> <profile> \t<id>code-coverage</id> \t<build> \t <plugins> <!-- Configured as described here: \t\t http://www.petrikainulainen.net/programming/maven/creating-code-coverage-reports-for-unit-and-integration-tests-with-the-jacoco-maven-plugin/ \t --> \t <plugin> \t <groupId>org.jacoco</groupId> \t <artifactId>jacoco-maven-plugin</artifactId> \t <executions> \t\t<!-- \t\t Prepares the property pointing to the JaCoCo runtime agent which \t\t is passed as VM argument when Maven the Surefire plugin is executed. \t\t--> \t\t<execution> \t\t <id>pre-unit-test</id> \t\t <goals> <goal>prepare-agent</goal> \t\t </goals> \t\t <configuration> <!-- Sets the path to the file which contains the execution data. --> <destFile>${project.build.directory}/coverage-reports/jacoco-ut.exec</destFile> \t\t </configuration> \t\t</execution> \t\t<!-- \t\t Ensures that the code coverage report for unit tests is created after \t\t unit tests have been run. \t\t--> \t\t<execution> \t\t <id>post-unit-test</id> \t\t <phase>test</phase> \t\t <goals> <goal>report</goal> \t\t </goals> \t\t <configuration> <!-- Sets the path to the file which contains the execution data. --> <dataFile>${project.build.directory}/coverage-reports/jacoco-ut.exec</dataFile> <!-- Sets the output directory for the code coverage report. --> <outputDirectory>${project.reporting.outputDirectory}/jacoco-ut</outputDirectory> \t\t </configuration> \t\t</execution> \t </executions> \t </plugin> \t </plugins> \t</build> </profile> </profiles> </project>\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/deeploki",
            "repo_link": "https://gitlab.com/qtb-hhu/marine/DeepLOKI",
            "content": {
                "codemeta": "",
                "readme": "# DeepLOKI Zooplankton plays a crucial role in the ocean's ecology, serving as a foundational component in the food chain by consuming phytoplankton or other zooplankton and furthermore influencing nutrient cycling. This pivotal role distinguishes them from other species that reside at higher trophic levels. The vertical distribution of zooplankton in the ocean is patchy, and its relation to hydrographical conditions cannot be fully deciphered using traditional net casts due to the large depth intervals sampled. Optical systems that continuously take images during the cast can help bridge this gap. The Lightframe On-sight Keyspecies Investigation (LOKI) concentrates zooplankton with a net that leads to a flow-through chamber with a camera taking images with up to 20 frames sec−1. These high-resolution images allow for the determination of zooplankton taxa, often even to genus or species level, and, in the case of copepods, developmental stages. Each cruise produces a substantial volume of images, ideally requiring onboard analysis, which presently consumes a significant amount of time and necessitates internet connectivity to access the EcoTaxa Web service. To enhance the analyses, we developed an AI-based software framework named DeepLOKI, utilizing Deep Transfer Learning with a Convolution Neural Network Backbone. Our DeepLOKI image recognition tool can be applied directly on board. We trained and validated the model on pre-labeled images from four cruises, while images from a fifth cruise were used for testing. The best-performing model, utilizing the self-supervised pre-trained ResNet18 Backbone, achieved a notable average classification accuracy of 83.9 %, surpassing the regularly and frequently used method EcoTaxa (default) in this field by a factor of two. In summary, we developed a tool for pre-sorting high-resolution black and white zooplankton images with high accuracy, which will simplify and quicken the final annotation process. In addition, we provide a user-friendly graphical interface for the DeepLOKI framework for efficient and concise processes leading up to the classification stage. Moreover, performing latent space analysis on the self-supervised pre-trained ResNet18 Backbone could prove advantageous in identifying anomalies such as deviations in image parameter settings. This, in turn, enhances the quality control of the data. Our methodology remains agnostic to the specific imaging end system used, such as Loki, UVP, or ZooScan, as long as there is a sufficient amount of appropriately labeled data available to enable effective task performance by our algorithms. # Installation Guide https://pytorch.org/get-started/locally/ pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 ``` brew install python@3.10 pip3 install torch torchvision torchaudio pip3 install -r requirements_.txt ``` # Usage First download the example_haul.zip, model_ckpt.zip, loki.zip. and the sort.zip from [Download Here](https://uni-duesseldorf.sciebo.de/s/okWh4728VwnCBGp). Extract them. loki folder in the DeepLoki_ folder on root level. (here are our models stored) Copy the update_allcruises_df_validated_5with_zoomie_20230727.csv to output. Copy the update_wo_artefacts_test_dataset_PS992_20230727_nicole.csv to output. Copy the example_haul folder to data/ . Copy the sort folder to data/5_cruises/ . Copy the content to saved_models/ . Image analysis: Run start_app.py Image Labeling: Run start_app_sort.py # Training - Data needed and computing power Training: Run train_pytorch_lightning_model.py PreTraining: Run pretrain/pretrain_with_dino_paper_resnet_dino450.py # Software used Training and Validation was performed on an Nvidia A$100$ (Nvidia Corp., Santa Clara, CA, USA) and on Apple M1 MAX with 32 GB (Apple, USA), depending on the computational power needed, for example self-supervised pre-training was performed on a Hyper performing cluster with Nvidia A$100$. <br> On the Macbook Pro (Apple, USA) we used:<br> Python VERSION:3.10.5<br> pyTorch VERSION:13.1.3<br> On the cluster we used cluster specifics versions of the software:<br> Python VERSION:3.10.5 <br> pyTorch VERSION:13.1.3<br> CUDNN VERSION:1107)<br> # Authors Raphael Kronberg and Ellen Oldenburg # Support If you **really** like this repository and find it useful, please consider (★) **starring** it, so that it can reach a broader audience of like-minded people. It would be highly appreciated ! # Contributing to DeepLOKI If you find a bug, create a GitHub issue, or even better, submit a pull request. Similarly, if you have questions, simply post them as GitHub ([Link text Here](https://github.com/rakro101/DeepLOKI)) issues. # License , citation and acknowledgements Please advice the **LICENSE.md** file. For usage of third party libraries and repositories please advise the respective distributed terms. Please cite our paper, when using this code: ``` @software{kronbergapplicationsdeeploki, title={DeepLOKI- A deep learning based approach to identify Zooplankton taxa on high-resolution images from the optical plankton recorder LOKI}, author={Kronberg, Raphael Marvin and Oldenburg, Ellen} year = {2023}, url = {https://github.com/rakro101/DeepLOKI}, } ```\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/deploy2zenodo",
            "repo_link": "https://gitlab.com/deploy2zenodo/deploy2zenodo",
            "content": {
                "codemeta": "",
                "readme": "--- author: Daniel Mohr date: 2025-03-27 license: Apache-2.0 home: https://gitlab.com/deploy2zenodo/deploy2zenodo mirror: https://github.com/deploy2zenodo/deploy2zenodo latest_release: https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest doi: 10.5281/zenodo.10112959 --- # `deploy2zenodo` [[_TOC_]] ## preamble [`deploy2zenodo`](https://gitlab.com/projects/51392274) is a [shell](https://en.wikipedia.org/wiki/Bourne_shell) script to deploy your data to [Zenodo](https://zenodo.org/). You can use it in a [CI pipeline](https://docs.gitlab.com/ee/ci/pipelines/) as an automatic workflow. Environmental variables allow very flexible use. Depending on the selected flags, the data can be curated before deployment in a merge request, in the zenodo web interface or not curated at all. **Note:** `deploy2zenodo` is primarily designed for the Zenodo API. It may also work with other APIs like [Invenio RDM](https://inveniosoftware.org/products/rdm/), but compatibility is not guaranteed. Test the script with any new API before using it. ## intention To satisfy the FAIR[^fair1] principles[^fair2], publications should be deployed to an open repository. In this way the publication gets a PID ([persistent identifier](https://en.wikipedia.org/wiki/Persistent_identifier)) and at least the metadata is publicly accessible, findable and citable. Furthermore, current discussions about KPIs ([key performance indicator](https://en.wikipedia.org/wiki/Performance_indicator)) for software and data publications also lead to the need to generate PIDs for software and data. [^fair1]: [FAIR Principles](https://www.go-fair.org/fair-principles/) [^fair2]: [An interpretation of the FAIR principles to guide implementations in the HMC digital ecosystem.](https://doi.org/10.3289/HMC_publ_01) Especially software usually is not citable by a PID. To overcome this and make software academically significant we provide here a tool for automatic publication to the open repository [zenodo](https://zenodo.org/). In principal the same is true for all kind of scientific data (e. g. measurements, software and results such as papers). For every data managed in a version control system an automatic publication to an open repository is useful[^versioning]. [^versioning]: [Guidance on Versioning of Digital Assets.](https://doi.org/10.3289/HMC_publ_04) Software in particular is subject to frequent changes, resulting in many versions. This leads to the urge to automate the publishing process. This is not only about making the software usable through software repositories, but also about the citability of individual versions. ## how-to There are many possibilities to use `deploy2zenodo` but in this how-to section we will focus on a few typically use cases. ### simple workflow This workflow reflects the primary focus of `deploy2zenodo`. Go to your zenodo account and create an [access token](https://developers.zenodo.org/?shell#authentication). Store it in a [GitLab CI/CD variable](https://docs.gitlab.com/ee/ci/variables/) as `DEPLOY2ZENODO_ACCESS_TOKEN`. Use the flags [Mask variable](https://docs.gitlab.com/ee/ci/variables/index.html#mask-a-cicd-variable) and [Protect variable](https://docs.gitlab.com/ee/ci/variables/index.html#protect-a-cicd-variable). Masking ensures that the variable is not displayed in the CI/CD logs, and protecting the variable limits access to authorized users. Keep in mind the token is sensitive and private information. Therefore you should not share it or make it public available. Then the [GitLab CI/CD pipeline](https://docs.gitlab.com/ee/ci/pipelines/) could look like (we use here [sandbox.zenodo.org](https://sandbox.zenodo.org/) instead of [zenodo.org](https://zenodo.org/) for testing purpose): ```yaml include: - remote: 'https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml' prepare_release_and_deploy2zenodo: stage: build image: name: alpine:latest variables: DEPLOY2ZENODO_JSON: \"mymetadata.json\" script: # prepare - TAG=$(grep version library.properties | cut -d \"=\" -f 2) - | echo '{\"metadata\":{\"creators\":[{\"name\":\"family, given\"}],\\ \"license\":{\"id\":\"GPL-3.0-or-later\"},\"title\":\"test script alpine\",\\ \"version\":\"***\",\"upload_type\":\"software\"}}' | \\ jq \".metadata.version = \\\"$TAG\\\"\" | tee \"$DEPLOY2ZENODO_JSON\" # prepare release - echo \"DESCRIPTION=README.md\" > variables.env - echo \"TAG=$TAG\" >> variables.env # prepare deploy2zenodo - echo \"DEPLOY2ZENODO_JSON=$DEPLOY2ZENODO_JSON\" >> variables.env - DEPLOY2ZENODO_UPLOAD=\"v$TAG.zip\" - git archive --format zip --output \"$DEPLOY2ZENODO_UPLOAD\" \"$TAG\" - echo \"DEPLOY2ZENODO_UPLOAD=$DEPLOY2ZENODO_UPLOAD\" >> variables.env artifacts: reports: dotenv: variables.env paths: - $DEPLOY2ZENODO_JSON release_job: stage: deploy rules: - if: $CI_COMMIT_TAG when: never - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH image: registry.gitlab.com/gitlab-org/release-cli:latest script: - cat /etc/os-release release: name: 'v$TAG' description: '$DESCRIPTION' tag_name: '$TAG' ref: '$CI_COMMIT_SHA' deploy2zenodo: rules: - if: $CI_COMMIT_TAG when: never - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH variables: DEPLOY2ZENODO_API_URL: \"https://sandbox.zenodo.org/api\" DEPLOY2ZENODO_DEPOSITION_ID: \"create NEW record\" ``` We use here 3 jobs: * The job `prepare_release_and_deploy2zenodo` prepares the variables and data for the following jobs. You can choose how to get the variables and data from your project/repository. (see hints in [DEPLOY2ZENODO_JSON](#deploy2zenodo_json) and [DEPLOY2ZENODO_UPLOAD](#deploy2zenodo_upload)) * The job `release_job` uses the workflow [Create release metadata in a custom script](https://docs.gitlab.com/ee/user/project/releases/release_cicd_examples.html#create-release-metadata-in-a-custom-script). * The job `deploy2zenodo` publishes the data to zenodo. The variables are passed between the jobs using [dotenv variables](https://docs.gitlab.com/ee/ci/yaml/artifacts_reports.html#artifactsreportsdotenv). And the data are passed using [job artifacts](https://docs.gitlab.com/ee/ci/jobs/job_artifacts.html). After the first run of the above pipeline (job `deploy2zenodo`) adapt `DEPLOY2ZENODO_DEPOSITION_ID` to store the record id. Only then you are able to release new versions to zenodo. In this example, `prepare_release_and_deploy2zenodo` always runs while the other jobs only run when the default branch is changed. This makes it possible to check the artifacts during a merge request. The used environment variables (see [script parameter](#script-parameter)) can be provided in many different ways as a [GitLab CI/CD variable](https://docs.gitlab.com/ee/ci/variables/), e. g.: * [CI/CD variable in the UI](https://docs.gitlab.com/ee/ci/variables/#define-a-cicd-variable-in-the-ui) * not stored in the repository * possible to [Mask variable](https://docs.gitlab.com/ee/ci/variables/index.html#mask-a-cicd-variable) * possible to [Protect variable](https://docs.gitlab.com/ee/ci/variables/index.html#protect-a-cicd-variable) * used for private data (e. g. access token) * [CI/CD variable in the .gitlab-ci.yml](https://docs.gitlab.com/ee/ci/variables/#define-a-cicd-variable-in-the-gitlab-ciyml-file) * stored in the repository * in public projects also publicly accessable You should think about which information to store at which place. Here a few simple considerations: | variable | private data | note | | ------ | ------ | ------ | | DEPLOY2ZENODO_API_URL | no | Should a user find your publication? | | DEPLOY2ZENODO_ACCESS_TOKEN | YES | Should not be shared with anyone! | | DEPLOY2ZENODO_DEPOSITION_ID | no | Should a user find your publication? | | DEPLOY2ZENODO_JSON | ? | Is the publication public? | | DEPLOY2ZENODO_UPLOAD | ? | Is the publication public? | Sometimes it is easier to change the variable in the UI. For example in your first step you should set `DEPLOY2ZENODO_API_URL=\"https://sandbox.zenodo.org/api\"` and `DEPLOY2ZENODO_DEPOSITION_ID=\"create NEW record\"` to initiate and test your pipeline. After success you should change to `DEPLOY2ZENODO_API_URL=\"https://zenodo.org/api\"`. And after you have created your first record, also change `DEPLOY2ZENODO_DEPOSITION_ID` to the returned value to update your dataset next time (and not create a new one). If you store these variables in the user interface, you can change them without touching your repository. On the other hand, the metadata provided via `DEPLOY2ZENODO_JSON` and the data provided via `DEPLOY2ZENODO_UPLOAD` may be created dynamically and it could therefore make sense to create these variables dynamically as well. There are also optional variables that can help to adapt the workflow to the the individual use case. For example, [DEPLOY2ZENODO_SKIP_PUBLISH](#deploy2zenodo_skip_publish) allows you to curate the upload to zenodo in the zenodo web interface before publishing. This is especially useful if you are setting up the workflow for the first time in your own project -- but can also be used at any time. Depending on where variables are defined, they have different priorities. For example, CI variables defined in the UI have priority and override the variables stored in the `.gitlab-ci.yml` file with the [keyword `variables`](https://docs.gitlab.com/ee/ci/yaml/#variables). Variables that are defined at job level, in the `script`, `before_script` or `after_script` sections, have the highest priority An example test project is [deploy2zenodo_test_simple_workflow_update](https://gitlab.com/projects/51647607). ### very simple workflow It is not necessary to create a release for publication. But we think this is the typically use case for software publication. For a very simple workflow running when creating a tag, you could use something like: ```yaml include: - remote: 'https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml' deploy2zenodo: stage: deploy rules: - if: $CI_COMMIT_TAG variables: DEPLOY2ZENODO_API_URL: \"https://sandbox.zenodo.org/api\" DEPLOY2ZENODO_JSON: \"CITATION.json\" DEPLOY2ZENODO_DEPOSITION_ID: \"create NEW record\" DEPLOY2ZENODO_UPLOAD: \"$CI_PROJECT_NAME-$CI_COMMIT_TAG.zip\" DEPLOY2ZENODO_ADD_IsCompiledBy_DEPLOY2ZENODO: \"yes\" DEPLOY2ZENODO_ADD_IsNewVersionOf: \"yes\" DEPLOY2ZENODO_ADD_IsPartOf: \"yes\" DEPLOY2ZENODO_GET_METADATA: \"result.json\" before_script: - env - echo https://dl-cdn.alpinelinux.org/alpine/edge/community >> /etc/apk/repositories - apk add --no-cache cffconvert curl git jq - publication_date=$(echo \"$CI_COMMIT_TIMESTAMP\" | grep -Eo \"^[0-9]{4}-[0-9]{2}-[0-9]{2}\") - | cffconvert -i CITATION.cff -f zenodo | \\ jq -c '{\"metadata\": .} | .metadata += {\"upload_type\": \"software\"}' | \\ jq -c \".metadata.related_identifiers += [ {\\\"relation\\\": \\\"isDerivedFrom\\\", \\\"identifier\\\": \\\"$CI_SERVER_URL/projects/$CI_PROJECT_ID\\\"}] | .metadata.version = \\\"$CI_COMMIT_TAG\\\" | .metadata.publication_date = \\\"$publication_date\\\"\" | \\ tee \"$DEPLOY2ZENODO_JSON\" | jq -C . - git archive --format zip --output \"$DEPLOY2ZENODO_UPLOAD\" \"$CI_COMMIT_TAG\" artifacts: paths: - $DEPLOY2ZENODO_JSON - $DEPLOY2ZENODO_GET_METADATA ``` Such a simple workflow uses * [deploy_deploy2zenodo_to_zenodo](https://gitlab.com/projects/52008252) in the job `deploy2zenodo` to publish itself. * [2024-10_waw_fdm](https://codebase.helmholtz.cloud/projects/14469) in the job `deploy2zenodo` to publish a poster. ### triggered workflow In many projects there is more than one maintainer. Therefore it is not possible to store the user token for zenodo as CI variable in the project. Otherwise, the user token would be shared with the other maintainers. Using this triggered workflow allows to restrict the use of the user token to a specific zenodo record for other maintainers. But the project `A` with more than one maintainer can trigger a pipeline in another (private) project `B` with only one maintainer, e. g.: ```yaml trigger: stage: .post rules: - if: $CI_COMMIT_TAG when: never - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH image: name: alpine:latest script: - apk add --no-cache curl - curl -X POST --fail -F token=\"$TRIGGER_TOKEN\" -F ref=main \"$TRIGGER_URL\" ``` Storing the `TRIGGER_TOKEN` as protected and [masked CI variable](https://docs.gitlab.com/ee/ci/variables/index.html#mask-a-cicd-variable) (or maybe even [hide CI variable](https://docs.gitlab.com/ee/ci/variables/index.html#hide-a-cicd-variable)) in project `A` allows any maintainer to use it and trigger the pipeline. In the project `B` only 1 mainainer exists and you can use deploy2zenodo as normal, e. g.: ```yaml include: - remote: 'https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml' prepare_deploy2zenodo: image: name: alpine:latest script: - PROJECT_A_REPO=$(mktemp -d) - git clone --branch main --depth 1 \"$PROJECT_A_URL\" \"$PROJECT_A_REPO\" # create zip archive from latest tag - | (cd \"$PROJECT_A_REPO\" && \\ git archive --format zip -o \"$DEPLOY2ZENODO_UPLOAD\" \\ \"$(git tag | sort -t \".\" -n -k 3 | tail -n 1)\") artifacts: expire_in: 1 hrs paths: - $DEPLOY2ZENODO_UPLOAD my_deploy2zenodo: extends: .deploy2zenodo # variables set in the script could not be overwritten by the trigger source before_script: - | DEPLOY2ZENODO_DEPOSITION_ID=\"create NEW record\" DEPLOY2ZENODO_API_URL=\"https://sandbox.zenodo.org/api\" - !reference [deploy2zenodo, before_script] deploy2zenodo: rules: - if: '\"0\" == \"1\"' when: never ``` There are various ways to trigger a pipeline, e. g: * [trigger a pipeline by trigger token](https://docs.gitlab.com/ee/ci/triggers/) * trigger using [Multi-project pipelines](https://docs.gitlab.com/ee/ci/pipelines/downstream_pipelines.html#multi-project-pipelines) In the CI pipeline above the token method is used. In the [CI pipeline of deploy2zenodo](https://gitlab.com/deploy2zenodo/deploy2zenodo/-/blob/main/.gitlab-ci.yml?ref_type=heads) the multi-project pipeline is used. **Be careful**: The trigger job from project `A` may overwrite variables in the triggered job from project `B`. This could lead to security concerns. Maybe [Restrict who can override variables](https://docs.gitlab.com/ee/ci/variables/index.html#restrict-who-can-override-variables) could help to overcome this. More details: In project `A` something exists that should be published on zenodo. In project `B` the content of project `A` is published on zenodo. The pipeline in project `B` can be triggered so that this happens automatically when corresponding changes are made in project `A` (e. g. merge to default branch). Project `B` should rely as little as possible on project `A`. Unfortunately, variables can be transferred when triggering (from project `A`) and these are not trustworthy. For example, a maintainer from project `A` could pass `DEPLOY2ZENODO_API_URL` in this way and thus force communication to another server. This could cause the user token to be leaked. To avoid this, define the variable in the script -- as shown in the example above. However, it is no problem to save the user token in project `B` as CI variable `DEPLOY2ZENODO_ACCESS_TOKEN`. This variable could then be overwritten from project `A`, but not read out. Another possibility is to use [Secrets management providers](https://docs.gitlab.com/ee/ci/pipelines/pipeline_security.html#secrets-management-providers). This triggered workflow is used in [file_hook_server_timestamping](https://gitlab.com/dlr-pa/file_hook_server_timestamping) together with [deploy_file_hook_server_timestamping_to_zenodo](https://gitlab.dlr.de/deploy2zenodo/deploy_file_hook_server_timestamping_to_zenodo). ### complex workflow This workflow splits the deploying to zenodo in steps. This allows to use the zenodo record (e. g. the DOI) already in the data to publish. ```yaml include: - remote: 'https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml' deploy2zenodo: rules: - if: '\"0\" == \"1\"' when: never prepare_deploy2zenodo_step1: script: - ... deploy2zenodo-step1: variables: - DEPLOY2ZENODO_SKIP_PUBLISH: \"true\" - DEPLOY2ZENODO_GET_METADATA: \"newmetadata.json\" extends: .deploy2zenodo image: name: alpine:latest before_script: - !reference [deploy2zenodo, before_script] script: - !reference [deploy2zenodo, script] - echo \"DEPLOY2ZENODO_GET_METADATA=$DEPLOY2ZENODO_GET_METADATA\" > variables.env artifacts: paths: - $DEPLOY2ZENODO_GET_METADATA reports: dotenv: variables.env prepare_release: script: - echo \"use the file \\\"$DEPLOY2ZENODO_GET_METADATA\\\"\" - ... release_job: script: - ... prepare_deploy2zenodo_step2: script: - ... deploy2zenodo-step2: variables: DEPLOY2ZENODO_SKIP_NEW_VERSION: \"true\" extends: .deploy2zenodo image: name: alpine:latest before_script: - apk add --no-cache curl jq ``` In the step `prepare_release` you can use [jq](https://github.com/jqlang/jq) to extract data. For example the preserved DOI is available by: ```sh jq .metadata.prereserve_doi.doi \"$DEPLOY2ZENODO_GET_METADATA\" ``` Such a complex workflow uses [2024-10_waw_fdm_talk](https://codebase.helmholtz.cloud/projects/14501) to publish a poster. Instead of creating a release, as shown above, the poster is built in the job `build` using the DOI previously created in the job `deploy2zenodo-step1`. ### very complex workflow `deploy2zenodo` uses a combination of the [triggered workflow](https://gitlab.com/deploy2zenodo/deploy2zenodo#triggered-workflow) and the [complex workflow](https://gitlab.com/deploy2zenodo/deploy2zenodo#complex-workflow) to publish itself. This is described in [deploy_deploy2zenodo_to_zenodo](https://gitlab.com/projects/52008252). ## script parameter Instead of command line parameters we use environment variables. You have to provide the following variables: | variable | content | | ------ | ------ | | DEPLOY2ZENODO_API_URL | The URL of the API to use. | | DEPLOY2ZENODO_ACCESS_TOKEN | access token of zenodo | | DEPLOY2ZENODO_DEPOSITION_ID | id of the deposition/record on zenodo | | DEPLOY2ZENODO_JSON | file name with metadata in JSON format to upload | | DEPLOY2ZENODO_UPLOAD | file name(s) to upload | There are other optional variables: | variable | content | | ------ | ------ | | DEPLOY2ZENODO_SKIP_PUBLISH | prepare record, but skip publishing | | DEPLOY2ZENODO_DRYRUN | skip communicating with the external URL | | DEPLOY2ZENODO_SKIPRUN | skip everything, only prints commands to execute | | DEPLOY2ZENODO_SKIP_NEW_VERSION | skip creating new version | | DEPLOY2ZENODO_GET_METADATA | write actual metadata to a file | | DEPLOY2ZENODO_SKIP_UPLOAD | skip upload of data | | DEPLOY2ZENODO_CURL_MAX_TIME | max time for curl | | DEPLOY2ZENODO_CURL_MAX_TIME_PUBLISH | max time for curl during publishing | | DEPLOY2ZENODO_ADD_IsCompiledBy_DEPLOY2ZENODO | reference deploy2zenodo | | DEPLOY2ZENODO_ADD_IsNewVersionOf | reference previous version | | DEPLOY2ZENODO_ADD_IsPartOf | reference DOI for all versions | ### DEPLOY2ZENODO_API_URL You can use the API of your own zenodo instance or you can use the official [zenodo instance](https://about.zenodo.org/): | state | URL | | ------ | ------ | | production | [`https://zenodo.org/api`](https://zenodo.org/api) | | testing | [`https://sandbox.zenodo.org/api`](https://sandbox.zenodo.org/api) | ### DEPLOY2ZENODO_ACCESS_TOKEN To access your zenodo account you have to provide an [access token](https://developers.zenodo.org/?shell#authentication). ### DEPLOY2ZENODO_DEPOSITION_ID To update an existing record you have to provide the `id` of this record. If you want to create a new record please set `DEPLOY2ZENODO_DEPOSITION_ID` to `create NEW record`, e. g. `DEPLOY2ZENODO_DEPOSITION_ID=\"create NEW record\"`. After creating this record read the script output and adapt `DEPLOY2ZENODO_DEPOSITION_ID` for the next run with the returned record `id`. ### DEPLOY2ZENODO_JSON The given file should contain the metadata in JSON format. You can write this file on your own, e. g.: ```json { \"metadata\": { \"title\": \"foo\", \"upload_type\": \"software\", \"creators\": [ { \"name\": \"ich\", \"affiliation\": \"bar\" } ], \"description\": \"foos description\" } } ``` You can find the necessary and possible fields on [zenodo: Deposit metadata](https://developers.zenodo.org/#representation). Or [cffconvert](https://github.com/citation-file-format/cffconvert) can help harvesting the necessary metadata in JSON format from a [CITATION.cff file](https://github.com/citation-file-format/citation-file-format). Unfortunately we need [jq](https://github.com/jqlang/jq) to correct the format, e. g.: ```sh cffconvert -i CITATION.cff -f zenodo | \\ jq '{\"metadata\": .} | .metadata += {\"upload_type\": \"software\"}' | \\ tee CITATION.json ``` Since you need to adapt the output of the conversion you can also use more general tools like [yq](https://mikefarah.gitbook.io/yq/) to convert a CITATION.cff file (YAML format) to JSON format. The JSON format zenodo accepts is much more general and provides many more options than the Citation File Format. For many purposes the CITATION.cff is enough, but otherwise you can see a description of the metadata in the GitHub integration of zenodo[^githubintegration] [^githubintegration2] [^githubintegration3] using `zenodo.json`, the description of the metadata in zenodo|Developers[^zenodoDevelopers] or InvenioRDM[^metadatareference] and the unofficial description of zenodo upload metadata schema[^zenodouploadmetadataschema]. [^githubintegration]: [developers.zenodo.org GitHub](https://developers.zenodo.org/#github) [^githubintegration2]: [github.com Referencing and citing content](https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content) [^githubintegration3]: [github: \"import\" past releases to Zenodo](https://github.com/zenodo/zenodo/issues/1463) [^zenodoDevelopers]: [zenodo|Developers](https://developers.zenodo.org/#deposit-metadata) [^metadatareference]: [InvenioRDM: Metadata reference](https://inveniordm.docs.cern.ch/reference/metadata/) [^zenodouploadmetadataschema]: [Zenodo upload metadata schema](https://github.com/zenodraft/metadata-schema-zenodo) As `description` you can use HTML. For example you could use [pandoc](https://pandoc.org/) to convert your `README.md` to HTML and [jq](https://github.com/jqlang/jq) to add the HTML code as JSON value (`jq` will escape appropriate characters if necessary): ```sh pandoc -o README.html README.md echo '{\"metadata\":{\"title\":\"foo\",\"upload_type\":\"software\", \"creators\":[{\"name\":\"ich\",\"affiliation\":\"bar\"}], \"description\":\"foos description\"}}' | \\ jq --rawfile README README.html '.metadata.description = $README' | \\ tee metadata.json ``` ### DEPLOY2ZENODO_UPLOAD The given file(s) will be uploaded as data. Typically this would be an archive. For example you can create an archive of a tag from a git repository: ```sh TAG=0.0.3 git archive --format zip --output $TAG.zip $TAG ``` File names with spaces are not supported. Instead, if `DEPLOY2ZENODO_UPLOAD` contains space(s), the string is split at the spaces. Each individual block represents a file and these files will be uploaded. The reason not supporting spaces is that [you cannot create a CI/CD variable that is an array](https://docs.gitlab.com/ee/ci/variables/index.html#store-multiple-values-in-one-variable). If you really not want to provide data set `DEPLOY2ZENODO_UPLOAD` to `do NOT provide data`, e. g. `DEPLOY2ZENODO_UPLOAD=\"do NOT provide data\"`. If you want to upload 4 files with these names change the order. Not every zenodo instance supports metadata-only records (configured by `canHaveMetadataOnlyRecords`?). For example the official [zenodo instance](https://about.zenodo.org/) does not allow metadata-only records! In this case an empty dummy file is uploaded. If this is the case, you should think about respecting the implicit request of the used zenodo instance to provide some data. ### DEPLOY2ZENODO_SKIP_PUBLISH If this variable is not empty the publishing step is skipped, e. g.: ```sh DEPLOY2ZENODO_SKIP_PUBLISH=\"true\" ``` Only the record is prepared -- metadata and data is uploaded -- but not published. You can see what will be published as a preview in the web interface of zenodo and initiate the publishing by pressing the button in the web interface. This helps to integrate `deploy2zenodo` in your project. But you may also want to curate the upload each time before it is published. Together with DEPLOY2ZENODO_SKIP_NEW_VERSION this allows to split deploying to zenodo in steps. ### DEPLOY2ZENODO_DRYRUN If this variable is not empty the communication to the given URL is skipped. But your parameters are analyzed. This could help to integrate `deploy2zenodo` in your project. ### DEPLOY2ZENODO_SKIPRUN If this variable is not empty nearly everything is skipped. Only the commands to be executed are echoed. This is for debugging purpose. ### DEPLOY2ZENODO_SKIP_NEW_VERSION If this variable is not empty the step creating a new version is skipped. This allows to split deploying to zenodo in steps. Between creating a new version and deploying to zenodo you can use the zenodo record (e. g. the DOI) already in the data to publish: ```sh jq .metadata.prereserve_doi.doi \"$DEPLOY2ZENODO_GET_METADATA\" >> README.md ``` Using a [manual job](https://docs.gitlab.com/ee/ci/jobs/job_control.html#create-a-job-that-must-be-run-manually) allows you to first check the artifacts and data to be published before the last job run. ### DEPLOY2ZENODO_GET_METADATA If this variable is not empty the metadata of the record is stored in a file with this name. This is useful for logging or further processing after deployment. To get these data at the end of the script an additional communication with the DEPLOY2ZENODO_API_URL server is done. In the CI pipeline you could store the result as artifacts, e. g.: ```yaml deploy2zenodo: variables: DEPLOY2ZENODO_GET_METADATA: \"result.json\" artifacts: paths: - $DEPLOY2ZENODO_GET_METADATA ``` You can extract values from the metadata. For example to get the DOI to site all versions: ```yaml my_deploy2zenodo: extends: .deploy2zenodo script: - !reference [deploy2zenodo, script] - jq -r .conceptdoi \"$DEPLOY2ZENODO_GET_METADATA\" ``` ### DEPLOY2ZENODO_SKIP_UPLOAD If this variable is not empty skip uploading the data. This is only allowed if DEPLOY2ZENODO_SKIP_PUBLISH is not empty, too. If you split deploying to zenodo in steps using DEPLOY2ZENODO_SKIP_PUBLISH and DEPLOY2ZENODO_SKIP_NEW_VERSION you can avoid unnecessary traffic by using also DEPLOY2ZENODO_SKIP_UPLOAD. ### DEPLOY2ZENODO_CURL_MAX_TIME Max time for curl (`--max-time` flag) in seconds for normal use. Default value is 60. ### DEPLOY2ZENODO_CURL_MAX_TIME_PUBLISH Max time for curl (`--max-time` flag) in seconds during publishing. Default value is 300. ### DEPLOY2ZENODO_ADD_IsCompiledBy_DEPLOY2ZENODO If this variable is not empty a reference to deploy2zenodo is added. Something like (but with the DOI of the used version) will be added to your provided JSON file: ```json { \"metadata\": { \"related_identifiers\": [ { \"relation\": \"IsCompiledBy\", \"identifier\": \"10.5281/zenodo.10112959\", \"scheme\": \"doi\", \"resource_type\": \"software\" } ] } } ``` ### DEPLOY2ZENODO_ADD_IsNewVersionOf If this variable is not empty a reference to the previous version of your record is referenced. Something like (but with the DOI of the old version and the appropriate resource_type) will be added to your provided JSON file: ```json { \"metadata\": { \"related_identifiers\": [ { \"relation\": \"IsNewVersionOf\", \"identifier\": \"10.5281/zenodo.10908332\", \"scheme\": \"doi\", \"resource_type\": \"software\" } ] } } ``` This can only work if DEPLOY2ZENODO_SKIP_NEW_VERSION is not used! If you split the run in 2 steps (the first one with DEPLOY2ZENODO_SKIP_PUBLISH and the second one with DEPLOY2ZENODO_SKIP_NEW_VERSION) you have to find the old version in the first run by yourself and provide it in the second run. This is done in [deploy_deploy2zenodo_to_zenodo](https://gitlab.com/projects/52008252) in the jobs `deploy_deploy2zenodo_step1` and `deploy_deploy2zenodo_step2` to publish `deploy2zenodo`. This is something like: ```yaml step1: variables: DEPLOY2ZENODO_ADD_IsNewVersionOf: \"yes\" after_script: - | LATESTDOI=\"$(jq -r \".metadata.related_identifiers[] | select(.relation==\\\"isNewVersionOf\\\") | .identifier\" \\ \"$DEPLOY2ZENODO_GET_METADATA\")\" - | LATESTUPLOADTYPE=\"$(jq -r \".metadata.related_identifiers[] | select(.relation==\\\"isNewVersionOf\\\") | .resource_type\" \\ \"$DEPLOY2ZENODO_GET_METADATA\")\" - | { echo \"LATESTDOI=$LATESTDOI\" echo \"LATESTUPLOADTYPE=$LATESTUPLOADTYPE\" } | tee variables.env artifacts: reports: dotenv: variables.env ``` ```yaml step2: needs: - job: step1 variables: DEPLOY2ZENODO_SKIP_NEW_VERSION: \"true\" before_script: - tmpjson=\"$(mktemp)\" - | jq \".metadata.related_identifiers += [ { \\\"relation\\\":\\\"IsNewVersionOf\\\", \\\"identifier\\\":\\\"$LATESTDOI\\\", \\\"scheme\\\":\\\"doi\\\", \\\"resource_type\\\":\\\"$LATESTUPLOADTYPE\\\" }]\" \"$DEPLOY2ZENODO_JSON\" | tee \"$tmpjson\" - mv \"$tmpjson\" \"$DEPLOY2ZENODO_JSON\" ``` Note that [after_script](https://docs.gitlab.com/ee/ci/yaml/#after_script) works differently than `before_script` or `script` and does not affect the job exit code. ### DEPLOY2ZENODO_ADD_IsPartOf If this variable is not empty a reference to all versions of your record is referenced. Something like (but with the DOI of all versions and the appropriate resource_type) will be added to your provided JSON file: ```json { \"metadata\": { \"related_identifiers\": [ { \"relation\": \"IsPartOf\", \"identifier\": \"10.5281/zenodo.10112959\", \"scheme\": \"doi\", \"resource_type\": \"software\" } ] } } ``` This only works, if DEPLOY2ZENODO_DEPOSITION_ID is not given as `create NEW record`. ## CI pipeline Using the keyword [`include`](https://docs.gitlab.com/ee/ci/yaml/index.html#include) it is possible to include YAML files and/or CI pipelines in your [GitLab](https://about.gitlab.com/) CI pipeline. In this way you can use a template of `deploy2zenodo` for your CI pipeline. You can use the latest version [deploy2zenodo.yaml](https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml) in your CI pipeline. Or you can use any special versions, e. g. [deploy2zenodo.yaml v0.1.0](https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/0.1.0/downloads/deploy2zenodo.yaml). The provided job is called `deploy2zenodo` and you can overwrite or enhance the defined job as you need (e. g. defining when to run or defining variables). A simple example choosing the stage to run could be: ```yaml include: - remote: 'https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml' deploy2zenodo: stage: deploy ``` The provided GitLab CI template of `deploy2zenodo` uses [`alpine:latest`](https://hub.docker.com/_/alpine) and installs necessary software [curl](https://curl.se/) and [jq](https://github.com/jqlang/jq) in `before_script`. To use other images you must adapt it, e. g.: ```yaml include: - remote: 'https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo.yaml' deploy2zenodo: image: name: almalinux:latest before_script: - echo \"nothing to do\" ``` ## script You can use the script directly. But that is not our main focus of `deploy2zenodo`, so we keep it short. For example: ```sh SCRIPTURL=https://gitlab.com/deploy2zenodo/deploy2zenodo/-/releases/permalink/latest/downloads/deploy2zenodo export DEPLOY2ZENODO_API_URL=https://sandbox.zenodo.org/api export DEPLOY2ZENODO_ACCESS_TOKEN=*** export DEPLOY2ZENODO_DEPOSITION_ID=\"create NEW record\" export DEPLOY2ZENODO_JSON=metadata.json export DEPLOY2ZENODO_UPLOAD=\"foo.zip bar.md\" export DEPLOY2ZENODO_SKIP_PUBLISH=\"true\" export DEPLOY2ZENODO_DRYRUN=\"\" export DEPLOY2ZENODO_SKIPRUN=\"\" export DEPLOY2ZENODO_SKIP_NEW_VERSION=\"\" export DEPLOY2ZENODO_GET_METADATA=\"upload.json\" export DEPLOY2ZENODO_SKIP_UPLOAD=\"\" export DEPLOY2ZENODO_CURL_MAX_TIME=\"\" export DEPLOY2ZENODO_CURL_MAX_TIME_PUBLISH=\"\" export DEPLOY2ZENODO_ADD_IsCompiledBy_DEPLOY2ZENODO=\"yes\" export DEPLOY2ZENODO_ADD_IsNewVersionOf=\"\" export DEPLOY2ZENODO_ADD_IsPartOf=\"yes\" curl -L \"$SCRIPTURL\" | tee deploy2zenodo.sh | sh ``` It's worth noting that we try to handle private information such as the access token in a secure way in the script, it is still a sensitive piece of information that should not be shared with anyone who does not need access to the corresponding Zenodo account. **Important:** Using the script as described in this section (e. g. on a desktop computer) does not allow for the masking of CI variables, which can expose sensitive information such as access tokens. We recommend that users take steps to mitigate this risk. ## harvesting As already mentioned you have to provide the metadata and the data to upload. In my opinion, this is very dependent on the project. In many programming languages, there is a convention to store metadata such as name, author, description, version and license in certain files (`pyproject.toml`, `library.properties`, ...). In order to deploy this information to zenodo, it must be available in a certain format and with a certain vocabulary. The already mentioned [cffconvert](https://github.com/citation-file-format/cffconvert) tries to do this at least for cff files. Other tools such as [somesy](https://github.com/Materials-Data-Science-and-Informatics/somesy) have a somewhat different focus, but they can also help in a pipeline/toolchain. For example, you could use it to convert a typical python `pyproject.toml` into `CITATION.cff` and then use [cffconvert](https://github.com/citation-file-format/cffconvert) and [jq](https://github.com/jqlang/jq) to get metadata for zenodo. However, [hermes](https://docs.software-metadata.pub/en/latest/) should also be mentioned here. hermes tries to merge metadata from different sources and to provide it for zenodo. ## curating You have various options for curating the data for publication. The typical workflow in software development is to work in developer or feature branches and then merge them with the default branch (e. g. `main`). This is usually done in a merge request. If the harvesting of metadata and data is already taking place in a CI pipeline at this point, this can also be checked in the merge request. If publishing is prevented by using `DEPLOY2ZENODO_SKIP_PUBLISH`, the preview in the zenodo web interface can be used to check the result. If you have implemented a stable, functioning process, curation can also be omitted and publishing can be fully automated. ## license: Apache-2.0 `deploy2zenodo` has the license [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0). ```txt Copyright 2023-2025 Daniel Mohr and Deutsches Zentrum fuer Luft- und Raumfahrt e. V., D-51170 Koeln Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ```\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/deus",
            "repo_link": "https://github.com/gfzriesgos/deus",
            "content": {
                "codemeta": "",
                "readme": "# deus [![codecov](https://codecov.io/gh/gfzriesgos/deus/branch/master/graph/badge.svg)](https://codecov.io/gh/gfzriesgos/deus) **D**amage-**E**xposure-**U**pdate-**S**ervice Command line program for the damage computation in a multi risk scenario pipeline for earthquakes, tsnuamis, ashfall & lahars. ## Citation Brinckmann, Nils; Gomez-Zapata, Juan Camilo; Pittore, Massimiliano; Rüster, Matthias (2021): DEUS: Damage-Exposure-Update-Service. V. 1.0. GFZ Data Services. https://doi.org/10.5880/riesgos.2021.011 ## What is it? This is the service to update a given exposure file (as it is the output of the assetmaster script) and update the building and damage classes with given fragility functions and intensity values. ## Copyright & License Copyright © 2021 Helmholtz Centre Potsdam GFZ German Research Centre for Geosciences, Potsdam, Germany Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ## Project Deus was developed in the scope of the RIESGOS project: Multi-Risk Analysis and Information System Components for the Andes Region (https://www.riesgos.de/en) ## Documentation You can look up several documentation pages: - [Setup and installation](doc/Setup.md) - [Example run](doc/ExposureModel.md) - [Shakemaps](doc/EarthQuakeShakemap.md) and [Intensity files](doc/IntensityFile.md) - [Exposure models](doc/ExposureModel.md) - [Fragility functions](doc/FragilityFunctions.md) - [Loss](doc/LossData.md) - [Schema mappings](doc/SchemaMapping.md) ## Scope of deus Deus was created in the riesgos project for working in multi risk scenarios. It should be provided as a web processing service by the GFZ. ## You still have questions If we don't cover important things in the documentation, please feel free to create an issue or send a mail at <nils.brinckmann@gfz-potsdam.de> or <pittore@gfz-potsdam.de>. ## Can I use deus for xyz? Yes! But you may have to code a bit yourself. The code is written against interfaces and already provides several implementations for some of them. Aims for the following development of deus is the support of more and more hazards with their intensity files, their fragility functions and their schemas. You can also take a look into the [TODOs](TODO.md). ## Will there only be one deus? There is deus and there is volcanus (a special deus version \"volcanus\" that works with shapefiles for intensities - it uses a column LOAD and a unit of kPa - to allow a special ashfall service in the RIESGOS demonstrator). The two services only differ in the intensity provider.\n",
                "dependencies": "# Copyright © 2021-2022 Helmholtz Centre Potsdam GFZ German Research Centre for # Geosciences, Potsdam, Germany # # Licensed under the Apache License, Version 2.0 (the \"License\"); you may not # use this file except in compliance with the License. You may obtain a copy of # the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the # License for the specific language governing permissions and limitations under # the License. affine==2.3.0 atomicwrites==1.3.0 attrs==19.1.0 Click==7.0 click-plugins==1.1.1 cligj==0.5.0 coverage==4.5.4 cycler==0.10.0 decorator==4.4.0 Deprecated==1.2.6 descartes==1.1.0 docopt==0.6.2 Fiona==1.8.18 GDAL==2.4.0 geopandas==0.5.1 imageio==2.5.0 importlib-metadata==0.19 joblib==0.17.0 kiwisolver==1.1.0 lxml==4.6.2 matplotlib==3.1.1 more-itertools==7.2.0 munch==2.3.2 networkx==2.3 numpy==1.17.4 packaging==19.1 palettable==3.3.0 pandas==0.25.0 Pillow==7.2.0 pluggy==0.12.0 py==1.8.0 pyparsing==2.4.2 pyproj==2.2.1 pysal==2.1.0 pytest==5.0.1 python-dateutil==2.8.0 pytz==2019.2 PyWavelets==1.1.1 rasterio==1.0.28 rasterstats==0.13.1 Rtree==0.9.7 scikit-image==0.17.2 scikit-learn==0.23.0 scipy==1.3.0 seaborn==0.9.0 Shapely==1.6.4.post2 simplejson==3.16.0 six==1.12.0 snuggs==1.4.6 tqdm==4.35.0 wcwidth==0.1.7 wrapt==1.11.2 zipp==0.5.2\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/digital-earth-viewer",
            "repo_link": "https://git.geomar.de/digital-earth/digital-earth-viewer",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/dirschema",
            "repo_link": "https://github.com/Materials-Data-Science-and-Informatics/dirschema",
            "content": {
                "codemeta": "{\"@context\":[\"https://doi.org/10.5063/schema/codemeta-2.0\",\"https://w3id.org/software-iodata\",\"https://raw.githubusercontent.com/jantman/repostatus.org/master/badges/latest/ontology.jsonld\",\"https://schema.org\",\"https://w3id.org/software-types\"],\"@type\":\"SoftwareSourceCode\",\"applicationCategory\":[\"Scientific/Engineering\",\"Software Development\"],\"audience\":[{\"@type\":\"Audience\",\"audienceType\":\"Developers\"},{\"@type\":\"Audience\",\"audienceType\":\"Science/Research\"}],\"author\":[{\"@id\":\"https://orcid.org/0000-0002-5077-7497\",\"@type\":\"Person\",\"affiliation\":{\"@type\":\"Organization\",\"legalName\":\"Forschungszentrum J\\u00fclich GmbH - Institute for Materials Data Science and Informatics (IAS9)\"},\"familyName\":\"Pirogov\",\"givenName\":\"Anton\"}],\"codeRepository\":\"https://github.com/Materials-Data-Science-and-Informatics/dirschema\",\"description\":\"Spec and validator for directories, files and metadata based on JSON Schema and regexes.\",\"developmentStatus\":\"https://www.repostatus.org/#wip\",\"identifier\":\"dirschema\",\"keywords\":[\"directory\",\"fair\",\"jsonschema\",\"metadata\",\"structure\",\"validation\"],\"license\":\"http://spdx.org/licenses/MIT\",\"maintainer\":{\"@type\":\"Person\",\"email\":\"a.pirogov@fz-juelich.de\",\"familyName\":\"Pirogov\",\"givenName\":\"Anton\"},\"name\":\"dirschema\",\"operatingSystem\":\"OS Independent\",\"runtimePlatform\":\"Python 3\",\"softwareHelp\":\"https://materials-data-science-and-informatics.github.io/dirschema\",\"softwareRequirements\":[{\"@type\":\"SoftwareApplication\",\"identifier\":\"entrypoints\",\"name\":\"entrypoints\",\"runtimePlatform\":\"Python 3\",\"version\":\"^0.4\"},{\"@type\":\"SoftwareApplication\",\"identifier\":\"h5py\",\"name\":\"h5py\",\"runtimePlatform\":\"Python 3\",\"version\":\"^3.4.0\"},{\"@type\":\"SoftwareApplication\",\"identifier\":\"jsonref\",\"name\":\"jsonref\",\"runtimePlatform\":\"Python 3\",\"version\":\"^0.2\"},{\"@type\":\"SoftwareApplication\",\"identifier\":\"jsonschema\",\"name\":\"jsonschema\",\"runtimePlatform\":\"Python 3\",\"version\":\"^4.4.0\"},{\"@type\":\"SoftwareApplication\",\"identifier\":\"numpy\",\"name\":\"numpy\",\"runtimePlatform\":\"Python 3\",\"version\":\"^1.21.2\"},{\"@type\":\"SoftwareApplication\",\"identifier\":\"pydantic\",\"name\":\"pydantic\",\"runtimePlatform\":\"Python 3\",\"version\":\"^1.8.2\"},{\"@type\":\"SoftwareApplication\",\"identifier\":\"python\",\"name\":\"python\",\"runtimePlatform\":\"Python 3\",\"version\":\"^3.8\"},{\"@type\":\"SoftwareApplication\",\"identifier\":\"ruamel.yaml\",\"name\":\"ruamel.yaml\",\"runtimePlatform\":\"Python 3\",\"version\":\"^0.17.16\"},{\"@type\":\"SoftwareApplication\",\"identifier\":\"typer\",\"name\":\"typer\",\"runtimePlatform\":\"Python 3\",\"version\":\"^0.9.0\"},{\"@type\":\"SoftwareApplication\",\"identifier\":\"typing-extensions\",\"name\":\"typing-extensions\",\"runtimePlatform\":\"Python 3\",\"version\":\"^4.5.0\"}],\"targetProduct\":{\"@type\":\"CommandLineApplication\",\"executableName\":\"dirschema\",\"name\":\"dirschema\",\"runtimePlatform\":\"Python 3\"},\"url\":\"https://materials-data-science-and-informatics.github.io/dirschema\",\"version\":\"0.1.0\"}\n",
                "readme": "![Project status](https://img.shields.io/badge/project%20status-alpha-%23ff8000) [ ![Docs](https://img.shields.io/badge/read-docs-success) ](https://materials-data-science-and-informatics.github.io/dirschema) [ ![CI](https://img.shields.io/github/actions/workflow/status/Materials-Data-Science-and-Informatics/dirschema/ci.yml?branch=main&label=ci) ](https://github.com/Materials-Data-Science-and-Informatics/dirschema/actions/workflows/ci.yml) [ ![Test Coverage](https://materials-data-science-and-informatics.github.io/dirschema/main/coverage_badge.svg) ](https://materials-data-science-and-informatics.github.io/dirschema/main/coverage) [ ![PyPIPkgVersion](https://img.shields.io/pypi/v/dirschema) ](https://pypi.org/project/dirschema/) <!-- --8<-- [start:abstract] --> # dirschema <br /> <div> <img style=\"center-align: middle;\" alt=\"DirSchema Logo\" src=\"https://raw.githubusercontent.com/Materials-Data-Science-and-Informatics/Logos/main/DirSchema/DirSchema_Logo_Text.png\" width=70% height=70% /> &nbsp;&nbsp; </div> <br /> A directory structure and metadata linter based on JSON Schema. [JSON Schema](https://json-schema.org/) is great for validating (files containing) JSON objects that e.g. contain metadata, but these are only the smallest pieces in the organization of a whole directory structure, e.g. of some dataset of project. When working on datasets of a certain kind, they might contain various types of data, each different file requiring different accompanying metadata, based on its file type and/or location. **DirSchema** combines JSON Schemas and regexes into a solution to enforce structural dependencies and metadata requirements in directories and directory-like archives. With it you can for example check that: * only files of a certain type are in a location (e.g. only `jpg` files in directory `img`) * for each data file there exists a metadata file (e.g. `test.jpg` has `test.jpg_meta.json`) * each metadata file is valid according to some JSON Schema If validating these kinds of constraints looks appealing to you, this tool is for you! **Dirschema features:** * Built-in support for schemas and metadata stored as JSON or YAML * Built-in support for checking contents of ZIP and HDF5 archives * Extensible validation interface for advanced needs beyond JSON Schema * Both a Python library and a CLI tool to perform the validation <!-- --8<-- [end:abstract] --> <!-- --8<-- [start:quickstart] --> ## Installation ``` pip install dirschema ``` ## Getting Started The `dirschema` tool needs as input: * a DirSchema YAML file (containing a specification), and * a path to a directory or file (e.g. zip file) that should be checked. You can run it like this: ``` dirschema my_dirschema.yaml DIRECTORY_OR_ARCHIVE_PATH ``` If the validation was successful, there will be no output. Otherwise, the tool will output a list of errors (e.g. invalid metadata, missing files, etc.). You can also use `dirschema` from other Python code as a library: ```python from dirschema.validate import DSValidator DSValidator(\"/path/to/dirschema\").validate(\"/dataset/path\") ``` Similarly, the method will return an error dict, which will be empty if the validation succeeded. <!-- --8<-- [end:quickstart] --> **You can find more information on using and contributing to this repository in the [documentation](https://materials-data-science-and-informatics.github.io/dirschema/main).** <!-- --8<-- [start:citation] --> ## How to Cite If you want to cite this project in your scientific work, please use the [citation file](https://citation-file-format.github.io/) in the [repository](https://github.com/Materials-Data-Science-and-Informatics/dirschema/blob/main/CITATION.cff). <!-- --8<-- [end:citation] --> <!-- --8<-- [start:acknowledgements] --> ## Acknowledgements We kindly thank all [authors and contributors](https://materials-data-science-and-informatics.github.io/dirschema/latest/credits). <div> <img style=\"vertical-align: middle;\" alt=\"HMC Logo\" src=\"https://github.com/Materials-Data-Science-and-Informatics/Logos/raw/main/HMC/HMC_Logo_M.png\" width=50% height=50% /> &nbsp;&nbsp; <img style=\"vertical-align: middle;\" alt=\"FZJ Logo\" src=\"https://github.com/Materials-Data-Science-and-Informatics/Logos/raw/main/FZJ/FZJ.png\" width=30% height=30% /> </div> <br /> This project was developed at the Institute for Materials Data Science and Informatics (IAS-9) of the Jülich Research Center and funded by the Helmholtz Metadata Collaboration (HMC), an incubator-platform of the Helmholtz Association within the framework of the Information and Data Science strategic initiative. <!-- --8<-- [end:acknowledgements] -->\n",
                "dependencies": "[tool.somesy.project] name = \"dirschema\" version = \"0.1.0\" description = \"Spec and validator for directories, files and metadata based on JSON Schema and regexes.\" license = \"MIT\" repository = \"https://github.com/Materials-Data-Science-and-Informatics/dirschema\" homepage = \"https://materials-data-science-and-informatics.github.io/dirschema\" documentation = \"https://materials-data-science-and-informatics.github.io/dirschema\" keywords = [\"jsonschema\", \"validation\", \"directory\", \"structure\", \"fair\", \"metadata\"] [[tool.somesy.project.people]] family-names = \"Pirogov\" given-names = \"Anton\" email = \"a.pirogov@fz-juelich.de\" orcid = \"https://orcid.org/0000-0002-5077-7497\" contribution_begin = \"2021-09-22\" contribution = \"Main author and maintainer.\" contribution_types = [\"code\"] author = true maintainer = true [tool.poetry] # ---- DO NOT EDIT, managed by somesy ---- name = \"dirschema\" version = \"0.1.0\" description = \"Spec and validator for directories, files and metadata based on JSON Schema and regexes.\" authors = [\"Anton Pirogov <a.pirogov@fz-juelich.de>\"] license = \"MIT\" repository = \"https://github.com/Materials-Data-Science-and-Informatics/dirschema\" homepage = \"https://materials-data-science-and-informatics.github.io/dirschema\" documentation = \"https://materials-data-science-and-informatics.github.io/dirschema\" keywords = [\"jsonschema\", \"validation\", \"directory\", \"structure\", \"fair\", \"metadata\"] # ---------------------------------------- readme = \"README.md\" classifiers = [ # see https://pypi.org/classifiers/ \"Development Status :: 3 - Alpha\", \"License :: OSI Approved :: MIT License\", \"Environment :: Console\", \"Operating System :: OS Independent\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", \"Topic :: Software Development\", \"Topic :: Scientific/Engineering\", \"Typing :: Typed\", ] # the Python packages that will be included in a built distribution: packages = [{include = \"dirschema\", from = \"src\"}] # always include basic info for humans and core metadata in the distribution, # include files related to test and documentation only in sdist: include = [ \"*.md\", \"LICENSE\", \"LICENSES\", \".reuse/dep5\", \"CITATION.cff\", \"codemeta.json\", { path = \"mkdocs.yml\", format = \"sdist\" }, { path = \"docs\", format = \"sdist\" }, { path = \"tests\", format = \"sdist\" }, ] maintainers = [\"Anton Pirogov <a.pirogov@fz-juelich.de>\"] [tool.poetry.dependencies] python = \"^3.8,<3.11\" pydantic = \"^1.8.2\" \"ruamel.yaml\" = \"^0.17.16\" jsonref = \"^0.2\" h5py = { version = \"^3.4.0\", optional = true } numpy = { version = \"^1.21.2\", optional = true } jsonschema = \"^4.4.0\" entrypoints = \"^0.4\" typing-extensions = \"^4.5.0\" typer = \"^0.9.0\" [tool.poetry.group.dev.dependencies] poethepoet = \"^0.18.1\" pre-commit = \"^3.1.1\" pytest = \"^7.2.2\" pytest-cov = \"^4.0.0\" hypothesis = \"^6.68.2\" licensecheck = \"^2023.1.1\" [tool.poetry.group.docs.dependencies] mkdocs = \"^1.4.2\" mkdocstrings = {extras = [\"python\"], version = \"^0.21.2\"} mkdocs-material = \"^9.1.6\" mkdocs-gen-files = \"^0.4.0\" mkdocs-literate-nav = \"^0.6.0\" mkdocs-section-index = \"^0.3.5\" mkdocs-macros-plugin = \"^0.7.0\" markdown-include = \"^0.8.1\" pymdown-extensions = \"^9.11\" markdown-exec = {extras = [\"ansi\"], version = \"^1.6.0\"} mkdocs-coverage = \"^0.2.7\" mike = \"^1.1.2\" anybadge = \"^1.14.0\" black = \"^23.3.0\" [tool.poetry.group.all-extras.dependencies] h5py = \"^3.8.0\" numpy = \"^1.24.3\" [tool.poetry.extras] h5 = [\"h5py\", \"numpy\"] [tool.poetry.scripts] dirschema = 'dirschema.cli:app' [tool.poetry.plugins.dirschema_validator] # dirschema supports entry-point-based validation plugins. # The default pydantic handler can be subclassed or # can serve as a template for your custom validation plugins. pydantic = \"dirschema.json.handler_pydantic:PydanticHandler\" [build-system] requires = [\"poetry-core\"] build-backend = \"poetry.core.masonry.api\" # NOTE: You can run the following with \"poetry poe TASK\" [tool.poe.tasks] init-dev = { shell = \"pre-commit install\" } lint = \"pre-commit run\" # pass --all-files to check everything test = \"pytest\" # pass --cov to also collect coverage info docs = \"mkdocs build\" # run this to generate local documentation licensecheck = \"licensecheck\" # run this when you add new deps # Tool Configurations # ------------------- [tool.pytest.ini_options] pythonpath = [\"src\"] addopts = \"--cov-report=term-missing:skip-covered\" filterwarnings = [ # Example: # \"ignore::DeprecationWarning:importlib_metadata.*\" ] [tool.coverage.run] source = [\"dirschema\"] [tool.coverage.report] exclude_lines = [ \"pragma: no cover\", \"def __repr__\", \"if self.debug:\", \"if settings.DEBUG\", \"raise AssertionError\", \"raise NotImplementedError\", \"if 0:\", \"if TYPE_CHECKING:\", \"if __name__ == .__main__.:\", \"class .*\\\\bProtocol\\\\):\", \"@(abc\\\\.)?abstractmethod\", ] [tool.ruff.lint] extend-select = [\"B\", \"D\", \"I\", \"S\"] ignore = [\"D203\", \"D213\", \"D407\", \"B008\"] [tool.ruff.lint.per-file-ignores] \"**/{tests,docs}/*\" = [\"ALL\"] [tool.licensecheck] using = \"poetry\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/displam",
            "repo_link": "https://gitlab.com/dlr-sy/displam",
            "content": {
                "codemeta": "",
                "readme": "[![doi](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.12628293-red.svg)](https://zenodo.org/records/12628293) [![PyPi](https://img.shields.io/pypi/v/displam?label=PyPi)](https://pypi.org/project/displam/) [![pipeline status](https://gitlab.com/dlr-sy/displam/badges/disp_python/pipeline.svg)]() # Displam Displam is a legacy Fortran-based program for the calculation of the phase velocity of Lamb waves of plate composite structures. > Installation from source requires an active open source fortran compiler (gfortran). Intel Fortran is not yet supported. ## Downloading Use GIT to get the latest code base. From the command line, use ``` git clone https://gitlab.dlr.de/fa_sw/displam displam ``` If you check out the repository for the first time, you have to initialize all submodule dependencies first. Execute the following from within the repository. ``` git submodule update --init --recursive ``` To update all refererenced submodules to the latest production level, use ``` git submodule foreach --recursive 'git pull origin $(git config -f $toplevel/.gitmodules submodule.$name.branch || echo master)' ``` ## Installation Displam can be installed directly using pip ``` pip install displam ``` or from source using [poetry](https://python-poetry.org). If you don't have [poetry](https://python-poetry.org) installed, run ``` pip install poetry --pre --upgrade ``` to install the latest version of [poetry](https://python-poetry.org) within your python environment. Use ``` poetry update ``` to update all dependencies in the lock file or directly execute ``` poetry install ``` to install all dependencies from the lock file. Last, you should be able to use Displam as a CLI tool: ```cmd displam <Input file> ``` If you omit the input file name displam will try to read the input data from a file named sample.inp. An output file disp.txt and a log file log.txt will be created. ## Output The output file contains the dispersion data for the problem defined in the input file. A header line is followed by one record per line. Each record consists of the following data: ``` 1. fd/(MHz*mm) - frequency * plate thickness 2. cp/(km/sec) - phase velocity 3. w/(1/�sec) - circular frequency 4. k/(1/m) - wave number 5. mode - wave mode ('S' for symmetric, 'A' for antisymmetric and 'H' for quasi-horizontal shear mode) 6. uu0 - uu2 - complex displacement vector at the upper surface of the laminate 7. ul0 - ul2 - complex displacement vector at the lower surface of the laminate ``` The columns are seperated by tab-characters. You can open the output file with Excel or any text editor. ## Example Copy and paste the following text up to the end of this file to a new text file to use it as a sample input file as explained under [Installation](#Installation) ``` # # Input file for displam # ############################################################### File format ### # # Input is parsed line by line. Everything after a '#' character is ignored, # so it's possible to append comments to input lines. Empty lines are # ignored. White space separates keywords and input parameters and may be # entered as spaces or tabs. Lines may not be longer than 1000 characters # (excluding comments). # # The following four sections must be present in this file. Each section is # initiated with its keyword in capital letters in an otherwise empty line. # # LAYUP - stacking sequence of the plate # WAVE - wave propagation parameters # RANGE - parameter ranges for circular frequency and phase velocity # MATERIAL - material database # # The format of the input lines in each section is explained in the comments # above each section. ############################################################# LAYUP section ### # # The layers of the stacking sequence are specified from top to bottom # of the laminate. Three parameters must be specified per line: # # 1. material id string as specified in the MATERIAL section # 2. layer thickness in mm # 3. layer orientation in degrees # # A layer orientation of 0 degree means that the 1-direction of local (layer # material) and global (laminate) direction coincide. # LAYUP cfrp_generic 0.250 0. titanium 0.500 90. cfrp_generic 0.250 0. ############################################################## WAVE section ### # # pa - angle of wave propagation w.r.t. global coordinates in degrees. # ia - angle of incident wave w.r.t. transverse axis in degrees. # 90.0 is horizontal (in-plane) incident wave. WAVE pa 0.01 ia 90.00 # values other than 90� are not validated yet ############################################################# RANGE section ### # # cp - 3 values for start, end, and number of increments for phase velocity. # Unit of phase velocity is in km / s. # fq - 3 values for start, end, and number of increments for frequency. # Unit of frequencies must be MHz. RANGE cp 0.1 14.0 200 # unit is km/s fq 0.1 3.0 200 # unit is MHz ########################################################## MATERIAL section ### # # Each line starts with an identification string of up to 12 alphanumeric # characters and '_'. It is followed by the number of independent stiffness # constants (nsc) and the density in g / cm^3. # # Transversely isotropic (nsc = 5) and orthotropic (nsc = 9) materials are # supported. After the density the stiffness parameters must be specified # in GPa for Young's and shear moduli in the following order # # transversely isotropic E1, E2, G12, G23, nu12 # orthotropic E1, E2, E3, G12, G23, G31, nu12, nu23, nu31 # # The 3-direction is the out-of-plane direction. # MATERIAL # 123456789012 nsc rho stiffness coefficients alu 5 2.70 69.9 70.1 26.30 26.20 0.33 titanium 5 4.50 115.9 116.1 43.95 43.85 0.32 cfrp_generic 5 1.55 150.0 9.00 5.00 4.00 0.30 cfrp_rose 5 1.58 123.9 11.256 6.73 3.81 0.31 ``` ## Contact * [Marc Garbade](mailto:marc.garbade@dlr.de)\n",
                "dependencies": "# TOML file create Displam # # @note: TOML file # Created on 29.11.2023 # # @version: 1.0 # ---------------------------------------------------------------------------------------------- # @requires: # - # # @change: # - # # @author: garb_ma [DLR-SY,STM Braunschweig] # ---------------------------------------------------------------------------------------------- [build-system] requires = [\"poetry-core>=1.0\"] build-backend = \"poetry.core.masonry.api\" [tool.poetry] name = \"displam\" version = \"1.5.1\" description = \"Calculation of the phase velocity of Lamb waves in plate composite structures\" authors = [\"Baaran, Jens <jens.baaran@haw-hamburg.de>\"] maintainers = [\"Garbade, Marc <marc.garbade@dlr.de>\", \"Raddatz, Florian <florian.raddatz@dlr.de>\", \"Schmidt, Daniel <daniel.schmidt@dlr.de>\", \"Moix-Bonet, Maria <maria.moix-bonet@dlr.de>\"] license = \"MIT\" packages = [{include=\"**/*\", from=\"bin\"}] exclude = [\"bin/*.py\"] repository = \"https://gitlab.com/dlr-sy/displam\" keywords = [\"analysis\",\"monitoring\",\"composite\"] readme = \"README.md\" classifiers = [ \"Development Status :: 7 - Inactive\", \"Topic :: Scientific/Engineering\", \"Programming Language :: Python :: 2\", \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: MIT License\", \"Operating System :: OS Independent\" ] [[tool.poetry.source]] name = \"dlr-pypi\" url = \"https://pypi.python.org/simple\" priority = \"primary\" [[tool.poetry.source]] name = \"dlr-sy\" url = \"https://gitlab.dlr.de/api/v4/groups/541/-/packages/pypi/simple\" priority = \"supplemental\" [tool.poetry.build] script = \"config/build.py\" generate-setup-file = false [tool.poetry.dependencies] python = \"~2.7 || ^3.5\" [tool.poetry.group.dev.dependencies] pyx-core = [{version = \"^1.18\", python = \"~2.7 || ^3.5,<3.7\"}, {git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake.git\", develop=true, python=\"^3.7\"}] \t\t\t [tool.poetry.scripts] displam = \"displam.cli:main\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/dl4pude",
            "repo_link": "https://github.com/PedestrianDynamics/DL4PuDe",
            "content": {
                "codemeta": "",
                "readme": "# `DL4PuDe:` A hybrid framework of deep learning and visualization for pushing behavior detection in pedestrian dynamics [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8257076.svg)](https://doi.org/10.5281/zenodo.8257076 ) [![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](./LICENSE) ![Python 3.7 | 3.8](https://img.shields.io/badge/Python-3.7|3.8-blue.svg) ![GPU](https://img.shields.io/badge/GPU-No-yellow) ![RAM16GB](https://img.shields.io/badge/RAM-16GB-red) This repository is for the DL4PuDe framework, along with its published [paper](https://www.mdpi.com/1424-8220/22/11/4040/htm), which is as follows. ``` Alia, Ahmed, Mohammed Maree, and Mohcine Chraibi. 2022. \"A Hybrid Deep Learning and Visualization Framework for Pushing Behavior Detection in Pedestrian Dynamics\" Sensors 22, no. 11: 4040. ``` ## Content 1. <a href=\"#aim\"> Framework aim. </a> 2. <a href=\"#motivation\"> Framework Motivation. </a> 3. <a href=\"#defention\"> Pushing Behavior Defention. </a> 4. <a href=\"#architecture\"> Framework Architecture. </a> 5. <a href=\"#install\"> How to install and use the framework. </a> 6. <a href=\"#demo\"> Demo. </a> 7. <a href=\"#videos\"> Experiments Videos. </a> 8. <a href=\"#cnn\"> CNN-based Classifiers </a> * <a href=\"#cnnsource\"> Source code for building and training CNN-based classifiers. </a> * <a href=\"#trained\"> Trained CNN-based classifiers. </a> * <a href=\"#evaluate\"> Source code for evaluating the trained CNN-based classifiers. </a> * <a href=\"#test\"> Test sets. </a> 9. <a href=\"#list\"> List of papers that cited this work. </a> ## Aim of `Dl4PuDe` Framework <a name=\"aim\"> `Dl4PuDe` aims to automatically detect and annotate pushing behavior at the patch level in video recordings of human crowds. ## Motivation of `Dl4PuDe` Framework <a name=\"motivation\"> To assist researchers in the field of crowd dynamics in gaining a better understanding of pushing dynamics, which is crucial for effectively managing a comfortable and safe crowd. ## Pushing Behavior Defention <a name=\"defention\"> In this article, pushing can be defined as a behavior that pedestrians use to reach a target faster. ###### An example of pushing strategy <img src=\"./files/example.gif\" width=\"300px\"/> ###### Entering the event faster <img src=\"./files/snakemotion.jpg\" width=\"300px\"/> ## The Architecture of `DL4PuDe` <a name=\"architecture\"> `DL4PuDe` mainly relied on the power of EfficientNet-B0-based classifier, RAFT and wheel visualization methods. <img src=\"./files/framework1.png\"/> Kindly note that we use the <a href=\"https://github.com/princeton-vl/RAFT\" />[RAFT repository] </a> for optical flow estimation in our project. **Example** <table border=\"0\" width=\"100%\" align=\"center\"> <tr> <th align=\"cenetr\"> Input video </th> <th align=\"cenetr\"> Output video * </th> </tr> <tr> <td align=\"center\"> <img src=\"./files/input150-distorted.gif\" width=\"300\"/> </td> <td align=\"center\"> <img src=\"./files/output150-distorted.gif\" width=\"200\"/> </td> </tr> <tr> <td colspan=\"2\"> * The framework detects pushing patches every 12 frames (12/25 s), the red boxes refer to the pushing patches. </td> </tr> </table> ## Installation <a name=\"install\"> 1. Clone the repository in your directory. ``` git clone https://github.com/PedestrianDynamics/DL4PuDe.git ``` 2. Install the required libraries. ``` pip install -r libraries.txt ``` 3. Run the framework. ``` python3 run.py --video [input video path] --roi [\"x coordinate of left-top ROI corner\" \"y coordinate of left-top ROI corner\" \"x coordinate of right-bottom ROI corner\" \"y coordinate of right-bottom ROI corner\" ] --patch [rows cols] --ratio [scale of video] --angle [angle in degrees for rotating the input video to make crowd flow direction from left to right ---> ] ``` ## Demo <a name=\"demo\"> >Run the following command ``` python3 run.py --video ./videos/150.mp4 --roi 380 128 1356 1294 --patch 3 3 --ratio 0.5 --angle 0 ``` > Then, you will see the following details. <img src=\"./files/run.png\"/> > When the progress of the framework is complete, it will generate the annotated video in the framework directory. Please note that the \"150 annotated video\" is available on the directory root under the \"150-demo.mp4\" name. ## Experiments Videos <a name=\"videos\"> The original experiments videos that are used in this work are available through the [Pedestrian Dynamics Data Archive hosted](http://ped.fz-juelich.de/da/2018crowdqueue) by the Forschungszentrum Juelich. Also, the undistorted videos are available by [this link.](https://drive.google.com/drive/folders/16eZhC9mnUQUXxUeIUXd6xwBU2fSf3qCz?usp=sharing) ## CNN-based Classifiers <a name=\"cnn\"> We use four CNN-based classifiers for building and evaluating our classifier, including EfficientNet-B0, MobileNet, InceptionV3, and ResNet50. The source code for building, training and evaluating the CNN-based classifiers, as well as the trained classifiers are available in the below links. 1. Source code for building and training the CNN-based classifiers. <a name=\"cnnsource\"> * [EfficientNet-B0-based classifier.](./CNN/CNN-Architectures/efficientNetB0.ipynb) * [MobileNet-based classifier.](./CNN/CNN-Architectures/InceptionV3.ipynb) * [InceptionV3-based classifier.](./CNN/CNN-Architectures/InceptionV3.ipynb) * [ResNet50-based classifier.](./CNN/CNN-Architectures/ResNet50.ipynb) 2. [Trained CNN-based classifiers.](https://drive.google.com/drive/folders/1vmgYufnt4_NNQUE9PGYZLkrn5DmErENu?usp=sharing) <a name=\"trained\"> 3. CNN-based classifiers Evaluation. <a name=\"#evaluate\"> * [Patch-based medium RAFT-MIM12 dataset.](./CNN/Classifiers-evaluation/patch-based-medium-RAFT-MIM12/) * [Patch-based medium RAFT-MIM25 dataset.](./CNN/Classifiers-evaluation/patch-based-medium-RAFT-MIM25/) * [Patch-based small RAFT-MIM12 dataset.](./CNN/Classifiers-evaluation/patch-based-small-RAFT-MIM12/) * [Patch-based small RAFT-MIM25 dataset.](./CNN/Classifiers-evaluation/patch-based-small-RAFT-MIM25/) * [Patch-based medium FB-MIM12 dataset.](./CNN/Classifiers-evaluation/patch-based-medium-FB-MM12/) * [Frame-based RAFT-MIM12 dataset.](./CNN/Classifiers-evaluation/frame-based-RAFT-MIM12/) * [Frame-based RAFT-MIM25 dataset.](./CNN/Classifiers-evaluation/frame-based-RAFT-MIM25/) 4. [Patch-based MIM test sets.](./CNN/Classifiers-evaluation/test-sets/) <a name=\"test\"> 5. MIM training and validation sets are available from the corresponding authors upon request. ## List of papers that cited this work <a name=\"list\"> To access the list of papers citing this work, kindly click on this [link.](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14553227952079022657&as_sdt=5) ## Citation If you utilize this framework or the generated dataset in your work, please cite it using the following BibTex entry: ``` Alia, Ahmed, Mohammed Maree, and Mohcine Chraibi. 2022. \"A Hybrid Deep Learning and Visualization Framework for Pushing Behavior Detection in Pedestrian Dynamics\" Sensors 22, no. 11: 4040. ``` ## Acknowledgments * This work was funded by the German Federal Ministry of Education and Research (BMBF: funding number 01DH16027) within the Palestinian-German Science Bridge project framework, and partially by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)--491111487. * Thanks to the Forschungszentrum Juelich, Institute for Advanced Simulation-7, for making the Pedestrian Dynamics Data Archive publicly accessible under the CC Attribution 4.0 International license. * Thanks to Anna Sieben, Helena Lügering, and Ezel Üsten for developing the rating system and annotating the pushing behavior in the video experiments. * Thanks to the authors of the paper titled ``RAFT: Recurrent All Pairs Field Transforms for Optical Flow'' for making the RAFT source code available.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/earth-system-model-evaluation-tool-esmvaltool",
            "repo_link": "https://github.com/ESMValGroup/ESMValTool",
            "content": {
                "codemeta": "",
                "readme": "[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity) [![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/) [![Documentation Status](https://readthedocs.org/projects/esmvaltool/badge/?version=latest)](https://esmvaltool.readthedocs.io/en/latest/?badge=latest) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3401363.svg)](https://doi.org/10.5281/zenodo.3401363) [![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/#ESMValGroup_Lobby:gitter.im) [![CircleCI](https://circleci.com/gh/ESMValGroup/ESMValTool/tree/main.svg?style=svg)](https://circleci.com/gh/ESMValGroup/ESMValTool/tree/main) [![Test in Full Development Mode](https://github.com/ESMValGroup/ESMValTool/actions/workflows/test-development.yml/badge.svg)](https://github.com/ESMValGroup/ESMValTool/actions/workflows/test-development.yml) [![Codacy Badge](https://app.codacy.com/project/badge/Grade/79bf6932c2e844eea15d0fb1ed7e415c)](https://app.codacy.com/gh/ESMValGroup/ESMValTool/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade) [![Docker Build Status](https://img.shields.io/docker/automated/esmvalgroup/esmvaltool)](https://hub.docker.com/r/esmvalgroup/esmvaltool/) [![Anaconda-Server Badge](https://img.shields.io/conda/vn/conda-forge/ESMValTool?color=blue&label=conda-forge&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/esmvaltool) ![stand with Ukraine](https://badgen.net/badge/stand%20with/UKRAINE/?color=0057B8&labelColor=FFD700) ![esmvaltoollogo](https://raw.githubusercontent.com/ESMValGroup/ESMValTool/main/doc/sphinx/source/figures/ESMValTool-logo-2-glow.png) - [**Documentation**](https://docs.esmvaltool.org/en/latest/) - [**ESMValTool Website**](https://www.esmvaltool.org/) - [**ESMValTool Tutorial**](https://tutorial.esmvaltool.org/index.html) - [**ESMValGroup Project on GitHub**](https://github.com/ESMValGroup) - [**Gallery**](https://docs.esmvaltool.org/en/latest/gallery.html) - [**`conda-forge` package feedstock**](https://github.com/conda-forge/esmvaltool-suite-feedstock) # Introduction ESMValTool is a community-developed climate model diagnostics and evaluation software package, driven both by computational performance and scientific accuracy and reproducibility. ESMValTool is open to both users and developers, encouraging open exchange of diagnostic source code and evaluation results from the Coupled Model Intercomparison Project [CMIP](https://www.wcrp-climate.org/wgcm-cmip) ensemble. For a comprehensive introduction to ESMValTool please visit our [documentation](https://docs.esmvaltool.org/en/latest/introduction.html) page. # Running esmvaltool Diagnostics from ESMValTool are run using [recipe](https://docs.esmvaltool.org/en/latest/recipes/index.html) files that contain pointers to the requested data types, directives for the preprocessing steps that data will be subject to, and directives for the actual diagnostics that will be run with the now preprocessed data. Data preprocessing is done via the [ESMValCore](https://docs.esmvaltool.org/projects/ESMValCore/en/latest/quickstart/index.html) package, a pure Python, highly-optimized scientific library, developed by the ESMValTool core developers, and that performs a number of common analysis tasks such as regridding, masking, levels extraction etc. [Diagnostics](https://docs.esmvaltool.org/en/latest/develop/diagnostic.html) are written in a variety of programming languages (Python, NCL, R, Julia) and are developed by the wider scientific community, and included after a scientific and technical review process. # Input data ESMValTool can run with the following types of [data as input](https://docs.esmvaltool.org/en/latest/input.html): - CMIP6 - CMIP5 - CMIP3 - [observational and re-analysis datasets](https://docs.esmvaltool.org/en/latest/input.html#supported-datasets-for-which-a-cmorizer-script-is-available) - obs4MIPs - ana4mips - CORDEX ([work in progress](https://docs.esmvaltool.org/en/latest/input.html#cordex-note)) # Getting started Please see [getting started](https://docs.esmvaltool.org/en/latest/quickstart/index.html) on our instance of Read the Docs as well as [ESMValTool tutorial](https://tutorial.esmvaltool.org/index.html). The tutorial is a set of lessons that together teach skills needed to work with ESMValTool in climate-related domains. ## Getting help The easiest way to get help, if you cannot find the answer in the documentation in our [docs](https://docs.esmvaltool.org), is to open an [issue on GitHub](https://github.com/ESMValGroup/ESMValTool/issues). ## Contributing If you would like to contribute a new diagnostic or feature, please have a look at our [contribution guidelines](https://docs.esmvaltool.org/en/latest/community/index.html).\n",
                "dependencies": "[build-system] requires = [\"setuptools >= 40.6.0\", \"wheel\", \"setuptools_scm>=6.2\"] build-backend = \"setuptools.build_meta\" [tool.setuptools_scm] version_scheme = \"release-branch-semver\" [tool.ruff] fix = true line-length = 79 show-fixes = true [tool.ruff.lint] select = [\"ALL\"] ignore = [ # TODO: when replacing `prospector` with `ruff`, ignore rules that are # overly technical or conflict with other rules. \"E501\", # Disable line-too-long as this is taken care of by the formatter. \"D105\", # Disable Missing docstring in magic method as these are well defined. ] [tool.ruff.lint.per-file-ignores] \"tests/**.py\" = [ \"B011\", # `assert False` is valid test code. # Docstrings in tests are only needed if the code is not self-explanatory. \"D100\", # Missing docstring in public module \"D101\", # Missing docstring in public class \"D102\", # Missing docstring in public method \"D103\", # Missing docstring in public function \"D104\", # Missing docstring in public package ] \"test_*.py\" = [ \"B011\", # `assert False` is valid test code. # Docstrings in tests are only needed if the code is not self-explanatory. \"D100\", # Missing docstring in public module \"D101\", # Missing docstring in public class \"D102\", # Missing docstring in public method \"D103\", # Missing docstring in public function \"D104\", # Missing docstring in public package ] [tool.ruff.lint.isort] known-first-party = [\"esmvaltool\"] [tool.ruff.lint.pydocstyle] convention = \"numpy\" # Configure linters that are run by Prospector [tool.pylint.main] jobs = 1 # Running more than one job in parallel crashes prospector. [tool.pylint.basic] good-names = [ \"_\", # Used by convention for unused variables \"i\", \"j\", \"k\", # Used by convention for indices \"logger\", # Our preferred name for the logger ] [tool.pylint.format] max-line-length = 79 [tool.pylint.\"messages control\"] disable = [ \"import-error\", # Needed because Codacy does not install dependencies \"file-ignored\", # Disable messages about disabling checks \"line-too-long\", # Disable line-too-long as this is taken care of by the formatter. \"locally-disabled\", # Disable messages about disabling checks ] [tool.pydocstyle] convention = \"numpy\"\n#!/usr/bin/env python \"\"\"ESMValTool installation script.\"\"\" import json import os import re import sys from pathlib import Path from setuptools import Command, setup PACKAGES = [ \"esmvaltool\", ] REQUIREMENTS = { # Installation script (this file) dependencies \"setup\": [ \"setuptools_scm\", ], # Installation dependencies # Use with pip install . to install from source \"install\": [ \"aiohttp\", \"cartopy\", \"cdo\", \"cdsapi\", \"cf-units\", \"cfgrib\", \"cftime\", \"cmocean\", \"dask!=2024.8.0\", # https://github.com/dask/dask/issues/11296 \"distributed\", \"ecmwf-api-client\", \"eofs\", \"ESMPy\", # not on PyPI \"esmvalcore\", \"esmf-regrid>=0.10.0\", # iris-esmf-regrid #342 \"fiona\", \"fire\", \"fsspec\", \"GDAL\", \"ipython<9.0\", # github.com/ESMValGroup/ESMValCore/issues/2680 \"jinja2\", \"joblib\", \"lime\", \"mapgenerator>=1.0.5\", \"matplotlib\", \"natsort\", \"nc-time-axis\", \"netCDF4\", \"numba\", \"numpy!=1.24.3\", # severe masking bug \"openpyxl\", \"packaging\", \"pandas\", \"progressbar2\", \"psyplot>=1.5.0\", # psy*<1.5.0 are not py312 compat \"psy-maps>=1.5.0\", \"psy-reg>=1.5.0\", \"psy-simple>=1.5.0\", \"pyproj>=2.1\", \"pys2index\", \"python-dateutil\", \"pyyaml\", \"rasterio>=1.3.10\", \"requests\", \"ruamel.yaml\", \"scikit-image\", \"scikit-learn>=1.4.0\", # github.com/ESMValGroup/ESMValTool/issues/3504 \"scipy\", \"scitools-iris>=3.11\", \"seaborn\", \"seawater\", \"shapely>=2\", \"xarray>=0.12.0\", \"xesmf>=0.7.1\", \"xgboost>1.6.1\", # github.com/ESMValGroup/ESMValTool/issues/2779 \"xlsxwriter\", \"zarr\", ], # Test dependencies (unit tests) # Execute `pip install .[test]` once and then use `pytest` to run tests \"test\": [ \"pre-commit\", \"pytest>=3.9,!=6.0.0rc1,!=6.0.0\", \"pytest-cov>=2.10.1\", \"pytest-env\", \"pytest-html!=2.1.0\", \"pytest-metadata>=1.5.1\", \"pytest-mock\", \"pytest-xdist\", ], # Documentation dependencies \"doc\": [ \"autodocsumm>=0.2.2\", \"nbsphinx\", \"sphinx>=6.1.3\", \"pydata-sphinx-theme\", ], # Development dependencies # Use pip install -e .[develop] to install in development mode \"develop\": [ \"codespell\", \"docformatter\", \"imagehash\", \"pre-commit\", \"prospector[with_pyroma]>=1.12\", \"vprof\", \"yamllint\", ], } def discover_python_files(paths, ignore): \"\"\"Discover Python files.\"\"\" def _ignore(path): \"\"\"Return True if `path` should be ignored, False otherwise.\"\"\" return any(re.match(pattern, path) for pattern in ignore) for path in sorted(set(paths)): for root, _, files in os.walk(path): if _ignore(path): continue for filename in files: filename = os.path.join(root, filename) if filename.lower().endswith(\".py\") and not _ignore(filename): yield filename class RunLinter(Command): \"\"\"Class to run a linter and generate reports.\"\"\" user_options = [] def initialize_options(self): \"\"\"Do nothing.\"\"\" def finalize_options(self): \"\"\"Do nothing.\"\"\" def install_deps_temp(self): \"\"\"Try to temporarily install packages needed to run the command.\"\"\" if self.distribution.install_requires: self.distribution.fetch_build_eggs( self.distribution.install_requires ) if self.distribution.tests_require: self.distribution.fetch_build_eggs(self.distribution.tests_require) def run(self): \"\"\"Run prospector and generate a report.\"\"\" check_paths = PACKAGES + [ \"setup.py\", \"tests\", \"util\", ] ignore = [ \"doc/\", ] # try to install missing dependencies and import prospector try: from prospector.run import main except ImportError: # try to install and then import self.distribution.fetch_build_eggs([\"prospector[with_pyroma]\"]) from prospector.run import main self.install_deps_temp() # run linter # change working directory to package root package_root = os.path.abspath(os.path.dirname(__file__)) os.chdir(package_root) # write command line files = discover_python_files(check_paths, ignore) sys.argv = [\"prospector\"] sys.argv.extend(files) # run prospector errno = main() sys.exit(errno) def read_authors(filename): \"\"\"Read the list of authors from .zenodo.json file.\"\"\" with Path(filename).open() as file: info = json.load(file) authors = [] for author in info[\"creators\"]: name = \" \".join(author[\"name\"].split(\",\")[::-1]).strip() authors.append(name) return \", \".join(authors) def read_description(filename): \"\"\"Read the description from .zenodo.json file.\"\"\" with Path(filename).open() as file: info = json.load(file) return info[\"description\"] setup( name=\"ESMValTool\", author=read_authors(\".zenodo.json\"), description=read_description(\".zenodo.json\"), long_description=Path(\"README.md\").read_text(), long_description_content_type=\"text/markdown\", url=\"https://www.esmvaltool.org\", download_url=\"https://github.com/ESMValGroup/ESMValTool\", license=\"Apache License, Version 2.0\", classifiers=[ \"Development Status :: 5 - Production/Stable\", \"Environment :: Console\", \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: Apache Software License\", \"Natural Language :: English\", \"Operating System :: POSIX :: Linux\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Topic :: Scientific/Engineering\", \"Topic :: Scientific/Engineering :: Atmospheric Science\", \"Topic :: Scientific/Engineering :: GIS\", \"Topic :: Scientific/Engineering :: Hydrology\", \"Topic :: Scientific/Engineering :: Physics\", ], packages=PACKAGES, # Include all version controlled files include_package_data=True, setup_requires=REQUIREMENTS[\"setup\"], install_requires=REQUIREMENTS[\"install\"], tests_require=REQUIREMENTS[\"test\"], extras_require={ \"develop\": REQUIREMENTS[\"develop\"] + REQUIREMENTS[\"test\"] + REQUIREMENTS[\"doc\"], \"doc\": REQUIREMENTS[\"doc\"], \"test\": REQUIREMENTS[\"test\"], }, entry_points={ \"console_scripts\": [ \"nclcodestyle = esmvaltool.utils.nclcodestyle.nclcodestyle:_main\", \"test_recipe = \" \"esmvaltool.utils.testing.recipe_settings.install_expand_run:main\", ], \"esmvaltool_commands\": [ \"colortables = \" \"esmvaltool.utils.color_tables.show_color_tables:ColorTables\", \"install = esmvaltool.install:Install\", \"data = esmvaltool.cmorizers.data.cmorizer:DataCommand\", ], }, cmdclass={ \"lint\": RunLinter, }, zip_safe=False, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/egsim",
            "repo_link": "https://github.com/rizac/egsim",
            "content": {
                "codemeta": "",
                "readme": "eGSIM is a web service for selecting and testing ground shaking models (GSIM) in Europe, developed by the [GFZ](https://www.gfz-potsdam.de/) in the framework of the Thematic Core Services for Seismology of [EPOS](https://www.epos-eu.org/) under the umbrella of [EFEHR](http://www.efehr.org/en/home/) <p align=\"middle\"> <a title='EFEHR' href='www.efehr.org'><img height='50' src='http://www.efehr.org/export/system/modules/ch.ethz.sed.bootstrap.efehr2021/resources/img/logos/efehr.png'></a> &nbsp; <a title='GFZ' href='https://www.gfz-potsdam.de/'><img height='50' src='https://www.gfz-potsdam.de/fileadmin/gfz/GFZ.svg'></a> &nbsp; <a title='EPOS' href='https://www.epos-eu.org/'><img height='50' src='https://www.epos-eu.org/themes/epos/logo.svg'></a> <br> </p> The web portal (and API documentation) is available at: # https://egsim.gfz-potsdam.de ## Citation > Zaccarelli, Riccardo; Weatherill, Graeme (2020): eGSIM - a Python library and web application to select and test Ground Motion models. GFZ Data Services. https://doi.org/10.5880/GFZ.2.6.2023.007 # Table of contents * [Installation](#installation) * [Usage](#usage) * [Packages upgrade](#packages-upgrade) * [Django](#django) * [Starting a Python terminal shell](#starting-a-python-terminal-shell) * [Complete DB reset](#Complete-DB-reset) * [Repopulating the DB](#Re-populating-the-DB) * [Admin panel](#admin-panel) * [Create a custom management command](#Create-a-custom-management-command) * [Add new predefined flatfiles](#Add-new-predefined-flatfiles) * [Add new regionalization](#Add-new-regionalization) DISCLAIMER: **This document does not cover the server installation of the web app**, which is publicly available at the URL above. **Here you can find instructions on**: - How to install eGSIM as local Python library (`import egsim.smtk` in your code) - (For developers and contributors) How to install the Django app locally for testing, features addition, maintenance # Installation ## Requirements ```bash sudo apt-get update # pre-requisite sudo apt-get install gcc # optional sudo apt-get install git python3-venv python3-pip python3-dev ``` (The command above are Ubuntu specific, in macOS install brew and type `brew install` instead of `apt-get install`. *Remove python3-dev as it does not exist on macOS*). This web service uses a *specific* version of Python (Open `setup.py` and check `python_requires=`. As of January 2022, it's `>=3.11`) *which you must install* in addition to the Python version required by your system, and use it. Any command `python3` hereafter will refer to the required Python version. ## Clone repository Select a `root directory` (e.g. `/root/path/to/egsim`), and clone egsim into the so-called egsim directory: ```bash git clone https://github.com/rizac/eGSIM.git egsim ``` ## Create and activate Python virtual env Move to whatever directory you want (usually the egsim directory above) and then: ```bash python3 -m venv .env/<ENVNAME> # create python virtual environment (venv) source .env/<ENVNAME>/bin/activate # activate venv ``` **NOTE: From now on, all following operations must have the virtualenv activated FIRST** ## Install Assuming you are in the egsim directory with a virtualenv <VENVNAME>: ```console source .env/<ENVNAME>/bin/activate pip install -r ./requirements.txt ``` ### eGSIM as local library If you want to use eGSIM locally using the strong motion toolkit package only (`from egsim.smtk import ...` in your code): ```console source .env/<ENVNAME>/bin/activate pip install -r ./requirements.lib.txt ``` #### Run tests (remember to `pip install pytest` first) ```bash pytest -vvv ./tests/smtk ``` ## Run Test (web app tests. For testing the library only, see above) > Note: the value of `DJANGO_SETTINGS_MODULE` in the examples below > must be changed in production Move in the `egsim directory` and type: ```bash export DJANGO_SETTINGS_MODULE=egsim.settings_debug; pytest -xvvv ./tests/ ``` (x=stop at first error, v*=increase verbosity). with coverage report: ```bash export DJANGO_SETTINGS_MODULE=egsim.settings_debug; pytest --cov=egsim --cov-report=html -xvvv ./tests/ ``` <details> <summary>Configure PyCharm</summary> For **PyCharm users**, you need to configure the environment variable for all tests. Go to: - Run - Edit Configurations - Python tests And then under **Environment variables:** add: `DJANGO_SETTINGS_MODULE=egsim.settings_debug` (type several env vars separated by ;) </details> # Usage > Note: the value of `DJANGO_SETTINGS_MODULE` in the examples below > must be changed in production If you didn't do already, perform a [Complete DB reset](#Complete-DB-reset) (**one-time only operation**) If you want to access the admin panel, see [the admin panel](#admin-panel). **To run the program in your local browser**, type: ```bash export DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py runserver ``` <details> <summary>Configure PyCharm</summary> For **PyCharm users**, you can implement a service, which can be run as any PyCharm configuration in debug mode, allowing to open the browser and stop at specific point in the code (the PyCharm window will popup automatically in case). To implement a service, go to: - Run - Edit Configurations - Add new configuration then under **Run**: - between `script` and `module` (should be a combo box) choose `script`, and in the next text field put `manage.py` - script parameters: `runserver` - And then under **Environment variables:** add: `DJANGO_SETTINGS_MODULE=egsim.settings_debug` (type several env vars separated by ;) You should see in the `Services` tab appearing the script name, so you can run / debug it normally </details> ## Packages upgrade ```console source .env/<ENVNAME>/bin/activate pip install --upgrade pip setuptools ``` Upgrade OpenQuake (**optional**). The operation below should be performed in very specific cases only (important bugfixes or features) because **being OpenQuake often backward incompatible** it might require additional code fixes and feedbacks from scientific experts or OpenQuake developers. First, **open `setup.py` and comment the line of `install_requires` where OpenQuake is installed** (should be starting with `openquake.engine`). Then (note that `pip install openquake` works but is not the recommended way): ```console pip install -r \"https://raw.githubusercontent.com/gem/oq-engine/master/requirements-py311-macos_x86_64.txt\" # pip install -r \"https://raw.githubusercontent.com/gem/oq-engine/master/requirements-py311-linux64.txt\" ``` Install eGSIM Python library, upgrading its dependencies: ```console pip install -U . && pip freeze >./requirements.lib.txt && pip install pytest ``` Run tests: ```console pytest -vvv ./tests/smtk ``` Install eGSIM web app, upgrading its dependencies: ```console pip install -U --upgrade-strategy eager \".[web]\" pip freeze > ./requirements.txt ``` Run tests: ```console export DJANGO_SETTINGS_MODULE=egsim.settings_debug; pytest -xvvv ./tests/ ``` Change `setup.py` and set the current OpenQuake version in `install_requires` (uncomment it if commented). Optionally, remove egsim from requirements.txt (it might interfere with Django web?*). Eventually, **commit and push** # Django Remember to **activate the Python virtualenv** in all examples below <details> <summary> Brief Introduction to some important concepts and key terms (click to show) </summary> - [Settings file](https://docs.djangoproject.com/en/stable/topics/settings/): A Django settings file contains all the configuration of your Django installation. The settings file referred in this document, included in this git repo, is for debug and local deployment only. On production, a separate settings file is used, located on the server outside the git repo and **not shared for security reasons**. - [manage.py](https://docs.djangoproject.com/en/stable/ref/django-admin/) or `django-admin` is Django's command-line utility for administrative tasks. It is invoked from the terminal within your Python virtualenv (see examples in this document) by providing the settings file via: ```bash export DJANGO_SETTINGS_MODULE=<settings_file_path> python manage.py <command> ``` Django allows also the implementation of custom management commands. eGSIM implements `egsim-init` in order to populate the db (more details below) - [app](https://docs.djangoproject.com/en/stable/intro/reusable-apps/) a Django app is a Python package that is specifically intended for use in a Django project. An application may use common Django conventions, such as having models, tests, urls, and views submodules. In our case, the Django project is the egsim root directory (created with the command `django-admin startproject egsim`), and the *Django apps* inside it are \"api\" (the core web API) and \"app\" (the *web app*, i.e. the part of eGSIM delivered over the Internet through a browser interface), that relies on the \"api\" code. Inside the settings file (variable `INSTALLED_APPS`) is configured the list of all applications that are enabled in the eGSIM project. This includes not only our \"api\" app, that tells Django to create the eGISM tables when initializing the database, but also several builtin Django apps, e.g. the Django `admin` app, visible through the [Admin panel](#admin-panel). </details> ## Starting a Python terminal shell > Note: the value of `DJANGO_SETTINGS_MODULE` in the examples below > must be changed in production Typing `python` on the terminal does not work as one needs to initialize Django settings. The Django `shell` command does this: ```bash export DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py shell ``` ## Get Django uploaded files directory If you did not set it explicitly in `settings.FILE_UPLOAD_TEMP_DIR` (by default is missing), then Django will put uploaded files in the standard temporary directory which you can get easily by typing: ```bash python -c \"import tempfile;print(tempfile.gettempdir())\" ``` ## Complete DB reset > Note: the value of `DJANGO_SETTINGS_MODULE` in the examples below > must be changed in production We perform a complete DB reset every time we change something in the Database schema (see `egsim.api.models.py`), e.g. a table, a column, a constraint. <details> <summary>(if you wonder why we do not use DB migrations, click here)</summary> The usual way to change a DB in a web app is to create and run migrations ([full details here](https://docs.djangoproject.com/en/stable/topics/migrations/)), which allow to keep track of all changes (moving back and forth if necessary) whilst preserving the data stored in the DB. However, none of those features is required in eGSIM: DB data is predefined and would be regenerated from scratch in any case after any new migration. Consequently, **upon changes in the DB, a complete DB reset is an easier procedure**. In any case (**just for reference**), the steps to create and run migrations in eGSIM are the following: ```bash export DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py makemigrations egsim --name <migration_name> export DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py migrate egsim ``` And then repopulate the db: ```bash export DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py egsim_init ``` Notes: - The `make_migration` command just generates a migration file, it doesn't change the db. The `migrate` command does that, by means of the migration files generated. For details on Django migrations, see: - https://realpython.com/django-migrations-a-primer/#changing-models - https://docs.djangoproject.com/en/stable/topics/migrations/#workflow - <migration_name> will be a suffix appended to the migration file, use it like you would use a commit message in `git`). - When running `migrate`, if the migration will introduce new non-nullable fields, maybe better to run `manage.py flush` first to empty all tables, to avoid conflicts \"egsim\" above is the app name. If you omit the app, all apps will be migrated. The command `migrate` does nothing if it detects that there is nothing to migrate </details> To perform a complete db reset: - delete or rename the database of the settings file used and *all* migration files. In dev mode they are: - `egsim/db.sqlite3` - `egsim/api/migrations/0001_initial.py` (there should be only one. If there are others, delete all of them) - Execute: ```bash export DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py makemigrations && python manage.py migrate && python manage.py egsim_init ``` - `git add` the newly created migration file (in dev mode it's `egsim/api/migrations/0001_initial.py`) - [**Optional**] re-add the Django admin superuser(s) as explained in the [admin panel](#admin-panel) Notes: - Commands explanation: - `makemigrations` creates the necessary migration file(s) from Python code and existing migration file(s) - `migrate` re-create the DB via the generated migration file(s) - `egsim_init` repopulates the db with eGSIM data ## Re-populating the DB We repopulate the DB when **its schema has not changed** but its data needs to, e.g., OpenQuake is upgraded, or new data is implemented (new regionalization or flatfile), or a bug in the code has been fixed. The operations are similar but simpler than a complete Db Rest: - delete or rename the database of the settings file used: - `egsim/db.sqlite3` - Execute: ```bash export DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py migrate && python manage.py egsim_init ``` - [**Optional**] most likely (not tested, please check) you need to re-add the Django admin superuser(s) as explained in the [admin panel](#admin-panel) ## Admin panel > Note: the value of `DJANGO_SETTINGS_MODULE` in the examples below > must be changed in production This command allows the user to check database data from the web browser. For further details, check the [Django doc](https://docs.djangoproject.com/en/stable/ref/django-admin/) The database must have been created and populated (see [Usage](#usage)). Create a superuser (to be done **once only** ): ```bash export DJANGO_SETTINGS_MODULE=\"egsim.settings_debug\";python manage.py createsuperuser ``` and follow the instructions. Start the program (see [Usage](#Usage)) and then navigate in the browser to `[SITE_URL]/admin` (in development mode, `http://127.0.0.1:8000/admin/`) *Note: Theoretically, you can modify db data from the browser, e.g., hide some model, regionalization or predefined flatfile. Persistent changes should be implemented in Python code and then run a [Complete DB reset](#Complete-DB-reset)* ## Create a custom management command See `egsim/api/management/commands/README.md`. The next two sections will describe how to store new data (regionalizations and flatfiles) that will be made available in eGSIM with the `egsim_init` command (see [Complete DB reset](#Complete-DB-reset) for details) ## Add new predefined flatfiles - Add the file (CSV or zipped CSV) in `managements/commands/data/flatfiles`. If the file is too big try to zip it. **If it is more than few tens of Mb, then do not commit it** (explain in the section `details` - see below - how to get the source file). When zipping in macOS you will probably need to [exclude or remove (after zipping) the MACOSX folder](https://stackoverflow.com/q/10924236)~~ - - Implement a new `FlatfileParser` class in `management/commands/flatfile_parsers`. Take another parser, copy it and follow instructions. The parser goal is to read the file and convert it into a harmonized HDF table - Add binding file -> parser in the Python `dict`: `management.commands._egsim_flatfiles.Command.PARSER` - (Optional) Add the file refs in `management/commands/data/references.yaml`, e.g. reference, url, the file name that will be used in the API (if missing, defaults to the file name without extension) - Repopulate all eGSIM tables (command `egsim_init`) Implemented flatfiles sources (click on the items below to expand) <details> <summary>ESM 2018 flatfile</summary> - Go to https://esm.mi.ingv.it//flatfile-2018/flatfile.php (with username and password, you must be registered beforehand it's relatively fast and simple) - Download `ESM_flatfile_2018.zip`, uncompress and extract `ESM_flatfile_SA.csv` from there - `ESM_flatfile_SA.csv` is our raw flatfile, compress it again (it's big) into this directory as `ESM_flatfile_2018_SA.zip` - If on macOS, type the command above to remove the macOS folder from the zip </details> ## Add new regionalization - Add two files *with the same basename* and extensions in `managements/commands/data/regionalization_files`: - <name>.geojson (regionalization, aka regions collection) and - <name>.json (region -> gsim mapping) See already implemented files for an example - (Optional) Add the file refs in `management/commands/data/references.yaml`, e.g. reference, url, the file name that will be used in the API (if missing, defaults to the file name without extension) - Repopulate all eGSIM tables (command `egsim_init`)\n",
                "dependencies": "asgiref==3.8.1 astroid==3.1.0 blosc2==2.6.2 certifi==2024.2.2 charset-normalizer==3.3.2 contourpy==1.2.1 coverage==7.4.4 cycler==0.12.1 decorator==5.1.1 dill==0.3.8 Django==5.0.4 docutils==0.21.1 # -e git+https://github.com/rizac/eGSIM.git@704f51b24afad75745c7985e9fdac00fad775c31#egg=egsim fonttools==4.51.0 h5py==3.11.0 idna==3.7 iniconfig==2.0.0 isort==5.13.2 kaleido==0.2.1 kiwisolver==1.4.5 matplotlib==3.8.4 mccabe==0.7.0 msgpack==1.0.8 ndindex==1.8 numexpr==2.10.0 numpy==1.26.4 openquake.engine==3.15.0 packaging==24.0 pandas==2.2.2 pillow==10.3.0 platformdirs==4.2.0 plotly==5.20.0 pluggy==1.4.0 psutil==5.9.8 py-cpuinfo==9.0.0 pylint==3.1.0 pyparsing==3.1.2 pyproj==3.6.1 pytest==8.1.1 pytest-cov==5.0.0 pytest-django==4.8.0 python-dateutil==2.9.0.post0 pytz==2024.1 PyYAML==6.0.1 pyzmq==25.1.2 requests==2.31.0 scipy==1.13.0 shapely==2.0.3 six==1.16.0 sqlparse==0.4.4 tables==3.9.2 tenacity==8.2.3 toml==0.10.2 tomlkit==0.12.4 tzdata==2024.1 urllib3==2.2.1\nfrom setuptools import setup, find_packages _README = \"\"\" Python and OpenQuake-based web service for selecting, comparing and testing Ground Shaking Intensity Models. \"\"\" setup( name='egsim', version='2.1.0', description=_README, url='https://github.com/rizac/eGSIM', packages=find_packages(exclude=['tests', 'tests.*']), python_requires='>=3.11', # Minimal requirements for the library (egsim.smtk package). # FOR DEV/TESTS, add: `pip install pytest` install_requires=[ 'openquake.engine==3.15.0', # 1st tested version was >3.5.0 'pandas>=2.2.2', 'pyyaml>=6.0', 'tables>=3.8.0', ], # List additional groups of dependencies here (e.g. development # dependencies). You can install these using the following syntax, # for example: # $ pip install -e \".[web]\" extras_require={ 'web': [ 'Django>=4.1.2', 'plotly>=5.10.0', 'kaleido>=0.2.1', # required by plotly to save images # test packages: 'pytest', 'pylint>=2.3.1', 'pytest-django>=3.4.8', 'pytest-cov>=2.6.1' ] }, author='r. zaccarelli', author_email='', maintainer='r. zaccarelli', maintainer_email='', classifiers=[ 'Development Status :: 1 - Beta', 'Intended Audience :: Education', 'Intended Audience :: Science/Research', 'License :: OSI Approved :: GNU Affero General Public License v3', 'Operating System :: OS Independent', 'Programming Language :: Python :: 3', 'Topic :: Scientific/Engineering', ], keywords=[ \"seismic hazard\", \"ground shaking intensity model\", \"gsim\", \"gmpe\", \"ground motion database\", \"flatfile\" ], license=\"AGPL3\", platforms=[\"any\"], # package_data={\"smtk\": [ # \"README.md\", \"LICENSE\"]}, # include_package_data=True, zip_safe=False, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ehrapy",
            "repo_link": "https://github.com/theislab/ehrapy",
            "content": {
                "codemeta": "",
                "readme": "[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![Build](https://github.com/theislab/ehrapy/actions/workflows/build.yml/badge.svg)](https://github.com/theislab/ehrapy/actions/workflows/build.yml) [![Codecov](https://codecov.io/gh/theislab/ehrapy/branch/master/graph/badge.svg)](https://codecov.io/gh/theislab/ehrapy) [![License](https://img.shields.io/github/license/theislab/ehrapy)](https://opensource.org/licenses/Apache2.0) [![PyPI](https://img.shields.io/pypi/v/ehrapy.svg)](https://pypi.org/project/ehrapy/) [![Python Version](https://img.shields.io/pypi/pyversions/ehrapy)](https://pypi.org/project/ehrapy) [![Read the Docs](https://img.shields.io/readthedocs/ehrapy/latest.svg?label=Read%20the%20Docs)](https://ehrapy.readthedocs.io/) [![Test](https://github.com/theislab/ehrapy/actions/workflows/test.yml/badge.svg)](https://github.com/theislab/ehrapy/actions/workflows/test.yml) [![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit) <img src=\"https://user-images.githubusercontent.com/21954664/156930990-0d668468-0cd9-496e-995a-96d2c2407cf5.png\" alt=\"ehrapy logo\"> # ehrapy overview ![fig1](https://github.com/user-attachments/assets/7927aa20-751c-4e73-8939-1e4b1c465570) ## Features - Exploratory and targeted analysis of Electronic Health Records - Quality control & preprocessing - Visualization & Exploration - Clustering & trajectory inference ## Installation You can install _ehrapy_ via [pip] from [PyPI]: ```console $ pip install ehrapy ``` ## Usage Please have a look at the [Usage documentation](https://ehrapy.readthedocs.io/en/latest/usage/usage.html) and the [tutorials](https://ehrapy.readthedocs.io/en/latest/tutorials/index.html). ```python import ehrapy as ep ``` ## Citation [Heumos, L., Ehmele, P., Treis, T. et al. An open-source framework for end-to-end analysis of electronic health record data. Nat Med (2024). https://doi.org/10.1038/s41591-024-03214-0](https://www.nature.com/articles/s41591-024-03214-0).\n",
                "dependencies": "[build-system] build-backend = \"hatchling.build\" requires = [\"hatchling\"] [project] name = \"ehrapy\" version = \"0.12.0\" description = \"Electronic Health Record Analysis with Python.\" readme = \"README.md\" requires-python = \">=3.11,<3.14\" license = {file = \"LICENSE\"} authors = [ {name = \"Lukas Heumos\"}, {name = \"Philipp Ehmele\"}, {name = \"Eljas Roellin\"}, {name = \"Lilly May\"}, {name = \"Tim Treis\"}, {name = \"Altana Namsaraeva\"}, {name = \"Vladimir Shitov\"}, {name = \"Luke Zappia\"}, {name = \"Xinyue Zhang\"}, ] maintainers = [ {name = \"Lukas Heumos\", email = \"lukas.heumos@posteo.net\"}, ] urls.Documentation = \"https://ehrapy.readthedocs.io\" urls.Source = \"https://github.com/theislab/ehrapy\" urls.Home-page = \"https://github.com/theislab/ehrapy\" classifiers = [ \"License :: OSI Approved :: Apache Software License\", \"Development Status :: 4 - Beta\", \"Environment :: Console\", \"Framework :: Jupyter\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", \"Natural Language :: English\", \"Operating System :: MacOS :: MacOS X\", \"Operating System :: POSIX :: Linux\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Programming Language :: Python :: 3.13\", \"Topic :: Scientific/Engineering :: Bio-Informatics\", \"Topic :: Scientific/Engineering :: Visualization\", ] dependencies = [ \"session-info2\", \"lamin_utils\", \"rich\", \"scanpy\", \"requests\", \"miceforest\", \"scikit-misc\", # required for seuratv3 highly variable features \"lifelines>=0.30.0\", \"missingno\", \"thefuzz[speedup]\", \"dowhy\", \"fhiry\", \"pyampute\", \"tableone\", \"imbalanced-learn\", \"fknni>=1.2.0\", \"python-dateutil\", \"filelock\", \"numpy>=2.0.0\", \"numba>=0.60.0\", \"igraph\", \"fast-array-utils\" ] [project.optional-dependencies] medcat = [ \"medcat\", ] dask = [ \"anndata[dask]\", \"dask-ml>=2025.1.0\", ] dev = [ \"pre-commit\", ] docs = [ \"docutils\", \"sphinx\", \"furo\", \"myst-nb\", \"myst-parser\", \"sphinxcontrib-bibtex\", \"sphinx-gallery\", \"sphinx-autodoc-typehints\", \"sphinx-copybutton\", \"sphinx-remove-toctrees\", \"sphinx-design\", \"sphinx-last-updated-by-git\", \"sphinx-automodapi\", \"sphinxext-opengraph\", \"pygments\", \"nbsphinx\", \"nbsphinx-link\", \"ipykernel\", \"ipython\", \"ehrapy[dask]\", ] test = [ \"ehrapy[dask]\", \"pytest\", \"pytest-cov\", \"pytest-mock\" ] [tool.hatch.version] source = \"vcs\" [tool.coverage.run] source_pkgs = [\"ehrapy\"] omit = [ \"**/test_*.py\", \"ehrapy/data/_datasets.py\", # Difficult to test ] [tool.pytest.ini_options] testpaths = \"tests\" xfail_strict = true addopts = [ \"--import-mode=importlib\", # allow using test files with same name ] markers = [ \"conda: marks a subset of tests to be ran on the Bioconda CI.\", \"extra: marks tests that require extra dependencies.\" ] filterwarnings = [ \"ignore::DeprecationWarning\", \"ignore::anndata.OldFormatWarning:\", \"ignore:X converted to numpy array with dtype object:UserWarning\", \"ignore:`flavor='seurat_v3'` expects raw count data, but non-integers were found:UserWarning\", \"ignore:All-NaN slice encountered:RuntimeWarning\", \"ignore:Observation names are not unique. To make them unique, call `.obs_names_make_unique`.:UserWarning\", \"ignore:Trying to modify attribute `.var` of view, initializing view as actual.:anndata.ImplicitModificationWarning\", \"ignore:Transforming to str index.:anndata.ImplicitModificationWarning:\" ] minversion = 6.0 norecursedirs = [ '.*', 'build', 'dist', '*.egg', 'data', '__pycache__'] [tool.ruff] line-length = 120 [tool.ruff.format] docstring-code-format = true [tool.ruff.lint] select = [ \"F\", # Errors detected by Pyflakes \"E\", # Error detected by Pycodestyle \"W\", # Warning detected by Pycodestyle \"I\", # isort \"D\", # pydocstyle \"B\", # flake8-bugbear \"TID\", # flake8-tidy-imports \"C4\", # flake8-comprehensions \"BLE\", # flake8-blind-except \"UP\", # pyupgrade \"RUF100\", # Report unused noqa directives \"TCH\", # Typing imports \"NPY\", # Numpy specific rules \"PTH\" # Use pathlib ] ignore = [ # line too long -> we accept long comment lines; black gets rid of long code lines \"E501\", # Do not assign a lambda expression, use a def -> lambda expression assignments are convenient \"E731\", # allow I, O, l as variable names -> I is the identity matrix \"E741\", # Missing docstring in public package \"D104\", # Missing docstring in public module \"D100\", # Missing docstring in __init__ \"D107\", # Errors from function calls in argument defaults. These are fine when the result is immutable. \"B008\", # __magic__ methods are are often self-explanatory, allow missing docstrings \"D105\", # first line should end with a period [Bug: doesn't work with single-line docstrings] \"D400\", # First line should be in imperative mood; try rephrasing \"D401\", ## Disable one in each pair of mutually incompatible rules # We don’t want a blank line before a class docstring \"D203\", # We want docstrings to start immediately after the opening triple quote \"D213\", # Imports unused \"F401\", # camcelcase imported as lowercase \"N813\", # module import not at top level of file \"E402\", ] [tool.ruff.lint.pydocstyle] convention = \"google\" [tool.ruff.lint.per-file-ignores] \"docs/*\" = [\"I\"] \"tests/*\" = [\"D\"] \"*/__init__.py\" = [\"F401\"] [tool.mypy] strict = false pretty = true show_column_numbers = true show_error_codes = true show_error_context = true ignore_missing_imports = true no_strict_optional = true [tool.cruft] skip = [ \"tests\", \"src/**/__init__.py\", \"src/**/basic.py\", \"docs/api.md\", \"docs/changelog.md\", \"docs/references.bib\", \"docs/references.md\", \"docs/notebooks/example.ipynb\" ]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/electrode",
            "repo_link": "https://github.com/robeme/lammps",
            "content": {
                "codemeta": "",
                "readme": "This is the LAMMPS software package.\n\nLAMMPS stands for Large-scale Atomic/Molecular Massively Parallel\nSimulator.\n\nCopyright (2003) Sandia Corporation.  Under the terms of Contract\nDE-AC04-94AL85000 with Sandia Corporation, the U.S. Government retains\ncertain rights in this software.  This software is distributed under\nthe GNU General Public License.\n\n----------------------------------------------------------------------\n\nLAMMPS is a classical molecular dynamics simulation code designed to\nrun efficiently on parallel computers.  It was developed at Sandia\nNational Laboratories, a US Department of Energy facility, with\nfunding from the DOE.  It is an open-source code, distributed freely\nunder the terms of the GNU Public License (GPL) version 2.\n\nThe code is maintained by the LAMMPS development team who can be emailed\nat developers@lammps.org.  The LAMMPS WWW Site at www.lammps.org has\nmore information about the code and its uses.\n\nThe LAMMPS distribution includes the following files and directories:\n\nREADME                     this file\nLICENSE                    the GNU General Public License (GPL)\nbench                      benchmark problems\ncmake                      CMake build files\ndoc                        documentation\nexamples                   simple test problems\nfortran                    Fortran wrapper for LAMMPS\nlib                        additional provided or external libraries\npotentials                 interatomic potential files\npython                     Python wrappers for LAMMPS\nsrc                        source files\ntools                      pre- and post-processing tools\n\nPoint your browser at any of these files to get started:\n\nhttps://docs.lammps.org/Manual.html         LAMMPS manual\nhttps://docs.lammps.org/Intro.html          hi-level introduction\nhttps://docs.lammps.org/Build.html          how to build LAMMPS\nhttps://docs.lammps.org/Run_head.html       how to run LAMMPS\nhttps://docs.lammps.org/Commands_all.html   Table of available commands\nhttps://docs.lammps.org/Library.html        LAMMPS library interfaces\nhttps://docs.lammps.org/Modify.html         how to modify and extend LAMMPS\nhttps://docs.lammps.org/Developer.html      LAMMPS developer info\n\nYou can also create these doc pages locally:\n\n% cd doc\n% make html                # creates HTML pages in doc/html\n% make pdf                 # creates Manual.pdf\n\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/elephant",
            "repo_link": "https://github.com/NeuralEnsemble/elephant",
            "content": {
                "codemeta": "{\"@context\":\"https://doi.org/10.5063/schema/codemeta-2.0\",\"@type\":\"SoftwareSourceCode\",\"license\":\"https://spdx.org/licenses/BSD-3-Clause\",\"codeRepository\":\"git+https://github.com/NeuralEnsemble/elephant.git\",\"contIntegration\":\"https://github.com/NeuralEnsemble/elephant/actions\",\"dateCreated\":\"2022-03-14\",\"datePublished\":\"2015-04-08\",\"dateModified\":\"2024-10-28\",\"downloadUrl\":\"https://files.pythonhosted.org/packages/83/2e/479328a34751bb6cd626beadca334f267763fe940706a0236979efd7301c/elephant-1.1.1.tar.gz\",\"issueTracker\":\"https://github.com/NeuralEnsemble/elephant/issues\",\"name\":\"Elephant\",\"version\":\"1.1.1\",\"identifier\":\"https://doi.org/10.5281/zenodo.1186602\",\"description\":\"Elephant (Electrophysiology Analysis Toolkit) is an open-source, community centered library for the analysis of electrophysiological data in the Python programming language. The focus of Elephant is on generic analysis functions for spike train data and time series recordings from electrodes, such as the local field potentials (LFP) or intracellular voltages.In addition to providing a common platform for analysis code from different laboratories, the Elephant project aims to provide a consistent and homogeneous analysis framework that is built on a modular foundation. \\nElephant is the direct successor to Neurotools and maintains ties to complementary projects such as OpenElectrophy and spykeviewer.\",\"applicationCategory\":\"library\",\"releaseNotes\":\"https://github.com/NeuralEnsemble/elephant/releases/tag/v1.1.1\",\"funding\":\"EU Grant 604102 (HBP), EU Grant 720270(HBP), EU Grant 785907(HBP), EU Grant 945539(HBP)\",\"developmentStatus\":\"active\",\"keywords\":[\"neuroscience\",\"neurophysiology\",\"electrophysiology\",\"statistics\",\"data-analysis\"],\"programmingLanguage\":[\"Python3\",\"\"],\"operatingSystem\":[\"Linux\",\"Windows\",\"MacOS\"],\"softwareRequirements\":[\"https://github.com/NeuralEnsemble/elephant/tree/v1.1.1/requirements\"],\"relatedLink\":[\"http://python-elephant.org\",\"http://elephant.readthedocs.org/\"],\"author\":[{\"@type\":\"Person\",\"@id\":\"https://orcid.org/0000-0003-1255-7300\",\"givenName\":\"Michael\",\"familyName\":\"Denker\",\"affiliation\":{\"@type\":\"Organization\",\"name\":\"Institute for Advanced Simulation (IAS-6), J\\u00fclich Research Centre, J\\u00fclich, Germany\"}},{\"@type\":\"Person\",\"@id\":\"https://orcid.org/0000-0001-7292-1982\",\"givenName\":\"Moritz\",\"familyName\":\"Kern\",\"affiliation\":{\"@type\":\"Organization\",\"name\":\"Institute for Advanced Simulation (IAS-6), J\\u00fclich Research Centre, J\\u00fclich, Germany\"}},{\"@type\":\"Person\",\"@id\":\"https://orcid.org/0009-0003-9352-9826\",\"givenName\":\"Felician\",\"familyName\":\"Richter\",\"affiliation\":{\"@type\":\"Organization\",\"name\":\"BioMEMS Lab, University of Applied Sciences Aschaffenburg, Germany\"}}]}\n",
                "readme": "# Elephant - Electrophysiology Analysis Toolkit [![Coverage Status](https://coveralls.io/repos/github/NeuralEnsemble/elephant/badge.svg?branch=master)](https://coveralls.io/github/NeuralEnsemble/elephant?branch=master) [![Documentation Status](https://readthedocs.org/projects/elephant/badge/?version=latest)](https://elephant.readthedocs.io/en/latest/?badge=latest) [![![PyPi]](https://img.shields.io/pypi/v/elephant)](https://pypi.org/project/elephant/) [![Statistics](https://img.shields.io/pypi/dm/elephant)](https://seladb.github.io/StarTrack-js/#/preload?r=neuralensemble,elephant) [![Gitter](https://badges.gitter.im/python-elephant/community.svg)](https://gitter.im/python-elephant/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge) [![DOI Latest Release](https://zenodo.org/badge/10311278.svg)](https://zenodo.org/badge/latestdoi/10311278) [![tests](https://github.com/NeuralEnsemble/elephant/actions/workflows/CI.yml/badge.svg)](https://github.com/NeuralEnsemble/elephant/actions/workflows/CI.yml) [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/6191/badge)](https://bestpractices.coreinfrastructure.org/projects/6191) [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu) [![Follow me on Bluesky](https://img.shields.io/badge/Bluesky-0285FF?logo=bluesky&logoColor=fff&label=Follow%20me%20on&color=0285FF)](https://bsky.app/profile/pyelephant.bsky.social) [![Fosstodon](https://img.shields.io/badge/@elephant-6364FF?logo=mastodon&logoColor=fff&style=flat)](https://fosstodon.org/@elephant) *Elephant* package analyses all sorts of neurophysiological data: spike trains, LFP, analog signals. The input-output data format is either [Neo](https://github.com/NeuralEnsemble/python-neo), Quantity or Numpy array. ### More Information * Documentation: https://elephant.readthedocs.io/en/latest/ * Mailing list: https://groups.google.com/group/neuralensemble #### Visualization of Elephant analysis objects Viziphant package (https://github.com/INM-6/viziphant) is developed by Elephant team for plotting and visualization of the output of Elephant functions in a few lines of code. #### License Modified BSD License, see [LICENSE.txt](LICENSE.txt) for details. #### Copyright :copyright: 2014-2024 by the [Elephant team](doc/authors.rst). #### Acknowledgments See [acknowledgments](doc/acknowledgments.rst). #### Citation See [citations](doc/citation.rst).\n",
                "dependencies": "# -*- coding: utf-8 -*- import os.path import platform import sys from setuptools import setup, Extension from setuptools.command.install import install from setuptools.command.develop import develop with open(os.path.join(os.path.dirname(__file__), \"elephant\", \"VERSION\")) as version_file: version = version_file.read().strip() with open(\"README.md\") as f: long_description = f.read() with open('requirements/requirements.txt') as fp: install_requires = fp.read().splitlines() extras_require = {} for extra in ['extras', 'docs', 'tests', 'tutorials', 'cuda', 'opencl']: with open('requirements/requirements-{0}.txt'.format(extra)) as fp: extras_require[extra] = fp.read() if platform.system() == \"Windows\": fim_module = Extension( name='elephant.spade_src.fim', sources=['elephant/spade_src/src/fim.cpp'], include_dirs=['elephant/spade_src/include'], language='c++', libraries=[], extra_compile_args=[ '-DMODULE_NAME=fim', '-DUSE_OPENMP', '-DWITH_SIG_TERM', '-Dfim_EXPORTS', '-fopenmp', '/std:c++17'], optional=True ) elif platform.system() == \"Darwin\": fim_module = Extension( name='elephant.spade_src.fim', sources=['elephant/spade_src/src/fim.cpp'], include_dirs=['elephant/spade_src/include'], language='c++', libraries=['pthread', 'omp'], extra_compile_args=[ '-DMODULE_NAME=fim', '-DUSE_OPENMP', '-DWITH_SIG_TERM', '-Dfim_EXPORTS', '-O3', '-pedantic', '-Wextra', '-Weffc++', '-Wunused-result', '-Werror', '-Werror=return-type', '-Xpreprocessor', '-fopenmp', '-std=gnu++17'], optional=True ) elif platform.system() == \"Linux\": fim_module = Extension( name='elephant.spade_src.fim', sources=['elephant/spade_src/src/fim.cpp'], include_dirs=['elephant/spade_src/include'], language='c++', libraries=['pthread', 'gomp'], extra_compile_args=[ '-DMODULE_NAME=fim', '-DUSE_OPENMP', '-DWITH_SIG_TERM', '-Dfim_EXPORTS', '-O3', '-pedantic', '-Wextra', '-Weffc++', '-Wunused-result', '-Werror', '-fopenmp', '-std=gnu++17'], optional=True ) setup_kwargs = { \"name\": \"elephant\", \"version\": version, \"packages\": ['elephant', 'elephant.test'], \"include_package_data\": True, \"install_requires\": install_requires, \"extras_require\": extras_require, \"author\": \"Elephant authors and contributors\", \"author_email\": \"contact@python-elephant.org\", \"description\": \"Elephant is a package for analysis of electrophysiology data in Python\", # noqa \"long_description\": long_description, \"long_description_content_type\": \"text/markdown\", \"license\": \"BSD\", \"url\": 'http://python-elephant.org', \"project_urls\": { \"Bug Tracker\": \"https://github.com/NeuralEnsemble/elephant/issues\", \"Documentation\": \"https://elephant.readthedocs.io/en/latest/\", \"Source Code\": \"https://github.com/NeuralEnsemble/elephant\", }, \"python_requires\": \">=3.8\", \"classifiers\": [ 'Development Status :: 5 - Production/Stable', 'Intended Audience :: Science/Research', 'License :: OSI Approved :: BSD License', 'Natural Language :: English', 'Operating System :: OS Independent', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.8', 'Programming Language :: Python :: 3.9', 'Programming Language :: Python :: 3.10', 'Programming Language :: Python :: 3.11', 'Programming Language :: Python :: 3 :: Only', 'Topic :: Scientific/Engineering'] } # no compile options and corresponding extensions options = {\"--no-compile\": None, \"--no-compile-spade\": fim_module} # check if any option was specified if not any([True for key in options.keys() if key in sys.argv]): if platform.system() in [\"Windows\", \"Linux\"]: setup_kwargs[\"ext_modules\"] = [fim_module] else: # ...any option was specified # select extensions accordingly extensions = [module for flag, module in options.items() if flag not in sys.argv] if None in extensions: # None indicates \"--no-compile\" not in sys.argv extensions.remove(None) setup_kwargs[\"ext_modules\"] = extensions class CommandMixin(object): \"\"\" This class acts as a superclass to integrate new commands in setuptools. \"\"\" user_options = [ ('no-compile', None, 'do not compile any C++ extension'), ('no-compile-spade', None, 'do not compile spade related C++ extension') # noqa ] def initialize_options(self): \"\"\" The method is responsible for setting default values for all the options that the command supports. Option dependencies should not be set here. \"\"\" super().initialize_options() # Initialize options self.no_compile_spade = None self.no_compile = None def finalize_options(self): \"\"\" Overriding a required abstract method. This method is responsible for setting and checking the final values and option dependencies for all the options just before the method run is executed. In practice, this is where the values are assigned and verified. \"\"\" super().finalize_options() def run(self): \"\"\" Sets global which can later be used in setup.py to remove c-extensions from setup call. \"\"\" # Use options global no_compile_spade global no_compile no_compile_spade = self.no_compile_spade no_compile = self.no_compile super().run() class InstallCommand(CommandMixin, install): \"\"\" This class extends setuptools.command.install class, adding user options. \"\"\" user_options = getattr( install, 'user_options', []) + CommandMixin.user_options class DevelopCommand(CommandMixin, develop): \"\"\" This class extends setuptools.command.develop class, adding user options. \"\"\" user_options = getattr( develop, 'user_options', []) + CommandMixin.user_options # add classes to setup-kwargs to add the user options setup_kwargs['cmdclass'] = {'install': InstallCommand, 'develop': DevelopCommand} setup(**setup_kwargs)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/emipy",
            "repo_link": "https://jugit.fz-juelich.de/network-science-group/emipy",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/enpt",
            "repo_link": "https://git.gfz-potsdam.de/EnMAP/GFZ_Tools_EnMAP_BOX/EnPT",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/enrichedheatmap",
            "repo_link": "https://github.com/jokergoo/EnrichedHeatmap",
            "content": {
                "codemeta": "",
                "readme": "# Make Enriched Heatmaps [![R-CMD-check](https://github.com/jokergoo/EnrichedHeatmap/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/EnrichedHeatmap/actions) [![codecov](https://img.shields.io/codecov/c/github/jokergoo/EnrichedHeatmap.svg)](https://codecov.io/github/jokergoo/EnrichedHeatmap) [![bioc](https://bioconductor.org/shields/downloads/devel/EnrichedHeatmap.svg)](https://bioconductor.org/packages/stats/bioc/EnrichedHeatmap/) [![bioc](http://www.bioconductor.org/shields/years-in-bioc/EnrichedHeatmap.svg)](http://bioconductor.org/packages/devel/bioc/html/EnrichedHeatmap.html) Enriched heatmap is a special type of heatmap which visualizes the enrichment of genomic signals on specific target regions. It is broadly used to visualize e.g. how histone marks are enriched to specific sites. There are several tools that can make such heatmap (e.g. [ngs.plot](https://github.com/shenlab-sinai/ngsplot) or [deepTools](https://github.com/fidelram/deepTools)). Here we implement Enriched heatmap by [ComplexHeatmap](https://github.com/jokergoo/ComplexHeatmap) package. Since this type of heatmap is just a normal heatmap but with some fixed settings, with the functionality of ComplexHeatmap, it would be much easier to customize the heatmap as well as concatenating a list of heatmaps to show correspondance between differnet data sources. ### Citation Zuguang Gu, et al., EnrichedHeatmap: an R/Bioconductor package for comprehensive visualization of genomic signal associations, 2018. BMC Genomics. [link](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-4625-x) ### Install **EnrichedHeatmap** is available on [Bioconductor](http://bioconductor.org/packages/devel/bioc/html/EnrichedHeatmap.html), you can install it by: ```{r} if (!requireNamespace(\"BiocManager\", quietly=TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"EnrichedHeatmap\") ``` If you want the latest version, install it directly from GitHub: ```{r} library(devtools) install_github(\"jokergoo/ComplexHeatmap\") install_github(\"jokergoo/EnrichedHeatmap\") ``` ### Example Like other tools, the task involves two steps: 1. Normalize the accosiations between genomic signals and target regions to a matrix. 2. Draw heatmaps. ```{r} mat1 = normalizeToMatrix(H3K4me3, tss, value_column = \"coverage\", extend = 5000, mean_mode = \"w0\", w = 50) mat2 = normalizeToMatrix(meth, tss, value_column = \"meth\", mean_mode = \"absolute\", extend = 5000, w = 50, background = NA, smooth = TRUE) ``` ```{r} partition = kmeans(mat1, centers = 3)$cluster lgd = Legend(at = c(\"cluster1\", \"cluster2\", \"cluster3\"), title = \"Clusters\", type = \"lines\", legend_gp = gpar(col = 2:4)) ht_list = Heatmap(partition, col = structure(2:4, names = as.character(1:3)), name = \"partition\", show_row_names = FALSE, width = unit(3, \"mm\")) + EnrichedHeatmap(mat1, col = c(\"white\", \"red\"), name = \"H3K4me3\", row_split = partition, top_annotation = HeatmapAnnotation(lines = anno_enriched(gp = gpar(col = 2:4))), column_title = \"H3K4me3\") + EnrichedHeatmap(mat2, name = \"methylation\", top_annotation = HeatmapAnnotation(lines = anno_enriched(gp = gpar(col = 2:4))), column_title = \"Methylation\") + Heatmap(log2(rpkm+1), col = c(\"white\", \"orange\"), name = \"log2(rpkm+1)\", show_row_names = FALSE, width = unit(5, \"mm\")) draw(ht_list, main_heatmap = \"H3K4me3\", gap = unit(c(2, 10, 2), \"mm\")) ``` ![image](https://cloud.githubusercontent.com/assets/449218/14768684/41a6d534-0a49-11e6-800a-36ce15ad83ca.png) Also when signals are discreate values. E.g. chromatin states: ![test](https://user-images.githubusercontent.com/449218/36900761-e3d2ff86-1e24-11e8-865c-2cedb2674707.png) Actually you can generate rather complex heatmaps: <img width=\"1043\" alt=\"screen shot 2017-10-13 at 10 42 42\" src=\"https://user-images.githubusercontent.com/449218/31608873-50c497d6-b272-11e7-8d81-cd88156d18aa.png\"> ### License MIT @ Zuguang Gu\n",
                "dependencies": "Package: EnrichedHeatmap Type: Package Title: Making Enriched Heatmaps Version: 1.33.1 Date: 2024-02-27 Authors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"), comment = c('ORCID'=\"0000-0002-7395-8709\")) Depends: R (>= 4.0.0), methods, grid, ComplexHeatmap (>= 2.11.0), GenomicRanges Imports: matrixStats, stats, GetoptLong, Rcpp, utils, locfit, circlize (>= 0.4.5), IRanges Suggests: testthat (>= 0.3), knitr, markdown, rmarkdown, genefilter, RColorBrewer VignetteBuilder: knitr Description: Enriched heatmap is a special type of heatmap which visualizes the enrichment of genomic signals on specific target regions. Here we implement enriched heatmap by ComplexHeatmap package. Since this type of heatmap is just a normal heatmap but with some special settings, with the functionality of ComplexHeatmap, it would be much easier to customize the heatmap as well as concatenating to a list of heatmaps to show correspondance between different data sources. biocViews: Software, Visualization, Sequencing, GenomeAnnotation, Coverage URL: https://github.com/jokergoo/EnrichedHeatmap License: MIT + file LICENSE LinkingTo: Rcpp\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/esm-tools",
            "repo_link": "https://github.com/esm-tools/esm_tools",
            "content": {
                "codemeta": "",
                "readme": "========= ESM Tools ========= Documentation ------------- .. image:: https://readthedocs.org/projects/esm-tools/badge/?version=latest For our complete documentation, please check https://esm-tools.readthedocs.io/en/latest/index.html. How to cite this software ------------------------- To cite ESM-Tools, please use the following DOI: https://zenodo.org/doi/10.5281/zenodo.3737927. This DOI represents all versions of the software, and will always pointing to the latest version available on https://zenodo.org. Before you continue ------------------- You will need python 3 (possibly version 3.6 or newer), a version of git that is not ancient (everything newer than 2.10 should be good), and up-to-date pip (``pip install -U pip``) to install the `esm_tools`. That means that on the supported machines, you could for example use the following settings: albedo:: $ module load git $ module load python levante.dkrz.de:: $ module load git $ module load python3 glogin.hlrn.de / blogin.hlrn.de:: $ module load git $ module load anaconda3 juwels.fz-juelich.de:: $ module load Stages/2022 $ module load git $ module load Python/3.9.6 aleph:: $ module load git $ module load python Note that some machines might raise an error ``conflict netcdf_c`` when loading ``anaconda3``. In that case you will need to swap ``netcdf_c`` with ``anaconda3``:: $ module unload netcdf_c $ module load anaconda3 Installing ---------- 1. First, make sure you add the following lines to one of your login or profile files, i.e. ``~/.bash_profile``, ``~/.bashrc``, ``~/.profile``, etc.:: $ export PATH=$PATH:~/.local/bin $ export LC_ALL=en_US.UTF-8 $ export LANG=en_US.UTF-8 2. Inside the same login or profile file, add also the ``module`` commands necessary for the HPC system you are using (find the lines in the section above). 3. You can choose to source now your login or profile file, so that the ``module`` and ``export`` commands are run (e.g. ``$ source ~/.bash_profile``). 4. To use the new version of the ESM-Tools, now rewritten in Python, clone this repository:: $ git clone https://github.com/esm-tools/esm_tools.git 5. Then, run the ``install.sh``:: $ ./install.sh You should now have the command line tools ``esm_master`` and ``esm_runscripts``, which replace the old version.\n",
                "dependencies": "#!/usr/bin/env bash # boolean variable to exit the program on error shall_exit=false # prints the error message as the first argument and exits the program with non-zero status function quit_install () { echo \"\" echo \"$(tput setaf 1)ERROR: ${1} $(tput sgr 0)\" echo \"please set the LANG and LC_ALL variables in your shell startup script (eg. .bashrc, .bash_profile) to the following values: \" echo \" export LC_ALL=en_US.UTF-8\" echo \" export LANG=en_US.UTF-8\" echo \"and re-execute them so that the changes take place. Exiting the installation process\" exit 1 } # check if LANG environment is set to the correct value if [[ -z ${LANG+x} ]]; then err_msg=\"LANG environment variable is not set\" shall_exit=true fi # check if LC_ALL variable is set to the correct value if [[ -z ${LC_ALL+x} ]]; then err_msg=\"LC_ALL variable is not set\" shall_exit=true fi # we have an error, terminate the script if [[ ${shall_exit} == true ]]; then quit_install \"${err_msg}\" fi git_error_message=\"You need git version >= 2.13 to install the esm_tools (see README.rst).\" if hash git 2>/dev/null; then \tgit_version=`git --version | rev | cut -d' ' -f 1 | rev` \tmajor_git_version=`git --version | rev | cut -d' ' -f 1 | rev | cut -d'.' -f 1` \tminor_git_version=`git --version | rev | cut -d' ' -f 1 | rev | cut -d'.' -f 2` \tif test ${major_git_version} -lt \"2\"; then \t\techo $git_error_message \t\techo \"git version found: ${git_version}\" \telse \t\tif test ${minor_git_version} -lt \"10\"; then \t\t\techo $git_error_message \t\t\techo \"git version found: ${git_version}\" \t\tfi \tfi else \techo $git_error_message \techo \"No installed git version found.\" fi # See here: https://tinyurl.com/5b57knvx if [ ! -z ${VIRTUAL_ENV+x} ]; then echo \"Detected virtual environment $VIRTUAL_ENV\" pip install -e . elif [ ! -z ${TYKKY_PREFIX+x} ]; then echo \"=======================\" echo \"Using TYKKY environment\" echo \"=======================\" echo \"WARNING: The use of a tykky environment for using ESM-Tools is currently not supported. Use at your own risk\" ESM_TOOLS_SRC_PATH=\"$(dirname \"$(realpath \"$0\")\")\" echo $ESM_TOOLS_SRC_PATH echo \"pip install -e ${ESM_TOOLS_SRC_PATH}\" > esm_tools_tikky_install.sh conda-containerize update ${TYKKY_PREFIX} --post-install esm_tools_tikky_install.sh rm esm_tools_tikky_install.sh elif [ ! -z ${CONDA_PREFIX+x} ]; then echo \"=======================\" echo \"Using CONDA environment\" echo \"=======================\" ${CONDA_PREFIX}/bin/pip install -e . else echo \"Standard install to user directory (likely ${HOME}/.local)\" pip install --user -e . fi\n#!/usr/bin/env python \"\"\"The setup script.\"\"\" from os import getenv from setuptools import find_packages, setup with open(\"README.rst\") as readme_file: readme = readme_file.read() with open(\"HISTORY.rst\") as history_file: history = history_file.read() requirements = [ \"Click>=8.0.4\", # Maximum version for Python 3.6 support \"PyGithub==1.55\", \"colorama==0.4.5\", \"coloredlogs==15.0.1\", # NOTE(PG): Should be removed during cleanup for loguru instead \"emoji==1.7.0\", \"f90nml==1.4.2\", \"gfw-creator==0.2.2\", \"gitpython==3.1.41\", # Maximum version for Python 3.6 support \"jinja2==3.1.4\", \"loguru==0.6.0\", \"numpy>=1.19.5\", # Maximum version for Python 3.6 support \"packaging==21.3\", \"pandas>=1.1.5\", # Correct compatiability with xarray for Python 3.6 \"psutil==5.9.1\", \"pytest==7.1.2\", \"pyyaml==6.0.1\", \"questionary==1.10.0\", \"ruamel.yaml==0.17.32\", \"semver==2.13.0\", \"sqlalchemy>=1.4.39\", \"tabulate==0.8.10\", \"tqdm==4.66.3\", \"typing_extensions>=4.1.1\", # Maximum number for Python 3.6 support \"xdgenvpy==2.3.5\", \"pydantic>=1.10.13\", \"h5netcdf>=0.8.1\", ] setup_requirements = [] test_requirements = [ \"pyfakefs==4.6.0\", ] setup( author=\"The ESM Tools Team\", author_email=[ \"dirk.barbi@awi.de\", \"paul.gierz@awi.de\", \"miguel.andres-martinez@awi.de\", \"deniz.ural@awi.de\", \"jan.streffing@awi.de\", \"sebastian.wahl@geomar.de\", \"kai.himstedt@dkrz.de\", ], python_requires=\">=3.6, <3.12\", classifiers=[ \"Development Status :: 3 - Alpha\", \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: GNU General Public License v2 (GPLv2)\", \"Natural Language :: English\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.6\", \"Programming Language :: Python :: 3.7\", \"Programming Language :: Python :: 3.8\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", ], description=\"ESM Tools external infrastructure for Earth System Modelling\", entry_points={ \"console_scripts\": [ \"esm_archive=esm_archiving.cli:main\", \"esm_cleanup=esm_cleanup.cli:main\", \"esm_database=esm_database.cli:main\", \"esm_master=esm_master.cli:main\", \"esm_plugins=esm_plugin_manager.cli:main\", \"esm_runscripts=esm_runscripts.cli:main\", \"esm_tests=esm_tests.cli:main\", \"esm_tools=esm_tools.cli:main\", \"esm_utilities=esm_utilities.cli:main\", ], }, install_requires=requirements, license=\"GNU General Public License v2\", long_description=readme + \"\\n\\n\" + history, include_package_data=True, keywords=\"esm_tools\", name=\"esm-tools\", packages=find_packages(\"src\") + [ \"esm_tools\", \"esm_tools.configs\", \"esm_tools.namelists\", \"esm_tools.runscripts\", \"esm_tools.couplings\", ], package_dir={ \"\": \"src\", \"esm_tools.configs\": \"configs\", \"esm_tools.namelists\": \"namelists\", \"esm_tools.runscripts\": \"runscripts\", \"esm_tools.couplings\": \"couplings\", }, package_data={ \"esm_tools.configs\": [\"../configs/*\"], \"esm_tools.namelists\": [\"../namelists/*\"], \"esm_tools.runscripts\": [\"../runscripts/*\"], \"esm_tools.couplings\": [\"../couplings/*\"], }, setup_requires=setup_requirements, test_suite=\"tests\", tests_require=test_requirements, url=\"https://github.com/esm-tools/esm_tools\", version=\"6.53.2\", zip_safe=False, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/esmvalcore",
            "repo_link": "https://github.com/ESMValGroup/ESMValCore",
            "content": {
                "codemeta": "",
                "readme": "# ESMValCore package [![Documentation Status](https://readthedocs.org/projects/esmvalcore/badge/?version=latest)](https://esmvaltool.readthedocs.io/en/latest/?badge=latest) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3387139.svg)](https://doi.org/10.5281/zenodo.3387139) [![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/#ESMValGroup_Lobby:gitter.im) [![CircleCI](https://circleci.com/gh/ESMValGroup/ESMValCore/tree/main.svg?style=svg)](https://circleci.com/gh/ESMValGroup/ESMValCore/tree/main) [![codecov](https://codecov.io/gh/ESMValGroup/ESMValCore/branch/main/graph/badge.svg?token=wQnDzguwq6)](https://codecov.io/gh/ESMValGroup/ESMValCore) [![Codacy Badge](https://app.codacy.com/project/badge/Grade/5d496dea9ef64ec68e448a6df5a65783)](https://app.codacy.com/gh/ESMValGroup/ESMValCore/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade) [![Anaconda-Server Badge](https://img.shields.io/conda/vn/conda-forge/ESMValCore?color=blue&label=conda-forge&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/esmvalcore) [![Github Actions Test](https://github.com/ESMValGroup/ESMValCore/actions/workflows/run-tests.yml/badge.svg)](https://github.com/ESMValGroup/ESMValCore/actions/workflows/run-tests.yml) [![pre-commit.ci status](https://results.pre-commit.ci/badge/github/ESMValGroup/ESMValCore/main.svg)](https://results.pre-commit.ci/latest/github/ESMValGroup/ESMValCore/main) ![esmvaltoollogo](https://raw.githubusercontent.com/ESMValGroup/ESMValCore/main/doc/figures/ESMValTool-logo-2-glow.png) ESMValCore: core functionalities for the ESMValTool, a community diagnostic and performance metrics tool for routine evaluation of Earth System Models in the Climate Model Intercomparison Project (CMIP). # Getting started Please have a look at the [documentation](https://docs.esmvaltool.org/projects/esmvalcore/en/latest/quickstart/install.html) to get started. ## Using the ESMValCore package to run recipes The ESMValCore package provides the `esmvaltool` command, which can be used to run [recipes](https://docs.esmvaltool.org/projects/esmvalcore/en/latest/recipe/overview.html) for working with CMIP-like data. A large collection of ready to use [recipes and diagnostics](https://docs.esmvaltool.org/en/latest/recipes/index.html) is provided by the [ESMValTool](https://github.com/ESMValGroup/ESMValTool) package. ## Using ESMValCore as a Python library The ESMValCore package provides various functions for: - Finding data in a directory structure typically used for CMIP data. - Reading CMIP/CMOR tables and using those to check model and observational data. - ESMValTool preprocessor functions based on [iris](https://scitools-iris.readthedocs.io) for e.g. regridding, vertical interpolation, statistics, correcting (meta)data errors, extracting a time range, etcetera. read all about it in the [API documentation](https://docs.esmvaltool.org/projects/esmvalcore/en/latest/api/esmvalcore.html). ## Getting help The easiest way to get help if you cannot find the answer in the documentation on [readthedocs](https://docs.esmvaltool.org), is to open an [issue on GitHub](https://github.com/ESMValGroup/ESMValCore/issues). ## Contributing Contributions are very welcome, please read our [contribution guidelines](https://docs.esmvaltool.org/projects/ESMValCore/en/latest/contributing.html) to get started.\n",
                "dependencies": "[build-system] requires = [ \"setuptools >= 40.6.0\", \"setuptools_scm>=6.2\", ] build-backend = \"setuptools.build_meta\" [project] authors = [ {name = \"ESMValTool Development Team\", email = \"esmvaltool-dev@listserv.dfn.de\"} ] classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Environment :: Console\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: Apache Software License\", \"Natural Language :: English\", \"Operating System :: POSIX :: Linux\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Programming Language :: Python :: 3.13\", \"Topic :: Scientific/Engineering\", \"Topic :: Scientific/Engineering :: Atmospheric Science\", \"Topic :: Scientific/Engineering :: GIS\", \"Topic :: Scientific/Engineering :: Hydrology\", \"Topic :: Scientific/Engineering :: Physics\", ] dynamic = [ \"readme\", \"version\", ] dependencies = [ \"cartopy\", \"cf-units\", \"dask[array,distributed]>=2025,!=2025.4.0\", # Core/issues/2503 and 2716 \"dask-jobqueue\", \"esgf-pyclient>=0.3.1\", \"esmf-regrid>=0.11.0\", \"esmpy\", # not on PyPI \"filelock\", \"fiona\", \"fire\", \"geopy\", \"humanfriendly\", \"iris-grib>=0.20.0\", # github.com/ESMValGroup/ESMValCore/issues/2535 \"isodate>=0.7.0\", \"jinja2\", \"nc-time-axis\", # needed by iris.plot \"nested-lookup\", \"netCDF4\", \"numpy!=1.24.3\", \"packaging\", \"pandas\", \"pillow\", \"prov\", \"psutil\", \"py-cordex\", \"pybtex\", \"pyyaml\", \"requests\", \"rich\", \"scipy>=1.6\", \"scitools-iris>=3.11\", # 3.11 first to support Numpy 2 and Python 3.13 \"shapely>=2.0.0\", \"stratify>=0.3\", \"yamale\", ] description = \"A community tool for pre-processing data from Earth system models in CMIP and running analysis scripts\" license = {text = \"Apache License, Version 2.0\"} name = \"ESMValCore\" requires-python = \">=3.10\" [project.optional-dependencies] test = [ \"pytest>6.0.0\", \"pytest-cov>=2.10.1\", \"pytest-env\", \"pytest-html!=2.1.0\", \"pytest-metadata>=1.5.1\", \"pytest-mock\", \"pytest-xdist\", \"ESMValTool_sample_data==0.0.3\", ] doc = [ \"autodocsumm>=0.2.2\", \"ipython<9.0\", # github.com/ESMValGroup/ESMValCore/issues/2680 \"nbsphinx>=0.9.7\", # github.com/ESMValGroup/ESMValCore/issues/2669 \"sphinx>=6.1.3\", \"pydata_sphinx_theme\", ] develop = [ \"esmvalcore[test,doc]\", \"pre-commit\", \"pylint\", \"pydocstyle\", \"vprof\", ] [project.scripts] esmvaltool = \"esmvalcore._main:run\" [project.urls] Code = \"https://github.com/ESMValGroup/ESMValCore\" Community = \"https://github.com/ESMValGroup/Community\" Documentation = \"https://docs.esmvaltool.org\" Homepage = \"https://esmvaltool.org\" Issues = \"https://github.com/ESMValGroup/ESMValCore/issues\" [tool.setuptools] include-package-data = true license-files = [\"LICENSE\"] packages = [\"esmvalcore\"] zip-safe = false [tool.setuptools.dynamic] readme = {file = \"README.md\", content-type = \"text/markdown\"} [tool.setuptools_scm] version_scheme = \"release-branch-semver\" # Configure tests [tool.pytest.ini_options] addopts = [ \"-ra\", \"--strict-config\", \"--strict-markers\", \"--doctest-modules\", \"--ignore=esmvalcore/cmor/tables/\", \"--cov-report=xml:test-reports/coverage.xml\", \"--cov-report=html:test-reports/coverage_html\", \"--html=test-reports/report.html\", ] log_cli_level = \"INFO\" env = {MPLBACKEND = \"Agg\"} log_level = \"WARNING\" minversion = \"6\" markers = [ \"installation: Test requires installation of dependencies\", \"use_sample_data: Run functional tests using real data\", ] testpaths = [\"tests\"] xfail_strict = true [tool.coverage.run] parallel = true source = [\"esmvalcore\"] [tool.coverage.report] exclude_lines = [ \"pragma: no cover\", \"if __name__ == .__main__.:\", \"if TYPE_CHECKING:\", ] # Configure type checks [tool.mypy] # See https://mypy.readthedocs.io/en/stable/config_file.html ignore_missing_imports = true enable_error_code = [ \"truthy-bool\", ] # Configure linters [tool.codespell] skip = \"*.ipynb,esmvalcore/config/extra_facets/ipslcm-mappings.yml,tests/sample_data/iris-sample-data/LICENSE\" ignore-words-list = \"emac,hist,oce,vas\" [tool.ruff] line-length = 79 [tool.ruff.lint] select = [ \"B\", \"D\", # pydocstyle \"E\", # pycodestyle \"F\", # pyflakes \"I\", # isort \"ISC001\", # pycodestyle \"W\", # pycodestyle ] ignore = [ \"E501\", # Disable line-too-long as this is taken care of by the formatter. \"D105\", # Disable Missing docstring in magic method as these are well defined. ] [tool.ruff.lint.per-file-ignores] \"tests/**.py\" = [ \"B011\", # `assert False` is valid test code. # Docstrings in tests are only needed if the code is not self-explanatory. \"D100\", # Missing docstring in public module \"D101\", # Missing docstring in public class \"D102\", # Missing docstring in public method \"D103\", # Missing docstring in public function \"D104\", # Missing docstring in public package ] [tool.ruff.lint.isort] known-first-party = [\"esmvalcore\"] [tool.ruff.lint.pydocstyle] convention = \"numpy\" # Configure linters that are run by Prospector # TODO: remove once we have enabled all ruff rules for the tools provided by # Prospector, see https://github.com/ESMValGroup/ESMValCore/issues/2528. [tool.pylint.main] jobs = 1 # Running more than one job in parallel crashes prospector. ignore-paths = [ \"doc/conf.py\", # Sphinx configuration file ] [tool.pylint.basic] good-names = [ \"_\", # Used by convention for unused variables \"i\", \"j\", \"k\", # Used by convention for indices \"logger\", # Our preferred name for the logger ] [tool.pylint.format] max-line-length = 79 [tool.pylint.\"messages control\"] disable = [ \"import-error\", # Needed because Codacy does not install dependencies \"file-ignored\", # Disable messages about disabling checks \"line-too-long\", # Disable line-too-long as this is taken care of by the formatter. \"locally-disabled\", # Disable messages about disabling checks ] [tool.pydocstyle] convention = \"numpy\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ethosfine-framework-for-integrated-energy-system-assessment",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/FINE",
            "content": {
                "codemeta": "",
                "readme": "<!-- markdownlint-disable line-length no-inline-html --> # ETHOS.FINE - Framework for Integrated Energy System Assessment [![Build Status](https://travis-ci.com/FZJ-IEK3-VSA/FINE.svg?branch=master)](https://travis-ci.com/FZJ-IEK3-VSA/FINE) [![Version](https://img.shields.io/pypi/v/FINE.svg)](https://pypi.python.org/pypi/FINE) [![Conda Version](https://img.shields.io/conda/vn/conda-forge/fine.svg)](https://anaconda.org/conda-forge/fine) [![Documentation Status](https://readthedocs.org/projects/vsa-fine/badge/?version=latest)](https://vsa-fine.readthedocs.io/en/latest/) [![PyPI - License](https://img.shields.io/pypi/l/FINE)](https://github.com/FZJ-IEK3-VSA/FINE/blob/master/LICENSE.txt) [![codecov](https://codecov.io/gh/FZJ-IEK3-VSA/FINE/branch/master/graph/badge.svg)](https://codecov.io/gh/FZJ-IEK3-VSA/FINE) [![DOI](https://joss.theoj.org/papers/10.21105/joss.06274/status.svg)](https://doi.org/10.21105/joss.06274) <a href=\"https://www.fz-juelich.de/en/ice/ice-2\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/JSA-Header.svg?raw=True\" alt=\"Forschungszentrum Juelich Logo\" width=\"300px\"></a> The ETHOS.FINE python package provides a framework for modeling, optimizing and assessing energy systems. With the provided framework, systems with multiple regions, commodities, time steps and investment periods can be modeled. Target of the optimization is the minimization of the systems net present value (NPV) while considering technical and environmental constraints. If only one investment period is considered, the net present value is equal to the total annual costs (TAC). Besides using the full temporal resolution, an interconnected typical period storage formulation can be applied, that reduces the complexity and computational time of the model. This Readme provides information on the installation of the package. For further information have a look at the [documentation](https://vsa-fine.readthedocs.io/en/latest/). ETHOS.FINE is used for the modelling of a diverse group of optimization problems within the [Energy Transformation PatHway Optimization Suite (ETHOS) at ICE-2](https://www.fz-juelich.de/de/ice/ice-2/leistungen/model-services). If you want to use ETHOS.FINE in a published work, please [**kindly cite following publication**](https://joss.theoj.org/papers/10.21105/joss.06274). The python package which provides the time series aggregation module and its corresponding literature can be found [here](https://github.com/FZJ-IEK3-VSA/tsam). ## Installation There are several options for the installation of ETHOS.FINE. You can install it via PyPI or from conda-forge. For detailed information, have a look at the [installation documentation](https://vsa-fine.readthedocs.io/en/latest/installationDoc.html). NOTE: If you want to work on the source code of FINE, see [Editable install from conda-forge](#editable-install-from-conda-forge). If you would like to run ETHOS.FINE for your analysis we recommend to install it directly from conda-forge into a new Python environment with ```bash mamba create --name fine --channel conda-forge fine ``` **Note on Mamba vs.Conda:** `mamba` commands can be substitued with `conda`. We highly recommend using [(Micro-)Mamba](https://mamba.readthedocs.io/en/latest/) instead of Conda. The recommended way to use Mamba on your system is to install the [Miniforge distribution](https://github.com/conda-forge/miniforge#miniforge3). They offer installers for Windows, Linux and OS X. In principle, Conda and Mamba are interchangeable. The commands and concepts are the same. The distributions differ in the methodology for determining dependencies when installing Python packages. Mamba relies on a more modern methodology, which (with the same result) leads to very significant time savings during the installation of ETHOS.FINE. Switching to Mamba usually does not lead to any problems, as it is virtually identical to Conda in terms of operation. **Note on the solver:** The functionality of ETHOS.FINE depends on the following C libraries that need to be installed on your system. If you do not know how to install those, consider installing from conda-forge. The mamba/conda installation comes with [GLPK](https://www.gnu.org/software/glpk/) [(installation for Windows)](https://sourceforge.net/projects/winglpk/files/latest/download) as Mixed Integer Linear Programming (MILP) solver. If you want to solve large problems it is highly recommended to install [GUROBI](http://www.gurobi.com/). See [\"Installation of an optimization solver\"](https://vsa-fine.readthedocs.io/en/latest/installationDoc.html#installation-of-an-optimization-solver) in the documentation for more information. ## Examples A number of [examples](https://github.com/FZJ-IEK3-VSA/FINE/tree/master/examples) shows the capabilities of ETHOS.FINE. - [00_Tutorial](https://github.com/FZJ-IEK3-VSA/FINE/tree/master/examples/00_Tutorial) - In this application, an energy supply system, consisting of two regions, is modeled and optimized. Recommended as starting point to get to know to ETHOS.FINE. - [01_1node_Energy_System_Workflow](https://github.com/FZJ-IEK3-VSA/FINE/tree/master/examples/01_1node_Energy_System_Workflow) - In this application, a single region energy system is modeled and optimized. The system includes only a few technologies. - [02_EnergyLand](https://github.com/FZJ-IEK3-VSA/FINE/tree/master/examples/02_EnergyLand) - In this application, a single region energy system is modeled and optimized. Compared to the previous examples, this example includes a lot more technologies considered in the system. - [03_Multi-regional_Energy_System_Workflow](https://github.com/FZJ-IEK3-VSA/FINE/tree/master/examples/03_Multi-regional_Energy_System_Workflow) - In this application, an energy supply system, consisting of eight regions, is modeled and optimized. The example shows how to model multi-regional energy systems. The example also includes a notebook to get to know the optional performance summary. The summary shows how the optimization performed. - [04_Model_Run_from_Excel](https://github.com/FZJ-IEK3-VSA/FINE/tree/master/examples/04_Model_Run_from_Excel) - ETHOS.FINE can also be run by excel. This example shows how to read and run a model using excel files. - [05_District_Optimization](https://github.com/FZJ-IEK3-VSA/FINE/tree/master/examples/05_District_Optimization) - In this application, a small district is modeled and optimized. This example also includes binary decision variables. - [06_Water_Supply_System](https://github.com/FZJ-IEK3-VSA/FINE/tree/master/examples/06_Water_Supply_System) - The application cases of ETHOS.FINE are not limited. This application shows how to model the water supply system. - [07_NetCDF_to_save_and_set_up_model_instance](https://github.com/FZJ-IEK3-VSA/FINE/tree/dmaster/examples/07_NetCDF_to_save_and_set_up_model_instance) - This example shows how to save the input and optimized results of an energy system Model instance to netCDF files to allow reproducibility. - [08_Spatial_and_technology_aggregation](https://github.com/FZJ-IEK3-VSA/FINE/tree/master/examples/08_Spatial_and_technology_aggregation) - These two examples show how to reduce the model complexity. Model regions can be aggregated to reduce the number of regions (spatial aggregation). Input parameters are automatically adapted. Furthermore, technologies can be aggregated to reduce complexity, e.g. reducing the number of different PV components (technology aggregation). Input parameters are automatically adapted. - [09_Stochastic_Optimization](https://github.com/FZJ-IEK3-VSA/FINE/tree/master/examples/9_Stochastic_Optimizatio) - In this application, a stochastic optimization is performed. It is possible to perform the optimization of an energy system model with different input parameter sets to receive a more robust solution. - [10_PerfectForesight](https://github.com/FZJ-IEK3-VSA/FINE/tree/master/examples/10_PerfectForesight) - In this application, a transformation pathway of an energy system is modeled and optimized showing how to handle several investment periods with time-dependent assumptions for costs and operation. - [11_Partload](https://github.com/FZJ-IEK3-VSA/FINE/tree/master/examples/11_Partload) - In this application, a hydrogen system is modeled and optimized considering partload behavior of the electrolyzer. ## Notes for developers ### Editable install from conda-forge It is recommended to create a clean environment with conda to use ETHOS.FINE because it requires many dependencies. ```bash mamba env create --name fine --file requirements_dev.yml mamba activate fine ``` Install ETHOS.FINE as editable install and without checking the dependencies from pypi with ```bash python -m pip install --no-deps --editable . ``` ### Editable install from pypi If you do not want to use conda-forge consider the steps in section [Installation from pipy](#Installation-from-pipy) and install ETHOS.FINE as editable install and with developer dependencies with ```bash python -m pip install --editable .[develop] ``` ### Good coding style We use [ruff](https://docs.astral.sh/ruff) to ensure good coding style. Make sure to use it before contributing to the code base with ```bash ruff check fine ``` ## License MIT License Copyright (C) 2016-2025 FZJ-ICE-2 Active Developers: Johannes Behrens, Theresa Klütz, Noah Pflugradt, Julian Belina, Arne Burdack, Toni Busch, Philipp Dunkel, David Franzmann, Maike Gnirß, Thomas Grube, Lars Hadidi, Heidi Heinrichs, Shitab Ishmam, Sebastian Kebrich, Jochen Linßen, Nils Ludwig, Lilly Madeisky, Drin Marmullaku, Gian Müller, Kenneth Okosun, Olalekan Omoyele, Shruthi Patil, Kai Schulze, Julian Schönau, Maximilian Stargardt, Lana Söltzer, Henrik Wenzel, Bernhard Wortmann, Lovindu Wijesinghe, Christoph Winkler, Detlef Stolten Alumni: Robin Beer, Henrik Büsing, Dilara Caglayan, Patrick Freitag, Maximilian Hoffmann, Jason Hu, Timo Kannengießer, Kevin Knosala, Leander Kotzur, Felix Kullmann, Stefan Kraus, Rachel Maier, Peter Markewitz, Lars Nolting, Jan Priesmann, Stanley Risch, Martin Robinius, Bismark Singh, Andreas Smolenko, Peter Stenzel, Chloi Syranidou, Johannes Thürauf, Lara Welder, Michael Zier You should have received a copy of the MIT License along with this program. If not, see https://opensource.org/licenses/MIT ## About Us <a href=\"https://www.fz-juelich.de/en/ice/ice-2\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/iek3-square.png?raw=True\" alt=\"Institute image ICE-2\" width=\"280\" align=\"right\" style=\"margin:0px 10px\"/></a> We are the <a href=\"https://www.fz-juelich.de/en/ice/ice-2\">Institute of Climate and Energy Systems (ICE) - Jülich Systems Analysis</a> belonging to the <a href=\"https://www.fz-juelich.de/en\">Forschungszentrum Jülich</a>. Our interdisciplinary department's research is focusing on energy-related process and systems analyses. Data searches and system simulations are used to determine energy and mass balances, as well as to evaluate performance, emissions and costs of energy systems. The results are used for performing comparative assessment studies between the various systems. Our current priorities include the development of energy strategies, in accordance with the German Federal Government's greenhouse gas reduction targets, by designing new infrastructures for sustainable and secure energy supply chains and by conducting cost analysis studies for integrating new technologies into future energy market frameworks. ## Contributions and Support Every contributions are welcome: - If you have a question, you can start a [Discussion](https://github.com/FZJ-IEK3-VSA/FINE/discussions). You will get a response as soon as possible. - If you want to report a bug, please open an [Issue](https://github.com/FZJ-IEK3-VSA/FINE/issues/new). We will then take care of the issue as soon as possible. - If you want to contribute with additional features or code improvements, open a [Pull request](https://github.com/FZJ-IEK3-VSA/FINE/pulls). ## Code of Conduct Please respect our [code of conduct](CODE_OF_CONDUCT.md). ## Acknowledgement This work was initially supported by the Helmholtz Association under the Joint Initiative [\"Energy System 2050 A Contribution of the Research Field Energy\"](https://www.helmholtz.de/en/research/energy/energy_system_2050/). The authors also gratefully acknowledge financial support by the Federal Ministry for Economic Affairs and Energy of Germany as part of the project [METIS](http://www.metis-platform.net/) (project number 03ET4064, 2018-2022). This work was supported by the Helmholtz Association under the program [\"Energy System Design\"](https://www.helmholtz.de/en/research/research-fields/energy/energy-system-design/). <p float=\"left\"> <a href=\"https://www.helmholtz.de/en/\"><img src=\"https://www.helmholtz.de/fileadmin/user_upload/05_aktuelles/Marke_Design/logos/HG_LOGO_S_ENG_RGB.jpg\" alt=\"Helmholtz Logo\" width=\"200px\"></a> </p>\n",
                "dependencies": "[build-system] requires = [\"setuptools>=61.0.0\", \"wheel\"] build-backend = \"setuptools.build_meta\" [project] name = \"fine\" version = \"2.4.1\" description = \"Framework for integrated energy systems assessment\" readme = \"README.md\" authors = [ { name = \"Johannes Behrens\", email = \"j.behrens@fz-juelich.de\"}, { name = \"Theresa Klütz\", email = \"t.kluetz@fz-juelich.de\" }, ] maintainers = [ { name = \"Johannes Behrens\", email = \"j.behrens@fz-juelich.de\"}, { name = \"Theresa Klütz\", email = \"t.kluetz@fz-juelich.de\" }, ] license = { file = \"LICENSE\" } classifiers = [ \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: MIT License\", \"Natural Language :: English\", \"Operating System :: OS Independent\", \"Programming Language :: Python\", \"Programming Language :: Python :: 3\", \"Topic :: Scientific/Engineering :: Mathematics\", \"Topic :: Software Development :: Libraries :: Python Modules\", ] keywords = [\"energy assesment\", \"energy system\", \"optimization\"] dependencies = [ \"geopandas<=0.14.4\", \"openpyxl<=3.1.5\", \"matplotlib<=3.9.2\", \"xlrd<=2.0.1\", \"pyomo<=6.8.0\", \"numpy<=1.26.4\", \"pandas>=2,<=2.2.3\", \"scipy<=1.14.1\", \"scikit-learn>=1.2,<=1.5.2\", \"xarray<=2024.3\", \"rasterio<=1.4.1\", \"netcdf4<=1.7.2\", \"tsam\", \"pwlf<=2.2.1\", \"psutil<=5.9.8\", \"gurobi-logtools<=3.1.0\", \"ipykernel<=6.29.5\", ] requires-python = \">=3.10,<3.13\" [project.optional-dependencies] develop = [ \"sphinx<=7.4.4\", \"sphinx_rtd_theme<=2.0.0\", \"myst-parser<=2.0.0\", \"pytest<=8.3.3\", \"pytest-cov<=4.1.0\", \"pytest-xdist<=3.6.1\", \"nbval<=0.11.0\", \"ruff<0.7.0\", ] #Configureation options # https://docs.pytest.org/en/7.1.x/reference/reference.html#configuration-options [tool.pytest.ini_options] testpaths = [\"test\"] console_output_style = \"progress\" # How to configure Filterwarning: # https://docs.python.org/3/library/warnings.html#warning-filter # action:message:category:module:line # Omit a field by add ing \":\" for each omitted field # Actions are: \"default\" # \"error\", \"ignore\", \"always\", \"module\", \"once\" filterwarnings = [] [project.urls] homepage = \"https://www.fz-juelich.de/en/ice/ice-2/research-1/open_source/fine\" repository = \"https://github.com/FZJ-IEK3-VSA/FINE\" documentation = \"https://vsa-fine.readthedocs.io/en/master/\" [tool.ruff] extend-include = [\"*.ipynb\"] [tool.ruff.lint] select = [\"E4\", \"E7\", \"E9\", \"F\", \"PL\"] # See https://docs.astral.sh/ruff/rules for explanations ignore = [ \"F403\", # ‘from module import *’ used; unable to detect undefined names \"PLR0913\", # Too many arguments in function definition ( > 5) \"PLR0912\", # Too many branches ( > 12) (if, else statements) \"PLR0915\", # Too many statements ( > 50) \"PLR0911\", # Too many return statements ( > 6) \"PLR2004\", # Magic value used in comparison, consider replacing `2` with a constant variable \"PLR1714\", # Consider merging multiple comparisons. Use a `set` if the elements are hashable. ]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/hisim",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/HiSim",
            "content": {
                "codemeta": "",
                "readme": "[![PyPI Version](https://img.shields.io/pypi/v/hisim.svg)](https://pypi.python.org/pypi/hisim) [![PyPI - License](https://img.shields.io/pypi/l/hisim)](LICENSE) <a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://www.fz-juelich.de/static/media/Logo.2ceb35fc.svg\" alt=\"Forschungszentrum Juelich Logo\" width=\"230px\"></a> # ETHOS.HiSim - Household Infrastructure and Building Simulator ETHOS.HiSim is a Python package for simulation and analysis of household scenarios and building systems using modern components as alternative to fossil fuel based ones. This package integrates load profiles generation of electricity consumption, heating demand, electricity generation, and smart strategies of modern components, such as heat pump, battery, electric vehicle or thermal energy storage. ETHOS.HiSim is a package under development by Forschungszentrum Jülich und Hochschule Emden/Leer. For detailed documentation, please access [ReadTheDocs](https://household-infrastructure-simulator.readthedocs.io/en/latest/) of this repository. # Install Graphviz If you want to use the feature that generates system charts, you need to install GraphViz in your system. If you don't have Graphviz installed, you will experience error messages about a missing dot.exe under Windows. Follow the installation instructions from here: https://www.graphviz.org/download/ (or simply disable the system charts) Clone Repository ----------------------- To clone this repository, enter the following command to your terminal: ```python git clone https://github.com/FZJ-IEK3-VSA/HiSim.git ``` Virtual Environment ----------------------- Before installing `ETHOS.Hisim`, it is recommended to set up a Python virtual environment. Let `hisimvenv` be the name of virtual environment to be created. For Windows users, setting the virtual environment in the path `\\Hisim` is done with the command line: ```python python -m venv hisimvenv ``` After its creation, the virtual environment can be activated in the same directory: ```python hisimvenv\\Scripts\\activate ``` For Linux/Mac users, the virtual environment is set up and activated as follows: ```python virtual hisimvenv source hisimvenv/bin/activate ``` Alternatively, Anaconda can be used to set up and activate the virtual environment: ```python conda create -n hisimvenv python=3.9 conda activate hisimvenv ``` With the successful activation, `ETHOS.HiSim` is ready to be locally installed. Install Package ------------------------ After setting up the virtual environment, install the package to your local libraries: ```python pip install -e . ``` Optional: Set Environment Variables ----------------------- Certain components might access APIs to retrieve data. In order to use them, you need to set the url and key as environment variables. This can be done with an `.env` file wihtin the HiSim root folder or with system tools. The environment variables are: ``` UTSP_URL UTSP_API_KEY ``` Run Simple System Setups ----------------------- Run the python interpreter in the `HiSim/system_setups` directory with the following command: ```python python ../hisim/hisim_main.py simple_system_setup_one.py ``` or ```python python ../hisim/hisim_main.py simple_system_setup_two.py ``` This command executes `hisim_main.py` on the setup function `setup_function` implemented in the files `simple_system_setup_one.py` and `simple_system_setup_two.py` that are stored in `HiSim/system_setups`. The results can be visualized under directory `results` created under the same directory where the script with the setup function is located. Run Basic Household System Setup ----------------------- The directory `HiSim/system_setups` also contains a basic household configuration in the script `basic_household.py`. It can be executed with the following command: ```python python ../hisim/hisim_main.py basic_household.py ``` The system is set up with the following elements: * Occupancy (Residents' Demands) * Weather * Photovoltaic System * Building * Heat Pump Hence, photovoltaic modules and the heat pump are responsible for covering the electricity and thermal energy demands as best as possible. As the name of the setup function says, the components are explicitly connected to each other, binding inputs to their corresponding output sequentially. This is different from automatically connecting inputs and outputs based on similarity. For a better understanding of explicit connection, proceed to section `IO Connecting Functions`. Generic Setup Function Walkthrough --------------------- The basic structure of a setup function is as follows: 1. Set the simulation parameters (See `SimulationParameters` class in `hisim/hisim/component.py`) 1. Create a `Component` object and add it to `Simulator` object 1. Create a `Component` object from one of the child classes implemented in `hisim/hisim/components` 1. Check if `Component` class has been correctly imported 1. If necessary, connect your object's inputs with previous created `Component` objects' outputs. 1. Finally, add your `Component` object to `Simulator` object 1. Repeat step 2 while all the necessary components have been created, connected and added to the `Simulator` object. Once you are done, you can run the setup function according to the description in the simple system setup run. Package Structure ----------- The main program is executed from `hisim/hisim/hisim_main.py`. The `Simulator`(`simulator.py`) object groups `Component`s declared and added from the setups functions. The `ComponentWrapper` (`simulator.py`) gathers together the `Component`s inside a `Simulator` object. The `Simulator` object performs the entire simulation under the function `run_all_timesteps` and stores the results in a Python pickle `data.pkl` in a subdirectory of `hisim/hisim/results` named after the executed setup function. Plots and the report are automatically generated from the pickle by the class `PostProcessor` (`hisim/hisim/postprocessing/postprocessing.py`). Component Class ----------- A child class inherits from the `Component` class in `hisim/hisim/component.py` and has to have the following methods implemented: * i_save_state: updates previous state variable with the current state variable * i_restore_state: updates current state variable with the previous state variable * i_simulate: performs a timestep iteration for the `Component` * i_doublecheck: checks if the values are expected throughout the iteration These methods are used by `Simulator` to execute the simulation and generate the results. List of `Component` Children ----------- Theses classes inherent from `Component` (`component.py`) class and can be used in your setup function to customize different configurations. All `Component` class children are stored in `hisim/hisim/components` directory. Some of these classes are: - `RandomNumbers` (`random_numbers.py`) - `SimpleWaterStorage` (`simple_water_storage.py`) - `Transformer` (`transformer_rectifier.py`) - `PVSystem` (`generic_pv_system.py`) - `SimpleCHP` (`generic_chp.py`) - `CSVLoader` (`csvloader.py`) - `SumBuilderForTwoInputs` (`sumbuilder.py`) - `SumBuilderForThreeInputs` (`sumbuilder.py`) - ToDo: more components to be added Connecting Input/Outputs ----------- Let `my_home_electricity_grid` and `my_appliance` be `Component` objects used in the setup function. The object `my_apppliance` has an output `ElectricityOutput` that has to be connected to an object `ElectricityGrid`. The object `my_home_electricity_grid` has an input `ElectricityInput`, where this connection takes place. In the setup function, the connection is performed with the method `connect_input` from the `Simulator` class: ```python my_home_electricity_grid.connect_input(input_fieldname=my_home_electricity_grid.ELECTRICITY_INPUT, src_object_name=my_appliance.component_name, src_field_name=my_appliance.ELECTRICITY_OUTPUT) ``` Configuration Automator ----------- A configuration automator is under development and has the goal to reduce connections calls among similar components. Post Processing ----------- After the simulator runs all time steps, the post processing (`postprocessing.py`) reads the persistent saved results, plots the data and generates a report. ## Contributions and Collaborations ETHOS.HiSim welcomes any kind of feedback, contributions, and collaborations. If you are interested in joining the project, adding new features, or providing valuable insights, feel free to reach out (email to k.rieck@fz-juelich.de) and participate in our HiSim developer meetings held every second Monday. Additionally, we encourage you to utilize our Issue section to share feedback or report any bugs you encounter. We look forward to your contributions and to making meaningful improvements. Happy coding! ## License MIT License Copyright (C) 2020-2021 Noah Pflugradt, Leander Kotzur, Detlef Stolten, Tjarko Tjaden, Kevin Knosala, Sebastian Dickler, Katharina Rieck, David Neuroth, Johanna Ganglbauer, Vitor Zago, Frank Burkard, Maximilian Hillen, Marwa Alfouly, Franz Oldopp, Markus Blasberg, Kristina Dabrock You should have received a copy of the MIT License along with this program. If not, see https://opensource.org/licenses/MIT ## About Us <a href=\"https://www.fz-juelich.de/en/ice/ice-2\"><img src=\"https://www.fz-juelich.de/SharedDocs/Bilder/IEK/IEK-3/Abteilungen2015/VSA_DepartmentPicture_2019-02-04_459x244_2480x1317.jpg?__blob=normal\" alt=\"Juelich Systems Analysis\"></a> We are the [Institute of Climate and Energy Systems - Juelich Systems Analysis](https://www.fz-juelich.de/en/ice/ice-2) belonging to the [Forschungszentrum Jülich](www.fz-juelich.de/en). Our interdisciplinary institute's research is focusing on energy-related process and systems analyses. Data searches and system simulations are used to determine energy and mass balances, as well as to evaluate performance, emissions and costs of energy systems. The results are used for performing comparative assessment studies between the various systems. Our current priorities include the development of energy strategies, in accordance with the German Federal Government's greenhouse gas reduction targets, by designing new infrastructures for sustainable and secure energy supply chains and by conducting cost analysis studies for integrating new technologies into future energy market frameworks. ## Contributions and Users Development Partners: **Hochschule Emden/Leer** inside the project \"Piegstrom\". **4ward Energy** inside the EU project \"WHY\" and the FFG project \"[AI4CarbonFreeHeating](https://www.4wardenergy.at/de/referenzen/ai4carbonfreeheating)\" ## Acknowledgement This work was supported by the Helmholtz Association under the Joint Initiative [\"Energy System 2050 A Contribution of the Research Field Energy\"](https://www.helmholtz.de/en/research/energy/energy_system_2050/). <a href=\"https://www.helmholtz.de/en/\"><img src=\"https://www.helmholtz.de/fileadmin/user_upload/05_aktuelles/Marke_Design/logos/HG_LOGO_S_ENG_RGB.jpg\" alt=\"Helmholtz Logo\" width=\"200px\" style=\"float:right\"></a> For this work weather data is based on data from [\"German Weather Service (Deutscher Wetterdienst-DWD)\"](https://www.dwd.de/DE/Home/home_node.html/, https://www.dwd.de/DE/service/rechtliche_hinweise/rechtliche_hinweise_node.html) and [\"NREL National Solar Radiation Database\"](https://nsrdb.nrel.gov/data-viewer/download/intro/) (License: Creative Commons Attribution 3.0 United States License, Creative Commons BY 4.0); individual values are averaged. <a href=\"https://www.dwd.de/\"><img src=\"https://www.dwd.de/SharedDocs/bilder/DE/logos/dwd/dwd_logo_258x69.png?__blob=normal&v=1\" alt=\"DWD Logo\" width=\"200px\" style=\"float:right\"></a> This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No. 891943. <img src=\"eulogo.png\" alt=\"EU Logo\" width=\"200px\" style=\"float:right\"></a> <a href=\"https://www.why-h2020.eu/\"><img src=\"whylogo.jpg\" alt=\"WHY Logo\" width=\"200px\" style=\"float:right\"></a> This project furthermore recieves funding by the FFG through the project \"[AI4CarbonFreeHeating](https://www.4wardenergy.at/de/referenzen/ai4carbonfreeheating)\". <a href=\"https://www.4wardenergy.at/de/referenzen/ai4carbonfreeheating\"><img src=\"AI4CFH-Logo-S.jpg\" alt=\"AI4CFH Logo\" width=\"200px\" style=\"float:right\"></a>\n===== HiSim ===== .. image:: https://img.shields.io/pypi/v/hisim.svg :target: https://pypi.python.org/pypi/hisim .. image:: https://img.shields.io/travis/audreyr/hisim.svg :target: https://travis-ci.com/audreyr/hisim .. image:: https://readthedocs.org/projects/hisim/badge/?version=latest :target: https://hisim.readthedocs.io/en/latest/?badge=latest :alt: Documentation Status HiSim is a house infrastructure simulator * Free software: MIT license * Documentation: https://hisim.readthedocs.io. Features -------- * TODO Credits ------- This package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template. .. _Cookiecutter: https://github.com/audreyr/cookiecutter .. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage\n",
                "dependencies": "[tool.black] line-length = 120\nnumpy pandas matplotlib seaborn reportlab pvlib # ==0.10.2 oemof.thermal openpyxl pytest sphinx sphinx-rtd-theme dataclasses_json hplib==1.9 bslib==0.6 psutil pydot graphviz dataclass_wizard==0.34.0 utspclient==0.1.6 pyam-iamc html2image control casadi plotly ordered_set typing_extensions python-dotenv windpowerlib # ==0.2.1 wetterdienst==0.63.0 cdsapi xarray pygfunction pyloadprofilegenerator\n#!/usr/bin/env python \"\"\"The setup script.\"\"\" # clean from setuptools import setup, find_packages with open(\"README.md\", encoding=\"utf-8\") as readme_file: readme = readme_file.read() with open(\"requirements.txt\", encoding=\"utf-8\") as requirements_file: requirements = requirements_file.read().splitlines() setup_requirements = [ \"pytest-runner\", ] test_requirements = [ \"pytest>=3\", ] setup( author=\"Noah Pflugradt\", author_email=\"n.pflugradt@fz-juelich.de\", python_requires=\">=3.5\", classifiers=[ \"Development Status :: 2 - Pre-Alpha\", \"Intended Audience :: Developers\", \"License :: OSI Approved :: MIT License\", \"Natural Language :: English\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.5\", \"Programming Language :: Python :: 3.6\", \"Programming Language :: Python :: 3.7\", \"Programming Language :: Python :: 3.8\", ], description=\"ETHOS.HiSim is a house infrastructure simulator\", entry_points={ \"console_scripts\": [ \"hisim=hisim.cli:main\", ], }, install_requires=requirements, license=\"MIT license\", long_description=readme, long_description_content_type=\"text/markdown\", package_data={\"hisim\": [\"inputs/*\"]}, include_package_data=True, keywords=\"hisim\", name=\"hisim\", packages=find_packages(include=[\"hisim\", \"hisim.*\"]), setup_requires=setup_requirements, test_suite=\"tests\", tests_require=test_requirements, url=\"https://github.com/FZJ-IEK3-VSA/HiSim\", version=\"1.2.2\", zip_safe=False, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ethospenalps",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/ETHOS_PeNALPS",
            "content": {
                "codemeta": "",
                "readme": "| Name | Version | Platforms | Daily Tests | |---|---|---|---| |[![Conda Recipe](https://img.shields.io/badge/recipe-ethos_penalps-green.svg)](https://anaconda.org/conda-forge/ethos_penalps)|[![Conda Version](https://img.shields.io/conda/vn/conda-forge/ethos_penalps.svg)](https://anaconda.org/conda-forge/ethos_penalps)|[![Conda Platforms](https://img.shields.io/conda/pn/conda-forge/ethos_penalps.svg)](https://anaconda.org/conda-forge/ethos_penalps) |![example workflow](https://github.com/FZJ-IEK3-VSA/ETHOS_PeNALPS/actions/workflows/daily_tests.yml/badge.svg) <a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/FJZ_IEK-3_logo.svg?raw=True\" alt=\"Forschungszentrum Juelich Logo\" width=\"300px\"></a> # ETHOS.PeNALPS ETHOS.PeNALPS (Petri Net Agent based Load Profile Simulator) is a Python library for the simulation of load profiles of industrial manufacturing processes. It is part of [ETHOS (Energy Transformation Pathway Optimization Suite)](https://go.fzj.de/ethos_suite). Load profiles are energy demand time series. Processes that can be simulated using ETHOS.PeNALPS include, for example, steel, paper, and industrial food production. One or multiple product orders are passed to the model which starts the simulation and eventually creates the desired load profiles. # Working Principle The figure below shows the main conceptual objects of ETHOS.PeNALPS which are: - Generic model objects - Material flow simulations - Production plans - Result load profiles The model of the material flow simulation is created by users based on generic simulation objects. After the material flow simulation is completed, a set of production orders is passed to the model to start the simulation. The simulation generates a production plan that tracks the activity of each node to fulfill the requested set of orders. Based on the activity in the production plan, the load profiles are created for each node in therein. ![Main Component Overview](paper/main_component_overview.png) *Depiction of the main components and workflow of ETHOS.PeNALPS* The [HTML documentation provides a tutorial](https://ethospenalps.readthedocs.io/en/latest/ethos_penalps_tutorial/0_overview.html) for ETHOS.PeNALPS. The executable files for the tutorial are located in the example section of this repository. Also two examples for a [toffee production process](https://ethospenalps.readthedocs.io/en/latest/examples/toffee_example.html) and a [b-pillar production process](https://ethospenalps.readthedocs.io/en/latest/examples/b_pillar_example.html) are available. # Installation ## Requirements The installation process uses a Conda-based Python package manager. We highly recommend using Mamba instead of Anaconda. The recommended way to use Mamba on your system is to install the Miniforge distribution. They offer installers for Windows, Linux and OS X. Have a look at the [Mamba installation guide](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html) for further details. If you prefer to stick to Anaconda you should install the [libmamba solver which is a lot faster than the classic conda solver](https://www.anaconda.com/blog/a-faster-conda-for-a-growing-community). Otherwise the installation of ETHOS.PeNALPS might take very long or does not succeed at all. ``` conda install -n base conda-libmamba-solver conda config --set solver libmamba ``` Please note that the installation time of the solver can be very long if you have installed a lot of other packages into you conda base environment. In the following the commands mamba and conda are exchangeable if you prefer to use conda. ## Installation via conda-forge The simplest way ist to install ETHOS.PeNALPS into a fresh environment from conda-forge with: Create a new environment ```python mamba create -n penalps_env ``` Activate the environment ```python mamba activate penalps_env ``` Install ETHOS.PeNALPS from conda forge ```python mamba install -c conda-forge ethos_penalps ``` ## Installation from Github for Development First the repository must be cloned from Github ```python git clone https://github.com/FZJ-IEK3-VSA/ETHOS_PeNALPS.git ``` Then change the directory to the root folder of the repository. ```python cd ETHOS_PeNALPS ``` Create a new environment from the environment.yml file with all required dependencies. ```python mamba env create --file=environment.yml ``` Activate the new environment. ```python mamba activate ethos_penalps ``` Install ethos_penalps locally in editable to mode for development. ```python pip install -e . ``` # Tests The library can be tested by running pytest with the following command from the root folder. ```python pytest ``` # Documentation A ReadTheDocs Documentation can be found [here](http://ethospenalps.readthedocs.io/). # Contributing Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: ## Types of Contributions ### Report Bugs Report bugs at https://github.com/FZJ-IEK3-VSA/ETHOS_PeNALPS/issues. If you are reporting a bug, please include: * Your operating system name and version. * Any details about your local setup that might be helpful in troubleshooting. * Detailed steps to reproduce the bug. ### Fix Bugs Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. ### Implement Features Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. ### Write Documentation ETHOS.PeNALPS could always use more documentation, whether as part of the official ETHOS.PeNALPS docs, in docstrings, or even on the web in blog posts, articles, and such. ### Submit Feedback The best way to send feedback is to file an issue at https://github.com/FZJ-IEK3-VSA/ETHOS_PeNALPS/issues. If you are proposing a feature: - Explain in detail how it would work. - Keep the scope as narrow as possible, to make it easier to implement. - Remember that this is a volunteer-driven project, and that contributions are welcome :)\n",
                "dependencies": "[build-system] requires = [\"setuptools>=64.0.0\", \"wheel\"] build-backend = \"setuptools.build_meta\" [tool.setuptools.package-data] \"ethos_penalps\" = [\"py.typed\"] [tool.setuptools.packages.find] where = [\"src\"] [project] name = \"ethos_penalps\" version = \"1.0.7\" description = \"A package to create energy load curves for industry locations in Germany\" readme = \"README.md\" authors = [ { name = \"Julian Belina\", email = \"j.belina@fz-juelich.de\" }, { name = \"Noah Pflugradt\", email = \"n.pflugradt@fz-juelich.de\" }, { name = \"Detlef Stolten\", email = \"d.stolten@fz-juelich.de\" }, ] license = { file = \"LICENSE\" } keywords = [ \"Load Profile\", \"Simulator\", \"Energy Simulation\", \"Industrial Process\", ] requires-python = \">=3.10\" #Configureation options # https://docs.pytest.org/en/7.1.x/reference/reference.html#configuration-options [tool.pytest.ini_options] filterwarnings = [ # \"ignore::DeprecationWarning:matplotlib\", # \"ignore::DeprecationWarning:pkg_resources\", # \"ignore::DeprecationWarning:distutils\", # \"ignore::DeprecationWarning:requests_toolbelt\", # \"ignore::DeprecationWarning:jupyter_client\", # \"ignore::DeprecationWarning:importlib\", \"ignore:invalid value encountered in cast:RuntimeWarning:numpy\", \"ignore::marshmallow.warnings.RemovedInMarshmallow4Warning\", \"ignore:The distutils package is deprecated and slated for removal in Python 3.12.:DeprecationWarning\", \"ignore: Deprecated API features detected!\", \"ignore: np.find_common_type is deprecated\", \"ignore:the load_module()\", \"ignore:the 'timedelta' type is not supported,\", \"ignore:the imp module is deprecated in favour\", \"ignore:Deprecated call to `pkg_resources.declare_namespace\", \"ignore: Deprecated call to `pkg_resources.declare_namespace\", \"ignore: distutils Version classes are deprecated.\", \"ignore: pkg_resources is deprecated as an API.\", \"ignore: Deprecated API features detected!\" ] # How to configure Filterwarning:minversion = \"6.0\" # https://docs.pytest.org/en/7.1.x/reference/reference.html#confval-pythonpath testpaths = [\"test\"] # Sets the path where to look for tests pythonpath =[\"test\"] # Sets the path which should be prepended to pythonpath relative to the root folder console_output_style = \"count\" # https://docs.python.org/3/library/warnings.html#warning-filter # action:message:category:module:line # Ommit a field by add ing \":\" for each ommited field # Actions are: \"default\" # \"error\", \"ignore\", \"always\", \"module\", \"once\" markers = [\"load_profile_entry_analyzer\",\"carpet_plot_tests\",\"matrix_resample_and_compression_tests\",\"resample_load_profile_meta_data\",\"production_plan_input_output\",\"test_load_profile_post_processing\"] [tool.mypy] python_version = \"3.10\" warn_return_any = true warn_unused_configs = true check_untyped_defs= true [[tool.mypy.overrides]] module = [\"cloudpickle.*\",\"datapane.*\",\"matplotlib.*\"] ignore_missing_imports = true [tool.ruff.lint] ignore = [\"F401\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ethosreflow",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/ethos.REFLOW",
            "content": {
                "codemeta": "",
                "readme": "# REFLOW: Renewable Energy potentials workFLOW manager <a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/FJZ_IEK-3_logo.svg?raw=True\" alt=\"Forschungszentrum Juelich Logo\" width=\"300px\"></a> REFLOW is a workflow manager tool designed to streamline and automate tasks related to renewable energy potential analyses. It is built with Luigi and provides an automated, robust framework for data acquisition, processing, land/sea eligibility analysis, technology placements, simulations and visualizations. It is build with transparency and reproducibility in mind. ## Requirements * Python * An IDE (e.g. PyCharm, Visual Studio Code, etc.) * Unix-like system or bash on Windows * *optional*: Docker Desktop if running in container ## Getting Started ### Try an example project - Aachen technical wind potential We highly recommend starting with the example workflow to get a feel for how REFLOW works. The example project is a simple technical wind energy potential analysis for a small region in Germany. To run this analysis, follow these steps: 1. Clone this repository to your local machine using: ```bash git clone https://github.com/FZJ-IEK3-VSA/ethos.REFLOW.git ``` 2. Navigate to the example project directory: ```bash cd reflow/example_workflows/aachen_technical ``` 3. Follow the instructions in the README.md file in the example project directory. ### Initial Setup for a new project To start your own new project using REFLOW, follow these steps: 1. Clone this repository to your local machine using: ```bash git clone https://github.com/FZJ-IEK3-VSA/ethos.REFLOW.git ``` 2. **Initialize a new Project:** Navigate to the **main REFLOW repo (this repo)** and run the initialize_project.py script by executing: ```bash python initialize_project.py ``` You will be prompted to enter the name of your new project and the parent directory where it should be created. 3. Create the main REFLOW python environment by running: ```bash conda env create -f required_software/requirements-reflow.yml ``` Activate the environment by running: ```bash conda activate reflow ``` 4. We recommend using a seperate conda environment for each software package which needs to be run outside of the main REFLOW environment. For example, if you are using WAsP, you can create a new environment which contains the PyWAsP package by: 4.1. Creating an environment file for the WAsP python package under the `required_software` directory. You can use the provided `requirements-glaes.yml` file from the Aachen technical example as a template. 4.2. If the new environment file is in the required_software directory, the environment will automatically be created during the first task of the REFLOW workflow. 4.3. You can then run whichever task's script is needed inside the environment within your main REFLOW workflow. Do this by using a wrapper script which activates the environment, runs the task, and then deactivates the environment. (Again, see the Aachen technical example for reference.) **Optional but recommended - work with GIT:** 5. **Create a New Git Repository**: Navigate into your new project directory and initilize it as a git repository: ```bash cd path/to/your-project-name git init git add . git commit -m \"Initial commit\" ``` 6. **Create an Empty Repository on Github** (or any other Git hosting service): Ensure the repository name matches your project's name. Do not initialize the repository with a README, .gitignore or license. 7. **Link your local repository to the remote repository**: Make sure you are in your new project directory and run the following commands: ```bash git remote add origin https://github.com/your-username/your-repo-name.git git branch -M main git push -u origin main ``` You can now start working on your project and push your changes to the remote repository. ## Examples The example workflows are located in the `example_workflows` directory. Each example contains a README.md file with detailed instructions on how to run the workflow. We recommend starting with the [Aachen technical wind potential example](example_workflows/aachen_technical/) to get a feel for how REFLOW works. ## License This project is licensed under the MIT License - see the [LICENSE](LICENSE.txt) file for details. Copyright (c) 2024 Tristan Pelser (FZJ IEK-3), Jann Michael Weinand (FZJ IEK-3), Patrick Kuckertz (FZJ IEK-3), Detlef Stolten (FZJ IEK-3) You should have received a copy of the MIT License along with this program. If not, see https://opensource.org/licenses/MIT ## About Us <a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/iek3-square.png?raw=True\" alt=\"Institute image IEK-3\" width=\"280\" align=\"right\" style=\"margin:0px 10px\"/></a> We are the [Complex Energy System Modesl and Data Structures](https://www.fz-juelich.de/en/iek/iek-3/research/integrated-models-and-strategies/complex-energy-system-models-and-data-structures) department at the [Institute of Energy and Climate Research: Techno-economic Systems Analysis (IEK-3)](https://www.fz-juelich.de/en/iek/iek-3), belonging to the Forschungszentrum Jülich. Our interdisciplinary department's research is focusing on energy-related process and systems analyses. Data searches and system simulations are used to determine energy and mass balances, as well as to evaluate performance, emissions and costs of energy systems. The results are used for performing comparative assessment studies between the various systems. Our current priorities include the development of energy strategies, in accordance with the German Federal Government's greenhouse gas reduction targets, by designing new infrastructures for sustainable and secure energy supply chains and by conducting cost analysis studies for integrating new technologies into future energy market frameworks. ## Acknowledgements The authors would like to thank the German Federal Government, the German State Governments, and the Joint Science Conference (GWK) for their funding and support as part of the NFDI4Ing consortium. Funded by the German Research Foundation (DFG) - 442146713, this work was also supported by the Helmholtz Association as part of the program \"Energy System Design\". <p float=\"left\"> <a href=\"https://www.helmholtz.de/en/\"><img src=\"https://www.helmholtz.de/fileadmin/user_upload/05_aktuelles/Marke_Design/logos/HG_LOGO_S_ENG_RGB.jpg\" alt=\"Helmholtz Logo\" width=\"200px\"></a> </p>\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ethoszoomin",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/ETHOS.zoomin",
            "content": {
                "codemeta": "",
                "readme": "ETHOS.zoomin: A spatial disaggregation workflow tool developed within the LOCALISED project. ============================== A workflow tool that: 1. Pulls data from a database. 2. Disaggregates it based on proxy specifications present in the database. 3. Evaulates quality rating - based on quality of the data to be disaggregated, proxy data and the confidence in the assigned proxy. 4. Dumps the disaggregated data into the database. -------- Installation steps ------------ 0. Before you begin: Please make sure you have miniforge installed on your machine Also create the initial database. Steps to create the Database will follow soon. 1. Clone this repository: ```bash git clone https://jugit.fz-juelich.de/iek-3/shared-code/localised/ETHOS.zoomin.git ``` 2. Install dependencies and the repo in a clean conda environment: ```bash cd ETHOS.zoomin mamba env create --file=requirements.yml conda activate zoomin pip install -e . ``` 3. Run the workflow from command line: ```bash bash run_deployment.sh ``` <p><small>Project based on the <a target=\"_blank\" href=\"https://drivendata.github.io/cookiecutter-data-science/\">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>\n",
                "dependencies": "\"\"\"Setup file for python packaging.\"\"\" import os import setuptools dir_path = os.path.dirname(os.path.realpath(__file__)) with open(os.path.join(dir_path, \"README.md\"), mode=\"r\", encoding=\"utf-8\") as fh: long_description = fh.read() setuptools.setup( name=\"zoomin\", version=\"0.1.0\", author=\"Shruthi Patil\", author_email=\"s.patil@fz-juelich.de\", description=\"Spatially disaggregates techo-economic data.\", long_description=long_description, long_description_content_type=\"text/markdown\", url=\"https://www.localised-project.eu/\", include_package_data=True, packages=setuptools.find_packages(), setup_requires=[\"setuptools-git\"], classifiers=[ \"Development Status :: 2 - Pre-Alpha\", \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: MIT License\", \"Natural Language :: English\", \"Operating System :: Microsoft :: Windows\", \"Operating System :: POSIX :: Linux\", \"Programming Language :: Python :: 3.10\", \"Topic :: Scientific/Engineering :: Information Analysis\", ], keywords=[\"data import\", \"spatial disaggregation\", \"proxy metrics\"], )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/elias",
            "repo_link": "https://gitlab.kit.edu/julian.quinting/elias-2.0",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/beta-faircore4eosc",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/beta-faircore4eosc",
            "content": {
                "codemeta": "{\"@context\":\"https://doi.org/10.5063/schema/codemeta-2.0\",\"@type\":\"SoftwareSourceCode\",\"description\":\"Beta Release - EOSC Research Software APIs and Connectors (Dagstuhl WP 6.2) \",\"name\":\"beta-faircore4eosc\",\"dateCreated\":\"2023-10-06\",\"datePublished\":\"2023-10-09\",\"license\":\"https://spdx.org/licenses/MIT.html\",\"readme\":\"https://github.com/dagstuhl-publishing/beta-faircore4eosc/blob/main/README.md\",\"softwareVersion\":\"1.0\",\"installUrl\":\"https://github.com/dagstuhl-publishing/beta-faircore4eosc/blob/main/README.md#installation-steps\",\"buildInstructions\":\"https://github.com/dagstuhl-publishing/beta-faircore4eosc/blob/main/README.md#b-deployment-steps-local-webserver-instance\",\"dateModified\":\"2023-10-13\",\"codeRepository\":\"https://github.com/dagstuhl-publishing/beta-faircore4eosc\",\"issueTracker\":\"https://github.com/dagstuhl-publishing/faircore4eosc/issues\",\"contIntegration\":\"https://github.com/dagstuhl-publishing/faircore4eosc/projects?query=is%3Aopen\",\"developmentStatus\":\"active\",\"operatingSystem\":\"Cross-platform\",\"author\":[{\"@type\":\"Person\",\"@id\":\"https://orcid.org/my-orcid?orcid=0000-0002-8504-3549\",\"affiliation\":{\"@type\":\"Organization\",\"name\":\"Schloss Dagstuhl - Leibniz Center for Informatics, Wadern\"},\"email\":\"ramy.ahmed@dagstuhl.de\",\"familyName\":\"Ahmed\",\"givenName\":\"Ramy-Badr\"}],\"programmingLanguage\":[\"PHP\",\"JavaScript\",\"Blade\"],\"softwareRequirements\":[\"PHP: ^8.1\",\"Laravel/Framework: ^10.10\",\"Livewire/Livewire: ^2.12\",\"guzzlehttp/guzzle: ^7.2\",\"laravel/tinker: ^2.8\",\"Laravel/Octane: ^2.0\",\"spiral/roadrunner-http: ^3.0.1\"],\"funder\":{\"@type\":\"Organization\",\"@id\":\"https://doi.org/10.3030/101057264\",\"funding\":\"EU-Horizon: 101057264\",\"name\":\"European Comission\"},\"relatedLink\":[\"https://faircore4eosc.dagstuhl.de/\",\"https://faircore4eosc.eu/\"],\"keywords\":[\"Dagstuhl\",\"Software Heritage\"],\"applicationCategory\":[\"MetaData\",\"API Connectors\",\"Research Software\"],\"maintainer\":{\"@type\":\"Person\",\"@id\":\"https://orcid.org/my-orcid?orcid=0000-0002-8504-3549\",\"affiliation\":{\"@type\":\"Organization\",\"name\":\"Schloss Dagstuhl - Leibniz Center for Informatics, Wadern\"},\"email\":\"ramy.ahmed@dagstuhl.de\",\"familyName\":\"Ahmed\",\"givenName\":\"Ramy-Badr\"}}\n",
                "readme": "# beta-faircore4eosc ![LARAVEL](https://img.shields.io/badge/LARAVEL-%23CC342D.svg?style=plastic&logo=laravel&logoColor=white) ![Livewire](https://img.shields.io/badge/Livewire-purple?style=plastic&logo=laravel&logoColor=white) ![PHP](https://img.shields.io/badge/PHP-777BB4?style=plastic&logo=php&logoColor=white) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808833.svg)](https://doi.org/10.5281/zenodo.12808833) > [!Note] > A demonstrable version can be accessed here: <a href=\"https://1959e979-c58a-4d3c-86bb-09ec2dfcec8a.ka.bw-cloud-instance.org/\" target=\"_blank\">**Demo Version**</a> #### Sample snapshot of the codemeta generator and converter demo: ![snap.PNG](snap.PNG) #### Sample archiving repositories demo: ![archive-samp.PNG](archive-samp.PNG) ## Installation Steps: 1) Clone this project. 2) Open a console session and navigate to the cloned directory, then: 2.1) Run \"composer install\". 2.2) Run \"npm install\". 3) (Optional) Create your local branch. 4) (Optional) Acquire SWH tokens for increased SWH-API Rate-Limits. 5) Prepare .env file: 5.1) Rename/Copy the cloned \".env.example\" file cp .env.example .env 5.2) ADD these Keys: SWH_TOKEN=Your_TOKEN_FROM_SWH_ACCOUNT # As set in step 4) SWH_API_URL=https://archive.softwareheritage.org SWH_TOKEN_STAGING=Your_STAGING_TOKEN_FROM_SWH_ACCOUNT # As set in step 4) SWH_API_URL_STAGING=https://webapp.staging.swh.network You can now proceed to either I) or II) #### I) SWH API First Steps: 1) In a console session inside the cloned directory. - Run 'php artisan tinker' to interact with the API modules. 2) Initialise a new API session: - Write: namespace App\\Modules\\SwhApi; use App\\Modules\\SwhApi; > - You can now proceed to the [SWH API Client documentation](https://github.com/Ramy-Badr-Ahmed/beta-faircore4eosc/blob/dev-cont/app/Modules/SwhApi/README.md) > - SWH Client as a standalone php library: https://github.com/Ramy-Badr-Ahmed/swh-client/wiki ___ #### II) Deployment Steps (Run your own local webserver instance): 1) Setup a Database (e.g. MariaDB/MySQL/PostgreSQL) and create a new DB schema relevant for this project. 2) Edit .env file 2.1) (Optional) Generate an Application Key --> Navigate to the cloned directory and Run \"php artisan key:generate\" 2.2) Edit Keys: APP_NAME=beta-faircore4eosc APP_ENV=local APP_DEBUG=true APP_URL=http://localhost DB_CONNECTION=mysql # As set in step 1) DB_HOST=127.0.0.1 # As set in step 1) DB_PORT=3306 # As set in step 1) DB_DATABASE=beta-faircore4eosc # As set in step 1): Name of your database schema for this project DB_USERNAME=root # As set in step 1) DB_PASSWORD= # As set in step 1) 3) Run \"php artisan migrate\". 4) (Optional) Checkout the first Commit: \"Initial Setup\". 5) Navigate to the cloned directory and Run \"php artisan serve\" in your console. 6) Visit \"http://127.0.0.1:8000\" from your browser. #### III) Quick Test After II) Deployment: 1) Navigate to an Archive page, e.g.: /beta/archival-view-3 2) Insert a group of repository URLs. 3) Navigate to the previously cloned directory and then Run \"php artisan swh:updateCron\" in your console to synchronise with SWH. Note: you may need to set up a CRON/Scheduler in your OS.\n",
                "dependencies": "{\"name\":\"laravel/laravel\",\"type\":\"project\",\"description\":\"The skeleton application for the Laravel framework.\",\"keywords\":[\"laravel\",\"framework\"],\"license\":\"MIT\",\"require\":{\"php\":\"^8.1\",\"ext-dom\":\"*\",\"ext-pdo\":\"*\",\"composer/spdx-licenses\":\"^1.5\",\"guzzlehttp/guzzle\":\"^7.2\",\"laravel/framework\":\"^10.10\",\"laravel/octane\":\"^2.0\",\"laravel/sanctum\":\"^3.2\",\"laravel/tinker\":\"^2.8\",\"laravel/ui\":\"^4.2\",\"livewire/livewire\":\"^2.12\",\"php-ds/php-ds\":\"^1.4\",\"seld/jsonlint\":\"^1.9\",\"spatie/array-to-xml\":\"^3.2\",\"spiral/roadrunner-cli\":\"^2.5.0\",\"spiral/roadrunner-http\":\"^3.0.1\"},\"require-dev\":{\"barryvdh/laravel-ide-helper\":\"^2.13\",\"fakerphp/faker\":\"^1.9.1\",\"laravel/pint\":\"^1.0\",\"laravel/sail\":\"^1.18\",\"mockery/mockery\":\"^1.4.4\",\"nunomaduro/collision\":\"^7.0\",\"phpunit/phpunit\":\"^10.1\",\"spatie/laravel-ignition\":\"^2.0\"},\"autoload\":{\"psr-4\":{\"App\\\\\":\"app/\",\"Database\\\\Factories\\\\\":\"database/factories/\",\"Database\\\\Seeders\\\\\":\"database/seeders/\"}},\"autoload-dev\":{\"psr-4\":{\"Tests\\\\\":\"tests/\"}},\"scripts\":{\"post-autoload-dump\":[\"Illuminate\\\\Foundation\\\\ComposerScripts::postAutoloadDump\",\"@php artisan package:discover --ansi\"],\"post-update-cmd\":[\"@php artisan vendor:publish --tag=laravel-assets --ansi --force\"],\"post-root-package-install\":[\"@php -r \\\"file_exists('.env') || copy('.env.example', '.env');\\\"\"],\"post-create-project-cmd\":[\"@php artisan key:generate --ansi\"]},\"extra\":{\"laravel\":{\"dont-discover\":[]}},\"config\":{\"optimize-autoloader\":true,\"preferred-install\":\"dist\",\"sort-packages\":true,\"allow-plugins\":{\"pestphp/pest-plugin\":true,\"php-http/discovery\":true}},\"minimum-stability\":\"stable\",\"prefer-stable\":true}\n{\"private\":true,\"type\":\"module\",\"scripts\":{\"dev\":\"vite\",\"build\":\"vite build\"},\"devDependencies\":{\"@popperjs/core\":\"^2.11.6\",\"@vitejs/plugin-react\":\"^4.0\",\"axios\":\"^1.7\",\"bootstrap\":\"^5.2.3\",\"chokidar\":\"^3.5.3\",\"laravel-vite-plugin\":\"^0.8.0\",\"lodash\":\"^4.17.19\",\"postcss\":\"^8.1.14\",\"react\":\"^18.2.0\",\"react-dom\":\"^18.2.0\",\"sass\":\"^1.56.1\",\"vite\":\"^4.5.5\",\"follow-redirects\":\"^1.15.4\"}}\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fairmq",
            "repo_link": "https://github.com/FairRootGroup/FairMQ",
            "content": {
                "codemeta": "{\"@context\":\"https://doi.org/10.5063/schema/codemeta-2.0\",\"@type\":\"SoftwareSourceCode\",\"name\":\"FairMQ\",\"description\":\"C++ Message Queuing Library and Framework\",\"license\":\"./COPYRIGHT\",\"datePublished\":\"2018-04-15\",\"developmentStatus\":\"active\",\"softwareVersion\":\"master\",\"releaseNotes\":\"https://github.com/FairRootGroup/FairMQ/releases\",\"codeRepository\":\"https://github.com/FairRootGroup/FairMQ/\",\"readme\":\"https://github.com/FairRootGroup/FairMQ/#readme\",\"issueTracker\":\"https://github.com/FairRootGroup/FairMQ/issues\",\"identifier\":\"https://doi.org/10.5281/zenodo.1689985\",\"maintainer\":[{\"@type\":\"ResearchOrganisation\",\"@id\":\"https://ror.org/02k8cbn47\",\"name\":\"GSI Helmholtz Centre for Heavy Ion Research\"}],\"author\":[{\"@type\":\"Person\",\"@id\":\"https://orcid.org/0000-0002-8071-4497\",\"givenName\":\"Mohammad\",\"familyName\":\"Al-Turany\"},{\"@type\":\"Person\",\"@id\":\"https://orcid.org/0000-0003-3787-1910\",\"givenName\":\"Dennis\",\"familyName\":\"Klein\"},{\"@type\":\"Person\",\"givenName\":\"Thorsten\",\"familyName\":\"Kollegger\"},{\"@type\":\"Person\",\"@id\":\"https://orcid.org/0000-0002-6249-155X\",\"givenName\":\"Alexey\",\"familyName\":\"Rybalchenko\"},{\"@type\":\"Person\",\"givenName\":\"Nicolas\",\"familyName\":\"Winckler\"}],\"contributor\":[{\"@type\":\"Person\",\"givenName\":\"Laurent\",\"familyName\":\"Aphecetche\"},{\"@type\":\"Person\",\"givenName\":\"Sebastien\",\"familyName\":\"Binet\"},{\"@type\":\"Person\",\"givenName\":\"Giulio\",\"familyName\":\"Eulisse\"},{\"@type\":\"Person\",\"givenName\":\"Radoslaw\",\"familyName\":\"Karabowicz\"},{\"@type\":\"Person\",\"givenName\":\"Matthias\",\"familyName\":\"Kretz\",\"email\":\"kretz@kde.org\"},{\"@type\":\"Person\",\"givenName\":\"Mikolaj\",\"familyName\":\"Krzewicki\"},{\"@type\":\"Person\",\"givenName\":\"Andrey\",\"familyName\":\"Lebedev\"},{\"@type\":\"Person\",\"givenName\":\"Teo\",\"familyName\":\"Mrnjavac\",\"email\":\"teo.m@cern.ch\"},{\"@type\":\"Person\",\"givenName\":\"Gvozden\",\"familyName\":\"Neskovic\"},{\"@type\":\"Person\",\"givenName\":\"Matthias\",\"familyName\":\"Richter\"},{\"@type\":\"Person\",\"@id\":\"https://orcid.org/0000-0002-5321-8404\",\"givenName\":\"Christian\",\"familyName\":\"Tacke\"},{\"@type\":\"Person\",\"givenName\":\"Florian\",\"familyName\":\"Uhlig\"},{\"@type\":\"Person\",\"givenName\":\"Sandro\",\"familyName\":\"Wenzel\"}]}\n",
                "readme": "<!-- {#mainpage} --> # FairMQ [![license](https://alfa-ci.gsi.de/shields/badge/license-LGPL--3.0-orange.svg)](COPYRIGHT) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1689985.svg)](https://doi.org/10.5281/zenodo.1689985) [![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/6915/badge)](https://bestpractices.coreinfrastructure.org/projects/6915) [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8B%20%20%E2%97%8F%20%20%E2%97%8F-yellow)](https://github.com/FairRootGroup/FairMQ/actions/workflows/fair-software.yml) [![Spack package](https://repology.org/badge/version-for-repo/spack/fairmq.svg)](https://repology.org/project/fairmq/versions) C++ Message Queuing Library and Framework Docs: [Book](https://github.com/FairRootGroup/FairMQ/blob/dev/README.md#documentation) Find all FairMQ releases [here](https://github.com/FairRootGroup/FairMQ/releases). ## Introduction FairMQ is designed to help implementing large-scale data processing workflows needed in next-generation Particle Physics experiments. FairMQ is written in C++ and aims to * provide **an asynchronous message passing abstraction** of different data transport technologies, * provide a reasonably **efficient data transport** service (zero-copy, high throughput), * be **data format agnostic**, and * provide **basic building blocks** that can be used to implement higher level data processing workflows. The core of FairMQ provides an abstract asynchronous message passing API with scalability protocols inspired by [ZeroMQ](https://github.com/zeromq/libzmq) (e.g. PUSH/PULL, PUB/SUB). FairMQ provides multiple implementations for its API (so-called \"transports\", e.g. `zeromq` and `shmem` (latest release of the `ofi` transport in v1.4.56, removed since v1.5+)) to cover a variety of use cases (e.g. inter-thread, inter-process, inter-node communication) and machines (e.g. Ethernet, Infiniband). In addition to this core functionality FairMQ provides a framework for creating \"devices\" - actors which are communicating through message passing. FairMQ does not only allow the user to use different transport but also to mix them; i.e: A Device can communicate using different transport on different channels at the same time. Device execution is modelled as a simple state machine that shapes the integration points for the user task. Devices also incorporate a plugin system for runtime configuration and control. Next to the provided [devices](https://github.com/FairRootGroup/FairMQ/tree/master/fairmq/devices) and [plugins](https://github.com/FairRootGroup/FairMQ/tree/master/fairmq/plugins) the user can extend FairMQ by developing his own plugins to integrate his devices with external configuration and control services. FairMQ has been developed in the context of its mother project [FairRoot](https://github.com/FairRootGroup/FairRoot) - a simulation, reconstruction and analysis framework. ## Installation from Source Recommended: ```bash git clone https://github.com/FairRootGroup/FairMQ fairmq_source cmake -S fairmq_source -B fairmq_build -GNinja -DCMAKE_BUILD_TYPE=Release [-DBUILD_TESTING=ON] cmake --build fairmq_build [ctest --test-dir fairmq_build --output-on-failure --schedule-random -j<ncpus>] # needs -DBUILD_TESTING=ON cmake --install fairmq_build --prefix $(pwd)/fairmq_install ``` Please consult the [manpages of your CMake version](https://cmake.org/cmake/help/latest/manual/cmake.1.html) for more options. If dependencies are not installed in standard system directories, you can hint the installation location via `-DCMAKE_PREFIX_PATH=...` or per dependency via `-D{DEPENDENCY}_ROOT=...` (`*_ROOT` variables can also be environment variables). ## Installation via Spack Prerequisite: [Spack](https://spack.readthedocs.io/en/latest/getting_started.html) ```bash spack info fairmq # inspect build options spack install fairmq # build latest packaged version with default options ``` Build FairMQ's dependencies via Spack for development: ```bash git clone -b dev https://github.com/FairRootGroup/FairMQ fairmq_source spack --env fairmq_source install # installs deps declared in fairmq_source/spack.yaml spack env activate fairmq_source # sets $CMAKE_PREFIX_PATH which is used by CMake to find FairMQ's deps cmake -S fairmq_source -B fairmq_build -GNinja -DCMAKE_BUILD_TYPE=Debug -DBUILD_TESTING=ON # develop, compile, test spack env deactivate # at end of dev session, or simply close the shell ``` ## Usage FairMQ ships as a CMake package, so in your `CMakeLists.txt` you can discover it like this: ```cmake find_package(FairCMakeModules 1.0 REQUIRED) include(FairFindPackage2) find_package2(FairMQ) find_package2_implicit_dependencies() ``` The [`FairFindPackage2` module](https://fairrootgroup.github.io/FairCMakeModules/latest/module/FairFindPackage2.html) is part of the [`FairCMakeModules` package](https://fairrootgroup.github.io/FairCMakeModules). If FairMQ is not installed in system directories, you can hint the installation: ```cmake list(PREPEND CMAKE_PREFIX_PATH /path/to/fairmq_install) ``` ## Dependencies * [Boost](https://www.boost.org/) * [CMake](https://cmake.org/) * [Doxygen](http://www.doxygen.org/) * [FairCMakeModules](https://github.com/FairRootGroup/FairCMakeModules) (optionally bundled) * [FairLogger](https://github.com/FairRootGroup/FairLogger) * [GTest](https://github.com/google/googletest) (optionally bundled) * [ZeroMQ](http://zeromq.org/) Which dependencies are required depends on which components are built. Supported platform is Linux. macOS is supported on a best-effort basis. ## CMake options On command line: * `-DDISABLE_COLOR=ON` disables coloured console output. * `-DBUILD_TESTING=OFF` disables building of tests. * `-DBUILD_EXAMPLES=OFF` disables building of examples. * `-DBUILD_DOCS=ON` enables building of API docs. * `-DFAIRMQ_CHANNEL_DEFAULT_AUTOBIND=OFF` disable channel `autoBind` by default * You can hint non-system installations for dependent packages, see the #installation-from-source section above After the `find_package(FairMQ)` call the following CMake variables are defined: | Variable | Info | | --- | --- | | `${FairMQ_PACKAGE_DEPENDENCIES}` | the list of public package dependencies | | `${FairMQ_<dep>_VERSION}` | the minimum `<dep>` version FairMQ requires | | `${FairMQ_<dep>_COMPONENTS}` | the list of `<dep>` components FairMQ depends on | | `${FairMQ_PACKAGE_COMPONENTS}` | the list of components FairMQ consists of | | `${FairMQ_#COMPONENT#_FOUND}` | `TRUE` if this component was built | | `${FairMQ_VERSION}` | the version in format `MAJOR.MINOR.PATCH` | | `${FairMQ_GIT_VERSION}` | the version in the format returned by `git describe --tags --dirty --match \"v*\"` | | `${FairMQ_PREFIX}` | the actual installation prefix | | `${FairMQ_BINDIR}` | the installation bin directory | | `${FairMQ_INCDIR}` | the installation include directory | | `${FairMQ_LIBDIR}` | the installation lib directory | | `${FairMQ_DATADIR}` | the installation data directory (`../share/fairmq`) | | `${FairMQ_CMAKEMODDIR}` | the installation directory of shipped CMake find modules | | `${FairMQ_BUILD_TYPE}` | the value of `CMAKE_BUILD_TYPE` at build-time | | `${FairMQ_CXX_FLAGS}` | the values of `CMAKE_CXX_FLAGS` and `CMAKE_CXX_FLAGS_${CMAKE_BUILD_TYPE}` at build-time | ## Documentation 1. [Device](docs/Device.md#1-device) 1. [Topology](docs/Device.md#11-topology) 2. [Communication Patterns](docs/Device.md#12-communication-patterns) 3. [State Machine](docs/Device.md#13-state-machine) 4. [Multiple devices in the same process](docs/Device.md#15-multiple-devices-in-the-same-process) 2. [Transport Interface](docs/Transport.md#2-transport-interface) 1. [Message](docs/Transport.md#21-message) 1. [Ownership](docs/Transport.md#211-ownership) 2. [Channel](docs/Transport.md#22-channel) 3. [Poller](docs/Transport.md#23-poller) 3. [Configuration](docs/Configuration.md#3-configuration) 1. [Device Configuration](docs/Configuration.md#31-device-configuration) 2. [Communication Channels Configuration](docs/Configuration.md#32-communication-channels-configuration) 1. [JSON Parser](docs/Configuration.md#321-json-parser) 2. [SuboptParser](docs/Configuration.md#322-suboptparser) 3. [Introspection](docs/Configuration.md#33-introspection) 4. [Development](docs/Development.md#4-development) 1. [Testing](docs/Development.md#41-testing) 2. [Static Analysis](docs/Development.md#42-static-analysis) 1. [CMake Integration](docs/Development.md#421-cmake-integration) 2. [Extra Compiler Arguments](docs/Development.md#422-extra-compiler-arguments) 5. [Logging](docs/Logging.md#5-logging) 1. [Log severity](docs/Logging.md#51-log-severity) 2. [Log verbosity](docs/Logging.md#52-log-verbosity) 3. [Color for console output](docs/Logging.md#53-color) 4. [File output](docs/Logging.md#54-file-output) 5. [Custom sinks](docs/Logging.md#55-custom-sinks) 6. [Examples](docs/Examples.md#6-examples) 7. [Plugins](docs/Plugins.md#7-plugins) 1. [Usage](docs/Plugins.md#71-usage) 2. [Development](docs/Plugins.md#72-development) 3. [Provided Plugins](docs/Plugins.md#73-provided-plugins) 1. [PMIx](docs/Plugins.md#731-pmix)\n",
                "dependencies": "################################################################################ # Copyright (C) 2018-2024 GSI Helmholtzzentrum fuer Schwerionenforschung GmbH # # # # This software is distributed under the terms of the # # GNU Lesser General Public Licence (LGPL) version 3, # # copied verbatim in the file \"LICENSE\" # ################################################################################ # Project ###################################################################### cmake_minimum_required(VERSION 3.15...3.30 FATAL_ERROR) list(PREPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/cmake) include(GitHelper) get_git_version() project(FairMQ VERSION ${PROJECT_VERSION} LANGUAGES CXX) include(FairMQProjectSettings) ################################################################################ # Build options ################################################################ include(FairMQBuildOption) fairmq_build_option(BUILD_FAIRMQ \"Build FairMQ library and devices.\" DEFAULT ON) fairmq_build_option(BUILD_TESTING \"Build tests.\" DEFAULT OFF REQUIRES \"BUILD_FAIRMQ\") fairmq_build_option(BUILD_EXAMPLES \"Build FairMQ examples.\" DEFAULT ON REQUIRES \"BUILD_FAIRMQ\") fairmq_build_option(BUILD_TIDY_TOOL \"Build the fairmq-tidy tool.\" DEFAULT OFF) fairmq_build_option(BUILD_DOCS \"Build FairMQ documentation.\" DEFAULT OFF) fairmq_build_option(USE_EXTERNAL_GTEST \"Do not use bundled GTest. Not recommended.\" DEFAULT OFF) fairmq_build_option(FAIRMQ_DEBUG_MODE \"Compile in debug mode (may decrease performance).\" DEFAULT OFF) ################################################################################ # Dependencies ################################################################# include(CTest) include(FairMQDependencies) ################################################################################ # Targets ###################################################################### if(BUILD_FAIRMQ) add_subdirectory(fairmq) endif() if(BUILD_TESTING) add_subdirectory(test) endif() if(BUILD_EXAMPLES) add_subdirectory(examples) endif() if(BUILD_DOCS) set(DOXYGEN_OUTPUT_DIRECTORY doxygen) set(DOXYGEN_PROJECT_NUMBER ${PROJECT_GIT_VERSION}) set(DOXYGEN_PROJECT_BRIEF \"C++ Message Queuing Library and Framework\") set(DOXYGEN_USE_MDFILE_AS_MAINPAGE README.md) set(DOXYGEN_HTML_FOOTER docs/footer.html) doxygen_add_docs(doxygen README.md fairmq) add_custom_target(docs ALL DEPENDS doxygen) endif() if(BUILD_TIDY_TOOL) add_subdirectory(fairmq/tidy) endif() ################################################################################ # Package components ########################################################### if(BUILD_FAIRMQ) list(APPEND PROJECT_PACKAGE_COMPONENTS fairmq) endif() if(BUILD_TESTING) list(APPEND PROJECT_PACKAGE_COMPONENTS tests) endif() if(BUILD_EXAMPLES) list(APPEND PROJECT_PACKAGE_COMPONENTS examples) endif() if(BUILD_DOCS) list(APPEND PROJECT_PACKAGE_COMPONENTS docs) endif() if(BUILD_TIDY_TOOL) list(APPEND PROJECT_PACKAGE_COMPONENTS tidy_tool) endif() ################################################################################ # Installation ################################################################# if(BUILD_FAIRMQ) install(FILES cmake/FindZeroMQ.cmake DESTINATION ${PROJECT_INSTALL_CMAKEMODDIR} ) endif() if(BUILD_DOCS) install(DIRECTORY ${CMAKE_BINARY_DIR}/doxygen/html DESTINATION ${PROJECT_INSTALL_DATADIR}/docs ) endif() if(BUILD_TIDY_TOOL) install(FILES cmake/FairMQTidy.cmake DESTINATION ${PROJECT_INSTALL_CMAKEMODDIR} ) endif() include(FairMQPackage) install_cmake_package() ################################################################################ # Summary ###################################################################### include(FairMQSummary) message(STATUS \"${BWhite}${PROJECT_NAME}${CR} ${PROJECT_GIT_VERSION} from ${PROJECT_DATE}\") fair_summary_global_cxx_flags_standard() fair_summary_build_types() fair_summary_package_dependencies() fairmq_summary_components() fairmq_summary_static_analysis() fairmq_summary_install_prefix() fairmq_summary_debug_mode() fairmq_summary_compile_definitions() message(STATUS \" \") ################################################################################\nspack: specs: - boost+container+program_options+filesystem+date_time+regex - faircmakemodules - fairlogger+pretty - fmt - libzmq view: true concretizer: unify: true\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fame",
            "repo_link": "https://gitlab.com/fame-framework/fame-io",
            "content": {
                "codemeta": "",
                "readme": "<!-- SPDX-FileCopyrightText: 2025 German Aerospace Center <fame@dlr.de> SPDX-License-Identifier: Apache-2.0 --> | | | |---------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | **Package** | ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/fameio) ![PyPI - Version](https://img.shields.io/pypi/v/fameio) [![PyPI license](https://img.shields.io/pypi/l/fameio.svg)](https://badge.fury.io/py/fameio) [![REUSE status](https://api.reuse.software/badge/gitlab.com/fame-framework/fame-io)](https://api.reuse.software/info/gitlab.com/fame-framework/fame-io) | | **Test** | [![pipeline status](https://gitlab.com/fame-framework/fame-io/badges/main/pipeline.svg)](https://gitlab.com/fame-framework/fame-io/commits/main) [![coverage report](https://gitlab.com/fame-framework/fame-io/badges/main/coverage.svg)](https://gitlab.com/fame-framework/fame-io/-/commits/main) | | **Activity** | ![GitLab last commit](https://img.shields.io/gitlab/last-commit/fame-framework%2Ffame-io) ![GitLab closed issues by-label](https://img.shields.io/gitlab/issues/closed/fame-framework%2Ffame-io) | | **Style** | [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![log style - common changelog](https://img.shields.io/badge/log_style-common_changelog-blue)](https://common-changelog.org/) ![Static Badge](https://img.shields.io/badge/type%20checked-mypy-039dfc) [![linting: pylint](https://img.shields.io/badge/linting-pylint-green)](https://github.com/pylint-dev/pylint) | | **Reference** | [![JOSS](https://joss.theoj.org/papers/10.21105/joss.04958/status.svg)](https://doi.org/10.21105/joss.04958) [![Zenodo](https://zenodo.org/badge/DOI/10.5281/zenodo.4314337.svg)](https://doi.org/10.5281/zenodo.4314337) | # FAME-Io *Tools for input preparation and output digestion of FAME models* FAME-Io compiles input for FAME models in protobuf format and extracts model outputs to human-readable files. Please visit the [FAME-Wiki](https://gitlab.com/fame-framework/wiki/-/wikis/home) to get an explanation of FAME and its components. # Installation We recommend installing `fameio` using PyPI: pip install fameio You may also use `pipx`. For detailed information please refer to the official `pipx` [documentation](https://github.com/pypa/pipx). pipx install fameio `fameio` is currently developed and tested for Python 3.8 or higher. See the `pyproject.toml` for a complete listing of dependencies. # Usage FAME-Io currently offers two main scripts `makeFameRunConfig` and `convertFameResults`. Both are automatically installed with the package. The first one creates a protobuf file for FAME applications using YAML definition files and CSV files. The latter one reads output files from FAME applications in protobuf format and converts them to CSV files. You may use the [example data](https://gitlab.com/dlr-ve/esy/amiris/examples) provided for the [AMIRIS](https://gitlab.com/dlr-ve/esy/amiris/amiris) model which can be used to simulate electricity markets in [Germany](https://gitlab.com/dlr-ve/esy/amiris/examples/-/tree/main/Germany2019), [Austria](https://gitlab.com/dlr-ve/esy/amiris/examples/-/tree/main/Austria2019), and a simple [proof-of-concept model](https://gitlab.com/dlr-ve/esy/amiris/examples/-/tree/main/Simple). ## Make a FAME run configuration Digests configuration files in YAML format, combines them with CSV data files and creates a single input file for FAME applications in protobuf format. Call structure: makeFameRunConfig -f <path/to/scenario.yaml> You may also specify any of the following arguments: | Command | Action | |------------------------|------------------------------------------------------------------------------------------------------------------------------------------| | `-l` or `--log` | Sets the logging level. Default is `info`. Options are `debug`, `info`, `warning`, `warn`, `error`, `critical`. | | `-lf` or `--logfile` | Sets the logging file. Default is `None`. If `None` is provided, all logs get only printed to the console. | | `-o` or `--output` | Sets the path of the compiled protobuf output file. Default is `config.pb`. | | `-enc` or `--encoding` | Sets the encoding of all yaml files to the given one (e.g. 'utf8' or 'cp1252'. Default is `None`, i.e. your operating system's standard. | This could look as follows: makeFameRunConfig -f <path/to/scenario.yaml> -l debug -lf <path/to/scenario.log> -o <path/to/config.pb> You may also call the configuration builder from any Python script with ```python from fameio.scripts.make_config import Options, run as make_config make_config({Options.FILE: \"path/to/scenario.yaml\", }) ``` Similar to the console call you may also specify custom run config arguments and add it in a dictionary to the function call. ```python from fameio.scripts.make_config import Options, run as make_config run_config = {Options.FILE: \"path/to/scenario.yaml\", Options.LOG_LEVEL: \"info\", Options.OUTPUT: \"output.pb\", Options.LOG_FILE: \"scenario.log\", } make_config(run_config) ``` You can also use the associated argument parser, to extract the run_config dynamically from a string: ```python from fameio.scripts.make_config import Options, run as make_config from fameio.cli.make_config import handle_args my_defaults = {Options.FILE: \"path/to/scenario.yaml\", Options.LOG_LEVEL: \"info\", Options.OUTPUT: \"output.pb\", Options.LOG_FILE: \"scenario.log\", } my_arg_string = ['-f', 'my/other/scenario.yaml', '-l', 'error'] run_config = handle_args(my_arg_string, my_defaults) make_config(run_config) ``` ### Scenario YAML The \"scenario.yaml\" file contains all configuration options for a FAME-based simulation. It consists of the sections `Schema`, `GeneralProperties`, `Agents` and `Contracts`, and the optional section `StringSets`. All of them are described below. #### Schema The Schema describes a model's components such as its types of agents, their inputs, what data they exchange, etc. It is also used to validate the model inputs provided in the `scenario.yaml`. Since the Schema is valid until the model itself is changed, it is recommended to defined it in a separate file and include the file here. Currently, the schema specifies: * which type of Agents can be created * what type of input attributes an Agent uses * what type of Products an Agent can send in Contracts, and * the names of the Java packages for the classes corresponding to Agents, DataItems and Portables. The Schema consists of the sections `JavaPackages` and `AgentTypes`. ##### JavaPackages This section defines the name of the Java packages in which the model code is located. A similar data set was formerly specified in the `fameSetup.yaml`, but is now specified in the schema. Each of the three sections `Agents`, `DataItems`, and `Portables` contain a list of fully qualified java package names of your model's classes. Package names can occur in multiple lists and may overlap. It is not necessary (but possible) to specify the nearest enclosing package for each Agent, DataItem or Portable. Specifying any super-package will also work. Also, package names occur on multiple lists for Agent, DataItem or Portable. For example, for a project with all its * Agent-derived java classes located in packages below the package named \"agents\", * DataItem implementation classes in a subpackage named \"msg\", * Portable implementation classes in a subpackages named \"portableItems\" and \"otherPortables\", the corresponding section in the schema would look like this: ```yaml JavaPackages: Agents: - \"agents\" DataItems: - \"msg\" Portables: - \"portableItems\" - \"otherPortables\" ``` One can leave out the `DataItems` specifications, but `Agents` and `Portables` are required and must not be empty. ##### AgentTypes Here, each type of agent that can be created in your FAME-based application is listed, its attributes and its available Products for Contracts. The structure of this section ```yaml AgentTypes: MyAgentType: Attributes: MyAttribute: ... MyOtherAttribute: ... Products: [ 'Product1', 'Product2', 'Product3' ] Outputs: [ 'Column1', 'Column2', 'Column3' ] Metadata: Some: \"Dict with Metadata that you would like to add\" MyOtherAgentWithoutProductsOrAttributes: ``` * `MyAgentType` Java's simple class name of the Agent type * `Attributes` indicates that beginning of the attribute definition section for this Agent type * `MyAttribute` Name of an attribute as specified in the corresponding Java source code of this Agent type (annotated with \"@Input\") * `MyOtherAttribute` Name of another attribute derived from Java source code * `Products` list or dictionary of Products that this Agent can send in Contracts; derived from Java source code of this Agent type (annotated with \"@Product\") * `Outputs` list or dictionary of Output columns that this Agent can write to; derived from Java source code of this Agent type (annotated with \"@Output\") * `Metadata` dictionary with any content that is assigned to this Agent type as additional information * `MyOtherAgentWithoutProductsOrAttributes` an Agent type that requires neither Attributes nor Products Attributes, Products, Outputs and Metadata are optional - there may be useful Agents that require none of them. Products and Outputs can both be lists of Strings, or dictionaries with additional Metadata. For example, you could write the above in the following way: ```yaml Products: Product1: Metadata: Any: \"information you would like to add to Product1 using a dictionary form\" Product2: Product3: Outputs: Column1: Column2: ThisEntry: \"is ignored, as it is not below the keyword: 'Metadata'\" Metadata: My: \"Metadata\" That: \"will be saved to Column2\" Column3: ``` Here, \"Product1\" and \"Column2\" have additional, optional Metadata assigned to them (using the keyword \"Metadata\"). The other Products and Columns have no metadata assigned to them - which is also ok. In the AgentType definition example above attribute definition was not shown explicitly (indicated by `...`). The next example provides details on how to define an attribute: ```yaml MySimpleAttribute: AttributeType: enum Mandatory: true List: false Values: [ 'AllowedValue1', 'AllowedValue2' ] Default: 'AllowedValue1' Help: 'My help text' Metadata: Go: \"here\" MyComplexAttribute: AttributeType: block NestedAttributes: InnerAttributeA: AttributeType: integer Values: 1: Metadata: Explain: \"1 is a allowed value\" 2: Metadata: Comment: \"2 is also allowed, but consider using 1\" InnerAttributeB: AttributeType: double ``` * `MySimpleAttribute`, `MyDoubleList`, `MyComplexAttribute` Names of the attributes as specified in the Java enum annotated with \"@Input\" * `AttributeType` (required) data type of the attribute; see options in table below * `Mandatory` (optional - true by default) if true: the attribute is required for this agent and validation will fail if the attribute is missing in the scenario **and** no default is provided * `List` (optional - false by default) * `AttributeType: time_series` cannot be true * `AttributeType: block` * if true: any nested element in the scenario must be part of a list element and thus can appear multiple times * if false: any nested element in the scenario can only appear once * any other AttributeType: the attribute is interpreted as list, i.e. multiple values can be assigned to this attribute in the scenario * `NestedAttributes` (required only if `AttributeType: block`, otherwise disallowed) starts an inner Attribute definition block - defined Attributes are sub-elements of `MyComplexAttribute` * `Values` (optional - None by default): * if present, defines a list or dictionary of allowed values for this attribute * if a dictionary is used, individual Metadata can be assigned to each allowed value using the `Metadata` keyword * `Default` (optional - None by default): * if present, defines a default value to be used if the scenario does not specify one * must match one of the entries in `Values` in case those are defined * can be a list if the attribute is a list * `Help` (optional - None by default): if present, defines a help text for your Attribute * `Metadata` (optional - None by default): if present, defines additional metadata assigned to the Attribute | AttributeType | value | |---------------|-------------------------------------------------------------------------------------------------------------------------| | `integer` | a 32-bit integer value | | `double` | a 64-bit floating-point value (integers also allowed) | | `long` | a 64-bit integer value | | `time_stamp` | either a FAME time stamp string or 64-bit integer value | | `string` | any string | | `string_set` | a string from a set of allowed `Values` defined in `StringSet` section in `scenario` | | `enum` | a string from a set of allowed `Values` defined in `schema` | | `time_series` | either a path to a .csv-file or a single 64-bit floating-point value; does not support `List: true` | | `block` | this attribute has no value of its own but hosts a group of nested Attributes; implies `NestedAttributes` to be defined | #### GeneralProperties Specifies FAME-specific properties of the simulation. Structure: ```yaml GeneralProperties: RunId: 1 Simulation: StartTime: 2011-12-31_23:58:00 StopTime: 2012-12-30_23:58:00 RandomSeed: 1 ``` Parameters: * `RunId` an ID that can be given to the simulation; use at your discretion * `StartTime` time stamp in the format YYYY-MM-DD_hh:mm:ss; first moment of the simulation. * `StopTime` time stamp in the format YYYY-MM-DD_hh:mm:ss; last moment of the simulation - i.e. simulation terminates after passing that time stamp * `RandomSeed` seed to initialise random number generation; each value leads to a unique series of random numbers. #### Agents Specifies all Agents to be created in the simulation in a list. Each Agent has its own entry. Structure: ```yaml Agents: - Type: MyAgentWithInputs Id: 1 Attributes: MyEnum: SAME_SHARES MyInteger: 2 MyDouble: 4.2 MyTimeSeries: \"./path/to/time_series.csv\" Metadata: Can: \"also be assigned\" - Type: MyAgentWithoutInputs Id: 2 ``` Agent Parameters: * `Type` Mandatory; Java's simple class name of the agent to be created * `Id` Mandatory; simulation-unique id of this agent; if two agents have the same ID, the configuration process will stop. * `Attributes` Optional; if the agent has any attributes, specify them here in the format \"AttributeName: value\"; please see attribute table above * `Metadata` Optional; can be assigned to each instance of an Agent, as well as to each of its Attributes * `Ext` Optional; Reserved key for parameters not used by fameio but its extensions, e.g., FAME-Gui A warning is logged for any other key at this level. The specified `Attributes` for each agent must match the specified `Attributes` options in the linked Schema (see above). For better structure and readability of the `scenario.yaml`, `Attributes` may also be specified in a nested way as demonstrated below. ```yaml Agents: - Type: MyAgentWithInputs Id: 1 Attributes: Parent: MyEnum: SAME_SHARES MyInteger: 2 Parent2: MyDouble: 4.2 Child: MyTimeSeries: \"./path/to/time_series.csv\" ``` In case Attributes are defined with `List: true` option, lists are assigned to an Attribute or Group: ```yaml Attributes: MyDoubleList: [ 5.2, 4.5, 7, 9.9 ] MyListGroup: - IntValueA: 5 IntValueB: 42 - IntValueA: 7 IntValueB: 100 ``` Here, `MyDoubleList` and `MyListGroup` need to specify `List: true` in the corresponding Schema. The shorter `[]`-notation was used to assign a list of floating-point values to `MyDoubleList`. Nested items `IntValueA` and `IntValueB` of `MyListGroup` are assigned within a list, allowing the specification of these nested items several times. ##### Attribute Metadata Metadata can be assigned to any value, list item, or superstructure. To assign Metadata to a primitive value, create a dictionary from it, set the actual value with the inner keyword `Value` and add the keyword `Metadata` like this: ```yaml ValueWithoutMetadata: 1 SameValueWithMetadata: Value: 1 Metadata: # describe `SameValueWithMetadata` herein ``` You can assign Metadata to a list of primitive values using the keyword `Values` like this: ```yaml ValueListWithoutMetadata: [ 1,2,3 ] SameValueListWithListMetadata: Values: [ 1,2,3 ] Metadata: # describe the whole list of values with Metadata here ``` or specify Metadata for each (or just some) value individually, like this: ```yaml ValueListWithoutMetadata: [ 1,2,3 ] SameValueListWithMetadataAtEachElement: - Value: 1 Metadata: # describe this specific value \"1\" with Metadata here - Value: 2 # this value has no Metadata attached, but you can still use the keyword `Value` - 3 # or use in the actual directly since this value has no Metadata anyway ``` or assign Metadata to both the list and any of its list entries, like this: ```yaml ValueListWithoutMetadata: [ 1,2,3 ] SameValueListWithAllMetadata: Metadata: # Recommendation: place the Metadata of the list first if the list of values is extensive, as in this case Values: - Value: 1 Metadata: # describe this specific value \"1\" with Metadata here - Value: 2 Metadata: # describe this specific value \"2\" with Metadata here - Value: 3 Metadata: # describe this specific value \"3\" with Metadata here ``` You can assign Metadata directly to a nested element by adding the Metadata keyword: ```yaml NestedItemWithoutMetadata: A: 1 B: 2 SameNestedItemWithMetadata: A: 1 B: 2 Metadata: # These Metadata describe `SameNestedItemWithMetadata` ``` Similar to lists of values, you can assign Metadata to a list of nested elements using the `Values` keyword, like this: ```yaml ListOfNestedItemsWithoutMetadata: - A: 1 B: 10 - A: 2 B: 20 SameListOfNestedItemsWithGeneralMetadata: Values: - A: 1 B: 10 - A: 2 B: 20 Metadata: # These Metadata describe `SameListOfNestedItemsWithGeneralMetadata` as a whole ``` and, similar to nested elements, you can assign Metadata directly to any list element, like this: ```yaml ListOfNestedItemsWithoutMetadata: - A: 1 B: 10 - A: 2 B: 20 SameListOfNestedItemsWithGeneralMetadata: - A: 1 B: 10 Metadata: # These Metadata describe the first list item - A: 2 B: 20 Metadata: # These Metadata describe the second list item ``` Again, you may apply both variants and apply Metadata to the list and each of its items if you wish. #### Contracts Specifies all Contracts, i.e. repetitive bilateral transactions in between agents. Contracts are given as a list. We recommend moving Contracts to separate files and to use the `!include` command to integrate them in the scenario. ```yaml Contracts: - SenderId: 1 ReceiverId: 2 ProductName: ProductOfAgent_1 FirstDeliveryTime: -25 DeliveryIntervalInSteps: 3600 Metadata: Some: \"additional information can go here\" - SenderId: 2 ReceiverId: 1 ProductName: ProductOfAgent_2 FirstDeliveryTime: -22 DeliveryIntervalInSteps: 3600 Attributes: ProductAppendix: value TimeOffset: 42 ``` Contract Parameters: * `SenderId` unique ID of agent sending the product * `ReceiverId` unique ID of agent receiving the product * `ProductName` name of the product to be sent * `FirstDeliveryTime` first time of delivery in the format \"seconds after the January 1st 2000, 00:00:00\" * `DeliveryIntervalInSteps` delay time in between deliveries in seconds * `Metadata` can be assigned to add further helpful information about a Contract * `Attributes` can be set to include additional information as `int`, `float`, `enum`, or `dict` data types ##### Definition of Multiple Similar Contracts Often, scenarios contain multiple agents of similar type that also have similar chains of contracts. Therefore, FAME-Io supports a compact definition of multiple similar contracts. `SenderId` and `ReceiverId` can both be lists and support One-to-N, N-to-One and N-to-N relations like in the following example: ```yaml Contracts: # effectively 3 similar contracts (0 -> 11), (0 -> 12), (0 -> 13) # with otherwise identical ProductName, FirstDeliveryTime & DeliveryIntervalInSteps - SenderId: 0 ReceiverId: [ 11, 12, 13 ] ProductName: MyOtherProduct FirstDeliveryTime: 100 DeliveryIntervalInSteps: 3600 # effectively 3 similar contracts (1 -> 10), (2 -> 10), (3 -> 10) # with otherwise identical ProductName, FirstDeliveryTime & DeliveryIntervalInSteps - SenderId: [ 1, 2, 3 ] ReceiverId: 10 ProductName: MyProduct FirstDeliveryTime: 100 DeliveryIntervalInSteps: 3600 # effectively 3 similar contracts (1 -> 11), (2 -> 12), (3 -> 13) # with otherwise identical ProductName, FirstDeliveryTime & DeliveryIntervalInSteps - SenderId: [ 1, 2, 3 ] ReceiverId: [ 11, 12, 13 ] ProductName: MyThirdProduct FirstDeliveryTime: 100 DeliveryIntervalInSteps: 3600 ``` Combined with YAML anchors complex contract chains can be easily reduced to a minimum of required configuration. The following example is equivalent to the previous one and allows a quick extension of contracts to a new couple of agents e.g. (4;14): ```yaml Groups: - &agentList1: [ 1,2,3 ] - &agentList2: [ 11,12,13 ] Contracts: - SenderId: 0 ReceiverId: *agentList2 ProductName: MyOtherProduct FirstDeliveryTime: 100 DeliveryIntervalInSteps: 3600 - SenderId: *agentList1 ReceiverId: 10 ProductName: MyProduct FirstDeliveryTime: 100 DeliveryIntervalInSteps: 3600 - SenderId: *agentList1 ReceiverId: *agentList2 ProductName: MyThirdProduct FirstDeliveryTime: 100 DeliveryIntervalInSteps: 3600 ``` #### StringSets This optional section defines values of type `string_set`. In contrast to `enum` values, which are **statically** defined in the `Schema`, `string_set` values can be **dynamically ** defined in this section. If an agent attribute is of type `string_set` and the attribute is set in the `scenario`, then 1. the section `StringSets` in the `scenario` must contain an entry named exactly like the attribute, and 2. the attribute value must be contained in the string set's `Values` declaration. For instance: In `schema`: ``` yaml AgentTypes: FuelsMarket: Attributes: FuelType: AttributeType: string_set ``` In `scenario`: ``` yaml StringSets: FuelType: Values: ['OIL', 'HARD_COAL', 'LIGNITE'] Agents: - Type: FuelsMarket Id: 1 Attributes: FuelType: OIL ``` Important: If different types of Agents shall refer to the same StringSet, their attributes in schema must have the **exact** same name. ### CSV files TIME_SERIES inputs are not directly fed into the Scenario YAML file. Instead, TIME_SERIES reference a CSV file that can be stored some place else. These CSV files follow a specific structure: * They should contain exactly two columns - any other columns are ignored. A warning is raised if more than two non-empty columns are detected. * The first column must be a time stamp in form `YYYY-MM-DD_hh:mm:ss` or a [FAME-Timestamp](https://gitlab.com/fame-framework/wiki/-/wikis/architecture/decisions/TimeStamp) integer value. * The second column must be a numerical value (either integer or floating-point) * The separator of the two columns is a semicolon * The data must **not** have headers, except for comments marked with `#` You may add comments using `#`. Exemplary content of a valid CSV file: # If you want an optional header, you must use a comment 2012-01-01_00:00:00;400 2013-01-01_00:00:00;720.5 2014-01-01_00:00:00;650 2015-01-01_00:00:00;99.27772 2016-01-01_00:00:00;42 # optional comment on this particular data point 2017-01-01_00:00:00;0.1 Please refer also to the detailed article about `TimeStamps` in the [FAME-Wiki](https://gitlab.com/fame-framework/wiki/-/wikis/TimeStamp). For large CSV files (with more than 20,000 rows) we recommend using the integer representation of FAME-Timestamps in the first column (instead of text representation) to improve conversion speed. A warning will be raised for very large files (exceeding 50,000 rows) that require time stamp conversion. ### Split and join multiple YAML files The user may include other YAML files into a YAML file to divide the content across files as convenient. We explicitly recommend using this feature for the `Schema` and `Contracts` sections. Otherwise, the scenario.yaml may become crowded. #### Command: !Include To hint YAML to load the content of another file use `!include \"path/relative/to/including/yaml/file.yml\"`. You can concatenate !include commands and can use !include in the included file as well. The path to the included file is always relative to the file using the !include command. So with the following file structure ###### file-structure ``` a.yaml folder/b.yaml folder/c.yaml folder/deeper_folder/d.yaml ``` the following !include commands work ###### in a.yaml ``` ToBe: !include \"folder/b.yaml\" OrNot: !include \"folder/deeper_folder/d.yaml\" ``` ###### in b.yaml ``` ThatIs: !include \"c.yaml\" TheQuestion: !include \"deeper_folder/d.yaml\" ``` Provided that ###### in c.yaml ``` Or: maybe ``` ###### d.yaml ``` not: \"?\" ``` the resulting file would look like this: ###### THe Joined file a.yaml ``` ToBe: ThatIs: Or: maybe TheQuestion: not: \"?\" OrNot: not: \"?\" ``` You may also specify absolute file paths if preferred by starting with a \"/\". When specifying only a file path, the complete content of the file is assigned to the given key. You always need a key to assign the !include command to. However, you cannot combine the value returned from !include with other values in the same key. Thus, the following combinations do not work: ###### caveats.yml ``` !include \"file.yaml\" # no key assigned Key: Some: OtherItem !include \"file.yaml\" # cannot join with other named items List: - an: entry !include \"file.yaml\" # cannot directly join with list items, even if !include returns a list ``` #### Integrate specific nodes of YAML files Instead of including *all* content in the included file, you may also pick a specific node within that file. For this use `!include [<relative/path/to/file.yaml>, Path:To:Field:In:Yaml]`. Here, `:` is used in the node-specifying string to select a sequence of nodes to follow - with custom depth. Consider the following two files: ###### file_to_be_included.yaml ```yaml Set1: Subset1: Key: Value Set2: OtherKey: OtherValue ``` ###### including_file.yaml ```yaml - Type: MyAgentWithInputs Id: 1 Attributes: !include_node [ file_to_be_included.yaml, Set1:Subset1 ] ``` Compiling \"including_file.yaml\" results in ###### resulting_file.yaml ```yaml - Type: MyAgentWithInputs Id: 1 Attributes: Key: Value ``` #### Load multiple files Using wildcards in the given path (e.g. \"path/to/many/*.yaml\") will lead to loading multiple files and assigning their content to the same key. You can make use of this feature with or without specifying a node selector. However, the elements to be joined across multiple files must be lists. These lists are then concatenated into a single list and then assigned to the key in the file calling !include. This feature is especially useful for Contracts: You can split the Contracts list into several files and place them in a separate folder. Then use !include to re-integrate them into your configuration. An example: ###### my_contract1.yaml ``` Contracts: - ContractA - ContractB ``` ###### my_contract2.yaml ``` Contracts: - ContractC - ContractD - ContractE ``` ###### including_file.yaml ``` Contracts: [!include \"my_contract*.yaml\", \"Contracts\"] ``` results in ###### result.yaml ``` Contracts: - ContractA - ContractB - ContractC - ContractD - ContractE ``` #### Ignoring files Files that have their name start with \"IGNORE_\" are not included with the !include command. You will see a debug output to notify you that the file was ignored. Use this to temporarily take files out ouf your configuration without deleting or moving them. ## Read FAME results Takes an output file in protobuf format of FAME-based applications and converts it into files in CSV format. An individual file for each type of Agent is created in a folder named after the protobuf input file. Call structure: convertFameResults -f <./path/to/protobuf_file.pb> You may also specify any of the following arguments: | Command | Action | |-----------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | `-l` or `--log` <option> | Sets the logging level. Default is `WARNING`. Options are `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`. | | `-lf` or `--logfile` <file> | Sets the logging file. Default is `None`. If `None` is provided, all logs get only printed to the console. | | `-a` or `--agents` <list-of-agents> | If specified, only a subset of agents is extracted from the protobuf file. Default is to extract all agents. | | `-o` or `--output` | Sets the path to where the generated output files are written to. If not specified, the folder's name is derived from the input file's name. Folder will be created if it does not exist. | | `-se` or `--single-export` | Enables export of individual agents to individual files, when present. If not present (the default) one file per `AgentType` is created. | | `-m` or `--memory-saving` | When specified, reduces memory usage profile at the cost of runtime. Use only when necessary. | | `-cc` or `--complex-column` <option> | Defines how to deal with complex indexed output columns (if any). `IGNORE` ignores complex columns. `SPLIT` creates a separate file for each complex indexed output column. | | `-t` or `--time` <option> | Option to define conversion of time steps to given format (default=`UTC`) by `-t/--time {UTC, INT, FAME}` | | `--input-recovery` or `--no-input-recovery` | If True, all input data are recovered in addition to the outputs (default=False). | | `-mt` or `--merge-times` <list-of-parameters> | Option to merge `TimeSteps` of a certain range of steps in the output files to associate multiple time steps with a common logical time in your simulation and reduce number of lines in output files | The option `--merge-times` requires exactly three integer arguments separated by spaces: | Position | Name | Meaning | |----------|--------------|------------------------------------------------------------------------------------------| | First | Focal point | TimeStep on which `steps-before` earlier and `steps-after` later TimeSteps are merged on | | Second | Steps before | Range of TimeSteps before the `focal-point` they get merged to, must be Zero or positive | | Third | Steps after | Range of TimeSteps after the `focal-point` they get merged to, must be Zero or positive | This could look as follows: convertFameResults -f <./path/to/protobuf_file.pb> -l debug -lf <path/to/output.log> -a AgentType1 AgentType2 -o myCsvFolder -m -cc SPLIT --merge-times 0 1799 1800 Make sure that in the range of time steps you specify for merging, there is only one value per column in the merged time range. If multiple values per column are merged values will get concatenated and might yield unexpected results. You may also call the conversion script from any Python script with: ```python from fameio.scripts.convert_results import Options, run as convert_results convert_results({Options.FILE: \"./path/to/protobuf_file.pb\"}) ``` Similar to the console call you may also specify custom run config arguments and add it in a dictionary to the function call. ```python from fameio.scripts.convert_results import Options, run as convert_results run_config = {Options.FILE: \"./path/to/protobuf_file.pb\", Options.LOG_LEVEL: \"info\", Options.LOG_FILE: \"scenario.log\", Options.OUTPUT: \"Output\", Options.AGENT_LIST: ['AgentType1', 'AgentType2'], Options.MEMORY_SAVING: False, Options.SINGLE_AGENT_EXPORT: False, Options.RESOLVE_COMPLEX_FIELD: \"SPLIT\", Options.TIME: \"INT\", Options.TIME_MERGING: {}, } convert_results(run_config) ``` You can also use the associated argument parser, to extract the run_config dynamically from a string: ```python from fameio.scripts.convert_results import Options, run as convert_results from fameio.cli.convert_results import handle_args my_defaults = {Options.FILE: \"./path/to/protobuf_file.pb\", Options.LOG_LEVEL: \"info\", Options.LOG_FILE: \"scenario.log\", Options.OUTPUT: \"Output\", Options.AGENT_LIST: ['AgentType1', 'AgentType2'], Options.MEMORY_SAVING: False, Options.SINGLE_AGENT_EXPORT: False, Options.RESOLVE_COMPLEX_FIELD: \"SPLIT\", Options.TIME: \"INT\", Options.TIME_MERGING: {}, } my_arg_string = ['-f', 'my/other/scenario.yaml', '-l', 'error'] run_config = handle_args(my_arg_string, my_defaults) convert_results(run_config) ``` ## Cite FAME-Io If you use FAME-Io for academic work, please cite as follows. Bibtex entry: ``` @article{fameio2023joss, author = {Felix Nitsch and Christoph Schimeczek and Ulrich Frey and Benjamin Fuchs}, title = {FAME-Io: Configuration tools for complex agent-based simulations}, journal = {Journal of Open Source Software}, year = {2023}, doi = {doi: https://doi.org/10.21105/joss.04958} } ``` ## Available Support This is a purely scientific project by (at the moment) one research group. Thus, there is no paid technical support available. However, we will give our best to answer your questions and provide support. If you experience any trouble with FAME-Io, you may contact the developers via [fame@dlr.de](mailto:fame@dlr.de). Please report bugs and make feature requests by filing issues following the provided templates (see also [Contribute](CONTRIBUTING.md)). For substantial enhancements, we recommend that you contact us via [fame@dlr.de](mailto:fame@dlr.de) for working together on the code in common projects or towards common publications and thus further develop FAME-Io.\n",
                "dependencies": "# SPDX-FileCopyrightText: 2025 German Aerospace Center <fame@dlr.de> # # SPDX-License-Identifier: Apache-2.0 [build-system] requires = [\"poetry-core>=2.0.0\"] build-backend = \"poetry.core.masonry.api\" [project] name = \"fameio\" version = \"3.2.0\" description = \"Tools for input preparation and output digestion of FAME models\" license = \"Apache-2.0\" readme = \"README.md\" requires-python = \">=3.9,<4.0\" authors = [ { name = \"Felix Nitsch\", email = \"fame@dlr.de\" }, { name = \"Christoph Schimeczek\", email = \"fame@dlr.de\" }, { name = \"Ulrich Frey\", email = \"fame@dlr.de\" }, { name = \"Benjamin Fuchs\", email = \"fame@dlr.de\" }, ] maintainers = [ { name = \"Felix Nitsch\", email = \"fame@dlr.de\" }, { name = \"Christoph Schimeczek\", email = \"fame@dlr.de\" }, ] keywords = [\"FAME\", \"fameio\", \"agent-based modelling\", \"energy systems\"] dynamic = [\"classifiers\"] dependencies = [ \"fameprotobuf>=2.0.2,<3.0\", \"pandas>= 1.0,<3.0\", \"pyyaml>=6.0,<7.0\", ] [project.urls] homepage = \"https://helmholtz.software/software/fame\" repository = \"https://gitlab.com/fame-framework/fame-io\" \"Changelog\" = \"https://gitlab.com/fame-framework/fame-io/-/blob/main/CHANGELOG.md\" \"Issue Tracking\" = \"https://gitlab.com/fame-framework/fame-io/-/issues\" [project.scripts] makeFameRunConfig = \"fameio.scripts:makeFameRunConfig\" convertFameResults = \"fameio.scripts:convertFameResults\" [tool.poetry] classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Operating System :: OS Independent\", \"Topic :: Scientific/Engineering\", \"Environment :: Console\", \"Intended Audience :: Science/Research\", ] packages = [{ include = \"fameio\", from = \"src\" }] include = [\"CHANGELOG.md\"] [tool.poetry.group.dev] optional = true [tool.poetry.group.dev.dependencies] pytest = \"^8.3\" mockito = \"^1.5\" pre-commit = \"^3.8\" coverage = { version = \"^7.6\", extras = [\"toml\"] } black = \"^24.8\" prospector = { version = \"^1.16\", extras = [\"with_mypy\"] } types-PyYAML = \"^6.0\" [tool.black] line-length = 120 [tool.coverage.run] branch = true source = [\"fameio\"] omit = [\"./src/fameio/scripts/*\"] command_line = \"-m pytest\" [tool.coverage.report] show_missing = true skip_covered = true skip_empty = true precision = 2 sort = \"Cover\" exclude_also = ['@overload'] [tool.coverage.xml] output = \"coverage.xml\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fastscape-toolbox",
            "repo_link": "https://github.com/fastscape-lem/fastscape",
            "content": {
                "codemeta": "",
                "readme": "Fastscape ========= |Build Status| |Doc Status| |Zenodo| A fast, versatile and user-friendly landscape evolution model. Fastscape is a Python package that provides a lot a small model components (i.e., processes) to use with the xarray-simlab_ modeling framework. Those components can readily be combined together in order to create custom Landscape Evolution Models (LEMs). Routines from the fastscapelib_ library are used for fast model execution. .. |Build Status| image:: https://github.com/fastscape-lem/fastscape/actions/workflows/tests.yml/badge.svg?branch=master :target: https://github.com/fastscape-lem/fastscape/actions/workflows/tests.yml :alt: Build Status .. |Doc Status| image:: https://readthedocs.org/projects/fastscape/badge/?version=latest :target: https://fastscape.readthedocs.io/en/latest/?badge=latest :alt: Documentation Status .. |Zenodo| image:: https://zenodo.org/badge/133702738.svg :target: https://zenodo.org/badge/latestdoi/133702738 :alt: Citation .. _xarray-simlab: https://github.com/benbovy/xarray-simlab .. _fastscapelib: https://github.com/fastscape-lem/fastscapelib-fortran Documentation ------------- Documentation is hosted on ReadTheDocs: https://fastscape.readthedocs.io License ------- 3-clause (\"Modified\" or \"New\") BSD license. See the LICENSE file for details. Acknowledgment -------------- Fastscape is developed at the `Earth Surface Process Modelling`__ group of the GFZ Helmholtz Centre Potsdam. __ http://www.gfz-potsdam.de/en/section/earth-surface-process-modelling/ Citing fastscape ---------------- If you use xarray-simlab in a scientific publication, we would appreciate a `citation`_. .. _`citation`: http://fastscape.readthedocs.io/en/latest/cite.html\n",
                "dependencies": "[build-system] build-backend = \"setuptools.build_meta\" requires = [ \"setuptools>=42\", \"setuptools-scm>=7\", ] [tool.setuptools.packages.find] include = [ \"fastscape\", \"fastscape.*\", ] [tool.setuptools_scm] fallback_version = \"9999\" [project] name = \"fastscape\" dynamic = [\"version\"] authors = [ {name = \"Benoît Bovy\", email = \"benbovy@gmail.com\"}, ] maintainers = [ {name = \"Fastscape contributors\"}, ] license = {text = \"BSD-3-Clause\"} description = \"A fast, versatile and user-friendly landscape evolution model\" keywords = [\"simulation\", \"toolkit\", \"modeling\", \"landscape\", \"geomorphology\"] readme = \"README.rst\" classifiers = [ \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: BSD License\", \"Programming Language :: Python :: 3\", ] requires-python = \">=3.9\" dependencies = [ \"xarray-simlab >= 0.5.0\", \"numba\", ] [project.optional-dependencies] dev = [\"pytest\"] [project.urls] Documentation = \"https://fastscape.readthedocs.io\" Repository = \"https://github.com/fastscape-lem/fastscape\" [tool.black] line-length = 100 [tool.ruff] # E402: module level import not at top of file # E501: line too long - let black worry about that # E731: do not assign a lambda expression, use a def ignore = [ \"E402\", \"E501\", \"E731\", ] select = [ \"F\", # Pyflakes \"E\", # Pycodestyle \"W\", \"I\", # isort \"UP\", # Pyupgrade ] exclude = [\".eggs\", \"doc\"] target-version = \"py39\" [tool.ruff.isort] known-first-party = [\"fastscape\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fastsurfer",
            "repo_link": "https://github.com/deep-MI/FastSurfer/",
            "content": {
                "codemeta": "",
                "readme": "[![DOI](https://zenodo.org/badge/211859022.svg)](https://zenodo.org/badge/latestdoi/211859022) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Deep-MI/FastSurfer/blob/stable/Tutorial/Tutorial_FastSurferCNN_QuickSeg.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Deep-MI/FastSurfer/blob/stable/Tutorial/Complete_FastSurfer_Tutorial.ipynb) <!-- start of content --> # Welcome to FastSurfer! ## Overview This README contains all information needed to run FastSurfer - a fast and accurate deep-learning based neuroimaging pipeline. FastSurfer provides a fully compatible [FreeSurfer](https://freesurfer.net/) alternative for volumetric analysis (within minutes) and surface-based thickness analysis (within only around 1h run time). FastSurfer is transitioning to sub-millimeter resolution support throughout the pipeline. The FastSurfer pipeline consists of two main parts for segmentation and surface reconstruction. - the segmentation sub-pipeline (`seg`) employs advanced deep learning networks for fast, accurate segmentation and volumetric calculation of the whole brain and selected substructures. - the surface sub-pipeline (`recon-surf`) reconstructs cortical surfaces, maps cortical labels and performs a traditional point-wise and ROI thickness analysis. ### Segmentation Modules - approximately 5 minutes (GPU), `--seg_only` only runs this part. Modules (all run by default): 1. `asegdkt:` [FastSurferVINN](FastSurferCNN/README.md) for whole brain segmentation (deactivate with `--no_asegdkt`) - the core, outputs anatomical segmentation and cortical parcellation and statistics of 95 classes, mimics FreeSurfer's DKTatlas. - requires a T1w image ([notes on input images](#requirements-to-input-images)), supports high-res (up to 0.7mm, experimental beyond that). - performs bias-field correction and calculates volume statistics corrected for partial volume effects (skipped if `--no_biasfield` is passed). 2. `cereb:` [CerebNet](CerebNet/README.md) for cerebellum sub-segmentation (deactivate with `--no_cereb`) - requires `asegdkt_segfile`, outputs cerebellar sub-segmentation with detailed WM/GM delineation. - requires a T1w image ([notes on input images](#requirements-to-input-images)), which will be resampled to 1mm isotropic images (no native high-res support). - calculates volume statistics corrected for partial volume effects (skipped if `--no_biasfield` is passed). 3. `hypothal`: [HypVINN](HypVINN/README.md) for hypothalamus subsegmentation (deactivate with `--no_hypothal`) - outputs a hypothalamic subsegmentation including 3rd ventricle, c. mammilare, fornix and optic tracts. - a T1w image is highly recommended ([notes on input images](#requirements-to-input-images)), supports high-res (up to 0.7mm, but experimental beyond that). - allows the additional passing of a T2w image with `--t2 <path>`, which will be registered to the T1w image (see `--reg_mode` option). - calculates volume statistics corrected for partial volume effects based on the T1w image (skipped if `--no_bias_field` is passed). ### Surface reconstruction - approximately 60-90 minutes, `--surf_only` runs only [the surface part](recon_surf/README.md). - supports high-resolution images (up to 0.7mm, experimental beyond that). <!-- start of image requirements --> ### Requirements to input images All pipeline parts and modules require good quality MRI images, preferably from a 3T MR scanner. FastSurfer expects a similar image quality as FreeSurfer, so what works with FreeSurfer should also work with FastSurfer. Notwithstanding module-specific limitations, resolution should be between 1mm and 0.7mm isotropic (slice thickness should not exceed 1.5mm). Preferred sequence is Siemens MPRAGE or multi-echo MPRAGE. GE SPGR should also work. See `--vox_size` flag for high-res behaviour. <!-- end of image requirements --> ![](doc/images/teaser.png) <!-- start of getting started --> ## Getting started ### Installation There are two ways to run FastSurfer (links are to installation instructions): 1. In a container ([Singularity](doc/overview/INSTALL.md#singularity) or [Docker](doc/overview/INSTALL.md#docker)) (OS: [Linux](doc/overview/INSTALL.md#linux), [Windows](doc/overview/INSTALL.md#windows), [MacOS on Intel](doc/overview/INSTALL.md#docker-currently-only-supported-for-intel-cpus)), 2. As a [native install](doc/overview/INSTALL.md#native-ubuntu-2004-or-ubuntu-2204) (all OS for segmentation part). We recommended you use Singularity or Docker on a Linux host system with a GPU. The images we provide on [DockerHub](https://hub.docker.com/r/deepmi/fastsurfer) conveniently include everything needed for FastSurfer. You will also need a [FreeSurfer license](https://surfer.nmr.mgh.harvard.edu/fswiki/License) file for the [Surface pipeline](#surface-reconstruction). We have detailed per-OS Installation instructions in the [INSTALL.md](doc/overview/INSTALL.md) file. ### Usage All installation methods use the `run_fastsurfer.sh` call interface (replace `*fastsurfer-flags*` with [FastSurfer flags](doc/overview/FLAGS.md#required-arguments)), which is the general starting point for FastSurfer. However, there are different ways to call this script depending on the installation, which we explain here: 1. For container installations, you need to define the hardware and mount the folders with the input (`/data`) and output data (`/output`): (a) For __singularity__, the syntax is ``` singularity exec --nv \\ --no-home \\ -B /home/user/my_mri_data:/data \\ -B /home/user/my_fastsurfer_analysis:/output \\ -B /home/user/my_fs_license_dir:/fs_license \\ ./fastsurfer-gpu.sif \\ /fastsurfer/run_fastsurfer.sh *fastsurfer-flags* ``` The `--nv` flag is needed to allow FastSurfer to run on the GPU (otherwise FastSurfer will run on the CPU). The `--no-home` flag tells singularity to not mount the home directory (see [Singularity documentation](Singularity/README.md#mounting-home) for more info). The `-B` flag is used to tell singularity, which folders FastSurfer can read and write to. See also __[Example 2](doc/overview/EXAMPLES.md#example-2-fastsurfer-singularity)__ for a full singularity FastSurfer run command and [the Singularity documentation](Singularity/README.md#fastsurfer-singularity-image-usage) for details on more singularity flags. (b) For __docker__, the syntax is ``` docker run --gpus all \\ -v /home/user/my_mri_data:/data \\ -v /home/user/my_fastsurfer_analysis:/output \\ -v /home/user/my_fs_license_dir:/fs_license \\ --rm --user $(id -u):$(id -g) \\ deepmi/fastsurfer:latest \\ *fastsurfer-flags* ``` The `--gpus` flag is needed to allow FastSurfer to run on the GPU (otherwise FastSurfer will run on the CPU). The `-v` flag is used to tell docker, which folders FastSurfer can read and write to. See also __[Example 1](doc/overview/EXAMPLES.md#example-1-fastsurfer-docker)__ for a full FastSurfer run inside a Docker container and [the Docker documentation](Docker/README.md#docker-flags) for more details on the docker flags including `--rm` and `--user`. 2. For a __native install__, you need to activate your FastSurfer environment (e.g. `conda activate fastsurfer_gpu`) and make sure you have added the FastSurfer path to your `PYTHONPATH` variable, e.g. `export PYTHONPATH=$(pwd)`. You will then be able to run fastsurfer with `./run_fastsurfer.sh *fastsurfer-flags*`. See also [Example 3](doc/overview/EXAMPLES.md#example-3-native-fastsurfer-on-subjectx-with-parallel-processing-of-hemis) for an illustration of the commands to run the entire FastSurfer pipeline (FastSurferCNN + recon-surf) natively. <!-- start of flags --> ### FastSurfer_Flags Please refer to [FASTSURFER_FLAGS](doc/overview/FLAGS.md). ## Examples All the examples can be found here: [FASTSURFER_EXAMPLES](doc/overview/EXAMPLES.md) - [Example 1: FastSurfer Docker](doc/overview/EXAMPLES.md#example-1-fastsurfer-docker) - [Example 2: FastSurfer Singularity](doc/overview/EXAMPLES.md#example-2-fastsurfer-singularity) - [Example 3: Native FastSurfer on subjectX with parallel processing of hemis](doc/overview/EXAMPLES.md#example-3-native-fastsurfer-on-subjectx-with-parallel-processing-of-hemis) - [Example 4: FastSurfer on multiple subjects](doc/overview/EXAMPLES.md#example-4-fastsurfer-on-multiple-subjects) - [Example 5: Quick Segmentation](doc/overview/EXAMPLES.md#example-5-quick-segmentation) - [Example 6: Running FastSurfer on a SLURM cluster via Singularity](doc/overview/EXAMPLES.md#example-6-running-fastsurfer-on-a-slurm-cluster-via-singularity) ## Output files Modules output can be found here: [FastSurfer_Output_Files](doc/overview/OUTPUT_FILES.md) - [Segmentation module](doc/overview/OUTPUT_FILES.md#segmentation-module) - [Cerebnet module](doc/overview/OUTPUT_FILES.md#cerebnet-module) - [Surface module](doc/overview/OUTPUT_FILES.md#surface-module) <!-- start of system requirements --> ## System Requirements **Recommendation: At least 8 GB system memory and 8 GB NVIDIA graphics memory** ### Minimum Requirements: | | --viewagg_device | Min GPU (in GB) | Min CPU (in GB) | |:------|------------------|----------------:|----------------:| | 1mm | gpu | 5 | 5 | | 1mm | cpu | 2 | 7 | | 0.7mm | gpu | 8 | 6 | | 0.7mm | cpu | 3 | 9 | | 0.7mm | --device cpu | 0 | 9 | The default device is the GPU. The view-aggregation device can be switched to CPU and requires less GPU memory. CPU-only processing ```--device cpu``` is much slower and not recommended. ## Expert usage Individual modules and the surface pipeline can be run independently of the full pipeline script documented in this documentation. This is documented in READMEs in subfolders, for example: [whole brain segmentation only with FastSurferVINN](FastSurferCNN/README.md), [cerebellum sub-segmentation](CerebNet/README.md), [hypothalamic sub-segmentation](HypVINN/README.md) and [surface pipeline only (recon-surf)](recon_surf/README.md). Specifically, the segmentation modules feature options for optimized parallelization of batch processing. ## FreeSurfer Downstream Modules FreeSurfer provides several Add-on modules for downstream processing, such as subfield segmentation ( [hippocampus/amygdala](https://surfer.nmr.mgh.harvard.edu/fswiki/HippocampalSubfieldsAndNucleiOfAmygdala), [brainstem](https://surfer.nmr.mgh.harvard.edu/fswiki/BrainstemSubstructures), [thalamus](https://freesurfer.net/fswiki/ThalamicNuclei) and [hypothalamus](https://surfer.nmr.mgh.harvard.edu/fswiki/HypothalamicSubunits) ) as well as [TRACULA](https://surfer.nmr.mgh.harvard.edu/fswiki/Tracula). We now provide symlinks to the required files, as FastSurfer creates them with a different name (e.g. using \"mapped\" or \"DKT\" to make clear that these file are from our segmentation using the DKT Atlas protocol, and mapped to the surface). Most subfield segmentations require `wmparc.mgz` and work very well with FastSurfer, so feel free to run those pipelines after FastSurfer. TRACULA requires `aparc+aseg.mgz` which we now link, but have not tested if it works, given that [DKT-atlas](https://mindboggle.readthedocs.io/en/latest/labels.html) merged a few labels. You should source FreeSurfer 7.3.2 to run these modules. ## Intended Use This software can be used to compute statistics from an MR image for research purposes. Estimates can be used to aggregate population data, compare groups etc. The data should not be used for clinical decision support in individual cases and, therefore, does not benefit the individual patient. Be aware that for a single image, produced results may be unreliable (e.g. due to head motion, imaging artefacts, processing errors etc). We always recommend to perform visual quality checks on your data, as also your MR-sequence may differ from the ones that we tested. No contributor shall be liable to any damages, see also our software [LICENSE](LICENSE). <!-- start of references --> ## References If you use this for research publications, please cite: _Henschel L, Conjeti S, Estrada S, Diers K, Fischl B, Reuter M, FastSurfer - A fast and accurate deep learning based neuroimaging pipeline, NeuroImage 219 (2020), 117012. https://doi.org/10.1016/j.neuroimage.2020.117012_ _Henschel L*, Kuegler D*, Reuter M. (*co-first). FastSurferVINN: Building Resolution-Independence into Deep Learning Segmentation Methods - A Solution for HighRes Brain MRI. NeuroImage 251 (2022), 118933. http://dx.doi.org/10.1016/j.neuroimage.2022.118933_ _Faber J*, Kuegler D*, Bahrami E*, et al. (*co-first). CerebNet: A fast and reliable deep-learning pipeline for detailed cerebellum sub-segmentation. NeuroImage 264 (2022), 119703. https://doi.org/10.1016/j.neuroimage.2022.119703_ _Estrada S, Kuegler D, Bahrami E, Xu P, Mousa D, Breteler MMB, Aziz NA, Reuter M. FastSurfer-HypVINN: Automated sub-segmentation of the hypothalamus and adjacent structures on high-resolutional brain MRI. Imaging Neuroscience 2023; 1 1-32. https://doi.org/10.1162/imag_a_00034_ Stay tuned for updates and follow us on [X/Twitter](https://twitter.com/deepmilab). <!-- start of acknowledgements --> ## Acknowledgements This project is partially funded by: - [Chan Zuckerberg Initiative](https://chanzuckerberg.com/eoss/proposals/fastsurfer-ai-based-neuroimage-analysis-package/) - [German Federal Ministry of Education and Research](https://www.gesundheitsforschung-bmbf.de/de/deepni-innovative-deep-learning-methoden-fur-die-rechnergestutzte-neuro-bildgebung-10897.php) The recon-surf pipeline is largely based on [FreeSurfer](https://surfer.nmr.mgh.harvard.edu/fswiki/FreeSurferMethodsCitation).\n",
                "dependencies": "[build-system] requires = ['setuptools >= 61.0.0'] build-backend = 'setuptools.build_meta' [project] name = 'fastsurfer' version = '2.5.0-dev' description = 'A fast and accurate deep-learning based neuroimaging pipeline' readme = 'README.md' license = {file = 'LICENSE'} requires-python = '>=3.10' authors = [{name = 'Martin Reuter et al.'}] maintainers = [{name = 'FastSurfer Developers'}] keywords = [ 'python', 'Deep learning', 'Segmentation', 'Brain segmentation', 'Brain analysis', 'volumetry', ] classifiers = [ 'Operating System :: Microsoft :: Windows', 'Operating System :: Unix', 'Operating System :: MacOS', 'Programming Language :: Python :: 3 :: Only', 'Programming Language :: Python :: 3.10', 'Programming Language :: Python :: 3.11', 'Programming Language :: Python :: 3.12', 'Natural Language :: English', 'License :: OSI Approved :: Apache Software License', 'Intended Audience :: Science/Research', ] dependencies = [ 'h5py>=3.7', 'lapy>=1.1.0', 'matplotlib>=3.7.1', 'nibabel>=5.1.0', 'numpy>=1.25,<2', 'pandas>=1.5.3', 'pyyaml>=6.0', 'requests>=2.31.0', 'scikit-image>=0.19.3', 'scikit-learn>=1.2.2', 'scipy>=1.10.1,!=1.13.0', 'simpleitk>=2.2.1', 'tensorboard>=2.12.1', 'torch>=2.0.1', 'torchio>=0.18.83', 'torchvision>=0.15.2', 'tqdm>=4.65', 'yacs>=0.1.8', ] [project.optional-dependencies] doc = [ 'furo!=2023.8.17', 'matplotlib', 'memory-profiler', 'myst-parser', 'numpydoc', # sphinx 8 handles importing of files differently in some manner and will cause the # build of the doc to fail. This will need to be addressed before we up to sphinx 8. 'sphinx>=7.3,<8', 'sphinxcontrib-bibtex', 'sphinxcontrib-programoutput', 'sphinx-argparse', 'sphinx-copybutton', 'sphinx-design', 'sphinx-gallery', 'sphinx-issues', 'pypandoc', 'nbsphinx', 'IPython', # For syntax highlighting in notebooks 'ipykernel', 'scikit-image', 'torchvision', 'scikit-learn', ] style = [ 'bibclean', 'codespell', 'pydocstyle[toml]', 'ruff', ] quicktest = [ 'pytest>=8.2.2', ] all = [ 'fastsurfer[doc]', 'fastsurfer[style]', 'fastsurfer[quicktest]', ] full = [ 'fastsurfer[all]', ] [project.urls] homepage = 'https://fastsurfer.org' documentation = 'https://fastsurfer.org' source = 'https://github.com/Deep-MI/FastSurfer' tracker = 'https://github.com/Deep-MI/FastSurfer/issues' [tool.setuptools] packages = ['FastSurferCNN','CerebNet','recon_surf'] [tool.pydocstyle] convention = 'numpy' ignore-decorators = '(copy_doc|property|.*setter|.*getter|pyqtSlot|Slot)' match = '^(?!setup|__init__|test_).*\\.py' match-dir = '^FastSurferCNN.*,^CerebNet.*,^recon-surf.*' add_ignore = 'D100,D104,D107' [tool.ruff] line-length = 120 target-version = \"py310\" extend-exclude = [ \"build\", \"checkpoints\", \"doc\", \"env\", ] [tool.ruff.lint] # https://docs.astral.sh/ruff/linter/#rule-selection select = [ \"E\", # pycodestyle \"F\", # Pyflakes \"UP\", # pyupgrade \"B\", # flake8-bugbear \"I\", # isort # \"SIM\", # flake8-simplify ] [tool.ruff.lint.per-file-ignores] \"__init__.py\" = [\"F401\"] \"Tutorial/*.ipynb\" = [\"E501\"] # exclude \"Line too long\" \"FastSurferCNN/utils/logging.py\" = [\"F401\"] # exclude \"Imported but unused\" \"FastSurferCNN/utils/parser_defaults.py\" = [\"F401\"] # exclude \"Imported but unused\" [tool.pytest.ini_options] minversion = '6.0' addopts = '--durations 20 --junit-xml=junit-results.xml --verbose' filterwarnings = [] [tool.coverage.run] branch = true cover_pylib = false omit = [ '**/__init__.py', ] [tool.coverage.report] exclude_lines = [ 'pragma: no cover', 'if __name__ == .__main__.:', ] precision = 2\n# # This file is autogenerated by kueglerd from fastsurfer:2.4.0-rc1 # by the following command from FastSurfer: # # env/export_pip-r.sh requirements.txt fastsurfer:2.4.0-rc1 # # Which ran the following command: # docker run --rm -u <user_id>:<group_id> --entrypoint /bin/bash fastsurfer:2.4.0-rc1 -c 'python --version && pip list --format=freeze --no-color --disable-pip-version-check --no-input --no-cache-dir' # # # Image was configured for cu126 # using python version 3.10.16 # --extra-index-url https://download.pytorch.org/whl/cu126 # Python 3.10.16 absl-py==2.1.0 Brotli==1.1.0 cached-property==1.5.2 certifi==2024.12.14 cffi==1.17.1 charset-normalizer==3.4.1 click==8.1.8 colorama==0.4.6 contourpy==1.3.1 cycler==0.12.1 Deprecated==1.2.18 filelock==3.17.0 fonttools==4.55.8 fsspec==2024.12.0 grpcio==1.67.1 h2==4.1.0 h5py==3.12.1 hpack==4.1.0 humanize==4.11.0 hyperframe==6.1.0 idna==3.10 imagecodecs==2024.12.30 imageio==2.37.0 importlib_metadata==8.6.1 importlib_resources==6.5.2 Jinja2==3.1.5 joblib==1.4.2 kiwisolver==1.4.7 lapy==1.2.0 lazy_loader==0.4 Markdown==3.6 markdown-it-py==3.0.0 MarkupSafe==3.0.2 matplotlib==3.10.0 mdurl==0.1.2 mpmath==1.3.0 munkres==1.1.4 narwhals==1.24.1 networkx==3.4.2 nibabel==5.3.2 numpy==1.26.4 nvidia-cublas-cu12==12.6.4.1 nvidia-cuda-cupti-cu12==12.6.80 nvidia-cuda-nvrtc-cu12==12.6.77 nvidia-cuda-runtime-cu12==12.6.77 nvidia-cudnn-cu12==9.5.1.17 nvidia-cufft-cu12==11.3.0.4 nvidia-curand-cu12==10.3.7.77 nvidia-cusolver-cu12==11.7.1.2 nvidia-cusparse-cu12==12.5.4.2 nvidia-cusparselt-cu12==0.6.3 nvidia-nccl-cu12==2.21.5 nvidia-nvjitlink-cu12==12.6.85 nvidia-nvtx-cu12==12.6.77 packaging==24.2 pandas==2.2.3 pillow==11.1.0 pip==25.0 plotly==6.0.0 protobuf==5.28.3 psutil==6.1.1 pycparser==2.22 Pygments==2.19.1 pyparsing==3.2.1 PySide6==6.8.1 PySocks==1.7.1 python-dateutil==2.9.0.post0 pytz==2024.1 PyWavelets==1.8.0 PyYAML==6.0.2 requests==2.32.3 rich==13.9.4 scikit-image==0.25.1 scikit-learn==1.6.1 scikit-sparse==0.4.15 scipy==1.15.1 setuptools==75.8.0 shellingham==1.5.4 shiboken6==6.8.1 SimpleITK==2.4.1 six==1.17.0 sympy==1.13.1 tensorboard==2.18.0 tensorboard_data_server==0.7.0 threadpoolctl==3.5.0 tifffile==2025.1.10 torch==2.6.0+cu126 torchio==0.20.4 torchvision==0.21.0+cu126 tornado==6.4.2 tqdm==4.67.1 triton==3.2.0 typer==0.15.1 typing_extensions==4.12.2 tzdata==2025.1 unicodedata2==16.0.0 urllib3==2.3.0 Werkzeug==3.1.3 wheel==0.45.1 wrapt==1.17.2 yacs==0.1.8 zipp==3.21.0 zstandard==0.23.0\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fatsegnet",
            "repo_link": "https://github.com/Deep-MI/FatSegNet",
            "content": {
                "codemeta": "",
                "readme": "# FatSegNet : A Fully Automated Deep Learning Pipeline for Adipose Segmentation on Abdominal Dixon MRI This repository contains the tool designed for the [Rhineland Study](https://www.rheinland-studie.de/) for segmenting visceral and subcuteneous adipose tissue on fat images from a two-point Dixon sequence. If you use this tool please cite: Estrada, Santiago, et al. \"FatSegNet: A fully automated deep learning pipeline for adipose tissue segmentation on abdominal dixon MRI.\" Magnetic resonance in medicine 83.4 (2020): 1471-1483. [https:// doi.org/10.1002/mrm.28022](https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.28022) ``` @article{estrada2020fatsegnet, title={FatSegNet: A fully automated deep learning pipeline for adipose tissue segmentation on abdominal dixon MRI}, author={Estrada, Santiago and Lu, Ran and Conjeti, Sailesh and Orozco-Ruiz, Ximena and Panos-Willuhn, Joana and Breteler, Monique MB and Reuter, Martin}, journal={Magnetic resonance in medicine}, volume={83}, number={4}, pages={1471--1483}, year={2020}, publisher={Wiley Online Library} } ``` ## Usage We wrap our tool on a docker image, so there is no need to install any library dependencies or drivers, the only requirement is to have docker (cpu) or nvidia-docker(gpu) installed. Prerequisites: * Docker (For running on CPU) (https://docs.docker.com/install/) * NVIDIA-Docker (For running on GPU ) (https://github.com/nvidia/nvidia-docker/wiki) ## Tool installation If the tool is run for the first time the FatSegNet docker image has to be created. Run the following steps 1. Run on the terminal `sudo git clone https://github.com/reuter-lab/FatSegNet.git` or download .zip file from the github repository 2. From the download repository directory run on the terminal: * `bash build_docker_cpu.sh` for CPU (In case GPU is not available)<br/> * `bash build_docker_gpu.sh` for GPU <br/> For checking that the FatSegNet image was created correctly type on the terminal<br/> `docker images` it should appear a repository with the name **adipose_tool** and the tag v1 for gpu or cpu_v1 for cpu. **Example** ``` bash REPOSITORY TAG IMAGE ID CREATED SIZE adipose_tool v1 xxxxxxxx xxxxxx xxxx adipose_tool cpu_v1 xxxxxxxx xxxxxx xxxx ``` **Note:** Both docker images for CPU and GPU can be created on the same machine. ## Running the tool ### **Input Data format** For running the tool the input data is expected to be a nifti volume with size of [256,224,72], if the scans have a different size they will be crop or padd to the correct size. Additionally the scans have to be arrange as follows(or see [example_data_folder](./example_data_folder), **NOTE** :This folder contain a ilustrative example of how images have to organized for FatSegNet to work. The Fat and water images scans are empty) : ``` #Input Scheme |-- my_dataset participants.csv |-- Subject_1 |-- FatImaging_F.nii.gz |-- FatImaging_W.nii.gz |-- Subject_2 |-- FatImaging_F.nii.gz |-- FatImaging_W.nii.gz |-- Subject_3 |-- FatImaging_F.nii.gz |-- FatImaging_W.nii.gz ........... |-- Subject_xx |-- FatImaging_F.nii.gz |-- FatImaging_W.nii.gz ``` The fat and water scans should have the same name, the name can be defined by the user, the default names are **FatImaging_F.nii.gz (Fat)** and **FatImaging_W.nii.gz(water)**. **Participants file (participants.csv)** : the purpose of this file is to configure the participants scans that should be process. The file has a one compulsory column that consist of the name of folder containing the water and fat scans. `participants.csv` example : ``` Subject_1 Subject_2 Subject_3 Subject_xx ``` ### Running FatSegNet For executing FatSegNet is necesary to configure the docker run options and the script input arguments as follows :<br/> ``` #For gpu nvidia-docker run [OPTIONS] adipose_tool:v1 [ARGUMENTS] #For Cpu docker run [OPTIONS] adipose_tool:v1 [ARGUMENTS] ``` #### Options A docker container doesnt have access to the system files so volumes has to be mounted. For our tool is necessary to mount the main data directory `my_dataset` to `/tool/Data` and the desire local output folder to `/tool/Output`. The output folder is where all pipeline output are going to be store (the input and output folder can be the same). We additionally recommend to use the following docker flags:<br/> * `--rm` : Automatically clean up the container and remove the file system when the container exits * `--user , -u `: Username or UID (format: <name|uid>[:<group|gid>]) * `--name` : Assign a name to the container * `--volume , -v`: Bind mount a volume **Example** ``` bash #For Gpu nvidia-docker run --rm --name fatsegnet -u $(id -u) -v ../my_dataset/:/tool/Data -v ../my_dataset_output/:/tool/Output adipose_tool:v1 [Arguments] # For CPU docker run -it --rm --name fatsegnet -u $(id -u) -v ../my_dataset/:/tool/Data -v ../my_dataset_output/:/tool/Output adipose_tool:cpu_v1 [Arguments] ``` #### Arguments * `--file,-f` : csv file containing the participants to process (default: participants.csv) * `--output_folder,-outp` : Parent folder for the scripts outputs (see output seccion) * `--fat_image,-fat` : Name of the fat image (default :FatImaging_F.nii.gz) * `--water_image,-water`: Name of the water image (default :FatImaging_W.nii.gz) * `--control_images,-No_QC` : Not to plot subjects predictions for visual quality control * `--run_localization,-loc` : run abdominal region localization model , by default the localization model is not run * `--axial,-axial` : run only axial segmentation model * `--order,-order` : Interpolation order (0=nearest,1=linear(default),2=quadratic,3=cubic),the tool standardizes the input resolutions to [2mm,2mm,5mm]; if the axial flag is selected only the axial plane is sample. * `--compartments,-comp` : Number of equal compartments to calculate the statistics (default=0.0) * `--increase_threshold,-AAT` : Warning flag for an increase in AAT over the define threhold between consecutive scans (default=0.4) * `--sat_to_vat_threshold,-ratio`: Warning flag for a vat to sat ratio higher than the define threshold (default=2.0) * `--runs_stats,-stats` : the AAT segmentations model are not deploy only image biomarkers are calculated,a fat scan and VAT and SAT segmentation map is required (AAT_pred.nii.gz) * `--gpu_id, -gpu_id` : GPU device ID, the container will only use the specified Gpu (default `device ID 0`). ***Note*** the script organize the GPU IDs by pci bus IDs. **Example** ``` # Run paper implementation nvidia-docker run --rm --name fatsegnet -u $(id -u) -v ../my_dataset/:/tool/Data -v ../my_dataset_output/:/tool/Output adipose_tool:v1 -loc # Change Participants files nvidia-docker run [Options] adipose_tool:v1 -f new_participants.csv -loc # Change name of water and fat images to search nvidia-docker run [Options] adipose_tool:v1 -fat fat_image.nii.gz -water water_image.nii.gz -loc # Select a specific GPU (ex: device ID 2) nvidia-docker run [Options] adipose_tool:v1 -loc -gpu_id 2 # run only the segmentation models on the axial plane and define interpolation order nvidia-docker run [Options] adipose_tool:v1 -axial -order 3 ``` ### **Output Data format** ``` bash #Output Scheme |-- my_dataset_output |-- Subject_1 |-- MRI (Only created if the images are resize or sample) |-- FatImaging_F.nii.gz (Fat_Scans) |-- FatImaging_W.nii.gz (Water_Scans) |-- QC |-- QC_[0-3].png (Quality control images) |-- Segmentations |-- AAT_pred.nii.gz (Only adipose tissues prediction map) |-- ALL_pred.nii.gz (adipose tissues and auxilary classes prediction map) |-- AAT_variables_summary.json (Calculated Image Biomarkers) |-- Subject_2 |-- MRI (Only created if the images are resize or sample) |-- FatImaging_F.nii.gz (Fat_Scans) |-- FatImaging_W.nii.gz (Water_Scans) |-- QC |-- QC_[0-3].png (Quality control images) |-- Segmentations |-- AAT_pred.nii.gz (Only adipose tissues prediction map) |-- ALL_pred.nii.gz (adipose tissues and auxilary classes prediction map) |-- AAT_variables_summary.json (Calculated Image Biomarkers) ............... |-- Subject_xx |-- MRI (Only created if the images are resize or sample) |-- FatImaging_F.nii.gz (Fat_Scans) |-- FatImaging_W.nii.gz (Water_Scans) |-- QC |-- QC_[0-3].png (Quality control images) |-- Segmentations |-- AAT_pred.nii.gz (Only adipose tissues prediction map) |-- ALL_pred.nii.gz (adipose tissues and auxilary classes prediction map) |-- AAT_variables_summary.json (Calculated Image Biomarkers) ``` **Image Biomarkers** For more information on the pipeline image biomarkers reported in the `AAT_variables_summary.json ` file please check the document [FatSegNet_Variables.pdf](./FatSegNet_Variables.pdf) **Quality Control Image Example** By default the tool creates 4 images for visually control of the input scan and predicted segmentation, as the one shown below. Top row fat images from axial, coronal ,sagittal view centered on the red dot; bottom row predicted segmentations (blue: SAT, green : VAT). ![](Images/QC_3.png) -------- For any questions and feedback, feel free to contact santiago.estrada(at).dzne.de<br/> --------\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fesom",
            "repo_link": "https://github.com/FESOM/fesom2",
            "content": {
                "codemeta": "",
                "readme": "The Finite Element Sea Ice-Ocean Model (FESOM2) ====== [![Build Status](https://github.com/FESOM/fesom2/workflows/FESOM2%20main%20test/badge.svg)](https://github.com/FESOM/fesom2/actions) Multi-resolution ocean general circulation model that solves the equations of motion describing the ocean and sea ice using finite-element and finite-volume methods on unstructured computational grids. The model is developed and supported by researchers at the Alfred Wegener Institute, Helmholtz Centre for Polar and Marine Research (AWI), in Bremerhaven, Germany. **Website:** [fesom.de](https://fesom.de/) **Documentation:** [fesom2.readthedocs.io](https://fesom2.readthedocs.io/en/latest/index.html) **Basic tutorial:** [Getting started](https://fesom2.readthedocs.io/en/latest/getting_started/getting_started.html) References ---------- [Complete list of references on fesom.de](https://fesom.de/publications/) * **[Ocean model formulation]** Danilov, S., Sidorenko, D., Wang, Q., and Jung, T.: The Finite-volumE Sea ice-Ocean Model (FESOM2), Geosci. Model Dev., 10, 765-789, https://doi.org/10.5194/gmd-10-765-2017, 2017. * **[Sea ice model formulation]** Danilov, S., Q. Wang, R. Timmermann, N. Iakovlev, D. Sidorenko, M. Kimmritz, T. Jung, and Schröter, J. (2015), Finite-Element Sea Ice Model (FESIM), version 2, Geosci. Model Dev., 8, 1747-1761, http://www.geosci-model-dev.net/8/1747/2015/ * **[Evaluation of standard sumulations]** Scholz, P., Sidorenko, D., Gurses, O., Danilov, S., Koldunov, N., Wang, Q., Sein, D., Smolentseva, M., Rakowsky, N., and Jung, T.: Assessment of the Finite-volumE Sea ice-Ocean Model (FESOM2.0) - Part 1: Description of selected key model elements and comparison to its predecessor version, Geosci. Model Dev., 12, 4875-4899, https://doi.org/10.5194/gmd-12-4875-2019, 2019. * **[Evaluation of computational performance]** Koldunov, N. V., Aizinger, V., Rakowsky, N., Scholz, P., Sidorenko, D., Danilov, S., and Jung, T.: Scalability and some optimization of the Finite-volumE Sea ice-Ocean Model, Version 2.0 (FESOM2), Geosci. Model Dev., 12, 3991-4012, https://doi.org/10.5194/gmd-12-3991-2019, 2019. * **[Version coupled with ECHAM6 atmosphere]** Sidorenko, D., Goessling, H. F., Koldunov, N. V., Scholz, P., Danilov, S., Barbi, D., et al ( 2019). Evaluation of FESOM2.0 coupled to ECHAM6.3: Pre‐industrial and HighResMIP simulations. Journal of Advances in Modeling Earth Systems, 11. https://doi.org/10.1029/2019MS001696 * **[Version with ICEPACK sea ice thermodynamics]** Zampieri, Lorenzo, Frank Kauker, Jörg Fröhle, Hiroshi Sumata, Elizabeth C. Hunke, and Helge Goessling. Impact of Sea-Ice Model Complexity on the Performance of an Unstructured-Mesh Sea-ice/ocean Model Under Different Atmospheric Forcings. Washington: American Geophysical Union, 2020. https://dx.doi.org/10.1002/essoar.10505308.1. * **[Version coupled with OpenIFS atmosphere]** Streffing, J., Sidorenko, D., Semmler, T., Zampieri, L., Scholz, P., Andrés-Martínez, M., et al ( 2022). AWI-CM3 coupled climate model: description and evaluation experiments for a prototype post-CMIP6 model. Geoscientific Model Development, 15. https://doi.org/10.5194/gmd-15-6399-2022\n",
                "dependencies": "cmake_minimum_required(VERSION 3.16) # set default build type cache entry (do so before project(...) is called, which would create this cache entry on its own) if(NOT CMAKE_BUILD_TYPE) message(STATUS \"setting default build type: Release\") set(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"Choose the type of build, options are: None(CMAKE_CXX_FLAGS or CMAKE_C_FLAGS used) Debug Release RelWithDebInfo MinSizeRel.\") endif() project(FESOM2.0) set(BUILD_SHARED_LIBS ON CACHE BOOL \"Default to using shared libs\") set(TOPLEVEL_DIR ${CMAKE_CURRENT_LIST_DIR}) set(FESOM_COUPLED OFF CACHE BOOL \"compile fesom standalone or with oasis support (i.e. coupled)\") set(OIFS_COUPLED OFF CACHE BOOL \"compile fesom coupled to OpenIFS. (Also needs FESOM_COUPLED to work)\") set(CRAY OFF CACHE BOOL \"compile with cray ftn\") set(USE_ICEPACK OFF CACHE BOOL \"compile fesom with the Iceapck modules for sea ice column physics.\") set(OPENMP_REPRODUCIBLE OFF CACHE BOOL \"serialize OpenMP loops that are critical for reproducible results\") set(RECOM_COUPLED OFF CACHE BOOL \"compile fesom including biogeochemistry, REcoM3\") set(CISO_COUPLED OFF CACHE BOOL \"compile ciso coupled to REcoM3. RECOM_COUPLED has to be active\") set(USE_MULTIO OFF CACHE BOOL \"Use MULTIO for IO, either grib or binary for now. This also means path to MULTIO installation has to provided using env MULTIO_INSTALL_PATH='..' and multio configuration yamls must be present to run the model with MULTIO\") set(OASIS_WITH_YAC OFF CACHE BOOL \"Useing a version of OASIS compiled with YAC instead of SCRIP for interpolation?\") set(ASYNC_ICEBERGS ON CACHE BOOL \"compile fesom with or without support for asynchronous iceberg computations\") set(VERBOSE OFF CACHE BOOL \"toggle debug output\") #add_subdirectory(oasis3-mct/lib/psmile) add_subdirectory(src) foreach( _file fesom-config.cmake fesom-config-version.cmake fesom-targets.cmake ) execute_process( COMMAND ${CMAKE_COMMAND} -E create_symlink ${PROJECT_BINARY_DIR}/src/${_file} ${PROJECT_BINARY_DIR}/${_file} ) endforeach() # Define ${PROJECT_NAME}_DIR in PARENT_SCOPE so that a `find_package( <this-project> )` in a bundle # will easily find the project without requiring a `HINT <this-project>_BINARY_DIR` argument if( NOT CMAKE_SOURCE_DIR STREQUAL PROJECT_SOURCE_DIR ) # Guard needed because PARENT_SCOPE cannot be used in top-level CMake project set( fesom_DIR ${fesom_DIR} PARENT_SCOPE ) endif()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fishinspector",
            "repo_link": "https://github.com/sscholz-UFZ/FishInspector",
            "content": {
                "codemeta": "",
                "readme": "# FishInspector **Annotation of features from zebrafish embryos** The software FishInspector allows annotation of features in images of zebrafish embryos. The recent version requires images of a lateral position. It is important that the position is precise since deviation may confound with feature annotations. Images from any source can be used. However, depending on the image properties parameters may have to be adjusted. Furthermore, images obtained with normal microscope and not using an automated position system with embryos in glass capillaries require conversion using a KNIME workflow (available [here](https://github.com/eteixido/Knime-workflows-FishInspector)). As a result of the analysis the software provides JSON files that contain the coordinates of the features. Coordinates are provided for eye, fish contour, notochord , otoliths, yolk sac, pericard and swimbladder. Furthermore, pigment cells in the notochord area are detected. Additional features can be manually annotated. It is the aim of the software to provide the coordinates, which may then be analysed subsequently to identify and quantify changes in the morphology of zebrafish embryos. ## [Available for Download Here](https://github.com//sscholz-UFZ/FishInspector/releases) ## User Guide The complete user guide can be checked [here](https://github.com/sscholz-UFZ/FishInspector/blob/master/docs/Index.md) ## Referencing Citations can be made using the following DOI: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1422642.svg)](https://doi.org/10.5281/zenodo.1422642) *Teixido, E., Kießling, T.R., Krupp, E., Quevedo, C., Muriana, A., Scholz, S., 2018. Automated morphological feature assessment for zebrafish embryo developmental toxicity screens. Tox. Sci. accepted.* ## License This project is licensed under a GNU General Public License - see the [LICENSE](LICENSE) file for details and also this [file](License.txt).\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fleur",
            "repo_link": "https://iffgit.fz-juelich.de/fleur/fleur",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/formatfuzzer",
            "repo_link": "https://github.com/uds-se/FormatFuzzer",
            "content": {
                "codemeta": "",
                "readme": "README.md\n# FormatFuzzer `FormatFuzzer` is a framework for *high-efficiency, high-quality generation and parsing of binary inputs.* It takes a *binary template* that describes the format of a binary input and generates an *executable* that produces and parses the given binary format. From a binary template for GIF, for instance, `FormatFuzzer` produces a GIF generator - also known as *GIF fuzzer*. Generators produced by `FormatFuzzer` are highly efficient, producing thousands of valid test inputs per second - in sharp contrast to mutation-based fuzzers, where the large majority of inputs is invalid. Inputs generated by `FormatFuzzer` are independent from the program under test (or actually, any program), so you can also use them in black-box settings. However, `FormatFuzzer` also integrates with AFL++ to produce valid inputs that also aim for maximum coverage. In our experiments, this \"best of two worlds\" approach surpasses all other settings; see [our paper](https://arxiv.org/abs/2109.11277) for details. The binary templates used by FormatFuzzer come from the [010 editor](https://www.sweetscape.com/010editor/). There are more than [170 binary templates](https://www.sweetscape.com/010editor/templates.html), which either can be used directly for `FormatFuzzer` or adapted for its use. Out of the box, `FormatFuzzer` produces formats such as AVI, BMP, GIF, JPG, MIDI, MP3, MP4, PCAP, PNG, WAV, and ZIP; and we keep on extending this list every week. Contributors are welcome! Visit the [FormatFuzzer project page](https://github.com/uds-se/FormatFuzzer) for filing ideas and issues, or adding pull requests. For details on how `FormatFuzzer` works and how it compares, read [our paper](https://arxiv.org/abs/2109.11277) for more info. ## Getting FormatFuzzer is available from the [FormatFuzzer project page](https://github.com/uds-se/FormatFuzzer). You can download and unpack the latest release from [the releases page](https://github.com/uds-se/FormatFuzzer/releases). For the very latest and greatest, you can also clone its git repository: ``` git clone https://github.com/uds-se/FormatFuzzer.git ``` All further actions take place in its main folder: ``` cd FormatFuzzer ``` ## Prerequisites To run FormatFuzzer, you need the following: * Python 3 * A C++ compiler with GNU libraries (notably `getopt_long()`) such as `clang` or `gcc` * The Python packages `py010parser`, `six`, and `intervaltree` * A `zlib` library (for compression functions) * A `boost` library (for checksum functions) If you plan to edit the build and configuration scripts (`.ac` and `.am` files), you will also need * GNU autoconf * GNU automake ### Installing Requirements on Linux (Debian Packages) ``` sudo apt install git g++ make automake python3-pip zlib1g-dev libboost1.71-dev pip3 install py010parser six intervaltree ``` ### Installing Requirements on MacOS (with Xcode & Homebrew) ``` xcode-select --install brew install python3 automake boost pip3 install py010parser six intervaltree ``` ### Installing Python Packages Only (All Operating Systems) On all systems, using `pip`: ``` pip install py010parser pip install six pip install intervaltree ``` ## Building Note: all building commands require you to be in the same folder as this `README` file. Building a fuzzer outside of this folder is not yet supported. ### Method 1: Using the build.sh script There's a `build.sh` script which automates all construction steps. Simply run ``` ./build.sh gif ``` to create a GIF fuzzer. This works for all file formats provided in `templates/`; if there is a file `templates/FOO.bt`, then `./build.sh FOO` will build a fuzzer. ### Method 2: Using Make There's a `Makefile` (source in `Makefile.am`) which automates all construction steps. (Requires `GNU make`.) First do ``` touch configure Makefile.in ``` then ``` ./configure ``` and then ``` make gif-fuzzer ``` to create a GIF fuzzer. This works for all file formats provided in `templates/`; if there is a file `templates/FOO.bt`, then `make FOO-fuzzer` will build a fuzzer. ### Method 3: Manual steps If the above `make` method does not work, or if you want more control, you may have to proceed manually. #### Step 1: Compiling Binary Template Files into C++ code Run the `ffcompile` compiler to compile the binary template into C++ code. It takes two arguments: the `.bt` binary template, and a `.cpp` C++ file to be generated. ``` ./ffcompile templates/gif.bt gif.cpp ``` #### Step 2: Compiling the C++ code Use the following commands to create a fuzzer `gif-fuzzer`. First, compile the generic command-line driver: ``` g++ -c -I . -std=c++17 -g -O3 -Wall fuzzer.cpp ``` (`-I .` denotes the location of the `bt.h` file; `-std=c++17` sets the C++ standard.) Then, compile the binary parser/compiler: ``` g++ -c -I . -std=c++17 -g -O3 -Wall gif.cpp ``` Finally, link the binary parser/compiler with the command-line driver to obtain an executable. If you use any extra libraries (such as `-lz`), be sure to specify these here too. ``` g++ -O3 gif.o fuzzer.o -o gif-fuzzer -lz ``` ## Running the Fuzzer FormatFuzzer can be run as a standalone parser, generator or mutator of specific formats. In addition, it can called by general-purpose fuzzers such as [AFL++](https://github.com/uds-se/AFLplusplus) to integrate those format-specific capabilities into the fuzzing process (see the section below on AFL++ integration). The generated fuzzer takes a _command_ as first argument, followed by options and arguments to that command. The most important command is `fuzz`, for producing outputs. Its arguments are files to be generated in the appropriate format. Run the generator as ``` ./gif-fuzzer fuzz output.gif ``` to create a random binary file `output.gif`, or ``` ./gif-fuzzer fuzz out1.gif out2.gif out3.gif ``` to create three GIF files `out1.gif`, `out2.gif`, and `out3.gif`. Note that the `gif.bt` template we provide has been augmented with special functions to make generation of valid files easier. If you use an original `.bt` template files without adaptations, you may get warnings during generation and create invalid files. ## Running Parsers You can also run the fuzzer as a _parser_ for binary files, using the `parse` command. This is useful if you want to test the accuracy of the binary template, or if you want to mutate an input (see `Decision Files', below). To run the parser, use ``` ./gif-fuzzer parse input.gif ``` You will see error messages if `input.gif` cannot be successfully parsed. ## Decision Files While parsing, you can also store all parsing decisions (i.e. which parsing alternatives were taken) in a _decision file_. This is a sequence of bytes enumerating the decisions taken. Each byte stands for a single parsing decision. A byte value of `0` means that the first alternative was taken, a byte value of `1` means that the second alternative was taken, and so on. You can generate such a decision file when parsing an input: ``` ./gif-fuzzer parse --decisions input.dec input.gif ``` Here, `input.dec` stores the decisions made for parsing `input.gif'. You can also use such a decision file when _generating_ inputs. The fuzzer will then take the exact same decisions as found during parsing. The following command generates a new GIF file using the decisions determined while parsing `input.gif': ``` ./gif-fuzzer fuzz --decisions input.dec input2.gif ``` If everything works well, both files should be identical: ``` cmp input.gif input2.gif ``` By _mutating_ a decision file (e.g. replacing individual bytes), you can create inputs that are _similar_ to the original file parsed. This is useful for interfacing with specific testing strategies and fuzzers such as AFL, where you can use `gif-fuzzer` and the like as _translators_ from decision files to binary files and back: AFL would mutate decision files, and the program under test would run on the translated binary files. In contrast to mutating binary files directly (as AFL would normally do), this would have the advantage of always having valid inputs - and thus progressing much faster towards coverage. ## AFL++ Integration In addition to the format-specific fuzzers, such as `gif-fuzzer`, FormatFuzzer can also be compiled into format-specific shared libraries, such as `gif.so` (for that, simply run `./build.sh gif` or `make gif.so`). Those shared libraries can be loaded by general-purpose fuzzers, such as [AFL++](https://github.com/AFLplusplus/AFLplusplus). To run AFL++ with FormatFuzzer, just follow the instructions on [our modified version of AFL++](https://github.com/uds-se/AFLplusplus). We support different fuzzing strategies, including: * AFL+FFMut: runs AFL++ using FormatFuzzer to provide format-specific smart mutations. * AFL+FFGen: uses FormatFuzzer as a format-specific generator, while AFL++ mutates its decision seeds. ## Creating and Customizing Binary Templates To write your own `.bt` binary templates (and thus create a high-efficiency fuzzer/parser for this format), read the section [Introduction to Templates and Scripts](https://www.sweetscape.com/010editor/manual/IntroTempScripts.htm) from the [010 Editor Manual](https://www.sweetscape.com/010editor/manual/). In many cases, a template of the format you are looking for (or a similar one) may already exist. Have a look at the 010 editor [binary template collection](https://www.sweetscape.com/010editor/templates.html) whether there is something that you can use or base your format on. Note that the `.bt` files provided in the repository generally target _parsing_ files. They _can_ be used for _generating_ files, too; but they often lack exact information which parts of the input are required. In this section, we discuss some of the ways in which you can customize `.bt` files to work well with `FormatFuzzer`. For example, for the GIF format, the file [templates/gif-orig.bt](templates/gif-orig.bt) shows the original binary template, which was only designed for parsing, while the file [templates/gif.bt](templates/gif.bt) is a modified version which is capable of generating valid GIFs. Comparing the two files, we see that a small number changes was required to achieve this. If you have created a `gif-fuzzer`, either by running `make gif-fuzzer` or by using the `ffcompile` tool, you have already obtained a C++ file `gif.cpp` which contains an implementation of the GIF generator and parser. This is useful to see how the changes you make to the binary template are translated into executable code. More details on the C++ code are presented on the next section. The GIF binary template makes use of _lookahead_ functions `ReadUByte()` and `ReadUShort()` to look ahead at the values of the next bytes in the file before actually parsing them into a struct field. At generation time, we allow those functions to receive an additional argument specifying a set of good known values to pick for the bytes that we look ahead. In addition, we also allow specifying a global set of good known values to always use when calling a particular lookahead function, such as `ReadUByte()`. Those are stored in the `ReadUByteInitValues` vector. By default, our translation procedure `ffcompile` tries to mine interesting values which have been used in comparisons against lookahead bytes and use them as a global set of known values. When running ``` ./ffcompile templates/gif.bt gif.cpp ``` a printed message shows the lookahead functions identified, as well as the mined interesting values: ``` Finished creating cpp generator. Lookahead functions found: ReadUByte ReadUShort Mined interesting values: GlobalColorTableFlag: ['1'] LocalColorTableFlag: ['1'] ReadUByte: ['0x3B', '0x2C'] ReadUShort: ['0xF921', '0xFE21', '0x0121', '0xFF21'] Signature: ['\"GIF\"'] ``` For GIF generation, however, it is better to specify the set of good known values for `ReadUByte()` individually at each call to the function. So we define an empty array (size 0) ``` const local UBYTE ReadUByteInitValues[0]; ``` to overwrite the set of global `ReadUByteInitValues` and for each call to `ReadUByte()`, we use an additional argument to specify the set of good values to use for that particular location. The binary template language is also powerful enough to allow this choice to be made based on runtime conditions. For example, in the following code we show how the choice of appropriate values for a `ReadUByte()` call can depend on the current GIF version we are generating. A GIF version `89a` allows one extra possible value for the byte (0x21). ``` if(GifHeader.Version == \"89a\") local UBYTE values[] = { 0x3B, 0x2C, 0x21 }; else local UBYTE values[] = { 0x3B, 0x2C }; while (ReadUByte(FTell(), values) != 0x3B) { ... } ``` The remaining edits required for the GIF binary template are similar. For example, for each struct field can also specify a set of known good values. For example this specifies the correct values for the `Version` field: `87a` and `89a`. ``` char Version[3] = { {\"87a\"}, {\"89a\"} }; ``` ## Understanding the Generated C++ Code For debugging purposes, as well as for understanding how to make appropriate changes to improve your generators and parsers, it may be useful to understand some inner workings of the generated C++ code. Ideally, you should be able to edit the binary template files until they can be used to generate valid files with high probability, so you wouldn't have to edit the generated C++ code. The C++ code creates a class for each `struct` and `union` defined in the binary template, as well as for native types, such as `int`. At construction time, when initializing a variable, we can define a set of good known values that this variable can assume. For example, the constructor call ``` char_array_class cname(cname_element, { \"IHDR\", \"tEXt\", \"PLTE\", \"cHRM\", \"sRGB\", \"iEXt\", \"zEXt\", \"tIME\", \"pHYs\", \"bKGD\", \"sBIT\", \"sPLT\", \"acTL\", \"fcTL\", \"fdAT\", \"IHDR\", \"IEND\" }); ``` would specify 17 good values to use for variable `cname`. But this is often not enough, since the choice of appropriate chunk types is context sensitive. So we also allow specifying a set of good values at generation time when generating a new chunk. For example, this call could be used to generate an instance of `chunk` for the first chunk, which must have type IHDR. ``` GENERATE(chunk, ::g->chunk.generate({ \"IHDR\" }, false)); ``` When generating the second chunk, we might use this long list of possible chunks that can come between the IHDR chunk and the PLTE chunk: ``` GENERATE(chunk, ::g->chunk.generate({ \"iCCP\", \"sRGB\", \"sBIT\", \"gAMA\", \"cHRM\", \"pHYs\", \"sPLT\", \"tIME\", \"zTXt\", \"tEXt\", \"iTXt\", \"eXIf\", \"oFFs\", \"pCAL\", \"sCAL\", \"acTL\", \"fcTL\", \"fdAT\", \"fRAc\", \"gIFg\", \"gIFt\", \"gIFx\", \"sTER\" }, true)); ``` The generator will then uniformly pick one of the good known values to use for the new instance. We also allow the choice of an evil value which is not one of the good known values with small probability 1/128. This feature can be enabled or disabled any time by using the method `set_evil_bit`. All the random choices taken by the generator are done by calling the `rand_int()` method. ``` long long rand_int(unsigned long long x, std::function<long long (unsigned char*)> parse); ``` When running the program as a generator, this method samples an integer from 0 to x-1 by reading bytes from the random buffer. When running the program as a parser, this method uses the `parse()` function to find out which random bytes must be present in the random buffer in order to generate the target file, and then writes those bytes to the random buffer. The `parse` function receives as an argument the buffer at the current position of the file and must then return which value would have to be returned by the current call to `rand_int()` in order to generate this exact file configuration. ## Authors FormatFuzzer was designed and written by Rafael Dutra &lt;rafael.dutra@cispa.de&gt;. The concept of a fuzzer compiler was introduced by Rahul Gopinath &lt;rahul.gopinath@cispa.de&gt; and Andreas Zeller &lt;zeller@cispa.de&gt;. ## Copyright and Licenses FormatFuzzer is Copyright &copy; 2020, 2021 by [CISPA Helmholtz Center for Information Security](https://cispa.de/). The following licenses apply: * _The FormatFuzzer code_ (notably, all C++ code and code related to its generation) is subject to the GNU GENERAL PUBLIC LICENSE, as found in [COPYING](COPYING). * As an exception to the above, _C++ code generated by FormatFuzzer_ (i.e., fuzzers and parsers for specific formats) is in the public domain. * _The original_ [pfp](https://github.com/d0c-s4vage/pfp) _code_, which FormatFuzzer is based upon, is subject to an MIT license, as found in [LICENSE-pfp](LICENSE-pfp).\n",
                "dependencies": "py010parser>=0.1.17 six>=1.10.0,<2.0.0 intervaltree>=3.0.2,<4.0.0\n#!/usr/bin/env python # encoding: utf-8 import os, sys from setuptools import setup setup( # metadata name=\"pfp\", description=\"An 010 template interpreter for Python\", long_description=\"\"\" pfp is an 010 template interpreter for Python. It accepts an input data stream and an 010 template and returns a modifiable DOM of the parsed data. Extensions have also been added to the 010 template syntax to allow for linked fields (e.g. checksums, length calculations, etc), sub structures in compressed data, etc. \"\"\", license=\"MIT\", version=\"{{VERSION}}\", author=\"James Johnson\", maintainer=\"James Johnson\", author_email=\"d0c.s4vage@gmail.com\", url=\"https://github.com/d0c-s4vage/pfp\", platforms=\"Cross Platform\", download_url=\"https://github.com/d0c-s4vage/pfp/tarball/v{{VERSION}}\", install_requires=open( os.path.join(os.path.dirname(__file__), \"requirements.txt\") ) .read() .split(\"\\n\"), classifiers=[ \"Programming Language :: Python :: 2\", \"Programming Language :: Python :: 3\", ], entry_points={ \"console_scripts\": [\"pfp = pfp.__main__:main\"] }, packages=[\"pfp\", \"pfp.native\", \"pfp.fuzz\"], )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fracspy",
            "repo_link": "https://github.com/FraCSPy/FraCSPy",
            "content": {
                "codemeta": "",
                "readme": "# FraCSPy [![DOI](https://zenodo.org/badge/619447827.svg)](https://zenodo.org/badge/latestdoi/619447827) FraCSPy stands for Python Framework for Conventional microSeismic Processing. ![FraCSPy logo](logo/fracspy_logo.png) It is a single python toolbox for the full microseismic pipeline from modelling to post-event analysis. This library is a single location leveraging the excellent work of other scientists (software developers) and adapts them for the specific use case of microseismic monitoring. Some functionalities include: - modelling script generation (for accompanying [SOFI3D](https://docs.csc.fi/apps/sofi3d/)) - event imaging: detection, location - moment tensor inversion Some python libraries that are utilised include: - pylops - torch - obspy - and more... ## Requirements Installation requires either [pip](https://pypi.org/project/pip/) package installer or [Conda](https://conda.io) package manager, e.g. one can use [miniforge](https://github.com/conda-forge/miniforge). ## Install using pip ```bash pip install fracspy ``` ## Install using conda ### Linux Simply run ```bash make install ``` It will create a new conda environment `fracspy` with all the required packages: Similarly, on Linux you can run: ```bash ./install.sh ``` ### Windows On Windows, the best way is to use [miniforge](https://github.com/conda-forge/miniforge) prompt and run: ```cmd install.bat ``` It will install the package to environment `fracspy` and activate it. To install development version use ```cmd install-dev.bat ``` Now you are ready to use the package. ### Uninstall If you need to add/change packages: ```bash conda deactivate conda remove -n fracspy -all ``` ## Documentation The latest stable documentation based on [Sphinx](https://www.sphinx-doc.org) is available online at: <https://fracspy.github.io/FraCSPy> One can also build the documentation locally: ```bash cd docs make html ``` If you want to rebuild the documentation: ```bash make clean make html ``` After a successful build, one can serve the documentation website locally: ```bash cd build/html python -m http.server ``` and open in browser: <http://localhost:8000> To build/rebuild documentation on Windows you can simply run ```cmd build_docs.bat ``` **Note:** check the exact port number in the output\n",
                "dependencies": "#!/bin/bash # # Installer for fracspy # # Run: ./install.sh # # C. Birnie, 13/04/2023 # Updated by D. Anikiev 30/05/2024 ENV_YAML=environment.yml ENV_NAME=fracspy PACKAGE_NAME=fracspy echo 'Creating $(ENV_NAME) environment' # create conda env conda env create -f $ENV_YAML source $CONDA_PREFIX/etc/profile.d/conda.sh conda activate $ENV_NAME pip install -e . conda env list echo 'Created and activated environment $(ENV_YAML):' $(which python) # Check echo 'Checking $(PACKAGE_NAME) version...' python -c 'import fracspy as fp; print(fp.__version__)' echo 'Done!'\n[build-system] requires = [ \"setuptools >= 65\", \"setuptools_scm[toml]\", \"wheel\", ] build-backend = \"setuptools.build_meta\" [project] name = \"fracspy\" description = \"A python package for general microseismic modelling, monitoring and analysis\" readme = \"README.md\" authors = [ {name = \"Claire Emma Birnie\", email = \"claire.birnie@kaust.edu.sa\"}, {name = \"Denis Anikiev\", email = \"denis.anikiev@gfz-potsdam.de\"}, {name = \"Omar Sad Aly\", email = \"omar.sadaly@kaust.edu.sa\"}, {name = \"Matteo Ravasi\", email = \"matteo.ravasi@kaust.edu.sa\"}, ] license = {file = \"LICENSE.md\"} keywords = [\"geophysics\", \"signal processing\", \"microseismic\"] classifiers = [ \"Development Status :: 1 - Planning\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", \"Intended Audience :: Education\", \"License :: OSI Approved :: MIT License\", \"Natural Language :: English\", \"Operating System :: OS Independent\", \"Programming Language :: Python :: 3 :: Only\", \"Programming Language :: Python :: 3.8\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", ] dependencies = [ \"numpy >= 1.15.0\", \"scipy >= 1.8.0\", \"matplotlib\", \"pylops >= 2.0.0\", \"torch\", \"cmcrameri\", \"openpyxl\", \"tqdm\", \"obspy\", ] dynamic = [\"version\"] [project.optional-dependencies] test = [ \"pytest\", \"pytest-cov\", ] [tool.setuptools.packages.find] exclude = [\"pytests\"] [tool.setuptools_scm] version_file = \"fracspy/version.py\"\n.\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/fsqc",
            "repo_link": "https://github.com/Deep-MI/fsqc",
            "content": {
                "codemeta": "",
                "readme": "# fsqc toolbox ## Description This package provides quality assurance / quality control scripts for FastSurfer- or FreeSurfer-processed structural MRI data. It will check outputs of these two software packages by means of quantitative and visual summaries. Prior processing of data using either FastSurfer or FreeSurfer is required, i.e. the software cannot be used on raw images. It is a revision, extension, and translation to the Python language of the [Freesurfer QA Tools](https://surfer.nmr.mgh.harvard.edu/fswiki/QATools). It has been augmented by additional functions from the [MRIQC toolbox](https://github.com/poldracklab/mriqc), and with code derived from the [LaPy](https://github.com/Deep-MI/lapy) and [BrainPrint](https://github.com/Deep-MI/brainprint) toolboxes. This page provides general, usage, and installation information. See [here](https://deep-mi.org/fsqc/dev/index.html) for the full documentation. ___ ## Contents - [Functionality](#functionality) - [Development](#development) - [News](#news) - [Main and development branches](#main-and-development-branches) - [Roadmap](#roadmap) - [Usage](#usage) - [As a command line tool](#as-a-command-line-tool) - [As a Python package](#as-a-python-package) - [As a Docker image](#as-a-docker-image) - [Installation](#installation) - [Installation as a Python package](#installation-as-a-python-package) - [Installation from GitHub](#installation-from-github) - [Download from GitHub](#download-from-github) - [Requirements](#requirements) - [Known issues](#known-issues) - [Authors](#authors) - [Citations](#citations) - [License](#license) ___ ## Functionality The core functionality of this toolbox is to compute the following features: variable | description ---------------|---------------------------------------------------------------- subject | subject ID wm_snr_orig | signal-to-noise ratio for white matter in orig.mgz gm_snr_orig | signal-to-noise ratio for gray matter in orig.mgz wm_snr_norm | signal-to-noise ratio for white matter in norm.mgz gm_snr_norm | signal-to-noise ratio for gray matter in norm.mgz cc_size | relative size of the corpus callosum lh_holes | number of holes in the left hemisphere rh_holes | number of holes in the right hemisphere lh_defects | number of defects in the left hemisphere rh_defects | number of defects in the right hemisphere topo_lh | topological fixing time for the left hemisphere topo_rh | topological fixing time for the right hemisphere con_lh_snr | wm/gm contrast signal-to-noise ratio in the left hemisphere con_rh_snr | wm/gm contrast signal-to-noise ratio in the right hemisphere rot_tal_x | rotation component of the Talairach transform around the x axis rot_tal_y | rotation component of the Talairach transform around the y axis rot_tal_z | rotation component of the Talairach transform around the z axis The program will use an existing output directory (or try to create it) and write a csv table into that location. The csv table will contain the above metrics plus a subject identifier. The program can also be run on images that were processed with [FastSurfer](https://github.com/Deep-MI/FastSurfer) (v1.1 or later) instead of FreeSurfer. In that case, simply add a `--fastsurfer` switch to your shell command. Note that FastSurfer's full processing stream must have been run, including surface reconstruction (i.e. brain segmentation alone is not sufficient). In addition to the core functionality of the toolbox there are several optional modules that can be run according to need: - screenshots module This module allows for the automated generation of cross-sections of the brain that are overlaid with the anatomical segmentations (asegs) and the white and pial surfaces. These images will be saved to the 'screenshots' subdirectory that will be created within the output directory. These images can be used for quickly glimpsing through the processing results. Note that no display manager is required for this module, i.e. it can be run on a remote server, for example. - surfaces module This module allows for the automated generation of surface renderings of the left and right pial and inflated surfaces, overlaid with the aparc annotation. These images will be saved to the 'surfaces' subdirectory that will be created within the output directory. These images can be used for quickly glimpsing through the processing results. Note that no display manager is required for this module, i.e. it can be run on a remote server, for example. - skullstrip module This module allows for the automated generation cross-sections of the brain that are overlaid with the colored and semi-transparent brainmask. This allows to check the quality of the skullstripping in FreeSurfer. The resulting images will be saved to the 'skullstrip' subdirectory that will be created within the output directory. - fornix module This is a module to assess potential issues with the segmentation of the corpus callosum, which may incorrectly include parts of the fornix. To assess segmentation quality, a screenshot of the contours of the corpus callosum segmentation overlaid on the norm.mgz will be saved as 'cc.png' for each subject within the 'fornix' subdirectory of the output directory. - modules for the amygdala, hippocampus, and hypothalamus These modules evaluate potential missegmentations of the amygdala, hippocampus, and hypothalamus. To assess segmentation quality, screenshots will be created These modules require prior processing of the MR images with FreeSurfer's dedicated toolboxes for the segmentation of the amygdala and hippocampus, and the hypothalamus, respectively. - shape module The shape module will run a shapeDNA / brainprint analysis to compute distances of shape descriptors between lateralized brain structures. This can be used to identify discrepancies and irregularities between pairs of corresponding structures. The results will be included in the main csv table, and the output directory will also contain a 'brainprint' subdirectory. - outlier module This is a module to detect extreme values among the subcortical ('aseg') segmentations as well as the cortical parcellations. If present, hypothalamic and hippocampal subsegmentations will also be included. The outlier detection is based on comparisons with the distributions of the sample as well as normative values taken from the literature (see References). For comparisons with the sample distributions, extreme values are defined in two ways: nonparametrically, i.e. values that are 1.5 times the interquartile range below or above the 25th or 75th percentile of the sample, respectively, and parametrically, i.e. values that are more than 2 standard deviations above or below the sample mean. Note that a minimum of 10 supplied subjects is required for running these analyses, otherwise `NaNs` will be returned. For comparisons with the normative values, lower and upper bounds are computed from the 95% prediction intervals of the regression models given in Potvin et al., 2016, and values exceeding these bounds will be flagged. As an alternative, users may specify their own normative values by using the '--outlier-table' argument. This requires a custom csv table with headers `label`, `upper`, and `lower`, where `label` indicates a column of anatomical names. It can be a subset and the order is arbitrary, but naming must exactly match the nomenclature of the 'aseg.stats' and/or '[lr]h.aparc.stats' file. If cortical parcellations are included in the outlier table for a comparison with aparc.stats values, the labels must have a 'lh.' or 'rh.' prefix. `upper` and `lower` are user-specified upper and lower bounds. The main csv table will be appended with the following summary variables, and more detailed output about will be saved as csv tables in the 'outliers' subdirectory of the main output directory. variable | description -------------------------|--------------------------------------------------- n_outliers_sample_nonpar | number of structures that are 1.5 times the IQR above/below the 75th/25th percentile n_outliers_sample_param | number of structures that are 2 SD above/below the mean n_outliers_norms | number of structures exceeding the upper and lower bounds of the normative values ___ ## Development ### Current status We are happy to announce the release of version 2.0 of the fsqc toolbox. With this release comes a change of the project name from `qatools` to `fsqc`, to reflect increased independence from the original FreeSurfer QA tools, and applicability to other neuroimaging analysis packages - such as [Fastsurfer](https://github.com/Deep-MI/FastSurfer). Recent changes include the addition of the hippocampus and hypothalamus modules as well as the addition of surface and skullstrip visualization modules. Technical changes include how the package is installed, imported, and run, see [below](https://github.com/Deep-MI/fsqc#usage) for details. A list of changes is available [here](CHANGES.md). ### Main and development branches This repository contains multiple branches, reflecting the ongoing development of the toolbox. The two primary branches are the main branch (`stable`) and the development branch (`dev`). New features will first be added to the development branch, and eventually be merged with the main branch. ### Roadmap The goal of the `fsqc` project is to create a modular and extensible software package that provides quantitative metrics and visual information for the quality control of FreeSurfer- or Fastsurfer-processed MR images. The package is currently under development, and new features are continuously added. New features will initially be available in the [development branch](https://github.com/Deep-MI/fsqc/tree/dev) of this toolbox and will be included in the [main branch](https://github.com/Deep-MI/fsqc/tree/stable) after a period of testing and evaluation. Unless explicitly announced, all new features will preserve compatibility with earlier versions. Feedback, suggestions, and [contributions](CONTRIBUTINNG.md) are always welcome, preferably via [issues](https://github.com/Deep-MI/fsqc/issues) and [pull requests](https://github.com/Deep-MI/fsqc/pulls). ___ ## Usage ### As a command line tool ``` run_fsqc --subjects_dir <directory> --output_dir <directory> [--subjects SubjectID [SubjectID ...]] [--subjects-file <file>] [--screenshots] [--screenshots-html] [--surfaces] [--surfaces-html] [--skullstrip] [--skullstrip-html] [--fornix] [--fornix-html] [--hippocampus] [--hippocampus-html] [--hippocampus-label ... ] [--hypothalamus] [--hypothalamus-html] [--shape] [--outlier] [--fastsurfer] [--no-group] [--group-only] [--exit-on-error] [--skip-existing] [-h] [--more-help] [...] required arguments: --subjects_dir <directory> subjects directory with a set of Freesurfer- or Fastsurfer-processed individual datasets. --output_dir <directory> output directory optional arguments: --subjects SubjectID [SubjectID ...] list of subject IDs --subjects-file <file> filename of a file with subject IDs (one per line) --screenshots create screenshots of individual brains --screenshots-html create screenshots of individual brains incl. html summary page --surfaces create screenshots of individual brain surfaces --surfaces-html create screenshots of individual brain surfaces and html summary page --skullstrip create screenshots of individual brainmasks --skullstrip-html create screenshots of individual brainmasks and html summary page --fornix check fornix segmentation --fornix-html check fornix segmentation and create html summary page of fornix evaluation --hypothalamus check hypothalamic segmentation --hypothalamus-html check hypothalamic segmentation and create html summary page --hippocampus check segmentation of hippocampus and amygdala --hippocampus-html check segmentation of hippocampus and amygdala and create html summary page --hippocampus-label specify label for hippocampus segmentation files (default: T1.v21). The full filename is then [lr]h.hippoAmygLabels-<LABEL>.FSvoxelSpace.mgz --shape run shape analysis --outlier run outlier detection --outlier-table specify normative values (only in conjunction with --outlier) --fastsurfer use FastSurfer instead of FreeSurfer output --no-group run script in subject-level mode. will compute individual files and statistics, but not create group-level summaries. --group-only run script in group mode. will create group-level summaries from existing inputs --exit-on-error terminate the program when encountering an error; otherwise, try to continue with the next module or case --skip-existing skips processing for a given case if output already exists, even with possibly different parameters or settings getting help: -h, --help display this help message and exit --more-help display extensive help message and exit expert options: --screenshots_base <image> filename of an image that should be used instead of norm.mgz as the base image for the screenshots. Can be an individual file (which would not be appropriate for multi-subject analysis) or can be a file without pathname and with the same filename across subjects within the 'mri' subdirectory of an individual FreeSurfer results directory (which would be appropriate for multi-subject analysis). --screenshots_overlay <image> path to an image that should be used instead of aseg.mgz as the overlay image for the screenshots; can also be none. Can be an individual file (which would not be appropriate for multi-subject analysis) or can be a file without pathname and with the same filename across subjects within the 'mri' subdirectory of an individual FreeSurfer results directory (which would be appropriate for multi-subject analysis). --screenshots_surf <surf> [<surf> ...] one or more surface files that should be used instead of [lr]h.white and [lr]h.pial; can also be none. Can be one or more individual file(s) (which would not be appropriate for multi-subject analysis) or can be a (list of) file(s) without pathname and with the same filename across subjects within the 'surf' subdirectory of an individual FreeSurfer results directory (which would be appropriate for multi-subject analysis). --screenshots_views <view> [<view> ...] one or more views to use for the screenshots in the form of x=<numeric> y=<numeric> and/or z=<numeric>. Order does not matter. Default views are x=-10 x=10 y=0 z=0. --screenshots_layout <rows> <columns> layout matrix for screenshot images. ``` *Examples:* - Run the QC pipeline for all subjects found in `/my/subjects/directory`: ```bash run_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory ``` - Run the QC pipeline for two specific subjects that need to be present in `/my/subjects/directory`: ```bash run_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --subjects mySubjectID1 mySubjectID2 ``` - Run the QC pipeline for all subjects found in `/my/subjects/directory` after full FastSurfer processing: ```bash run_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --fastsurfer ``` - Run the QC pipeline plus the screenshots module for all subjects found in `/my/subjects/directory`: ```bash run_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --screenshots ``` - Run the QC pipeline plus the fornix pipeline for all subjects found in `/my/subjects/directory`: ```bash run_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --fornix ``` - Run the QC pipeline plus the shape analysis pipeline for all subjects found in `/my/subjects/directory`: ```bash run_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --shape ``` - Run the QC pipeline plus the outlier detection module for all subjects found in `/my/subjects/directory`: ```bash run_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --outlier ``` - Run the QC pipeline plus the outlier detection module with a user-specific table of normative values for all subjects found in `/my/subjects/directory`: ```bash run_fsqc --subjects_dir /my/subjects/directory --output_dir /my/output/directory --outlier --outlier-table /my/table/with/normative/values.csv ``` - Note that the `--screenshots`, `--fornix`, `--shape`, and `--outlier` (and other) arguments can also be used in conjunction. ### As a Python package As an alternative to their command-line usage, the fsqc scripts can also be run within a pure Python environment, i.e. installed and imported as a Python package. Use `import fsqc` (or sth. equivalent) to import the package within a Python environment, and use the `run_fsqc` function from the `fsqc` module to run an analysis. In its most basic form: ```python import fsqc fsqc.run_fsqc(subjects_dir='/my/subjects/dir', output_dir='/my/output/dir') ``` Specify subjects as a list: ```python import fsqc fsqc.run_fsqc(subjects_dir='/my/subjects/dir', output_dir='/my/output/dir', subjects=['subject1', 'subject2', 'subject3']) ``` And as a more elaborate example: ```python import fsqc fsqc.run_fsqc(subjects_dir='/my/subjects/dir', output_dir='/my/output/dir', subject_file='/my/subjects/file.txt', screenshots_html=True, surfaces_html=True, skullstrip_html=True, fornix_html=True, hypothalamus_html=True, hippocampus_html=True, hippocampus_label=\"T1.v21\", shape=True, outlier=True) ``` Call `help(fsqc.run_fsqc)` for further usage info and additional options. ### As a Docker image We provide configuration files that can be used to create a Docker or Singularity image for the fsqc scripts. Documentation can be found on the [Docker](docker/Docker.md) and [Singularity](singularity/Singularity.md) pages. ___ ## Installation ### Installation as a Python package Use: ```bash pip install fsqc ``` to install the fsqc package and all of its dependencies. This is the recommended way of installing the package, and allows for both command-line execution and execution as a Python function. We also recommend to do this installation within a Python virtual environment, which can be created and activated as follows: ```bash virtualenv /path/to/my/virtual/environment source /path/to/my/virtual/environment/bin/activate ``` ### Installation from GitHub Use the following code to download, build and install the fsqc package from its GitHub repository into your local Python package directory: ```bash pip install git+https://github.com/deep-mi/fsqc.git ``` This can be useful if you want to install a particular branch - such as the `dev` branch in the following example: ```bash pip install git+https://github.com/deep-mi/fsqc.git@dev ``` ### Download from GitHub This software can also be downloaded from its GitHub repository at `https://github.com/Deep-MI/fsqc`, or cloned directly via `git clone https://github.com/Deep-MI/fsqc`. The `run_fsqc` script will then be executable from the command line, as detailed above. Note, however, that the required dependencies will have to be installed manually. See the [requirements](#requirements) section for instructions. ___ ## Requirements - At least one structural MR image that was processed with Freesurfer 6.0, 7.x, or FastSurfer 1.1 or later (including the surface pipeline). - A Python version >= 3.8 is required to run this script. - Required packages include (among others) the nibabel and skimage package for the core functionality, plus the matplotlib, pandas, and transform3d packages for some optional functions and modules. See the `requirements.txt` file for a complete list. Use `pip install -r requirements.txt` to install these packages. - If installing the toolbox as a Python package or if using the Docker image, all required packages will be installed automatically and manual installation as detailed above will not be necessary. - This software has been tested on Ubuntu 20.04 and 22.04. - A working [FreeSurfer](https://freesurfer.net) installation (version 6 or newer) is required for running the 'shape' module of this toolbox. Also make sure that FreeSurfer is sourced (i.e., `FREESURFER_HOME` is set as an environment variable) before running an analysis. ___ ## Known issues - Aborted / restarted recon-all runs: the program will analyze recon-all logfiles, and may fail or return erroneous results if the logfile is appended by multiple restarts of recon-all runs. Ideally, the logfile should therefore consist of just a single, successful recon-all run. ___ ## Authors - fsqc toolbox: Kersten Diers, Tobias Wolff, and Martin Reuter. - Freesurfer QA Tools: David Koh, Stephanie Lee, Jenni Pacheco, Vasanth Pappu, and Louis Vinke. - lapy and brainprint toolboxes: Martin Reuter. ___ ## Citations - Esteban O, Birman D, Schaer M, Koyejo OO, Poldrack RA, Gorgolewski KJ; 2017; MRIQC: Advancing the Automatic Prediction of Image Quality in MRI from Unseen Sites; PLOS ONE 12(9):e0184661; doi:10.1371/journal.pone.0184661. - Wachinger C, Golland P, Kremen W, Fischl B, Reuter M; 2015; BrainPrint: a Discriminative Characterization of Brain Morphology; Neuroimage: 109, 232-248; doi:10.1016/j.neuroimage.2015.01.032. - Reuter M, Wolter FE, Shenton M, Niethammer M; 2009; Laplace-Beltrami Eigenvalues and Topological Features of Eigenfunctions for Statistical Shape Analysis; Computer-Aided Design: 41, 739-755; doi:10.1016/j.cad.2009.02.007. - Potvin O, Mouiha A, Dieumegarde L, Duchesne S, & Alzheimer's Disease Neuroimaging Initiative; 2016; Normative data for subcortical regional volumes over the lifetime of the adult human brain; Neuroimage: 137, 9-20; doi.org/10.1016/j.neuroimage.2016.05.016 ___ ## License This software is licensed under the MIT License, see associated LICENSE file for details. Copyright (c) 2019 Image Analysis Group, DZNE e.V.\n",
                "dependencies": "[build-system] requires = ['setuptools >= 61.0.0'] build-backend = 'setuptools.build_meta' [project] name = 'fsqc' description = 'Quality control scripts for FastSurfer and FreeSurfer structural MRI data' license = {file = 'LICENSE'} requires-python = '>=3.9' authors = [ {name = 'Kersten Diers', email = 'kersten.diers@dzne.de'}, {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'} ] maintainers = [ {name = 'Kersten Diers', email = 'kersten.diers@dzne.de'}, {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'} ] keywords = [ 'FastSurfer', 'FreeSurfer', 'Quality control', 'Quality assurance', ] classifiers = [ 'Operating System :: Unix', 'Operating System :: MacOS', 'Programming Language :: Python :: 3 :: Only', 'Programming Language :: Python :: 3.9', 'Programming Language :: Python :: 3.10', 'Programming Language :: Python :: 3.11', 'Programming Language :: Python :: 3.12', 'Natural Language :: English', 'License :: OSI Approved :: MIT License', 'Intended Audience :: Science/Research', ] dynamic = [\"version\", \"readme\", \"dependencies\"] [project.optional-dependencies] build = [ 'build', 'twine', ] doc = [ 'furo!=2023.8.17', 'matplotlib', 'memory-profiler', 'numpydoc', 'sphinx!=7.2.*', 'sphinxcontrib-bibtex', 'sphinx-copybutton', 'sphinx-design', 'sphinx-gallery', 'sphinx-issues', 'pypandoc', 'nbsphinx', 'IPython', # For syntax highlighting in notebooks 'ipykernel', ] style = [ 'bibclean', 'codespell', 'pydocstyle[toml]', 'ruff', ] test = [ 'pytest', 'pytest-cov', 'pytest-timeout', ] all = [ 'fsqc[build]', 'fsqc[doc]', 'fsqc[style]', ] full = [ 'fsqc[all]', ] [project.urls] homepage = 'https://github.com/Deep-MI/fsqc' documentation = 'https://github.com/Deep-MI/fsqc' source = 'https://github.com/Deep-MI/fsqc' tracker = 'https://github.com/Deep-MI/fsqc/issues' [project.scripts] run_fsqc = 'fsqc.cli:main' fsqc-sys_info = 'fsqc.commands.sys_info:run' [tool.setuptools.dynamic] version = {file = 'VERSION'} readme = {file = 'DESCRIPTION.md', content-type = \"text/markdown\"} dependencies = {file = 'requirements.txt'} #[tool.setuptools] ### probably not needed #include-package-data = false [tool.setuptools.packages.find] include = ['fsqc', 'fsqc.cli', 'fsqc.commands', 'fsqc.utils'] exclude = ['docker', 'singularity'] [tool.pydocstyle] convention = 'numpy' ignore-decorators = '(copy_doc|property|.*setter|.*getter|pyqtSlot|Slot)' match = '^(?!setup|__init__|test_).*\\.py' match-dir = '^fsqc.*' add_ignore = 'D100,D104,D107' [tool.ruff] line-length = 88 extend-exclude = [ \".github\", \"doc\", \"docker\", \"setup.py\", \"singularity\", ] ignore = [\"E501\"] # line too long (should be enforced soon) [tool.ruff.lint] # https://docs.astral.sh/ruff/linter/#rule-selection select = [ \"E\", # pycodestyle \"F\", # Pyflakes \"UP\", # pyupgrade \"B\", # flake8-bugbear \"I\", # isort # \"SIM\", # flake8-simplify ] [tool.ruff.lint.per-file-ignores] \"__init__.py\" = [\"F401\"] [tool.pytest.ini_options] minversion = '6.0' filterwarnings = [] addopts = [ \"--import-mode=importlib\", \"--junit-xml=junit-results.xml\", \"--durations=20\", \"--verbose\", ] [tool.coverage.run] branch = true cover_pylib = false omit = [ '**/__init__.py', '**/fsqc/_version.py', '**/fsqc/commands/*', '**/tests/**', ] [tool.coverage.report] exclude_lines = [ 'pragma: no cover', 'if __name__ == .__main__.:', ] precision = 2\nbrainprint==0.4.0 lapy>=1.0.0,<2 kaleido matplotlib nibabel numpy pandas scipy scikit-image transforms3d\n# https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html # If compatibility with legacy builds or versions of tools that don’t support # certain packaging standards (e.g. PEP 517 or PEP 660), a simple setup.py # script can be added to your project [1] (while keeping the configuration in # pyproject.toml): from setuptools import setup setup()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/gasnetsim",
            "repo_link": "https://jugit.fz-juelich.de/iek-10/public/simulation/gasnetsim",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ginkgo",
            "repo_link": "https://github.com/ginkgo-project/ginkgo/",
            "content": {
                "codemeta": "",
                "readme": "<p align=\"center\"><img src=\"/assets/logo.png\" alt=\"Ginkgo\" width=\"60%\" height=\"60%\"></p> <div align=\"center\"> [![License](https://img.shields.io/github/license/ginkgo-project/ginkgo.svg)](./LICENSE)|[![c++ standard](https://img.shields.io/badge/c%2B%2B-17-blue.svg)](https://en.wikipedia.org/wiki/C%2B%2B#Standardization)|[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://ginkgo-project.github.io/ginkgo-generated-documentation/doc/develop/)|[![DOI](https://joss.theoj.org/papers/10.21105/joss.02260/status.svg)](https://doi.org/10.21105/joss.02260) |:-:|:-:|:-:|:-:| [![Build status](https://gitlab.com/ginkgo-project/ginkgo-public-ci/badges/develop/pipeline.svg)](https://gitlab.com/ginkgo-project/ginkgo-public-ci/-/pipelines?page=1&scope=branches&ref=develop)|[![OSX-build](https://github.com/ginkgo-project/ginkgo/actions/workflows/osx.yml/badge.svg)](https://github.com/ginkgo-project/ginkgo/actions/workflows/osx.yml)|[![codecov](https://codecov.io/gh/ginkgo-project/ginkgo/branch/develop/graph/badge.svg)](https://codecov.io/gh/ginkgo-project/ginkgo)|[![Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=ginkgo-project_ginkgo&metric=sqale_rating)](https://sonarcloud.io/dashboard?id=ginkgo-project_ginkgo)|[![Reliability Rating](https://sonarcloud.io/api/project_badges/measure?project=ginkgo-project_ginkgo&metric=reliability_rating)](https://sonarcloud.io/dashboard?id=ginkgo-project_ginkgo)|[![CDash dashboard](https://img.shields.io/badge/CDash-Access-blue.svg)](https://my.cdash.org/index.php?project=Ginkgo+Project) |:-:|:-:|:-:|:-:|:-:|:-:| </div> Ginkgo is a high-performance numerical linear algebra library for many-core systems, with a focus on solution of sparse linear systems. It is implemented using modern C++ (you will need an at least C++17 compliant compiler to build it), with GPU kernels implemented for NVIDIA, AMD and Intel GPUs. --- **[Prerequisites](#prerequisites)** | **[Building Ginkgo](#building-and-installing-ginkgo)** | **[Tests, Examples, Benchmarks](#tests-examples-and-benchmarks)** | **[Bug reports](#bug-reports-and-support)** | **[Licensing](#licensing)** | **[Contributing](#contributing-to-ginkgo)** | **[Citing](#citing-ginkgo)** # Prerequisites ### Linux and Mac OS For Ginkgo core library: * _cmake 3.16+_ * C++17 compliant compiler, one of: * _gcc 7+_ * _clang 5+_ * _Intel compiler 2019+_ * _Apple Clang 15.0_ is tested. Earlier versions might also work. * _Cray Compiler 14.0.1+_ * _NVHPC Compiler 22.7+_ The Ginkgo CUDA module has the following __additional__ requirements: * _cmake 3.18+_ (If CUDA was installed through the NVIDIA HPC Toolkit, we require _cmake 3.22+_) * _CUDA 11.0+_ or _NVHPC Package 22.7+_ * Any host compiler restrictions your version of CUDA may impose also apply here. For the newest CUDA version, this information can be found in the [CUDA installation guide for Linux](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) or [CUDA installation guide for Mac Os X](https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html) The Ginkgo HIP module has the following __additional__ requirements: * _ROCm 4.5+_ * the HIP, hipBLAS, hipSPARSE, hip/rocRAND and rocThrust packages compiled with the ROCm backend * if the hipFFT package is available, it is used to implement the FFT LinOps. * _cmake 3.21+_ The Ginkgo DPC++(SYCL) module has the following __additional__ requirements: * _oneAPI 2023.1+_ * Set `dpcpp` or `icpx` as the `CMAKE_CXX_COMPILER` * The following oneAPI packages should be available: * oneMKL * oneDPL The Ginkgo MPI module has the following __additional__ requirements: * MPI 3.1+, ideally GPU-Aware, for best performance In addition, if you want to contribute code to Ginkgo, you will also need the following: * _clang-format 14_ (downloaded automatically by `pre-commit`) * _clang-tidy_ (optional, when setting the flag `-DGINKGO_WITH_CLANG_TIDY=ON`) * _iwyu_ (Include What You Use, optional, when setting the flag `-DGINKGO_WITH_IWYU=ON`) ### Windows * _cmake 3.16+_ * C++17 compliant 64-bit compiler: * _MinGW : gcc 7+_ * _Microsoft Visual Studio : VS 2019+_ The Ginkgo CUDA module has the following __additional__ requirements: * _CUDA 11.0+_ * _Microsoft Visual Studio_ * Any host compiler restrictions your version of CUDA may impose also apply here. For the newest CUDA version, this information can be found in the [CUDA installation guide for Windows](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html) The Ginkgo OMP module has the following __additional__ requirements: * _MinGW_ In these environments, two problems can be encountered, the solution for which is described in the [windows section in INSTALL.md](INSTALL.md#building-ginkgo-in-windows): * `ld: error: export ordinal too large` needs the compilation flag `-O1` * `cc1plus.exe: out of memory allocating 65536 bytes` requires a modification of the environment __NOTE:__ Some restrictions will also apply on the version of C and C++ standard libraries installed on the system. This needs further investigation. # Building and Installing Ginkgo To build Ginkgo, you can use the standard CMake procedure. ```sh mkdir build; cd build cmake -G \"Unix Makefiles\" .. && cmake --build . cmake --install . ``` By default, `GINKGO_BUILD_REFERENCE` is enabled. You should be able to run examples with this executor. By default, Ginkgo tries to enable the relevant modules depending on your machine environment (present of CUDA, ...). You can also explicitly compile with the OpenMP, CUDA, HIP or DPC++(SYCL) modules enabled to run the examples with these executors. Please refer to the [Installation page](./INSTALL.md) for more details. Tip: After installation, in your CMake project, Ginkgo can be added with `find_package(Ginkgo)` and the the target that is exported is `Ginkgo::ginkgo`. An example can be found in the [`test_install`](test/test_install/CMakeLists.txt). # Tests, Examples and Benchmarks ### Testing Ginkgo does comprehensive unit tests using Google Tests. These tests are enabled by default and can be disabled if necessary by passing the `-DGINKGO_BUILD_TESTS=NO` to the cmake command. More details about running tests can be found in the [TESTING.md page](./TESTING.md). ### Running examples Various examples are available for you to understand and play with Ginkgo within the `examples/` directory. They can be compiled by passing the `-DGINKGO_BUILD_EXAMPLES=ON` to the cmake command. Each of the examples have commented code with explanations and this can be found within the [online documentation](https://ginkgo-project.github.io/ginkgo-generated-documentation/doc/develop/Examples.html). ### Benchmarking A unique feature of Ginkgo is the ability to run benchmarks and view your results with the help of the [Ginkgo Performance Explorer (GPE)](https://ginkgo-project.github.io/gpe/). More details about this can be found in the [BENCHMARKING.md page](./BENCHMARKING.md) # Bug reports and Support If you have any questions about using Ginkgo, please use [Github discussions](https://github.com/ginkgo-project/ginkgo/discussions). If you would like to request a feature, or have encountered a bug, please [create an issue](https://github.com/ginkgo-project/ginkgo/issues/new). Please be sure to describe your problem and provide as much information as possible. You can also send an email to [Ginkgo's main email address](mailto:ginkgo.library@gmail.com). # Licensing Ginkgo is available under the [3-clause BSD license](LICENSE). All contributions to the project are added under this license. Depending on the configuration options used when building Ginkgo, third party software may be pulled as additional dependencies, which have their own licensing conditions. Refer to [ABOUT-LICENSING.md](ABOUT-LICENSING.md) for details. # Contributing to Ginkgo We are glad that that you would like to contribute to Ginkgo and we are happy to help you with any questions you may have. If you are contributing for the first time, please add yourself to the list of external contributors with the following info ``` text Name Surname <email@domain> Institution(s) ``` #### Declaration Ginkgo's source is distributed with a BSD-3 clause license. By contributing to Ginkgo and adding yourself to the contributors list, you agree to the following statement (also in [contributors.txt](contributors.txt)): ``` text I hereby place all my contributions in this codebase under a BSD-3-Clause license, as specified in the repository's LICENSE file. ``` #### Contribution Guidelines When contributing to Ginkgo, to ease the review process, please follow the guidelines mentioned in [CONTRIBUTING.md](CONTRIBUTING.md). It also contains other general recommendations such as writing proper commit messages, understanding Ginkgo's library design, relevant C++ information etc. # Citing Ginkgo The main Ginkgo paper describing Ginkgo's purpose, design and interface is available through the following reference: ``` bibtex @article{ginkgo-toms-2022, title = {{Ginkgo: A Modern Linear Operator Algebra Framework for High Performance Computing}}, volume = {48}, copyright = {All rights reserved}, issn = {0098-3500}, shorttitle = {Ginkgo}, url = {https://doi.org/10.1145/3480935}, doi = {10.1145/3480935}, number = {1}, urldate = {2022-02-17}, journal = {ACM Transactions on Mathematical Software}, author = {Anzt, Hartwig and Cojean, Terry and Flegar, Goran and Göbel, Fritz and Grützmacher, Thomas and Nayak, Pratik and Ribizel, Tobias and Tsai, Yuhsiang Mike and Quintana-Ortí, Enrique S.}, month = feb, year = {2022}, keywords = {ginkgo, healthy software lifecycle, High performance computing, multi-core and manycore architectures}, pages = {2:1--2:33} } ``` For more information on topical subjects, please refer to the [CITING.md page](CITING.md).\n",
                "dependencies": "cmake_minimum_required(VERSION 3.16) project( Ginkgo LANGUAGES CXX VERSION 1.10.0 DESCRIPTION \"A numerical linear algebra library targeting many-core architectures\" ) set(Ginkgo_VERSION_TAG \"develop\") set(PROJECT_VERSION_TAG ${Ginkgo_VERSION_TAG}) if(Ginkgo_VERSION_TAG STREQUAL \"master\") set(GINKGO_VERSION_TAG_DEPRECATED ON) else() set(GINKGO_VERSION_TAG_DEPRECATED OFF) endif() if(GINKGO_VERSION_TAG_DEPRECATED) message( WARNING \"The branch ${Ginkgo_VERSION_TAG} is deprecated and will stop receiving updates after 2025. \" \"Please use the main branch for the latest release, or the develop branch for the latest development updates.\" ) endif() # Cuda and Hip also look for Threads. Set it before any find_package to ensure the Threads setting is not changed. set(THREADS_PREFER_PTHREAD_FLAG ON) # Determine which modules can be compiled include(cmake/autodetect_executors.cmake) list(APPEND CMAKE_MODULE_PATH \"${PROJECT_SOURCE_DIR}/cmake/Modules/\") include(cmake/autodetect_system_libs.cmake) # rename helper include(cmake/rename.cmake) # Ginkgo configuration options option(GINKGO_DEVEL_TOOLS \"Add development tools to the build system\" OFF) option(GINKGO_BUILD_TESTS \"Generate build files for unit tests\" ON) option(GINKGO_BUILD_EXAMPLES \"Build Ginkgo's examples\" ON) option(GINKGO_BUILD_BENCHMARKS \"Build Ginkgo's benchmarks\" ON) option(GINKGO_BUILD_REFERENCE \"Compile reference CPU kernels\" ON) option(GINKGO_BUILD_OMP \"Compile OpenMP kernels for CPU\" ${GINKGO_HAS_OMP}) option(GINKGO_BUILD_MPI \"Compile the MPI module\" ${GINKGO_HAS_MPI}) gko_rename_cache(GINKGO_BUILD_DPCPP GINKGO_BUILD_SYCL BOOL \"Compile SYCL kernels for Intel GPUs or other SYCL enabled hardware\") option( GINKGO_BUILD_SYCL \"Compile SYCL kernels for Intel GPUs or other SYCL enabled hardware\" ${GINKGO_HAS_SYCL} ) option(GINKGO_BUILD_CUDA \"Compile kernels for NVIDIA GPUs\" ${GINKGO_HAS_CUDA}) option( GINKGO_BUILD_HIP \"Compile kernels for AMD or NVIDIA GPUs\" ${GINKGO_HAS_HIP} ) option(GINKGO_BUILD_DOC \"Generate documentation\" OFF) option( GINKGO_FAST_TESTS \"Reduces the input size for a few tests known to be time-intensive\" OFF ) option( GINKGO_TEST_NONDEFAULT_STREAM \"Uses non-default streams in CUDA and HIP tests\" OFF ) option( GINKGO_MIXED_PRECISION \"Instantiate true mixed-precision kernels (otherwise they will be conversion-based using implicit temporary storage)\" OFF ) option(GINKGO_ENABLE_HALF \"Enable the use of half precision\" ON) # We do not support half precision in MSVC and msys2 (MINGW). if(MSVC OR MINGW) message(STATUS \"We do not support half precision in MSVC and MINGW.\") set(GINKGO_ENABLE_HALF OFF CACHE BOOL \"Enable the use of half precision\" FORCE ) endif() option( GINKGO_SKIP_DEPENDENCY_UPDATE \"Do not update dependencies each time the project is rebuilt\" ON ) option( GINKGO_WITH_CLANG_TIDY \"Make Ginkgo call `clang-tidy` to find programming issues.\" OFF ) option( GINKGO_WITH_IWYU \"Make Ginkgo call `iwyu` (Include What You Use) to find include issues.\" OFF ) option( GINKGO_WITH_CCACHE \"Use ccache if available to speed up C++ and CUDA rebuilds by caching compilations.\" ON ) option( GINKGO_CHECK_CIRCULAR_DEPS \"Enable compile-time checks detecting circular dependencies between libraries and non-self-sufficient headers.\" OFF ) option( GINKGO_CONFIG_LOG_DETAILED \"Enable printing of detailed configuration log to screen in addition to the writing of files,\" OFF ) option( GINKGO_BENCHMARK_ENABLE_TUNING \"Enable tuning variables in the benchmarks. For specific use cases, manual code changes could be required.\" OFF ) set(GINKGO_VERBOSE_LEVEL \"1\" CACHE STRING \"Verbosity level. Put 0 to turn off. 1 activates a few important messages.\" ) set(GINKGO_CUDA_ARCHITECTURES \"Auto\" CACHE STRING \"A list of target NVIDIA GPU architectures. See README.md for more detail.\" ) # the details of fine/coarse grain memory and unsafe atomic are available https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#floating-point-fp-atomic-operations-and-coarse-fine-grained-memory-allocations option( GINKGO_HIP_AMD_UNSAFE_ATOMIC \"Compiler uses unsafe floating point atomic (only for AMD GPU and ROCM >= 5). Default is ON because we use hipMalloc, which is always on coarse grain. Must turn off when allocating memory on fine grain\" ON ) option( GINKGO_SPLIT_TEMPLATE_INSTANTIATIONS \"Split template instantiations for slow-to-compile files. This improves parallel build performance\" ON ) mark_as_advanced(GINKGO_SPLIT_TEMPLATE_INSTANTIATIONS) option( GINKGO_JACOBI_FULL_OPTIMIZATIONS \"Use all the optimizations for the CUDA Jacobi algorithm\" OFF ) option(BUILD_SHARED_LIBS \"Build shared (.so, .dylib, .dll) libraries\" ON) option(GINKGO_BUILD_HWLOC \"Build Ginkgo with HWLOC. Default is OFF.\" OFF) option( GINKGO_BUILD_PAPI_SDE \"Build Ginkgo with PAPI SDE. Enabled if a system installation is found.\" ${PAPI_SDE_FOUND} ) option( GINKGO_DPCPP_SINGLE_MODE \"Do not compile double kernels for the DPC++ backend.\" OFF ) option(GINKGO_INSTALL_RPATH \"Set the RPATH when installing its libraries.\" ON) option( GINKGO_INSTALL_RPATH_ORIGIN \"Add $ORIGIN (Linux) or @loader_path (MacOS) to the installation RPATH.\" ON ) option( GINKGO_INSTALL_RPATH_DEPENDENCIES \"Add dependencies to the installation RPATH.\" OFF ) option( GINKGO_FORCE_GPU_AWARE_MPI \"Assert that the MPI library is GPU aware. This forces Ginkgo to assume that GPU aware functionality is available (OFF (default) or ON), but may fail catastrophically in case the MPI implementation is not GPU Aware, and GPU aware functionality has been forced\" OFF ) set(GINKGO_CI_TEST_OMP_PARALLELISM \"4\" CACHE STRING \"The number of OpenMP threads to use for a test binary during CTest resource file-constrained test.\" ) option( GINKGO_EXTENSION_KOKKOS_CHECK_TYPE_ALIGNMENT \"Enables mapping to Kokkos types to check the alignment of the source and target type.\" ON ) gko_rename_cache(GINKGO_COMPILER_FLAGS CMAKE_CXX_FLAGS BOOL \"Flags used by the CXX compiler during all build types.\") gko_rename_cache(GINKGO_CUDA_COMPILER_FLAGS CMAKE_CUDA_FLAGS BOOL \"Flags used by the CUDA compiler during all build types.\") # load executor-specific configuration if(GINKGO_BUILD_CUDA) include(cmake/cuda.cmake) if(CUDAToolkit_VERSION VERSION_LESS 11.6) message( STATUS \"Disable custom thrust namespace for cuda before 11.6 because it has no effect in the thrust shipped by cuda before 11.6\" ) set(GINKGO_CUDA_CUSTOM_THRUST_NAMESPACE OFF) else() message(STATUS \"Enable custom thrust namespace for cuda\") set(GINKGO_CUDA_CUSTOM_THRUST_NAMESPACE ON) endif() endif() if(GINKGO_BUILD_HIP) include(cmake/hip.cmake) if(GINKGO_HIP_PLATFORM_AMD AND GINKGO_HIP_VERSION VERSION_LESS 5.7) # Hip allow custom namespace but does not fully make everything in the custom namespace before rocm-5.7 # more specific pr: https://github.com/ROCm/rocThrust/pull/286 message( STATUS \"Disable custom thrust namespace for hip before 5.7 because hip does not fully support it before 5.7\" ) set(GINKGO_HIP_CUSTOM_THRUST_NAMESPACE OFF) else() message(STATUS \"Enable custom thrust namespace for hip\") set(GINKGO_HIP_CUSTOM_THRUST_NAMESPACE ON) endif() endif() if(GINKGO_BUILD_SYCL) include(cmake/sycl.cmake) endif() if(GINKGO_BUILD_OMP) find_package(OpenMP 3.0 REQUIRED) endif() find_package(Threads REQUIRED) include(cmake/build_type_helpers.cmake) # Load other CMake helpers include(cmake/build_helpers.cmake) include(cmake/install_helpers.cmake) include(cmake/compiler_features.cmake) include(cmake/generate_ginkgo_hpp.cmake) if(MSVC) set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /bigobj\") endif() if(MINGW OR CYGWIN) if(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\") # Otherwise, dynamic_cast to the class marked by final will be failed. # https://reviews.llvm.org/D154658 should be relevant set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fno-assume-unique-vtables\") else() set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wa,-mbig-obj\") endif() endif() # For now, PGI/NVHPC nvc++ compiler doesn't seem to support # `#pragma omp declare reduction` # # The math with optimization level -O2 doesn't follow IEEE standard, so we # enable that back as well. if(CMAKE_CXX_COMPILER_ID MATCHES \"PGI|NVHPC\") set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Kieee\") endif() set(GINKGO_CIRCULAR_DEPS_FLAGS \"-Wl,--no-undefined\") # Use ccache as compilation launcher if(GINKGO_WITH_CCACHE) find_program(CCACHE_PROGRAM ccache) if(CCACHE_PROGRAM) set(CMAKE_CXX_COMPILER_LAUNCHER \"${CCACHE_PROGRAM}\") if(GINKGO_BUILD_CUDA) set(CMAKE_CUDA_COMPILER_LAUNCHER \"${CCACHE_PROGRAM}\") endif() endif() endif() if(GINKGO_BENCHMARK_ENABLE_TUNING) # In this state, the tests and examples cannot be compiled without extra # complexity/intrusiveness, so we simply disable them. set(GINKGO_BUILD_TESTS OFF) set(GINKGO_BUILD_EXAMPLES OFF) endif() if(GINKGO_BUILD_TESTS) message(STATUS \"GINKGO_BUILD_TESTS is ON, enabling GINKGO_BUILD_REFERENCE\") set(GINKGO_BUILD_REFERENCE ON CACHE BOOL \"Compile reference CPU kernels\" FORCE ) endif() if(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES) message(STATUS \"Setting build type to 'Release' as none was specified.\") set(CMAKE_BUILD_TYPE Release CACHE STRING \"Choose the type of build.\" FORCE) endif() # Ensure we have a debug postfix if(NOT DEFINED CMAKE_DEBUG_POSTFIX) set(CMAKE_DEBUG_POSTFIX \"d\") endif() if(GINKGO_BUILD_TESTS) # Configure CTest configure_file( ${CMAKE_CURRENT_LIST_DIR}/cmake/CTestCustom.cmake.in ${CMAKE_CURRENT_BINARY_DIR}/CTestCustom.cmake @ONLY ) # For testing, we need some special matrices add_subdirectory(matrices) enable_testing() include(CTest) add_custom_target(quick_test \"${CMAKE_CTEST_COMMAND}\" -R 'core|reference') endif() if(GINKGO_WITH_CLANG_TIDY) find_program(GINKGO_CLANG_TIDY_PATH clang-tidy) endif() if(GINKGO_WITH_IWYU) find_program(GINKGO_IWYU_PATH iwyu) endif() # Find important header files, store the definitions in # include/ginkgo/config.h.in For details, see # https://gitlab.kitware.com/cmake/community/wikis/doc/tutorials/How-To-Write-Platform-Checks include(CheckIncludeFileCXX) check_include_file_cxx(cxxabi.h GKO_HAVE_CXXABI_H) # Automatically find TAU set(GINKGO_HAVE_TAU 0) find_package(PerfStubs QUIET) if(PerfStubs_FOUND) set(GINKGO_HAVE_TAU 1) endif() # Automatically find VTune set(GINKGO_HAVE_VTUNE 0) find_package(VTune) if(VTune_FOUND) set(GINKGO_HAVE_VTUNE 1) endif() # Automatically find METIS set(GINKGO_HAVE_METIS 0) find_package(METIS) if(METIS_FOUND) set(GINKGO_HAVE_METIS 1) endif() # Automatically detect ROCTX (see hip.cmake) set(GINKGO_HAVE_ROCTX 0) if(GINKGO_BUILD_HIP AND ROCTX_FOUND) set(GINKGO_HAVE_ROCTX 1) endif() # Switch off HWLOC for Windows and MacOS if(GINKGO_BUILD_HWLOC AND (MSVC OR WIN32 OR CYGWIN OR APPLE)) set(GINKGO_BUILD_HWLOC OFF CACHE BOOL \"Build Ginkgo with HWLOC. Default is OFF. Ginkgo does not support HWLOC on Windows/MacOS\" FORCE ) message( WARNING \"Ginkgo does not support HWLOC on Windows/MacOS, switch GINKGO_BUILD_HWLOC to OFF\" ) endif() set(GINKGO_HAVE_GPU_AWARE_MPI OFF) set(GINKGO_HAVE_OPENMPI_PRE_4_1_X OFF) if(GINKGO_BUILD_MPI) find_package(MPI 3.1 COMPONENTS CXX REQUIRED) if(GINKGO_FORCE_GPU_AWARE_MPI) set(GINKGO_HAVE_GPU_AWARE_MPI ON) else() set(GINKGO_HAVE_GPU_AWARE_MPI OFF) endif() # use try_compile instead of try_run to prevent cross-compiling issues try_compile( uses_openmpi ${Ginkgo_BINARY_DIR} ${Ginkgo_SOURCE_DIR}/cmake/openmpi_test.cpp COMPILE_DEFINITIONS -DCHECK_HAS_OPEN_MPI=1 LINK_LIBRARIES MPI::MPI_CXX ) if(uses_openmpi) try_compile( valid_openmpi_version ${Ginkgo_BINARY_DIR} ${Ginkgo_SOURCE_DIR}/cmake/openmpi_test.cpp COMPILE_DEFINITIONS -DCHECK_OPEN_MPI_VERSION=1 LINK_LIBRARIES MPI::MPI_CXX ) if(NOT valid_openmpi_version) message( WARNING \"OpenMPI v4.0.x has several bugs that forces us to use non-optimal communication in our distributed \" \"matrix class. To enable faster, non-blocking communication, consider updating your OpenMPI version or \" \"switch to a different vendor.\" ) set(GINKGO_HAVE_OPENMPI_PRE_4_1_X ON) endif() unset(valid_openmpi_version) endif() unset(uses_openmpi) endif() # Try to find the third party packages before using our subdirectories if(GINKGO_BUILD_TESTS) find_package(GTest 1.10.0) # No need for QUIET as CMake ships FindGTest endif() if(GINKGO_BUILD_BENCHMARKS) find_package(gflags 2.2.2 QUIET) endif() if(GINKGO_BUILD_TESTS OR GINKGO_BUILD_BENCHMARKS OR GINKGO_BUILD_EXAMPLES) find_package(nlohmann_json 3.9.1 QUIET) endif() # System provided, third party libraries (not bundled!) set(GINKGO_HAVE_HWLOC 0) if(GINKGO_BUILD_HWLOC) find_package(HWLOC 2.1 REQUIRED) set(GINKGO_HAVE_HWLOC 1) message( WARNING \"The GINKGO_BUILD_HWLOC option has no beneficial effect. Consider setting it to GINKGO_BUILD_HWLOC=OFF.\" ) endif() set(GINKGO_HAVE_PAPI_SDE 0) if(GINKGO_BUILD_PAPI_SDE) find_package(PAPI 7.0.1.0 COMPONENTS sde) if(PAPI_SDE_FOUND) set(GINKGO_HAVE_PAPI_SDE 1) else() message( WARNING \"PAPI (SDE) could not be found. PAPI_SDE support will be disabled.\" ) set(GINKGO_BUILD_PAPI_SDE OFF CACHE BOOL \"PAPI_SDE support was disabled because a system package could not be found.\" FORCE ) endif() endif() # Bundled third party libraries add_subdirectory(third_party) # Third-party tools and libraries if(MSVC) if(BUILD_SHARED_LIBS) set(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS TRUE) else() set(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS FALSE) endif() endif() if(GINKGO_BUILD_SYCL) ginkgo_extract_dpcpp_version(${CMAKE_CXX_COMPILER} GINKGO_DPCPP_MAJOR_VERSION __LIBSYCL_MAJOR_VERSION) ginkgo_extract_dpcpp_version(${CMAKE_CXX_COMPILER} GINKGO_DPCPP_MINOR_VERSION __LIBSYCL_MINOR_VERSION) ginkgo_extract_dpcpp_version(${CMAKE_CXX_COMPILER} GINKGO_DPCPP_VERSION __SYCL_COMPILER_VERSION) else() set(GINKGO_DPCPP_MAJOR_VERSION \"0\") set(GINKGO_DPCPP_MINOR_VERSION \"0\") endif() ginkgo_generate_ginkgo_hpp() configure_file( ${Ginkgo_SOURCE_DIR}/include/ginkgo/config.hpp.in ${Ginkgo_BINARY_DIR}/include/ginkgo/config.hpp @ONLY ) configure_file( ${Ginkgo_SOURCE_DIR}/include/ginkgo/extensions/kokkos/config.hpp.in ${Ginkgo_BINARY_DIR}/include/ginkgo/extensions/kokkos/config.hpp @ONLY ) # Ginkgo core libraries # Needs to be first in order for `CMAKE_CUDA_DEVICE_LINK_EXECUTABLE` to be # propagated to the other parts of Ginkgo in case of building as static libraries add_subdirectory(devices) # Basic device functionalities. Always compiled. add_subdirectory(common) # Import list of unified kernel source files if(GINKGO_BUILD_CUDA) add_subdirectory(cuda) # High-performance kernels for NVIDIA GPUs endif() if(GINKGO_BUILD_REFERENCE) add_subdirectory(reference) # Reference kernel implementations endif() if(GINKGO_BUILD_HIP) add_subdirectory(hip) # High-performance kernels for AMD or NVIDIA GPUs endif() if(GINKGO_BUILD_SYCL) add_subdirectory(dpcpp) # High-performance DPC++ kernels endif() if(GINKGO_BUILD_OMP) add_subdirectory(omp) # High-performance omp kernels endif() add_subdirectory(core) # Core Ginkgo types and top-level functions add_subdirectory(include) # Public API self-contained check if(GINKGO_BUILD_TESTS) add_subdirectory(test) # Tests running on all executors endif() # Non core directories and targets add_subdirectory(extensions) if(GINKGO_BUILD_EXAMPLES) add_subdirectory(examples) endif() if(GINKGO_BUILD_BENCHMARKS) add_subdirectory(benchmark) endif() if(GINKGO_DEVEL_TOOLS) find_program(PRE_COMMIT pre-commit) if(NOT PRE_COMMIT) message( FATAL_ERROR \"The pre-commit command was not found. It is necessary if you want to commit changes to Ginkgo. \" \"If that is not the case, set GINKGO_DEVEL_TOOLS=OFF. \" \"Otherwise install pre-commit via pipx (or pip) using:\\n\" \" pipx install pre-commit\" ) endif() execute_process( COMMAND \"${PRE_COMMIT}\" \"install\" WORKING_DIRECTORY ${Ginkgo_SOURCE_DIR} RESULT_VARIABLE pre-commit-result OUTPUT_VARIABLE pre-commit-output ERROR_VARIABLE pre-commit-error ) if(pre-commit-result) message( FATAL_ERROR \"Failed to install the git hooks via pre-commit. Please check the error message:\\n\" \"${pre-commit-output}\\n${pre-commit-error}\" ) endif() if( pre-commit-output MATCHES \"^Running in migration mode with existing hooks\" ) message( WARNING \"An existing git hook was encountered during `pre-commit install`. The old git hook \" \"will also be executed. Consider removing it with `pre-commit install -f`\" ) elseif(NOT pre-commit-output MATCHES \"^pre-commit installed at\") message( WARNING \"`pre-commit install` did not exit normally. Please check the output message:\\n\" \"${pre-commit-output}\" ) endif() add_custom_target( format COMMAND bash -c \"${PRE_COMMIT} run\" WORKING_DIRECTORY ${Ginkgo_SOURCE_DIR} VERBATIM ) endif() # Installation include(cmake/information_helpers.cmake) ginkgo_pkg_information() ginkgo_git_information() include(cmake/get_info.cmake) if(GINKGO_BUILD_DOC) add_subdirectory(doc) endif() # WINDOWS NVCC has \" inside the string, add escape character # to avoid config problem. ginkgo_modify_flags(CMAKE_CUDA_FLAGS) ginkgo_modify_flags(CMAKE_CUDA_FLAGS_DEBUG) ginkgo_modify_flags(CMAKE_CUDA_FLAGS_RELEASE) ginkgo_install() ginkgo_export_binary_dir() set(GINKGO_TEST_INSTALL_SRC_DIR \"${Ginkgo_SOURCE_DIR}/test/test_install/\") set(GINKGO_TEST_INSTALL_BIN_DIR \"${Ginkgo_BINARY_DIR}/test/test_install/\") set(GINKGO_TEST_EXPORTBUILD_SRC_DIR \"${Ginkgo_SOURCE_DIR}/test/test_exportbuild/\" ) set(GINKGO_TEST_EXPORTBUILD_BIN_DIR \"${Ginkgo_BINARY_DIR}/test/test_exportbuild/\" ) set(GINKGO_TEST_PKGCONFIG_SRC_DIR \"${Ginkgo_SOURCE_DIR}/test/test_pkgconfig/\") set(GINKGO_TEST_PKGCONFIG_BIN_DIR \"${Ginkgo_BINARY_DIR}/test/test_pkgconfig/\") get_property(GINKGO_USE_MULTI_CONFIG GLOBAL PROPERTY GENERATOR_IS_MULTI_CONFIG) # GINKGO_CONFIG_PREFIX contains / in the end. set(GINKGO_CONFIG_PREFIX \"$<$<BOOL:${GINKGO_USE_MULTI_CONFIG}>:$<CONFIG>/>\") set(GINKGO_TEST_INSTALL_CMD ${GINKGO_TEST_INSTALL_BIN_DIR}/${GINKGO_CONFIG_PREFIX}test_install ) set(GINKGO_TEST_EXPORTBUILD_CMD ${GINKGO_TEST_EXPORTBUILD_BIN_DIR}/${GINKGO_CONFIG_PREFIX}test_exportbuild ) set(GINKGO_TEST_PKGCONFIG_CMD ${GINKGO_TEST_PKGCONFIG_BIN_DIR}/${GINKGO_CONFIG_PREFIX}test_pkgconfig ) if(GINKGO_BUILD_CUDA) set(GINKGO_TEST_INSTALL_CUDA_CMD ${GINKGO_TEST_INSTALL_BIN_DIR}/${GINKGO_CONFIG_PREFIX}test_install_cuda ) endif() if(GINKGO_BUILD_HIP) set(GINKGO_TEST_INSTALL_HIP_CMD ${GINKGO_TEST_INSTALL_BIN_DIR}/${GINKGO_CONFIG_PREFIX}test_install_hip ) endif() file(MAKE_DIRECTORY \"${GINKGO_TEST_INSTALL_BIN_DIR}\") file(MAKE_DIRECTORY \"${GINKGO_TEST_EXPORTBUILD_BIN_DIR}\") set(TOOLSET \"\") if(NOT \"${CMAKE_GENERATOR_TOOLSET}\" STREQUAL \"\") set(TOOLSET \"-T${CMAKE_GENERATOR_TOOLSET}\") endif() add_custom_target( test_install COMMAND ${CMAKE_COMMAND} -G${CMAKE_GENERATOR} ${TOOLSET} -S${GINKGO_TEST_INSTALL_SRC_DIR} -B${GINKGO_TEST_INSTALL_BIN_DIR} -DCMAKE_BUILD_TYPE=${CMAKE_BUILD_TYPE} -DCMAKE_PREFIX_PATH=${CMAKE_INSTALL_PREFIX} -DCMAKE_CXX_COMPILER=${CMAKE_CXX_COMPILER} -DCMAKE_CUDA_COMPILER=${CMAKE_CUDA_COMPILER} -DCMAKE_CXX_FLAGS=${CMAKE_CXX_FLAGS} # `--config cfg` is ignored by single-configuration generator. # `$<CONFIG>` is always be the same as `CMAKE_BUILD_TYPE` in # single-configuration generator. COMMAND ${CMAKE_COMMAND} --build ${GINKGO_TEST_INSTALL_BIN_DIR} --config $<CONFIG> COMMAND ${GINKGO_TEST_INSTALL_CMD} COMMAND ${GINKGO_TEST_INSTALL_CUDA_CMD} COMMAND ${GINKGO_TEST_INSTALL_HIP_CMD} WORKING_DIRECTORY ${GINKGO_TEST_INSTALL_BIN_DIR} COMMENT \"Running a test on the installed binaries. \" \"This requires running `(sudo) make install` first.\" ) add_custom_target( test_exportbuild COMMAND ${CMAKE_COMMAND} -G${CMAKE_GENERATOR} ${TOOLSET} -S${GINKGO_TEST_EXPORTBUILD_SRC_DIR} -B${GINKGO_TEST_EXPORTBUILD_BIN_DIR} -DCMAKE_CXX_COMPILER=${CMAKE_CXX_COMPILER} -DCMAKE_CUDA_COMPILER=${CMAKE_CUDA_COMPILER} -DCMAKE_CXX_FLAGS=${CMAKE_CXX_FLAGS} -DGinkgo_ROOT=${Ginkgo_BINARY_DIR} # `--config cfg` is ignored by single-configuration generator. # `$<CONFIG>` is always be the same as `CMAKE_BUILD_TYPE` in # single-configuration generator. COMMAND ${CMAKE_COMMAND} --build ${GINKGO_TEST_EXPORTBUILD_BIN_DIR} --config $<CONFIG> COMMAND ${GINKGO_TEST_EXPORTBUILD_CMD} COMMENT \"Running a test on Ginkgo's exported build directory.\" ) # static linking with pkg-config is not possible with HIP, since # some linker information cannot be expressed in pkg-config files if(BUILD_SHARED_LIBS OR NOT GINKGO_BUILD_HIP) add_custom_target( test_pkgconfig COMMAND ${CMAKE_COMMAND} -G${CMAKE_GENERATOR} ${TOOLSET} -S${GINKGO_TEST_PKGCONFIG_SRC_DIR} -B${GINKGO_TEST_PKGCONFIG_BIN_DIR} -DCMAKE_CXX_COMPILER=${CMAKE_CXX_COMPILER} -DCMAKE_CUDA_COMPILER=${CMAKE_CUDA_COMPILER} -DCMAKE_CXX_FLAGS=${CMAKE_CXX_FLAGS} # `--config cfg` is ignored by single-configuration generator. # `$<CONFIG>` is always be the same as `CMAKE_BUILD_TYPE` in # single-configuration generator. COMMAND ${CMAKE_COMMAND} --build ${GINKGO_TEST_PKGCONFIG_BIN_DIR} --config $<CONFIG> COMMAND ${GINKGO_TEST_PKGCONFIG_CMD} COMMENT \"Running a test on Ginkgo's PkgConfig\" \"This requires installing Ginkgo first\" ) endif() # Setup CPack set(CPACK_PACKAGE_DESCRIPTION_FILE \"${Ginkgo_SOURCE_DIR}/README.md\") set(CPACK_RESOURCE_FILE_LICENSE \"${Ginkgo_SOURCE_DIR}/LICENSE\") set(CPACK_PACKAGE_ICON \"${Ginkgo_SOURCE_DIR}/assets/logo.png\") set(CPACK_PACKAGE_CONTACT \"ginkgo.library@gmail.com\") include(CPack) # And finally, print the configuration to screen: if(GINKGO_CONFIG_LOG_DETAILED) file(READ ${PROJECT_BINARY_DIR}/detailed.log GINKGO_LOG_SUMMARY) else() file(READ ${PROJECT_BINARY_DIR}/minimal.log GINKGO_LOG_SUMMARY) endif() message(STATUS \"${GINKGO_LOG_SUMMARY}\") # make sure no build files get committed accidentally if(NOT EXISTS ${CMAKE_CURRENT_BINARY_DIR}/.gitignore) file(WRITE ${CMAKE_CURRENT_BINARY_DIR}/.gitignore \"*\") endif()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/gipptools",
            "repo_link": "https://git.gfz-potsdam.de/gipp/gipptools",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/glaes",
            "repo_link": "https://github.com/FZJ-IEK3-VSA/geokit",
            "content": {
                "codemeta": "",
                "readme": "| master | dev | | :-----------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------: | | [![Build Status](https://travis-ci.com/FZJ-IEK3-VSA/geokit.svg?branch=master)](https://travis-ci.com/FZJ-IEK3-VSA/geokit) | [![Build Status](https://travis-ci.com/FZJ-IEK3-VSA/geokit.svg?branch=dev)](https://travis-ci.com/FZJ-IEK3-VSA/geokit) | --- <a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/FJZ_IEK-3_logo.svg?raw=True\" alt=\"Forschungszentrum Juelich Logo\" width=\"300px\"></a> # GeoKit - **Geo**spatial tool**kit** for Python GeoKit communicates directly with functions and objects within the Geospatial Data Abstraction Library (<a href=\"www.gdal.org\">GDAL</a>) and exposes them in such a way that is particularly useful for programmatic general purpose geospatial analyses. It gives low overhead control of fundamental operations; such as reading, writing, and mutating geospatial data sets, manipulating and translating geometries, warping and resampling raster data, and much more. Via the RegionMask object, GeoKit even allows for seamless integration of information expressed across multiple geospatial datasets in many formats and reference systems into the context of a single region. GeoKit is not intended to replace the GDAL library, as only very small subset of GDAL's capabilities are exposed. Nor is it intended to compete with other libraries with similar functionalities. Instead GeoKit evolved in an ad hoc manner in order to realize the Geospatial Land Eligibility for Energy Systems (<a href=\"https://github.com/FZJ-IEK3-VSA/glaes\">GLAES</a>) model which is intended for rapid land eligibility analyses of renewable energy systems and is also available on GitHub. Nevertheless, GeoKit quickly emerged as a general purpose GIS toolkit with capabilities far beyond computing land eligibility. Therefore, it is our pleasure to offer it to anyone who is interested in its use. [![DOI](https://zenodo.org/badge/114900977.svg)](https://zenodo.org/badge/latestdoi/114900977) ## Features - Direct exposure of functions and objects in the GDAL library - Reading, writing, and manipulating raster and vector datasets - Translation between data formats and projection systems - Direct conversion of raster data into NumPy matrices ## Installation ### Installation via conda-forge The easiest way to install GeoKit into a new environment is from `conda-forge` with: ```bash conda create -n geokit -c conda-forge geokit ``` or into an existing environment with: ```bash conda install -c conda-forge geokit ``` ### Installation from a local folder 1. First clone a local copy of the repository to your computer, and move into the created directory ``` git clone https://github.com/FZJ-IEK3-VSA/geokit.git cd geokit ``` 1. (Alternative) If you want to use the 'dev' branch (or another branch) then use: ``` git checkout dev ``` 2. When using [Anaconda](https://www.anaconda.com/) / [(Micro-)Mamba](https://mamba.readthedocs.io/en/latest/) (recommended), GeoKit should be installable to a new environment with: ``` conda env create --file requirements.yml conda activate geokit pip install . --no-deps ``` 2. (Alternative) Or into an existing environment with: ``` conda env update --file requirements.yml -n <ENVIRONMENT-NAME> conda activate geokit pip install . --no-deps ``` 2. (Alternative) If you want to install GeoKit in editable mode, and also with jupyter notebook and with testing functionalities use: ``` conda env create --file requirements-dev.yml conda activate geokit pip install . --no-deps -e ``` ## Examples See the [Examples page](Examples/) ## Docker We are trying to get GeoKit to work within a Docker container. Try it out! - First pull the image with: ```bash docker pull sevberg/geokit:latest ``` - You can then start a basic python interpreter with: ```bash docker run -it sevberg/geokit:latest -c \"python\" ``` - Or you can start a jupyter notebook using: ```bash docker run -it \\ -p 8888:8888 \\ sevberg/geokit:latest \\ -c \"jupyter notebook --ip='*' --port=8888 --no-browser --allow-root\" ``` - Which can then be connected to at the address \"localhost:8888:<API-KEY>\" - The API Key can be found from the output of the earlier command * Finally, you might want to mount a volume to access geospatial data. For this you can use: ```bash docker run -it \\ --mount target=/notebooks,type=bind,src=<PATH-TO-DIRECTORY> \\ -p 8888:8888 \\ sevberg/geokit:latest \\ -c \"jupyter notebook --notebook-dir=/notebooks --ip='*' --port=8888 --no-browser --allow-root\" ``` ## License MIT License Active Developers: Julian Schönau, Rachel Maier, Christoph Winkler, Shitab Ishmam, David Franzmann, Julian Belina, Noah Pflugradt, Heidi Heinrichs, Jochen Linßen, Detlef Stolten Alumni: David Severin Ryberg, Martin Robinius, Stanley Risch You should have received a copy of the MIT License along with this program. If not, see <https://opensource.org/licenses/MIT> ## About Us <a href=\"https://www.fz-juelich.de/en/iek/iek-3\"><img src=\"https://github.com/FZJ-IEK3-VSA/README_assets/blob/main/iek3-square.png?raw=True\" alt=\"Institute image IEK-3\" width=\"280\" align=\"right\" style=\"margin:0px 10px\"/></a> We are the <a href=\"https://www.fz-juelich.de/en/iek/iek-3\">Institute of Energy and Climate Research - Techno-economic Systems Analysis (IEK-3)</a> belonging to the <a href=\"https://www.fz-juelich.de/en\">Forschungszentrum Jülich</a>. Our interdisciplinary department's research is focusing on energy-related process and systems analyses. Data searches and system simulations are used to determine energy and mass balances, as well as to evaluate performance, emissions and costs of energy systems. The results are used for performing comparative assessment studies between the various systems. Our current priorities include the development of energy strategies, in accordance with the German Federal Government's greenhouse gas reduction targets, by designing new infrastructures for sustainable and secure energy supply chains and by conducting cost analysis studies for integrating new technologies into future energy market frameworks. ## Acknowledgment This work was supported by the Helmholtz Association under the Joint Initiative [\"Energy System 2050 A Contribution of the Research Field Energy\"](https://www.helmholtz.de/en/research/energy/energy_system_2050/). <a href=\"https://www.helmholtz.de/en/\"><img src=\"https://www.helmholtz.de/fileadmin/user_upload/05_aktuelles/Marke_Design/logos/HG_LOGO_S_ENG_RGB.jpg\" alt=\"Helmholtz Logo\" width=\"200px\" style=\"float:right\"></a>\n",
                "dependencies": "from setuptools import setup, find_packages setup( name='geokit', version='1.4.0', author='GeoKit Developer Team', url='https://github.com/FZJ-IEK3-VSA/geokit', packages=find_packages(), include_package_data=True, install_requires=[ \"gdal>=2.4.0, ==3.4.*\", \"numpy\", \"descartes\", \"pandas\", \"scipy\", \"matplotlib\", \"smopy\", ] )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/global-benchmark-database-gbd",
            "repo_link": "https://github.com/Udopia/gbd",
            "content": {
                "codemeta": "",
                "readme": "# Global Benchmark Database (GBD) [![DOI](https://zenodo.org/badge/141396410.svg)](https://zenodo.org/doi/10.5281/zenodo.10213943) GBD is a comprehensive suite of tools for provisioning and sustainably maintaining benchmark instances and their metadata for empirical research on hard algorithmic problem classes. For an introduction to the GBD concept, the underlying data model, and specific use cases, please refer to our [2024 SAT Tool Paper](https://doi.org/10.4230/LIPIcs.SAT.2024.18). ## GBD contributes data to your algorithmic evaluations GBD provides benchmark instance identifiers, feature extractors, and instance transformers for hard algorithmic problem domains, now including propositional satisfiability (SAT) and optimization (MaxSAT), and pseudo-Boolean optimization (PBO). ## GBD solves several problems - benchmark instance identification - identification of equivalence classes of benchmark instances - distribution of benchmark instances and benchmark metadata - initialization and maintenance of instance feature databases - transformation algorithms for benchmark instances GBD provides an extensible set of problem domains, feature extractors, and instance transformers. For a description of those currently supported, see the [GBDC documentation](https://udopia.github.io/gbdc/doc/Index.html). GBDC is a Python extension module for GBD's performance-critical code (written in C++), maintained in a separate [repository](https://github.com/Udopia/gbdc). ## Installation and Configuration - Run `pip install gbd-tools` - Run `pip install gbdc` (optional, installation of extension module gbdc) - Obtain a GBD database, e.g. download [https://benchmark-database.de/getdatabase/meta.db](https://benchmark-database.de/getdatabase/meta.db). - Configure your environment by registering paths to databases like this `export GBD_DB=path/to/database1:path/to/database2`. - Test the command line interface with the `gbd info` and `gbd --help` commands. ## GBD Interfaces GBD provides the command-line tool `gbd`, the web interface `gbd serve`, and the Python interface `gbd_core.api.GBD`. ### GBD Command-Line Interface Central commands in gbd are those for data access `gbd get` and database initialization `gbd init`. See `gbd --help` for more commands. Once a database is registered in the environment variable `GBD_DB`, the `gbd get` command can be used to access data. See `gbd get --help` for more information. `gbd init` provides access to registered feature extractors, such as those provided by the `gdbc` extension module. All initialization routines can be run in parallel, and resource limits can be set per process. See `gbd init --help` for more information. ### GBD Server The GBD server can be started locally with gbd serve. Our instance of the GBD server is hosted at [https://benchmark-database.de/](https://benchmark-database.de/). You can download benchmark instances and prebuilt feature databases from there. ### GBD Python Interface The GBD Python interface is used by all programs in the GBD ecosystem. Important here is the query command, which returns GBD data in the form of a Pandas dataframe for further analysis, as shown in the following example. ```Python from gbd_core.api import GBD with GBD(['path/to/database1', 'path/to/database2', ..] as gbd: df = gbd.query(\"family = hardware-bmc\", resolve=['verified-result', 'runtime-kissat']) ``` Scripts and use cases of GBD's Python interface are available on [https://udopia.github.io/gbdeval/](https://udopia.github.io/gbdeval/). The [evaluation demo](https://udopia.github.io/gbdeval/demo_evaluation.html) demonstrates portfolio analysis and subsequent category-wise performance evaluation using the 2023 SAT competition data. The [prediction demo](https://udopia.github.io/gbdeval/demo_prediction.html) demonstrates category prediction from instance features and subsequent feature importance evaluation.\n",
                "dependencies": "[build-system] requires = [\"setuptools>=42\", \"wheel\"] build-backend = \"setuptools.build_meta\" [project] name = \"gbd_tools\" version = \"4.9.11\" description = \"GBD Tools: Maintenance and Distribution of Benchmark Instances and their Attributes\" readme = \"README.md\" license-files = [\"LICENSE\"] requires-python = \">=3.6\" authors = [ { name = \"Markus Iser\", email = \"markus.iser@kit.edu\" } ] urls = { Homepage = \"https://github.com/Udopia/gbd\" } classifiers = [ \"Programming Language :: Python :: 3\" ] dependencies = [ \"flask\", \"tatsu\", \"pandas\", \"waitress\", \"pebble\", \"gbdc\" ] scripts = { gbd = \"gbd:main\" } [tool.setuptools] include-package-data = true py-modules = [\"gbd\"] packages = [\"gbd_core\", \"gbd_init\", \"gbd_server\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/gmgpolar",
            "repo_link": "https://github.com/mknaranja/GMGPolar",
            "content": {
                "codemeta": "",
                "readme": "[![GMGPolarCI](https://github.com/SciCompMod/GMGPolar/actions/workflows/main.yml/badge.svg?branch=main)](https://github.com/SciCompMod/GMGPolar/actions/workflows/main.yml) [![codecov](https://codecov.io/gh/SciCompMod/GMGPolar/graph/badge.svg?token=D0IVLUW51J)](https://codecov.io/gh/SciCompMod/GMGPolar) ![gmgpolar_logo](gmgpolar_small.png) GMGPolar is a performant geometric multigrid solver using implicit extrapolation to raise the convergence order. It is based on meshes in tensor- or product-format. GMGPolar's focus applications are geometries that can be described by polar or curvilinear coordinates for which suited smoothing procedures have been developed. If using GMGPolar, please cite: M. J. Kühn, C. Kruse, U. Rüde. Implicitly extrapolated geometric multigrid on disk-like domains for the gyrokinetic Poisson equation from fusion plasma applications. Journal of Scientific Computing, 91 (28). Springer (2022). Link: https://link.springer.com/article/10.1007/s10915-022-01802-1 ## Obtaining the source code The GMGPolar Solver can run with or without the sparse direct solver ``MUMPS``, though using MUMPS is recommended for optimal performance. This guide provides instructions on obtaining the code and installing the necessary dependencies. ## Clone the GMGPolar Repository To begin, download the latest stable version of GMGPolar by running the following commands in your terminal: # Clone the repository. This will create a directory named GMGPolar. git clone https://github.com/mknaranja/GMGPolar ## Configuring the Solver After cloning the repository, you'll need to configure the solver for your system. Edit the ``CMakeLists.txt`` file to reflect your system's configuration (e.g., paths to libraries, file names, etc.). ## Installing MUMPS using Spack We highly recommend using Spack to manage and install external dependencies such as MUMPS. The following steps outline the process for installing MUMPS and related tools. ## Step 1: Install Spack To install and set up Spack, execute the following commands in your terminal: # Clone the Spack repository git clone https://github.com/spack/spack.git # Add Spack to your environment by sourcing its setup script echo \". $HOME/spack/share/spack/setup-env.sh\" >> ~/.bashrc # Refresh your terminal or source your .bashrc source ~/.bashrc ## Step 2: Install MUMPS With Spack set up, you can now install MUMPS. The following command installs version 5.5.1 of MUMPS with specific options that are recommended for GMGPolar: spack install mumps@5.5.1 blr_mt=false complex=false double=true float=true incfort=false int64=false metis=true mpi=false openmp=true parmetis=false ptscotch=false scotch=false shared=true or in one line: spack install mumps@5.5.1~blr_mt~complex+double+float~incfort~int64+metis~mpi+openmp~parmetis~ptscotch~scotch+shared ### Note on AVX / AVX-512 Compatibility If your system does not support AVX or AVX-512 instructions (e.g., on AMD processors), install MUMPS with the following command: spack install mumps target=x86_64 ## Step 3: Configure CMake for GMGPolar After installing MUMPS and other dependencies, ensure that the paths to the libraries are correctly set in the CMakeLists.txt file. ## Final Step: Compiling the GMGPolar Solver Once everything is configured, compile the solver by running the following commands: ```bash cd scripts ./compile.sh [Debug|Release] ``` After executing ./compile.sh [Debug|Release], the script will compile the solver using the specified build type. You can also run ./compile.sh without any arguments afterward, and it will automatically use the last configured build type. Currently, the default build process only supports gnu compiler although Intel compiler has been successfully tested for some configurations. ## Optional: Measuring performance We use `Likwid` for performance monitoring. You can install it using Spack as well: **Install Likwid (Performance Monitoring Tool)**: ```bash spack install likwid ``` ## Running GMGPolar You can run the solver without having to write a code (as we do in the next section). After building the library, a binary is created called ``./build/gmgpolar``, it takes parameters directly from command-line. # To try GMGPolar on a small problem size, without having to write any code, # ./build/gmgpolar uses default parameters with a grid 33 x 64. ./build/gmgpolar # For more details on the available parameters, see the scripts/tutorial/run.sh script. ## Issue tracker If you find any bug, didn't understand a step in the documentation, or if you have a feature request, submit your issue on our `Issue Tracker`: https://github.com/mknaranja/GMGPolar/issues by giving: - reproducible parameters - computing environment (compiler, etc.) ## Release Notes ### GMGPolar 1.0.0 1) Working multigrid cycle 2) In-house solver and possibility to link with MUMPS for the smoothing and coarse grid solution 3) Extrapolation strategies: a. No extrapolation (--extrapolation 0) b. Default implicit extrapolation (--extrapolation 1) c. Non-default implicit extrapolation with smoothing of all nodes on the finest level [experimental, use with care, convergence cannot be observed with residual] (--extrapolation 2) 6) Optimization of apply_A / build_rhs / apply_prolongation / build_Asc / apply_Asc_ortho ### GMGPolar 2.0.0 1) **Enhancements and New Class Layout:** - **Linear Algebra:** - Introduced custom Vector and SparseMatrix classes. - Added a (cyclic) Tridiagonal Solver for improved performance and usability. - **Input Functions:** - Separated into distinct components: DomainGeometry, BoundaryConditions, SourceTerm, etc. - **Polar Grid:** - Indexing is now based on circle/radial smoother. - **Residual:** - Improved the residual calculation by addressing the unintuitive behavior that previously applied only to the interior part of the matrix. - **Direct Solver:** - Fixed a bug where boundary values were not treated correctly. - Built matrices to be symmetric, reducing factorization time. - **Smoother:** - Separated into extrapolated and standard smoothers. - Replaced the LU-Decomposition algorithm with the Thomas algorithm for improved efficiency. 2) **New Features** - Introduced W- and F cycles for enhanced solving capabilities. - Added FMG (Full Multigrid) to obtain improved starting solutions. - Implemented advanced caching behavior options for the \"Give\" implementation strategy. - Added a faster strategy named 'Take,' which is appropriate for cases where memory is less of a constraint, resulting in an 80% increase in memory usage. - Comprehensive Unit Tests: Integrated Google Unit Tests for all classes, ensuring robust and reliable functionality across the codebase. 3) **Performance Improvements** - Removed the task-based approach, which did not scale well with increasing parallelization. - Reduced maximum usage by 61.5% by constructing symmetric matrices and utilizing the tridiagonal structure of smoother matrices. 4) **Updated Features** - Added a new LU decomposition solver, allowing users to choose between MUMPS and the in-house solver for greater flexibility and performance.\n",
                "dependencies": "cmake_minimum_required(VERSION 3.12) project(GMGPolar VERSION 2.0.0 LANGUAGES CXX) # Options should be defined before they're used option(GMGPOLAR_BUILD_TESTS \"Build GMGPolar unit tests.\" ON) option(GMGPOLAR_USE_LIKWID \"Use LIKWID to measure code (regions).\" OFF) option(GMGPOLAR_USE_MUMPS \"Use MUMPS to solve linear systems.\" OFF) option(GMGPOLAR_ENABLE_COVERAGE \"Enable code coverage reporting (requires GCC/Clang)\" OFF) set(CMAKE_CXX_STANDARD 20) set(CMAKE_CXX_STANDARD_REQUIRED True) set(CMAKE_MODULE_PATH \"${PROJECT_SOURCE_DIR}/cmake\" ${CMAKE_MODULE_PATH}) # Set build type - must come before compiler flags if(NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE Release) endif() # Set compiler flags set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -Wall -Wextra -pedantic -Wno-unused -Wno-psabi\") set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} -O2 -mtune=generic -Wno-psabi\") # Set coverage compiler flags - must come before any targets are defined if(GMGPOLAR_ENABLE_COVERAGE) if(CMAKE_CXX_COMPILER_ID MATCHES \"GNU|Clang\") message(STATUS \"Enabling code coverage flags\") # Use generator expressions to apply only to specific configurations add_compile_options($<$<CONFIG:Debug>:--coverage>) add_link_options($<$<CONFIG:Debug>:--coverage>) # Force Debug build when coverage is enabled if(CMAKE_BUILD_TYPE STREQUAL \"Release\") message(STATUS \"Forcing Debug build for coverage\") set(CMAKE_BUILD_TYPE Debug CACHE STRING \"Build type\" FORCE) endif() else() message(WARNING \"Code coverage requires GCC or Clang. Current compiler: ${CMAKE_CXX_COMPILER_ID}\") endif() endif() include_directories(include) add_subdirectory(src) add_executable(gmgpolar src/main.cpp) add_executable(convergence_order src/convergence_order.cpp) add_executable(weak_scaling src/weak_scaling.cpp) add_executable(strong_scaling src/strong_scaling.cpp) target_link_libraries(gmgpolar PRIVATE GMGPolarLib) target_link_libraries(convergence_order PRIVATE GMGPolarLib) target_link_libraries(weak_scaling PRIVATE GMGPolarLib) target_link_libraries(strong_scaling PRIVATE GMGPolarLib) if(GMGPOLAR_BUILD_TESTS) enable_testing() add_subdirectory(third-party) add_subdirectory(tests) # Add coverage target - moved after test configuration if(GMGPOLAR_ENABLE_COVERAGE) find_program(LCOV_PATH lcov) find_program(GENHTML_PATH genhtml) if(LCOV_PATH AND GENHTML_PATH) add_custom_target(coverage # Reset counters COMMAND ${LCOV_PATH} --directory ${CMAKE_BINARY_DIR} --zerocounters # Run tests COMMAND ctest --test-dir ${CMAKE_BINARY_DIR} || true # Capture coverage data COMMAND ${LCOV_PATH} --directory ${CMAKE_BINARY_DIR} --capture --output-file ${CMAKE_BINARY_DIR}/coverage.info --ignore-errors mismatch,unused --rc geninfo_unexecuted_blocks=1 # Filter out system and unwanted directories COMMAND ${LCOV_PATH} --remove ${CMAKE_BINARY_DIR}/coverage.info '/usr/*' '*/tests/*' '*/third-party/*' '*/_deps/googletest-src/*' '*/include/InputFunctions/*' '*/src/InputFunctions/*' --output-file ${CMAKE_BINARY_DIR}/coverage-filtered.info --ignore-errors unused # Generate HTML report COMMAND ${GENHTML_PATH} ${CMAKE_BINARY_DIR}/coverage-filtered.info --output-directory ${CMAKE_BINARY_DIR}/coverage-report --ignore-errors source WORKING_DIRECTORY ${CMAKE_BINARY_DIR} COMMENT \"Generating code coverage report\" ) endif() endif() endif()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/goac",
            "repo_link": "https://iffgit.fz-juelich.de/k.koester/goac",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/golem",
            "repo_link": "https://github.com/ajacquey/golem",
            "content": {
                "codemeta": "",
                "readme": "<h1 align=\"center\"> <br> <a href=\"https://github.com/ajacquey/Golem\"><img src=\"images/golem_logo.png\" alt=\"GOLEM\" width=\"600\"></a> <br> A MOOSE-based application <br> </h1> <h4 align=\"center\">A numerical simulator for modelling coupled THM processes in faulted geothermal reservoirs based on <a href=\"http://mooseframework.org/\" target=\"blank\">MOOSE</a>.</h4> <p align=\"center\"> <a href=\"LICENSE\"> <img src=\"https://img.shields.io/badge/license-GPLv3-blue.svg\" alt=\"GitHub License\"> </a> <a href=\"https://gitter.im/Golem-Moose/golem\"> <img src=\"https://img.shields.io/gitter/room/nwjs/nw.js.svg\" alt=\"Gitter\"> </a> <a href=\"https://zenodo.org/record/999401#.Wc5NqBdx1pg\"> <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.999401.svg\" alt=\"DOI\"> </a> </p> ## About GOLEM is a numerical simulator for modelling coupled Thermo-Hydro-Mechanical processes in faulted geothermal reservoirs. The simulator is developed by [Antoine Jacquey](http://www.gfz-potsdam.de/en/staff/antoine-jacquey/) <a href=\"https://orcid.org/0000-0002-6259-4305\" target=\"orcid.widget\" rel=\"noopener noreferrer\" style=\"vertical-align:top;\"><img src=\"https://orcid.org/sites/default/files/images/orcid_16x16.png\" style=\"width:1em;margin-right:.5em;\" alt=\"ORCID iD icon\"></a><a href=\"https://github.com/ajacquey/\" target=\"github.widget\" rel=\"noopener noreferrer\" style=\"vertical-align:top;\"><img src=\"images/GitHub-Mark-32px.png\" width=\"16\" margin-right=\".5em;\" alt=\"GitHub icon id\"></a> and [Mauro Cacace](http://www.gfz-potsdam.de/en/section/basin-modeling/staff/profil/mauro-cacace/) <a href=\"https://orcid.org/0000-0001-6101-9918\" target=\"orcid.widget\" rel=\"noopener noreferrer\" style=\"vertical-align:top;\"><img src=\"https://orcid.org/sites/default/files/images/orcid_16x16.png\" style=\"width:1em;margin-right:.5em;\" alt=\"ORCID iD icon\"></a><a href=\"https://github.com/mcacace\" target=\"github.widget\" rel=\"noopener noreferrer\" style=\"vertical-align:top;\"><img src=\"images/GitHub-Mark-32px.png\" width=\"16\" margin-right=\".5em;\" alt=\"GitHub icon id\"></a> at the [GFZ German Research Centre for Geosciences](http://www.gfz-potsdam.de/en/home/) from the section [Basin Modelling](http://www.gfz-potsdam.de/en/section/basin-modeling/). GOLEM is a MOOSE-based application. Visit the [MOOSE framework](http://mooseframework.org) page for more information. ## Licence GOLEM is distributed under the [GNU GENERAL PUBLIC LICENSE v3](https://github.com/ajacquey/Golem/blob/master/LICENSE). ## Getting Started #### Minimum System Requirements The following system requirements are from the MOOSE framework (see [Getting Started](http://mooseframework.inl.gov/getting_started/) for more information): * Compiler: C++11 Compliant GCC 4.8.4, Clang 3.4.0, Intel20130607 * Python 2.7+ * Memory: 16 GBs (debug builds) * Processor: 64-bit x86 * Disk: 30 GBs * OS: UNIX compatible (OS X, most flavors of Linux) #### 1. Setting Up a MOOSE Installation To install GOLEM, you need first to have a working and up-to-date installation of the MOOSE framework. To do so, please visit the [Getting Started](http://mooseframework.inl.gov/getting_started/) page of the MOOSE framework and follow the instructions. If you encounter difficulties at this step, you can ask for help on the [MOOSE-users Google group](https://groups.google.com/forum/#!forum/moose-users). #### 2. Clone GOLEM GOLEM can be cloned directly from [GitHub](https://github.com/ajacquey/Golem) using [Git](https://git-scm.com/). In the following, we refer to the directory `projects` which you created during the MOOSE installation (by default `~/projects`): cd ~/projects git clone https://github.com/ajacquey/Golem.git cd ~/projects/golem git checkout master *Note: the \"master\" branch of GOLEM is the \"stable\" branch which is updated only if all tests are passing.* #### 3. Compile GOLEM You can compile GOLEM by following these instructions: cd ~/projects/golem make -j4 #### 4. Test GOLEM To make sure that everything was installed properly, you can run the tests suite of GOLEM: cd ~/projects/golem ./run_tests -j2 If all the tests passed, then your installation is working properly. You can now use the GOLEM simulator! ## Usage To run GOLEM from the command line with multiple processors, use the following command: mpiexec -n <nprocs> ~/projects/golem/golem-opt -i <input-file> Where `<nprocs>` is the number of processors you want to use and `<input-file>` is the path to your input file (extension `.i`). Information about the structure of the GOLEM input files can be found in the documentation (link to follow). ## Cite If you use GOLEM for your work please cite: * This repository: Antoine B. Jacquey, & Mauro Cacace. (2017, September 29). GOLEM, a MOOSE-based application. Zenodo. http://doi.org/10.5281/zenodo.999401 * The publication presenting GOLEM: Cacace, M. and Jacquey, A. B.: Flexible parallel implicit modelling of coupled thermal-hydraulic-mechanical processes in fractured rocks, Solid Earth, 8, 921-941, https://doi.org/10.5194/se-8-921-2017, 2017. Please read the [CITATION](https://github.com/ajacquey/Golem/blob/master/CITATION) file for more information. ## Publications using GOLEM * Freymark, J., Bott, J., Cacace, M., Ziegler, M., Scheck-Wenderoth, M.: Influence of the Main Border Faults on the 3D Hydraulic Field of the Central Upper Rhine Graben, *Geofluids*, 2019. * Blöcher, G., Cacace, M., Jacquey, A. B., Zang, A., Heidbach, O., Hofmann, H., Kluge, C., Zimmermann, G.: Evaluating Micro-Seismic Events Triggered by Reservoir Operations at the Geothermal Site of Groß Schönebeck (Germany), *Rock Mechanics and Rock Engineering*, 2018. * Jacquey, A. B., Urpi, L., Cacace, M., Blöcher, G., Zimmermann, G., Scheck-Wenderoth, M.: Far field poroelastic response of geothermal reservoirs to hydraulic stimulation treatment: Theory and application at the Groß Schönebeck geothermal research facility, *International Journal of Rock Mechanics and Mining Sciences*, 2018. * Peters, E., Blöcher, G., Salimzadeh, S., Egberts, P. J. P., Cacace, M.: Modelling of multi-lateral well geometries for geothermal applications, *Advances in Geosciences*, 2018. * Magri, F., Cacace, M., Fischer, T., Kolditz, O., Wang, W., Watanabe, N.: Thermal convection of viscous fluids in a faulted system: 3D benchmark for numerical codes, *Energy Procedia*, 2017. * Cacace, M. and Jacquey, A. B.: Flexible parallel implicit modelling of coupled Thermal-Hydraulic-Mechanical processes in fractured rocks, Solid Earth, 2017. * Jacquey, A. B.: Coupled Thermo-Hydro-Mechanical Processes in Geothermal Reservoirs: a Multiphysic and Multiscale Approach Linking Geology and 3D Numerical Modelling, PhD thesis, RWTH Aachen, 2017. * Jacquey, A. B., Cacace, M., Blöcher, G.: Modelling coupled fluid flow and heat transfer in fractured reservoirs: description of a 3D benchmark numerical case, Energy Procedia, 2017. * Jacquey, A. B., Cacace, M., Blöcher, G., Milsch, H., Deon, F., Scheck-Wenderoth, M.: Processes Responsible for Localized Deformation within Porous Rocks: Insights from Laboratory Experiments and Numerical Modelling, 6th Biot Conference on Poromechanics, Paris 2017.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/gr-framework",
            "repo_link": "https://github.com/sciapp/gr",
            "content": {
                "codemeta": "",
                "readme": "GR - a universal framework for visualization applications ========================================================= [![MIT license](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE.md) [![GitHub tag](https://img.shields.io/github/tag/jheinen/gr.svg)](https://github.com/jheinen/gr/releases) [![PyPI version](https://img.shields.io/pypi/v/gr.svg)](https://pypi.python.org/pypi/gr) [![DOI](https://zenodo.org/badge/17747322.svg)](https://zenodo.org/badge/latestdoi/17747322) *GR* is a universal framework for cross-platform visualization applications. It offers developers a compact, portable and consistent graphics library for their programs. Applications range from publication quality 2D graphs to the representation of complex 3D scenes. *GR* is essentially based on an implementation of a Graphical Kernel System (GKS). As a self-contained system it can quickly and easily be integrated into existing applications (i.e. using the `ctypes` mechanism in Python or `ccall` in Julia). The *GR* framework can be used in imperative programming systems or integrated into modern object-oriented systems, in particular those based on GUI toolkits. *GR* is characterized by its high interoperability and can be used with modern web technologies. The *GR* framework is especially suitable for real-time or signal processing environments. *GR* was developed by the Scientific IT-Systems group at the Peter Grünberg Institute at Forschunsgzentrum Jülich. The main development has been done by Josef Heinen who currently maintains the software, but there are other developers who currently make valuable contributions. Special thanks to Florian Rhiem (*GR3*) and Christian Felder (qtgr, setup.py). Starting with release 0.6 *GR* can be used as a backend for [Matplotlib](http://matplotlib.org) and significantly improve the performance of existing Matplotlib or PyPlot applications written in Python or Julia, respectively. In [this](http://gr-framework.org/tutorials/matplotlib.html) tutorial section you can find some examples. Beginning with version 0.10.0 *GR* supports inline graphics which shows up in IPython's Qt Console or interactive computing environments for *Python* and *Julia*, such as [IPython](http://ipython.org) and [Jupyter](https://jupyter.org). An interesting example can be found [here](http://pgi-jcns.fz-juelich.de/pub/doc/700K_460.html). ## Installation and Getting Started To install *GR* and try it using *Python*, *Julia* or *C*, please see the corresponding documentation: - [Python package gr](https://gr-framework.org/python.html) - [Julia package GR](https://gr-framework.org/julia.html) - [C library GR](https://gr-framework.org/c.html) - [Ruby package GR](https://github.com/red-data-tools/GR.rb) ## Documentation You can find more information about *GR* on the [GR home page](http://gr-framework.org). ## Contributing If you want to improve *GR*, please read the [contribution guide](https://github.com/sciapp/gr/blob/develop/CONTRIBUTING.md) for a few notes on how to report issues or submit changes. ## Support If you have any questions about *GR* or run into any issues setting up or running GR, please [open an issue on GitHub](https://github.com/sciapp/gr/issues/new), either in this repo or in the repo for the language binding you are using ([Python](https://github.com/sciapp/python-gr/issues/new), [Julia](https://github.com/jheinen/GR.jl/issues/new), [Ruby](https://github.com/red-data-tools/GR.rb/issues/new)).\n",
                "dependencies": "cmake_minimum_required(VERSION 3.5...4.0 FATAL_ERROR) list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\") include(GetVersionFromGit) get_version_from_git(GR_VERSION GR_VERSION_FULL) project( GR VERSION ${GR_VERSION} LANGUAGES C CXX ) include(GNUInstallDirs) include(CheckCXXCompilerFlag) check_cxx_compiler_flag(\"-Werror=implicit\" ERROR_IMPLICIT_SUPPORTED) if(ERROR_IMPLICIT_SUPPORTED) set(COMPILER_OPTION_ERROR_IMPLICIT \"-Werror=implicit\" CACHE INTERNAL \"Compiler flag for generating errors on implicit declarations\" ) else() set(COMPILER_OPTION_ERROR_IMPLICIT \"\" CACHE INTERNAL \"Compiler flag for generating errors on implicit declarations\" ) endif() if(UNIX) if(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT) set(CMAKE_INSTALL_PREFIX \"/usr/local/gr\" CACHE PATH \"GR install prefix\" FORCE ) endif() endif() set(GR_DIRECTORY \"${CMAKE_INSTALL_PREFIX}\" CACHE STRING \"Default value for GRDIR\" ) option(GR_BUILD_DEMOS \"Build demos for GR\" OFF) option(GR_BUILD_GKSM \"Build GKS metafile reader for GR\" OFF) option(GR_INSTALL \"Create installation target for GR\" ON) option(GR_USE_BUNDLED_LIBRARIES \"Use thirdparty libraries bundled with GR\" OFF) option(GR_MANUAL_MOC_AND_RCC \"Manually run moc and rcc instead of relying on AUTOMOC and AUTORCC\" OFF) option(GR_FIND_QT5_BY_VARIABLES \"Find Qt5 based on the variables Qt5_INCLUDE_DIR and Qt5_LIBRARY_DIR\" OFF) option(GR_PREFER_XCODEBUILD \"Prefer xcodebuild to build macOS applications if available\" ON) if(GR_USE_BUNDLED_LIBRARIES) list(APPEND CMAKE_FIND_ROOT_PATH \"${CMAKE_CURRENT_LIST_DIR}/3rdparty/build/\") set(GR_THIRDPARTY_LIBRARY_PREFIX ${CMAKE_STATIC_LIBRARY_PREFIX}) set(GR_THIRDPARTY_LIBRARY_SUFFIX ${CMAKE_STATIC_LIBRARY_SUFFIX}) else() set(GR_THIRDPARTY_LIBRARY_PREFIX ${CMAKE_SHARED_LIBRARY_PREFIX}) set(GR_THIRDPARTY_LIBRARY_SUFFIX ${CMAKE_SHARED_LIBRARY_SUFFIX}) endif() if(GR_FIND_QT5_BY_VARIABLES) list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake/optional/qt5\") endif() if(WIN32) set(GR_PLUGIN_SUFFIX \".dll\") else() set(GR_PLUGIN_SUFFIX \".so\") endif() if(APPLE) set(DEFAULT_SHARED_LIBRARY_SUFFIX \".dylib\") else() set(DEFAULT_SHARED_LIBRARY_SUFFIX \"${GR_PLUGIN_SUFFIX}\") endif() set(GR_SHARED_LIBRARY_SUFFIX \"${DEFAULT_SHARED_LIBRARY_SUFFIX}\" CACHE STRING \"File suffix for shared libraries\" ) set(CMAKE_FIND_PACKAGE_PREFER_CONFIG TRUE CACHE BOOL \"\" ) # Find the following packages always in system locations even if `GR_USE_BUNDLED_LIBRARIES` is set, because they are not # located in 3rdparty find_package(X11) find_package(Fontconfig) find_package(OpenGL OPTIONAL_COMPONENTS OpenGL) if(${CMAKE_VERSION} VERSION_GREATER \"3.16.0\") find_package( Qt6 OPTIONAL_COMPONENTS Widgets Core Network Gui PrintSupport ) endif() find_package( Qt5 OPTIONAL_COMPONENTS Widgets Core Network Gui PrintSupport ) find_package(Qt4) # Find the following packages only in 3rdparty, if `GR_USE_BUNDLED_LIBRARIES` is set if(GR_USE_BUNDLED_LIBRARIES) set(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY) set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY) set(CMAKE_FIND_ROOT_PATH_MODE_PACKAGE ONLY) set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM ONLY) endif() find_package(Freetype) if(TARGET freetype AND NOT TARGET Freetype::Freetype) add_library(Freetype::Freetype ALIAS freetype) endif() find_package(Jpeg REQUIRED) find_package(Libpng REQUIRED) find_package(ZLIB REQUIRED) find_package(Qhull REQUIRED) if(TARGET Qhull::qhullstatic_r AND NOT TARGET Qhull::qhull_r) add_library(Qhull::qhull_r ALIAS Qhull::qhullstatic_r) endif() find_package(Tiff) find_package(Ffmpeg 57.48.100) find_package(glfw3) if(TARGET glfw AND NOT TARGET Glfw::Glfw) add_library(Glfw::Glfw ALIAS glfw) endif() find_package(ZeroMQ) if(NOT TARGET ZeroMQ::ZeroMQ) if(TARGET libzmq) add_library(ZeroMQ::ZeroMQ ALIAS libzmq) elseif(TARGET libzmq-static) add_library(ZeroMQ::ZeroMQ ALIAS libzmq-static) get_target_property(ZeroMQ_LIBRARY libzmq-static LOCATION) endif() endif() find_package(Cairo) find_package(Agg) find_package(Gs) find_package(XercesC) if(APPLE) set(INSTALL_RPATH \"${GR_DIRECTORY}/lib/;@loader_path/.\") else() set(INSTALL_RPATH \"${GR_DIRECTORY}/lib/;$ORIGIN/.\") endif() if(Qt4_FOUND OR (Qt5Widgets_FOUND AND Qt5Core_FOUND AND Qt5Network_FOUND ) OR (Qt6Widgets_FOUND AND Qt6Core_FOUND AND Qt6Network_FOUND ) ) if(GR_MANUAL_MOC_AND_RCC) if(NOT QT_MOC_EXECUTABLE) find_program(QT_MOC_EXECUTABLE moc CMAKE_FIND_ROOT_PATH_BOTH) endif() if(NOT QT_MOC_EXECUTABLE) message(FATAL_ERROR \"Could not find moc but GR_MANUAL_MOC_AND_RCC is set.\") endif() if(NOT QT_RCC_EXECUTABLE) find_program(QT_RCC_EXECUTABLE rcc CMAKE_FIND_ROOT_PATH_BOTH) endif() if(NOT QT_RCC_EXECUTABLE) message(FATAL_ERROR \"Could not find rcc but GR_MANUAL_MOC_AND_RCC is set.\") endif() endif() endif() if(Qt4_FOUND) set(QT4_MOC_INCLUDE_FLAGS \"\") foreach(DIR IN LISTS QT_INCLUDE_DIR) set(QT4_MOC_INCLUDE_FLAGS ${QT4_MOC_INCLUDE_FLAGS} -I${DIR}) endforeach() if(NOT DEFINED Qt4_LIBRARY_DIR) get_filename_component(Qt4_LIBRARY_DIR \"${QT_LIBRARY_DIR}/../..\" ABSOLUTE) endif() endif() if(Qt5Widgets_FOUND AND Qt5Core_FOUND AND Qt5Network_FOUND ) set(QT5_MOC_INCLUDE_FLAGS \"\") foreach(DIR IN LISTS Qt5Core_INCLUDE_DIRS Qt5Gui_INCLUDE_DIRS Qt5Widgets_INCLUDE_DIRS) set(QT5_MOC_INCLUDE_FLAGS ${QT5_MOC_INCLUDE_FLAGS} -I${DIR}) endforeach() if(NOT DEFINED Qt5_LIBRARY_DIR) get_filename_component(Qt5_LIBRARY_DIR \"${Qt5_DIR}/../..\" ABSOLUTE) endif() endif() if(Qt6Widgets_FOUND AND Qt6Core_FOUND AND Qt6Network_FOUND ) set(QT6_MOC_INCLUDE_FLAGS \"\") foreach(DIR IN LISTS Qt6Core_INCLUDE_DIRS Qt6Gui_INCLUDE_DIRS Qt6Widgets_INCLUDE_DIRS) set(QT6_MOC_INCLUDE_FLAGS ${QT6_MOC_INCLUDE_FLAGS} -I${DIR}) endforeach() if(NOT DEFINED Qt6_LIBRARY_DIR) get_filename_component(Qt6_LIBRARY_DIR \"${Qt6_DIR}/../..\" ABSOLUTE) endif() endif() if(X11_FOUND) # older versions of FindX11.cmake set variables but do not create targets if(NOT TARGET X11::X11 AND X11_INCLUDE_DIR AND X11_X11_LIB AND X11_Xft_LIB ) add_library(X11::X11 UNKNOWN IMPORTED) set_target_properties( X11::X11 PROPERTIES INTERFACE_INCLUDE_DIRECTORIES \"${X11_INCLUDE_DIR}/\" IMPORTED_LINK_INTERFACE_LANGUAGES \"C\" IMPORTED_LOCATION \"${X11_X11_LIB}\" INTERFACE_LINK_LIBRARIES \"${X11_Xft_LIB}\" ) endif() if(NOT TARGET X11::Xt AND X11_INCLUDE_DIR AND X11_Xt_LIB AND TARGET X11::X11 ) add_library(X11::Xt UNKNOWN IMPORTED) set_target_properties( X11::Xt PROPERTIES INTERFACE_INCLUDE_DIRECTORIES \"${X11_INCLUDE_DIR}/\" IMPORTED_LINK_INTERFACE_LANGUAGES \"C\" IMPORTED_LOCATION \"${X11_Xt_LIB}\" INTERFACE_LINK_LIBRARIES \"X11::X11\" ) endif() endif() set(GKS_SOURCES lib/gks/afm.c lib/gks/font.c lib/gks/socket.c lib/gks/ft.c lib/gks/malloc.c lib/gks/util.c lib/gks/compress.c lib/gks/gks.c lib/gks/mf.c lib/gks/win.c lib/gks/gksforbnd.c lib/gks/pdf.c lib/gks/wiss.c lib/gks/dl.c lib/gks/plugin.c lib/gks/error.c lib/gks/io.c lib/gks/ps.c lib/gks/resample.c ) add_library(gks_static STATIC ${GKS_SOURCES}) add_library(gks_shared SHARED ${GKS_SOURCES}) foreach(LIBRARY gks_static gks_shared) if(LIBRARY MATCHES \"static\") set(GKS_LINK_MODE PUBLIC) else() set(GKS_LINK_MODE PRIVATE) endif() if(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\") target_compile_definitions(${LIBRARY} PRIVATE _CRT_SECURE_NO_WARNINGS) endif() if(NOT (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")) target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} pthread) target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} m) endif() if(Freetype_FOUND) target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} Freetype::Freetype) else() target_compile_definitions( ${LIBRARY} ${GKS_LINK_MODE} NO_FT ) endif() target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} ZLIB::ZLIB) target_compile_definitions( ${LIBRARY} ${GKS_LINK_MODE} HAVE_ZLIB ) if(UNIX) target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} dl) elseif(WIN32) target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} ws2_32) target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} msimg32) target_link_libraries(${LIBRARY} ${GKS_LINK_MODE} gdi32) endif() if(NOT (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")) target_compile_options(${LIBRARY} ${GKS_LINK_MODE} -pthread) endif() target_compile_options(${LIBRARY} PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) target_include_directories( ${LIBRARY} PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}> ) target_compile_definitions(${LIBRARY} PRIVATE GRDIR=\"${GR_DIRECTORY}\") set_target_properties( ${LIBRARY} PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON POSITION_INDEPENDENT_CODE ON PREFIX \"lib\" IMPORT_PREFIX \"lib\" OUTPUT_NAME GKS INSTALL_RPATH \"${INSTALL_RPATH}\" ) endforeach() unset(GKS_LINK_MODE) if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\") set_target_properties(gks_static PROPERTIES OUTPUT_NAME GKSstatic) endif() set_target_properties(gks_shared PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" EXPORT_NAME GKS) add_library(GR::GKS ALIAS gks_static) file(WRITE ${CMAKE_CURRENT_BINARY_DIR}/gr_version.h \"#ifndef GR_VERSION\\n#define GR_VERSION \\\"${GR_VERSION_FULL}\\\"\\n#endif\\n\" ) set_source_files_properties(${CMAKE_CURRENT_BINARY_DIR}/gr_version.h PROPERTIES GENERATED TRUE) set(GR_SOURCES lib/gr/boundary.c lib/gr/contour.c lib/gr/contourf.c lib/gr/delaunay.c lib/gr/gr.c lib/gr/grforbnd.c lib/gr/gridit.c lib/gr/image.c lib/gr/import.c lib/gr/interp2.c lib/gr/stream.c lib/gr/md5.c lib/gr/shade.c lib/gr/spline.c lib/gr/strlib.c lib/gr/text.c lib/gr/mathtex2.c lib/gr/mathtex2_kerning.c lib/gr/mathtex2.tab.c lib/gr/threadpool.c ${CMAKE_CURRENT_BINARY_DIR}/gr_version.h ) add_library(gr_static STATIC ${GR_SOURCES}) add_library(gr_shared SHARED ${GR_SOURCES}) foreach(LIBRARY gr_static gr_shared) if(LIBRARY MATCHES \"static\") set(GR_LINK_MODE PUBLIC) else() set(GR_LINK_MODE PRIVATE) endif() target_link_libraries(${LIBRARY} ${GR_LINK_MODE} gks_static) if(NOT (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")) target_link_libraries(${LIBRARY} ${GR_LINK_MODE} m) endif() target_link_libraries(${LIBRARY} ${GR_LINK_MODE} Qhull::qhull_r) target_link_libraries(${LIBRARY} ${GR_LINK_MODE} Jpeg::Jpeg) target_link_libraries(${LIBRARY} ${GR_LINK_MODE} Libpng::Libpng) if(WIN32) target_link_libraries(${LIBRARY} ${GR_LINK_MODE} ws2_32) endif() target_include_directories( ${LIBRARY} PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}> ) target_include_directories(${LIBRARY} PRIVATE $<BUILD_INTERFACE:${CMAKE_CURRENT_BINARY_DIR}>) if(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\") target_compile_definitions(${LIBRARY} PRIVATE _CRT_SECURE_NO_WARNINGS) endif() target_compile_definitions(${LIBRARY} PRIVATE GRDIR=\"${GR_DIRECTORY}\") target_compile_options(${LIBRARY} PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( ${LIBRARY} PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON POSITION_INDEPENDENT_CODE ON INSTALL_RPATH \"${INSTALL_RPATH}\" PREFIX \"lib\" IMPORT_PREFIX \"lib\" OUTPUT_NAME GR ) endforeach() unset(GR_LINK_MODE) if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\") set_target_properties(gr_static PROPERTIES OUTPUT_NAME GRstatic) endif() set_target_properties(gr_shared PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" EXPORT_NAME GR) add_library(GR::GR ALIAS gr_shared) set(GR3_SOURCES lib/gr3/gr3.c lib/gr3/gr3_convenience.c lib/gr3/gr3_gr.c lib/gr3/gr3_html.c lib/gr3/gr3_jpeg.c lib/gr3/gr3_mc.c lib/gr3/gr3_png.c lib/gr3/gr3_povray.c lib/gr3/gr3_slices.c lib/gr3/gr3_sr.c ) add_library(gr3_static STATIC ${GR3_SOURCES}) add_library(gr3_shared SHARED ${GR3_SOURCES}) foreach(LIBRARY gr3_static gr3_shared) if(LIBRARY MATCHES \"static\") set(GR3_LINK_MODE PUBLIC) else() set(GR3_LINK_MODE PRIVATE) endif() if(APPLE) target_sources(${LIBRARY} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr3/gr3_cgl.c) elseif(UNIX AND NOT APPLE) target_sources(${LIBRARY} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr3/gr3_glx.c) elseif(WIN32) target_sources(${LIBRARY} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr3/gr3_win.c) endif() target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} GR::GR) target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} GR::GKS) target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} Jpeg::Jpeg) target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} Libpng::Libpng) if(NOT (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")) target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} pthread) target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} m) endif() if((APPLE OR WIN32) AND OpenGL_FOUND) target_link_libraries(${LIBRARY} ${GR3_LINK_MODE} OpenGL::GL) endif() if(APPLE) # Apple has deprecated OpenGL in macOS 10.14 target_compile_definitions(${LIBRARY} PRIVATE GL_SILENCE_DEPRECATION) endif() if(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\") target_compile_definitions(${LIBRARY} PRIVATE _CRT_SECURE_NO_WARNINGS) endif() target_compile_definitions(${LIBRARY} PRIVATE GRDIR=\"${GR_DIRECTORY}\") if(NOT (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")) target_compile_options(${LIBRARY} ${GR3_LINK_MODE} -pthread) endif() target_compile_options(${LIBRARY} PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) target_include_directories( ${LIBRARY} PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/lib/gr3/> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}> ) set_target_properties( ${LIBRARY} PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON POSITION_INDEPENDENT_CODE ON INSTALL_RPATH \"${INSTALL_RPATH}\" PREFIX \"lib\" IMPORT_PREFIX \"lib\" OUTPUT_NAME GR3 ) endforeach() unset(GR3_LINK_MODE) if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\") set_target_properties(gr3_static PROPERTIES OUTPUT_NAME GR3static) endif() set_target_properties(gr3_shared PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" EXPORT_NAME GR3) add_library(GR::GR3 ALIAS gr3_shared) if(UNIX AND NOT APPLE) if(TARGET X11::X11 AND TARGET OpenGL::GLX AND TARGET OpenGL::GL ) add_library(gr3platform SHARED lib/gr3/gr3_platform_glx.c) target_link_libraries(gr3platform PUBLIC GR::GR) target_link_libraries(gr3platform PUBLIC X11::X11) target_link_libraries(gr3platform PUBLIC OpenGL::GLX) target_link_libraries(gr3platform PUBLIC OpenGL::GL) target_compile_options(gr3platform PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( gr3platform PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON POSITION_INDEPENDENT_CODE ON INSTALL_RPATH \"${INSTALL_RPATH}\" OUTPUT_NAME GR3platform ) else() target_compile_definitions(gr3_static PUBLIC -DNO_GL) target_compile_definitions(gr3_shared PUBLIC -DNO_GL) message(WARNING \"libGR3platform.so requires X11 and will not be built.\") endif() elseif(NOT OpenGL_FOUND) target_compile_definitions(gr3_static PUBLIC -DNO_GL) target_compile_definitions(gr3_shared PUBLIC -DNO_GL) endif() set(GRM_SOURCES lib/grm/src/grm/args.c lib/grm/src/grm/backtrace.c lib/grm/src/grm/base64.c lib/grm/src/grm/bson.c lib/grm/src/grm/dump.c lib/grm/src/grm/dynamic_args_array.c lib/grm/src/grm/error.c lib/grm/src/grm/event.c lib/grm/src/grm/interaction.cxx lib/grm/src/grm/json.c lib/grm/src/grm/layout_c.cxx lib/grm/src/grm/layout.cxx lib/grm/src/grm/layout_error.cxx lib/grm/src/grm/logging.c lib/grm/src/grm/memwriter.c lib/grm/src/grm/net.c lib/grm/src/grm/plot.cxx lib/grm/src/grm/util.c lib/grm/src/grm/import.cxx lib/grm/src/grm/utilcpp.cxx lib/grm/src/grm/datatype/double_map.c lib/grm/src/grm/datatype/size_t_list.c lib/grm/src/grm/datatype/string_array_map.c lib/grm/src/grm/datatype/string_list.c lib/grm/src/grm/datatype/string_map.c lib/grm/src/grm/datatype/uint_map.c lib/grm/src/grm/dom_render/context.cxx lib/grm/src/grm/dom_render/render.cxx lib/grm/src/grm/dom_render/Drawable.cxx lib/grm/src/grm/dom_render/graphics_tree/Comment.cxx lib/grm/src/grm/dom_render/graphics_tree/Document.cxx lib/grm/src/grm/dom_render/graphics_tree/Element.cxx lib/grm/src/grm/dom_render/graphics_tree/Node.cxx lib/grm/src/grm/dom_render/graphics_tree/Value.cxx lib/grm/src/grm/dom_render/graphics_tree/util.cxx lib/grm/src/grm/dom_render/ManageCustomColorIndex.cxx lib/grm/src/grm/dom_render/ManageGRContextIds.cxx lib/grm/src/grm/dom_render/ManageZIndex.cxx ) add_library(grm_static STATIC ${GRM_SOURCES}) add_library(grm_shared SHARED ${GRM_SOURCES}) add_library(grm_shared_internal SHARED ${GRM_SOURCES}) foreach(LIBRARY grm_static grm_shared grm_shared_internal) if(LIBRARY MATCHES \"static\") set(GRM_LINK_MODE PUBLIC) else() set(GRM_LINK_MODE PRIVATE) endif() if(NOT LIBRARY MATCHES \"internal\") set_target_properties(${LIBRARY} PROPERTIES C_VISIBILITY_PRESET hidden CXX_VISIBILITY_PRESET hidden) endif() target_link_libraries(${LIBRARY} ${GRM_LINK_MODE} GR::GKS) target_link_libraries(${LIBRARY} ${GRM_LINK_MODE} GR::GR) target_link_libraries(${LIBRARY} ${GRM_LINK_MODE} GR::GR3) if(TARGET XercesC::XercesC) target_link_libraries(${LIBRARY} ${GRM_LINK_MODE} XercesC::XercesC) else() target_compile_definitions(${LIBRARY} PRIVATE NO_XERCES_C) endif() if(NOT (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\")) target_link_libraries(${LIBRARY} ${GRM_LINK_MODE} m) endif() if(WIN32) target_link_libraries(${LIBRARY} ${GRM_LINK_MODE} ws2_32) endif() target_include_directories( ${LIBRARY} PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/include/> $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/src/> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}> ) target_compile_definitions(${LIBRARY} PRIVATE BUILDING_GR) target_compile_definitions(${LIBRARY} PRIVATE GRDIR=\"${GR_DIRECTORY}\") target_compile_options(${LIBRARY} PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\") target_compile_options(${LIBRARY} PRIVATE /permissive-) endif() set_target_properties( ${LIBRARY} PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON CXX_STANDARD 17 CXX_EXTENSIONS OFF CXX_STANDARD_REQUIRED ON POSITION_INDEPENDENT_CODE ON INSTALL_RPATH \"${INSTALL_RPATH}\" PREFIX \"lib\" IMPORT_PREFIX \"lib\" OUTPUT_NAME GRM ) endforeach() unset(GRM_LINK_MODE) set_target_properties(grm_shared_internal PROPERTIES C_VISIBILITY_PRESET default) set_target_properties(grm_shared_internal PROPERTIES OUTPUT_NAME GRM_int) target_include_directories(grm_shared_internal PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/>) if(WIN32) target_compile_definitions(grm_static PRIVATE GR_STATIC_LIB) target_compile_definitions(grm_shared PRIVATE BUILDING_DLL) target_compile_definitions(grm_shared_internal PRIVATE BUILDING_DLL) endif() if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\") set_target_properties(grm_static PROPERTIES OUTPUT_NAME GRMstatic) endif() set_target_properties(grm_shared PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" EXPORT_NAME GRM) set_target_properties(grm_shared_internal PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\") add_library(GR::GRM ALIAS grm_shared) set(GR_REPORT \"GKS plugins:\\n\") add_library(cairoplugin SHARED lib/gks/plugin/cairoplugin.c) target_link_libraries(cairoplugin PUBLIC gks_static) if(Cairo_FOUND) target_link_libraries(cairoplugin PRIVATE Cairo::Cairo) target_link_libraries(cairoplugin PRIVATE Jpeg::Jpeg) string(APPEND GR_REPORT \"- cairoplugin:\\n\") string(APPEND GR_REPORT \"\\tPNG output: Yes\\n\") string(APPEND GR_REPORT \"\\tBMP output: Yes\\n\") string(APPEND GR_REPORT \"\\tJPEG output: Yes\\n\") if(Tiff_FOUND) target_link_libraries(cairoplugin PRIVATE Tiff::Tiff) string(APPEND GR_REPORT \"\\tTiff output: Yes\\n\") else() string(APPEND GR_REPORT \"\\tTiff output: No (libtiff not found)\\n\") target_compile_definitions(cairoplugin PRIVATE NO_TIFF) endif() target_compile_options(cairoplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) string(APPEND GR_REPORT \"\\tSixel output: Yes\\n\") string(APPEND GR_REPORT \"\\tMemory output: Yes\\n\") string(APPEND GR_REPORT \"\\tX11 output: No (not available in CMake build)\\n\") else() target_compile_definitions(cairoplugin PRIVATE NO_CAIRO) string(APPEND GR_REPORT \"- cairoplugin: No (Cairo not found)\\n\") endif() # Cairo X11 support is disabled to allow users to generate images using Cairo on systems without X11 installed target_compile_definitions(cairoplugin PRIVATE NO_X11) set_target_properties( cairoplugin PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) add_library(aggplugin SHARED lib/gks/plugin/aggplugin.cxx) target_link_libraries(aggplugin PUBLIC gks_static) if(Agg_FOUND) target_link_libraries(aggplugin PRIVATE Agg::Agg) target_link_libraries(aggplugin PRIVATE Jpeg::Jpeg) target_link_libraries(aggplugin PRIVATE Libpng::Libpng) string(APPEND GR_REPORT \"- aggplugin: Yes\\n\") target_compile_options(aggplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) if(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\") # Do not define `min` and `max` as macros because this can have unintended side effects (like error messages when # using `std::min`) target_compile_definitions(aggplugin PRIVATE NOMINMAX) endif() else() target_compile_definitions(aggplugin PRIVATE NO_AGG) string(APPEND GR_REPORT \"- aggplugin: No (Agg not found)\\n\") endif() set_target_properties( aggplugin PROPERTIES CXX_STANDARD 11 CXX_EXTENSIONS OFF CXX_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) add_library(videoplugin SHARED lib/gks/plugin/gif.c lib/gks/plugin/vc.c lib/gks/plugin/videoplugin.c) target_link_libraries(videoplugin PUBLIC gks_static) if(NOT Cairo_FOUND) target_compile_definitions(videoplugin PRIVATE NO_CAIRO) target_compile_definitions(videoplugin PRIVATE NO_AV) string(APPEND GR_REPORT \"- videoplugin: No (Cairo not found)\\n\") elseif(NOT Ffmpeg_FOUND) target_compile_definitions(videoplugin PRIVATE NO_CAIRO) target_compile_definitions(videoplugin PRIVATE NO_AV) string(APPEND GR_REPORT \"- videoplugin: No (ffmpeg / ogg / theora / vpx / openh264 not found)\\n\") else() string(APPEND GR_REPORT \"- videoplugin:\\n\") string(APPEND GR_REPORT \"\\tMP4 output: Yes\\n\") string(APPEND GR_REPORT \"\\tMOV output: Yes\\n\") string(APPEND GR_REPORT \"\\tWEBM output: Yes\\n\") string(APPEND GR_REPORT \"\\tOGG output: Yes\\n\") string(APPEND GR_REPORT \"\\tGIF output: Yes\\n\") string(APPEND GR_REPORT \"\\tAPNG output: Yes\\n\") target_link_libraries(videoplugin PUBLIC Ffmpeg::Ffmpeg) endif() target_compile_options(videoplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( videoplugin PROPERTIES C_STANDARD 99 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON LINKER_LANGUAGE CXX PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) add_library(pgfplugin SHARED lib/gks/plugin/pgfplugin.c) target_link_libraries(pgfplugin PUBLIC gks_static) target_link_libraries(pgfplugin PUBLIC Libpng::Libpng) target_compile_options(pgfplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( pgfplugin PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) string(APPEND GR_REPORT \"- pgfplugin:\\n\") string(APPEND GR_REPORT \"\\tTeX output: Yes\\n\") add_library(wmfplugin SHARED lib/gks/plugin/wmfplugin.c) target_link_libraries(wmfplugin PUBLIC gks_static) target_compile_options(wmfplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( wmfplugin PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) string(APPEND GR_REPORT \"- wmfplugin:\\n\") string(APPEND GR_REPORT \"\\tWMF output: Yes\\n\") add_library(gsplugin SHARED lib/gks/plugin/gsplugin.c) target_link_libraries(gsplugin PUBLIC gks_static) if(NOT TARGET X11::Xt) string(APPEND GR_REPORT \"- gsplugin: No (X11 / Xt not found)\\n\") target_compile_definitions(gsplugin PRIVATE NO_GS) elseif(NOT Gs_FOUND) string(APPEND GR_REPORT \"- gsplugin: No (ghostscript not found)\\n\") target_compile_definitions(gsplugin PRIVATE NO_GS) else() target_link_libraries(gsplugin PUBLIC X11::Xt) target_link_libraries(gsplugin PUBLIC Gs::Gs) string(APPEND GR_REPORT \"- gsplugin:\\n\") string(APPEND GR_REPORT \"\\tPNG outout: Yes\\n\") string(APPEND GR_REPORT \"\\tBMP outout: Yes\\n\") string(APPEND GR_REPORT \"\\tJPEG outout: Yes\\n\") string(APPEND GR_REPORT \"\\tTiff outout: Yes\\n\") endif() target_compile_options(gsplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( gsplugin PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) add_library(svgplugin SHARED lib/gks/plugin/svgplugin.c) target_link_libraries(svgplugin PUBLIC gks_static) target_link_libraries(svgplugin PUBLIC Libpng::Libpng) target_compile_options(svgplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( svgplugin PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) string(APPEND GR_REPORT \"- svgplugin:\\n\") string(APPEND GR_REPORT \"\\tSVG output: Yes\\n\") add_library(glplugin SHARED lib/gks/plugin/glplugin.c) target_link_libraries(glplugin PUBLIC gks_static) if(TARGET Glfw::Glfw AND OpenGL_FOUND) string(APPEND GR_REPORT \"- glplugin: Yes\\n\") target_link_libraries(glplugin PUBLIC Glfw::Glfw) target_link_libraries(glplugin PUBLIC OpenGL::GL) else() string(APPEND GR_REPORT \"- glplugin: No (GLFW / OpenGL not found)\\n\") target_compile_definitions(glplugin PRIVATE NO_GLFW) endif() if(NOT Freetype_FOUND) target_compile_definitions(glplugin PRIVATE NO_FT) endif() if(APPLE) # Apple has deprecated OpenGL in macOS 10.14 target_compile_definitions(glplugin PRIVATE GL_SILENCE_DEPRECATION) endif() target_compile_options(glplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( glplugin PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) add_library(zmqplugin SHARED lib/gks/plugin/zmqplugin.c) target_link_libraries(zmqplugin PUBLIC gks_static) if(ZeroMQ_FOUND) string(APPEND GR_REPORT \"- zmqplugin: Yes\\n\") target_link_libraries(zmqplugin PUBLIC ZeroMQ::ZeroMQ) else() string(APPEND GR_REPORT \"- zmqplugin: No (ZeroMQ not found)\\n\") target_compile_definitions(zmqplugin PRIVATE NO_ZMQ) endif() target_compile_options(zmqplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( zmqplugin PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) add_library(x11plugin SHARED lib/gks/plugin/x11plugin.c) target_link_libraries(x11plugin PUBLIC gks_static) if(NOT Freetype_FOUND) target_compile_definitions(x11plugin PRIVATE NO_FT) endif() if(TARGET X11::Xt) if(Xft_FOUND AND Fontconfig_FOUND) target_link_libraries(x11plugin PUBLIC Fontconfig::Fontconfig) else() target_compile_definitions(x11plugin PRIVATE NO_XFT) endif() target_link_libraries(x11plugin PUBLIC X11::Xt) string(APPEND GR_REPORT \"- x11plugin: Yes\\n\") else() target_compile_definitions(x11plugin PRIVATE NO_X11) target_compile_definitions(x11plugin PRIVATE NO_XFT) string(APPEND GR_REPORT \"- x11plugin: No (X11 / Xft / Xt not found)\\n\") endif() target_compile_options(x11plugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( x11plugin PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON CXX_STANDARD 11 CXX_EXTENSIONS OFF CXX_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) add_library(qt5plugin SHARED lib/gks/plugin/qt5plugin.cxx) target_link_libraries(qt5plugin PUBLIC gks_static) if(Qt5Widgets_FOUND) string(APPEND GR_REPORT \"- qt5plugin: Yes\\n\") target_link_libraries(qt5plugin PUBLIC Qt5::Widgets) else() string(APPEND GR_REPORT \"- qt5plugin: No (Qt5 not found)\\n\") target_compile_definitions(qt5plugin PRIVATE NO_QT5) endif() target_compile_options(qt5plugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( qt5plugin PROPERTIES CXX_STANDARD 11 CXX_EXTENSIONS OFF CXX_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) add_library(qt6plugin SHARED lib/gks/plugin/qt6plugin.cxx) target_link_libraries(qt6plugin PUBLIC gks_static) if(Qt6Widgets_FOUND AND ${CMAKE_VERSION} VERSION_GREATER \"3.16.0\") string(APPEND GR_REPORT \"- qt6plugin: Yes\\n\") target_link_libraries(qt6plugin PUBLIC Qt6::Widgets) set_target_properties( qt6plugin PROPERTIES CXX_STANDARD 17 CXX_EXTENSIONS OFF CXX_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) else() if(${CMAKE_VERSION} VERSION_LESS \"3.16.0\") string(APPEND GR_REPORT \"- qt6plugin: No (CMake version < 3.16.0)\\n\") else() string(APPEND GR_REPORT \"- qt6plugin: No (Qt6 not found)\\n\") endif() set_target_properties( qt6plugin PROPERTIES CXX_STANDARD 11 CXX_EXTENSIONS OFF CXX_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) target_compile_definitions(qt6plugin PRIVATE NO_QT6) endif() target_compile_options(qt6plugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) add_library(qtplugin SHARED lib/gks/plugin/qtplugin.cxx) target_link_libraries(qtplugin PUBLIC gks_static) if(Qt4_FOUND) string(APPEND GR_REPORT \"- qtplugin: Yes\\n\") target_link_libraries(qtplugin PUBLIC Qt4::QtGui) else() string(APPEND GR_REPORT \"- qtplugin: No (Qt4 not found)\\n\") target_compile_definitions(qtplugin PRIVATE NO_QT) endif() target_compile_options(qtplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( qtplugin PROPERTIES CXX_STANDARD 11 CXX_EXTENSIONS OFF CXX_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) add_library(gtkplugin SHARED lib/gks/plugin/gtkplugin.c) target_link_libraries(gtkplugin PUBLIC gks_static) set_target_properties( gtkplugin PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) target_compile_options(gtkplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) string(APPEND GR_REPORT \"- gtkplugin: No (not implemented yet)\\n\") if(APPLE) add_library(quartzplugin SHARED lib/gks/plugin/quartzplugin.m) target_link_libraries(quartzplugin PUBLIC gks_static) if(ZeroMQ_FOUND) target_link_libraries(quartzplugin PUBLIC ZeroMQ::ZeroMQ) target_link_libraries(quartzplugin PUBLIC \"-framework Foundation -framework ApplicationServices -framework AppKit\") target_compile_definitions(quartzplugin PRIVATE GRDIR=\"${GR_DIRECTORY}\") else() target_compile_definitions(quartzplugin PRIVATE NO_GKSTERM) endif() target_compile_options(quartzplugin PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( quartzplugin PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON PREFIX \"\" SUFFIX \"${GR_PLUGIN_SUFFIX}\" ) endif() if(Qt4_FOUND) add_library(qt4gr SHARED lib/gr/qtgr/grwidget.cxx) target_link_libraries(qt4gr PUBLIC GR::GR) target_link_libraries(qt4gr PUBLIC Qt4::QtCore Qt4::QtGui) if(MINGW AND CMAKE_SYSTEM_PROCESSOR STREQUAL \"i686\") target_compile_options(qt4gr PRIVATE -fno-exceptions) endif() target_compile_options(qt4gr PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties(qt4gr PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" INSTALL_RPATH \"${INSTALL_RPATH}\") if(GR_MANUAL_MOC_AND_RCC) set_target_properties(qt4gr PROPERTIES AUTOMOC OFF AUTORCC OFF) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_qt4_grwidget.cxx COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${QT4_MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.h -o ${CMAKE_CURRENT_BINARY_DIR}/moc_qt4_grwidget.cxx DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.cxx ) target_sources(qt4gr PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/moc_qt4_grwidget.cxx) else() set_target_properties(qt4gr PROPERTIES AUTOMOC ON AUTORCC ON) target_sources(qt4gr PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.h) endif() add_library(GR::qt4gr ALIAS qt4gr) endif() if(Qt5Widgets_FOUND AND Qt5Core_FOUND) add_library(qt5gr SHARED lib/gr/qtgr/grwidget.cxx) target_link_libraries(qt5gr PUBLIC GR::GR) target_link_libraries(qt5gr PUBLIC Qt5::Core Qt5::Widgets) if(MINGW AND CMAKE_SYSTEM_PROCESSOR STREQUAL \"i686\") target_compile_options(qt5gr PRIVATE -fno-exceptions) endif() target_compile_options(qt5gr PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties(qt5gr PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" INSTALL_RPATH \"${INSTALL_RPATH}\") if(GR_MANUAL_MOC_AND_RCC) set_target_properties(qt5gr PROPERTIES AUTOMOC OFF AUTORCC OFF) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_qt5_grwidget.cxx COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${QT5_MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.h -o ${CMAKE_CURRENT_BINARY_DIR}/moc_qt5_grwidget.cxx DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.cxx ) target_sources(qt5gr PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/moc_qt5_grwidget.cxx) else() set_target_properties(qt5gr PROPERTIES AUTOMOC ON AUTORCC ON) target_sources(qt5gr PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.h) endif() add_library(GR::qt5gr ALIAS qt5gr) endif() if(Qt6Widgets_FOUND AND Qt6Core_FOUND) add_library(qt6gr SHARED lib/gr/qtgr/grwidget.cxx) target_link_libraries(qt6gr PUBLIC GR::GR) target_link_libraries(qt6gr PUBLIC Qt6::Core Qt6::Widgets) if(MINGW AND CMAKE_SYSTEM_PROCESSOR STREQUAL \"i686\") target_compile_options(qt6gr PRIVATE -fno-exceptions) endif() target_compile_options(qt6gr PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties(qt6gr PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" INSTALL_RPATH \"${INSTALL_RPATH}\") if(GR_MANUAL_MOC_AND_RCC) set_target_properties(qt6gr PROPERTIES AUTOMOC OFF AUTORCC OFF) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_qt6_grwidget.cxx COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${QT6_MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.h -o ${CMAKE_CURRENT_BINARY_DIR}/moc_qt6_grwidget.cxx DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.cxx ) target_sources(qt6gr PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/moc_qt6_grwidget.cxx) else() set_target_properties(qt6gr PROPERTIES AUTOMOC ON AUTORCC ON) target_sources(qt6gr PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gr/qtgr/grwidget.h) endif() add_library(GR::qt6gr ALIAS qt6gr) endif() if(Qt6Widgets_FOUND AND Qt6Core_FOUND AND (NOT MINGW OR NOT CMAKE_SYSTEM_PROCESSOR STREQUAL \"i686\") ) add_library( grplotWidget SHARED lib/grm/grplot/gredit/AddElementWidget.cpp lib/grm/grplot/gredit/BoundingLogic.cpp lib/grm/grplot/gredit/BoundingObject.cpp lib/grm/grplot/gredit/CustomTreeWidgetItem.cpp lib/grm/grplot/gredit/EditElementWidget.cpp lib/grm/grplot/gredit/TableWidget.cpp lib/grm/grplot/gredit/TreeWidget.cpp lib/grm/grplot/grplotWidget.cxx lib/grm/grplot/qtterm/ArgsWrapper.cpp lib/grm/grplot/qtterm/Receiver.cpp lib/grm/grplot/util.cxx ) target_link_libraries(grplotWidget PUBLIC GR::GRM) target_link_libraries(grplotWidget PUBLIC GR::GR) target_link_libraries(grplotWidget PUBLIC Qt6::Core Qt6::Widgets) target_include_directories(grplotWidget INTERFACE $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}/grplotWidget>) target_compile_definitions(grplotWidget PRIVATE GRDIR=\"${GR_DIRECTORY}\") if(NOT TARGET XercesC::XercesC) target_compile_definitions(grplotWidget PRIVATE NO_XERCES_C) endif() target_compile_options(grplotWidget PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties(grplotWidget PROPERTIES SUFFIX \"${GR_SHARED_LIBRARY_SUFFIX}\" INSTALL_RPATH \"${INSTALL_RPATH}\") if(GR_MANUAL_MOC_AND_RCC) set_target_properties(grplotWidget PROPERTIES AUTOMOC OFF AUTORCC OFF) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_grplotWidget.cxx COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${QT6_MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplotWidget.hxx -o ${CMAKE_CURRENT_BINARY_DIR}/moc_grplotWidget.cxx DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplotWidget.cxx ) target_sources(grplotWidget PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/moc_grplotWidget.cxx) else() set_target_properties(grplotWidget PROPERTIES AUTOMOC ON AUTORCC ON) target_sources(grplotWidget PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplotWidget.hxx) endif() add_library(GR::grplotWidget ALIAS grplotWidget) endif() string(APPEND GR_REPORT \"\\nGKS applications:\\n\") if(APPLE) if(ZeroMQ_FOUND) if(GR_PREFER_XCODEBUILD) find_program(XCODE_BUILD xcodebuild CMAKE_FIND_ROOT_PATH_BOTH) if(XCODE_BUILD) execute_process(COMMAND ${XCODE_BUILD} -version RESULT_VARIABLE XCODE_BUILD_CHECK_RESULT) if(NOT XCODE_BUILD_CHECK_RESULT EQUAL 0) set(XCODE_BUILD \"XCODE_BUILD-NOTFOUND\") endif() endif() endif() if(GR_PREFER_XCODEBUILD AND XCODE_BUILD) set(GKSTERM_SOURCES lib/gks/quartz/GKSTerm.h lib/gks/quartz/GKSTerm.icns lib/gks/quartz/GKSTerm.m lib/gks/quartz/GKSTerm_Prefix.pch lib/gks/quartz/GKSView.h lib/gks/quartz/GKSView.m lib/gks/quartz/Info.plist lib/gks/quartz/MacOSXBundleInfo.plist.in lib/gks/quartz/main.m lib/gks/quartz/English.lproj/InfoPlist.strings lib/gks/quartz/English.lproj/ExtendSavePanel.nib/designable.nib lib/gks/quartz/English.lproj/ExtendSavePanel.nib/keyedobjects.nib lib/gks/quartz/English.lproj/MainMenu.nib/designable.nib lib/gks/quartz/English.lproj/MainMenu.nib/keyedobjects.nib lib/gks/quartz/GKSTerm.xcodeproj/project.pbxproj lib/gks/quartz/GKSTerm.xcodeproj/project.xcworkspace lib/gks/quartz/GKSTerm.xcodeproj/project.xcworkspace/contents.xcworkspacedata ) add_custom_target( GKSTerm ALL COMMAND ${XCODE_BUILD} -arch ${CMAKE_SYSTEM_PROCESSOR} -project GKSTerm.xcodeproj CONFIGURATION_BUILD_DIR=${CMAKE_CURRENT_BINARY_DIR}/GKSTerm WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm-build/quartz DEPENDS ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm-build/quartz/GKSTerm.xcodeproj ZeroMQ::ZeroMQ gks_static ) # Disable autoformat to not break the `sed` command # cmake-format: off add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm-build/quartz/GKSTerm.xcodeproj COMMAND ${CMAKE_COMMAND} -E copy_directory lib/gks ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm-build COMMAND sed -e s%../../../3rdparty/build/lib/libzmq.a%${ZeroMQ_LIBRARY}% -e s%../../../3rdparty/build/include/zmq.h%${ZeroMQ_INCLUDE_DIR}/zmq.h% ${CMAKE_SOURCE_DIR}/lib/gks/quartz/GKSTerm.xcodeproj/project.pbxproj >${CMAKE_CURRENT_BINARY_DIR}/GKSTerm-build/quartz/GKSTerm.xcodeproj/project.pbxproj WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} DEPENDS ${GKS_SOURCES} ${GKSTERM_SOURCES} ) # cmake-format: on string(APPEND GR_REPORT \"- GKSTerm: Yes (Xcode build)\\n\") else() add_executable(GKSTerm MACOSX_BUNDLE lib/gks/quartz/GKSTerm.m lib/gks/quartz/GKSView.m lib/gks/quartz/main.m) target_link_libraries(GKSTerm PUBLIC ZeroMQ::ZeroMQ) target_link_libraries( GKSTerm PUBLIC gks_static \"-framework CoreGraphics -framework CoreFoundation -framework CoreVideo -framework Cocoa\" ) add_custom_command( TARGET GKSTerm POST_BUILD COMMAND ${CMAKE_COMMAND} -E copy_directory ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/quartz/English.lproj ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm.app/Contents/Resources/English.lproj BYPRODUCTS ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm.app/Contents/Resources/English.lproj ) target_sources(GKSTerm PRIVATE lib/gks/quartz/GKSTerm.icns) set_source_files_properties(lib/gks/quartz/GKSTerm.icns PROPERTIES MACOSX_PACKAGE_LOCATION Resources/) target_compile_options(GKSTerm PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties( GKSTerm PROPERTIES MACOSX_BUNDLE_BUNDLE_NAME \"GKSTerm\" MACOSX_BUNDLE_GUI_IDENTIFIER \"de.fz-juelich.GKSTerm\" MACOSX_BUNDLE_ICON_FILE \"GKSTerm\" MACOSX_BUNDLE_BUNDLE_VERSION \"${GR_VERSION}\" MACOSX_BUNDLE_INFO_PLIST \"${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/quartz/MacOSXBundleInfo.plist.in\" ) set(GKSTerm_MACOSX_BUNDLE_SIGNATURE \"GKST\") set(GKSTerm_MACOSX_MAIN_NIB_FILE \"MainMenu\") string(APPEND GR_REPORT \"- GKSTerm: Yes (Build without Xcode)\\n\") endif() else() string(APPEND GR_REPORT \"- GKSTerm: No (ZeroMQ not found)\\n\") endif() endif() if(Qt4_FOUND OR (Qt5Widgets_FOUND AND Qt5Core_FOUND AND Qt5Network_FOUND ) OR (Qt6Widgets_FOUND AND Qt6Core_FOUND AND Qt6Network_FOUND ) ) add_executable(gksqt WIN32 MACOSX_BUNDLE lib/gks/qt/gksqt.cxx lib/gks/qt/gksserver.cxx lib/gks/qt/gkswidget.cxx) target_link_libraries(gksqt PUBLIC gks_static) if(Qt6Widgets_FOUND AND Qt6Core_FOUND AND Qt6Network_FOUND ) target_link_libraries(gksqt PUBLIC Qt6::Widgets Qt6::Core Qt6::Network) set(gksqt_INSTALL_RPATH \"${INSTALL_RPATH};${Qt6_LIBRARY_DIR}\") elseif( Qt5Widgets_FOUND AND Qt5Core_FOUND AND Qt5Network_FOUND ) target_link_libraries(gksqt PUBLIC Qt5::Widgets Qt5::Core Qt5::Network) set(gksqt_INSTALL_RPATH \"${INSTALL_RPATH};${Qt5_LIBRARY_DIR}\") elseif(Qt4_FOUND) target_link_libraries(gksqt PUBLIC Qt4::QtCore Qt4::QtGui Qt4::QtNetwork) set(gksqt_INSTALL_RPATH \"${INSTALL_RPATH};${Qt4_LIBRARY_DIR}\") endif() set_target_properties( gksqt PROPERTIES CXX_STANDARD 11 CXX_EXTENSIONS OFF CXX_STANDARD_REQUIRED ON INSTALL_RPATH \"${gksqt_INSTALL_RPATH}\" ) if(MINGW AND CMAKE_SYSTEM_PROCESSOR STREQUAL \"i686\") target_compile_options(gksqt PRIVATE -fno-exceptions) endif() target_compile_options(gksqt PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) if(GR_MANUAL_MOC_AND_RCC) set_target_properties(gksqt PROPERTIES AUTOMOC OFF AUTORCC OFF) if(Qt6Widgets_FOUND AND Qt6Core_FOUND AND Qt6Network_FOUND ) set(MOC_INCLUDE_FLAGS ${QT6_MOC_INCLUDE_FLAGS}) elseif( Qt5Widgets_FOUND AND Qt5Core_FOUND AND Qt5Network_FOUND ) set(MOC_INCLUDE_FLAGS ${QT5_MOC_INCLUDE_FLAGS}) else() set(MOC_INCLUDE_FLAGS ${QT4_MOC_INCLUDE_FLAGS}) endif() add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_gkswidget.cxx COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gkswidget.h -o ${CMAKE_CURRENT_BINARY_DIR}/moc_gkswidget.cxx DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gkswidget.h ) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_gksserver.cxx COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gksserver.h -o ${CMAKE_CURRENT_BINARY_DIR}/moc_gksserver.cxx DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gksserver.h ) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/qrc_gksqt.cxx COMMAND ${QT_RCC_EXECUTABLE} ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gksqt.qrc -o ${CMAKE_CURRENT_BINARY_DIR}/qrc_gksqt.cxx DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gksqt.qrc ) target_sources( gksqt PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/moc_gksserver.cxx ${CMAKE_CURRENT_BINARY_DIR}/moc_gkswidget.cxx ${CMAKE_CURRENT_BINARY_DIR}/qrc_gksqt.cxx ) else() set_target_properties(gksqt PROPERTIES AUTOMOC ON AUTORCC ON) target_sources( gksqt PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gksserver.h ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gkswidget.h ${CMAKE_CURRENT_SOURCE_DIR}/lib/gks/qt/gksqt.qrc ) endif() if(APPLE) target_sources(gksqt PRIVATE lib/gks/qt/gksqt.icns) set_source_files_properties(lib/gks/qt/gksqt.icns PROPERTIES MACOSX_PACKAGE_LOCATION Resources/) set_target_properties( gksqt PROPERTIES MACOSX_BUNDLE_BUNDLE_NAME \"gksqt\" MACOSX_BUNDLE_GUI_IDENTIFIER \"de.fz-juelich.gksqt\" MACOSX_BUNDLE_ICON_FILE \"gksqt\" MACOSX_BUNDLE_BUNDLE_VERSION \"${GR_VERSION}\" ) set(gksqt_MACOSX_BUNDLE_SIGNATURE \"gksqt\") set(gksqt_MACOSX_MAIN_NIB_FILE \"\") endif() string(APPEND GR_REPORT \"- gksqt: Yes\\n\") else() string(APPEND GR_REPORT \"- gksqt: No (Qt4 / Qt5 / Qt6 not found)\\n\") endif() if((Qt6Widgets_FOUND AND Qt6Core_FOUND AND Qt6Gui_FOUND ) OR (Qt5Widgets_FOUND AND Qt5Core_FOUND AND Qt5Gui_FOUND ) ) if(NOT MINGW OR NOT CMAKE_SYSTEM_PROCESSOR STREQUAL \"i686\") add_executable( grplot WIN32 MACOSX_BUNDLE lib/grm/grplot/gredit/AddElementWidget.cpp lib/grm/grplot/gredit/BoundingLogic.cpp lib/grm/grplot/gredit/BoundingObject.cpp lib/grm/grplot/gredit/CustomTreeWidgetItem.cpp lib/grm/grplot/gredit/EditElementWidget.cpp lib/grm/grplot/gredit/TableWidget.cpp lib/grm/grplot/gredit/TreeWidget.cpp lib/grm/grplot/grplot.cxx lib/grm/grplot/grplotMainwindow.cxx lib/grm/grplot/grplotWidget.cxx lib/grm/grplot/qtterm/ArgsWrapper.cpp lib/grm/grplot/qtterm/Receiver.cpp lib/grm/grplot/util.cxx ) if(Qt6Widgets_FOUND AND Qt6Core_FOUND) target_link_libraries(grplot PRIVATE Qt6::Widgets Qt6::Core Qt6::Gui grm_static) set(grplot_INSTALL_RPATH \"${INSTALL_RPATH};${Qt6_LIBRARY_DIR}\") else() target_link_libraries(grplot PRIVATE Qt5::Widgets Qt5::Core Qt5::Gui grm_static) set(grplot_INSTALL_RPATH \"${INSTALL_RPATH};${Qt5_LIBRARY_DIR}\") endif() target_compile_definitions(grplot PRIVATE GRDIR=\"${GR_DIRECTORY}\") if(NOT TARGET XercesC::XercesC) target_compile_definitions(grplot PRIVATE NO_XERCES_C) endif() set_target_properties( grplot PROPERTIES CXX_STANDARD 17 CXX_EXTENSIONS OFF CXX_STANDARD_REQUIRED ON INSTALL_RPATH \"${grplot_INSTALL_RPATH}\" ) if(WIN32) if(MINGW) target_link_options(grplot PRIVATE -mconsole) else() target_link_options(grplot PRIVATE /SUBSYSTEM:CONSOLE) endif() endif() if(GR_MANUAL_MOC_AND_RCC) set_target_properties(grplot PROPERTIES AUTOMOC OFF AUTORCC OFF) if(Qt6Widgets_FOUND AND Qt6Core_FOUND) set(MOC_INCLUDE_FLAGS ${QT6_MOC_INCLUDE_FLAGS}) else() set(MOC_INCLUDE_FLAGS ${QT5_MOC_INCLUDE_FLAGS}) endif() add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_grplotWidget.cxx COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplotWidget.hxx -o ${CMAKE_CURRENT_BINARY_DIR}/moc_grplotWidget.cxx DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplotWidget.hxx ) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_grplotMainwindow.cxx COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplotMainwindow.hxx -o ${CMAKE_CURRENT_BINARY_DIR}/moc_grplotMainwindow.cxx DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplotMainwindow.hxx ) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_ArgsWrapper.cpp COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/qtterm/ArgsWrapper.hxx -o ${CMAKE_CURRENT_BINARY_DIR}/moc_ArgsWrapper.cpp DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/qtterm/ArgsWrapper.hxx ) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_Receiver.cpp COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/qtterm/Receiver.hxx -o ${CMAKE_CURRENT_BINARY_DIR}/moc_Receiver.cpp DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/qtterm/Receiver.hxx ) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_BoundingLogic.cpp COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/BoundingLogic.hxx -o ${CMAKE_CURRENT_BINARY_DIR}/moc_BoundingLogic.cpp DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/BoundingLogic.hxx ) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_BoundingObject.cpp COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/BoundingObject.hxx -o ${CMAKE_CURRENT_BINARY_DIR}/moc_BoundingObject.cpp DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/BoundingObject.hxx ) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_CustomTreeWidgetItem.cpp COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/CustomTreeWidgetItem.hxx -o ${CMAKE_CURRENT_BINARY_DIR}/moc_CustomTreeWidgetItem.cpp DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/CustomTreeWidgetItem.hxx ) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_TreeWidget.cpp COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/TreeWidget.hxx -o ${CMAKE_CURRENT_BINARY_DIR}/moc_TreeWidget.cpp DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/TreeWidget.hxx ) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_AddElementWidget.cpp COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/AddElementWidget.hxx -o ${CMAKE_CURRENT_BINARY_DIR}/moc_AddElementWidget.cpp DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/AddElementWidget.hxx ) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_EditElementWidget.cpp COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/EditElementWidget.hxx -o ${CMAKE_CURRENT_BINARY_DIR}/moc_EditElementWidget.cpp DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/EditElementWidget.hxx ) add_custom_command( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/moc_TableWidget.cpp COMMAND ${QT_MOC_EXECUTABLE} -DGRDIR=\\\"$(GR_DIRECTORY)\\\" ${MOC_INCLUDE_FLAGS} ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/TableWidget.hxx -o ${CMAKE_CURRENT_BINARY_DIR}/moc_TableWidget.cpp DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/TableWidget.hxx ) target_sources( grplot PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/moc_grplotMainwindow.cxx ${CMAKE_CURRENT_BINARY_DIR}/moc_grplotWidget.cxx ${CMAKE_CURRENT_BINARY_DIR}/moc_AddElementWidget.cpp ${CMAKE_CURRENT_BINARY_DIR}/moc_BoundingLogic.cpp ${CMAKE_CURRENT_BINARY_DIR}/moc_BoundingObject.cpp ${CMAKE_CURRENT_BINARY_DIR}/moc_CustomTreeWidgetItem.cpp ${CMAKE_CURRENT_BINARY_DIR}/moc_EditElementWidget.cpp ${CMAKE_CURRENT_BINARY_DIR}/moc_TableWidget.cpp ${CMAKE_CURRENT_BINARY_DIR}/moc_TreeWidget.cpp ${CMAKE_CURRENT_BINARY_DIR}/moc_ArgsWrapper.cpp ${CMAKE_CURRENT_BINARY_DIR}/moc_Receiver.cpp ) else() set_target_properties(grplot PROPERTIES AUTOMOC ON AUTORCC ON) target_sources( grplot PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplotMainwindow.hxx ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/grplotWidget.hxx ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/AddElementWidget.hxx ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/BoundingLogic.hxx ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/BoundingObject.hxx ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/CustomTreeWidgetItem.hxx ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/EditElementWidget.hxx ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/TableWidget.hxx ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/gredit/TreeWidget.hxx ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/qtterm/ArgsWrapper.hxx ${CMAKE_CURRENT_SOURCE_DIR}/lib/grm/grplot/qtterm/Receiver.hxx ) endif() if(APPLE) set_target_properties(grplot PROPERTIES MACOSX_BUNDLE_GUI_IDENTIFIER \"de.fz-juelich.grplot\") endif() if(WIN32) target_compile_definitions(grplot PRIVATE GR_STATIC_LIB) endif() target_compile_options(grplot PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) else() add_executable(grplot lib/grm/grplot/grplot.cxx) endif() string(APPEND GR_REPORT \"- grplot: Yes\\n\") else() string(APPEND GR_REPORT \"- grplot: No (Qt6 and Qt5 not found)\\n\") endif() string(APPEND GR_REPORT \"\\nGRM integrations:\\n\") if(TARGET XercesC::XercesC) string(APPEND GR_REPORT \"- Xerces-C++: Yes\\n\") else() string(APPEND GR_REPORT \"- Xerces-C++: No\\n\") endif() if(GR_BUILD_DEMOS) add_executable(gksdemo lib/gks/demo.c) target_link_libraries(gksdemo PUBLIC gks_static) target_compile_options(gksdemo PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties(gksdemo PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON) add_executable(grdemo lib/gr/demo.c) target_link_libraries(grdemo PUBLIC GR::GR) target_link_libraries(grdemo PUBLIC GR::GKS) target_compile_options(grdemo PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties(grdemo PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON) add_subdirectory(lib/grm/test/public_api/grm grm_test_public_api) add_subdirectory(lib/grm/test/internal_api/grm grm_test_internal_api) endif() if(GR_BUILD_GKSM) add_executable(gksm lib/gks/gksm.c) target_link_libraries(gksm PUBLIC gks_static) target_compile_options(gksm PRIVATE ${COMPILER_OPTION_ERROR_IMPLICIT}) set_target_properties(gksm PROPERTIES C_STANDARD 90 C_EXTENSIONS OFF C_STANDARD_REQUIRED ON) endif() if(GR_INSTALL) install(FILES LICENSE.md DESTINATION ${CMAKE_INSTALL_DATADIR}/licenses/GR) install( FILES lib/grm/src/grm/dom_render/graphics_tree/schema.xsd DESTINATION ${CMAKE_INSTALL_DATADIR}/xml/GRM RENAME grm_graphics_tree_schema.xsd ) install( FILES lib/grm/src/grm/dom_render/graphics_tree/private_schema.xsd DESTINATION ${CMAKE_INSTALL_DATADIR}/xml/GRM RENAME grm_graphics_tree_private_schema.xsd ) include(CMakePackageConfigHelpers) configure_package_config_file( \"cmake/Config.cmake.in\" \"GRConfig.cmake\" INSTALL_DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/GR\" ) write_basic_package_version_file( ${CMAKE_CURRENT_BINARY_DIR}/GRConfigVersion.cmake VERSION ${GR_VERSION} COMPATIBILITY SameMajorVersion ) install(FILES \"${CMAKE_CURRENT_BINARY_DIR}/GRConfig.cmake\" \"${CMAKE_CURRENT_BINARY_DIR}/GRConfigVersion.cmake\" DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/GR\" ) install( TARGETS gks_shared gr_shared gr3_shared grm_shared EXPORT GRTargets ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} ) if(TARGET qt4gr) install( TARGETS qt4gr EXPORT GRTargets ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} ) endif() if(TARGET qt5gr) install( TARGETS qt5gr EXPORT GRTargets ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} ) endif() if(TARGET qt6gr) install( TARGETS qt6gr EXPORT GRTargets ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} ) endif() if(TARGET grplotWidget) install( TARGETS grplotWidget EXPORT GRTargets ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} ) endif() install( EXPORT GRTargets DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/GR\" NAMESPACE GR:: ) foreach(PACKAGE gr gr3 gks grm) configure_file(\"${PACKAGE}.pc.in\" \"${CMAKE_CURRENT_BINARY_DIR}/${PACKAGE}.pc\" @ONLY) install(FILES \"${CMAKE_CURRENT_BINARY_DIR}/${PACKAGE}.pc\" DESTINATION \"${CMAKE_INSTALL_LIBDIR}/pkgconfig/\") endforeach() install( TARGETS gks_static gr_static gr3_static grm_static aggplugin cairoplugin glplugin gsplugin gtkplugin pgfplugin svgplugin videoplugin wmfplugin x11plugin zmqplugin qtplugin qt5plugin qt6plugin ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} ) if(TARGET gr3platform) install( TARGETS gr3platform LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} ) endif() if(TARGET qt4gr) install( TARGETS qt4gr ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} ) endif() if(TARGET qt5gr) install( TARGETS qt5gr ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} ) endif() if(TARGET qt6gr) install( TARGETS qt6gr ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} ) endif() if(TARGET quartzplugin) install( TARGETS quartzplugin ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} ) endif() if(TARGET GKSTerm) if(XCODE_BUILD) install( DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/GKSTerm/GKSTerm.app DESTINATION Applications USE_SOURCE_PERMISSIONS ) else() install(TARGETS GKSTerm BUNDLE DESTINATION Applications) endif() endif() if(TARGET gksqt) install( TARGETS gksqt RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} BUNDLE DESTINATION Applications ) endif() if(TARGET grplotWidget) install( TARGETS grplotWidget RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} BUNDLE DESTINATION Applications ) endif() if(TARGET grplot) install( TARGETS grplot RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR} BUNDLE DESTINATION Applications ) install( FILES lib/grm/grplot/README.md DESTINATION ${CMAKE_INSTALL_DATAROOTDIR}/doc/grplot RENAME grplot.man.md ) endif() install(FILES lib/gr/gr.h lib/gks/gks.h lib/gr3/gr3.h lib/grm/include/grm.h DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/) install(DIRECTORY lib/grm/include/grm DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/) if(TARGET qt4gr OR TARGET qt5gr OR TARGET qt6gr ) install(FILES lib/gr/qtgr/grwidget.h DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/) endif() if(TARGET grplotWidget) install(FILES lib/grm/grplot/grplotWidget.hxx lib/grm/grplot/util.hxx DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/grplotWidget ) file(GLOB GREDIT_HEADERS \"lib/grm/grplot/gredit/*.h\") install(FILES ${GREDIT_HEADERS} DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/grplotWidget/gredit) file(GLOB QTTERM_HEADERS \"lib/grm/grplot/qtterm/*.h\") install(FILES ${QTTERM_HEADERS} DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/grplotWidget/qtterm) endif() install( DIRECTORY lib/gks/fonts DESTINATION ${CMAKE_INSTALL_PREFIX}/ USE_SOURCE_PERMISSIONS ) endif() message(${GR_REPORT})\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/gstools",
            "repo_link": "https://github.com/GeoStat-Framework/GSTools",
            "content": {
                "codemeta": "",
                "readme": "# Welcome to GSTools [![GMD](https://img.shields.io/badge/GMD-10.5194%2Fgmd--15--3161--2022-orange)](https://doi.org/10.5194/gmd-15-3161-2022) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1313628.svg)](https://doi.org/10.5281/zenodo.1313628) [![PyPI version](https://badge.fury.io/py/gstools.svg)](https://badge.fury.io/py/gstools) [![Conda Version](https://img.shields.io/conda/vn/conda-forge/gstools.svg)](https://anaconda.org/conda-forge/gstools) [![Build Status](https://github.com/GeoStat-Framework/GSTools/workflows/Continuous%20Integration/badge.svg?branch=main)](https://github.com/GeoStat-Framework/GSTools/actions) [![Coverage Status](https://coveralls.io/repos/github/GeoStat-Framework/GSTools/badge.svg?branch=main)](https://coveralls.io/github/GeoStat-Framework/GSTools?branch=main) [![Documentation Status](https://readthedocs.org/projects/gstools/badge/?version=latest)](https://geostat-framework.readthedocs.io/projects/gstools/en/stable/?badge=stable) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black) <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/GeoStat-Framework/GSTools/main/docs/source/pics/gstools.png\" alt=\"GSTools-LOGO\" width=\"251px\"/> </p> <p align=\"center\"><b>Get in Touch!</b></p> <p align=\"center\"> <a href=\"https://github.com/GeoStat-Framework/GSTools/discussions\"><img src=\"https://img.shields.io/badge/GitHub-Discussions-f6f8fa?logo=github&style=flat\" alt=\"GH-Discussions\"/></a> <a href=\"https://swung.slack.com/messages/gstools\"><img src=\"https://img.shields.io/badge/Swung-Slack-4A154B?style=flat&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAABoAAAAaCAYAAACpSkzOAAAABmJLR0QA%2FwD%2FAP%2BgvaeTAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAAB3RJTUUH5AYaFSENGSa5qgAABmZJREFUSMeFlltsVNcVhr%2B1z5m7Zzy%2BxaBwcQrGQOpgCAkKtSBQIqJepKhPBULpQ6sKBVWVKqXtSy%2BR0qYXqa2qRmlDCzjBEZGKUCK1TWqlNiGIEKDQBtf4Fki4OIxnxrex53LOXn2YwbjEtOvlHG3tvX%2Btf%2B21%2Fl%2BYJ1QVEbn1vwLYBWwCVgG1lW0ZoA%2FoAQ6LSP%2BdZ%2BeGzAMiIqK%2Bem0GpxNYVeBj3j2b4NCfM2QnfAAaa11al4fZuCZK24owQJ9v%2BbLryIVbd9wVSNUaEWNVtQPYfXHmAD0T32ZJeBM1Q8d0zzMDUpMwAFgLJU%2BxClURw9NfqedLWxMAHSKyR1WNiNhPAM0B6c%2FbdPORTLuOeUMSNkmMBHgyeo32bwwRDMh8bDM%2BZVl0j6uvPrdYknFnSESWzwUzt%2BkyVlUHx7zh5j%2BmPkXBjosjLkWdominiMQ%2BoiEZxuq8OFRXGXJ5K5%2Fde5nha8VlqjooIlZVBcBUiqeqemjGppd1ptfhSpS8pmmN7GVf4whPNY4Di9m%2BMcR03nK3sBbCQeFbv7gBsExVOyp3l6nz1VtjcM4fTK3Uok5IXtPsrHuPevcBXk8d4dWPX6I%2BsIB9wf1s%2B2Y%2FVbFynUIBIeDeplIECiXl5Iv3kbLogogRgbWukfNumT%2FnlYszBxj3hwXg0cQvqXcfYNu5tVyYPE%2B1G8dXn%2BfW72fH49U8sSlOPGr4SccoF4cKs3WzFrY%2BFCMUNmz%2Ba0aeWR1l15JwJ7DaVPpk1YnJ7xIxtQRNjDXRvTx%2F9ef0Tl0g6SYQhAlvmkH%2Fgv74qUaiTSG8ewJ0%2FGgRK5aG8Cts5ouWDa1RxoDRovK9i9MAq1S12QA7b5ROUdBxBIeQ1ACG49m%2FEXPis7Qk3ChHbx6Qw1dgXVeWB7uyDOctP%2Fx6w2zdrIVIyFCyiq8wXlJOZzyAXQbY%2FGGhC8EAilJ%2BVg7ufxU6IAHeSvewfQEadiDuCr%2B6NE1LU4hwUFAF1xFGRkvEjVDlgiPwVqoEsNkAq0ZKp3EIYrFM2xGm7Uc8u%2FzXjHkTmHIHoCiDM73E3IIsDCtRV3gn7QHQ0hTCt0ooKLw%2FWCAM1AcNISOcHSsBrDRAbc7eQMQBFFciHM18kaZIMz3r%2F0HO5mazytsiw%2FmTtCYiGGCkQlltwkEVjMDVmyUA6oIGR%2BDGjAWoM3f2giHAhH%2BFI5nPsDrWxqWNE9S4tUz5k1S7cQ5df4k9S6qY9JRipXtr4w5WQYH0eHkWrqxy8FTn3AvpmFmIqj%2B76EiQjNfHH1JNWFKc3vABj9V9npw%2FRXfmBNsaoTRnRAQDAgqqMJr1KBWUtUmHaR8WRgzAqAH6FgYexqd4R2Yuns5wcLSFK4U36bj%2FdbbUbGdoZoCi3uS%2Bqtt73TlNWygpqXGfZTGXnKesrwkA9BmgZ0noMZT5R0tQ4hzLfo4rhS46W%2F%2BCAn3T7%2BhDySiWMl2RkHArP8dAesKjPixYVbbUBwB6DHB4QWADIamuHPtkhE0t3ZP7ANhe9zgvXP2dfK0pymRJmQLiEYNW6mEVljYGuDzlkwwaHq51AQ4bERkAetvjP2XCT6H480AJeZsB4N7QYt7OnuSROtRXJV2wNNS4qIJvlbUtERJxhxcv5%2FlNWwygV0QGyzKBv%2FP%2ByFfZXf%2ButoR3UuXcS95mKNgxSjpN3qZZFHwUgFPjx5n2c9wo9ktrtcOZtMeWB2NEw4b2thivPLuIS1M%2BAzmrTy4O4ys7Zv1B5fsnVdWCr7PxYf7vej73ex2YeU1VVY9nu7ShG63vRo%2Fe%2FK1%2B518FbXkjo3OjO1XU2LFRzRZ9VdWDczFQ1VsCOHgpd1G%2FcG6jHrj2vPbn%2BjVdHNfr%2BRH92eXva2MPuvxEQpe%2BHdEnzm%2FQf4%2BrRo%2BldMUbGd393oS2dWU0cDSlw1OequrALVG9Q8rLsquqg2OlzLL2Myu1N5eShgB4CjEnSMSJYrX8Oj0t8UH7NMnX0iSDwmhBWRl3tKs9IcmgGRSRZqtqzFwpL4uWWKvWiMjyZKC24%2F1HbsrLn95Pwk3gCpS0yIw%2Fg6clPC2RLc3QmzvJupoARQsvrItxZmtSkkFz6E6Q%2F2m3PFta44jbCaw%2BO3GK7uybnJs8xfXC1fLYCdTz9NIfsCS0mYVhAHp9ZYdr5J%2F%2F127dxUA2AzuBzRUDWVfZlq4YyG6gs9ImdzWQ%2FwFNRlgCFdG5bAAAAABJRU5ErkJggg%3D%3D\" alt=\"Slack-Swung\"/></a> <a href=\"https://gitter.im/GeoStat-Framework/GSTools\"><img src=\"https://img.shields.io/badge/Gitter-GeoStat--Framework-ed1965?logo=gitter&style=flat\" alt=\"Gitter-GSTools\"/></a> <a href=\"mailto:info@geostat-framework.org\"><img src=\"https://img.shields.io/badge/Email-GeoStat--Framework-468a88?style=flat&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbDpzcGFjZT0icHJlc2VydmUiIHdpZHRoPSI1MDAiIGhlaWdodD0iNTAwIj48cGF0aCBkPSJNNDQ4IDg4SDUyYy0yNyAwLTQ5IDIyLTQ5IDQ5djIyNmMwIDI3IDIyIDQ5IDQ5IDQ5aDM5NmMyNyAwIDQ5LTIyIDQ5LTQ5VjEzN2MwLTI3LTIyLTQ5LTQ5LTQ5em0xNiA0OXYyMjZsLTIgNy0xMTUtMTE2IDExNy0xMTd6TTM2IDM2M1YxMzdsMTE3IDExN0wzOCAzNzBsLTItN3ptMjE5LTYzYy0zIDMtNyAzLTEwIDBMNjYgMTIxaDM2OHptLTc5LTIzIDQ2IDQ2YTM5IDM5IDAgMCAwIDU2IDBsNDYtNDYgMTAxIDEwMkg3NXoiIHN0eWxlPSJmaWxsOiNmNWY1ZjU7ZmlsbC1vcGFjaXR5OjEiLz48L3N2Zz4=\" alt=\"Email\"/></a> <a href=\"https://twitter.com/GSFramework\"><img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/GSFramework?style=social\"></a> </p> <p align=\"center\"><b>Youtube Tutorial on GSTools</b><br></p> <p align=\"center\"> <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=qZBJ-AZXq6Q\" target=\"_blank\"> <img src=\"http://img.youtube.com/vi/qZBJ-AZXq6Q/0.jpg\" alt=\"GSTools Transform 22 tutorial\" width=\"480\" height=\"360\" border=\"0\" /> </a> </p> ## Purpose <img align=\"right\" width=\"450\" src=\"https://raw.githubusercontent.com/GeoStat-Framework/GSTools/main/docs/source/pics/demonstrator.png\" alt=\"\"> GSTools provides geostatistical tools for various purposes: - random field generation, including periodic boundaries - simple, ordinary, universal and external drift kriging - conditioned field generation - incompressible random vector field generation - (automated) variogram estimation and fitting - directional variogram estimation and modelling - data normalization and transformation - many readily provided and even user-defined covariance models - metric spatio-temporal modelling - plurigaussian field simulations (PGS) - plotting and exporting routines ## Installation ### conda GSTools can be installed via [conda][conda_link] on Linux, Mac, and Windows. Install the package by typing the following command in a command terminal: conda install gstools In case conda forge is not set up for your system yet, see the easy to follow instructions on [conda forge][conda_forge_link]. Using conda, the parallelized version of GSTools should be installed. ### pip GSTools can be installed via [pip][pip_link] on Linux, Mac, and Windows. On Windows you can install [WinPython][winpy_link] to get Python and pip running. Install the package by typing the following command in a command terminal: pip install gstools To install the latest development version via pip, see the [documentation][doc_install_link]. One thing to point out is that this way, the non-parallel version of GSTools is installed. In case you want the parallel version, follow these easy [steps][doc_install_link]. ## Citation If you are using GSTools in your publication please cite our paper: > Müller, S., Schüler, L., Zech, A., and Heße, F.: > GSTools v1.3: a toolbox for geostatistical modelling in Python, > Geosci. Model Dev., 15, 3161-3182, https://doi.org/10.5194/gmd-15-3161-2022, 2022. You can cite the Zenodo code publication of GSTools by: > Sebastian Müller & Lennart Schüler. GeoStat-Framework/GSTools. Zenodo. https://doi.org/10.5281/zenodo.1313628 If you want to cite a specific version, have a look at the [Zenodo site](https://doi.org/10.5281/zenodo.1313628). ## Documentation for GSTools You can find the documentation under [geostat-framework.readthedocs.io][doc_link]. ### Tutorials and Examples The documentation also includes some [tutorials][tut_link], showing the most important use cases of GSTools, which are - [Random Field Generation][tut1_link] - [The Covariance Model][tut2_link] - [Variogram Estimation][tut3_link] - [Random Vector Field Generation][tut4_link] - [Kriging][tut5_link] - [Conditioned random field generation][tut6_link] - [Field transformations][tut7_link] - [Geographic Coordinates][tut8_link] - [Spatio-Temporal Modelling][tut9_link] - [Normalizing Data][tut10_link] - [Plurigaussian Field Generation (PGS)][tut11_link] - [Miscellaneous examples][tut0_link] The associated python scripts are provided in the `examples` folder. ## Spatial Random Field Generation The core of this library is the generation of spatial random fields. These fields are generated using the randomisation method, described by [Heße et al. 2014][rand_link]. [rand_link]: https://doi.org/10.1016/j.envsoft.2014.01.013 ### Examples #### Gaussian Covariance Model This is an example of how to generate a 2 dimensional spatial random field with a gaussian covariance model. ```python import gstools as gs # structured field with a size 100x100 and a grid-size of 1x1 x = y = range(100) model = gs.Gaussian(dim=2, var=1, len_scale=10) srf = gs.SRF(model) srf((x, y), mesh_type='structured') srf.plot() ``` <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/GeoStat-Framework/GSTools/main/docs/source/pics/gau_field.png\" alt=\"Random field\" width=\"600px\"/> </p> GSTools also provides support for [geographic coordinates](https://en.wikipedia.org/wiki/Geographic_coordinate_system). This works perfectly well with [cartopy](https://scitools.org.uk/cartopy/docs/latest/index.html). ```python import matplotlib.pyplot as plt import cartopy.crs as ccrs import gstools as gs # define a structured field by latitude and longitude lat = lon = range(-80, 81) model = gs.Gaussian(latlon=True, len_scale=777, geo_scale=gs.KM_SCALE) srf = gs.SRF(model, seed=12345) field = srf.structured((lat, lon)) # Orthographic plotting with cartopy ax = plt.subplot(projection=ccrs.Orthographic(-45, 45)) cont = ax.contourf(lon, lat, field, transform=ccrs.PlateCarree()) ax.coastlines() ax.set_global() plt.colorbar(cont) ``` <p align=\"center\"> <img src=\"https://github.com/GeoStat-Framework/GeoStat-Framework.github.io/raw/master/img/GS_globe.png\" alt=\"lat-lon random field\" width=\"600px\"/> </p> A similar example but for a three dimensional field is exported to a [VTK](https://vtk.org/) file, which can be visualized with [ParaView](https://www.paraview.org/) or [PyVista](https://docs.pyvista.org) in Python: ```python import gstools as gs # structured field with a size 100x100x100 and a grid-size of 1x1x1 x = y = z = range(100) model = gs.Gaussian(dim=3, len_scale=[16, 8, 4], angles=(0.8, 0.4, 0.2)) srf = gs.SRF(model) srf((x, y, z), mesh_type='structured') srf.vtk_export('3d_field') # Save to a VTK file for ParaView mesh = srf.to_pyvista() # Create a PyVista mesh for plotting in Python mesh.contour(isosurfaces=8).plot() ``` <p align=\"center\"> <img src=\"https://github.com/GeoStat-Framework/GeoStat-Framework.github.io/raw/master/img/GS_pyvista.png\" alt=\"3d Random field\" width=\"600px\"/> </p> ## Estimating and Fitting Variograms The spatial structure of a field can be analyzed with the variogram, which contains the same information as the covariance function. All covariance models can be used to fit given variogram data by a simple interface. ### Example This is an example of how to estimate the variogram of a 2 dimensional unstructured field and estimate the parameters of the covariance model again. ```python import numpy as np import gstools as gs # generate a synthetic field with an exponential model x = np.random.RandomState(19970221).rand(1000) * 100. y = np.random.RandomState(20011012).rand(1000) * 100. model = gs.Exponential(dim=2, var=2, len_scale=8) srf = gs.SRF(model, mean=0, seed=19970221) field = srf((x, y)) # estimate the variogram of the field bin_center, gamma = gs.vario_estimate((x, y), field) # fit the variogram with a stable model. (no nugget fitted) fit_model = gs.Stable(dim=2) fit_model.fit_variogram(bin_center, gamma, nugget=False) # output ax = fit_model.plot(x_max=max(bin_center)) ax.scatter(bin_center, gamma) print(fit_model) ``` Which gives: ```python Stable(dim=2, var=1.85, len_scale=7.42, nugget=0.0, anis=[1.0], angles=[0.0], alpha=1.09) ``` <p align=\"center\"> <img src=\"https://github.com/GeoStat-Framework/GeoStat-Framework.github.io/raw/master/img/GS_vario_est.png\" alt=\"Variogram\" width=\"600px\"/> </p> ## Kriging and Conditioned Random Fields An important part of geostatistics is Kriging and conditioning spatial random fields to measurements. With conditioned random fields, an ensemble of field realizations with their variability depending on the proximity of the measurements can be generated. ### Example For better visualization, we will condition a 1d field to a few \"measurements\", generate 100 realizations and plot them: ```python import numpy as np import matplotlib.pyplot as plt import gstools as gs # conditions cond_pos = [0.3, 1.9, 1.1, 3.3, 4.7] cond_val = [0.47, 0.56, 0.74, 1.47, 1.74] # conditioned spatial random field class model = gs.Gaussian(dim=1, var=0.5, len_scale=2) krige = gs.krige.Ordinary(model, cond_pos, cond_val) cond_srf = gs.CondSRF(krige) # same output positions for all ensemble members grid_pos = np.linspace(0.0, 15.0, 151) cond_srf.set_pos(grid_pos) # seeded ensemble generation seed = gs.random.MasterRNG(20170519) for i in range(100): field = cond_srf(seed=seed(), store=f\"field_{i}\") plt.plot(grid_pos, field, color=\"k\", alpha=0.1) plt.scatter(cond_pos, cond_val, color=\"k\") plt.show() ``` <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/GeoStat-Framework/GSTools/main/docs/source/pics/cond_ens.png\" alt=\"Conditioned\" width=\"600px\"/> </p> ## User Defined Covariance Models One of the core-features of GSTools is the powerful [CovModel][cov_link] class, which allows to easy define covariance models by the user. ### Example Here we re-implement the Gaussian covariance model by defining just a [correlation][cor_link] function, which takes a non-dimensional distance ``h = r/l``: ```python import numpy as np import gstools as gs # use CovModel as the base-class class Gau(gs.CovModel): def cor(self, h): return np.exp(-h**2) ``` And that's it! With ``Gau`` you now have a fully working covariance model, which you could use for field generation or variogram fitting as shown above. Have a look at the [documentation ][doc_link] for further information on incorporating optional parameters and optimizations. ## Incompressible Vector Field Generation Using the original [Kraichnan method][kraichnan_link], incompressible random spatial vector fields can be generated. ### Example ```python import numpy as np import gstools as gs x = np.arange(100) y = np.arange(100) model = gs.Gaussian(dim=2, var=1, len_scale=10) srf = gs.SRF(model, generator='VectorField', seed=19841203) srf((x, y), mesh_type='structured') srf.plot() ``` yielding <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/GeoStat-Framework/GSTools/main/docs/source/pics/vec_srf_tut_gau.png\" alt=\"vector field\" width=\"600px\"/> </p> [kraichnan_link]: https://doi.org/10.1063/1.1692799 ## Plurigaussian Field Simulation (PGS) With PGS, more complex categorical (or discrete) fields can be created. ### Example ```python import gstools as gs import numpy as np import matplotlib.pyplot as plt N = [180, 140] x, y = range(N[0]), range(N[1]) # we need 2 SRFs model = gs.Gaussian(dim=2, var=1, len_scale=5) srf = gs.SRF(model) field1 = srf.structured([x, y], seed=20170519) field2 = srf.structured([x, y], seed=19970221) # with `lithotypes`, we prescribe the categorical data and its relations # here, we use 2 categories separated by a rectangle. rect = [40, 32] lithotypes = np.zeros(N) lithotypes[ N[0] // 2 - rect[0] // 2 : N[0] // 2 + rect[0] // 2, N[1] // 2 - rect[1] // 2 : N[1] // 2 + rect[1] // 2, ] = 1 pgs = gs.PGS(2, [field1, field2]) P = pgs(lithotypes) fig, axs = plt.subplots(1, 2) axs[0].imshow(lithotypes, cmap=\"copper\") axs[1].imshow(P, cmap=\"copper\") plt.show() ``` <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/GeoStat-Framework/GSTools/main/docs/source/pics/2d_pgs.png\" alt=\"PGS\" width=\"600px\"/> </p> ## VTK/PyVista Export After you have created a field, you may want to save it to file, so we provide a handy [VTK][vtk_link] export routine using the `.vtk_export()` or you could create a VTK/PyVista dataset for use in Python with to `.to_pyvista()` method: ```python import gstools as gs x = y = range(100) model = gs.Gaussian(dim=2, var=1, len_scale=10) srf = gs.SRF(model) srf((x, y), mesh_type='structured') srf.vtk_export(\"field\") # Saves to a VTK file mesh = srf.to_pyvista() # Create a VTK/PyVista dataset in memory mesh.plot() ``` Which gives a RectilinearGrid VTK file ``field.vtr`` or creates a PyVista mesh in memory for immediate 3D plotting in Python. <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/GeoStat-Framework/GSTools/main/docs/source/pics/pyvista_export.png\" alt=\"pyvista export\" width=\"600px\"/> </p> ## Requirements: - [NumPy >= 1.20.0](https://www.numpy.org) - [SciPy >= 1.1.0](https://www.scipy.org/scipylib) - [hankel >= 1.0.0](https://github.com/steven-murray/hankel) - [emcee >= 3.0.0](https://github.com/dfm/emcee) - [pyevtk >= 1.1.1](https://github.com/pyscience-projects/pyevtk) - [meshio >= 5.1.0](https://github.com/nschloe/meshio) ### Optional - [GSTools-Core >= 0.2.0](https://github.com/GeoStat-Framework/GSTools-Core) - [matplotlib](https://matplotlib.org) - [pyvista](https://docs.pyvista.org/) ## Contact You can contact us via <info@geostat-framework.org>. ## License [LGPLv3][license_link] © 2018-2025 [pip_link]: https://pypi.org/project/gstools [conda_link]: https://docs.conda.io/en/latest/miniconda.html [conda_forge_link]: https://github.com/conda-forge/gstools-feedstock#installing-gstools [conda_pip]: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-pkgs.html#installing-non-conda-packages [pipiflag]: https://pip-python3.readthedocs.io/en/latest/reference/pip_install.html?highlight=i#cmdoption-i [winpy_link]: https://winpython.github.io/ [license_link]: https://github.com/GeoStat-Framework/GSTools/blob/main/LICENSE [cov_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/generated/gstools.covmodel.CovModel.html#gstools.covmodel.CovModel [stable_link]: https://en.wikipedia.org/wiki/Stable_distribution [doc_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/ [doc_install_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/#pip [tut_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/tutorials.html [tut1_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/01_random_field/index.html [tut2_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/02_cov_model/index.html [tut3_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/03_variogram/index.html [tut4_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/04_vector_field/index.html [tut5_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/05_kriging/index.html [tut6_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/06_conditioned_fields/index.html [tut7_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/07_transformations/index.html [tut8_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/08_geo_coordinates/index.html [tut9_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/09_spatio_temporal/index.html [tut10_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/10_normalizer/index.html [tut11_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/11_plurigaussian/index.html [tut0_link]: https://geostat-framework.readthedocs.io/projects/gstools/en/stable/examples/00_misc/index.html [cor_link]: https://en.wikipedia.org/wiki/Autocovariance#Normalization [vtk_link]: https://www.vtk.org/\n",
                "dependencies": "[build-system] requires = [ \"setuptools>=64\", \"setuptools_scm>=7\", \"numpy>=2.0.0rc1,<2.3; python_version >= '3.9'\", \"oldest-supported-numpy; python_version < '3.9'\", \"Cython>=3.0.10,<3.1.0\", \"extension-helpers>=1\", ] build-backend = \"setuptools.build_meta\" [project] requires-python = \">=3.8\" name = \"gstools\" description = \"GSTools: A geostatistical toolbox.\" authors = [ {name = \"Sebastian Müller, Lennart Schüler\", email = \"info@geostat-framework.org\"}, ] readme = \"README.md\" license = {text = \"LGPL-3.0\"} dynamic = [\"version\"] classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Intended Audience :: Developers\", \"Intended Audience :: End Users/Desktop\", \"Intended Audience :: Science/Research\", \"Intended Audience :: Education\", \"License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)\", \"Natural Language :: English\", \"Operating System :: Unix\", \"Operating System :: Microsoft\", \"Operating System :: MacOS\", \"Programming Language :: Python\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3 :: Only\", \"Programming Language :: Python :: 3.8\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Programming Language :: Python :: 3.13\", \"Topic :: Scientific/Engineering\", \"Topic :: Scientific/Engineering :: GIS\", \"Topic :: Scientific/Engineering :: Hydrology\", \"Topic :: Scientific/Engineering :: Mathematics\", \"Topic :: Scientific/Engineering :: Physics\", \"Topic :: Utilities\", ] dependencies = [ \"emcee>=3.0.0\", \"hankel>=1.0.0\", \"meshio>=5.1.0\", \"numpy>=1.20.0\", \"pyevtk>=1.1.1\", \"scipy>=1.1.0\", ] [project.optional-dependencies] doc = [ \"myst_parser\", \"matplotlib>=3.7\", \"meshzoo>=0.7\", \"numpydoc>=1.1\", \"pykrige>=1.5,<2\", \"pyvista>=0.40\", \"sphinx>=7\", \"sphinx-gallery>=0.8\", \"sphinx-rtd-theme>=3\", \"sphinxcontrib-youtube>=1.1\", ] plotting = [ \"matplotlib>=3.7\", \"pyvista>=0.40\", ] rust = [\"gstools_core>=1.0.0\"] test = [\"pytest-cov>=3\"] lint = [ \"black>=24\", \"pylint\", \"isort[colors]\", \"cython-lint\", ] [project.urls] Changelog = \"https://github.com/GeoStat-Framework/GSTools/blob/main/CHANGELOG.md\" Conda-Forge = \"https://anaconda.org/conda-forge/gstools\" Documentation = \"https://gstools.readthedocs.io\" Homepage = \"https://geostat-framework.org/#gstools\" Source = \"https://github.com/GeoStat-Framework/GSTools\" Tracker = \"https://github.com/GeoStat-Framework/GSTools/issues\" [tool.setuptools] license-files = [\"LICENSE\"] [tool.setuptools_scm] write_to = \"src/gstools/_version.py\" write_to_template = \"__version__ = '{version}'\" local_scheme = \"no-local-version\" fallback_version = \"0.0.0.dev0\" [tool.isort] profile = \"black\" multi_line_output = 3 line_length = 79 [tool.black] line-length = 79 target-version = [ \"py38\", \"py39\", \"py310\", \"py311\", \"py312\", \"py313\", ] [tool.coverage] [tool.coverage.run] source = [\"gstools\"] omit = [ \"*docs*\", \"*examples*\", \"*tests*\", \"*/src/gstools/covmodel/plot.py\", \"*/src/gstools/field/plot.py\", ] [tool.coverage.report] exclude_lines = [ \"pragma: no cover\", \"def __repr__\", \"def __str__\", ] [tool.pylint] [tool.pylint.main] extension-pkg-whitelist = [ \"numpy\", \"scipy\", \"gstools_core\", ] ignore = \"_version.py\" load-plugins = [ \"pylint.extensions.no_self_use\", ] [tool.pylint.message_control] disable = [ \"R0801\", ] [tool.pylint.reports] output-format = \"colorized\" [tool.pylint.design] max-args = 20 max-locals = 50 max-branches = 30 max-statements = 85 max-attributes = 25 max-public-methods = 80 max-positional-arguments=20 [tool.cibuildwheel] # Switch to using build build-frontend = \"build\" # Disable building PyPy wheels on all platforms, 32bit for py3.10/11/12/13, musllinux builds, py3.6/7 skip = [\"cp36-*\", \"cp37-*\", \"pp*\", \"*-win32\", \"*-manylinux_i686\", \"*-musllinux_*\"] # Run the package tests using `pytest` test-extras = \"test\" test-command = \"pytest -v {package}/tests\"\n\"\"\"GSTools: A geostatistical toolbox.\"\"\" import os import numpy as np from Cython.Build import cythonize from extension_helpers import add_openmp_flags_if_available from setuptools import Extension, setup # cython extensions CY_MODULES = [ Extension( name=f\"gstools.{ext}\", sources=[os.path.join(\"src\", \"gstools\", *ext.split(\".\")) + \".pyx\"], include_dirs=[np.get_include()], define_macros=[(\"NPY_NO_DEPRECATED_API\", \"NPY_1_7_API_VERSION\")], ) for ext in [\"field.summator\", \"variogram.estimator\", \"krige.krigesum\"] ] # you can set GSTOOLS_BUILD_PARALLEL=0 or GSTOOLS_BUILD_PARALLEL=1 open_mp = False if int(os.getenv(\"GSTOOLS_BUILD_PARALLEL\", \"0\")): added = [add_openmp_flags_if_available(mod) for mod in CY_MODULES] if any(added): open_mp = True print(f\"## GSTools setup: OpenMP used: {open_mp}\") else: print(\"## GSTools setup: OpenMP not wanted by the user.\") # setup - do not include package data to ignore .pyx files in wheels setup( ext_modules=cythonize(CY_MODULES, compile_time_env={\"OPENMP\": open_mp}), include_package_data=False, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/hcocena",
            "repo_link": "https://github.com/MarieOestreich/hCoCena",
            "content": {
                "codemeta": "",
                "readme": "```diff - I have taken on a new role and the repository will therefore be continued by Thomas Ulas' lab: ``` https://github.com/Ulas-lab/hcocena # hCoCena - Horizontal integration and analysis of transcriptomics datasets [[paper](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btac589/6677225)] hCoCena is an R-package that allows you to integrate and jointly analyse multiple transcriptomic datasets or simply analyse a single dataset if you don't want to do any data integration! hCoCena uses network representations of the data as the basis for integration. You can find more details of how that works in our [paper](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btac589/6677225) . Below, you will find some info on how to install the package and tips for using it. ## Installation To install hcocena (v1.1.2) from this repo, run the codeline provided in the `install_hcocena.R` script. To install versioned dependencies, use the script `install_versioned_dependecies.R`. ## Usage **hCoCena is divided into 2 parts:** **1.** the main analysis that comprises the mandatory steps to process and integrate the data and **2.** the satellite functions that offer you a plethora of analysis options in a pick & mix kind of fashion. The figure below illustrates this: the main analysis is at the center, while the satellite functions can be found in the orbits around it. A step-by-step walkthrough of the main analysis steps can be found in the `hcocena_main.Rmd`, the satellite functions are in the `hcocena_saltellite.Rmd`. hCoCena was written with user-friendliness and customizability in mind. We are doing our best to provide you with plenty of supplementary information that make the usage of the tool easy for you. You can also always extend the tool's functionalities with your on custom scripts and functions to adapt the analysis to your needs! For more details on hCoCena's object structure and where to find the outputs of different analysis steps for customization, please refer to the overview in the [Wiki](https://github.com/MarieOestreich/hCoCena/wiki/Structure-of-the-hcobject) and the extensive function documentations you can access from within R Studio. ![hCoCenaFig1](https://user-images.githubusercontent.com/50077786/158609782-2048c06e-0420-4c3f-8680-5d99f91d6905.jpg) *Marie Oestreich, Lisa Holsten, Shobhit Agrawal, Kilian Dahm, Philipp Koch, Han Jin, Matthias Becker, Thomas Ulas, hCoCena: horizontal integration and analysis of transcriptomics datasets, Bioinformatics, Volume 38, Issue 20, 15 October 2022, Pages 4727-4734, https://doi.org/10.1093/bioinformatics/btac589* ## Showcase To rerun the showcase example from our original publication, please refer to the branch of version [1.0.1](https://github.com/MarieOestreich/hCoCena/tree/v-1.0.1). ## Wiki For loads of additional information regarding the [satellite functions](https://github.com/MarieOestreich/hCoCena/wiki/Satellite-Functions), [community detection](https://github.com/MarieOestreich/hCoCena/wiki/Background-Info-on-the-Community-Detection-Algorithms) algorithms etc. please check out our carefully curated Wiki pages!\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/heat",
            "repo_link": "https://github.com/helmholtz-analytics/heat",
            "content": {
                "codemeta": "",
                "readme": "<div align=\"center\"> <img src=\"https://raw.githubusercontent.com/helmholtz-analytics/heat/main/doc/images/logo.png\"> </div> --- Heat is a distributed tensor framework for high performance data analytics. # Project Status [![CPU/CUDA/ROCm tests](https://codebase.helmholtz.cloud/helmholtz-analytics/ci/badges/heat/base/pipeline.svg)](https://codebase.helmholtz.cloud/helmholtz-analytics/ci/-/commits/heat/base) [![Documentation Status](https://readthedocs.org/projects/heat/badge/?version=latest)](https://heat.readthedocs.io/en/latest/?badge=latest) [![coverage](https://codecov.io/gh/helmholtz-analytics/heat/branch/main/graph/badge.svg)](https://codecov.io/gh/helmholtz-analytics/heat) [![license: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT) [![PyPI Version](https://img.shields.io/pypi/v/heat)](https://pypi.org/project/heat/) [![Downloads](https://pepy.tech/badge/heat)](https://pepy.tech/project/heat) [![Anaconda-Server Badge](https://anaconda.org/conda-forge/heat/badges/version.svg)](https://anaconda.org/conda-forge/heat) [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu) [![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/helmholtz-analytics/heat/badge)](https://securityscorecards.dev/viewer/?uri=github.com/helmholtz-analytics/heat) [![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7688/badge)](https://bestpractices.coreinfrastructure.org/projects/7688) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2531472.svg)](https://doi.org/10.5281/zenodo.2531472) [![Benchmarks](https://img.shields.io/badge/Grafana-Benchmarks-2ea44f)](https://57bc8d92-72f2-4869-accd-435ec06365cb.ka.bw-cloud-instance.org:3000/d/adjpqduq9r7k0a/heat-cb?orgId=1) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![JuRSE Code Pick of the Month](https://img.shields.io/badge/JuRSE_Code_Pick-August_2024-blue)](https://www.fz-juelich.de/en/rse/jurse-community/jurse-code-of-the-month/august-2024) # Table of Contents - [What is Heat for?](#what-is-heat-for) - [Features](#features) - [Getting Started](#getting-started) - [Installation](#installation) - [Requirements](#requirements) - [pip](#pip) - [conda](#conda) - [Support Channels](#support-channels) - [Contribution guidelines](#contribution-guidelines) - [Resources](#resources) - [License](#license) - [Citing Heat](#citing-heat) - [FAQ](#faq) - [Acknowledgements](#acknowledgements) # What is Heat for? Heat builds on [PyTorch](https://pytorch.org/) and [mpi4py](https://mpi4py.readthedocs.io) to provide high-performance computing infrastructure for memory-intensive applications within the NumPy/SciPy ecosystem. With Heat you can: - port existing NumPy/SciPy code from single-CPU to multi-node clusters with minimal coding effort; - exploit the entire, cumulative RAM of your many nodes for memory-intensive operations and algorithms; - run your NumPy/SciPy code on GPUs (CUDA, ROCm, coming up: Apple MPS). For a example that highlights the benefits of multi-node parallelism, hardware acceleration, and how easy this can be done with the help of Heat, see, e.g., our [blog post on trucated SVD of a 200GB data set](https://helmholtz-analytics.github.io/heat/2023/06/16/new-feature-hsvd.html). Check out our [coverage tables](coverage_tables.md) to see which NumPy, SciPy, scikit-learn functions are already supported. If you need a functionality that is not yet supported: - [search existing issues](https://github.com/helmholtz-analytics/heat/issues) and make sure to leave a comment if someone else already requested it; - [open a new issue](https://github.com/helmholtz-analytics/heat/issues/new/choose). Check out our [features](#features) and the [Heat API Reference](https://heat.readthedocs.io/en/latest/autoapi/index.html) for a complete list of functionalities. # Features * High-performance n-dimensional arrays * CPU, GPU, and distributed computation using MPI * Powerful data analytics and machine learning methods * Seamless integration with the NumPy/SciPy ecosystem * Python array API (work in progress) # Getting Started Go to [Quick Start](quick_start.md) for a quick overview. For more details, see [Installation](#installation). **You can test your setup** by running the [`heat_test.py`](https://github.com/helmholtz-analytics/heat/blob/main/scripts/heat_test.py) script: ```shell mpirun -n 2 python heat_test.py ``` It should print something like this: ```shell x is distributed: True Global DNDarray x: DNDarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=ht.int32, device=cpu:0, split=0) Global DNDarray x: Local torch tensor on rank 0 : tensor([0, 1, 2, 3, 4], dtype=torch.int32) Local torch tensor on rank 1 : tensor([5, 6, 7, 8, 9], dtype=torch.int32) ``` Check out our Jupyter Notebook [**Tutorials**](https://github.com/helmholtz-analytics/heat/blob/main/tutorials/), choose `local` to try things out on your machine, or `hpc` if you have access to an HPC system. The complete documentation of the latest version is always deployed on [Read the Docs](https://heat.readthedocs.io/). <!-- # Goals Heat is a flexible and seamless open-source software for high performance data analytics and machine learning. It provides highly optimized algorithms and data structures for tensor computations using CPUs, GPUs, and distributed cluster systems on top of MPI. The goal of Heat is to fill the gap between single-node data analytics and machine learning libraries, and high-performance computing (HPC). Heat's interface integrates seamlessly with the existing data science ecosystem and makes writing scalable scientific and data science applications as effortless as using NumPy. Heat allows you to tackle your actual Big Data challenges that go beyond the computational and memory needs of your laptop and desktop. --> # Installation ## Requirements ### Basics - python >= 3.9 - MPI (OpenMPI, MPICH, Intel MPI, etc.) - mpi4py >= 3.0.0 - pytorch >= 2.0.0 ### Parallel I/O - h5py - netCDF4 ### GPU support In order to do computations on your GPU(s): - your CUDA or ROCm installation must match your hardware and its drivers; - your [PyTorch installation](https://pytorch.org/get-started/locally/) must be compiled with CUDA/ROCm support. ### HPC systems On most HPC-systems you will not be able to install/compile MPI or CUDA/ROCm yourself. Instead, you will most likely need to load a pre-installed MPI and/or CUDA/ROCm module from the module system. Maybe, you will even find PyTorch, h5py, or mpi4py as (part of) such a module. Note that for optimal performance on GPU, you need to usa an MPI library that has been compiled with CUDA/ROCm support (e.g., so-called \"CUDA-aware MPI\"). ## pip Install the latest version with ```bash pip install heat[hdf5,netcdf] ``` where the part in brackets is a list of optional dependencies. You can omit it, if you do not need HDF5 or NetCDF support. ## **conda** The conda build includes all dependencies **including OpenMPI**. ```bash conda install -c conda-forge heat ``` # Support Channels Go ahead and ask questions on [GitHub Discussions](https://github.com/helmholtz-analytics/heat/discussions). If you found a bug or are missing a feature, then please file a new [issue](https://github.com/helmholtz-analytics/heat/issues/new/choose). You can also get in touch with us on [Mattermost](https://mattermost.hzdr.de/signup_user_complete/?id=3sixwk9okpbzpjyfrhen5jpqfo) (sign up with your GitHub credentials). Once you log in, you can introduce yourself on the `Town Square` channel. # Contribution guidelines **We welcome contributions from the community, if you want to contribute to Heat, be sure to review the [Contribution Guidelines](contributing.md) and [Resources](#resources) before getting started!** We use [GitHub issues](https://github.com/helmholtz-analytics/heat/issues) for tracking requests and bugs, please see [Discussions](https://github.com/helmholtz-analytics/heat/discussions) for general questions and discussion. You can also get in touch with us on [Mattermost](https://mattermost.hzdr.de/signup_user_complete/?id=3sixwk9okpbzpjyfrhen5jpqfo) (sign up with your GitHub credentials). Once you log in, you can introduce yourself on the `Town Square` channel. If you're unsure where to start or how your skills fit in, reach out! You can ask us here on GitHub, by leaving a comment on a relevant issue that is already open. **If you are new to contributing to open source, [this guide](https://opensource.guide/how-to-contribute/) helps explain why, what, and how to get involved.** ## Resources * [Heat Tutorials](https://github.com/helmholtz-analytics/heat/tree/main/tutorials) * [Heat API Reference](https://heat.readthedocs.io/en/latest/autoapi/index.html) ### Parallel Computing and MPI: * David Henty's [course](https://www.archer2.ac.uk/training/courses/200514-mpi/) * Wes Kendall's [Tutorials](https://mpitutorial.com/tutorials/) * Rolf Rabenseifner's [MPI course material](https://www.hlrs.de/training/self-study-materials/mpi-course-material) (including C, Fortran **and** Python via `mpi4py`) ### mpi4py * [mpi4py docs](https://mpi4py.readthedocs.io/en/stable/tutorial.html) * [Tutorial](https://www.kth.se/blogs/pdc/2019/08/parallel-programming-in-python-mpi4py-part-1/) # License Heat is distributed under the MIT license, see our [LICENSE](LICENSE) file. # Citing Heat <!-- If you find Heat helpful for your research, please mention it in your publications. You can cite: --> Please do mention Heat in your publications if it helped your research. You can cite: * Götz, M., Debus, C., Coquelin, D., Krajsek, K., Comito, C., Knechtges, P., Hagemeier, B., Tarnawa, M., Hanselmann, S., Siggel, S., Basermann, A. & Streit, A. (2020). HeAT - a Distributed and GPU-accelerated Tensor Framework for Data Analytics. In 2020 IEEE International Conference on Big Data (Big Data) (pp. 276-287). IEEE, DOI: 10.1109/BigData50022.2020.9378050. ``` @inproceedings{heat2020, title={{HeAT -- a Distributed and GPU-accelerated Tensor Framework for Data Analytics}}, author={ Markus Götz and Charlotte Debus and Daniel Coquelin and Kai Krajsek and Claudia Comito and Philipp Knechtges and Björn Hagemeier and Michael Tarnawa and Simon Hanselmann and Martin Siggel and Achim Basermann and Achim Streit }, booktitle={2020 IEEE International Conference on Big Data (Big Data)}, year={2020}, pages={276-287}, month={December}, publisher={IEEE}, doi={10.1109/BigData50022.2020.9378050} } ``` # FAQ Work in progress... <!-- - Users - Developers - Students - system administrators --> ## Acknowledgements *This work is supported by the [Helmholtz Association Initiative and Networking Fund](https://www.helmholtz.de/en/about_us/the_association/initiating_and_networking/) under project number ZT-I-0003 and the Helmholtz AI platform grant.* *This project has received funding from Google Summer of Code (GSoC) in 2022.* *This work is partially carried out under a [programme](https://activities.esa.int/index.php/4000144045) of, and funded by, the European Space Agency. Any view expressed in this repository or related publications can in no way be taken to reflect the official opinion of the European Space Agency.* --- <div align=\"center\"> <a href=\"https://www.dlr.de/EN/Home/home_node.html\"><img src=\"https://raw.githubusercontent.com/helmholtz-analytics/heat/main/doc/images/dlr_logo.svg\" height=\"50px\" hspace=\"3%\" vspace=\"20px\"></a><a href=\"https://www.fz-juelich.de/portal/EN/Home/home_node.html\"><img src=\"https://raw.githubusercontent.com/helmholtz-analytics/heat/main/doc/images/fzj_logo.svg\" height=\"40px\" hspace=\"3%\" vspace=\"20px\"></a><a href=\"http://www.kit.edu/english/index.php\"><img src=\"https://raw.githubusercontent.com/helmholtz-analytics/heat/main/doc/images/kit_logo.svg\" height=\"40px\" hspace=\"3%\" vspace=\"5px\"></a><a href=\"https://www.helmholtz.de/en/\"><img src=\"https://raw.githubusercontent.com/helmholtz-analytics/heat/main/doc/images/helmholtz_logo.svg\" height=\"50px\" hspace=\"3%\" vspace=\"5px\"></a><a href=\"https://www.esa.int/\"><img src=\"https://github.com/user-attachments/assets/2ee251b4-733e-44ea-8d1c-8b75928eef55\" height=\"45px\" hspace=\"3%\" vspace=\"20px\"></a>\n",
                "dependencies": "[build-system] requires = [\"setuptools\"] build-backend = \"setuptools.build_meta\" [tool.black] line-length = 100\nfrom setuptools import setup, find_packages import codecs with codecs.open(\"README.md\", \"r\", \"utf-8\") as handle: long_description = handle.read() __version__ = None # appeases flake, assignment in exec() below with open(\"./heat/core/version.py\") as handle: exec(handle.read()) setup( name=\"heat\", packages=find_packages(exclude=(\"*tests*\", \"*benchmarks*\")), package_data={\"heat.datasets\": [\"*.csv\", \"*.h5\", \"*.nc\"], \"heat\": [\"py.typed\"]}, version=__version__, description=\"A framework for high-performance data analytics and machine learning.\", long_description=long_description, long_description_content_type=\"text/markdown\", author=\"Helmholtz Association\", author_email=\"martin.siggel@dlr.de\", url=\"https://github.com/helmholtz-analytics/heat\", keywords=[\"data\", \"analytics\", \"tensors\", \"distributed\", \"gpu\"], python_requires=\">=3.9\", classifiers=[ \"Development Status :: 4 - Beta\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"License :: OSI Approved :: MIT License\", \"Intended Audience :: Science/Research\", \"Topic :: Scientific/Engineering\", ], install_requires=[ \"mpi4py>=3.0.0\", \"numpy>=1.22.0, <2\", \"torch>=2.0.0, <2.6.1\", \"scipy>=1.10.0\", \"pillow>=6.0.0\", \"torchvision>=0.15.2, <0.21.1\", ], extras_require={ \"docutils\": [\"docutils>=0.16\"], \"hdf5\": [\"h5py>=2.8.0\"], \"netcdf\": [\"netCDF4>=1.5.6\"], \"dev\": [\"pre-commit>=1.18.3\"], \"examples\": [\"scikit-learn>=0.24.0\", \"matplotlib>=3.1.0\"], \"cb\": [\"perun>=0.8\"], \"pandas\": [\"pandas>=1.4\"], \"zarr\": [\"zarr\"], }, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/heatnetsim",
            "repo_link": "https://jugit.fz-juelich.de/iek-10/public/simulation/heatnetsim",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/heliport",
            "repo_link": "https://codebase.helmholtz.cloud/heliport/heliport",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/hifis-rsd",
            "repo_link": "https://codebase.helmholtz.cloud/research-software-directory/RSD-as-a-service",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/higgs-dataset-training",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Higgs-Dataset-Training",
            "content": {
                "codemeta": "",
                "readme": "![Python](https://img.shields.io/badge/Python-3670A0?style=plastic&logo=python&logoColor=ffdd54) ![Pandas](https://img.shields.io/badge/Pandas-%23150458.svg?style=plastic&logo=pandas&logoColor=white) ![NumPy](https://img.shields.io/badge/Numpy-777BB4.svg?style=plastic&logo=numpy&logoColor=white) ![Matplotlib](https://img.shields.io/badge/Matplotlib-%233F4F75.svg?style=plastic&logo=plotly&logoColor=white) ![Keras](https://img.shields.io/badge/Keras-%23D00000.svg?style=plastic&logo=Keras&logoColor=white) ![scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=plastic&logo=scikit-learn&logoColor=white) ![Dask](https://img.shields.io/badge/Dask-%23870000.svg?style=plastic&logo=dask&logoColor=white) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/Higgs-Dataset-Training?style=plastic) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13133945.svg)](https://doi.org/10.5281/zenodo.13133945) # Model Training and Evaluation for Higgs Dataset ## Overview This repository demonstrates training and evaluating a Keras model using the Higgs dataset available from the UCI ML Repository. > [Higgs Dataset](http://archive.ics.uci.edu/ml/datasets/HIGGS) The dataset has been studied in this publication: > [Searching for Exotic Particles in High-energy Physics with Deep Learning.<br>Baldi, P., P. Sadowski, and D. Whiteson. Nature Communications 5, 4308 (2014)](https://www.nature.com/articles/ncomms5308) The ML pipeline includes downloading the dataset, data preparation, model training, evaluation, feature importance analysis, and visualization of results. Dask is utilised for handling this large datasets for parallel processing. ### Installation 1) Create and source virtual environment: ```shell python -m venv env source env/bin/activate # On Windows use `env\\Scripts\\activate` ``` 2) Install the dependencies: ```shell pip install -r requirements.txt ``` ### Data The Higgs dataset can be downloaded directly from the provided scripts in separate steps - `download_data.py` ~ 2.6 GB - `data_extraction.py` ~ 7 GB - `data_preparation.py` ~ test dataset: 240 MB, trained dataset: 5 GB Alternatively, you can run directly the main script from the `data/src/main.py`: ```shell python data/src/main.py ``` #### Downloading Data Download a dataset file from the specified URL with a progress bar. ##### Script ```shell python data/download_data.py ``` ##### Example Usage ```python zipDataUrl = 'https://archive.ics.uci.edu/static/public/280/higgs.zip' # Higgs dataset URL zipPath = '../higgs/higgs.zip' downloadDataset(zipDataUrl, zipPath) cleanUp(zipPath) # Clean up downloaded zip file (~ 2.6 GB) ``` #### Data Extraction Extract the contents of a zip dataset and decompress the .gz dataset file to a specified output path. ##### Script ```shell python data/data_extraction.py ``` ##### Example Usage ```python zipDataUrl = 'https://archive.ics.uci.edu/static/public/280/higgs.zip' # Higgs dataset URL extractTo = '../higgs' zipPath = os.path.join(extractTo, 'higgs.zip') gzCsvPath = os.path.join(extractTo, 'higgs.csv.gz') finalCsvPath = os.path.join(extractTo, 'higgs.csv') extractZippedData(zipPath, extractTo) decompressGzFile(gzCsvPath, finalCsvPath) cleanUp(gzCsvPath) # Clean up gzipped file (~ 2.6 GB) ``` #### Data Preparation Set column names and separates the test set from the training data based on the dataset description (500,000 test sets). Dataset Description: The first column is the class label (1 for signal, 0 for background), followed by the 28 features (21 low-level features then 7 high-level features). The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features. ##### Script ```shell python data/data_preparation.py ``` ##### Example Usage ```python prepareFrom = '../higgs' csvPath = os.path.join(prepareFrom, 'higgs.csv') preparedCsvPath = os.path.join(prepareFrom, 'prepared-higgs.csv') prepareData(csvPath, preparedCsvPath) cleanUp(csvPath) # Clean up gzipped file (~ 7.5 GB) ``` ### Loading Data #### Using Pandas Use the `dataLoader/data_loader.py` script to load the prepared dataset into a Pandas DataFrame. ##### Script ```shell python data/src/data_loader.py ``` ##### Example Usage ```python filepath = '../data/higgs/prepared-higgs_train.csv' # prepared-higgs_test.csv dataLoader = DataLoader(filepath) dataFrame = dataLoader.loadData() dataLoader.previewData(dataFrame) ``` #### Using Dask Use the `dataLoader/data_loader_dask.py` script to load the prepared dataset into a Dask DataFrame, which is beneficial for this large dataset. ##### Script ```shell python data/src/data_loader_dask.py ``` ##### Example Usage: ```python filepath = '../data/higgs/prepared-higgs_train.csv' # prepared-higgs_test.csv dataLoader = DataLoaderDask(filepath) dataFrame = dataLoader.loadData() dataLoader.previewData(dataFrame) ``` ### Exploratory Data Analysis (EDA) Provides various functions for performing EDA, including visualising correlations, checking missing values, and plotting feature distributions. The data analysis plots are saved under `eda/plots`. ##### Script ```shell python exploration/eda.py ``` ##### Example Usage: ```python filepath = '../data/higgs/prepared-higgs_train.csv' # prepared-higgs_test.csv # using Dask data frame dataLoaderDask = DataLoaderDask(filepath) dataFrame = dataLoaderDask.loadData() eda = EDA(dataFrame) eda.describeData() eda.checkMissingValues() eda.visualiseFeatureCorrelation() eda.visualizeTargetDistribution() eda.visualizeFeatureDistribution('feature_1') eda.visualizeAllFeatureDistributions() eda.visualizeFeatureScatter('feature_1', 'feature_2') eda.visualizeTargetDistribution() eda.visualizeFeatureBoxplot('feature_2') ``` ### Usage #### Training the Model The model is defined using Keras with the following default architecture for binary classification: - Input layer with 128 neurons (dense) - Hidden layer with 64 neurons (dense) - Output layer with 1 neuron (activation function: sigmoid) You can customise the model architecture by providing a different `modelBuilder` callable in the ModelTrainer class. The trained models and training loss plots are saved under `kerasModel/trainer/trainedModels`. ##### Script ```shell python kerasModel/trainer/model_trainer.py ``` ##### Example Usage: ```python filePath = '../../data/higgs/prepared-higgs_train.csv' def customModel(inputShape: int) -> Model: \"\"\"Example of a custom model builder function for classification\"\"\" model = keras.Sequential([ layers.Input(shape=(inputShape,)), layers.Dense(512, activation='relu'), layers.Dropout(0.3), layers.Dense(256, activation='relu'), layers.Dense(128, activation='relu'), layers.Dense(64, activation='relu'), layers.Dense(1, activation='sigmoid') # Sigmoid for binary classification ]) return model dataLoaderDask = DataLoaderDask(filePath) dataFrame = dataLoaderDask.loadData() ## Optional: Define model training/compiling/defining parameters as a dictionary and pass it to the class constructor params = { \"epochs\": 10, \"batchSize\": 32, \"minSampleSize\": 100000, \"learningRate\": 0.001, \"modelBuilder\": customModel, # callable \"loss\": 'binary_crossentropy', \"metrics\": ['accuracy'] } trainer = ModelTrainer(dataFrame, params) trainer.trainKerasModel() # optional: Train the Keras model with sampling, Set: trainKerasModel(sample = true, frac = 0.1). trainer.plotTrainingHistory() ``` #### Evaluating the Model The evaluation script computes metrics like: - Accuracy - Precision - Recall (Sensitivity - F1 Score - Classification Report The evaluation includes visualizations such as - Confusion Matrix - ROC Curve The evaluation results are logged and saved to a file under `kerasModel/evaluator/evaluationPlots`. ##### Script ```shell python kerasModel/evaluator/model_evaluator.py ``` ##### Example Usage: ```python modelPath = '../trainer/trainedModels/keras_model_trained_dataset.keras' filePath = '../../data/higgs/prepared-higgs_train.csv' dataLoaderDask = DataLoaderDask(filePath) dataFrame = dataLoaderDask.loadData() evaluator = ModelEvaluator(modelPath, dataFrame) evaluator.evaluate() ``` #### Feature Importance Analysis The feature importance is computed using permutation importance and visualised using a bar chart. It is implemented once using the Pandas approach (with SciKit) and another using Dask for parallel processing. The chart and the result CSV file are saved under `kerasModel/featureImportance/featureImportancePlots`. ##### Script ```shell python kerasModel/featureImportance/feature_importance.py ``` ##### Example Usage: ```python modelPath = '../trainer/trainedModels/keras_model_test_dataset.keras' filePath = '../../data/higgs/prepared-higgs_test.csv' dataLoaderDask = DataLoaderDask(filePath) dataFrame = dataLoaderDask.loadData() evaluator = FeatureImportanceEvaluator(modelPath, dataFrame) evaluator.evaluate() # Alternatively evaluator = FeatureImportanceEvaluator(modelPath, dataFrame, sampleFraction = 0.1, nRepeats=32) # with sampling evaluator.evaluate(withDask = False) # with pandas ```\n",
                "dependencies": "numpy~=1.26.4 matplotlib~=3.9.1 pandas~=2.2.2 requests~=2.32.3 tqdm~=4.66.4 seaborn~=0.13.2 scikit-learn~=1.5.1 dask[complete]~=2024.7.1 tensorflow~=2.17.0 cupy-cuda11x~=13.2.0 joblib~=1.4.2\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/hilbertcurve",
            "repo_link": "https://github.com/jokergoo/HilbertCurve",
            "content": {
                "codemeta": "",
                "readme": "# HilbertCurve <img width=\"300\" alt=\"image\" src=\"https://github.com/jokergoo/HilbertCurve/assets/449218/e40159f5-bbca-4d61-960b-1ba1d744f9e2\" align=\"right\"> [![R-CMD-check](https://github.com/jokergoo/HilbertCurve/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/HilbertCurve/actions) [![codecov](https://img.shields.io/codecov/c/github/jokergoo/HilbertCurve.svg)](https://codecov.io/github/jokergoo/HilbertCurve) [![bioc](https://bioconductor.org/shields/downloads/devel/HilbertCurve.svg)](https://bioconductor.org/packages/stats/bioc/HilbertCurve/) [![bioc](http://www.bioconductor.org/shields/years-in-bioc/HilbertCurve.svg)](http://bioconductor.org/packages/devel/bioc/html/HilbertCurve.html) [Hilbert curve](https://en.wikipedia.org/wiki/Hilbert_curve) is a type of space-filling curves that fold one dimensional axis into a two dimensional space, but with still keeping the locality. It has advantages to visualize data with long axis in following two aspects: 1. greatly improve resolution for the visualization; 2. easy to visualize clusters because generally data points in the cluster will also be close in the Hilbert curve. This package aims to provide an easy and flexible way to visualize data through Hilbert curve. The implementation and example figures are based on following sources: - http://mkweb.bcgsc.ca/hilbert/ - http://corte.si/posts/code/hilbert/portrait/index.html - http://bioconductor.org/packages/devel/bioc/html/HilbertVis.html ### Citation Zuguang Gu, et al., [HilbertCurve: an R/Bioconductor package for high-resolution visualization of genomic data.](https://doi.org/10.1093/bioinformatics/btw161) Bioinformatics 2016 ### Install The package is at [Bioconductor](http://bioconductor.org/packages/devel/bioc/html/HilbertCurve.html) now and you can install the newest version by: ```r library(devtools) install_github(\"jokergoo/ComplexHeatmap\") # in order to get the newest version of ComplexHeatmap install_github(\"jokergoo/HilbertCurve\") ``` ### Usage Basically, there are two steps to make a Hilbert curve. 1. Initialize the curve and also map the one-dimensional axis to the curve. 2. add low-level graphics by `hc_points()`, `hc_segments()`, ... by giving the positions of the graphics. ```r hc = HilbertCurve(1, 100, level = 4) hc_points(hc, ...) hc_segments(hc, ...) hc_rect(hc, ...) hc_text(hc, ...) ``` There is another 'pixel' mode which provides a high resolution for visualizing genomic data by the Hilbert curve. ```r hc = HilbertCurve(1, 100000000000, level = 10) hc_layer(hc, ...) # this can be repeated several times to add multiple layers on the curve hc_png(hc, ...) ``` ### Examples Rainbow color spectrum: ![](https://cloud.githubusercontent.com/assets/449218/12678993/f184c4de-c6a1-11e5-8c8c-ed3ed938c487.png) Chinese dynasty: ![](https://cloud.githubusercontent.com/assets/449218/12678995/f18981cc-c6a1-11e5-8b66-6222bed67c63.png) GC percent and genes on chromosome 1: ![](https://cloud.githubusercontent.com/assets/449218/12678996/f18a6646-c6a1-11e5-9e0b-c99cc7a93f0e.png) Association between H3K36me3 histone modification and gene bodies: ![](https://cloud.githubusercontent.com/assets/449218/12678992/f1848320-c6a1-11e5-8225-e6fef169f29b.png) Methylation on chromosome 1: ![](https://cloud.githubusercontent.com/assets/449218/12678994/f186827e-c6a1-11e5-884a-b9135f24146e.png) Copy number alterations in 22 chromosomes: ![](https://cloud.githubusercontent.com/assets/449218/12678997/f18e405e-c6a1-11e5-9478-3d8fdc4bc834.png) ### License MIT @ Zuguang Gu\n",
                "dependencies": "Package: HilbertCurve Type: Package Title: Making 2D Hilbert Curve Version: 1.99.2 Date: 2024-10-08 Authors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"), comment = c('ORCID'=\"0000-0002-7395-8709\")) Depends: R (>= 4.0.0), grid Imports: methods, utils, png, grDevices, circlize (>= 0.3.3), IRanges, GenomicRanges, polylabelr, Rcpp Suggests: knitr, testthat (>= 1.0.0), ComplexHeatmap (>= 1.99.0), markdown, RColorBrewer, RCurl, GetoptLong, rmarkdown VignetteBuilder: knitr Description: Hilbert curve is a type of space-filling curves that fold one dimensional axis into a two dimensional space, but with still preserves the locality. This package aims to provide an easy and flexible way to visualize data through Hilbert curve. biocViews: Software, Visualization, Sequencing, Coverage, GenomeAnnotation URL: https://github.com/jokergoo/HilbertCurve, https://jokergoo.github.io/HilbertCurve/ License: MIT + file LICENSE LinkingTo: Rcpp Collate: 00_S4_generic_methods.R HilbertCurve.R hc_polygon.R hc_legend.R utils.R GenomicHilbertCurve.R RcppExports.R zzz.R\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/hipsta",
            "repo_link": "https://github.com/Deep-MI/hipsta",
            "content": {
                "codemeta": "",
                "readme": "# Hippocampal Shape and Thickness Analysis ## Purpose: This repository contains the Hipsta package, a collection of scripts for hippocampal shape and thickness analysis as described in our recent [publication](https://doi.org/10.1016/j.neuroimage.2023.120182). ## Documentation: Please see the documentation pages for a general overview, usage information and examples, and output description. Brief usage information is also available [here](hipsta/doc/DOCUMENTATION.md). Some suggestions for running the script can be found in the [tutorial](TUTORIAL.md). ## Current status: The hippocampal shape and thickness analysis package is currently in its beta stage, which means that it's open for testing, but may still contain unresolved issues. Future changes with regard to the algorithms, interfaces, and package structure are to be expected. ## Feedback: Questions, suggestions, and feedback are welcome, and should preferably be submitted as an [issue](https://github.com/Deep-MI/Hipsta/issues). ## Installation: It is recommended to run this pipeline within its own virtual environment. A virtual environment can, for example, be created using Python's `virtualenv` command: `virtualenv /path/to/a/new/directory/of/your/choice` Activate the virtual environment as follows: `source /path/to/a/new/directory/of/your/choice/bin/activate` The package is available on `pypi.org`, and can be installed as follows (including all required dependencies): `pip install hipsta` Alternatively, the following code can be used to download this package from its GitHub repository (this will create a 'Hipsta' directory within the current working directory): `git clone https://github.com/Deep-MI/Hipsta.git` Use the following code to install the downloaded files as a Python package (after changing into the 'Hipsta' directory). It will also install all required dependencies: `pip install .` The above steps are not necessary when running the [Docker](docker/Docker.md) or [Singularity](singularity/Singularity.md) versions of the package. ## Requirements: Unless using the [Docker](docker/Docker.md) or [Singularity](singularity/Singularity.md) versions of the package, the following conditions need to be met for running an analysis: 1. A FreeSurfer version (6.x or 7.x) must be sourced, i.e. FREESURFER_HOME must exist as an environment variable and point to a valid FreeSurfer installation. 2. A hippocampal subfield segmentation created by FreeSurfer 7.11 or later or the ASHS software. A custom segmentation is also permissible (some restrictions and settings apply; see [Supported Segmentations](https://github.com/Deep-MI/Hipsta#supported-segmentations)). 3. Python 3.8 or higher including the lapy, numpy, scipy, nibabel, pyvista, and pyacvd libraries, among others. See `requirements.txt` for a full list, and use `pip install -r requirements.txt` to install. 4. The gmsh package (version 2.x; http://gmsh.info) must be installed. Can be downloaded e.g. as binaries for [linux](https://gmsh.info/bin/Linux/gmsh-2.16.0-Linux64.tgz) or [MacOSX](https://gmsh.info/bin/MacOSX/gmsh-2.16.0-MacOSX.dmg) . The 'gmsh' binary must be on the $PATH: `export PATH=${PATH}:/path/to/gmsh-directory/bin` ## References: Please cite the following publications if you use these scripts in your work: - Diers, K., Baumeister, H., Jessen, F., Düzel, E., Berron, D., & Reuter, M. (2023). An automated, geometry-based method for hippocampal shape and thickness analysis. Neuroimage, 276:120182. doi: [10.1016/j.neuroimage.2023.120182](https://doi.org/10.1016/j.neuroimage.2023.120182). Please also consider citing the these publications: - Geuzaine, C., & Remacle, J.-F. (2009). Gmsh: a three-dimensional finite element mesh generator with built-in pre- and post-processing facilities. International Journal for Numerical Methods in Engineering, 79, 1309-1331. - Andreux, M., Rodola, E., Aubry, M., & Cremers, D. (2014). Anisotropic Laplace-Beltrami operators for shape analysis. In European Conference on Computer Vision (pp. 299-312). Springer, Cham. - Iglesias, J. E., Augustinack, J. C., Nguyen, K., Player, C. M., Player, A., Wright, M., ... & Fischl, B. (2015). A computational atlas of the hippocampal formation using ex vivo, ultra-high resolution MRI: application to adaptive segmentation of in vivo MRI. Neuroimage, 115, 117-137. - Yushkevich, P. A., Pluta, J., Wang, H., Ding, S.L., Xie, L., Gertje, E., Mancuso, L., Kliot, D., Das, S. R., & Wolk, D.A. (2015). Automated Volumetry and Regional Thickness Analysis of Hippocampal Subfields and Medial Temporal Cortical Structures in Mild Cognitive Impairment. Human Brain Mapping, 36, 258-287.\n",
                "dependencies": "[build-system] requires = ['setuptools >= 61.0.0'] build-backend = 'setuptools.build_meta' [project] name = 'hipsta' description = 'A python package for hippocampal shape and thickness analysis' license = {file = 'LICENSE'} requires-python = '>=3.9' authors = [ {name = 'Kersten Diers', email = 'kersten.diers@dzne.de'}, {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'} ] maintainers = [ {name = 'Kersten Diers', email = 'kersten.diers@dzne.de'}, {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'} ] keywords = [ 'hippocampus', 'shape', 'thickness', 'geometry', ] classifiers = [ 'Operating System :: Unix', 'Operating System :: MacOS', 'Programming Language :: Python :: 3 :: Only', 'Programming Language :: Python :: 3.9', 'Programming Language :: Python :: 3.10', 'Programming Language :: Python :: 3.11', 'Programming Language :: Python :: 3.12', 'Natural Language :: English', 'License :: OSI Approved :: MIT License', 'Intended Audience :: Science/Research', ] dynamic = [\"version\", \"readme\", \"dependencies\"] [project.optional-dependencies] build = [ 'build', 'twine', ] doc = [ 'furo!=2023.8.17', 'matplotlib', 'memory-profiler', 'numpydoc', 'sphinx!=7.2.*', 'sphinxcontrib-bibtex', 'sphinx-copybutton', 'sphinx-design', 'sphinx-gallery', 'sphinx-issues', 'myst-parser', 'pypandoc', 'nbsphinx', 'IPython', # For syntax highlighting in notebooks 'ipykernel', ] style = [ 'bibclean', 'codespell', 'pydocstyle[toml]', 'ruff', ] test = [ 'pytest', 'pytest-cov', 'pytest-timeout', ] all = [ 'hipsta[doc]', 'hipsta[build]', 'hipsta[style]', ] full = [ 'hipsta[all]', ] [project.urls] homepage = 'https://github.com/Deep-MI/Hipsta' documentation = 'https://github.com/Deep-MI/Hipsta' source = 'https://github.com/Deep-MI/Hipsta' tracker = 'https://github.com/Deep-MI/Hipsta/issues' [project.scripts] run_hipsta = 'hipsta.cli:main' [tool.setuptools.dynamic] version = {file = 'VERSION'} readme = {file = 'README.md', content-type = \"text/markdown\"} dependencies = {file = 'requirements.txt'} [tool.setuptools.packages.find] include = ['hipsta*'] exclude = ['docker', 'singularity'] [tool.setuptools.package-data] \"hipsta.doc\" = [\"*.md\"] [tool.pydocstyle] convention = 'numpy' ignore-decorators = '(copy_doc|property|.*setter|.*getter|pyqtSlot|Slot)' match = '^(?!setup|__init__|test_).*\\.py' match-dir = '^fsqc.*' add_ignore = 'D100,D104,D107' [tool.ruff] line-length = 120 extend-exclude = [ \".github\", \"doc\", \"docker\", \"images\", \"singularity\", ] [tool.ruff.lint] # https://docs.astral.sh/ruff/linter/#rule-selection select = [ \"E\", # pycodestyle \"F\", # Pyflakes \"UP\", # pyupgrade \"B\", # flake8-bugbear \"I\", # isort # \"SIM\", # flake8-simplify ] [tool.ruff.per-file-ignores] \"__init__.py\" = [\"F401\"] \"computeThickness.py\" = [\"E501\"] # long lines for easier readability [tool.pytest.ini_options] minversion = '6.0' filterwarnings = [] addopts = [ \"--import-mode=importlib\", # \"--junit-xml=junit-results.xml\", # \"--durations=20\", # \"--verbose\", ]\nnibabel>=5.2 numpy>=1.22.3, <2.0 pandas>=1.3.0 plotly>=5.6.0 pyacvd>=0.2.9 pyvista>=0.37.0 scipy>=1.8.0, !=1.13.0 nilearn>=0.8.1 lapy>=1.0.0 kaleido>=0.2.1 scikit-image importlib_resources pykdtree!=1.3.13\n# https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html # If compatibility with legacy builds or versions of tools that don’t support # certain packaging standards (e.g. PEP 517 or PEP 660), a simple setup.py # script can be added to your project [1] (while keeping the configuration in # pyproject.toml): from setuptools import setup setup()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ideal-equilibrium-oxygen-membrane-reactor",
            "repo_link": "https://github.com/KabitGit/Ideal-Equilibrium-Oxygen-Membrane-Reactor",
            "content": {
                "codemeta": "",
                "readme": "Prequisites -- - Numpy - SciPy - Cantera (tested with version 2.6.0) - Matplotlib (used for plotting in the examples) Description -- The Python script OMR_model.py allows to simulate oxygen membrane reactors with continuous gas flow rates as described in [1]. Model input values are the initial condition of the feed and sweep gas entering the chambers separated by the membrane, while the output values refer to the chemical equilibrium state in both chambers, which is affected by the oxygen flux through the membrane. Chemical equilibrium and perfect mixing is assumed in the entire sweep and feed chamber on both sides of the membrane. The oxygen flux through the membrane is modelled using the Wagner equation and included into the chemical equilibrium calculation. The entire problem is then solved as a nested problem: The inner problem is the equilibrium calculation including an assumed oxygen flux using Cantera [2]. The outer problem is a root-finding problem to find the oxygen flux satisfying the Wagner equation in the equilibrium state which is solved using SciPy [3]. A detailed explanation of the assumptions, limitations and equations including experimental validation can be found in [1]. The implementation published here uses thermodynamic data from the Gri 3.0 mechanism [4] and subsequently considers 53 species. Notable species herein include: [O2, H2O, H2, CH4, CO, CO2, N2, AR] Usage -------------------------------------------------------------------------------------------------------------------------------------- To import the script into your Python code, download OMR_model.py and import the function Simulate_OMR at the beginning of the code as: `from OMR_model import Simulate_OMR` After defining the input parameters for the model [T,N_f0,x_f0,P_f,N_s0,x_s0,P_s,A_mem,sigma,L,Lc], a simulation can then be performed by calling: `N_f, x_f, p_o2_f, N_s, x_s, p_o2_s, N_o2, dH, x_comp, conv = Simulate_OMR(T,N_f0,x_f0,P_f,N_s0,x_s0,P_s,A_mem,sigma,L,Lc)` Examples for using the function with scalar input parameters, as well as array shaped input parameters are given in the Examples folder. Inputs and outputs of the model -- Parameters (Inputs) ---------- T : Float (scalar or array with length n) Temperature in °C. N_f0 : Float (scalar or array with length n) Initial feed gas molar flow rate in mol/min. x_f0 : String or list of strings with length n Initial feed gas mole fractions specified as a string in the format 'A:x_A_f0, B:x_B_f0, C:x_C_f0, ...', where x_A_f0, x_B_f0, x_C_f0 are the mole fractions of the respective species. P_f : Float (scalar or array with length n) Feed gas pressure in Pa. N_s0 : Float (scalar or array with length n) Initial sweep gas molar flow rate in mol/min. x_s0 : String or list of strings with length n Initial sweep gas mole fractions specified as a string in the format 'A:x_A_s0, B:x_B_s0, C:x_C_s0, ...', where x_A_s0, x_B_s0, x_C_s0 are the mole fractions of the respective species. P_s : Float (scalar or array with length n) Sweep gas pressure in Pa. A_mem : Float (scalar or array with length n) Active membrane area in cm^2. sigma : Float (scalar or array with length n) Ambipolar conductivity in S/m. L : Float (scalar or array with length n) Membrane thickness in mum. Lc : Float (scalar or array with length n) Characteristic length in mum. Returns (Outputs) ------- N_f : Float (array with length n) Feed gas molar flow rate in mol/min. x_f : List of array of floats with size n*53 Mole fraction array, consisting of the mole fractions of the feed gas, related to the composition array. p_o2_f : Float (array with length n) Feed gas oxygen partial pressure in Pa. N_s : Float (array with length n) Sweep gas molar flow rate in mol/min. x_s : List of array of floats with size n*53 Mole fraction array, consisting of the mole fractions of the sweep gas, related to the composition array. p_o2_s : Float (array with length n) Sweep gas oxygen partial pressure in Pa. p_o2_f : Float (array with length n) Feed gas oxygen partial pressure in Pa. N_o2 : Float (array with length n) Molar oxygen flux through the membrane in mol/min. dH : List of array of floats with size n*53 Outlet-Inlet enthalpy flow difference (reaction heat) in W; If positive: The reaction is endothermic; If negative: The reaction is exothermic. x_comp : List of strings with size 53 Composition array consisting of the considered species in the calculation. conv : Integer (array with length n) Check whether convergence was achieved for the respective array index; Equal to 1 if converged. References -- [1] Bittner, K., Margaritis, N., Schulze-Küppers, F., Wolters, J., & Natour, G. (2023). A mathematical model for initial design iterations and feasibility studies of oxygen membrane reactors by minimizing Gibbs free energy. Journal of Membrane Science, 685, 121955. DOI: https://doi.org/10.1016/j.memsci.2023.121955 [2] David G. Goodwin, Harry K. Moffat, Ingmar Schoegl, Raymond L. Speth, and Bryan W. Weber. Cantera: An object-oriented software toolkit for chemical kinetics, thermodynamics, and transport processes. https://www.cantera.org, 2023. Version 3.0.0. DOI: 10.5281/zenodo.8137090 [3] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, CJ Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E.A. Quintero, Charles R Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. (2020) SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17(3), 261-272. DOI: https://doi.org/10.1038/s41592-019-0686-2 [4] G.P. Smith, D.M. Golden, M. Frenklach, N.W. Moriarty, B. Eiteneer, M. Goldenberg, C.T. Bowman, R.K. Hanson, S. Song, W.C.J. Gardiner, V.V. Lissianski, Z. Qin, Gri-Mech 3.0\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/igmas",
            "repo_link": "https://git.gfz-potsdam.de/igmas/igmas-releases",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/interactivecomplexheatmap",
            "repo_link": "https://github.com/jokergoo/InteractiveComplexHeatmap",
            "content": {
                "codemeta": "",
                "readme": "# Make Interactive Complex Heatmaps [![R-CMD-check](https://github.com/jokergoo/InteractiveComplexHeatmap/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/InteractiveComplexHeatmap/actions) [![bioc](http://www.bioconductor.org/shields/downloads/devel/InteractiveComplexHeatmap.svg)](https://bioconductor.org/packages/stats/bioc/InteractiveComplexHeatmap/) [![bioc](http://www.bioconductor.org/shields/years-in-bioc/InteractiveComplexHeatmap.svg)](http://bioconductor.org/packages/devel/bioc/html/InteractiveComplexHeatmap.html) <img width=\"1150\" alt=\"Screenshot 2021-07-19 at 21 31 14\" src=\"https://user-images.githubusercontent.com/449218/126217251-8eee8ce8-7e7f-4251-b844-800dd72481bd.png\"> **InteractiveComplexHeatmap** is an R package that converts static heatmaps produced from [**ComplexHeatmap**](https://github.com/jokergoo/ComplexHeatmap) package into an interactive Shiny app only with one extra line of code. The first example is the default layout of the interactive complex heatmap widget. <img src=\"https://user-images.githubusercontent.com/449218/110212910-e6147e00-7e9d-11eb-94ed-0ac549247888.gif\" width='100%' border=\"black\" /> The second example demonstrates a DESeq2 result with integrating the package **shinydashboard**. <img src=\"https://user-images.githubusercontent.com/449218/111832925-b16ae280-88f1-11eb-8530-290374f9f2c2.gif\" width=\"100%\" border=\"black\" /> ### Citation Zuguang Gu, et al., Make Interactive Complex Heatmaps in R, 2021, Bioinformatics, https://doi.org/10.1093/bioinformatics/btab806 ## Install **InteractiveComplexHeatmap** is available on [Bioconductor](http://bioconductor.org/packages/devel/bioc/html/InteractiveComplexHeatmap.html), you can install it by: ```r if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"InteractiveComplexHeatmap\") ``` If you want the latest version, install it directly from GitHub: ```r library(devtools) install_github(\"jokergoo/InteractiveComplexHeatmap\") ``` ## Documentation There are the following vignettes along with the package: 1. [How to visualize heatmaps interactively](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/InteractiveComplexHeatmap.html) 2. [How interactive complex heatmap is implemented](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/implementation.html) 3. [Functions for Shiny app development](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/shiny_dev.html) 4. [Decorations on heatmaps](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/decoration.html) 5. [Interactivate heatmaps indirectly generated by pheatmap(), heatmap.2() and heatmap()](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/interactivate_indirect.html) 6. [A Shiny app for visualizing DESeq2 results](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/deseq2_app.html) 7. [Implement interactive heatmap from scratch](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/from_scratch.html) 8. [Share interactive heatmaps to collaborators](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/share.html) A printer-friendly version of the documentation is available at [bioRxiv](https://doi.org/10.1101/2021.03.08.434289). ## Usage ### Directly turn heatmaps interactive With any `Heatmap`/`HeatmapList` object, directly send to `htShiny()` to create a Shiny app for your heatmap(s): ```r htShiny(ht_list) ``` If the heatmaps are already drawn, `ht_list` can be omitted and the last heatmap object is retrieved automatically: ```r Heatmap(...) + other_heatmaps_or_annotations # or other functions that internally use Heatmap() htShiny() ``` ### Shiny app development There are also two functions for Shiny app development: - `InteractiveComplexHeatmapOutput()`: for the UI on the client side. - `makeInteractiveComplexHeatmap()`: for processing on the sever side. ```r library(InteractiveComplexHeatmap) library(ComplexHeatmap) ht = Heatmap(m) ht = draw(ht) ui = fluidPage( InteractiveComplexHeatmapOutput() ) server = function(input, output, session) { makeInteractiveComplexHeatmap(input, output, session, ht) } shiny::shinyApp(ui, server) ``` You can also put multiple interactive heatmaps widgets in the same Shiny app: ```r ht1 = Heatmap(m, col = c(\"white\", \"blue\")) ht1 = draw(ht1) ht2 = Heatmap(m, col = c(\"white\", \"red\")) ht2 = draw(ht2) ui = fluidPage( h3(\"The first heatmap\"), InteractiveComplexHeatmapOutput(\"ht1\"), hr(), h3(\"The second heatmap\"), InteractiveComplexHeatmapOutput(\"ht2\") ) server = function(input, output, session) { makeInteractiveComplexHeatmap(input, output, session, ht1, \"ht1\") makeInteractiveComplexHeatmap(input, output, session, ht2, \"ht2\") } shiny::shinyApp(ui, server) ``` Two additional functions to let you dynamically load interactive heatmap widgets: - `InteractiveComplexHeatmapModal()`: The interactive heatmap widget is inserted as a \"modal\". - `InteractiveComplexHeatmapWidget()`: The interactive heatmap widget is inserted into a place defined by users. ```r m = matrix(rnorm(100), 10) ht = Heatmap(m) ui = fluidPage( actionButton(\"show_heatmap\", \"Generate_heatmap\"), ) server = function(input, output, session) { observeEvent(input$show_heatmap, { InteractiveComplexHeatmapModal(input, output, session, ht) }) } shiny::shinyApp(ui, server) # or use InteractiveComplexHeatmapWidget() ui = fluidPage( actionButton(\"show_heatmap\", \"Generate_heatmap\"), htmlOutput(\"heatmap_output\") ) server = function(input, output, session) { observeEvent(input$show_heatmap, { InteractiveComplexHeatmapWidget(input, output, session, ht, output_id = \"heatmap_output\") }) } shiny::shinyApp(ui, server) ``` ## Interactivate pheatmap(), heatmap.2() and heatmap() If you directly use these three funtions, simply replace them with `ComplexHeatmap::pheatmap()`, `ComplexHeatmap:::heatmap.2()` and `ComplexHeatmap:::heatmap()`. If the three functions are used indirectly, e.g. a function `foo()` (maybe from another packages or other people's functions) which internally uses these three heatmap functions, check the vignette [\"Interactivate indirect use of pheatmap(), heatmap.2() and heatmap()\"](https://jokergoo.github.io/InteractiveComplexHeatmap/articles/interactivate_indirect.html) to find out how. ## Live examples Following lists several live examples of interactive heatmaps. Details can be found in the package vignette. - https://jokergoo.shinyapps.io/interactive_complexheatmap/ - https://jokergoo.shinyapps.io/interactive_complexheatmap_vertical/ - https://jokergoo.shinyapps.io/interactive_densityheatmap/ - https://jokergoo.shinyapps.io/interactive_oncoprint/ - https://jokergoo.shinyapps.io/interactive_enrichedheatmap/ - https://jokergooo.shinyapps.io/interactive_upset/ - https://jokergooo.shinyapps.io/interactive_pheatmap/ - https://jokergooo.shinyapps.io/interactive_heatmap/ - https://jokergooo.shinyapps.io/interactive_heatmap_2/ - https://jokergooo.shinyapps.io/interactive_tidyheatmap/ There are also many other examples provided in the package. ```r htShinyExample() ``` ``` There are following examples. Individual example can be run by e.g. htShinyExample(1.1). ──────── 1. Simple examples ───────────────────────────────────────────────────────── 1.1 A single heatmap with minimal arguments. 1.2 A single heatmap from a character matrix. 1.3 A single heatmap with annotations on both rows and columns. 1.4 A single heatmap where rows and columns are split. 1.5 A list of two heatmaps. 1.6 A list of two vertically concatenated heatmaps 1.7 Use last generated heatmap, an example from cola package. 1.8 Use last generated heatmap, an app with three interactive heatmaps 1.9 Demonstrate hover, click and dblclick actions to select cells. 1.10 Only response to one of click/hover/dblclick/hover events. Please use htShinyExample('1.10') to get this example (quote the index, or else htShinyExample(1.10) will be treated as the same as htShinyExample(1.1)). 1.11 Interactive heatmap under compact mode. ──────── 2. On other plots and packages ───────────────────────────────────────────── 2.1 A density heatmap. 2.2 An oncoPrint. 2.3 A UpSet plot. 2.4 An interactive heatmap from pheatmap(). 2.5 An interactive heatmap from heatmap(). 2.6 An interactive heatmap from heatmap.2(). 2.7 A heatmap produced from tidyHeatmap package. 2.8 Genome-scale heatmap. 2.9 A package-dependency heatmap. You can try to control \"Fill figure region\" and \"Remove empty rows and columns\" in the tools under the sub-heatmap. ──────── 3. Enriched heatmaps ─────────────────────────────────────────────────────── 3.1 A single enriched heatmap. 3.2 A list of enriched heatmaps. 3.3 An enriched heatmap with discrete signals. ──────── 4. On public datasets ────────────────────────────────────────────────────── 4.1 An example from Lewis et al 2019. 4.2 Visualize cell heterogeneity from single cell RNASeq. 4.3 Correlations between methylation, expression and other genomic features. ──────── 5. Shiny app development ─────────────────────────────────────────────────── 5.1 A single Shiny app with two interactive heatmap widgets. 5.2 Self-define the output. The selected sub-matrix is shown as a text table. 5.3 Self-define the output. Additional annotations for the selected genes are shown. 5.4 Visualize Gene Ontology similarities. A list of selected GO IDs as well as their descriptions are shown in the output. 5.5 Interactive correlation heatmap. Clicking on the cell generates a scatterplot of the two corresponding variables. 5.6 A heatmap on Jaccard coefficients for a list of genomic regions. Clicking on the cell generates a Hilbert curve of how the two sets of genomic regions overlap. 5.7 Implement interactivity from scratch. Instead of generating the whole interactive heatmap widget, it only returns the information of rows and columns that user have selected on heatmap and users can use this information to build their own interactive heatmap widgets. 5.8 Implement interactivity from scratch. A visualization of 2D density distribution. Brushing on heatmap triggers a new 2D density estimation only on the subset of data. ──────── 6. Dynamically generate heatmap widget in Shiny app ──────────────────────── 6.1 The matrix with different dimensions is dynamically generated. 6.2 Reorder by a column that is specified by user. 6.3 Dynamically generate the widget with InteractiveComplexHeatmapModal(). The modal is triggered by an action button. 6.4 Dynamically select interactive heatmaps. The modal is triggered by radio buttons. 6.5 Dynamically generate the widget. A customized Javascript code is inserted after the UI to change the default behavior of the action button. 6.6 The widget is generated by InteractiveComplexHeatmapWidget() where the UI is directly put in the place defined by htmlOutput(). 6.7 The widget is generated by InteractiveComplexHeatmapWidget() and a customized Javascript code is inserted after the UI. ──────── 7. Interactive R markdown document ───────────────────────────────────────── 7.1 Integrate in an interactive R Markdown document. 7.2 Integrate in an interactive R Markdown document where the heatmap widgets are dynamically generated. ──────── 8. Interactivate heatmaps indirectly generated by heatmap()/heatmap.2()/pheatmap() 8.1 Indirect use of pheatmap(). 8.2 Indirect use of heatmap.2(). 8.3 Two interactive heatmap widgets from indirect use of pheatmap(). ──────── 9. Float output UI along with mouse positions ────────────────────────────── 9.1 A simple example that demonstrates output UI floating with the three actions: hover, click and dblclick. 9.2 Floating self-defined outputs. 9.3 Floating output only from one event on heatmap, i.e. hover/click/dblclick/brush-output. ──────── 10. Work with shinydashboard ──────────────────────────────────────────────── 10.1 Separate the three UI components into three boxes. 10.2 The three UI components are draggable. 10.3 A Shiny dashboard with two tabs. 10.4 Only contain the original heatmap where output floats. 10.5 A complex dashboard that visualizes a DESeq2 results. ``` ## License MIT @ Zuguang Gu\n",
                "dependencies": "Package: InteractiveComplexHeatmap Type: Package Title: Make Interactive Complex Heatmaps Version: 1.11.1 Date: 2024-02-27 Authors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"), comment = c('ORCID'=\"0000-0002-7395-8709\")) Depends: R (>= 4.0.0), ComplexHeatmap (>= 2.11.0) Imports: grDevices, stats, shiny, grid, GetoptLong, S4Vectors (>= 0.26.1), digest, IRanges, kableExtra (>= 1.3.1), utils, svglite, htmltools, clisymbols, jsonlite, RColorBrewer, fontawesome Suggests: knitr, rmarkdown, testthat, EnrichedHeatmap, GenomicRanges, data.table, circlize, GenomicFeatures, tidyverse, tidyHeatmap, cluster, org.Hs.eg.db, simplifyEnrichment, GO.db, SC3, GOexpress, SingleCellExperiment, scater, gplots, pheatmap, airway, DESeq2, DT, cola, BiocManager, gridtext, HilbertCurve (>= 1.21.1), shinydashboard, SummarizedExperiment, pkgndep, ks VignetteBuilder: knitr Description: This package can easily make heatmaps which are produced by the ComplexHeatmap package into interactive applications. It provides two types of interactivities: 1. on the interactive graphics device, and 2. on a Shiny app. It also provides functions for integrating the interactive heatmap widgets for more complex Shiny app development. biocViews: Software, Visualization, Sequencing URL: https://github.com/jokergoo/InteractiveComplexHeatmap BugReports: https://github.com/jokergoo/InteractiveComplexHeatmap/issues License: MIT + file LICENSE\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/interactivevis",
            "repo_link": "https://github.com/martindyrba/DeepLearningInteractiveVis",
            "content": {
                "codemeta": "",
                "readme": "# Deep Learning Interactive Visualization This project contains all code to learn a convolutional neural network model to detect Alzheimer's disease and visualize contributing brain regions with high relevance. **Further details on the procedures including samples, image processing, neural network modeling, evaluation, and validation were published in:** Dyrba et al. (2021) Improving 3D convolutional neural network comprehensibility via interactive visualization of relevance maps: evaluation in Alzheimer's disease. *Alzheimer's research & therapy* 13. DOI: [10.1186/s13195-021-00924-2](https://doi.org/10.1186/s13195-021-00924-2). ![Screenshot of the InteractiveVis app](InteractiveVis.png)*Screenshot of the InteractiveVis app* *** ### Running the interactive visualization The interactive Bokeh web application [InteractiveVis](InteractiveVis) can be used for deriving and inspecting the relevance maps overlaid on the original input images. To run it, there are three options. 1. **We set up a public web service to quickly try it out:** <https://explaination.net/demo> 2. Alternatively, download the docker container from DockerHub: `sudo docker pull martindyrba/interactivevis` Then use the scripts `sudo ./run_docker_intvis.sh` and `sudo ./stop_docker_intvis.sh` to run or stop the Bokeh app. (You find both files above in this repository.) After starting the docker container, the app will be available from your web browser: <http://localhost:5006/InteractiveVis> 3. Download this Git repository. Install the required Python modules (see below). Then point the Anaconda prompt or terminal console to the DeepLearningInteractiveVis main directory and run the Bokeh app using: `bokeh serve InteractiveVis --show` ### Requirements and installation: To be able to run the interactive visualization from the Git sources, you will need Python <3.8, in order to install tensorflow==1.15. Also, we recommend to first create a new Python environment (using [Anaconda](https://www.anaconda.com/download) or [virtualenv/venv](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)) to avoid messing up your local Python modules/versions when you have other coding projects or a system shared by multiple users. ```console # for Anaconda: conda create -n InteractiveVis python=3.7 conda activate InteractiveVis ``` Run pip to install the dependencies: ```console pip install -r requirements.txt ``` Then you can start the Bokeh application: ```console bokeh serve InteractiveVis --show ``` *** ### CNN model training and performance evaluation The code for training the CNN models and evaluation is provided in this repository in the subdirectory [scripts](scripts). The order of script execution was as follows: - [1_CreateResiduals_ADNI2_StoreModels.ipynb](scripts/1_CreateResiduals_ADNI2_StoreModels.ipynb) and other scripts for the validation samples [4_CreateResiduals_DELCODE_applying_ADNI2_regr_model.ipynb](scripts/4_CreateResiduals_DELCODE_applying_ADNI2_regr_model.ipynb) (execution time: each 15-30 minutes). - [2_Train_3D_CNN_ADNI2_xVal_wb_mwp1_CAT12_MNI_shuffle_checkpoint.ipynb](scripts/2_Train_3D_CNN_ADNI2_xVal_wb_mwp1_CAT12_MNI_shuffle_checkpoint.ipynb) for model training based on tenfold cross-validation to evaluate general model accuracy for the residualized data (execution time: 2-10 hrs with CUDA-GPU) and [3_Train_3D_CNN_ADNI2_whole_dataset_wb_mwp1_CAT12_MNI_shuffle.ipynb](scripts/3_Train_3D_CNN_ADNI2_whole_dataset_wb_mwp1_CAT12_MNI_shuffle.ipynb) for training the model based on the whole ADNI-GO/2 dataset. - [5_Validate_3D_CNN_xVal_wb_mwp1_CAT12_MNI_DELCODE.ipynb](scripts/5_Validate_3D_CNN_xVal_wb_mwp1_CAT12_MNI_DELCODE.ipynb) and [6_Validate_3D_CNN_whole_ds_wb_mwp1_CAT12_MNI_DELCODE.ipynb](scripts/6_Validate_3D_CNN_whole_ds_wb_mwp1_CAT12_MNI_DELCODE.ipynb) for the evaluation of the models using the validation data sets (execution time: each 15-30 minutes with CUDA-GPU). - [7_Train_3D_CNN_ADNI2_xVal_wb_rawdat_mwp1_CAT12_MNI_shuffle_checkpoint.ipynb](scripts/7_Train_3D_CNN_ADNI2_xVal_wb_rawdat_mwp1_CAT12_MNI_shuffle_checkpoint.ipynb) and [8_Train_3D_CNN_ADNI2_whole_dataset_wb_rawdat_mwp1_CAT12_MNI_shuffle.ipynb](scripts/8_Train_3D_CNN_ADNI2_whole_dataset_wb_rawdat_mwp1_CAT12_MNI_shuffle.ipynb) for training the models for the raw datasets (execution time: each 2-10 hrs with CUDA-GPU). - [9_Validate_3D_CNN_whole_ds_wb_rawdat_mwp1_CAT12_MNI_DELCODE.ipynb](scripts/9_Validate_3D_CNN_whole_ds_wb_rawdat_mwp1_CAT12_MNI_DELCODE.ipynb) and [9_Validate_3D_CNN_xVal_wb_mwp1_CAT12_MNI_DELCODE.ipynb](scripts/9_Validate_3D_CNN_xVal_wb_mwp1_CAT12_MNI_DELCODE.ipynb) for the evaluation of the models using the validation data sets (execution time: each 15-30 minutes with CUDA-GPU). - [x_extract_hippocampus_relevance_lrpCMP_DELCODE.ipynb](scripts/x_extract_hippocampus_relevance_lrpCMP_DELCODE.ipynb) to extract the hippocampus relevance for all models (execution time: 15-30 minutes with CUDA-GPU). - [x_extract_relevance_maps_as_nifti_DELCODE.ipynb](scripts/x_extract_relevance_maps_as_nifti_DELCODE.ipynb) to extract the relevance maps directly as nifti file for all participants/scans (execution time: 30 minutes with CUDA-GPU). - [hippocampus_volume_relevance_analysis_DELCODE.html](scripts/hippocampus_volume_relevance_analysis_DELCODE.html) for the baseline group separation analysis of hippocampus volume and the correlation analysis of hippocampus volume and relevance (see also other R/Rmd scripts). - [y_occlusion_analysis.ipynb](scripts/y_occlusion_analysis.ipynb) code for the occlusion sensitivity analysis (execution time: 90 minutes with CUDA-GPU). - [z_CreateResiduals_demo_dataset_applying_ADNI2_regr_model.ipynb](scripts/z_CreateResiduals_demo_dataset_applying_ADNI2_regr_model.ipynb) to create the example files being used by the InteractiveVis demo. It contains a sample of 15 people per diagnostic group, representatively selected from the ADNI-2 phase based on the criteria: amyloid status (positive for Alzheimer's dementia and amnestic mild cogntive impairment, negative for controls), MRI field strength of 3 Tesla, RID greater than 4000, and age of 65 or older. *** ### InteractiveVis architecture overview *InteractiveVis UML class diagram (v4)* ![InteractiveVis class diagram (v4)](InteractiveVis_class_diagram_v4.svg) *Select subject UML sequence diagram (v3)* ![Select subject sequence diagram (v3)](select_subject_sequence_diagram_v3.svg) *** ### License: Copyright (c) 2020 Martin Dyrba martin.dyrba@dzne.de, German Center for Neurodegenerative Diseases (DZNE), Rostock, Germany This project and included source code is published under the MIT license. See [LICENSE](LICENSE) for details.\n",
                "dependencies": "protobuf==3.20 innvestigate==1.0.9 keras==2.2.4 tensorflow~=1.15 jinja2==3.0 bokeh<=2.2.3 numpy==1.18.5 openpyxl==3.0.5 h5py==2.10.0 pandas==1.3.5 matplotlib==3.3.3 nibabel==3.2.1 scikit-learn==0.23.2 scikit-image==0.17.2 scipy==1.6.0\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ioproc",
            "repo_link": "https://gitlab.com/dlr-ve/esy/ioproc",
            "content": {
                "codemeta": "",
                "readme": "[![PyPI version](https://badge.fury.io/py/ioproc.svg)](https://badge.fury.io/py/ioproc) [![PyPI license](https://img.shields.io/pypi/l/ioproc.svg)](https://badge.fury.io/py/ioproc) [![pipeline status](https://gitlab.dlr.de/ioproc/ioproc/badges/development/pipeline.svg)](https://gitlab.dlr.de/ioproc/ioproc/-/commits/development) [![coverage report](https://gitlab.dlr.de/ioproc/ioproc/badges/development/coverage.svg)](https://gitlab.dlr.de/ioproc/ioproc/-/commits/development) # The ioProc workflow manager `ioproc` is a light-weight workflow manager for Python ensuring robust, scalable and reproducible data pipelines. The tool is developed at the German Aerospace Center (DLR) for and in the scientific context of energy systems analysis, however, it is widely applicable in other scientific fields. ## how-to install Setup a new Python environment and install ioProc using pip install ioproc ## how-to configure Configure your pipeline in the `user.yaml`. The `workflow` is defined by a list of actions. These must contain the fields `project`, `call` and `data` (with sub fields `read_from_dmgr`, and `write_to_dmgr`). The user may specify additional fields for each action under the optional key `args`. You may get inspiration from the default actions in `general.py`. You may also have a look into the [snippets](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets) section where several basic `ioproc` functionalities are described: - [Set up your first workflow](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327213) - [Define your first action](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327210) - [Make use of checkpoints](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327214) - [Define an action making use of the ioproc datamanger](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327212) - [Add additional yaml files to your workflow](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327209) - [Define global parameters](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327207) - [Starting ioproc workflow via command line with additional input parameters](https://gitlab.com/dlr-ve/esy/ioproc/-/snippets/2327208) ## default actions provided by ioProc ### `readExcel` This function is used to parse Excel files and storing it in the Data manager. ```python @action('general') def parse_excel(dmgr, config, params): ''' Parses given `excelFile` for specified `excelSheets` as dataframe object and stores it in the datamanager by the key specified in `write_to_dmgr`. `excelHeader` can be set to `True` or `False`. The action may be specified in the user.yaml as follows: - action: project: general call: parse_excel data: read_from_dmgr: null write_to_dmgr: parsedData args: excelFile: spreadsheet.xlsx excelSheet: sheet1 excelHeader: True ''' args = params['args'] file = get_field(args, 'excelFile') excel_sheet = get_excel_sheet(args) header = get_header(get_field(args, 'excelHeader')) parsed_excel = pd.read_excel(io=file, sheet_name=excel_sheet, header=header) with dmgr.overwrite: dmgr[params['data']['write_to_dmgr']] = parsed_excel ``` ### `checkpoint` Checkpoints save the current state and content of the data manger to disk in HDF5 format. The workflow can be resumed at any time from previously created checkpoints. ```python @action('general') def checkpoint(dmgr, config, params): ''' creates a checkpoint file in the current working directory with name Cache_TAG while TAG is supplied by the action config. :param tag: the tag for this checkpoint, this can never be \"start\" ''' assert params['tag'] != 'start', 'checkpoints can not be named start' dmgr.toCache(params['tag']) mainlogger.info('set checkpoint \"{}\"'.format(params['tag'])) ``` ### `printData` This action prints all data stored in the data manager to the console. It can therefore be used for conveniently debugging a workflow. ```python @action('general') def printData(dmgr, config, params): ''' simple debugging printing function. Prints all data in the data manager. Does not have any parameters. ''' for k, v in dmgr.items(): mainlogger.info(k+' = \\n'+str(v)) ```\n",
                "dependencies": "[tool.poetry] name = \"ioproc\" version = \"2.2.0\" description = \"Workflow framework for data pre- and postprocessing.\" authors = [\"Felix Nitsch, Benjamin Fuchs, Judith Riehm, Jan Buschmann <felix.nitsch@dlr.de, benjamin.fuchs@dlr.de, judith.riehm@dlr.de, jan.buschmann@dlr.de>\"] license = \"MIT\" maintainers = [\"Felix Nitsch, Benjamin Fuchs, Jan Buschmann <ioproc@dlr.de>\"] homepage = 'https://gitlab.com/dlr-ve/ioproc' readme = 'README.md' keywords = [\"workflow management\", \"data pipeline\", \"data science\"] [tool.poetry.scripts] ioproc = 'ioproc:run' [tool.poetry.dependencies] python = \">=3.8\" Cerberus = \">=1.3.4\" pandas = \">=1.4.2\" PyYAML = \">=6.0\" tables = \">=3.7.0\" frozendict = \">=2.3.2\" arrow = \">=1.2.2\" click = \">=8.1.3\" Jinja2 = \">=3.1.2\" attrs = \">=21.4.0\" cattrs = \">=22.1.0\" toml = \">=0.10.2\" [tool.poetry.dev-dependencies] [build-system] requires = [\"poetry-core>=1.0.0\"] build-backend = \"poetry.core.masonry.api\"\n# This file may be used to create an environment using: # $ conda create --name <env> --file <this file> # platform: win-64 arrow=0.13.1 blas=1.0 ca-certificates=2020.1.1 cerberus=1.3.2 certifi=2019.11.28 click=7.0 frozendict=1.2 future=0.18.2 gdxcc=7.28.20 gdxpds=1.1.0 icc_rt=2019.0.0 intel-openmp=2020.0 mkl=2020.0 mkl-service=2.3.0 mkl_fft=1.0.15 mkl_random=1.1.0 numexpr=2.7.1 numpy=1.18.1 numpy-base=1.18.1 openssl=1.1.1d pandas=1.0.1 pip=20.0.2 python=3.7.6 python-dateutil=2.8.1 pytz=2019.3 pyyaml=5.3 setuptools=45.2.0 six=1.14.0 sqlite=3.31.1 tables=3.6.1 vc=14.1 vs2015_runtime=14.16.27012 wheel=0.34.2 wincertstore=0.2 yaml=0.1.7 poetry=1.1.6\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/iqtools",
            "repo_link": "https://github.com/xaratustrah/iqtools",
            "content": {
                "codemeta": "",
                "readme": "# Welcome to `iqtools` [![documentation](https://img.shields.io/badge/docs-mkdocs%20material-blue.svg?style=flat)](https://xaratustrah.github.io/iqtools)[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7615693.svg)](https://doi.org/10.5281/zenodo.7615693) <div style=\"margin-left:auto;margin-right:auto;text-align:center\"> <img src=\"https://raw.githubusercontent.com/xaratustrah/iqtools/main/docs/img/icon.png\" width=\"128\"> </div> Collection of code for working with offline complex valued time series data ([inphase and quadrature](https://en.wikipedia.org/wiki/In-phase_and_quadrature_components) or IQ Data) with numpy written for Python3. ## Installation and usage ### Quick instructions There are many ways to install `iqtools` either fully or partly. One way to do a complete install is this: #### TL;DR (for Linux and Mac): Quick but full installation if you have `mamba` installed. Tested on Linux and Mac. First clone the repo, then go to the directory and run these commands. Currently Python Version 3.10 works best: ``` mamba create -n my_env mamba activate my_env mamba install -y python=3.10 root pyqt pyfftw pip install -r requirements.txt pip install . ``` Where `my_env` can be any name you like. #### Test your installation You can test your installation by typing: ```bash python3 -c 'import ROOT;import PyQt5;from iqtools import*' ``` If the command returns without any error, then you are good, i.e. you should be able to use the library, or run one of the user interfaces. ### Detailed installation instructions #### Preparation If you do not need to use `iqtools` with ROOT features, you can skip to the next section. If you like to use `iqtools` with ROOT features within PyROOT, please make sure you have a proper installation of ROOT and PyROOT in your python environment. There are several alternatives of how to install ROOT: * System wide installation on Linux (Please refer to the web site of [PyROOT](https://root.cern/manual/python/) ). This approach is not recommended * An easier way is to install ROOT using `conda-forge` as described [here](https://anaconda.org/conda-forge/root/) or [here](https://iscinumpy.gitlab.io/post/root-conda/). * Most recommended is to use `mamba`. For that just install [mamba](https://mamba.readthedocs.io/en/latest/installation.html). Before installing, it is recommended to create a new mamba env and do your work there: ``` mamba create -n my_env mamba activate my_env mamba install root pyqt ``` Same goes with the installation of `pyqt`. If you are not interested in the GUI script, you can just ignore the installation of `pyqt` in the previous step. You will not be able to use the GUI, but you can still use the CLI and of course the library itself. #### Installing packages Clone the repository or download the source from [GitHUB](https://github.com/xaratustrah/iqtools). Then use `pip` for installing and uninstalling `iqtools`. pip install -r requirements.txt pip install . ### Windows Under windows, ROOT / PyROOT needs to be installed in a different manner. Please refer to the installation instructions on the corresponding [web page](https://root.cern/) of the ROOT project. If you do not need the ROOT functions, you can still run the library, CLI and GUI under Windows. Specifically, it is recommended to download the latest version of [WinPython](https://winpython.github.io/) which contains the `PyQt` library. Just unpacking WinPython is enough, no installation is needed. After this, you can just follow the instructions above to install the requirements and packages via `pip`. Some stand alone static versions of the `iqgui` for windows may be made available in the future in the release section for the corresponding tags. ### Quick usage `iqtools` is a library that can be embedded in data analysis projects. You can use its full functionality in your own codes by importing it: ```python from iqtools import * ``` and use it accordingly. `iqtools` offers user interface which do not implement the full functionality of the library, but can be useful for quick access or conversions, so it can be run as a command line program for processing data file as well. For example: iqtools --help The `iqgui` script is a graphical user interface (GUI) written in Qt with limited functionality, but nevertheless interesting features. You can run it by simply typing: ```bash iqgui ``` A simple window will appear, where you can accesss some quick feartures. For more information on the GUI frontend please refer to the [documentation page](https://xaratustrah.github.io/iqtools). <img src=\"https://raw.githubusercontent.com/xaratustrah/iqtools/main/docs/img/iqgui.png\" width=\"512\"> ## Documentation For more information please refer to the [documentation page](https://xaratustrah.github.io/iqtools). ## Citation for publications If you are using this code in your publications, please refer to [DOI:10.5281/zenodo.7615693](https://doi.org/10.5281/zenodo.7615693) for citation, or cite as: <small> Shahab Sanjari. (2023). <i>iqtools: Collection of code for working with offline complex valued time series data in Python.</i> Zenodo. <a href=\"https://doi.org/10.5281/zenodo.7615693\">https://doi.org/10.5281/zenodo.7615693</a> </small> ## Licensing Please see the file [LICENSE.md](./LICENSE.md) for further information about how the content is licensed. ## Acknowledgements Many thanks to @tfoerst3r for providing help with the project structure and licensing issues and to @carlkl for helping with creating a static Windows version of the GUI.\n",
                "dependencies": "[build-system] requires = [\"setuptools\", \"wheel\"] build-backend = \"setuptools.build_meta\"\nnumpy scipy matplotlib beautifulsoup4 nibabel npTDMS pyTDMS uproot3 uproot3_methods mkdocs mkdocs-material mkdocstrings mkdocstrings-python\nfrom setuptools import setup if __name__ == '__main__': setup()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/isaac",
            "repo_link": "https://github.com/ComputationalRadiationPhysics/isaac",
            "content": {
                "codemeta": "",
                "readme": "![ISAAC](/isaac.png?raw=true \"ISAAC\") In Situ Animation of Accelerated Computations ===================================================== ![Wakefield visualization from PIConGPU](/example_renderings/picongpu_wakefield_1.png?raw=true \"Wakefield visualization from PIConGPU\") About ISAAC ----------- Many computations like physics or biologists simulations these days run on accelerated hardware like CUDA GPUs or Intel Xeon Phi, which are itself distributed in a big compute cluster communicating over MPI. The goal of ISAAC is to visualize this data without the need to download it to the host while using the high computation speed of the accelerator. ISAAC insists of two parts: The server and the insitu library. Furthermore there needs to be a client, which is able to show the transfered stream and meta data. A reference HTML5 client is provided, but needs to be adapted to specific simulations and is not part of ISAAC itself (but still in the repository). Simulation code has just to add some calls and settings to the insitu template library. After that the server will notice when a simulation is running and give the user some options to observe the computations _on the fly_. It is also possible to send meta data back to the simulation, e.g. to restart it with improved settings. Installing requirements, building and using in own application -------------------------------------------------------------- Please see in [INSTALL.md](./INSTALL.md) for installing, building and using ISAAC. If you need to install ISAAC on a server not accessible from the outside you need to [tunnel the connections](./TUNNEL.md) of the clients. A more detailed __documentation__ about using ISAAC __can be [found here](http://computationalradiationphysics.github.io/isaac)__. Client ------ Inside the client directory lay three files: * interface.htm * interface_vlc.htm * interface_presentation.htm The very first uses direct JSON injection, but no real streaming protocol like RTP. The second can use RTP, but needs the (free) vlc browser plugin installed to work. Furthermore the server most be able to directly send UDP packages to you, which are blocked by most firewalls. Last but not least this adds some latency for the h264 encoding. However the stream itself will use way less bandwidth. The last client version is for presentation purposes, the table with meta data values is not shown, but some are shown directly in the stream box. E.g. if you visualize __[PIConGPU](https://github.com/ComputationalRadiationPhysics/picongpu)__ with ISAAC enabled this will show you the count of particles and cells used. Known issues ------------ * If streaming over twitch or another rtmp compatible service is used, but the rtmp port (1935) ist blocked or a wrong url passed, the server will crash because of the underlying gStreamer rtmp implementation. Licensing --------- ISAAC is licensed under the LGPLv3.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jemris",
            "repo_link": "https://github.com/JEMRIS/jemris",
            "content": {
                "codemeta": "",
                "readme": "General Information =================== JEMRIS is a general MRI simulation framework. The general process of simulation consists of preparation by choice or implementation of sequence, sample and coil setup and the invocation of the simulation run itself. Documentation ============= It is _highly_ recommended to read the provided documentation online. Please find the build, install, developer and user documentation under http://www.jemris.org. Licensing ========= This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA For an explicit declaration of licensing refer to the COPYING file in the root directory of this package. Contact ======= This package is maintained by Tony Stoecker <tony.stoecker@dzne.de>. Please find problem specific contact addresses on http://www.jemris.org. Installation ============ Please visit http://www.jemris.org for detailed installation instructions. How to report bugs ================== If you have identified a bug in JEMRIS you are welcome to send a detailed bug report to <tony.stoecker@dzne.de>. Please include` * Information about your system - Which operating system and version (uname -a) - Which C compiler and version (gcc --version) And anything else you think is relevant. * Information about your version of JEMRIS - Version and release number * How to reproduce the bug - If it is a systematical bug in JEMRIS please provide the sequence, the sample, the coils and the outputs from sequence or simulation GUI to help us to reproduce the bug. Patches are most welcome. If possible please provide a pull request on github.\n",
                "dependencies": "cmake_minimum_required (VERSION 2.8) project (jemris) set (jemris_VERSION_MAJOR 2) set (jemris_VERSION_MINOR 9) set (jemris_VERSION_PATCH 2) set (jemris_VERSION \"${jemris_VERSION_MAJOR}.${jemris_VERSION_MINOR}.${jemris_VERSION_PATCH}\") set(CMAKE_BUILD_TYPE \"Release\") set (CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} ${CMAKE_SOURCE_DIR}/cmake) macro(set_config_option VARNAME STRING) set(${VARNAME} TRUE) list(APPEND CONFIG_OPTIONS ${STRING}) message(STATUS \"Found \" ${STRING}) endmacro(set_config_option) include(GetGitRevisionDescription) get_git_head_revision(GIT_REFSPEC GIT_SHA1) include(ConfigureChecks.cmake) add_definitions(-DHAVE_CONFIG_H) # OS type ----------------------------------------------------------------------- if (${CMAKE_SYSTEM_NAME} MATCHES \"Windows\") set(WINDOWS TRUE) elseif (${CMAKE_SYSTEM_NAME} MATCHES \"Linux\") set(LINUX TRUE) elseif (${CMAKE_SYSTEM_NAME} MATCHES \"Darwin\") set(MACOSX TRUE) endif() # Architecture ------------------------------------------------------------------ # include (VcMacros) DB: not needed, right? include (OptimizeForArchitecture) #find_package (OpenMP) #if(OPENMP_FOUND) #message(\"-- Found OpenMP\") #set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${OpenMP_C_FLAGS}\") #set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}\") #set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${OpenMP_EXE_LINKER_FLAGS}\") #endif() # C++ flags --------------------------------------------------------------------- # THis should be highly discouraged as it directly sets the compiler flags - this should be handled by cmake to allow # proper RELEASE, DEBUG, ... builds set(CMAKE_CXX_COMPILER \"mpicxx\") if (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\") set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-psabi -DTIXML_USE_STL -fPIC -O3\") elseif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\") set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DTIXML_USE_STL -O3 -Itr1 -std=c++14 -stdlib=libc++\") elseif (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\") set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /DTIXML_USE_STL /EHsc /Ox /nologo /MT /wd4267 /wd4244 /wd4190 /wd4996 /LD /MD /DEXP_STL\") set (CMAKE_LINKER_FLAGS \"${CMAKE_LINKER_FLAGS} /NODEFAULTLIB:LIBCMT\") set (CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} /NODEFAULTLIB:LIBCMT\") set (CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} /NODEFAULTLIB:LIBCMT\") endif() include(CheckTypeSize) include(CheckFunctionExists) include(CheckIncludeFile) enable_testing() include (VcMacros) include (OptimizeForArchitecture) check_type_size(\"void*\" SIZEOF_VOID_P) if(SIZEOF_VOID_P EQUAL 8) set_config_option(HAVE_64BIT_SIZE_T \"Have64BitSizeT\") endif(SIZEOF_VOID_P EQUAL 8) find_package(Boost) if(Boost_FOUND) include_directories(${Boost_INCLUDE_DIRS}) set (HAVE_BOOST ON CACHE BOOL \"Found boost\") endif() find_package(Xerces 1.3.0 REQUIRED) include_directories (${Xerces_INCLUDE_DIRS}) find_package(GiNaC REQUIRED) include_directories (${GINAC_INCLUDE_DIRS}) # Hmm does not support components... find_package(Sundials REQUIRED) include_directories (${SUNDIALS_INCLUDE_DIR}) # Handle Nvector stuff... here we need to deal with the actual version if (SUNDIALS_FOUND) add_definitions(-DHAVE_CVODE_CVODE_H) add_definitions(-DHAVE_NVECTOR_NVECTOR_SERIAL_H) add_definitions(-DHAVE_CVODE_CVODE_DIAG_H) endif(SUNDIALS_FOUND) find_package(MPI) if(MPI_C_FOUND) include_directories (${MPI_INCLUDE_PATH}) set (HAVE_MPI_THREADS ON CACHE BOOL \"Found MPI with threads\") elseif(MPI_C_FOUND) set(MPIRUN \"MPIRUN-UNAVAILABLE\") endif() find_package(HDF5 COMPONENTS C CXX REQUIRED) if (NOT DEFINED HDF5_CXX_LIBRARIES) MESSAGE(STATUS \"HDF5 CXX Libraries not set but library present... using alternative! Probably everything is fine!\") LIST(APPEND HDF5_CXX_LIBRARIES ${HDF5_hdf5_cpp_LIBRARY}) LIST(APPEND HDF5_CXX_LIBRARIES ${HDF5_hdf5_LIBRARY}) LIST(APPEND HDF5_CXX_LIBRARIES ${HDF5_z_LIBRARY}) LIST(APPEND HDF5_CXX_LIBRARIES ${HDF5_sz_LIBRARY}) LIST(APPEND HDF5_CXX_LIBRARIES ${HDF5_m_LIBRARY}) LIST(APPEND HDF5_CXX_LIBRARIES ${HDF5_hdf5_hl_LIBRARY}) endif(NOT DEFINED HDF5_CXX_LIBRARIES) include_directories (${HDF5_INCLUDE_DIR}) # ISMRMRD find_package(ISMRMRD REQUIRED) link_directories(${ISMRMRD_LIBRARY_DIRS}) include_directories(${ISMRMRD_INCLUDE_DIRS}) # Docker image message( NOTICE \"pulling docker image for reconstruction server...\") execute_process(COMMAND \"docker\" \"pull\" \"mavel101/bart-reco-server\" RESULT_VARIABLE ret OUTPUT_FILE CMD_OUTPUT) if(ret EQUAL \"1\") message( WARNING \"Docker image for reconstruction server cannot be pulled. Docker may not be installed.\") else() message( NOTICE \"Succesfully pulled Docker image.\") endif() # Client conda environment if ( NOT DEFINED SKIP_CONDA) message( NOTICE \"creating conda environment for reconstruction client.\") configure_file(${CMAKE_SOURCE_DIR}/ismrmrd_client.yml ${CMAKE_CURRENT_BINARY_DIR}/ismrmrd_client.yml) execute_process(COMMAND \"conda\" \"list\" \"-n\" \"ismrmrd_client\" RESULT_VARIABLE ret OUTPUT_QUIET ERROR_QUIET) if(ret EQUAL \"1\") execute_process(COMMAND \"conda\" \"env\" \"create\" \"-f\" \"ismrmrd_client.yml\" RESULT_VARIABLE ret) if(ret EQUAL \"1\") message( WARNING \"Conda environment for reconstruction client cannot be installed. Python may not be installed.\") else() message( NOTICE \"Succesfully installed conda environment.\") endif() else() message( NOTICE \"Conda environment already installed. Environment will be updated if necessary.\") execute_process(COMMAND \"conda\" \"run\" \"-n\" \"ismrmrd_client\" \"conda\" \"env\" \"update\" \"--file\" \"ismrmrd_client.yml\" RESULT_VARIABLE ret) endif() else() message( NOTICE \"Skipping creation of conda environment.\") endif() # Is this here really necessary? if (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\") set(CMAKE_LINK_FLAGS \"${CMAKE_LINK_FLAGS} hdf5.lib hdf5_cpp.lib\") set(ZLIB_LIBRARY \"${HDF5_DIR}/../../lib/zlib.lib\") set(ZLIB_INCLUDE_DIR \"${HDF5_DIR}/../../include\") set(ZLIB_FOUND \"1\") endif() SET(prefix \"${CMAKE_INSTALL_PREFIX}\") configure_file ( \"cmake/config.h.in\" \"${PROJECT_SOURCE_DIR}/src/config.h\" @ONLY ) add_subdirectory (src) add_subdirectory (share) # Packaging --------------------------------------------------------------------- # All include (InstallRequiredSystemLibraries) set (CPACK_RESOURCE_FILE_LICENSE \"${CMAKE_CURRENT_SOURCE_DIR}/COPYING\") set (CPACK_GENERATOR \"TBZ2\") set (CPACK_PACKAGE_VERSION \"${jemris_VERSION_MAJOR}.${jemris_VERSION_MINOR}.${jemris_VERSION_PATCH}\") set (CPACK_PACKAGE_NAME \"jemris\") set (CPACK_PACKAGE_CONTACT \"Tony Stoecker <tony.stoecker@dzne.de>\") set (CPACK_PACKAGE_DESCRIPTION_SUMMARY \"JEMRIS is a general purpose MRI simulator. Visit http://www.jemris.org for more details.\") # DEB set (CPACK_DEBIAN_PACKAGE_ARCHITECTURE \"amd64\") set (CPACK_DEBIAN_PACKAGE_DEPENDS \"ginac-tools, openmpi-bin, libsundials-cvode4, libxerces-c3.2, libhdf5-cpp-103\") include (CPack)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jplag",
            "repo_link": "https://github.com/jplag/JPlag/",
            "content": {
                "codemeta": "",
                "readme": "<p align=\"center\"> <img alt=\"JPlag logo\" src=\"core/src/main/resources/de/jplag/logo-dark.png\" width=\"350\"> </p> # JPlag - Detecting Source Code Plagiarism [![CI Build](https://github.com/jplag/jplag/actions/workflows/maven.yml/badge.svg)](https://github.com/jplag/jplag/actions/workflows/maven.yml) [![Latest Release](https://img.shields.io/github/release/jplag/jplag.svg)](https://github.com/jplag/jplag/releases/latest) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/de.jplag/jplag/badge.svg)](https://maven-badges.herokuapp.com/maven-central/de.jplag/jplag) [![License](https://img.shields.io/github/license/jplag/jplag.svg)](https://github.com/jplag/jplag/blob/main/LICENSE) [![GitHub commit activity](https://img.shields.io/github/commit-activity/y/jplag/JPlag)](https://github.com/jplag/JPlag/pulse) [![SonarCloud Coverage](https://sonarcloud.io/api/project_badges/measure?project=jplag_JPlag&metric=coverage)](https://sonarcloud.io/component_measures?metric=Coverage&view=list&id=jplag_JPlag) [![Java Version](https://img.shields.io/badge/java-SE%2021-yellowgreen)](#download-and-installation) JPlag finds pairwise similarities among a set of multiple programs. It can reliably detect software plagiarism and collusion in software development, even when obfuscated. All similarities are calculated locally; no source code or plagiarism results are ever uploaded online. JPlag supports a large number of programming and modeling languages. * 📈 [JPlag Demo](https://jplag.github.io/Demo/) * 🏛️ [JPlag on Helmholtz RSD](https://helmholtz.software/software/jplag) * 🤩 [Give us Feedback in a **short (<5 min) survey**](https://docs.google.com/forms/d/e/1FAIpQLSckqUlXhIlJ-H2jtu2VmGf_mJt4hcnHXaDlwhpUL3XG1I8UYw/viewform?usp=sf_link) ## Supported Languages All supported languages and their supported versions are listed below. | Language | Version | CLI Argument Name | [state](https://github.com/jplag/JPlag/wiki/2.-Supported-Languages) | parser | |--------------------------------------------------------|---------------------------------------------------------------------------------------:|-------------------|:-------------------------------------------------------------------:|:---------:| | [Java](https://www.java.com) | 21 | java | mature | JavaC | | [C](https://isocpp.org) | 11 | c | legacy | JavaCC | | [C++](https://isocpp.org) | 14 | cpp | mature | ANTLR 4 | | [C#](https://docs.microsoft.com/en-us/dotnet/csharp/) | 6 | csharp | mature | ANTLR 4 | | [Python](https://www.python.org) | 3.6 | python3 | mature | ANTLR 4 | | [JavaScript](https://www.javascript.com/) | ES6 | javascript | beta | ANTLR 4 | | [TypeScript](https://www.typescriptlang.org/) | [~5](https://github.com/antlr/grammars-v4/tree/master/javascript/typescript/README.md) | typescript | beta | ANTLR 4 | | [Go](https://go.dev) | 1.17 | golang | beta | ANTLR 4 | | [Kotlin](https://kotlinlang.org) | 1.3 | kotlin | mature | ANTLR 4 | | [R](https://www.r-project.org/) | 3.5.0 | rlang | mature | ANTLR 4 | | [Rust](https://www.rust-lang.org/) | 1.60.0 | rust | mature | ANTLR 4 | | [Swift](https://www.swift.org) | 5.4 | swift | beta | ANTLR 4 | | [Scala](https://www.scala-lang.org) | 2.13.8 | scala | mature | Scalameta | | [LLVM IR](https://llvm.org) | 15 | llvmir | beta | ANTLR 4 | | [Scheme](http://www.scheme-reports.org) | ? | scheme | legacy | JavaCC | | [EMF Metamodel](https://www.eclipse.org/modeling/emf/) | 2.25.0 | emf | beta | EMF | | [EMF Model](https://www.eclipse.org/modeling/emf/) | 2.25.0 | emf-model | alpha | EMF | | [SCXML](https://www.w3.org/TR/scxml/) | 1.0 | scxml | alpha | XML | | Text (naive, use with caution) | - | text | legacy | CoreNLP | ## Download and Installation You need Java SE 21 to run or build JPlag. ### Downloading a release * Download a [released version](https://github.com/jplag/jplag/releases). * In case you depend on the legacy version of JPlag, we refer to the [legacy release v2.12.1](https://github.com/jplag/jplag/releases/tag/v2.12.1-SNAPSHOT) and the [legacy branch](https://github.com/jplag/jplag/tree/legacy). ### Via Maven JPlag is released on [Maven Central](https://search.maven.org/search?q=de.jplag), it can be included as follows: ```xml <dependency> <groupId>de.jplag</groupId> <artifactId>jplag</artifactId> <version><!--desired version--></version> </dependency> ``` ### Building from sources 1. Download or clone the code from this repository. 2. Run `mvn clean package` from the root of the repository to compile and build all submodules. Run `mvn clean package assembly:single` instead if you need the full jar, which includes all dependencies. Run `mvn -P with-report-viewer clean package assembly:single` to build the full jar with the report viewer. In this case, you'll need [Node.js](https://nodejs.org/en/download) installed. 3. You will find the generated JARs in the subdirectory `cli/target`. ## Usage JPlag can either be used via the CLI or directly via its Java API. For more information, see the [usage information in the wiki](https://github.com/jplag/JPlag/wiki/1.-How-to-Use-JPlag). If you are using the CLI, the report viewer UI will launch automatically. No data will leave your computer! ### CLI *Note that the [legacy CLI](https://github.com/jplag/jplag/blob/legacy/README.md) is varying slightly.* The language can either be set with the -l parameter or as a subcommand (`jplag [jplag options] -l <language name> [language options]`). A subcommand takes priority over the -l option. Language-specific arguments can be set when using the subcommand. A list of language-specific options can be obtained by requesting the help page of a subcommand (e.g., `jplag java --h`). ``` Parameter descriptions: [root-dirs[,root-dirs...]...] Root-directory with submissions to check for plagiarism. If mode is set to VIEW, this parameter can be used to specify a report file to open. In that case only a single file may be specified. -bc, --bc, --base-code=<baseCode> Path to the base code directory (common framework used in all submissions). -l, --language=<language> Select the language of the submissions (default: java). See subcommands below. -M, --mode=<{RUN, VIEW, RUN_AND_VIEW, AUTO}> The mode of JPlag. One of: RUN, VIEW, RUN_AND_VIEW, AUTO (default: null). If VIEW is chosen, you can optionally specify a path to an existing report. -n, --shown-comparisons=<shownComparisons> The maximum number of comparisons that will be shown in the generated report, if set to -1 all comparisons will be shown (default: 2500) -new, --new=<newDirectories>[,<newDirectories>...] Root-directories with submissions to check for plagiarism (same as root). --normalize Activate the normalization of tokens. Supported for languages: Java, C++. -old, --old=<oldDirectories>[,<oldDirectories>...] Root-directories with prior submissions to compare against. -r, --result-file=<resultFile> Name of the file in which the comparison results will be stored (default: results). Missing .jplag endings will be automatically added. -t, --min-tokens=<minTokenMatch> Tunes the comparison sensitivity by adjusting the minimum token required to be counted as a matching section. A smaller value increases the sensitivity but might lead to more false-positives. Advanced --csv-export Export pairwise similarity values as a CSV file. -d, --debug Store on-parsable files in error folder. --log-level=<{ERROR, WARN, INFO, DEBUG, TRACE}> Set the log level for the cli. -m, --similarity-threshold=<similarityThreshold> Comparison similarity threshold [0.0-1.0]: All comparisons above this threshold will be saved (default: 0.0). --overwrite Existing result files will be overwritten. -p, --suffixes=<suffixes>[,<suffixes>...] comma-separated list of all filename suffixes that are included. -P, --port=<port> The port used for the internal report viewer (default: 1996). -s, --subdirectory=<subdirectory> Look in directories <root-dir>/*/<dir> for programs. -x, --exclusion-file=<exclusionFileName> All files named in this file will be ignored in the comparison (line-separated list). Clustering --cluster-alg, --cluster-algorithm=<{AGGLOMERATIVE, SPECTRAL}> Specifies the clustering algorithm. Available algorithms: agglomerative, spectral (default: spectral). --cluster-metric=<{AVG, MIN, MAX, INTERSECTION}> The similarity metric used for clustering. Available metrics: average similarity, minimum similarity, maximal similarity, matched tokens (default: average similarity). --cluster-skip Skips the cluster calculation. Subsequence Match Merging --gap-size=<maximumGapSize> Maximal gap between neighboring matches to be merged (between 1 and minTokenMatch, default: 6). --match-merging Enables merging of neighboring matches to counteract obfuscation attempts. --neighbor-length=<minimumNeighborLength> Minimal length of neighboring matches to be merged (between 1 and minTokenMatch, default: 2). --required-merges=<minimumRequiredMerges> Minimal required merges for the merging to be applied (between 1 and 50, default: 6). Languages: c cpp csharp emf emf-model go java javascript kotlin llvmir multi python3 rlang rust scala scheme scxml swift text typescript ``` ### Java API The new API makes it easy to integrate JPlag's plagiarism detection into external Java projects: <!-- To assure that the code example is always correct, it must be kept in sync with [`ReadmeCodeExampleTest#testReadmeCodeExample`](core/src/test/java/de/jplag/special/ReadmeCodeExampleTest.java). --> ```java Language language = new JavaLanguage(); Set<File> submissionDirectories = Set.of(new File(\"/path/to/rootDir\")); File baseCode = new File(\"/path/to/baseCode\"); JPlagOptions options = new JPlagOptions(language, submissionDirectories, Set.of()).withBaseCodeSubmissionDirectory(baseCode); try { JPlagResult result = JPlag.run(options); // Optional ReportObjectFactory reportObjectFactory = new ReportObjectFactory(new File(\"/path/to/output\")); reportObjectFactory.createAndSaveReport(result); } catch (ExitException e) { // error handling here } catch (FileNotFoundException e) { // handle IO exception here } ``` ## Contributing We're happy to incorporate all improvements to JPlag into this codebase. Feel free to fork the project and send pull requests. Please consider our [guidelines for contributions](https://github.com/jplag/JPlag/wiki/3.-Contributing-to-JPlag). ## Contact If you encounter bugs or other issues, please report them [here](https://github.com/jplag/jplag/issues). For other purposes, you can contact us at jplag@ipd.kit.edu. We would love to hear about your research related to JPlag. Feel free to contact us! ### More information can be found in our [Wiki](https://github.com/jplag/JPlag/wiki)!\n",
                "dependencies": "<?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>de.jplag</groupId> <artifactId>aggregator</artifactId> <version>${revision}</version> <packaging>pom</packaging> <name>JPlag Plagiarism Detector</name> <description>JPlag is a system that finds similarities among multiple sets of source code files. This way it can detect software plagiarism and collusion in software development.</description> <url>http://jplag.de</url> <organization> <name>Karlsruhe Institute of Technology (KIT), KASTEL – Institute of Information Security and Dependability</name> <url>https://sdq.kastel.kit.edu</url> </organization> <licenses> <license> <name>GNU General Public License v3.0</name> <url>https://www.gnu.org/licenses/gpl-3.0.txt</url> </license> </licenses> <developers> <developer> <id>tsaglam</id> <name>Timur Sağlam</name> <email>saglam@kit.edu</email> <url>https://dsis.kastel.kit.edu/staff_saglam.php</url> <organization>KASTEL</organization> <organizationUrl>https://dsis.kastel.kit.edu/</organizationUrl> <timezone>GMT+1</timezone> </developer> <developer> <id>sebinside</id> <name>Sebastian Hahner</name> <email>hahner@kit.edu</email> <url>https://dsis.kastel.kit.edu/staff_sebastian_hahner.php</url> <organization>KASTEL</organization> <organizationUrl>https://dsis.kastel.kit.edu/</organizationUrl> <timezone>GMT+1</timezone> </developer> </developers> <scm> <connection>scm:git:git://github.com/jplag/JPlag.git</connection> <developerConnection>scm:git:ssh://github.com:jplag/JPlag.git</developerConnection> <url>https://github.com/jplag/JPlag</url> </scm> <issueManagement> <system>GitHub</system> <url>https://github.com/jplag/jplag</url> </issueManagement> <ciManagement> <system>GitHub Actions</system> <url>https://github.com/jplag/jplag/actions</url> </ciManagement> <distributionManagement> <snapshotRepository> <id>ossrh</id> <url>https://s01.oss.sonatype.org/content/repositories/snapshots</url> </snapshotRepository> </distributionManagement> <properties> <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> <sonar.projectKey>jplag_JPlag</sonar.projectKey> <sonar.moduleKey>${project.groupId}:${project.artifactId}</sonar.moduleKey> <sonar.organization>jplag</sonar.organization> <sonar.host.url>https://sonarcloud.io</sonar.host.url> <!--suppress UnresolvedMavenProperty --> <sonar.coverage.jacoco.xmlReportPaths>${maven.multiModuleProjectDirectory}/coverage-report/target/site/jacoco-aggregate/jacoco.xml</sonar.coverage.jacoco.xmlReportPaths> <maven.compiler.source>21</maven.compiler.source> <maven.compiler.target>21</maven.compiler.target> <spotless.version>2.44.4</spotless.version> <slf4j.version>2.0.17</slf4j.version> <junit.version>5.12.2</junit.version> <antlr2.version>2.7.7</antlr2.version> <antlr4.version>4.13.2</antlr4.version> <emf.version>2.38.0</emf.version> <emf.ecore.version>2.41.0</emf.ecore.version> <emf.ecore.xmi.version>2.38.0</emf.ecore.xmi.version> <eclipse.core.version>3.22.100</eclipse.core.version> <emfatic.version>1.1.0</emfatic.version> <!-- The Revision of JPlag --> <revision>6.1.0</revision> </properties> <dependencyManagement> <dependencies> <!-- ANTLR --> <dependency> <groupId>antlr</groupId> <artifactId>antlr</artifactId> <version>${antlr2.version}</version> </dependency> <dependency> <groupId>org.antlr</groupId> <artifactId>antlr4-runtime</artifactId> <version>${antlr4.version}</version> </dependency> <dependency> <groupId>org.antlr</groupId> <artifactId>antlr4</artifactId> <version>${antlr4.version}</version> </dependency> <dependency> <groupId>net.sourceforge.argparse4j</groupId> <artifactId>argparse4j</artifactId> <version>0.9.0</version> </dependency> <!-- CoreNLP --> <dependency> <groupId>edu.stanford.nlp</groupId> <artifactId>stanford-corenlp</artifactId> <version>4.5.9</version> </dependency> <!-- LOGGER --> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version>${slf4j.version}</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-simple</artifactId> <version>${slf4j.version}</version> </dependency> <dependency> <groupId>org.kohsuke.metainf-services</groupId> <artifactId>metainf-services</artifactId> <version>1.11</version> </dependency> <dependency> <groupId>com.fasterxml.jackson.core</groupId> <artifactId>jackson-databind</artifactId> <version>2.18.3</version> </dependency> </dependencies> </dependencyManagement> <dependencies> <dependency> <groupId>org.junit.jupiter</groupId> <artifactId>junit-jupiter-engine</artifactId> <version>${junit.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.junit.jupiter</groupId> <artifactId>junit-jupiter-params</artifactId> <version>${junit.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>com.github.stefanbirkner</groupId> <artifactId>system-lambda</artifactId> <version>1.2.1</version> <scope>test</scope> </dependency> <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-core</artifactId> <version>5.17.0</version> <scope>test</scope> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-simple</artifactId> <scope>test</scope> </dependency> </dependencies> <build> <pluginManagement> <plugins> <plugin> <groupId>com.helger.maven</groupId> <artifactId>ph-javacc-maven-plugin</artifactId> <version>4.1.5</version> </plugin> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>antlr-maven-plugin</artifactId> <version>2.1</version> </plugin> <plugin> <groupId>org.antlr</groupId> <artifactId>antlr4-maven-plugin</artifactId> <version>${antlr4.version}</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-jar-plugin</artifactId> <version>3.4.2</version> <configuration> <archive> <manifest> <addDefaultImplementationEntries>true</addDefaultImplementationEntries> <addDefaultSpecificationEntries>true</addDefaultSpecificationEntries> </manifest> </archive> </configuration> <executions> <execution> <goals> <goal>test-jar</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-assembly-plugin</artifactId> <version>3.7.1</version> <configuration> <descriptorRefs> <descriptorRef>jar-with-dependencies</descriptorRef> </descriptorRefs> <archive> <manifest> <addDefaultImplementationEntries>true</addDefaultImplementationEntries> <addDefaultSpecificationEntries>true</addDefaultSpecificationEntries> </manifest> </archive> </configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>3.5.3</version> </plugin> <plugin> <groupId>org.jacoco</groupId> <artifactId>jacoco-maven-plugin</artifactId> <version>0.8.13</version> <executions> <execution> <id>prepare-agent</id> <goals> <goal>prepare-agent</goal> </goals> </execution> <execution> <id>report</id> <goals> <goal>report</goal> </goals> <configuration> <formats> <format>XML</format> </formats> </configuration> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-gpg-plugin</artifactId> <version>3.2.7</version> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-deploy-plugin</artifactId> <version>3.1.4</version> </plugin> </plugins> </pluginManagement> <plugins> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>flatten-maven-plugin</artifactId> <version>1.7.0</version> <configuration> <updatePomFile>true</updatePomFile> <flattenMode>resolveCiFriendliesOnly</flattenMode> <pomElements> <dependencyManagement>expand</dependencyManagement> <dependencies>expand</dependencies> </pomElements> </configuration> <executions> <execution> <id>flatten</id> <goals> <goal>flatten</goal> </goals> <phase>process-resources</phase> </execution> <execution> <id>flatten.clean</id> <goals> <goal>clean</goal> </goals> <phase>clean</phase> </execution> </executions> </plugin> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>build-helper-maven-plugin</artifactId> <version>3.6.0</version> <executions> <execution> <id>add-source</id> <goals> <goal>add-source</goal> </goals> <phase>generate-sources</phase> <configuration> <sources> <source>${project.build.directory}/generated-sources/javacc/</source> <source>${project.build.directory}/generated-sources/antlr/</source> </sources> </configuration> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-javadoc-plugin</artifactId> <version>3.11.2</version> <executions> <execution> <id>attach-javadocs</id> <goals> <goal>jar</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-source-plugin</artifactId> <version>3.3.1</version> <executions> <execution> <id>attach-sources</id> <goals> <goal>jar</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>org.sonatype.plugins</groupId> <artifactId>nexus-staging-maven-plugin</artifactId> <version>1.7.0</version> <extensions>true</extensions> <configuration> <serverId>ossrh</serverId> <nexusUrl>https://s01.oss.sonatype.org/</nexusUrl> <autoReleaseAfterClose>true</autoReleaseAfterClose> </configuration> </plugin> <plugin> <groupId>com.diffplug.spotless</groupId> <artifactId>spotless-maven-plugin</artifactId> <version>${spotless.version}</version> <configuration> <java> <eclipse> <!--suppress UnresolvedMavenProperty --> <file>${maven.multiModuleProjectDirectory}/formatter.xml</file> </eclipse> <importOrder> <file>spotless.importorder</file> </importOrder> <removeUnusedImports></removeUnusedImports> </java> <pom> <sortPom> <encoding>UTF-8</encoding> <keepBlankLines>true</keepBlankLines> <indentBlankLines>false</indentBlankLines> <nrOfIndentSpace>4</nrOfIndentSpace> </sortPom> </pom> </configuration> </plugin> <plugin> <groupId>org.jacoco</groupId> <artifactId>jacoco-maven-plugin</artifactId> </plugin> </plugins> </build> <profiles> <profile> <id>module-defaults</id> <activation> <activeByDefault>true</activeByDefault> </activation> <modules> <module>cli</module> <module>core</module> <module>coverage-report</module> <module>endtoend-testing</module> <module>languages</module> <module>language-api</module> <module>language-antlr-utils</module> <module>language-testutils</module> </modules> </profile> <profile> <id>deployment</id> <activation> <activeByDefault>false</activeByDefault> </activation> <modules> <module>core</module> <module>languages</module> <module>language-api</module> <module>language-testutils</module> <module>language-antlr-utils</module> </modules> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-gpg-plugin</artifactId> <executions> <execution> <id>sign-artifacts</id> <goals> <goal>sign</goal> </goals> <phase>verify</phase> <configuration> <keyname>0xC6E9DAC2</keyname> <!--suppress UnresolvedMavenProperty --> <passphrase>${env.GPG_PASSPHRASE}</passphrase> </configuration> </execution> </executions> </plugin> </plugins> </build> </profile> </profiles> </project>\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jube",
            "repo_link": "https://github.com/FZJ-JSC/JUBE",
            "content": {
                "codemeta": "",
                "readme": "<div align=\"center\"> <img src=\"docs/logo/JUBE-Logo.svg\" alt=\"JUBE\" height=\"170em\"/> </div> [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7534372.svg)](https://doi.org/10.5281/zenodo.7534372) [![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0) # What is JUBE? The JUBE benchmarking environment provides a script-based framework for easily creating benchmark and workflow sets, running those sets on different computer systems, and evaluating the results. It is actively developed by the [Juelich Supercomputing Centre](https://www.fz-juelich.de/en/ias/jsc). It focuses on managing the complexity of combinatorial benchmarks and ensuring reproducibility of the benchmarks. JUBE provides support for different workflows and the ability to use vendor-supplied platform configurations. The benchmark configuration and scripts can be specified in either YAML or XML format. JUBE is primarily designed for use on supercomputers with *scheduding* systems like Slurm or PBS, but also works on laptops running Linux or MacOS operating systems. ## Documentation JUBE is not (yet) available on `pypi` (it is work in progress). The source code can be downloaded from any of the following places: - [GitHub](https://github.com/FZJ-JSC/JUBE) - [JSC JUBE Webpage](https://www.fz-juelich.de/en/ias/jsc/services/user-support/software-tools/jube/download) JUBE can be installed using `pip` or `setup.py` and needs *python 3.2* or higher. You will also need *SQLite* version 3.35.0 (or higher) to use the database as a result output. Installation instructions can be found [here](https://apps.fz-juelich.de/jsc/jube/docu/tutorial.html#installation). The documentation for JUBE is split into Beginner Tutorial, Advanced Tutorial, FAQ, CLI, and Glossary and can be found in the **[User Guide](https://apps.fz-juelich.de/jsc/jube/docu/index.html)**. In addition to the documentation, there are also [tutorial examples](examples) which are described in the tutorials of the user guide and [benchmark examples](https://github.com/FZJ-JSC/jube-configs), which are curated examples of JUBE benchmarks (the latter will be either replaced or updated/extended soon). For more information on the design and architecture of JUBE, please refer to this [paper](https://ebooks.iospress.nl/DOI/10.3233/978-1-61499-621-7-431). ## Community and Contributing JUBE is an open-source project and we welcome your questions, discussions and contributions. Questions can be asked directly to the JSC JUBE developers via mail to [jube.jsc@fz-juelich.de](mailto:jube.jsc@fz-juelich.de) and issues can be reported in the issue tracker. We also welcome contributions in the form of pull requests. Contributions can include anything from bug fixes and documentation to new features. JUBE development is currently still taking place on an internal GitLab instance. However, we are in a transition phase to move development to GitHub. The complete move will take some time. In the meantime, we will decide individually how to proceed with Pull Requests opened on GitHub. Before you start implementing new features, we would recommended to contact us, as we still have several open branches in GitLab. - **[GitHub Issue Tracker](https://github.com/FZJ-JSC/JUBE/issues)** - **[Github Discussions](https://github.com/FZJ-JSC/JUBE/discussions)** - **[GitHub Pull Requests](https://github.com/FZJ-JSC/JUBE/pulls)** Please ensure that your contributions to JUBE are compliant with the [contribution](CONTRIBUTING.md), [developer](https://apps.fz-juelich.de/jsc/jube/docu/devel.html) and [community](CODE_OF_CONDUCT.md) guidelines. # Citing JUBE If you use JUBE in your work, please cite the [software release](https://zenodo.org/records/7534372) and the [paper](https://ebooks.iospress.nl/DOI/10.3233/978-1-61499-621-7-431). # Acknowledgments We gratefully acknowledge the support of the following research projects and institutions in the development of JUBE and for granting compute time to develop JUBE. - UNSEEN (BMWi project, ID: 03EI1004A-F) - Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) and the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS at Jülich Supercomputing Centre (JSC)\n",
                "dependencies": "# JUBE Benchmarking Environment # Copyright (C) 2008-2024 # Forschungszentrum Juelich GmbH, Juelich Supercomputing Centre # http://www.fz-juelich.de/jsc/jube # # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see <http://www.gnu.org/licenses/>. # For installation you can use: # # python setup.py install --user # # to install it into your .local folder. .local/bin must be inside your $PATH. # You can also change the folder by using --prefix instead of --user import os add_opt = dict() try: from setuptools import setup import sys add_opt[\"install_requires\"] = ['pyyaml'] if sys.hexversion < 0x02070000: add_opt[\"install_requires\"].append(\"argparse\") except ImportError: from distutils.core import setup SHARE_PATH = \"share/jube\" def rel_path(directory, new_root=\"\"): \"\"\"Return list of tuples (directory, list of files) recursively from directory\"\"\" setup_dir = os.path.join(os.path.dirname(__file__)) cwd = os.getcwd() result = list() if setup_dir != \"\": os.chdir(setup_dir) for path_info in os.walk(directory): root = path_info[0] filenames = path_info[2] files = list() for filename in filenames: path = os.path.join(root, filename) if (os.path.isfile(path)) and (filename[0] != \".\"): files.append(path) if len(files) > 0: result.append((os.path.join(new_root, root), files)) if setup_dir != \"\": os.chdir(cwd) return result config = {'name': 'JUBE', 'description': 'JUBE Benchmarking Environment', 'author': 'Forschungszentrum Juelich GmbH', 'url': 'www.fz-juelich.de/ias/jsc/jube', 'download_url': 'www.fz-juelich.de/ias/jsc/jube', 'author_email': 'jube.jsc@fz-juelich.de', 'version': '2.7.1', 'packages': ['jube','jube.result_types','jube.util'], 'package_data': {'jube': ['help.txt']}, 'data_files': ([(os.path.join(SHARE_PATH, 'docu'), ['docs/JUBE.pdf']), (SHARE_PATH, ['AUTHORS','LICENSE','RELEASE_NOTES','CITATION.cff', 'CODE_OF_CONDUCT.md', 'CONTRIBUTING.md'])] + rel_path(\"examples\", SHARE_PATH) + rel_path(\"contrib\", SHARE_PATH) + rel_path(\"platform\", SHARE_PATH)), 'scripts': ['bin/jube', 'bin/jube-autorun'], 'long_description': ( \"Automating benchmarks is important for reproducibility and \" \"hence comparability which is the major intent when \" \"performing benchmarks. Furthermore managing different \" \"combinations of parameters is error-prone and often \" \"results in significant amounts work especially if the \" \"parameter space gets large.\\n\" \"In order to alleviate these problems JUBE helps performing \" \"and analyzing benchmarks in a systematic way. It allows \" \"custom work flows to be able to adapt to new architectures.\\n\" \"For each benchmark application the benchmark data is written \" \"out in a certain format that enables JUBE to deduct the \" \"desired information. This data can be parsed by automatic \" \"pre- and post-processing scripts that draw information, \" \"and store it more densely for manual interpretation.\\n\" \"The JUBE benchmarking environment provides a script based \" \"framework to easily create benchmark sets, run those sets \" \"on different computer systems and evaluate the results. It \" \"is actively developed by the Juelich Supercomputing Centre \" \"of Forschungszentrum Juelich, Germany.\"), 'license': 'GPLv3', 'platforms': 'Linux', 'classifiers': [ \"Development Status :: 5 - Production/Stable\", \"Environment :: Console\", \"Intended Audience :: End Users/Desktop\", \"Intended Audience :: Developers\", \"Intended Audience :: System Administrators\", \"License :: OSI Approved :: GNU General Public License v3 \" + \"(GPLv3)\", \"Operating System :: POSIX :: Linux\", \"Programming Language :: Python :: 3.2\", \"Topic :: System :: Monitoring\", \"Topic :: System :: Benchmark\", \"Topic :: Software Development :: Testing\"], 'keywords': 'JUBE Benchmarking Environment'} config.update(add_opt) setup(**config) try: import ruamel.yaml except ImportError: print(\"Warning: The python package 'ruamel.yaml' is not installed. The validity of yaml files cannot be checked properly and silent errors can occur. Nevertheless, the installation is complete.\")\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jukkr",
            "repo_link": "https://iffgit.fz-juelich.de/kkr/jukkr",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/julearn",
            "repo_link": "https://github.com/juaml/julearn",
            "content": {
                "codemeta": "",
                "readme": "# julearn ![PyPI](https://img.shields.io/pypi/v/julearn?style=flat-square) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/julearn?style=flat-square) ![PyPI - Wheel](https://img.shields.io/pypi/wheel/julearn?style=flat-square) [![Anaconda-Server Badge](https://anaconda.org/conda-forge/julearn/badges/version.svg)](https://anaconda.org/conda-forge/julearn) ![GitHub](https://img.shields.io/github/license/juaml/julearn?style=flat-square) ![Codecov](https://img.shields.io/codecov/c/github/juaml/julearn?style=flat-square) [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json)](https://github.com/charliermarsh/ruff) [![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit) ## About The Forschungszentrum Jülich Machine Learning Library Check our full documentation here: https://juaml.github.io/julearn/index.html It is currently being developed and maintained at the [Applied Machine Learning](https://www.fz-juelich.de/en/inm/inm-7/research-groups/applied-machine-learning-aml) group at [Forschungszentrum Juelich](https://www.fz-juelich.de/en), Germany. ## Installation Use `pip` to install from PyPI like so: ``` pip install julearn ``` You can also install via `conda`, like so: ``` conda install -c conda-forge julearn ``` ## Licensing julearn is released under the AGPL v3 license: julearn, FZJuelich AML machine learning library. Copyright (C) 2020, authors of julearn. This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program. If not, see <http://www.gnu.org/licenses/>. ## Citing If you use julearn in a scientific publication, please use the following reference > Hamdan, Sami, Shammi More, Leonard Sasse, Vera Komeyer, Kaustubh R. Patil, and Federico Raimondo. 'Julearn: An Easy-to-Use Library for Leakage-Free Evaluation and Inspection of ML Models'. arXiv, 19 October 2023. https://doi.org/10.48550/arXiv.2310.12568. Since julearn is also heavily reliant on scikit-learn, please also cite them: https://scikit-learn.org/stable/about.html#citing-scikit-learn\n",
                "dependencies": "[build-system] requires = [ \"setuptools >= 61.0.0\", \"wheel\", \"setuptools_scm[toml] >= 6.2\" ] build-backend = \"setuptools.build_meta\" [project] name = \"julearn\" description = \"Juelich Machine Learning Library\" readme = \"README.md\" requires-python = \">=3.8\" license = {text = \"AGPL-3.0-only\"} authors = [ { name = \"Fede Raimondo\", email = \"f.raimondo@fz-juelich.de\" }, { name = \"Sami Hamdan\", email = \"s.hamdan@fz-juelich.de\" }, ] maintainers = [ {name = \"Sami Hamdan\", email = \"s.hamdan@fz-juelich.de\"}, ] keywords = [ \"machine-learning\", ] classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Intended Audience :: Science/Research\", \"Intended Audience :: Developers\", \"License :: OSI Approved\", \"Natural Language :: English\", \"Topic :: Software Development\", \"Topic :: Scientific/Engineering\", \"Operating System :: OS Independent\", \"Programming Language :: Python :: 3.8\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", ] dependencies = [ \"numpy>=1.24,<1.27\", \"pandas>=1.5.0,<2.3\", \"statsmodels>=0.13,<0.15\", \"scikit-learn>=1.2.0,<1.6.0\", ] dynamic = [\"version\"] [project.urls] homepage = \"https://juaml.github.io/julearn\" documentation = \"https://juaml.github.io/julearn\" repository = \"https://github.com/juaml/julearn\" [project.optional-dependencies] dev = [\"tox\", \"pre-commit\"] docs = [ \"seaborn>=0.12.2,<0.13\", \"sphinx>=5.3.0,<7.3\", \"sphinx-gallery>=0.13.0,<0.15\", \"furo>=2022.9.29,<2024.0.0\", \"sphinx_copybutton>=0.5.0,<0.6\", \"numpydoc>=1.5.0,<1.6\", \"towncrier<24\", \"scikit-optimize>=0.10.0,<0.11\", \"optuna>=3.6.0,<3.7\", \"optuna_integration>=3.6.0,<4.1\", ] deslib = [\"deslib>=0.3.5,<0.4\"] viz = [ \"panel>=1.3.0\", \"bokeh>=3.0.0\", \"param>=2.0.0\", ] skopt = [\"scikit-optimize>=0.10.0,<0.11\"] optuna = [ \"optuna>=3.6.0,<3.7\", \"optuna_integration>=3.6.0,<4.1\", ] # Add all optional functional dependencies (skip deslib until its fixed) # This does not include dev/docs building dependencies all = [\"julearn[viz,skopt,optuna]\"] ################ # Tool configs # ################ [tool.setuptools] packages = [\"julearn\"] [tool.setuptools_scm] version_scheme = \"guess-next-dev\" local_scheme = \"no-local-version\" write_to = \"julearn/_version.py\" [tool.black] line-length = 79 target-version = [\"py38\", \"py39\", \"py310\", \"py311\"] [tool.codespell] skip = \"*/auto_examples/*,*.html,.git/,*.pyc,*/_build/*,*/api/generated/*.examples,julearn/external/*\" count = \"\" quiet-level = 3 ignore-words = \"ignore_words.txt\" interactive = 0 builtin = \"clear,rare,informal,names,usage,code\" [tool.ruff] line-length = 79 extend-exclude = [ \"__init__.py\", \"docs\", \"examples\", \"external\", ] [tool.ruff.lint] select = [ # flake8-bugbear \"B\", # flake8-blind-except \"BLE\", # flake8-comprehensions \"C4\", # mccabe \"C90\", # pydocstyle \"D\", # pycodestyle errors \"E\", # pyflakes \"F\", # isort \"I\", # pep8-naming \"N\", # pygrep-hooks \"PGH\", # ruff \"RUF\", # flake8-type-checking \"TCH\", # pyupgrade \"UP\", # pycodestyle warnings \"W\", # flake8-2020 \"YTT\", ] extend-ignore = [ # Use of `functools.lru_cache` or `functools.cache` on methods can lead to # memory leaks. The cache may retain instance references, preventing garbage # collection. \"B019\", # abstract class with no abstract methods \"B024\", \"D202\", # missing docstring in __init__, incompatible with numpydoc \"D107\", # use r\"\"\" if any backslashes in a docstring \"D301\", # class names should use CapWords convention \"N801\", # function name should be lowercase \"N802\", # variable in function should be lowercase \"N806\", # use specific rule codes when ignoring type issues \"PGH003\", ] [tool.ruff.lint.isort] lines-after-imports = 2 known-first-party = [\"julearn\"] known-third-party =[ \"numpy\", \"pandas\", \"sklearn\", \"statsmodels\", \"bokeh\", \"panel\", \"param\", \"deslib\", \"pytest\", ] [tool.ruff.lint.mccabe] max-complexity = 20 [tool.towncrier] directory = \"docs/changes/newsfragments\" filename = \"docs/whats_new.rst\" package = \"julearn\" # to use gh_substitutions issue_format = \":gh:`{issue}`\" # modify to have proper toctree underlines = \"-^~\" # set line length to 79 wrap = true # Need to put default explicitly as custom is not combined with default [tool.towncrier.fragment.bugfix] name = \"Bugfixes\" showcontent = true [tool.towncrier.fragment.doc] name = \"Improved Documentation\" showcontent = true [tool.towncrier.fragment.feature] name = \"Features\" showcontent = true [tool.towncrier.fragment.misc] name = \"Misc\" showcontent = true [tool.towncrier.fragment.removal] name = \"Deprecations and Removals\" showcontent = true # Add custom towncrier fragment for enhancements [tool.towncrier.fragment.enh] name = \"Enhancements\" showcontent = true # Add custom towncrier fragment for API changes [tool.towncrier.fragment.change] name = \"API Changes\" showcontent = true ## Configure pyright to ignore assignment types until scikit-learn stubs are updated [tool.pyright] reportAssignmentType = \"none\" exclude = [ \"docs/auto_examples/\", \"*.html\", \".git/\", \"*.pyc,\", \"*/_build/*\", \"*/api/generated/*.examples\", \"build/\", \"examples/XX_disabled/\", \".tox\", \".eggs\", \"examples/\", # Lots of problems due to bad stubs, avoid filling the example with # type:ignore \"julearn/external\", # External code, not to be checked \"scratch/\", # place to prototype, not to be checked ]\n\"\"\"Set up julearn package.\"\"\" # Authors: Federico Raimondo <f.raimondo@fz-juelich.de> # Sami Hamdan <s.hamdan@fz-juelich.de> # Synchon Mandal <s.mandal@fz-juelich.de> # License: AGPL from setuptools import setup if __name__ == \"__main__\": setup()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jumpdiff",
            "repo_link": "https://github.com/LRydin/jumpdiff",
            "content": {
                "codemeta": "",
                "readme": "![PyPI - License](https://img.shields.io/pypi/l/jumpdiff) ![PyPI](https://img.shields.io/pypi/v/jumpdiff) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/jumpdiff) [![Build Status](https://github.com/LRydin/jumpdiff/actions/workflows/CI.yml/badge.svg)](https://github.com/LRydin/jumpdiff/actions/workflows/CI.yml) [![codecov](https://codecov.io/gh/LRydin/jumpdiff/branch/master/graph/badge.svg)](https://codecov.io/gh/LRydin/jumpdiff) [![Documentation Status](https://readthedocs.org/projects/jumpdiff/badge/?version=latest)](https://jumpdiff.readthedocs.io/en/latest/?badge=latest) # jumpdiff `jumpdiff` is a `python` library with non-parametric Nadaraya─Watson estimators to extract the parameters of jump-diffusion processes. With `jumpdiff` one can extract the parameters of a jump-diffusion process from one-dimensional timeseries, employing both a kernel-density estimation method combined with a set on second-order corrections for a precise retrieval of the parameters for short timeseries. ## Installation To install `jumpdiff`, run ``` pip install jumpdiff ``` Then on your favourite editor just use ```python import jumpdiff as jd ``` ## Dependencies The library parameter estimation depends on `numpy` and `scipy` solely. The mathematical formulae depend on `sympy`. It stems from [`kramersmoyal`](https://github.com/LRydin/KramersMoyal) project, but functions independently from it<sup>3</sup>. ## Documentation You can find the documentation [here](https://jumpdiff.readthedocs.io/). # Jump-diffusion processes ## The theory Jump-diffusion processes<sup>1</sup>, as the name suggest, are a mixed type of stochastic processes with a diffusive and a jump term. One form of these processes which is mathematically traceable is given by the [Stochastic Differential Equation](https://en.wikipedia.org/wiki/Stochastic_differential_equation) <img src=\"/Others/SDE_1.png\" title=\"A jump diffusion process\" height=\"25\"/> which has 4 main elements: a drift term <img src=\"/Others/a_xt.png\" title=\"drift term\" height=\"18\"/>, a diffusion term <img src=\"/Others/b_xt.png\" title=\"diffusion term\" height=\"18\"/>, and jump amplitude term <img src=\"/Others/xi.png\" title=\"jump amplitude term\" height=\"18\"/>, which is given by a Gaussian distribution, and finally a jump rate <img src=\"/Others/lambda.png\" title=\"jump rate term\" height=\"14\"/>. You can find a good review on this topic in Ref. 2. ## Integrating a jump-diffusion process Let us use the functions in `jumpdiff` to generate a jump-difussion process, and subsequently retrieve the parameters. This is a good way to understand the usage of the integrator and the non-parametric retrieval of the parameters. First we need to load our library. We will call it `jd` ```python import jumpdiff as jd ``` Let us thus define a jump-diffusion process and use `jd_process` to integrate it. Do notice here that we need the drift <img src=\"/Others/a_xt.png\" title=\"drift term\" height=\"18\"/> and diffusion <img src=\"/Others/b_xt.png\" title=\"diffusion term\" height=\"18\"/> as functions. ```python # integration time and time sampling t_final = 10000 delta_t = 0.001 # A drift function def a(x): return -0.5*x # and a (constant) diffusion term def b(x): return 0.75 # Now define a jump amplitude and rate xi = 2.5 lamb = 1.75 # and simply call the integration function X = jd.jd_process(t_final, delta_t, a=a, b=b, xi=xi, lamb=lamb) ``` This will generate a jump diffusion process `X` of length `int(10000/0.001)` with the given parameters. <img src=\"/Others/X_trajectory.png\" title=\"A jump-difussion process\" height=\"200\"/> ## Using `jumpdiff` to retrieve the parameters ### Moments and Kramers─Moyal coefficients Take the timeseries `X` and use the function `moments` to retrieve the conditional moments of the process. For now let us focus on the shortest time lag, so we can best approximate the Kramers─Moyal coefficients. For this case we can simply employ ```python edges, moments = jd.moments(timeseries = X) ``` In the array `edges` are the limits of our space, and in our array `moments` are recorded all 6 powers/order of our conditional moments. Let us take a look at these before we proceed, to get acquainted with them. We can plot the first moment with any conventional plotter, so lets use here `plotly` from `matplotlib` ```python import matplotlib.plotly as plt # we want the first power, so we need 'moments[1,...]' plt.plot(edges, moments[1,...]) ``` The first moment here (i.e., the first Kramers─Moyal coefficient) is given solely by the drift term that we have selected `-0.5*x` <img src=\"/Others/1_moment.png\" title=\"The 1st Kramers─Moyal coefficient\" height=\"200\"/> And the second moment (i.e., the second Kramers─Moyal coefficient) is a mixture of both the contributions of the diffusive term <img src=\"/Others/b_xt.png\" title=\"diffusion term\" height=\"18\"/> and the jump terms <img src=\"/Others/xi.png\" title=\"jump amplitude term\" height=\"18\"/> and <img src=\"/Others/lambda.png\" title=\"jump rate term\" height=\"14\"/>. <img src=\"/Others/2_moment.png\" title=\"The 2nd Kramers─Moyal coefficient\" height=\"200\"/> You have this stored in `moments[2,...]`. ### Retrieving the jump-related terms Naturally one of the most pertinent questions when addressing jump-diffusion processes is the possibility of recovering these same parameters from data. For the given jump-diffusion process we can use the `jump_amplitude` and `jump_rate` functions to non-parametrically estimate the jump amplitude <img src=\"/Others/xi.png\" title=\"jump amplitude term\" height=\"18\"/> and jump rate <img src=\"/Others/lambda.png\" title=\"jump rate term\" height=\"18\"/> terms. After having the `moments` in hand, all we need is ```python # first estimate the jump amplitude xi_est = jd.jump_amplitude(moments = moments) # and now estimated the jump rate lamb_est = jd.jump_rate(moments = moments) ``` which resulted in our case in `(xi_est) ξ = 2.43 ± 0.17` and `(lamb_est) λ = 1.744 * delta_t` (don't forget to divide `lamb_est` by `delta_t`)! ### Other functions and options Include in this package is also the [Milstein scheme](https://en.wikipedia.org/wiki/Milstein_method) of integration, particularly important when the diffusion term has some spacial `x` dependence. `moments` can actually calculate the conditional moments for different lags, using the parameter `lag`. In `formulae` the set of formulas needed to calculate the second order corrections are given (in `sympy`). # Contributions We welcome reviews and ideas from everyone. If you want to share your ideas, upgrades, doubts, or simply report a bug, open an [issue](https://github.com/LRydin/jumpdiff/issues) here on GitHub, or contact us directly. If you need help with the code, the theory, or the implementation, drop us an email. We abide to a [Conduct of Fairness](contributions.md). # Changelog - Version 0.4 - Designing a set of self-consistency checks, the documentation, examples, and a trial code. Code at PyPi. - Version 0.3 - Designing a straightforward procedure to retrieve the jump amplitude and jump rate functions, alongside with a easy `sympy` displaying the correction. - Version 0.2 - Introducing the second-order corrections to the moments - Version 0.1 - Design an implementation of the `moments` functions, generalising `kramersmoyal` `km`. # Literature and Support ### History This project was started in 2017 at the [neurophysik](https://www.researchgate.net/lab/Klaus-Lehnertz-Lab-2) by Leonardo Rydin Gorjão, Jan Heysel, Klaus Lehnertz, and M. Reza Rahimi Tabar, and separately by Pedro G. Lind, at the Department of Computer Science, Oslo Metropolitan University. From 2019 to 2021, Pedro G. Lind, Leonardo Rydin Gorjão, and Dirk Witthaut developed a set of corrections and an implementation for python, presented here. ### Funding Helmholtz Association Initiative _Energy System 2050 - A Contribution of the Research Field Energy_ and the grant No. VH-NG-1025 and *STORM - Stochastics for Time-Space Risk Models* project of the Research Council of Norway (RCN) No. 274410. --- ##### Bibliography <sup>1</sup> Tabar, M. R. R. *Analysis and Data-Based Reconstruction of Complex Nonlinear Dynamical Systems.* Springer, International Publishing (2019), Chapter [*Stochastic Processes with Jumps and Non-vanishing Higher-Order Kramers-Moyal Coefficients*](https://doi.org/10.1007/978-3-030-18472-8_11). <sup>2</sup> Friedrich, R., Peinke, J., Sahimi, M., Tabar, M. R. R. *Approaching complexity by stochastic methods: From biological systems to turbulence,* [Physics Reports 506, 87-162 (2011)](https://doi.org/10.1016/j.physrep.2011.05.003). <sup>3</sup> Rydin Gorjão, L., Meirinhos, F. *kramersmoyal: Kramers-Moyal coefficients for stochastic processes.* [Journal of Open Source Software, **4**(44) (2019)](https://doi.org/10.21105/joss.01693). ##### Extended Literature You can find further reading on SDE, non-parametric estimatons, and the general principles of the Fokker-Planck equation, Kramers-Moyal expansion, and related topics in the classic (physics) books - Risken, H. *The Fokker-Planck equation.* Springer, Berlin, Heidelberg (1989). - Gardiner, C.W. *Handbook of Stochastic Methods.* Springer, Berlin (1985). And an extensive review on the subject [here](http://sharif.edu/~rahimitabar/pdfs/80.pdf)\n",
                "dependencies": "numpy scipy sympy\nimport setuptools with open(\"README.md\", \"r\") as fh: long_description = fh.read() setuptools.setup( name=\"jumpdiff\", version=\"0.4.2\", author=\"Leonardo Rydin Gorjão\", author_email=\"leonardo.rydin@gmail.com\", description=\"jumpdiff: Non-parametric estimators for jump-diffusion processes for Python.\", long_description=long_description, long_description_content_type=\"text/markdown\", url=\"https://github.com/LRydin/jumpdiff\", packages=setuptools.find_packages(), install_requires=[ \"numpy\", \"scipy\", \"sympy\", ], classifiers=[ \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: MIT License\", \"Operating System :: OS Independent\", ], license=\"MIT License\", python_requires='>=3.5', )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jupedsim",
            "repo_link": "https://github.com/PedestrianDynamics/jupedsim",
            "content": {
                "codemeta": "",
                "readme": "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1293771.svg)](https://doi.org/10.5281/zenodo.1293771) [![GitHub license](https://img.shields.io/badge/license-LGPL-blue.svg)](https://raw.githubusercontent.com/PedestrianDynamics/jupedsim/master/LICENSE) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/jupedsim) ![PyPI - Version](https://img.shields.io/pypi/v/jupedsim) # Jülich Pedestrian Simulator - JuPedSim JuPedSim is a library to simulate pedestrian dynamics. This software was initiated at the Institute for Civil Safety [IAS-7](https://www.fz-juelich.de/en/ias/ias-7) of the Jülich Research Center (Forschungszentrum Jülich) in Germany and continues to evolve with support and contributions from a diverse and engaged community. ## Contributors We are grateful to all the contributors who have helped shape JuPedSim into what it is today. You can find the full list of contributors [here](CONTRIBUTORS.md). ## Installation It is easiest to install directly with pip from [PyPi.org](https://pypi.org/project/jupedsim/) ``` pip install jupedsim ``` ## Usage Please consult our [documentation.](http://jupedsim.org) ## Contributing JuPedSim is licensed under [GNU LGPLv3](LICENSE) hence we are looking forward to your contributions and would be happy to see questions, issues and pull requests. ### Questions If you have a question or a problem please open a new topic in [GitHub discussions](https://github.com/PedestrianDynamics/jupedsim/discussions). ### Issues If you found a bug and want to give us a chance to fix it we would be very happy to hear from you. To make it easy for us to help you please include the following information when you open a [new issue](https://github.com/PedestrianDynamics/jupedsim/issues): * What did JuPedSim do? * What did you expect JuPedSim to do? * How can we reproduce the issue? ### Pull Requests If you encounter a bug and are would like to submit a fix feel free to open a PR, we will look into it. Before embarking on larger work it is a good idea to [discuss](https://github.com/PedestrianDynamics/jupedsim/discussions) what you plan. While we are very happy if you contribute we reserve us the right to decline your PR because it may not fit into our vision of JuPedSim. ## License [GNU LGPLv3](LICENSE) ## Building from source Here you have two options. ### With setuptools You will need a C++20 capable compiler and CMake >= 3.19 installed on your system. Then install our python dependencies via pip. Our python package dependencies are listed in `requirements.txt` in the root of this repository. Now you can call `pip install .` E.g.: ```bash cd jupedsim pip install -r requirements.txt pip install . ``` ### Compile yourself You will need a C++20 capable compiler and CMake >= 3.19 installed on your system. Then install our python dependencies via pip. Our python package dependencies are listed in `requirements.txt` in the root of this repository. Now you can generate makefiles with CMake, then compile and run the python library. ```bash pip install -r jupedsim/requirements.txt mkdir jupedsim-build cd jupedsim-build cmake ../jupedsim make -j source ./environment ``` The last line in the above description will populate the python path with the location of our python code and the native library. > [!WARNING] > > When sourcing `./environment` from the build folder you need to ensure JuPedSim > is not installed in the current python environment. Otherwise there will be > erroneous calls to the wrong python code, resulting in crashes and/or > exceptions.\n",
                "dependencies": "################################################################################ # Project setup ################################################################################ cmake_minimum_required(VERSION 3.22 FATAL_ERROR) project(JuPedSim VERSION 1.3.0 LANGUAGES CXX) set(CMAKE_CXX_STANDARD 20) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(CMAKE_CXX_EXTENSIONS OFF) set(CMAKE_MODULE_PATH \"${CMAKE_SOURCE_DIR}/cmake_modules\") set(CMAKE_EXPORT_COMPILE_COMMANDS ON) include(helper_functions) print_var(PROJECT_VERSION) # Set default build type to release set(default_build_type \"Release\") if(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES) message(STATUS \"Setting build type to '${default_build_type}' as none was specified.\") set(CMAKE_BUILD_TYPE \"${default_build_type}\" CACHE STRING \"Choose the type of build.\" FORCE) endif() set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib) set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib) set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin) set(USE_IPO ON) check_prefix_path() ################################################################################ # Optional features ################################################################################ set(WERROR OFF CACHE BOOL \"Build wth -Werror enabled. Not supported on Windows\") print_var(WERROR) set(BUILD_TESTS OFF CACHE BOOL \"Build tests\") print_var(BUILD_TESTS) set(BUILD_WITH_ASAN OFF CACHE BOOL \"Build with address sanitizer support. Linux / macOS only.\") print_var(BUILD_WITH_ASAN) if(BUILD_WITH_ASAN AND ${CMAKE_SYSTEM} MATCHES \"Windows\") message(FATAL_ERROR \"Address sanitizer builds are not supported on Windows\") endif() set(BUILD_BENCHMARKS OFF CACHE BOOL \"Build micro benchmark\") print_var(BUILD_BENCHMARKS) set(WITH_FORMAT OFF CACHE BOOL \"Create format tools\") print_var(WITH_FORMAT) if(WITH_FORMAT AND ${CMAKE_SYSTEM} MATCHES \"Windows\") message(FATAL_ERROR \"Format target not supported on Windows\") endif() ################################################################################ # Compilation flags ################################################################################ # Note: Setting global compile flags via CMAKE_CXX_FLAGS has the drawback that # generator expressions cannot be used. This leads to all kind of # conditional adding of flags. It is generally preferable to use generator # expresssions. # # WARNING: Do not break the lines, each option has to be on its own line or # CMake will enclose multiple flags in '' which the compiler then # treats as a single flag and does not understand. list(APPEND COMMON_COMPILE_OPTIONS $<$<AND:$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>,$<BOOL:${WERROR}>>:-Werror> $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-Wall> $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-Wextra> $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fdiagnostics-color=always> $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fPIC> $<$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>,$<CXX_COMPILER_ID:GNU>>:-fno-omit-frame-pointer> $<$<CXX_COMPILER_ID:MSVC>:/W2> $<$<CXX_COMPILER_ID:MSVC>:/EHsc> $<$<CXX_COMPILER_ID:MSVC>:/MP> $<$<AND:$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>>,$<BOOL:${BUILD_WITH_ASAN}>>:-fno-optimize-sibling-calls> $<$<AND:$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>>,$<BOOL:${BUILD_WITH_ASAN}>>:-fsanitize=address> $<$<AND:$<OR:$<CXX_COMPILER_ID:Clang>,$<CXX_COMPILER_ID:AppleClang>>,$<BOOL:${BUILD_WITH_ASAN}>>:-shared-libasan> ) ################################################################################ # Dependencies ################################################################################ add_subdirectory(third-party) ################################################################################ # VCS info ################################################################################ find_package(Git QUIET) find_program(GIT_SCM git DOC \"Git version control\") mark_as_advanced(GIT_SCM) find_file(GITDIR NAMES .git PATHS ${CMAKE_SOURCE_DIR} NO_DEFAULT_PATH) if (GIT_SCM AND GITDIR) # the commit's SHA1, and whether the building workspace was dirty or not # describe --match=NeVeRmAtCh --always --tags --abbrev=40 --dirty execute_process(COMMAND \"${GIT_EXECUTABLE}\" --no-pager describe --match=Nevermatch --tags --always --dirty WORKING_DIRECTORY \"${CMAKE_SOURCE_DIR}\" OUTPUT_VARIABLE GIT_SHA1 ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE) # branch execute_process( COMMAND \"${GIT_EXECUTABLE}\" rev-parse --abbrev-ref HEAD WORKING_DIRECTORY \"${CMAKE_SOURCE_DIR}\" OUTPUT_VARIABLE GIT_BRANCH OUTPUT_STRIP_TRAILING_WHITESPACE ) # the date of the commit execute_process(COMMAND \"${GIT_EXECUTABLE}\" log -1 --format=%ad --date=local WORKING_DIRECTORY \"${CMAKE_SOURCE_DIR}\" OUTPUT_VARIABLE GIT_DATE ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE) execute_process(COMMAND \"${GIT_EXECUTABLE}\" describe --tags --abbrev=0 WORKING_DIRECTORY \"${CMAKE_SOURCE_DIR}\" OUTPUT_VARIABLE GIT_TAG ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE) # the subject of the commit execute_process(COMMAND \"${GIT_EXECUTABLE}\" log -1 --format=%s WORKING_DIRECTORY \"${CMAKE_SOURCE_DIR}\" OUTPUT_VARIABLE GIT_COMMIT_SUBJECT ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE) # remove # from subject string(REGEX REPLACE \"[\\#\\\"]+\" \"\" GIT_COMMIT_SUBJECT ${GIT_COMMIT_SUBJECT}) else() message(STATUS \"Not in a git repo\") set(GIT_SHA1 \"UNKNOWN\") set(GIT_DATE \"UNKNOWN\") set(GIT_COMMIT_SUBJECT \"UNKNOWN\") set(GIT_BRANCH \"UNKNOWN\") set(GIT_TAG \"UNKNOWN\") endif() add_library(git-info INTERFACE) target_compile_definitions(git-info INTERFACE GIT_COMMIT_HASH=\"${GIT_SHA1}\" GIT_COMMIT_DATE=\"${GIT_DATE}\" GIT_TAG=\"${GIT_TAG}\" GIT_COMMIT_SUBJECT=\"${GIT_COMMIT_SUBJECT}\" GIT_BRANCH=\"${GIT_BRANCH}\" ) configure_file(cmake_templates/BuildInfo.hpp.in ${CMAKE_BINARY_DIR}/generated/build_info/BuildInfo.hpp @ONLY) add_library(build_info INTERFACE) target_include_directories(build_info INTERFACE ${CMAKE_BINARY_DIR}/generated/build_info ) if(UNIX) configure_file(cmake_templates/unix_env.in ${CMAKE_BINARY_DIR}/environment @ONLY) endif() ################################################################################ # Testing ################################################################################ if(BUILD_TESTS) if(UNIX) set(pytest-wrapper-in ${CMAKE_SOURCE_DIR}/cmake_templates/run-systemtests.unix.in) set(pytest-wrapper-out ${CMAKE_BINARY_DIR}/run-systemtests) else() set(pytest-wrapper-in ${CMAKE_SOURCE_DIR}/cmake_templates/run-systemtests.windows.in) set(pytest-wrapper-out ${CMAKE_BINARY_DIR}/run-systemtests.cmd) endif() configure_file( ${pytest-wrapper-in} ${pytest-wrapper-out} @ONLY ) add_custom_target(tests DEPENDS systemtests unittests ) add_custom_target(systemtests COMMENT \"Running system tests\" COMMAND ${pytest-wrapper-out} -vv -s --basetemp=${CMAKE_BINARY_DIR}/systemtest-out --junit-xml=result-systemtests.xml DEPENDS py_jupedsim ) set(unittests_dependency_list libjupedsim-unittests libsimulator-unittests) add_custom_target(unittests DEPENDS ${unittests_dependency_list} ) add_custom_target(libjupedsim-unittests COMMENT \"Running libjupedsim unittests\" COMMAND $<TARGET_FILE:libjupedsim-tests> --gtest_output=xml:result-libjupedsim-unittests.xml DEPENDS libjupedsim-tests ) add_custom_target(libsimulator-unittests COMMENT \"Running unit tests\" COMMAND $<TARGET_FILE:libsimulator-tests> --gtest_output=xml:result-libsimulator-unittests.xml DEPENDS libsimulator-tests ) endif() if(UNIX) set(performancetests-wrapper-in ${CMAKE_SOURCE_DIR}/cmake_templates/run-performancetests.unix.in) set(performancetests-wrapper-out ${CMAKE_BINARY_DIR}/run-performancetests) configure_file( ${performancetests-wrapper-in} ${performancetests-wrapper-out} @ONLY ) endif() ################################################################################ # Add libraries / executables ################################################################################ add_subdirectory(libjupedsim) add_subdirectory(libcommon) add_subdirectory(libsimulator) add_subdirectory(python_bindings_jupedsim) ################################################################################ # Code formatting ################################################################################ if(UNIX AND WITH_FORMAT) set(clang-format-version 19) find_program(CLANG_FORMAT NAMES clang-format-${clang-format-version} clang-format REQUIRED ) if(CLANG_FORMAT) execute_process( COMMAND ${CLANG_FORMAT} --version OUTPUT_VARIABLE version_string ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE ) if(version_string MATCHES \"^.*clang-format version .*\") string(REGEX REPLACE \"^.*clang-format version ([.0-9]+).*\" \"\\\\1\" version \"${version_string}\" ) if(version MATCHES \"^${clang-format-version}.*\") message(STATUS \"Found clang-format ${version}, add format-check and reformat targets\") set(folders libcommon libjupedsim libsimulator) add_custom_target(check-format COMMENT \"Checking format with clang-format\" COMMAND find ${folders} -name '*.cpp' -o -name '*.c' -o -name '*.h' -o -name '*.hpp' | xargs ${CLANG_FORMAT} -n -Werror --style=file WORKING_DIRECTORY ${CMAKE_SOURCE_DIR} ) add_custom_target(reformat COMMENT \"Reformatting code with clang-format\" COMMAND find ${folders} -name '*.cpp' -o -name '*.c' -o -name '*.h' -o -name '*.hpp' | xargs ${CLANG_FORMAT} -i --style=file WORKING_DIRECTORY ${CMAKE_SOURCE_DIR} ) else () message(FATAL_ERROR \"Could not create formatting target, clang-format version ${version} found, but ${clang-format-version} required\") endif() else () message(FATAL_ERROR \"Could not create formatting target, clang-format ${version_string} does not yield a version number\") endif() endif() endif() ################################################################################ # Integration tests ################################################################################ if (BUILD_TESTS) # Find libraries needed by python tests find_python_library(pytest) endif (BUILD_TESTS)\n[tool.ruff] extend-exclude = [\"third-party\", \"docs\"] line-length = 80 indent-width = 4 target-version = \"py312\" extend-include = [\"*.ipynb\"] [tool.ruff.lint] # Enable Pyflakes (`F`) and a subset of the pycodestyle (`E`) codes by default. # Unlike Flake8, Ruff doesn't enable pycodestyle warnings (`W`) or # McCabe complexity (`C901`) by default. # select = [\"E4\", \"E7\", \"E9\", \"F\"] select = [ \"E\", # pycodestyle \"F\", # Pyflakes \"I\", # isort \"N\", # Naming conventions for variables, functions, and attributes ] ignore = [\"E501\"] # Allow fix for all enabled rules (when `--fix`) is provided. fixable = [\"ALL\"] unfixable = [] # Allow unused variables when underscore-prefixed. dummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\" [tool.ruff.format] # Like Black, use double quotes for strings. quote-style = \"double\" # Like Black, indent with spaces, rather than tabs. indent-style = \"space\" # Like Black, respect magic trailing commas. skip-magic-trailing-comma = false # Like Black, automatically detect the appropriate line ending. line-ending = \"auto\" # Enable auto-formatting of code examples in docstrings. Markdown, # reStructuredText code/literal blocks and doctests are all supported. # # This is currently disabled by default, but it is planned for this # to be opt-out in the future. docstring-code-format = false # Set the line length limit used when formatting code snippets in # docstrings. # # This only has an effect when the `docstring-code-format` setting is # enabled. docstring-code-line-length = \"dynamic\" # Ignore naming issues for camelCase deprecated properties [tool.ruff.lint.pep8-naming] extend-ignore-names = [\"desiredSpeed\", \"reactionTime\", \"agentScale\", \"obstacleScale\", \"forceDistance\", \"bodyForce\", \"IncorrectTrajectory\", \"IncorrectTrajectoryError\"] [tool.ruff.lint.per-file-ignores] \"test_perftests.py\" = [\"N\"] \"**{jupedsim_visualizer}/*\" = [\"N\"] \"serialization.py\" = [\"N\"] \"journey.ipynb\" = [\"N\"]\nnumpy~=2.2 shapely~=2.0 pyside6~=6.8 vtk~=9.4 deprecated~=1.2.18 # build deps wheel build setuptools # test deps pytest~=8.3 pandas~=2.2 # ci deps jinja2 ruff\n# SPDX-License-Identifier: LGPL-3.0-or-later import glob import os import pathlib import re import shutil import subprocess import sys import tempfile import textwrap from pathlib import Path from setuptools import Extension, setup from setuptools.command.build_ext import build_ext min_cpp_standard = 20 min_cmake_version = \"3.19\" # Read version number from CMakeLists.txt with open(\"CMakeLists.txt\", \"r\", encoding=\"utf-8\") as cmakelist: cmake_input = cmakelist.read() version_line = re.findall(r\"project\\(JuPedSim.*\", cmake_input)[0] start_index = version_line.rfind(\"VERSION\") + len(\"VERSION\") end_index = version_line.find(\"LANGUAGES\") version = version_line[start_index:end_index].strip() with open(\"pypi-readme.md\", \"r\", encoding=\"utf-8\") as fh: long_description = fh.read() # Convert distutils Windows platform specifiers to CMake -A arguments PLAT_TO_CMAKE = { \"win32\": \"Win32\", \"win-amd64\": \"x64\", \"win-arm32\": \"ARM\", \"win-arm64\": \"ARM64\", } def check_cmake(): try: result = subprocess.run( [\"cmake\", \"--version\"], check=True, capture_output=True, text=True ) # Check CMake version found_cmake_version = re.search( r\"(\\d+).(\\d+).(\\d+)\", str(result.stdout) ) for min_version, found_version in zip( min_cmake_version.split(\".\"), found_cmake_version.groups() ): if found_version < min_version: return False except Exception as _: return False return True def check_cpp_compiler(): with tempfile.TemporaryDirectory() as tmp_dir: simple_main = textwrap.dedent( \"\"\" int main(){ return 0; } \"\"\" ) cpp_file = tmp_dir + \"/main.cpp\" with open(cpp_file, \"w\") as cp_file: cp_file.write(simple_main) simple_cmake = textwrap.dedent( f\"\"\" cmake_minimum_required(VERSION 3.19) project(SimpleTest) set(CMAKE_CXX_STANDARD {min_cpp_standard}) set(CMAKE_CXX_STANDARD_REQUIRED ON) add_executable(simple_test main.cpp) \"\"\" ) cmake_file = tmp_dir + \"/CMakeLists.txt\" with open(cmake_file, \"w\") as cm_file: cm_file.write(simple_cmake) tmp_dir_build = pathlib.Path(tmp_dir) / \"build\" tmp_dir_build.mkdir() try: subprocess.run( [\"cmake\", \"-S\", str(tmp_dir), \"-B\", str(tmp_dir_build)], check=True, ) except Exception as _: return False return True # A CMakeExtension needs a sourcedir instead of a file list. # The name must be the _single_ output extension from the CMake build. # If you need multiple extensions, see scikit-build. class CMakeExtension(Extension): def __init__(self, name: str, sourcedir: str = \"\") -> None: super().__init__(name, sources=[]) self.sourcedir = os.fspath(Path(sourcedir).resolve()) class CMakeBuild(build_ext): def build_extension(self, ext: CMakeExtension) -> None: if not check_cmake(): raise ModuleNotFoundError( f\"No CMake or no CMake >= {min_cmake_version} installation \" f\"found on the system, please install \" f\"CMake >= {min_cmake_version} to install JuPedSim.\" ) if not check_cpp_compiler(): raise ModuleNotFoundError( \"Could not compile a simple C++ program, \" f\"please install a C++ compiler with C++{min_cpp_standard} \" f\"support to install JuPedSim.\" ) # Must be in this form due to bug in .resolve() only fixed in Python 3.10+ ext_fullpath = Path.cwd() / self.get_ext_fullpath(ext.name) extdir = ext_fullpath.parent.resolve() # Using this requires trailing slash for auto-detection & inclusion of # auxiliary \"native\" libs debug = ( int(os.environ.get(\"DEBUG\", 0)) if self.debug is None else self.debug ) cfg = \"Debug\" if debug else \"Release\" # CMake lets you override the generator - we need to check this. # Can be set with Conda-Build, for example. cmake_generator = os.environ.get(\"CMAKE_GENERATOR\", \"\") # Set Python_EXECUTABLE instead if you use PYBIND11_FINDPYTHON # EXAMPLE_VERSION_INFO shows you how to pass a value into the C++ code # from Python. cmake_args = [ f\"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY={extdir}{os.sep}\", f\"-DCMAKE_BUILD_TYPE={cfg}\", # not used on MSVC, but no harm \"-DCMAKE_UNITY_BUILD=ON\", f\"-DPython_EXECUTABLE={sys.executable}\", ] # Pile all .so in one place and use $ORIGIN as RPATH cmake_args += [\"-DCMAKE_BUILD_WITH_INSTALL_RPATH=TRUE\"] cmake_args += [\"-DCMAKE_INSTALL_RPATH={}\".format(\"$ORIGIN\")] build_args = [] # Adding CMake arguments set as environment variable # (needed e.g. to build for ARM OSx on conda-forge) if \"CMAKE_ARGS\" in os.environ: cmake_args += [ item for item in os.environ[\"CMAKE_ARGS\"].split(\" \") if item ] if self.compiler.compiler_type != \"msvc\": # Using Ninja-build since it a) is available as a wheel and b) # multithreads automatically. MSVC would require all variables be # exported for Ninja to pick it up, which is a little tricky to do. # Users can override the generator with CMAKE_GENERATOR in CMake # 3.15+. if not cmake_generator or cmake_generator == \"Ninja\": try: import ninja ninja_executable_path = Path(ninja.BIN_DIR) / \"ninja\" cmake_args += [ \"-GNinja\", f\"-DCMAKE_MAKE_PROGRAM:FILEPATH={ninja_executable_path}\", ] except ImportError: pass else: # Single config generators are handled \"normally\" single_config = any( x in cmake_generator for x in {\"NMake\", \"Ninja\"} ) # CMake allows an arch-in-generator style for backward compatibility contains_arch = any(x in cmake_generator for x in {\"ARM\", \"Win64\"}) # Specify the arch if using MSVC generator, but only if it doesn't # contain a backward-compatibility arch spec already in the # generator name. if not single_config and not contains_arch: cmake_args += [\"-A\", PLAT_TO_CMAKE[self.plat_name]] # Multi-config generators have a different way to specify configs if not single_config: cmake_args += [ f\"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_{cfg.upper()}={extdir}\" ] build_args += [\"--config\", cfg] if sys.platform.startswith(\"darwin\"): # Cross-compile support for macOS - respect ARCHFLAGS if set archs = re.findall(r\"-arch (\\S+)\", os.environ.get(\"ARCHFLAGS\", \"\")) if archs: cmake_args += [ \"-DCMAKE_OSX_ARCHITECTURES={}\".format(\";\".join(archs)) ] # Set CMAKE_BUILD_PARALLEL_LEVEL to control the parallel build level # across all generators. if \"CMAKE_BUILD_PARALLEL_LEVEL\" not in os.environ: # self.parallel is a Python 3 only way to set parallel jobs by hand # using -j in the build_ext call, not supported by pip or PyPA-build. if hasattr(self, \"parallel\") and self.parallel: # CMake 3.12+ only. build_args += [f\"-j{self.parallel}\"] build_temp = Path(self.build_temp) / ext.name if not build_temp.exists(): build_temp.mkdir(parents=True) build_args += [\"-j\"] subprocess.run( [\"cmake\", ext.sourcedir, *cmake_args], cwd=build_temp, check=True ) subprocess.run( [\"cmake\", \"--build\", \".\", *build_args], cwd=build_temp, check=True ) # Copy library files to root build folder files = glob.glob(str(build_temp) + \"/lib/py_jupedsim*.so\") files.extend(glob.glob(str(build_temp) + \"/lib/py_jupedsim*.dylib\")) files.extend(glob.glob(str(build_temp) + \"/lib/py_jupedsim*.pyd\")) for lib_file in files: shutil.copy(dst=extdir / \"jupedsim\", src=lib_file) # The information here can also be placed in setup.cfg - better separation of # logic and declaration, and simpler if you include description/version in a file. setup( name=\"jupedsim\", version=version, maintainer=\"JuPedSim Development Core Team\", maintainer_email=\"dev@jupedsim.org\", description=\"JuPedSim is an open source pedestrian dynamics simulator\", long_description=long_description, long_description_content_type=\"text/markdown\", license_files=(\"LICENSE\",), ext_modules=[CMakeExtension(\"python_bindings_jupedsim\")], cmdclass={\"build_ext\": CMakeBuild}, zip_safe=False, python_requires=\">=3.10,<3.14\", packages=[ \"jupedsim\", \"jupedsim.models\", \"jupedsim.internal\", \"jupedsim.native\", \"jupedsim_visualizer\", ], package_dir={ \"jupedsim\": \"python_modules/jupedsim/jupedsim\", \"jupedsim.models\": \"python_modules/jupedsim/jupedsim/models\", \"jupedsim.internal\": \"python_modules/jupedsim/jupedsim/internal\", \"jupedsim.native\": \"python_modules/jupedsim/jupedsim/native\", \"jupedsim_visualizer\": \"python_modules/jupedsim_visualizer/jupedsim_visualizer\", }, install_requires=[ \"numpy~=2.2\", \"shapely~=2.0\", \"pyside6~=6.8\", \"vtk~=9.4\", \"deprecated~=1.2.18\", ], scripts=[\"python_modules/jupedsim_visualizer/bin/jupedsim-visualizer\"], url=\"https://www.jupedsim.org\", project_urls={ \"Documentation\": \"https://www.jupedsim.org\", \"Source\": \"https://github.com/PedestrianDynamics/jupedsim\", \"Tracker\": \"https://github.com/PedestrianDynamics/jupedsim/issues\", }, classifiers=[ \"Development Status :: 4 - Beta\", \"License :: OSI Approved :: GNU Lesser General Public License v3 or later (LGPLv3+)\", \"Operating System :: Microsoft :: Windows\", \"Operating System :: Unix\", \"Operating System :: MacOS\", \"Natural Language :: English\", \"Programming Language :: C++\", \"Programming Language :: C\", \"Programming Language :: Python :: 3 :: Only\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Programming Language :: Python :: 3.13\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", ], )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jupyterhub-outpost",
            "repo_link": "https://github.com/kreuzert/jupyterhub-outpost",
            "content": {
                "codemeta": "",
                "readme": "[![Documentation Status](https://readthedocs.org/projects/jupyterhub-outpost/badge/?version=latest)](https://jupyterhub-outpost.readthedocs.io/en/latest/?badge=latest) [![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/jupyterhub-outpost)](https://artifacthub.io/packages/search?repo=jupyterhub-outpost) # JupyterHub Outpost JupyterHub Outpost can be used as an additional, external source to start and manage single-user servers. Like in JupyterHub itself, different Spawners can be configured at the Outpost. It's best used together with the [jupyterhub-outpostspawner](https://pypi.org/project/jupyterhub-outpostspawner/) configured at JupyterHub. ## Documentation Link to [documentation](https://jupyterhub-outpost.readthedocs.io). ## Overview The JupyterHub community created many useful [JupyterHub Spawner](https://jupyterhub.readthedocs.io/en/latest/reference/spawners.html#examples) over the past years, to allow JupyterHub to use the specific resources of different systems. For most of these Spawners JupyterHub has to run at the system itself. The JupyterHub Outpost service allows the use of these Spawners on remote systems, if JupyterHub uses the [OutpostSpawner](https://github.com/kreuzert/jupyterhub-outpostspawner/).. Other Spawners like [SSHSpawner](https://github.com/NERSC/sshspawner) can spawn single-user servers on remote systems, but are not able to use system-specific features like [KubeSpawner](https://github.com/jupyterhub/kubespawner) or [BatchSpawner](https://github.com/jupyterhub/batchspawner). The JupyterHub Outpost service in combination with the OutpostSpawner enables a single JupyterHub to offer multiple remote systems of different types. - Use one JupyterHub to offer single-user servers on multiple systems. - Each system may use a different JupyterHub Spawner. - Integrated SSH port forwarding solution to reach remote single-user server. - supports the JupyterHub `internal_ssl` feature. - shows events gathered by the remote Spawner to the user. - Users can override the configuration of the remote Spawner at runtime (e.g. to select a different Docker Image). - One JupyterHub Outpost can be connected to multiple JupyterHubs, without interfering with each other. ## Requirements JupyterHub must run on a Kubernetes Cluster (recommended is the use of Zero2JupyterHub). The JupyterHub Outpost must fulfill the requirements of the configured Spawner class.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jupyterhub-outpostspawner",
            "repo_link": "https://github.com/kreuzert/jupyterhub-outpostspawner",
            "content": {
                "codemeta": "",
                "readme": "[![Documentation Status](https://readthedocs.org/projects/jupyterhub-outpostspawner/badge/?version=latest)](https://jupyterhub-outpostspawner.readthedocs.io/en/latest/?badge=latest) # OutpostSpawner The OutpostSpawner in combination with the [JupyterHub Outpost service](https://github.com/kreuzert/jupyterhub-outpost) enables JupyterHub to spawn single-user notebook servers on multiple remote resources. ## Documentation Link to [documentation](https://jupyterhub-outpostspawner.readthedocs.io). ## Overview The JupyterHub community created many useful [JupyterHub Spawner](https://jupyterhub.readthedocs.io/en/latest/reference/spawners.html#examples) over the past years, to allow JupyterHub to use the specific resources of different systems. For most of these Spawners JupyterHub has to run at the system itself. The OutpostSpawner enables the use of these Spawners on remote systems. Other Spawners like [SSHSpawner](https://github.com/NERSC/sshspawner) can spawn single-user servers on remote systems, but are not able to use system-specific features like [KubeSpawner](https://github.com/jupyterhub/kubespawner) or [BatchSpawner](https://github.com/jupyterhub/batchspawner). The JupyterHub Outpost service in combination with the OutpostSpawner enables a single JupyterHub to offer multiple remote systems of different types. - Use one JupyterHub to offer single-user servers on multiple systems. - Each system may use a different JupyterHub Spawner. - Integrated SSH port forwarding solution to reach remote single-user server. - supports the JupyterHub `internal_ssl` feature. - shows events gathered by the remote Spawner to the user. - Users can override the configuration of the remote Spawner at runtime (e.g. to select a different Docker Image). - One JupyterHub Outpost can be connected to multiple JupyterHubs, without interfering with each other. ## Requirements JupyterHub must run on a Kubernetes Cluster (recommended is the use of Zero2JupyterHub). The JupyterHub Outpost must fulfill the requirements of the configured Spawner class.\n",
                "dependencies": "[build-system] requires = [\"hatchling\"] build-backend = \"hatchling.build\" [tool.hatch.build.targets.wheel] packages = [\"outpostspawner\"] include = [\"*.py\"] exclude = [\"test*\"] [project] name = \"jupyterhub-outpostspawner\" description = \"JupyterHub OutpostSpawner enables start on multiple, remote system\" readme = \"README.md\" requires-python = \">=3.9\" license = {file = \"LICENSE\"} keywords = [\"jupyterhub\", \"spawner\", \"kubernetes\"] authors = [ {name = \"Tim Kreuzer\", email = \"t.kreuzer@fz-juelich.de\"}, {name = \"Alice Grosch\", email = \"a.grosch@fz-juelich.de\"} ] dependencies = [ \"escapism\", \"jinja2\", \"jupyterhub>=4.0.0\", \"traitlets\", \"urllib3\", \"jupyterhub-forwardbasespawner\", \"kubernetes\" ] dynamic = [\"version\"] [tool.black] target_version = [ \"py39\", \"py310\", \"py311\", \"py312\", ] [tool.hatch.version] path = \"outpostspawner/_version.py\" [tool.isort] profile = \"black\" [tool.tbump] # Uncomment this if your project is hosted on GitHub: github_url = \"https://github.com/kreuzert/jupyterhub-outpostspawner\" [tool.tbump.version] current = \"1.1.2\" regex = ''' (?P<major>\\d+) \\. (?P<minor>\\d+) \\. (?P<patch>\\d+) (?P<pre>((a|b|rc)\\d+)|) \\.? (?P<post>((post)\\d+)|) (?P<dev>(?<=\\.)dev\\d*|) ''' [tool.tbump.git] message_template = \"Bump to {new_version}\" tag_template = \"{new_version}\" # For each file to patch, add a [[tool.tbump.file]] config # section containing the path of the file, relative to the # pyproject.toml location. #[[tool.tbump.file]] #src = \"pyproject.toml\" #search = 'version = \"{current_version}\"' [[tool.tbump.file]] src = \"outpostspawner/_version.py\" #version_template = '({major}, {minor}, {patch}, \"{pre}\", \"{dev}\")' #search = \"version_info = {current_version}\" #[[tool.tbump.file]] #src = \"docs/source/_static/rest-api.yml\" #search = \"version: {current_version}\" [tool.djlint] indent = 2 profile = \"jinja\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/jurassic",
            "repo_link": "https://github.com/slcs-jsc/jurassic",
            "content": {
                "codemeta": "",
                "readme": "# Juelich Rapid Spectral Simulation Code The Juelich Rapid Spectral Simulation Code (JURASSIC) is a fast infrared radiative transfer model for the analysis of atmospheric remote sensing measurements. ![logo](https://github.com/slcs-jsc/jurassic/blob/master/docs/logo/JURASSIC_320px.png) [![release (latest by date)](https://img.shields.io/github/v/release/slcs-jsc/jurassic)](https://github.com/slcs-jsc/jurassic/releases) [![commits since latest release (by SemVer)](https://img.shields.io/github/commits-since/slcs-jsc/jurassic/latest)](https://github.com/slcs-jsc/jurassic/commits/master) [![last commit](https://img.shields.io/github/last-commit/slcs-jsc/jurassic.svg)](https://github.com/slcs-jsc/jurassic/commits/master) [![top language](https://img.shields.io/github/languages/top/slcs-jsc/jurassic.svg)](https://github.com/slcs-jsc/jurassic/tree/master/src) [![code size](https://img.shields.io/github/languages/code-size/slcs-jsc/jurassic.svg)](https://github.com/slcs-jsc/jurassic/tree/master/src) [![repo size](https://img.shields.io/github/repo-size/slcs-jsc/jurassic.svg)](https://github.com/slcs-jsc/jurassic/tree/master/src) [![codacy](https://api.codacy.com/project/badge/Grade/aaba414eaf9e4e6784f13458a285ec2f)](https://app.codacy.com/gh/slcs-jsc/jurassic?utm_source=github.com&utm_medium=referral&utm_content=slcs-jsc/jurassic&utm_campaign=Badge_Grade_Settings) [![codecov](https://codecov.io/gh/slcs-jsc/jurassic/branch/master/graph/badge.svg?token=TYGWEJMOLI)](https://codecov.io/gh/slcs-jsc/jurassic) [![tests](https://img.shields.io/github/actions/workflow/status/slcs-jsc/jurassic/tests.yml?branch=master&label=tests)](https://github.com/slcs-jsc/jurassic/actions) [![docs](https://img.shields.io/github/actions/workflow/status/slcs-jsc/jurassic/docs.yml?branch=master&label=docs)](https://slcs-jsc.github.io/jurassic) [![license](https://img.shields.io/github/license/slcs-jsc/jurassic.svg)](https://github.com/slcs-jsc/jurassic/blob/master/COPYING) [![doi](https://zenodo.org/badge/DOI/10.5281/zenodo.4572889.svg)](https://doi.org/10.5281/zenodo.4572889) ## Features * JURASSIC uses the emissivity growth approximation (EGA) or the Curtis-Godson approximation (CGA) to conduct infrared radiative transfer calculations. Band transmittances are obtained from pre-calculated look-up tables from line-by-line calculations. * The model was carefully tested in intercomparisons with the Karlsruhe Optimized and Precise Radiative Transfer Algorithm (KOPRA), the Reference Forward Model (RFM), and the Stand-alone AIRS Radiative Transfer Algorithm (SARTA). * JURASSIC features an MPI-OpenMP hybrid parallelization for efficient use on HPC systems. * Distributed open source under the terms and conditions of the GNU GPL. ## Getting started ### Prerequisites This documentation describes the installation of JURASSIC on a Linux system. A number of standard tools (gcc, git, make) and software libraries are needed to install JURASSIC. The [GNU Scientific Library](https://www.gnu.org/software/gsl) is required for numerical calculations. A copy of this library can be found in the git repository. Start by downloading the source code from the git repository: git clone https://github.com/slcs-jsc/jurassic.git To update an existing installation use: git pull https://github.com/slcs-jsc/jurassic.git ### Installation First, compile the GSL library needed for JURASSIC by using the build script: cd [jurassic_directory]/libs ./build.sh Next, change to the source directory, edit the Makefile according to your needs, and try to compile the code: cd [jurassic_directory]/src emacs Makefile make The binaries will be linked statically, i.e., they can be copied and run on other machines. Sometimes this causes problems. In this case remove the '-static' flag from the CFLAGS in the Makefile and compile again. By default we use rather strict compiler warnings. All warning messages will be turned into errors and no binaries will be produced. This behavior is enforced by the flag '-Werror'. The binaries will remain in the jurassic/src/ directory. To run the test cases to check the installation, please use: make check This will run sequentially through a set of tests. The execution of the tests will stop if any of the tests fails. Please inspect the log messages. ### Run the examples JURASSIC provides a project directory for testing the examples and also to store other experiments: cd [jurassic_directory]/projects This shows how to run the example for the nadir sounder: cd nadir ./run.sh This shows how to run the example for the limb sounder: cd ../limb ./run.sh In both examples, we generate an observation geometry file, cat obs.tab a standard atmosphere for mid-latitudes, cat atm.tab and conduct radiative transfer calculations for two or three detector channels: cat rad.tab The output of the simulation is verified by comparing it to reference data. Additionally, gnuplot is used to create plots of the radiance data: <p align=\"center\"> <img src=\"projects/limb/plot_rad.png\" alt=\"limb radiance data\" width=\"45%\"/> &emsp; <img src=\"projects/nadir/plot_rad.png\" alt=\"nadir radiance data\" width=\"45%\"/> </p> Kernel functions are calculated using a finite difference method: <p align=\"center\"> <img src=\"projects/limb/plot_kernel_temperature_792.png\" alt=\"limb temperature kernel function\" width=\"45%\"/> &emsp; <img src=\"projects/nadir/plot_kernel_temperature_668.5410.png\" alt=\"nadir temperature kernel function\" width=\"45%\"/> </p> <p align=\"center\"> <img src=\"projects/limb/plot_kernel_H2O_792.png\" alt=\"limb water vapor kernel function\" width=\"45%\"/> &emsp; <img src=\"projects/nadir/plot_kernel_CO2_668.5410.png\" alt=\"nadir water vapor kernel function\" width=\"45%\"/> </p> ## Further information More detailed information for new users and developers of JURASSIC is collected in the [GitHub wiki](https://github.com/slcs-jsc/jurassic/wiki). These are the main references for citing the JURASSIC model in scientific publications: * Baumeister, P. F. and Hoffmann, L.: Fast infrared radiative transfer calculations using graphics processing units: JURASSIC-GPU v2.0, Geosci. Model Dev., 15, 1855-1874, <https://doi.org/10.5194/gmd-15-1855-2022>, 2022. * Hoffmann, L., and M. J. Alexander, Retrieval of stratospheric temperatures from Atmospheric Infrared Sounder radiance measurements for gravity wave studies, J. Geophys. Res., 114, D07105, <https://doi.org/10.1029/2008JD011241>, 2009. * Hoffmann, L., Kaufmann, M., Spang, R., Müller, R., Remedios, J. J., Moore, D. P., Volk, C. M., von Clarmann, T., and Riese, M.: Envisat MIPAS measurements of CFC-11: retrieval, validation, and climatology, Atmos. Chem. Phys., 8, 3671-3688, <https://doi.org/10.5194/acp-8-3671-2008>, 2008. * You can cite the source code of JURASSIC by using the DOI <https://doi.org/10.5281/zenodo.4572889>. This DOI represents all versions, and will always resolve to the latest one. Specific DOIs for each release of JURASSIC can be found on the zenodo web site. Please see the [citation file](https://github.com/slcs-jsc/jurassic/blob/master/CITATION.cff) for further information. ## Contributing We are interested in sharing JURASSIC for operational or research applications. Please do not hesitate to contact us, if you have any further questions or need support. ## License JURASSIC is distributed under the [GNU General Public License v3.0](https://github.com/slcs-jsc/jurassic/blob/master/COPYING). ## Contact Dr. Lars Hoffmann Jülich Supercomputing Centre, Forschungszentrum Jülich e-mail: <l.hoffmann@fz-juelich.de>\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/juri",
            "repo_link": "https://github.com/FZJ-JSC/JURI",
            "content": {
                "codemeta": "",
                "readme": "# JURI - Jülich Reporting Interface [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10232352.svg)](https://doi.org/10.5281/zenodo.10232352) JURI provides a template driven fully client based web framework to visualize data lists and associated data graphs. JURI is currently used for the [LLview Job Reporting](https://github.com/FZJ-JSC/LLview) and Kontview @JSC. ## Installation Installation instructions can be currently found on [LLview's documentation page](https://apps.fz-juelich.de/jsc/llview/docu/install/). ## Further Information For further information please see: https://www.fz-juelich.de/jsc/llview Contact: [llview.jsc@fz-juelich.de](mailto:llview.jsc@fz-juelich.de) ## Copyright, License and CLA Copyright (c) 2023 Forschungszentrum Juelich GmbH, Juelich Supercomputing Centre https://www.fz-juelich.de/jsc/llview This is an open source software distributed under the GPLv3 license. More information see the LICENSE file at the top level. Contributions must follow the Contributor License Agreement. More information see the CONTRIBUTING.md file at the top level.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kaapana",
            "repo_link": "https://github.com/kaapana/kaapana",
            "content": {
                "codemeta": "",
                "readme": "<p align=\"center\"> <img src=\"https://www.kaapana.ai/kaapana-downloads/kaapana-docs/stable/img/kaapana_logo_2.png\" height=170 alt=\"kaapana\" border=\"0\" /> </p> [![Documentation Status](https://readthedocs.org/projects/kaapana/badge/?version=latest)](https://kaapana.readthedocs.io/en/latest/?badge=latest) <a href=\"https://join.slack.com/t/kaapana/shared_invite/zt-hilvek0w-ucabihas~jn9PDAM0O3gVQ/\"><img src=\"https://img.shields.io/badge/chat-slack-blueviolet\" /></a> ## What is Kaapana? <p> <a href=\"https://www.kaapana.ai/kaapana-downloads/kaapana-docs/stable/gif/kaapana-v0.2.1-showcase.mp4\" target=\"_blank\"> <img src=\"https://www.kaapana.ai/kaapana-downloads/kaapana-docs/stable/img/thumbnail_kaapana_vid.png\" /> </a> </p> Kaapana is an open-source toolkit for state-of-the-art platform provisioning in the field of medical data analysis. The applications comprise AI-based workflows and federated learning scenarios with a focus on radiological and radiotherapeutic imaging. The name Kaapana comes from the Hawaiian word kaʻāpana, meaning \"distributor\" or \"part\". Obtaining large amounts of medical data necessary for developing and training modern machine learning methods is an extremely challenging effort that often fails in a multi-center setting, e.g. due to technical, organizational and legal hurdles. A federated approach where the data remains under the authority of the individual institutions and is only processed on-site is, in contrast, a promising approach ideally suited to overcome these difficulties. Following this federated concept, the goal of Kaapana is to provide a framework and a set of tools for sharing data processing algorithms, for standardized workflow design and execution as well as for performing distributed method development. This will facilitate data analysis in a compliant way enabling researchers and clinicians to perform large-scale multi-center studies. By adhering to established standards and by adopting widely used open technologies for private cloud development and containerized data processing, Kaapana integrates seamlessly with the existing clinical IT infrastructure, such as the Picture Archiving and Communication System (PACS), and ensures modularity and easy extensibility. Core components of Kaapana: * **Workflow management:** Large-scale image processing with SOTA deep learning algorithms, such as [nnU-Net](https://github.com/MIC-DKFZ/nnunet) image segmentation and [TotalSegmentator](https://github.com/wasserth/TotalSegmentator) * **Datasets:** Exploration, visualization and curation of medical images * **Extensions:** Simple integration of new, customized algorithms and applications into the framework * **Storage:** An integrated PACS system and Minio for other types of data * **System monitoring:** Extensive resource and system monitoring for administrators * **User management** Simple user management via [Keycloak](https://www.keycloak.org/) Core technologies used in Kaapana: * [Kubernetes](https://kubernetes.io/): Container orchestration system * [Helm](https://helm.sh/): The package manager for Kubernetes * [Airflow](https://airflow.apache.org/): Workflow management system enabling complex and flexible data processing workflows * [OpenSearch](https://opensearch.org/): Search engine for DICOM metadata-based searches * [dcm4chee](https://www.dcm4che.org/): Open source PACS system serving as a central DICOM data storage * [Prometheus](https://github.com/prometheus/prometheus): Collecting metrics for system monitoring * [Grafana](https://github.com/grafana/grafana): Visualization for monitoring metrics * [Keycloak](https://www.keycloak.org/): User authentication * [OHIF Viewers](https://ohif.org/): Visualization of medical images Currently, Kaapana is used in multiple projects in which a Kaapana-based platform is deployed at multiple clinical sites with the objective of distributed radiological image analysis and quantification. The projects include [RACOON](https://racoon.network/) initiated by [NUM](https://www.netzwerk-universitaetsmedizin.de) with all 38 German university clinics participating, the Joint Imaging Platform ([JIP](https://jip.dktk.dkfz.de/jiphomepage/)) initiated by the German Cancer Consortium ([DKTK](https://dktk.dkfz.de/)) with 11 university clinics participating as well as [DART](https://cce-dart.com) initiated by the [Cancer Core Europe](https://cancercoreeurope.eu/) with 7 cancer research centers participating. For more information, please also take a look at our publication of the Kaapana-based [Joint Imaging Platform in JCO Clinical Cancer Informatics](https://ascopubs.org/doi/full/10.1200/CCI.20.00045). ## Documentation Check out the [documentation](https://kaapana.readthedocs.io/en/latest/) for further information about how Kaapana works, for instructions on how to build, deploy, use and further develop the platform. ## Where to find us * [GitLab](https://gitlab.hzdr.de/kaapana/kaapana/): The main Kaapana repository, mirrored on GitHub. * [Slack](https://join.slack.com/t/kaapana/shared_invite/zt-hilvek0w-ucabihas~jn9PDAM0O3gVQ/): Join the community for discussions and updates. * [YouTube](https://www.youtube.com/@KaapanaAI): Tutorials, demos and more in-depth presentations. * [Website](https://kaapana.ai/) ## Versioning As of Kaapana 0.2.0 we follow strict [SemVer](https://semver.org/) approach to versioning. ## Citations Please [cite](https://ascopubs.org/action/showCitFormats?doi=10.1200/CCI.20.00045) the [following paper](https://ascopubs.org/doi/full/10.1200/CCI.20.00045) when using Kaapana: Jonas Scherer, Marco Nolden, Jens Kleesiek, Jasmin Metzger, Klaus Kades, Verena Schneider, Michael Bach, Oliver Sedlaczek, Andreas M. Bucher, Thomas J. Vogl, ...Klaus Maier-Hein. Joint Imaging Platform for Federated Clinical Data Analytics. JCO Clinical Cancer Informatics, 4:10271038, November 2020. doi: 10.1200/CCI.20.00045. URL https://ascopubs.org/doi/full/10.1200/CCI.20.00045. When using Kapaana for federated learning please also [cite](https://link.springer.com/chapter/10.1007/978-3-031-18523-6_13#citeas) the [following paper](https://link.springer.com/book/10.1007/978-3-031-18523-6): Klaus Kades, Jonas Scherer, Maximilian Zenk, Marius Kempf, and Klaus MaierHein. Towards Real-World Federated Learning in Medical Image Analysis Using Kaapana. In Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health, pages 130140, Cham, 2022b. Springer Nature Switzerland. ISBN 978-3-031-18523-6. doi: 10.1007/978-3-031-18523-6_13. URL https://doi.org/10.1007/978-3-031-18523-6_13. ## Licence This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details. You should have received a copy of the GNU Affero General Public License along with this program (see file LICENCE). If not, see <https://www.gnu.org/licenses/>. ## Considerations on our license choice You can use Kaapana to build any product you like, including commercial closed-source ones since it is a highly modular system. Kaapana is licensed under the [GNU Affero General Public License](https://www.gnu.org/licenses/agpl-3.0.en.html) for now since we want to ensure that we can integrate all developments and contributions to its core system for maximum benefit to the community and give everything back. We consider switching to a more liberal license in the future. This decision will depend on how our project develops and what the feedback from the community is regarding the license. Kaapana is built upon the great work of many other open-source projects, see the documentation for details. For now, we only release source code we created ourselves since providing pre-built docker containers and licensing for highly modular container-based systems is [a complex task](https://www.linuxfoundation.org/blog/2020/04/docker-containers-what-are-the-open-source-licensing-considerations/). We have done our very best to fulfill all requirements, and the choice of AGPL was motivated mainly to make sure we can improve and advance Kaapana in the best way for the whole community. If you have thoughts about this or if you disagree with our way of using a particular third-party toolkit or miss something please let us know and get in touch. We are open to any feedback and advice on this challenging topic. ## Contribution guide If you want to contribute to Kaapana by submitting an issue or code please have a look at our [contribution guide](https://codebase.helmholtz.cloud/kaapana/kaapana/-/blob/develop/CONTRIBUTION.md). ## Acknowledgments ### Supporting projects **Building Data Rich Clinical Trials - CCE_DART**: This project has received funding from the European Union's Horizon 2020 research and innovation program under grant agreement No 965397. Website: <https://cce-dart.com/> **Capturing Tumor Heterogeneity in Hepatocellular Carcinoma - A Radiomics Approach Systematically Tested in Transgenic Mice** This project is partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - 410981386. Website: <https://gepris.dfg.de/gepris/projekt/410981386> **Data Science Driven Surgical Oncology Project**: This work was partially supported by the Data Science Driven Surgical Oncology Project (DSdSO), funded by the Surgical Oncology Program at the National Center for Tumor Diseases (NCT), Heidelberg, a partnership by DKFZ, UKHD, Heidelberg University. Website: <https://www.nct-heidelberg.de/forschung/precision-local-therapy-and-image-guidance/surgical-oncology.html> **Joint Imaging Platform**: This work was partially supported by Joint Imaging Platform, funded by the German Cancer Consortium. Website: <https://jip.dktk.dkfz.de/jiphomepage/> **HiGHmed**: This work was partially supported by the HiGHmed Consortium, funded by the German Federal Ministry of Education and Research (BMBF, funding code 01ZZ1802A). Website: <https://highmed.org/> **RACOON**: This work was partially supported by RACOON, funded by the German Federal Ministry of Education and ResearchDieses in the Netzwerk Universitätsmedizin (NUM; funding code 01KX2021). Website: <https://www.netzwerk-universitaetsmedizin.de/projekte/racoon> **Trustworthy Federated Data Analysis - TFDA**: This work is partially funded by the Helmholtz Association within the project \"Trustworthy Federated Data Analytics\" (TFDA) (funding number ZT-I-OO1 4). Website: <https://tfda.hmsp.center/> Copyright (C) 2025 German Cancer Research Center (DKFZ)\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kadi4mat",
            "repo_link": "https://gitlab.com/iam-cms/kadi",
            "content": {
                "codemeta": "",
                "readme": "# Kadi4Mat **Kadi4Mat**, or **Kadi** for short, is a generic and open source virtual research environment. Originally developed in the context of materials science, Kadi4Mat can be used for the management of any type of research data within different research disciplines and use cases. For more information about the project, please see its [website](https://kadi.iam.kit.edu) and [documentation](https://kadi.readthedocs.io/en/stable). ## Installation While the packaged code of Kadi4Mat can easily be installed as a Python package via [pip](https://pypi.org/project/kadi), a complete installation requires a few additional dependencies and considerations. Please refer to the [stable documentation](https://kadi.readthedocs.io/en/stable) for full installation instructions. ## Development Contributions to the code are always welcome. However, please consider creating an issue first, as described below, if you are planning to make larger changes. Please refer to the [latest documentation](https://kadi.readthedocs.io/en/latest) for instructions on how to set up a development environment of Kadi4Mat as well as other useful information, such as how to set up a separate fork of the [main repository](https://gitlab.com/iam-cms/kadi). In order to merge any contributions back into the main repository, please open a corresponding [merge request](https://gitlab.com/iam-cms/kadi/-/merge_requests). Typically, the source branch of the merge request would be a separate (feature) branch of your forked repository containing the changes to merge, while the target branch should correspond to the `master` branch of the main repository. Depending on the changes, please make sure to add appropriate tests, documentation, translations, etc. and also add a corresponding entry to the changelog in [`HISTORY.md`](https://gitlab.com/iam-cms/kadi/-/blob/master/HISTORY.md), if necessary. Furthermore, you can add yourself as a contributor to [`AUTHORS.md`](https://gitlab.com/iam-cms/kadi/-/blob/master/AUTHORS.md). ## Issues For any issues regarding Kadi4Mat (bugs, suggestions, discussions, etc.) please use the [issue tracker](https://gitlab.com/iam-cms/kadi/-/issues) of this project. Make sure to add one or more fitting labels to each issue in order to keep them organized. Before creating a new issue, please also check whether a similar issue is already open. Note that creating or interacting with issues requires a GitLab account. For **bugs** in particular, please use the provided [`Bug` template](https://gitlab.com/iam-cms/kadi/-/issues/new?issuable_template=Bug) when creating a new issue, which also adds the `Bug` label to the issue automatically. For **security-related** issues or concerns, please see [`SECURITY.md`](https://gitlab.com/iam-cms/kadi/-/blob/master/SECURITY.md).\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kagen-communication-free-massively-distributed-graph-generators",
            "repo_link": "https://github.com/KarlsruheGraphGeneration/KaGen",
            "content": {
                "codemeta": "",
                "readme": "# Communication-free Graph Generators (+ others) This is the code to accompany our eponymous paper: *Funke, D., Lamm, S., Sanders, P., Schulz, C., Strash, D. and von Looz, M., 2017. Communication-free Massively Distributed Graph Generation. arXiv preprint arXiv:1710.07565.* You can find a freely accessible online version [in the arXiv](https://arxiv.org/abs/1710.07565). If you use this library in the context of an academic publication, we ask that you cite our paper: ```bibtex @inproceedings{funke2017communication, title={Communication-free Massively Distributed Graph Generation}, author={Funke, Daniel and Lamm, Sebastian and Sanders, Peter and Schulz, Christian and Strash, Darren and von Looz, Moritz}, booktitle={2018 {IEEE} International Parallel and Distributed Processing Symposium, {IPDPS} 2018, Vancouver, BC, Canada, May 21 -- May 25, 2018}, year={2018}, } ``` Additionally, if you use the Barabassi-Albert generator, we ask that you cite the [paper](https://arxiv.org/abs/1602.07106): ```bibtex @article{sanders2016generators, title={Scalable generation of scale-free graphs}, journal={Information Processing Letters}, volume={116}, number={7}, pages={489 -- 491}, year={2016}, author={Sanders, Peter and Schulz, Christian}, } ``` If you use the R-MAT generator, we ask that you cite the [paper](https://www.cambridge.org/core/journals/network-science/article/linear-work-generation-of-rmat-graphs/68A0DDA58A7B84E9B3ACA2DBB123A16C): ```bibtex @article{HubSan2020RMAT, title={Linear Work Generation of {R-MAT} Graphs}, volume={8}, number={4}, journal={Network Science}, publisher={Cambridge University Press}, author={H{\\\"u}bschle-Schneider, Lorenz and Sanders, Peter}, year={2020}, pages={543 -- 550}, } ``` ## Introduction Network generators serve as a tool to alleviate the need for synthethic instances with controllable parameters by algorithm developers and researchers. However, many generators fail to provide instances on a massive scale due to their sequential nature or resource constraints. In our work, we present novel generators for a variety of network models commonly found in practice. By making use of pseudorandomization and divide-and-conquer schemes, our generators follow a communication-free paradigm. The resulting generators are often embarrassingly parallel and have a near optimal scaling behavior. This allows us to generate instances of up to $2^{43}$ vertices and $2^{47}$ edges in less than 22 minutes on 32,768 cores. Therefore, our generators allow new graph families to be used on an unprecedented scale. ## Requirements In order to compile the generators, you require: * A modern, C++17-ready compiler such as `g++` version 9 or higher or `clang` version 11 or higher. * Note: Apple Clang is **not** supported. * OpenMPI * [Google Sparsehash](https://github.com/sparsehash/sparsehash) * CGAL (optional, only required for the Delaunay generators) You can install these dependencies via your package manager: ```shell # Ubuntu, Debian apt-get install gcc-12 g++-12 libopenmpi-dev libcgal-dev libsparsehash-dev # Arch Linux, Manjaro pacman -S gcc sparsehash openmpi cgal # Fedora dnf install gcc openmpi sparsehash-devel CGAL-devel # macOS using Homebrew brew install gcc open-mpi google-sparsehash cgal # macOS using MacPorts port install gcc12 openmpi sparsehash cgal5 ``` ## Building KaGen To compile the code either run `compile.sh` or use the following instructions: ```shell git submodule update --init --recursive cmake -B build -DCMAKE_BUILD_TYPE=Release cmake --build build --parallel ``` ## Running KaGen After building KaGen, the standalone application is located at `build/app/KaGen`. A list of all command line options is available using the `./KaGen --help` option. To view the options of a specific graph generator, use: ```shell ./KaGen <gnm-undirected|gnm-directed|gnp-undirected|gnp-directed|rgg2d|rgg3d|grid2d|grid3d|rdg2d|rdg3d|rhg|ba|kronecker|rmat> --help ``` By default, the generated graph is written to a single file `out` (`-o` option) in DIMACS edge list format (`-f` option). Other output formats include: - `-f edgelist`: DIMACS edge list format (default) - `-f binary-edgelist`: DIMACS binary edge list format, use `--32` to write the file with 32 bit data types - `-f metis`: Metis graph format - `-f hmetis`: hMetis hypergraph format; **note:** KaGen still generates a graph, i.e., every hyperedge will contain two pins - `-f dot`: GraphViz dot file (add `-C` to include vertex coordinates for 2D graph generators) - `-f coordinates`: Text file containing vertex coordinates - `-f parhip`: Binary graph format used by [ParHIP](https://github.com/KaHIP/KaHIP) - `-f xtrapulp`: Binary graph format used by [XtraPuLP](https://github.com/HPCGraphAnalysis/PuLP), use `--32` to write the file with 32 bit data types Experimental output formats include: - `-f experimental/hmetis-ep`: hMetis hypergraph format, but the graph is transformed s.t. a partition of the hypergraph is an edge partition of the generated graph - `-f experimental/freight-netl`: hypergraph format used by FREIGHT; **note:** KaGen still generates a graph, i.e., every hyperedge will contain two pins - `-f experimental/freight-netl-ep`: hypergraph format used by FREIGHT, but the graph is transformed s.t. a partition of the hypergraph is an edge partition of the generated graph One graph can be stored in multiple formats by passing the `-f` repeatedly, e.g., `-o out -f metis -f coordinates` will write two files `out.metis` and `out.xyz`. If you want each PE to write its edges to a seperate file, use the `--distributed-output` flag. ## Using the KaGen Library The KaGen library is located at `build/library/libkagen.a` (use `-DBUILD_SHARED_LIBS=On` to build a shared library instead) and can be used in C++ and C projects. If you are using CMake, you can use KaGen by adding this repository as a Git submodule to your project and including it in your CMake configuration: ```cmake add_subdirectory(external/KaGen) target_link_libraries(<your-target> PUBLIC KaGen::KaGen) ``` Alternatively, you can use `FetchContent`: ```cmake include(FetchContent) FetchContent_Declare(KaGen GIT_REPOSITORY https://github.com/sebalamm/KaGen.git GIT_TAG master) FetchContent_MakeAvailable(KaGen) set_property(DIRECTORY \"${KaGen_SOURCE_DIR}\" PROPERTY EXCLUDE_FROM_ALL YES) # optional target_link_libraries(<your-target> PUBLIC KaGen::KaGen) ``` Examples on how to use the C and C++ interfaces are available in the `examples/` directory. The examples given below only show the C++ interface. **Note**: Instead of calling the library functions listed below, you can also use `KaGen::GenerateFromOptionString()` to pass the generator options as a string (documentation is available in `kagen/kagen.h`). The library functions return the generated graph as an instance of type `kagen::Graph`. By default, the graph is represented as an edge list, i.e., a vector `kagen::Graph::edges[]` containing pairs of vertices. To generate a graph in compressed sparse row (CSR) format, call `kagen::KaGen::UseCSRRepresentation()` before generating the graph. Then, access the graph via `kagen::Graph::xadj[]` and `kagen::Graph::adjncy[]`. ## General Graph Format Unless noted otherwise, KaGen generates **simple**, **undirected** graphs, i.e., graphs without self-loops, without multi-edges and where for every edge (u, v), there is also a reverse edge (v, u). When using KaGen in a distributed setting, each PE owns an equally sized range of consecutive vertices. An edge is owned by the PE that owns its tail vertex. Thus, an edge (u, v) is in the edge list of the PE that owns vertex u, while the reverse edge (v, u) is in the edge list of the PE owning vertex v. ## Communication-free Graph Generators ### Erdos-Renyi Graphs with Fixed Number of Edges Generate a random Erdos-Renyi graph with a fixed number of edges. The graph can either be directed or undirected and can contain self-loops. #### Application ``` mpirun -n <nproc> ./KaGen <gnm-directed|gnm-undirected> -n <number of vertices> [-N <number of vertices as a power of two>] -m <number of edges> [-M <number of edges as a power of two>] [--self-loops] [-k <number of chunks>] [-s <seed>] ``` #### Library ```c++ KaGen gen(MPI_COMM_WORLD); Graph graph_directed = gen.GenerateDirectedGNM(n, m, self_loops = false); Graph graph_undirected = gen.GenerateUndirectedGNM(n, m, self_loops = false); ``` --- ### Erdos-Renyi graphs with Fixed Edge Probability Generate a random Erdos-Renyi graph with a fixed edge probability. The graph can either be directed or undirected and can contain self-loops. #### Application ``` mpirun -n <nproc> ./KaGen <gnp_directed|gnp_undirected> -n <number of vertices> [-N <number of vertices as a power of two>] -p <edge probability> [--self-loops] [-k <number of chunks>] [-s <seed>] ``` #### Library ```c++ KaGen gen(MPI_COMM_WORLD); Graph graph_directed = gen.GenerateDirectedGNP(n, p, self_loops = false); Graph graph_undirected = gen.GenerateUndirectedGNP(n, p, self_loops = false); ``` --- ### Random Geometric Graphs Generate an undirected random geometric graph. **Note:** This generator is parameterized by the number of vertices in the graph and its edge radius. Either parameter can be omitted in favor of the desired number of edges, in which case the omitted parameter is approximated such that the expected number of edges matches the desired number of edges. #### Application ``` mpirun -n <nproc> ./KaGen <rgg2d|rgg3d> -n <number of vertices> [-N <number of vertices as a power of two>] -r <edge radius> -m <number of edges> # only if -n or -r are omitted [-M <number of edges as a power of two>] # only if -n or -r are omitted [-k <number of chunks>] [-s <seed>] ``` #### Library ```c++ KaGen gen(MPI_COMM_WORLD); Graph graph = gen.GenerateRGG2D(n, r, coordinates = false); Graph graph = gen.GenerateRGG2D_NM(n, m, coordinates = false); // deduce r s.t. E[# edges] = m Graph graph = gen.GenerateRGG2D_MR(m, r, coordinates = false); // deduce n s.t. E[# edges] = m Graph graph = gen.GenerateRGG3D(n, r, coordinates = false); Graph graph = gen.GenerateRGG3D_NM(n, m, coordinates = false); // deduce r s.t. E[# edges] = m Graph graph = gen.GenerateRGG3D_MR(m, r, coordinates = false); // deduce n s.t. E[# edges] = m ``` --- ### Random Delaunay Graphs Generate an undirected random delaunay graph. **Note:** The graph can be generated with periodic boundary conditions to avoid long edges at the border using the `-p` flag. However, this can yield unexpected results when using less than 9 PEs (2D) / 27 PEs (3D) to generate the graph. #### Application ``` mpirun -n <nproc> ./KaGen <rdg2d|rdg3d> -n <number of vertices> [-N <number of vertices as a power of two>] [--periodic] [-s <seed>] ``` #### Library ```c++ KaGen gen(MPI_COMM_WORLD); Graph graph = gen.GenerateRDG2D(n, periodic, coordinates = false); Graph graph = gen.GenerateRDG2D_M(m, periodic, coordinates = false); Graph graph = gen.GenerateRDG3D(n, coordinates = false); Graph graph = gen.GenerateRDG3D_M(m, coordinates = false); ``` --- ### Random Grid Graphs Generate an undirected random grid graph. #### Application ``` mpirun -n <nproc> ./KaGen <grid2d|grid3d> -x <width of grid> [-X <width of grid as a power of two>] -y <height of grid> [-Y <height of grid as a power of two>] -z <depth of grid (grid3d only)> [-Z <depth of grid as a power of two (grid3d only)>] -p <edge probability> -m <number of edges> # only if -p is omitted [-M <number of edges as a power of two>] # only if -p is omitted [--periodic] [-k <number of cunks>] [-s <seed>] ``` #### Library ```c++ KaGen gen(MPI_COMM_WORLD); Graph graph = gen.GenerateGrid2D(x, y, p, periodic, coordinates = false); Graph graph = gen.GenerateGrid2D_N(n, p, periodic, coordinates = false); // x, y = sqrt(n) Graph graph = gen.GenerateGrid2D_NM(n, m, periodic, coordinates = false); // x, y = sqrt(n) Graph graph = gen.GenerateGrid3D(x, y, z, p, periodic, coordinates = false); Graph graph = gen.GenerateGrid3D_N(n, p, periodic, coordinates = false); // x, y, z = cbrt(n) Graph graph = gen.GenerateGrid3D_NM(n, m, periodic, coordinates = false); // x, y, z = cbrt(n) ``` --- ### Random Hyperbolic Graphs Generate a two dimensional undirected random hyperbolic graph. **Note:** On x86 systems, the generator can use 64 bit or 80 bit floating point numbers. This can be controlled explicitly by using the `--hp-floats` or `--no-hp-floats` flags. If neither flag is set, KaGen switches to 80 bit precision automatically if the generated graph has more than 2^29 vertices. **Note:** Due to floating point inaccuracies, this generator performs communication in a post-processing step. #### Application ``` mpirun -n <nproc> ./KaGen rhg -n <number of vertices> [-N <number of vertices as a power of two>] -g <power-law exponent> -d <average vertex degree> [-k <number of chunks>] [--hp-floats] [--no-hp-floats] [-s <seed>] ``` #### Library ```c++ KaGen gen(MPI_COMM_WORLD); Graph graph = gen.GenerateRHG(gamma, n, d, coordinates = false); Graph graph = gen.GenerateRHG_NM(gamma, n, m, coordinates = false); // deduce d s.t. E[# edges] = m Graph graph = gen.GenerateRHG_MD(gamma, m, d, coordinates = false); // deduce n s.t. E[# edges] = m ``` ## Non-communication-free Graph Generators Since the original publication, several other graph generators have been integrated into the KaGen framework. ### Barabassi-Albert Graphs Generate a random Barabassi-Albert graph. The graph may contain self-loops and multi edges. #### Application ``` mpirun -n <nproc> ./KaGen ba -n <number of vertices> [-N <number of vertices as a power of two>] -d <minimum degree for each vertex> [--directed] [--self-loops] [-k <number of chunks>] [-s <seed>] ``` #### Library ```c++ KaGen gen(MPI_COMM_WORLD); Graph graph = gen.GenerateBA(n, d, directed = false, self_loops = false); Graph graph = gen.GenerateBA_NM(n, m, directed = false, self_loops = false); Graph graph = gen.GenerateBA_MD(m, d, directed = false, self_loops = false); ``` --- ### R-MAT Graphs Generate a random RMAT graph. Each PE generates a random R-MAT graph with n vertices and m/\\<nproc\\> edges. Afterwards, the vertices are assigned to PEs round-robin style and edges are distributed accordingly. #### Application ``` mpirun -n <nproc> ./KaGen rmat -n <number of vertices> # should be a power of two [-N <number of vertices as a power of two>] -m <number of edges> [-M <number of edges as a power of two>] -a <probability for an edge to land in block a> -b <probability for an edge to land in block b> -c <probability for an edge to land in block c> [--directed] [--self-loops] [-s <seed>] ``` #### Library ```c++ KaGen gen(MPI_COMM_WORLD); Graph graph = gen.GenerateRMAT(n, m, a, b, c, directed = false, self_loops = false); ``` --- ### Kronecker Graphs Generate a random Kronecker graph. Each PE generates a random Kronecker graph with n vertices and m/\\<nproc\\> edges. Afterwards, the vertices are assigned to PEs round-robin style and edges are distributed accordingly. #### Application ``` mpirun -n <nproc> ./KaGen kronecker -n <number of vertices> # should be a power of two [-N <number of vertices as a power of two>] -m <number of edges> [-M <number of edges as a power of two>] [--directed] [--self-loops] [-s <seed>] ``` #### Library ```c++ KaGen gen(MPI_COMM_WORLD); Graph graph = gen.GenerateKronecker(n, m, directed = false, self_loops = false); ``` ## Static Graph Generators ### Image Graph Generator Generates a graph based on an input image. Each pixel is represented by a vertex with edges to its neighboring vertices. The image has to be converted to KARGB format first (a simple binary file containing the uncompressed R, G, B channels of the image) by using the `img2kargb` or `upsb2kargb` tool shipped with KaGen. #### Application ``` mpirun -n <nproc> ./KaGen image --filename=<path to kargb file> [--weight-model=<l2, inv-l2, inv-ratio>] [--weight-multiplier=1] [--weight-offset=0] [--min-weight-threshold=1] [--max-weight-threshold=inf] [--neighborhood=<4, 8, 24>] [--max-grid-x=<...>] [--max-grid-y=<...>] [--grid-x=<...>] [--grid-y=<...>] [--cols-per-pe=<...>] [--rows-per-pe=<...>] ``` #### Library ```c++ KaGen gen(MPI_COMM_WORLD); Graph graph = gen.GenerateFromOptionString(\"image;filename=<...>;...\"); ``` --- ### File Graph Generator Pseudo-generator that loads a static graph from disk. Can be used to convert input formats to output format, or to load static graphs when using KaGen as a library. #### Application ``` mpirun -n <nproc> ./KaGen file --filename=<path to graph> --input-format=<metis|parhip> [--distribution=<balance-vertices|balance-edges>] ``` #### Library ```c++ KaGen gen(MPI_COMM_WORLD); Graph graph = gen.GenerateFromOptionString(\"file;filename=<...>;input_format=<...>;distribution=<...>\"); ``` ## Tools Tools can be installed via `cmake --install build --component tools`. The following tools are included: ```shell # graphstats: compute some basic statistics for the given graphs mpirun ./app/tools/graphstats <path to graph(s), ...> [-f <format, e.g., metis, parhip, plain-edgelist>] # chkgraph: validate a graph file in any supported input format mpirun -n <nproc> ./app/tools/chkgraph <path to graph> [-f <format, e.g., metis, parhip, plain-edgelist>] [--64bits] # allow 64 bit weights and IDs [--self-loops] # allow self loops [--directed] # allow directed graphs (i.e., not all reverse edges are present) [--multi-edges] # allow multi edges [--negative-edge-weights] # allow negative edge weights [--negative-vertex-weights] # allow negative vertex weights # pangraph: convert a graph file between supported formats in external memory ./app/tools/pangraph --input-format=<...> --input-filename=<...> --output-format=<...> --output-filename=<...> [-C <num chunks = 1>] # split the graph into <num chunks> chunks; only one chunk has to fit into internal memory at a time [-T <tmp directory = /tmp>] # directory to be used for temporary files (requires free space roughly the size of the input graph) [--remove-self-loops] # remove any self-loops during convertion [--add-reverse-edges] # make all edges undirected by adding potentially missing reverse edges [--sort-edges] # sort the outgoing edges by destination vertex ID [-n <num vertices>] # provide the number of vertices in the graph -- currently only used for the plain-edgelist input format ``` --- **[License](/LICENSE):** 2-clause BS\n",
                "dependencies": "################################################################################ # CMakeLists.txt # # Root CMake build script for generator # # Copyright (C) 2016-2017 Sebastian Lamm <lamm@ira.uka.de> # # All rights reserved. Published under the BSD-2 license in the LICENSE file. ################################################################################ cmake_minimum_required(VERSION 3.16) project(kagen LANGUAGES C CXX) set(CMAKE_CXX_STANDARD 20) list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\") ################################################################################ option(KAGEN_NODEPS \"Build KaGen without any dependencies.\" OFF) option(KAGEN_USE_MARCH_NATIVE \"Compile with -march=native.\" OFF) option(KAGEN_USE_CGAL \"If available, link against CGal to enable RDG generators.\" ON) option(KAGEN_USE_SPARSEHASH \"Build with Google Sparsehash. If turned off, fall back to std::unordered_map<>.\" ON) option(KAGEN_USE_FAST_MATH \"Use -ffast-math.\" OFF) option(KAGEN_USE_MKL \"Build with Intel MKL random generator.\" OFF) option(KAGEN_USE_XXHASH \"Build with xxHash. If turned off, path permutation will not be available.\" ON) option(KAGEN_WARNINGS_ARE_ERRORS \"Make compiler warnings compiler errors.\" OFF) option(KAGEN_BUILD_TESTS \"Build unit tests.\" OFF) option(KAGEN_BUILD_APPS \"Build binaries.\" ON) option(KAGEN_BUILD_EXAMPLES \"Build examples.\" ON) option(INSTALL_KAGEN \"Install KaGen.\" ON) ################################################################################ include(FetchContent) FetchContent_Declare(googletest SYSTEM GIT_REPOSITORY https://github.com/google/googletest.git GIT_TAG release-1.12.1) ################################################################################ # If KAGEN_NODEPS is set, disable all dependency flags if (KAGEN_NODEPS) message(STATUS \"Building without any dependencies.\") set(KAGEN_USE_CGAL OFF) set(KAGEN_USE_SPARSEHASH OFF) set(KAGEN_USE_MKL OFF) set(KAGEN_USE_XXHASH OFF) set(KAGEN_BUILD_TESTS OFF) # requires GoogleTest endif () # Prohibit in-source builds if (\"${PROJECT_SOURCE_DIR}\" STREQUAL \"${PROJECT_BINARY_DIR}\") message(SEND_ERROR \"In-source builds are not allowed.\") endif () # Default to Release building for single-config generators if (NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES) message(STATUS \"Defaulting CMAKE_BUILD_TYPE to Release\") set(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"Build type\") endif () # Warning flags list(APPEND KAGEN_WARNING_FLAGS \"-W\" \"-Wall\" \"-Wextra\" \"-Wpedantic\" \"-Wno-unused-local-typedefs\" ) if (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\") list(APPEND KAGEN_WARNING_FLAGS \"-Wextra-semi\" \"-fcolor-diagnostics\" \"-Wdeprecated\" ) endif () if (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\") list(APPEND KAGEN_WARNING_FLAGS \"-Wsuggest-override\" \"-fdiagnostics-color=always\" \"-Wcast-qual\" \"-Winit-self\" \"-Woverloaded-virtual\" \"-Wredundant-decls\" ) endif () if (KAGEN_WARNINGS_ARE_ERRORS) list(APPEND KAGEN_WARNING_FLAGS \"-Werror\") endif () # Enable -march=native on Debug and Release builds if (KAGEN_USE_MARCH_NATIVE) include(CheckCXXCompilerFlag) check_cxx_compiler_flag(\"-march=native\" KAGEN_HAS_MARCH_NATIVE) if(KAGEN_HAS_MARCH_NATIVE) list(APPEND CMAKE_CXX_FLAGS \"-march=native\") endif () endif () # Remove -rdynamic from linker flags (smaller binaries which cannot be loaded # with dlopen() -- something no one needs) string(REGEX REPLACE \"-rdynamic\" \"\" CMAKE_SHARED_LIBRARY_LINK_C_FLAGS \"${CMAKE_SHARED_LIBRARY_LINK_C_FLAGS}\") string(REGEX REPLACE \"-rdynamic\" \"\" CMAKE_SHARED_LIBRARY_LINK_CXX_FLAGS \"${CMAKE_SHARED_LIBRARY_LINK_CXX_FLAGS}\") # Enable UndefinedBehaviorSanitizer if (OFF) list(APPEND CMAKE_CXX_FLAGS \"-fsanitize=undefined\") endif () # Use -ffast-math if (KAGEN_USE_FAST_MATH) list(APPEND CMAKE_CXX_FLAGS \"-ffast-math\") endif () ############################################################################### # libmorton ############################################################################### add_library(morton INTERFACE) target_include_directories(morton SYSTEM INTERFACE extlib/libmorton/include) list(APPEND KAGEN_LINK_LIBRARIES morton) ############################################################################### # xxHash ############################################################################### if (KAGEN_USE_XXHASH) message(STATUS \"Building with xxHash\") set(${PROJECT_NAME}_XXHASH_DIR \"${CMAKE_CURRENT_LIST_DIR}/extlib/xxHash\") set(BUILD_SHARED_LIBS OFF) set(XXHASH_BUILD_ENABLE_INLINE_API ON) set(XXHASH_BUILD_XXHSUM OFF) add_subdirectory(\"${${PROJECT_NAME}_XXHASH_DIR}/cmake_unofficial\" EXCLUDE_FROM_ALL) list(APPEND KAGEN_LINK_LIBRARIES xxHash::xxhash) add_definitions(-DKAGEN_XXHASH_FOUND) endif () ############################################################################### # MPI ############################################################################### set(MPI_DETERMINE_LIBRARY_VERSION TRUE) # needed for KaTestrophe find_package(MPI REQUIRED) list(APPEND KAGEN_LINK_LIBRARIES MPI::MPI_CXX) ############################################################################### # pthread ############################################################################### find_package(Threads REQUIRED) list(APPEND KAGEN_LINK_LIBRARIES Threads::Threads) ############################################################################### # Google Sparsehash ############################################################################### if (KAGEN_USE_SPARSEHASH) message(STATUS \"Building with Sparsehash\") find_package(Sparsehash REQUIRED) list(APPEND KAGEN_LINK_LIBRARIES Sparsehash::Sparsehash) add_definitions(-DKAGEN_SPARSEHASH_FOUND) endif () ############################################################################### # CGAL ############################################################################### if (KAGEN_USE_CGAL) set(CGAL_DO_NOT_WARN_ABOUT_CMAKE_BUILD_TYPE TRUE CACHE BOOL \"Do not warn about Debug mode\") set(CGAL_DONT_OVERRIDE_CMAKE_FLAGS TRUE CACHE BOOL \"Force CGAL to maintain CMAKE flags\") find_package(CGAL QUIET) if (CGAL_FOUND) add_definitions(-DKAGEN_CGAL_FOUND) if (${CGAL_VERSION} VERSION_GREATER_EQUAL 5) list(APPEND KAGEN_LINK_LIBRARIES CGAL::CGAL) else() include(${CGAL_USE_FILE}) endif() else () message(STATUS \"Could not find the CGAL library: Random Delaunay Graphs will not be available\") endif () endif () ############################################################################### # Sampling library -> MKL ############################################################################### if (KAGEN_USE_MKL) find_package(MKL) if (MKL_FOUND) message(STATUS \"Building with MKL\") list(APPEND KAGEN_INCLUDE_DIRS ${MKL_INCLUDE_DIR}) list(APPEND KAGEN_LINK_LIBRARIES ${MKL_LP_LIBRARY} ${MKL_CORE_LIBRARY} ${MKL_SEQUENTIAL_LIBRARY}) add_definitions(-DSAMPLING_HAVE_MKL) add_definitions(-DRMAT_HAVE_MKL) add_definitions(-DKAGEN_MKL_FOUND) else () message(STATUS \"MKL requested but not found, building without MKL\") endif () endif () ################################################################################ add_subdirectory(kagen) if (KAGEN_BUILD_APPS) add_subdirectory(app) else () message(STATUS \"Apps disabled.\") endif () if (KAGEN_BUILD_EXAMPLES) add_subdirectory(examples) else () message(STATUS \"Examples disabled.\") endif () ################################################################################ add_library(KaGen::KaGen ALIAS kagen) add_library(KaGen::cKaGen ALIAS kagen) # @deprecated, use KaGen::KaGen ################################################################################ if (KAGEN_BUILD_TESTS) enable_testing() add_subdirectory(tests) endif ()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kahypar",
            "repo_link": "https://github.com/kahypar/",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kaminpar",
            "repo_link": "https://github.com/KaHIP/KaMinPar",
            "content": {
                "codemeta": "",
                "readme": "# KaMinPar This is **KaMinPar**, a parallel heuristic solver for the balanced k-way graph partitioning problem. Given a graph, it aims to divide its vertices into k disjoint blocks of approximately equal weight, minimizing the number of edges crossing between blocks. KaMinPar offers high efficiency and low memory overheads while achieving partitions of similar quality as the widely used Metis algorithm. For example, it can partition the massive [hyperlink-2012](https://webdatacommons.org/hyperlinkgraph/index.html) graph (approx. 3.5 billion vertices and 112 billion edges) into 30,000 blocks in under 6 minutes on 96 cores, using around 300 GiB RAM. Notably, KaMinPar is also optimized for large k, where it often achieves an order-of-magnitude speedup over competing partitioners. For unweighted input graphs, it guarantees strict adherence to the balance constraint. ## Quick-Start (Python and NetworKit) KaMinPar offers bindings for Python (only available on Linux and macOS), which can be installed via `pip`: ```shell pip install kaminpar ``` Check out our [documentation](bindings/python/README.md) and [examples](examples/bindings-python/) to get started. Additionally, we offer bindings for [NetworKit](https://networkit.github.io/), which can also be installed via `pip`: ```shell pip install kaminpar-networkit ``` For instance, these allow you to generate, partition and visualize a graph in just a few lines of code: ```python import networkit as nk import kaminpar_networkit as kaminpar from networkit import vizbridges # Generate a random hyperbolic graph with 100 vertices, average degree 16 and power-law exponent 2.7 graph = nk.generators.HyperbolicGenerator(100, k = 16, gamma = 2.7, T = 0).generate() # Partition the graph into 4 blocks with a maximum imbalance of 3% partition = kaminpar.KaMinPar(graph).computePartitionWithEpsilon(4, 0.03) # Draw the graph and the partition fig = nk.vizbridges.widgetFromGraph( graph, dimension = nk.vizbridges.Dimension.Three, nodePartition = partition, nodePalette = [(0, 0, 0), (255, 0, 0), (0, 255, 0), (0, 0, 255)] ) fig.write_image(\"partitioned_hyperbolic.png\") ``` Refer to our [documentation](bindings/networkit/README.md) and [examples](examples/bindings-networkit/) to get started. ## Installation Notes (C/C++) ### Requirements * **Compiler:** C++20-ready GCC or Clang compiler * **Dependencies:** CMake, oneAPI TBB, Google Sparsehash (optional), MPI (optional) * **System:** Linux (x86, ARM) or macOS (ARM) ### Building from Source After cloning the repository, follow the standard CMake build procedure: ```shell cmake -B build --preset=default && cmake --build build --parallel ``` > [!NOTE] > Use `--preset=distributed` instead if you want to build the distributed-memory components. ### Using the Command Line Binaries To partition a graph in [Metis](docs/graph_file_format.md) format, run: ```shell # KaMinPar: shared-memory partitioning ./build/apps/KaMinPar <graph filename> <number of blocks> [-P default|terapart|strong|largek] [-t <nproc>] [-e <eps, e.g., 0.03 for 3%>] [-o <output partition>] # dKaMinPar: distributed partitioning mpirun -n <nproc> ./build/apps/dKaMinPar <graph filename> <number of blocks> [-P default|strong|xterapart] [-e <eps, e.g., 0.03 for 3%>] [-o <output partition>] ``` The computed partition is written to a text file (controlled via `-o <filename>`), where the n-th line contains the block ID (0-based) of the n-th node. There are multiple configuration presets that tune the algorithm for different scenarios: * `-P default`: fast partitioning with quality comparable to Metis. * `-P terapart`: same partition quality as `default`, but with reduced memory consumption (slightly slower). * `-P strong`: better quality than `default` through additional FM refinement at the cost of increased runtime. * `-P largek`: faster for large values of k (e.g., k > 1024); reduces partition quality slightly for smaller k. The `-k <k>` option directs (d)KaMinPar to partition the graph into k blocks of roughly equal weight (use `-e <eps>` to control the maximum allowed imbalance, e.g., `-e 0.03` for 3%). Alternatively, one can specify the maximum weight for each block explicitly through one of the following options: * `-B <W0> <W2> ... <Wk-1>`: Explicitly specifies the maximum block weights, i.e., the weight of the i-th block should be bounded by `Wi`. * `-b <w0> <w2> ... <wk-1>`: Same as `-B`, but specifies the maximum weights as fractions of the total node weight, i.e., the weight of the i-th block should be bounded by `wi * total node weight`. Other common command line options include: * `--help`: Prints all command line options. * `--version`: Prints the build configuration and current version. * `--validate`: Enables basic input graph validation. * `-q`: Quiet mode, suppresses all console output. ## Using the C++ Library Interface If you are using CMake, you can use the partitioners as libraries by adding this repository as a Git submodule to your project and including it in your CMake configuration: ```cmake add_subdirectory(external/KaMinPar) target_link_libraries(<your-target> PUBLIC KaMinPar::KaMinPar) # Shared-memory partitioning target_link_libraries(<your-target> PUBLIC KaMinPar::dKaMinPar) # Distributed partitioning ``` Alternatively, you can use `FetchContent`: ```cmake include(FetchContent) FetchContent_Declare(KaMinPar GIT_REPOSITORY https://github.com/KaHIP/KaMinPar.git GIT_TAG main EXCLUDE_FROM_ALL) set(KAMINPAR_BUILD_DISTRIBUTED ON CACHE BOOL \"\" FORCE) # optional, required for dKaMinPar FetchContent_MakeAvailable(KaMinPar) target_link_libraries(<your-target> PUBLIC KaMinPar::KaMinPar) # Shared-memory partitioning target_link_libraries(<your-target> PUBLIC KaMinPar::dKaMinPar) # Distributed partitioning ``` The shared-memory partitioner can be used as follows: ```c++ #include <kaminpar.h> using namespace kaminpar; KaMinPar shm(int num_threads, shm::create_default_context()); // Pass a copy of the graph: shm.copy_graph( std::span<const EdgeID> xadj, std::span<const NodeID> adjncy, std::span<const NodeWeight> vwgt = {}, std::span<const EdgeWeight> adjwgt = {} ); // Alternatively, let KaMinPar borrow the graph: this avoids the copy, but the // spans must stay valid throughout partitioning and KaMinPar might modify the // data: shm.borrow_and_mutate_graph( std::span<EdgeID> xadj, std::span<NodeID> adjncy, std::span<NodeWeight> vwgt = {}, std::span<EdgeWeight> adjwgt = {} ); // Compute a `k`-way partition where each block weight is bounded by // `(1 + epsilon) * average`: shm.compute_partition(BlockID k, double epsilon, std::span<BlockID> out_partition); // Compute a `max_block_weights.size()`-way partition where the `i`-th block // weight is bounded by `max_block_weights[i]`: shm.compute_partition(std::vector<BlockWeight> max_block_weights, std::span<BlockID> out_partition); // Compute a `max_block_weight_factors.size()`-way partition where the `i`-th // block weight is bounded by `max_block_weight_factors[i] * total_weight`: shm.compute_partition(std::vector<double> max_block_weight_factors, std::span<BlockID> out_partition); ``` > [!TIP] > If you want to partition a graph that does not fit into memory, you can use our [Builder interface](docs/graph_compression.md) to construct a graph while compressing it on-the-fly. The distributed-memory partitioner can be used as follows: ```c++ #include <dkaminpar.h> using namespace kaminpar; dKaMinPar dist(MPI_Comm comm, int num_threads, dist::create_default_context()); // Pass a copy of the graph: dist.copy_graph( std::span<GlobalNodeID> vtxdist, std::span<GlobalEdgeID> xadj, std::span<GlobalNodeID> adjncy, std::span<GlobalNodeWeight> vwvgt = {}, std::span<GlobalEdgeWeight> adjwgt = {} ); // Compute a `k`-way partition where each block weight is bounded by // `(1 + epsilon) * average`: dist.compute_partition(BlockID k, double epsilon, std::span<BlockID> out_partition); // Compute a `max_block_weights.size()`-way partition where the `i`-th block // weight is bounded by `max_block_weights[i]`: dist.compute_partition(std::vector<BlockWeight> max_block_weights, std::span<BlockID> out_partition); // Compute a `max_block_weight_factors.size()`-way partition where the `i`-th // block weight is bounded by `max_block_weight_factors[i] * total_weight`: dist.compute_partition(std::vector<double> max_block_weight_factors, std::span<BlockID> out_partition); ``` Check out our [examples](examples/) to see the library interface in action. ## Licensing KaMinPar is free software provided under the MIT license. If you use KaMinPar in an academic setting, please cite the appropriate publication(s) listed below. ``` // KaMinPar @InProceedings{DeepMultilevelGraphPartitioning, author = {Lars Gottesb{\\\"{u}}ren and Tobias Heuer and Peter Sanders and Christian Schulz and Daniel Seemaier}, title = {Deep Multilevel Graph Partitioning}, booktitle = {29th Annual European Symposium on Algorithms, {ESA} 2021}, series = {LIPIcs}, volume = {204}, pages = {48:1--48:17}, publisher = {Schloss Dagstuhl - Leibniz-Zentrum f{\\\"{u}}r Informatik}, year = {2021}, url = {https://doi.org/10.4230/LIPIcs.ESA.2021.48}, doi = {10.4230/LIPIcs.ESA.2021.48} } // dKaMinPar (distributed KaMinPar) @InProceedings{DistributedDeepMultilevelGraphPartitioning, author = {Sanders, Peter and Seemaier, Daniel}, title = {Distributed Deep Multilevel Graph Partitioning}, booktitle = {Euro-Par 2023: Parallel Processing}, year = {2023}, publisher = {Springer Nature Switzerland}, pages = {443--457}, isbn = {978-3-031-39698-4} } // [x]TeraPart (memory-efficient [d]KaMinPar) @misc{TeraPart, title={Tera-Scale Multilevel Graph Partitioning}, author={Daniel Salwasser and Daniel Seemaier and Lars Gottesbüren and Peter Sanders}, year={2024}, eprint={2410.19119}, archivePrefix={arXiv}, primaryClass={cs.DS}, url={https://arxiv.org/abs/2410.19119}, } // Worst-Case Linear-Time Multilevel Graph Partitioning @misc{LinearTimeMGP, title={Linear-Time Multilevel Graph Partitioning via Edge Sparsification}, author={Lars Gottesbüren and Nikolai Maas and Dominik Rosch and Peter Sanders and Daniel Seemaier}, year={2025}, eprint={2504.17615}, archivePrefix={arXiv}, primaryClass={cs.DS}, url={https://arxiv.org/abs/2504.17615}, } ```\n",
                "dependencies": "cmake_minimum_required(VERSION 3.21) list(APPEND CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake/modules) project( KaMinPar VERSION 3.4.1 DESCRIPTION \"Shared-memory and distributed-memory Graph Partitioner\" HOMEPAGE_URL \"https://github.com/KaHIP/KaMinPar\" LANGUAGES C CXX ) set(PROJECT_VENDOR \"Daniel Seemaier\") set(PROJECT_CONTACT \"daniel.seemaier@kit.edu\") set(CMAKE_CXX_STANDARD 20) set(CMAKE_CXX_STANDARD_REQUIRED ON) ################################################################################ ## Declare Options ## ################################################################################ # Control what to build ####################### option(KAMINPAR_BUILD_APPS \"Build binaries.\" ON) option(KAMINPAR_BUILD_DISTRIBUTED \"Build distributed partitioner.\" OFF) option(KAMINPAR_BUILD_TESTS \"Build unit tests\" OFF) option(KAMINPAR_BUILD_TOOLS \"Build tool binaries.\" OFF) option(KAMINPAR_BUILD_BENCHMARKS \"Build benchmark binaries.\" OFF) option(KAMINPAR_BUILD_EXAMPLES \"Build examples.\" OFF) option(KAMINPAR_BUILD_EXPERIMENTAL_FEATURES \"Include experimental features in the build. This might increase compile times drastically.\" OFF) option(KAMINPAR_DOWNLOAD_TBB \"Download TBB instead of requiring it to be installed.\" OFF) # Control how to build ###################### option(KAMINPAR_ENABLE_HEAP_PROFILING \"Profile and output heap memory usage.\" OFF) option(KAMINPAR_ENABLE_PAGE_PROFILING \"Profile pages allocated via mmap.\" OFF) option(KAMINPAR_ENABLE_STATISTICS \"Generate and output detailed statistics.\" OFF) option(KAMINPAR_ENABLE_TIMERS \"Measure running times. Must be set to 'OFF' if the library interface is used from multiple threads simulatinously.\" ON) option(KAMINPAR_ENABLE_TIMER_BARRIERS \"Add additional MPI_Barrier() instructions for more accurate time measurements.\" ON) option(KAMINPAR_ENABLE_TBB_MALLOC \"Use tbb malloc for (some) allocations.\" ON) option(KAMINPAR_ENABLE_THP \"Use transparent huge pages for large memory allocations (Linux only).\" ON) option(KAMINPAR_BUILD_WITH_ASAN \"Enable address sanitizer.\" OFF) option(KAMINPAR_BUILD_WITH_UBSAN \"Enable undefined behaviour sanitizer.\" OFF) option(KAMINPAR_BUILD_WITH_MTUNE_NATIVE \"Build with -mtune=native.\" ON) option(KAMINPAR_BUILD_WITH_CCACHE \"Use ccache to build.\" ON) option(KAMINPAR_BUILD_WITH_DEBUG_SYMBOLS \"Always build with debug symbols, even in Release mode.\" ON) option(KAMINPAR_BUILD_WITH_MTKAHYPAR \"If Mt-KaHyPar can be found, build the Mt-KaHyPar initial partitioner.\" OFF) option(KAMINPAR_BUILD_WITH_SPARSEHASH \"Build with Google Sparsehash.\" ON) option(KAMINPAR_BUILD_WITH_PG \"Build with the -pg option for profiling.\" OFF) option(KAMINPAR_BUILD_WITH_BACKWARD \"Build with backward-cpp for stack traces (distributed partitioner only).\" OFF) option(KAMINPAR_BUILD_WITH_KASSERT \"Use KASSERT for assertions. If disabled, the assertion level is ignored.\" ON) # Control whether to install KaMinPar ##################################### option(INSTALL_KAMINPAR \"Install KaMinPar.\" ON) # Control data type sizes ######################### # These IDs refer to the shared-memory partitioner + local IDs of the distributed partitioner option(KAMINPAR_64BIT_IDS \"Use 64 bits for node and edge IDs.\" OFF) option(KAMINPAR_64BIT_EDGE_IDS \"Use 64 bits for edge IDs.\" OFF) option(KAMINPAR_64BIT_NODE_IDS \"Use 64 bits for node IDs.\" OFF) # Node and edge weights for the shared-memory partitioner (+ used as initial partitioner of the distributed partitioner) option(KAMINPAR_64BIT_WEIGHTS \"Use 64 bit for node and edge weights.\" OFF) # Local node and edge weights for the distributed partitioner; should be 64 bit when using DMGP option(KAMINPAR_64BIT_LOCAL_WEIGHTS \"Use 64 bit for local node and edge weights.\" OFF) # The distributed partitioner requires 64 bit node and edge weights for the coarsest graph, # which is copied to each PE and build with data types of the shared-memory partitioner. # Thus, force 64 bit weights for the shared-memory partitioner in this case. if (KAMINPAR_BUILD_DISTRIBUTED) message(STATUS \"Distributed build: enabling 64 bit weights.\") set(KAMINPAR_64BIT_WEIGHTS ON) endif () # Control graph compression options ################################### option(KAMINPAR_COMPRESSION_HIGH_DEGREE_ENCODING \"Use high-degree encoding for the compressed graph.\" ON) option(KAMINPAR_COMPRESSION_INTERVAL_ENCODING \"Use interval encoding for the compressed graph.\" ON) option(KAMINPAR_COMPRESSION_STREAMVBYTE_ENCODING \"Use StreamVByte encoding for the compressed graph.\" OFF) option(KAMINPAR_COMPRESSION_FAST_DECODING \"Use a fast PEXT-based decoding routine for the compressed graph.\" OFF) if (KAMINPAR_64BIT_NODE_IDS AND KAMINPAR_COMPRESSION_STREAM_ENCODING) message(FATAL_ERROR \"StreamVByte encoding cannot be used with 64-bit NodeIDs.\") endif () ################################################################################ ## Declare dependencies ## ################################################################################ include(CheckCXXCompilerFlag) set(KAMINPAR_ASSERTION_LEVEL \"light\" CACHE STRING \"Assertion level.\") set_property(CACHE KAMINPAR_ASSERTION_LEVEL PROPERTY STRINGS none light normal heavy) message(STATUS \"KAssertion level: ${KAMINPAR_ASSERTION_LEVEL}\") if (KAMINPAR_ASSERTION_LEVEL STREQUAL \"none\") set(KASSERT_ASSERTION_LEVEL 0) elseif (KAMINPAR_ASSERTION_LEVEL STREQUAL \"light\") set(KASSERT_ASSERTION_LEVEL 10) elseif (KAMINPAR_ASSERTION_LEVEL STREQUAL \"normal\") set(KASSERT_ASSERTION_LEVEL 30) elseif (KAMINPAR_ASSERTION_LEVEL STREQUAL \"heavy\") set(KASSERT_ASSERTION_LEVEL 40) else () message(WARNING \"Invalid assertion level: ${KAMINPAR_ASSERTION_LEVEL}\") endif () # Export compile commands set(CMAKE_EXPORT_COMPILE_COMMANDS ON) # Set warning flags list(APPEND KAMINPAR_WARNING_FLAGS \"-W\" \"-Wall\" \"-Wextra\" \"-Wpedantic\" \"-Wno-unused-local-typedefs\" ) if (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\") list(APPEND KAMINPAR_WARNING_FLAGS \"-Wextra-semi\" \"-fcolor-diagnostics\" \"-Wdeprecated\" ) endif () if (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\") list(APPEND KAMINPAR_WARNING_FLAGS \"-Wsuggest-override\" \"-fdiagnostics-color=always\" \"-Wcast-qual\" \"-Winit-self\" \"-Woverloaded-virtual\" \"-Wredundant-decls\" \"-Wno-psabi\" ) endif () # Build experimental features that increase compile times if (KAMINPAR_BUILD_EXPERIMENTAL_FEATURES) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_EXPERIMENTAL\") endif () # Always enable Debug symbols (including in Release mode) if (KAMINPAR_BUILD_WITH_DEBUG_SYMBOLS) add_compile_options(-g) endif () # Set compile flags set(CMAKE_REQUIRED_FLAGS -Werror) # otherwise the check fails for Apple Clang check_cxx_compiler_flag(-msse4.1 COMPILER_SUPPORTS_MSSE41) if (COMPILER_SUPPORTS_MSSE41) add_compile_options(-msse4.1) endif () check_cxx_compiler_flag(-mcx16 COMPILER_SUPPORTS_MCX16) if (COMPILER_SUPPORTS_MCX16) add_compile_options(-mcx16) endif () if (KAMINPAR_BUILD_WITH_MTUNE_NATIVE) add_compile_options(-mtune=native -march=native) endif () if (KAMINPAR_BUILD_WITH_ASAN) add_compile_options(-fsanitize=address) add_link_options(-fsanitize=address) endif () if (KAMINPAR_BUILD_WITH_UBSAN) add_compile_options(-fsanitize=undefined) add_link_options(-fsanitize=undefined) endif () if (KAMINPAR_BUILD_WITH_PG) add_compile_options(-pg) endif () # Pass CMake options to code if (KAMINPAR_ENABLE_STATISTICS) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_ENABLE_STATISTICS\") message(STATUS \"Statistics: enabled\") else () message(STATUS \"Statistics: disabled\") endif () if (KAMINPAR_ENABLE_HEAP_PROFILING) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_ENABLE_HEAP_PROFILING\") message(STATUS \"Heap Profiling: enabled\") else () message(STATUS \"Heap Profiling: disabled\") endif () if (KAMINPAR_ENABLE_PAGE_PROFILING) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_ENABLE_PAGE_PROFILING\") message(STATUS \"Page Profiling: enabled\") else () message(STATUS \"Page Profiling: disabled\") endif () if (KAMINPAR_ENABLE_TIMERS) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_ENABLE_TIMERS\") message(STATUS \"Timers: enabled\") else () message(STATUS \"Timers: disabled\") endif () if (KAMINPAR_ENABLE_TIMER_BARRIERS) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_ENABLE_TIMER_BARRIERS\") message(STATUS \"Timer barriers: enabled\") else () message(STATUS \"Timer barriers: disabled\") endif () if (KAMINPAR_ENABLE_TBB_MALLOC) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_ENABLE_TBB_MALLOC\") message(STATUS \"TBB malloc: enabled\") else () message(STATUS \"TBB malloc: disabled\") endif () if (KAMINPAR_ENABLE_THP) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_ENABLE_THP\") message(STATUS \"Huge pages: enabled\") else () message(STATUS \"Huge pages: disabled\") endif () message(STATUS \"Graph compression summary:\") if (KAMINPAR_COMPRESSION_HIGH_DEGREE_ENCODING) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_COMPRESSION_HIGH_DEGREE_ENCODING\") message(\" High-degree encoding: enabled\") else () message(\" High-degree encoding: disabled\") endif () if (KAMINPAR_COMPRESSION_INTERVAL_ENCODING) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_COMPRESSION_INTERVAL_ENCODING\") message(\" Interval encoding: enabled\") else () message(\" Interval encoding: disabled\") endif () if (KAMINPAR_COMPRESSION_RUN_LENGTH_ENCODING) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_COMPRESSION_RUN_LENGTH_ENCODING\") message(\" Run-length encoding: enabled\") else () message(\" Run-length encoding: disabled\") endif () if (KAMINPAR_COMPRESSION_STREAMVBYTE_ENCODING) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_COMPRESSION_STREAMVBYTE_ENCODING\") message(\" StreamVByte encoding: enabled\") else () message(\" StreamVByte encoding: disabled\") endif () if (KAMINPAR_COMPRESSION_FAST_DECODING) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_COMPRESSION_FAST_DECODING\") add_compile_options(-mbmi2) message(\" Fast decoding: enabled\") else () message(\" Fast decoding: disabled\") endif () if (KAMINPAR_64BIT_NODE_IDS OR KAMINPAR_64BIT_IDS) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_64BIT_NODE_IDS\") set(KAMINPAR_SHM_NODE_ID_STR \"std::uint64_t\") else () set(KAMINPAR_SHM_NODE_ID_STR \"std::uint32_t\") endif () if (KAMINPAR_64BIT_EDGE_IDS OR KAMINPAR_64BIT_IDS) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_64BIT_EDGE_IDS\") set(KAMINPAR_SHM_EDGE_ID_STR \"std::uint64_t\") else () set(KAMINPAR_SHM_EDGE_ID_STR \"std::uint32_t\") endif () if (KAMINPAR_64BIT_WEIGHTS) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_64BIT_WEIGHTS\") set(KAMINPAR_SHM_WEIGHT_STR \"std::int64_t\") else () set(KAMINPAR_SHM_WEIGHT_STR \"std::int32_t\") endif () if (KAMINPAR_64BIT_LOCAL_WEIGHTS) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_64BIT_LOCAL_WEIGHTS\") set(KAMINPAR_DIST_WEIGHT_STR \"std::int64_t\") else () set(KAMINPAR_DIST_WEIGHT_STR \"std::int32_t\") endif () message(STATUS \"Data type summary:\") message(\" {shm, dist}::NodeID: ${KAMINPAR_SHM_NODE_ID_STR}\") message(\" {shm, dist}::EdgeID: ${KAMINPAR_SHM_EDGE_ID_STR}\") message(\" shm::{Node, Edge}Weight: ${KAMINPAR_SHM_WEIGHT_STR}\") message(\" {dist::Global{Node, Edge}ID: std::uint64_t\") message(\" dist::Global{Node, Edge}Weight: std::int64_t\") message(\" dist::{Node, Edge}Weight: ${KAMINPAR_DIST_WEIGHT_STR}\") ################################################################################ ## Search and fetch dependencies ## ################################################################################ if (CMAKE_PROJECT_NAME STREQUAL PROJECT_NAME) include(CTest) endif() include(FindGit) include(FetchContent) find_library(NUMA_LIB numa) # Fetch KAssert for assertions if (KAMINPAR_BUILD_WITH_KASSERT) FetchContent_Declare( kassert GIT_REPOSITORY https://github.com/kamping-site/kassert.git GIT_TAG 988b7d54b79ae6634f2fcc53a0314fb1cf2c6a23 EXCLUDE_FROM_ALL UPDATE_DISCONNECTED TRUE ) FetchContent_MakeAvailable(kassert) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_KASSERT_FOUND\") endif () # Intel TBB if (KAMINPAR_DOWNLOAD_TBB) FetchContent_Declare( TBB GIT_REPOSITORY https://github.com/uxlfoundation/oneTBB.git GIT_TAG 0c0ff192a2304e114bc9e6557582dfba101360ff # v2022.0.0 EXCLUDE_FROM_ALL UPDATE_DISCONNECTED TRUE ) set(TBB_TEST OFF CACHE BOOL \"\" FORCE) FetchContent_MakeAvailable(TBB) else () find_package(TBB REQUIRED) endif () # Google Sparsehash if (KAMINPAR_BUILD_WITH_SPARSEHASH) find_package(Sparsehash REQUIRED) list(APPEND KAMINPAR_DEFINITIONS \"-DKAMINPAR_SPARSEHASH_FOUND\") endif () if (KAMINPAR_BUILD_WITH_CCACHE) find_program(CCACHE_PROGRAM ccache) if (CCACHE_PROGRAM) set(CMAKE_CXX_COMPILER_LAUNCHER \"${CCACHE_PROGRAM}\") endif () endif () if (KAMINPAR_BUILD_DISTRIBUTED) # MPI set(MPI_DETERMINE_LIBRARY_VERSION TRUE) find_package(MPI) if (NOT MPI_FOUND) message(WARNING \"MPI not available: cannot build the distributed partitioner\") set(KAMINPAR_BUILD_DISTRIBUTED OFF) endif () # Growt add_subdirectory(external/growt EXCLUDE_FROM_ALL) add_library(growt INTERFACE) target_include_directories(growt SYSTEM INTERFACE $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/external/growt>) if (KAMINPAR_BUILD_WITH_BACKWARD) FetchContent_Declare( Backward GIT_REPOSITORY https://github.com/kamping-site/bakward-mpi.git GIT_TAG 89de1132cdccb60aa4994d00396cc30d47402f95 EXCLUDE_FROM_ALL UPDATE_DISCONNECTED TRUE ) FetchContent_MakeAvailable(Backward) endif () endif () # If we can find Mt-KaHyPar, make it available for initial partitioning and refinement if (KAMINPAR_BUILD_WITH_MTKAHYPAR) find_library(LIB_MTKAHYPAR_GRAPH mtkahypar) if (NOT LIB_MTKAHYPAR_GRAPH) message(STATUS \"Mt-KaHyPar initial partitioning not available: library could not be found on this system\") set(KAMINPAR_BUILD_WITH_MTKAHYPAR OFF) else () message(STATUS \"Found Mt-KaHyPar at ${LIB_MTKAHYPAR_GRAPH}\") endif () endif () # Fetch minimal KaGen for graph IO if ((KAMINPAR_BUILD_DISTRIBUTED AND KAMINPAR_BUILD_APPS) OR KAMINPAR_BUILD_BENCHMARKS) FetchContent_Declare( KaGen GIT_REPOSITORY https://github.com/KarlsruheGraphGeneration/KaGen.git GIT_TAG 70386f48e513051656f020360c482ce6bff9a24f PATCH_COMMAND ${GIT_EXECUTABLE} apply ${CMAKE_CURRENT_SOURCE_DIR}/scripts/KaGen.patch EXCLUDE_FROM_ALL UPDATE_DISCONNECTED TRUE ) set(KAGEN_NODEPS ON CACHE BOOL \"\" FORCE) set(KAGEN_BUILD_APPS OFF CACHE BOOL \"\" FORCE) set(KAGEN_BUILD_EXAMPLES OFF CACHE BOOL \"\" FORCE) set(KAGEN_BUILD_TESTS OFF CACHE BOOL \"\" FORCE) set(INSTALL_KAGEN OFF CACHE BOOL \"\" FORCE) FetchContent_MakeAvailable(KaGen) endif () # Fetch Google Test for unit tests if ((CMAKE_PROJECT_NAME STREQUAL PROJECT_NAME OR KAMINPAR_BUILD_TESTS) AND BUILD_TESTING) FetchContent_Declare( googletest GIT_REPOSITORY https://github.com/google/googletest.git GIT_TAG 5a37b517ad4ab6738556f0284c256cae1466c5b4 EXCLUDE_FROM_ALL UPDATE_DISCONNECTED TRUE ) set(INSTALL_GTEST OFF CACHE BOOL \"\" FORCE) FetchContent_MakeAvailable(googletest) endif () ################################################################################ ## Add targets in subdirectories ## ################################################################################ # Start include paths on project root include_directories(${PROJECT_SOURCE_DIR}) # Add core library targets add_subdirectory(kaminpar-common) # Common utilities shared across the project add_subdirectory(kaminpar-shm) # Shared-memory components if (KAMINPAR_BUILD_DISTRIBUTED) add_subdirectory(kaminpar-mpi) # MPI-related utilities add_subdirectory(kaminpar-dist) # Distributed components endif () add_subdirectory(kaminpar-io) # IO components # Create an interface library that groups all core components and ensures the full # project include path is available to dependent targets add_library(KaMinParFull INTERFACE) add_library(KaMinPar::KaMinParFull ALIAS KaMinParFull) target_include_directories(KaMinParFull INTERFACE ${CMAKE_CURRENT_SOURCE_DIR}) target_link_libraries(KaMinParFull INTERFACE KaMinParIO KaMinPar) if (KAMINPAR_BUILD_DISTRIBUTED) target_link_libraries(KaMinParFull INTERFACE dKaMinParIO dKaMinPar) endif () # Tests are built either when running as the main project or when explicitly enabled if ((CMAKE_PROJECT_NAME STREQUAL PROJECT_NAME OR KAMINPAR_BUILD_TESTS) AND BUILD_TESTING) add_subdirectory(tests) endif () if (KAMINPAR_BUILD_APPS) add_subdirectory(kaminpar-cli) add_subdirectory(apps) endif () if (KAMINPAR_BUILD_EXAMPLES) add_subdirectory(examples/kaminpar) add_subdirectory(examples/bindings-c) endif () ################################################################################ ## Install targets ## ################################################################################ if (NOT INSTALL_KAMINPAR) return() endif () # If the installation prefix is not set by the user for Unix builds, # assign a default value to ensure GNUInstallDirs selects the correct paths. if (UNIX AND CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT) set(CMAKE_INSTALL_PREFIX \"/usr/\" CACHE STRING \"\" FORCE) endif () include(CMakePackageConfigHelpers) include(GNUInstallDirs) set(KAMINPAR_INSTALL_INCLUDE_DIRS include/kaminpar-io/ include/kaminpar-shm/) if (KAMINPAR_BUILD_DISTRIBUTED) list(APPEND KAMINPAR_INSTALL_INCLUDE_DIRS include/kaminpar-dist/) endif () install( DIRECTORY ${KAMINPAR_INSTALL_INCLUDE_DIRS} DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}\" COMPONENT KaMinPar_Development ) set(KAMINPAR_INSTALL_TARGETS KaMinParCommon KaMinParIO KaMinPar) if (KAMINPAR_BUILD_WITH_KASSERT) list(APPEND KAMINPAR_INSTALL_TARGETS kassert_base kassert) endif () if (KAMINPAR_BUILD_APPS) list(APPEND KAMINPAR_INSTALL_TARGETS KaMinParCLI KaMinParApp) endif () if (KAMINPAR_BUILD_DISTRIBUTED) list(APPEND KAMINPAR_INSTALL_TARGETS morton kagen growt KaMinParMPI dKaMinParIO dKaMinPar) if (KAMINPAR_BUILD_APPS) list(APPEND KAMINPAR_INSTALL_TARGETS dKaMinParCLI dKaMinParApp) endif () endif () install( TARGETS ${KAMINPAR_INSTALL_TARGETS} EXPORT KaMinParTargets RUNTIME COMPONENT KaMinPar_Runtime LIBRARY COMPONENT KaMinPar_Runtime NAMELINK_COMPONENT KaMinPar_Development ARCHIVE COMPONENT KaMinPar_Development INCLUDES DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}\" ) # Allow package maintainers to freely override the path for the configs set( KAMINPAR_INSTALL_CMAKEDIR \"${CMAKE_INSTALL_LIBDIR}/cmake/KaMinPar\" CACHE STRING \"CMake package config location relative to the install prefix\" ) set_property(CACHE KAMINPAR_INSTALL_CMAKEDIR PROPERTY TYPE PATH) mark_as_advanced(KAMINPAR_INSTALL_CMAKEDIR) install( FILES ${PROJECT_SOURCE_DIR}/cmake/modules/FindSparsehash.cmake DESTINATION ${KAMINPAR_INSTALL_CMAKEDIR} COMPONENT KaMinPar_Development ) configure_file(cmake/KaMinParConfig.cmake.in KaMinParConfig.cmake @ONLY) install( FILES ${PROJECT_BINARY_DIR}/KaMinParConfig.cmake DESTINATION ${KAMINPAR_INSTALL_CMAKEDIR} COMPONENT KaMinPar_Development ) write_basic_package_version_file( \"KaMinParConfigVersion.cmake\" VERSION ${PACKAGE_VERSION} COMPATIBILITY SameMajorVersion ) install( FILES ${PROJECT_BINARY_DIR}/KaMinParConfigVersion.cmake DESTINATION ${KAMINPAR_INSTALL_CMAKEDIR} COMPONENT KaMinPar_Development ) install( EXPORT KaMinParTargets NAMESPACE KaMinPar:: DESTINATION ${KAMINPAR_INSTALL_CMAKEDIR} COMPONENT KaMinPar_Development ) ################################################################################ ## Package targets ## ################################################################################ set(CPACK_THREADS 0) # Given 0 CPack will try to use all available CPU cores. set(CPACK_VERBATIM_VARIABLES ON) set(CPACK_PACKAGE_DIRECTORY \"${CMAKE_CURRENT_BINARY_DIR}/packages\") set(CPACK_PACKAGING_INSTALL_PREFIX ${CMAKE_INSTALL_PREFIX} CACHE STRING \"\") set(CPACK_PACKAGE_VENDOR ${PROJECT_VENDOR}) set(CPACK_PACKAGE_CONTACT \"${PROJECT_VENDOR} <${PROJECT_CONTACT}>\") set(CPACK_RESOURCE_FILE_LICENSE \"${CMAKE_CURRENT_SOURCE_DIR}/LICENSE\") set(CPACK_RESOURCE_FILE_README \"${CMAKE_CURRENT_SOURCE_DIR}/README.MD\") set(CPACK_STRIP_FILES ON) set(CPACK_COMPONENTS_GROUPING ALL_COMPONENTS_IN_ONE) set( CPACK_INSTALL_DEFAULT_DIRECTORY_PERMISSIONS OWNER_READ OWNER_WRITE OWNER_EXECUTE GROUP_READ GROUP_EXECUTE WORLD_READ WORLD_EXECUTE ) set(CPACK_DEB_COMPONENT_INSTALL ON) set(CPACK_DEBIAN_FILE_NAME DEB-DEFAULT) set(CPACK_DEBIAN_PACKAGE_RELEASE 1) set(CPACK_DEBIAN_PACKAGE_GENERATE_SHLIBS ON) set(CPACK_DEBIAN_PACKAGE_DEPENDS \"libc6 (>= 2.34), libgcc-s1 (>= 3.0), libstdc++6 (>= 10.2), libtbb-dev (>= 2018~U6-4), libsparsehash-dev (>= 2.0.2-1)\") if (NUMA_LIB) set(CPACK_DEBIAN_PACKAGE_DEPENDS \"${CPACK_DEBIAN_PACKAGE_DEPENDS}, libnuma-dev (>= 2.0.12-1)\") endif () if (KAMINPAR_BUILD_DISTRIBUTED) set(CPACK_DEBIAN_PACKAGE_DEPENDS \"${CPACK_DEBIAN_PACKAGE_DEPENDS}, libopenmpi-dev (>= 3.1.3-11)\") endif () set(CPACK_RPM_COMPONENT_INSTALL ON) set(CPACK_RPM_FILE_NAME RPM-DEFAULT) set(CPACK_RPM_PACKAGE_LICENSE \"MIT\") set(CPACK_RPM_PACKAGE_RELEASE 1) set(CPACK_RPM_PACKAGE_AUTOREQ OFF) set(CPACK_RPM_PACKAGE_REQUIRES \"tbb-devel, sparsehash-devel\") if (NUMA_LIB) set(CPACK_RPM_PACKAGE_REQUIRES \"${CPACK_RPM_PACKAGE_REQUIRES}, numactl-devel\") endif () if (KAMINPAR_BUILD_DISTRIBUTED) set(CPACK_RPM_PACKAGE_REQUIRES \"${CPACK_RPM_PACKAGE_REQUIRES}, openmpi-devel\") endif () include(CPack)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kamping-karlsruhe-mpi-next-generation",
            "repo_link": "https://github.com/kamping-site/kamping",
            "content": {
                "codemeta": "",
                "readme": "[![C/C++ CI](https://github.com/kamping-site/kamping/actions/workflows/build.yml/badge.svg)](https://github.com/kamping-site/kamping/actions/workflows/build.yml) ![GitHub](https://img.shields.io/github/license/kamping-site/kamping) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10949643.svg)](https://doi.org/10.5281/zenodo.10949643) # KaMPIng: Karlsruhe MPI next generation :rocket: ![KaMPIng logo](./docs/images/logo.svg) This is KaMPIng [kampɪŋ], a (near) zero-overhead MPI wrapper for modern C++. It covers the whole range of abstraction levels from low-level MPI calls to convenient STL-style bindings, where most parameters are inferred from a small subset of the full parameter set. This allows for both rapid prototyping and fine-tuning of distributed code with predictable runtime behavior and memory management. Using template-metaprogramming, only code paths required for computing parameters not provided by the user are generated at compile time, which results in (near) zero-overhead bindings. **:running: Quick Start:** KaMPIng is header-only, compatible with all major MPI implementations and requires a C++17-ready compiler. The easiest way to get started is to include KaMPIng using CMake's FetchContent module. ```cmake include(FetchContent) FetchContent_Declare( kamping GIT_REPOSITORY https://github.com/kamping-site/kamping.git GIT_TAG v0.1.1 ) FetchContent_MakeAvailable(kamping) target_link_libraries(myapp PRIVATE kamping::kamping) ``` It is fully compatible with your existing MPI code and you can start using it right away. Just include the headers for the main communicator class and the MPI call that you want to use. ``` c++ #include <kamping/communicator.hpp> #include <kamping/collectives/allgather.hpp> kamping::Communicator comm; std::vector<int> input(comm.rank(), comm.rank_signed()); auto const result = comm.allgatherv(kamping::send_buf(input)); ``` We provide a wide range of [usage](./examples/usage) and [simple applications](./examples/applications) examples (start with [`allgatherv`](./examples/usage/allgatherv_example.cpp)). Or checkout the [documentation](https://kamping-site.github.io/kamping/) for a description of KaMPIng's core concepts and a full reference. KaMPIng is developed at the [Algorithm Engineering Group](https://ae.iti.kit.edu/english/index.php) at Karlsruhe Institute of Technology. If you use KaMPIng in the context of an academic publication, we kindly ask you to cite our [SC'24 paper](https://doi.org/10.1109/SC41406.2024.00050): ``` bibtex @inproceedings{kamping2024, author = {Uhl, Tim Niklas and Schimek, Matthias and Hübner, Lukas and Hespe, Demian and Kurpicz, Florian and Seemaier, Daniel and Stelz, Christoph and Sanders, Peter}, booktitle = {SC24: International Conference for High Performance Computing, Networking, Storage and Analysis}, title = {KaMPIng: Flexible and (Near) Zero-Overhead C++ Bindings for MPI}, year = {2024}, pages = {1-21}, doi = {10.1109/SC41406.2024.00050} } ``` You can also find a [freely accessibly post-print in the arXiv.](https://arxiv.org/abs/2404.05610) ## Features :sparkles: ### Named Parameters :speech_balloon: Using plain MPI, operations like `MPI_Allgatherv` often lead to verbose and error-prone boilerplate code: ``` c++ std::vector<T> v = ...; // Fill with data int size; MPI_Comm_size(comm, &size); int n = static_cast<int>(v.size()); std::vector<int> rc(size), rd(size); MPI_Allgather(&n, 1, MPI_INT, rc.data(), 1, MPI_INT, comm); std::exclusive_scan(rc.begin(), rc.end(), rd.begin(), 0); int n_glob = rc.back() + rd.back(); std::vector<T> v_glob(v_global_size); MPI_Allgatherv(v.data(), v_size, MPI_TYPE, v_glob.data(), rc.data(), rd.data(), MPI_TYPE, comm); ``` In contrast, KaMPIng introduces a streamlined syntax inspired by Python's named parameters. For example, the `allgatherv` operation becomes more intuitive and concise: ```c++ std::vector<T> v = ...; // Fill with data std::vector<T> v_glob = comm.allgatherv(kamping::send_buf(v)); ``` Empowered by named parameters, KaMPIng allows users to name and pass parameters in arbitrary order, computing default values only for the missing ones. This not only improves readability but also streamlines the code, providing a user-friendly and efficient way of writing MPI applications. ### Controlling memory allocation :floppy_disk: KaMPIng's *resize policies* allow for fine-grained control over when allocation happens: | resize policy | | |--------------------------|-------------------------------------------------------------------------| | `kamping::resize_to_fit` | resize the container to exactly accommodate the data | | `kamping::no_resize` | assume that the container has enough memory available to store the data | | `kamping::grow_only` | only resize the container if it not large enough | ``` c++ // easy to use with sane defaults std::vector<int> v = comm.recv<int>(kamping::source(kamping::rank::any)); // flexible memory control std::vector<int> v_out; v_out.resize(enough_memory_to_fit); // already_known_counts are the recv_counts that may have been computed already earlier and thus do not need to be computed again comm.recv<int>(kamping::recv_buf<kamping::no_resize>(v_out), kamping::recv_count(i_know_already_know_that), kamping::source(kamping::rank::any)); ``` ### STL support :books: - KaMPIng works with everything that is a `std::contiguous_range`, everywhere. - Builtin C++ types are automatically mapped to their corresponding MPI types. - All internally used containers can be altered via template parameters. ### Expandability :jigsaw: - Don't like the performance of your MPI implementation's reduce algorithm? Just override it using our plugin architecture. - Add additional functionality to communicator objects, without altering any application code. - Easy to integrate with existing MPI code. - Flexible core library for a new toolbox :toolbox: of distributed datastructures and algorithms ### And much more ... :arrow_upper_right: - Safety guarantees for non-blocking communication and easy handling of multiple requests via request pools - Compile time and runtime error checking (which can be completely deactivated). - Collective hierarchical timers to speed up your evaluation workflow. - ... Dive into the [documentation](https://kamping-site.github.io/kamping/) or [tests](https://github.com/kamping-site/kamping/tree/main/tests) to find out more ... ### (Near) zero overhead - for development and performance :chart_with_upwards_trend: Using template-metaprogramming, KaMPIng only generates the code paths required for computing parameters not provided by the user. The following shows a complete implementation of distributed sample sort with KaMPIng. ```c++ void sort(MPI_Comm comm_, std::vector<T>& data, size_t seed) { Communicator<> comm(comm_); size_t const oversampling_ratio = 16 * static_cast<size_t>(std::log2(comm.size())) + 1; std::vector<T> local_samples(oversampling_ratio); std::sample(data.begin(), data.end(), local_samples.begin(), oversampling_ratio, std::mt19937{seed}); auto global_samples = comm.allgather(kamping::send_buf(local_samples)); std::sort(global_samples.begin(), global_samples.end()); for (size_t i = 0; i < comm.size() - 1; i++) { global_samples[i] = global_samples[oversampling_ratio * (i + 1)]; } global_samples.resize(num_splitters); std::vector<std::vector<T>> buckets(global_samples.size() + 1); for (auto& element: data) { auto const bound = std::upper_bound(global_samples.begin(), global_samples.end(), element); buckets[static_cast<size_t>(bound - global_samples.begin())].push_back(element); } data.clear(); std::vector<int> scounts; for (auto& bucket: buckets) { data.insert(data.end(), bucket.begin(), bucket.end()); scounts.push_back(static_cast<int>(bucket.size())); } data = comm.alltoallv(kamping::send_buf(data), kamping::send_counts(scounts)); std::sort(data.begin(), data.end()); } ``` It is a lot more concise than the [(verbose) plain MPI implementation](./examples/applications/sample-sort/mpi.hpp), but also introduces no additional overhead to achieve this, as can be seen the following experiment. There we compare the sorting implementation in KaMPIng to other MPI bindings. ![](./plot.svg) ## Platform :desktop_computer: - intensively tested with GCC and Clang and OpenMPI - requires a C++17 ready compiler - easy integration into other projects using modern CMake ## Other MPI bindings | | [MPI](https://www.mpi-forum.org/) | [Boost.MPI](https://www.boost.org/doc/libs/1_84_0/doc/html/mpi.html) | [RWTH MPI](https://github.com/VRGroupRWTH/mpi) | [MPL](https://github.com/rabauke/mpl) | ![KaMPIng](./docs/images/icon.svg) | |------------------------------------------------------|:---------------------------------:|:--------------------------------------------------------------------:|:----------------------------------------------:|:-------------------------------------:|:-----------------------------------------------:| | STL support | :x: | :heavy_check_mark:[^2] | :heavy_check_mark:[^3] | :heavy_check_mark:[^2] | :white_check_mark: | | computation of defaults via additional communication | :x: | :x: | :white_check_mark: | :x: | :white_check_mark: | | custom reduce operations via lambdas | :x: | :white_check_mark: | :x: | :heavy_check_mark:[^4] | :white_check_mark: | | containers can be resized automatically | :x: | :heavy_check_mark:[^1] | :heavy_check_mark:[^3] | :x: | :white_check_mark: | | error handling | :white_check_mark: | :white_check_mark: | :white_check_mark: | :x: | :white_check_mark: | | actively maintained | :white_check_mark: | :x: | :heavy_check_mark: | :white_check_mark: | :white_check_mark: | [^1]: partial [^2]: only `std::vector` [^3]: only for send and receive buffers [^4]: not mapped to builtin operations ## LICENSE KaMPIng is released under the GNU Lesser General Public License. See [COPYING](COPYING) and [COPYING.LESSER](COPYING.LESSER) for details\n",
                "dependencies": "cmake_minimum_required(VERSION 3.25) list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\") # Project setup project( KaMPIng DESCRIPTION \"Flexible and (near) zero-overhead C++ bindings for MPI\" LANGUAGES CXX VERSION 0.1.2 ) include(FetchContent) if (PROJECT_IS_TOP_LEVEL) # folder support for IDEs set_property(GLOBAL PROPERTY USE_FOLDERS ON) # this has to be enabled in the main CMakeLists file include(CTest) add_subdirectory(docs) FetchContent_Declare( Format.cmake GIT_REPOSITORY https://github.com/TheLartians/Format.cmake GIT_TAG v1.8.1 ) FetchContent_MakeAvailable(Format.cmake) endif () # Require out-of-source builds file(TO_CMAKE_PATH \"${PROJECT_BINARY_DIR}/CMakeLists.txt\" LOC_PATH) if (EXISTS \"${LOC_PATH}\") message( FATAL_ERROR \"You cannot build in a source directory (or any directory with a CMakeLists.txt file). Please make a build \" \"subdirectory. Feel free to remove CMakeCache.txt and CMakeFiles.\" ) endif () option(KAMPING_WARNINGS_ARE_ERRORS OFF) option(KAMPING_BUILD_EXAMPLES_AND_TESTS OFF) option(KAMPING_TESTS_DISCOVER OFF) option(KAMPING_ENABLE_ULFM \"Enable User-Level Failure-Mitigation (ULFM)\" OFF) option(KAMPING_ENABLE_SERIALIZATION \"Enable support for serialization (requires Cereal)\" ON) option(KAMPING_ENABLE_REFLECTION \"Enable support for reflecting struct members (requires Boost.PFR)\" ON) option( KAMPING_REFLECTION_USE_SYSTEM_BOOST_FOR_PFR \"Use Boost.PFR from system installed Boost, instead of using a standalone PFR install or building PFR from source.\" OFF ) # Enable compilation with ccache. Defaults to ON if this is the main project. if (PROJECT_IS_TOP_LEVEL) option(KAMPING_USE_CCACHE \"Globally enable ccache.\" ON) else () option(KAMPING_USE_CCACHE \"Globally enable ccache.\" OFF) endif () if (KAMPING_USE_CCACHE) include(CCache) endif () set(MPI_DETERMINE_LIBRARY_VERSION TRUE) find_package(MPI REQUIRED) add_subdirectory(extern) add_library(kamping_base INTERFACE) target_include_directories(kamping_base INTERFACE include) # set C++ standard to C++17 target_compile_features(kamping_base INTERFACE cxx_std_17) target_link_libraries(kamping_base INTERFACE MPI::MPI_CXX) list( APPEND KAMPING_WARNING_FLAGS \"-Wall\" \"-Wextra\" \"-Wconversion\" \"-Wnon-virtual-dtor\" \"-Woverloaded-virtual\" \"-Wshadow\" \"-Wsign-conversion\" \"-Wundef\" \"-Wunreachable-code\" \"-Wunused\" ) if (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\") list( APPEND KAMPING_WARNING_FLAGS \"-Wcast-align\" \"-Wnull-dereference\" \"-Wpedantic\" \"-Wextra-semi\" \"-Wno-gnu-zero-variadic-macro-arguments\" ) endif () if (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\") list( APPEND KAMPING_WARNING_FLAGS \"-Wcast-align\" \"-Wnull-dereference\" \"-Wpedantic\" \"-Wnoexcept\" \"-Wsuggest-attribute=const\" \"-Wsuggest-attribute=noreturn\" \"-Wsuggest-override\" ) endif () # OFF by default. if (KAMPING_WARNINGS_ARE_ERRORS) list(APPEND KAMPING_WARNING_FLAGS \"-Werror\") endif () # Target for user-code add_library(kamping INTERFACE) target_link_libraries(kamping INTERFACE kamping_base) # If enabled, use exceptions, otherwise use KASSERT() option(KAMPING_EXCEPTION_MODE \"Use exceptions to report recoverable errors.\" ON) if (KAMPING_EXCEPTION_MODE) set(KASSERT_EXCEPTION_MODE 1) target_compile_definitions(kamping INTERFACE -DKASSERT_EXCEPTION_MODE) message(STATUS \"Build with exceptions enabled.\") else () set(KASSERT_EXCEPTION_MODE 0) message(STATUS \"Build with exceptions disabled. Assertions are used instead.\") endif () # The assertion level controls which assertions are enabled during runtime: # # * Level 0: Disable all assertions # * Level 10: Exception assertions = only enable exceptions (if not in exception mode) # * Level 20: Light assertions = assertions that do not affect the running time of library operations significantly. # * Level 30: Normal assertions = assertions that might slow down some operations of the library by a constant factor. # Should only be used in debug mode. # * Level 40: Light communication assertions = assertions that perform additional communication causing small running # time overheads. # * Level 50: Heavy communication assertions = assertions that perform additional communication causing significant # running time overheads. # * Level 60: Heavy assertions = assertions that introduce overhead which renders some library operations infeasible # when invoked with any significant work load. # # Assertion levels can be set explicitly using the -DKAMPING_ASSERTION_LEVEL=... flag. If no level is set explicitly, we # set it to 10 (exceptions only) in Release mode and 30 (up to normal assertions) in Debug mode. set(KAMPING_ASSERTION_LEVEL $<IF:$<CONFIG:Debug>,\"normal\",\"exceptions\"> CACHE STRING \"Assertion level\" ) set_property( CACHE KAMPING_ASSERTION_LEVEL PROPERTY STRINGS none exceptions light normal light_communication heavy_communication heavy ) message(STATUS \"Assertion level: ${KAMPING_ASSERTION_LEVEL}\") # If KAMPING_ASSERTION_LEVEL defaults to the generator expression, ${KAMPING_ASSERTION_LEVEL} may not be quoted However, # if it is explicitly set to some constant string, it must be quoted Thus, all levels are listed twice, once with and # without quotes @todo find a better solution for this problem string( CONCAT KASSERT_ASSERTION_LEVEL $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"none\">:0> $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"none\">:0> $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"exceptions\">:10> $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"exceptions\">:10> $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"light\">:20> $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"light\">:20> $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"normal\">:30> $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"normal\">:30> $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"light_communication\">:40> $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"light_communication\">:40> $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"heavy_communication\">:50> $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"heavy_communication\">:50> $<$<STREQUAL:${KAMPING_ASSERTION_LEVEL},\"heavy\">:60> $<$<STREQUAL:\"${KAMPING_ASSERTION_LEVEL}\",\"heavy\">:60> ) FetchContent_Declare( kassert GIT_REPOSITORY https://github.com/kamping-site/kassert GIT_TAG v0.1.0 ) FetchContent_MakeAvailable(kassert) target_link_libraries(kamping_base INTERFACE kassert::kassert) FetchContent_Declare( pfr GIT_REPOSITORY https://github.com/boostorg/pfr GIT_TAG 2.2.0 SYSTEM FIND_PACKAGE_ARGS 2.2.0 ) if (KAMPING_ENABLE_REFLECTION) if (KAMPING_REFLECTION_USE_SYSTEM_BOOST_FOR_PFR) find_package(Boost 1.75 COMPONENTS headers CONFIG) if (NOT Boost_FOUND) message( FATAL_ERROR \"Boost.PFR: No compatible Boost version found. Use KAMPING_REFLECTION_USE_SYSTEM_BOOST_FOR_PFR=OFF to use standalone Boost.PFR.\" ) else () message(STATUS \"Found Boost ${Boost_VERSION}: ${Boost_DIR}\") message(STATUS \"Using system Boost for Boost.PFR\") add_library(kamping_pfr INTERFACE) # when using system installed Boost, it does not provide a PFR target, so we have to link to the headers # target target_link_libraries(kamping_pfr INTERFACE Boost::headers) add_library(Boost::pfr ALIAS kamping_pfr) endif () else () FetchContent_MakeAvailable(pfr) if (pr_FOUND) message(STATUS \"Found Boost.PFR: ${pfr_DIR}\") else () message(STATUS \"Boost.PFR: building from source.\") endif () endif () target_link_libraries(kamping_base INTERFACE Boost::pfr) target_compile_definitions(kamping_base INTERFACE KAMPING_ENABLE_REFLECTION) message(STATUS \"Reflection: enabled\") else () message(STATUS \"Reflection: disabled\") endif () if (KAMPING_ENABLE_SERIALIZATION) FetchContent_Declare( cereal GIT_REPOSITORY https://github.com/USCiLab/cereal GIT_TAG v1.3.2 SYSTEM FIND_PACKAGE_ARGS 1.3.2 ) set(JUST_INSTALL_CEREAL ON) FetchContent_MakeAvailable(cereal) if (cereal_FOUND) message(STATUS \"Found cereal: ${cereal_DIR}\") else () message(STATUS \"Cereal: building from source.\") endif () target_link_libraries(kamping_base INTERFACE cereal::cereal) target_compile_definitions(kamping_base INTERFACE KAMPING_ENABLE_SERIALIZATION) message(STATUS \"Serialization: enabled\") else () message(STATUS \"Serialization: disabled\") endif () add_library(kamping::kamping ALIAS kamping) # Testing and examples are only built if this is the main project or if KAMPING_BUILD_EXAMPLES_AND_TESTS is set (OFF by # default) if (PROJECT_IS_TOP_LEVEL OR KAMPING_BUILD_EXAMPLES_AND_TESTS) add_subdirectory(examples) add_subdirectory(tests) endif ()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/karri",
            "repo_link": "https://github.com/molaupi/karri",
            "content": {
                "codemeta": "",
                "readme": "# KaRRi This repository contains the C++20 implementation of KaRRi, a state-of-the-art dispatcher for the dynamic taxi sharing problem with meeting points. KaRRi uses engineered on-the-fly shortest path queries based on bucket contraction hierarchies (BCHs) to allow for fast query times with maximum flexibility. For more information on KaRRi's novel techniques, we refer to the related publication: * Moritz Laupichler, and Peter Sanders. Fast Many-to-Many Routing for Dynamic Taxi Sharing with Meeting Points. 2024 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX), 2024\\. https://doi.org/10.1137/1.9781611977929.6 If you use KaRRi in your scientific publication, we ask that you cite the paper above. ## License All files in this repository except the files in the directory `External` are licensed under the MIT license. External libraries are licensed under their respective licenses. This source code is based on a fork of https://github.com/vbuchhold/routing-framework. Large parts of the project structure as well as basic data structures and shortest path algorithms are directly taken or adapted from the original framework. The copyright statements in each file state the respective author or authors of the file. ## Prerequisites To build KaRRi, you need to have some tools and libraries installed. On Debian and its derivatives (such as Ubuntu) the `apt-get` tool can be used: ``` $ sudo apt-get install build-essential $ sudo apt-get install cmake $ sudo apt-get install python3 python3-pip; pip3 install -r python_requirements.txt $ sudo apt-get install sqlite3 libsqlite3-dev $ sudo apt-get install zlib1g-dev ``` Next, you need to clone the libraries in the `External` subdirectory and build the `RoutingKit` library. To do so, type the following commands at the top-level directory of the framework: ``` $ git submodule update --init $ make -C External/RoutingKit lib/libroutingkit.so ``` ## Constructing KaRRi Input We provide bash scripts to generate the input data for the `Berlin-1pct`, `Berlin-10pct`, `Ruhr-1pct`, and `Ruhr-10pct` problem instances for the KaRRi algorithm. Inputs are generated based on OpenStreetMap (OSM) data, which requires the [osmium tool](https://osmcode.org/osmium-tool/). On Debian and its derivatives osmium can be installed using ``` sudo apt-get install osmium-tool ``` As an example, you can generate the input data for the `Berlin-1pct` instance by typing the following commands at the top-level directory: (Downloads multiple GiB of raw OSM data and requires at least 10 GiB of RAM.) ``` $ cd Publications/KaRRi $ bash DownloadGermanyOSMData.sh . $ bash FilterGermanyOSMData.sh . $ bash PreprocessOSMData.sh . Germany Berlin BoundaryPolygons $ bash GenerateKnownInstanceInputData.sh . Berlin-1pct pedestrian ``` To generate the input data for the other instances, simply replace `Berlin-1pct` with the instance name (`Berlin-10pct`, `Ruhr-1pct`, `Ruhr-10pct`) and replace `Berlin` with `Ruhr` for the Ruhr instances. ## Running KaRRi To run KaRRi in its default configuration (using collective last stop searches, sorted buckets, and SIMD instructions), use the provided bash script by typing the following commands at the top-level directory: ``` $ cd Publications/KaRRi $ bash RunKaRRiDefault.sh . <instance-name> <output-dir> ``` where `<instance-name>` can be any of `Berlin-1pct`, `Berlin-10pct`, `Ruhr-1pct`, and `Ruhr-10pct`, and `<output-dir>` is the path to the directory where the output files will be stored. We provide functions for a basic evaluation of results in `Publications/KaRRi/eval.R`.\n",
                "dependencies": "# ****************************************************************************** # MIT License # # Copyright (c) 2020 Valentin Buchhold # Copyright (c) 2023 Moritz Laupichler # # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the \"Software\"), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in all # copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE # SOFTWARE. # ****************************************************************************** cmake_minimum_required(VERSION 3.24 FATAL_ERROR) project(RoutingFramework CXX) # Determine the language standard. set(CMAKE_CXX_STANDARD 20) set(CMAKE_CXX_EXTENSIONS OFF) set(CMAKE_CXX_STANDARD_REQUIRED ON) # Flags when building for the Devel configuration. if (CMAKE_CXX_COMPILER_ID MATCHES GNU|Clang) set(CMAKE_CXX_FLAGS_DEVEL -O3) endif () if (NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE Devel) endif () # Enable the compiler to use extended instructions in generated code. if (CMAKE_CXX_COMPILER_ID MATCHES GNU|Clang) set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -march=native\") endif () option(DISABLE_AVX \"Disable use of instructions in the AVX extended instruction set.\" OFF) if (DISABLE_AVX) if (CMAKE_CXX_COMPILER_ID MATCHES GNU|Clang) message(\"Disabling use of AVX instructions.\") set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mno-avx\") endif () endif () # Request warnings. if (CMAKE_CXX_COMPILER_ID MATCHES GNU) set(FULL_WARNINGS \"-Werror\" \"-Wfatal-errors\" \"-Wpedantic\" \"-pedantic-errors\" \"-Wall\" \"-Wextra\" \"-ftemplate-backtrace-limit=1\" \"-Wno-unknown-pragmas\" \"-Wno-sign-compare\") elseif (CMAKE_CXX_COMPILER_ID MATCHES Clang) set(FULL_WARNINGS \"-Werror\" \"-Wall\" \"-Wextra\" \"-pedantic-errors\" \"-ferror-limit=1\" \"-ftemplate-backtrace-limit=1\" \"-Wno-sign-compare\") endif () include_directories(${CMAKE_SOURCE_DIR}) # Dependencies installed via package manager find_package(OpenMP) # Dependencies from git submodules and bundled libraries add_subdirectory(External) # Dependencies fetched at configure time: include(FetchContentDependencies.cmake) # Targets for this project are defined in directories Launchers and RawData set(TARGETS_DIRECTORIES Launchers RawData) foreach (TAR_DIR ${TARGETS_DIRECTORIES}) add_subdirectory(${TAR_DIR}) endforeach ()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kd-tree-python",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/KD-Tree-Python",
            "content": {
                "codemeta": "",
                "readme": "![Python](https://img.shields.io/badge/Python-3670A0?style=plastic&logo=python&logoColor=ffdd54) ![NumPy](https://img.shields.io/badge/Numpy-777BB4.svg?style=plastic&logo=numpy&logoColor=white) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/KD-Tree-Python?style=plastic&cached) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13384095.svg)](https://doi.org/10.5281/zenodo.13384095) # KD-Tree Implementation in Python This repository contains Python implementation of the kd-tree data structure and performing k-nearest neighbour search. Its Matlab implementation is located here: [KD-Tree-Matlab](https://github.com/Ramy-Badr-Ahmed/KD-Tree-Matlab) ### About The kd-tree is a space-partitioning data structure for organizing points in a k-dimensional space. > [Mathematica Link](https://reference.wolfram.com/language/ref/datastructure/KDTree.html) ### Scripts 1. `build_kdtree.py` > Builds a kd-tree from a set of points. 2. `nearest_neighbour_search.py` > Performs nearest neighbour search using the built kd-tree. 3. `hypercube_points.py` > Generates n-Dimensional Points Uniformly in an n-Dimensional Hypercube. ### Example Usage ```python from kdtree.build_kdtree import build_kdtree from kdtree.nearest_neighbor_search import nearest_neighbor_search from examples.hypercube_points import hypercube_points num_points = 5000 cube_size = 10 num_dimensions = 10 points = hypercube_points(num_points, cube_size, num_dimensions) hypercube_kdtree = build_kdtree(points.tolist()) query_point = np.random.rand(num_dimensions).tolist() nearest_point, nearest_dist, nodes_visited = nearest_neighbor_search(hypercube_kdtree, query_point) print(f\"Query point: {query_point}\") print(f\"Nearest point: {nearest_point}\") print(f\"Distance: {nearest_dist:.4f}\") print(f\"Nodes visited: {nodes_visited}\")\n",
                "dependencies": "numpy pytest\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/key",
            "repo_link": "https://github.com/keyproject/key",
            "content": {
                "codemeta": "",
                "readme": "# KeY -- Deductive Java Program Verifier [![Tests](https://github.com/KeYProject/key/actions/workflows/tests.yml/badge.svg)](https://github.com/KeYProject/key/actions/workflows/tests.yml) [![CodeQL](https://github.com/KeYProject/key/actions/workflows/codeql.yml/badge.svg)](https://github.com/KeYProject/key/actions/workflows/codeql.yml) [![CodeQuality](https://github.com/KeYProject/key/actions/workflows/code_quality.yml/badge.svg)](https://github.com/KeYProject/key/actions/workflows/code_quality.yml) This repository is the home of the interactive theorem prover KeY for formal verification and analysis of Java programs. KeY comes as a standalone GUI application, which allows you to verify the functional correctness of Java programs with respect to formal specifications formulated in the Java Modeling Language JML. Moreover, KeY can also be used as a library e.g. for symbolic program execution, first order reasoning, or test case generation. For more information, refer to * [The KeY homepage](https://key-project.org) * [The KeY book](https://www.key-project.org/thebook2/) * [The KeY developer documentation](https://keyproject.github.io/key-docs/) * KeY's success stories: * [Severe bug discovered in JDK sorting routine (TimSort)](http://www.envisage-project.eu/proving-android-java-and-python-sorting-algorithm-is-broken-and-how-to-fix-it/), * [Verification of `java.util.IdentityHashMap`](https://doi.org/10.1007/978-3-031-07727-2_4), * [Google Award for analysing a bug in `LinkedList`](https://www.key-project.org/2023/07/23/cwi-researchers-win-google-award-for-finding-a-bug-in-javas-linkedlist-using-key/) The current version of KeY is 2.12.2, licensed under GPL v2. Feel free to use the project templates to get started using KeY: * [For Verification Projects](https://github.com/KeYProject/verification-project-template) * [Using as a Library](https://github.com/KeYProject/key-java-example) * [Using as a Symbolic Execution Backend](https://github.com/KeYProject/symbex-java-example) ## Requirements * Hardware: >=2 GB RAM * Operating System: Linux/Unix, MacOSX, Windows * Java 17 or newer * Optionally, KeY can make use of the following binaries: * SMT Solvers: * [Z3](https://github.com/Z3Prover/z3#z3) * [cvc5](https://cvc5.github.io/) * [CVC4](https://cvc4.github.io/) * [Princess](http://www.philipp.ruemmer.org/princess.shtml) ## Content of the KeY folder This folder provides a [gradle](https://gradle.org)-managed project following [Maven's standard folder layout](https://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html). There are several subprojects in this folder. In general, every `key.*/` subproject contains a core component of KeY. Additional and optional components are in `keyext.*/` folders. The file `build.gradle` is the root build script describing the dependencies and common build tasks for all subprojects. `key.util`, `key.core` and `key.ui` are the base for the product \"KeY Prover\". Special care is needed if you plan to make changes here. ## Compile and Run KeY Assuming you are in the directory of this README file, you can create a runnable and deployable version with one of these commands: 1. With `./gradlew key.ui:run` you can run the user interface of KeY directly from the repository. Use `./gradlew key.ui:run --args='--experimental'` to enable experimental features. 2. Use `./gradlew classes` to compile KeY, which includes running JavaCC and Antlr. Likewise, use `./gradlew testClasses` if you also want to compile the JUnit test classes. 3. Test your installation with `./gradlew test`. Be aware that this will usually take multiple hours to complete. With `./gradlew testFast`, you can run a more lightweight test suite that should complete in a few minutes. You can select a specific test case with the `--tests` argument. Wildcards are allowed. ```sh ./gradlew :key.<subproject>:test --tests \"<class>.<method>\" ``` You can debug KeY by adding the `--debug-jvm` option, then attaching a debugger at `localhost:5005`. 4. You can create a single jar-version, aka *fat jar*, of KeY with ```sh ./gradlew :key.ui:shadowJar ``` The file is generated in `key.ui/build/libs/key-*-exe.jar`. 5. A distribution is build with ```sh ./gradlew :key.ui:installDist :key.ui:distZip ``` The distribution can be tested by calling `key.ui/install/key/bin/key.ui` and is zipped in `key.ui/build/distributions`. The distribution gives you potential of using single jar files. # Developing KeY * Quality is automatically assessed using [SonarQube](https://sonarqube.org) on each pull request. The results of the assessments (pass/fail) can be inspected in the checks section of the PR. The rules and quality gate are maintained by Alexander Weigl <weigl@kit.edu> currently. * More guideline and documentation for the KeY development can be found under [key-docs](https://keyproject.github.io/key-docs/devel/). # Issues and Bug Reports * For bug reports, please use the [issue tracker](https://github.com/KeYProject/key/issues) or send a mail to support@key-project.org. * For discussions, you may want to subscribe and use the mailing list <key-all@lists.informatik.kit.edu> or use [GitHub discussions](https://github.com/KeYProject/key/discussions). # Contributing to KeY Feel free to submit [pull requests](https://github.com/KeYProject/key/pulls) via GitHub. Pull requests are assessed using automatic tests, formatting and static source checkers, as well as a manual review by one of the developers. More guidelines and documentation for the KeY development can be found under [key-docs](https://keyproject.github.io/key-docs/devel/). # License Remark ``` This is the KeY project - Integrated Deductive Software Design Copyright (C) 2001-2011 Universität Karlsruhe, Germany Universität Koblenz-Landau, Germany and Chalmers University of Technology, Sweden Copyright (C) 2011-2024 Karlsruhe Institute of Technology, Germany Technical University Darmstadt, Germany Chalmers University of Technology, Sweden The KeY system is protected by the GNU General Public License. See LICENSE.TXT for details. ```\n",
                "dependencies": "import groovy.transform.Memoized plugins { //Support for IntelliJ IDEA //https://docs.gradle.org/current/userguide/idea_plugin.html id(\"idea\") //Support for Eclipse //https://docs.gradle.org/current/userguide/eclipse_plugin.html id(\"eclipse\") //support for Eclipse //Checks and adds the license header of the source files: // Task: `licenseMain' and `licenseFormatMain' //https://github.com/hierynomus/license-gradle-plugin id \"com.github.hierynomus.license-base\" version \"0.16.1\" //Generates reports on the license of used packages: Task `downloadLicenses' //Some Licenses requires an entry in the credits (MIT, BSD) id \"com.github.hierynomus.license-report\" version \"0.16.1\" // Gives `gradle dependencyUpdate` to show which dependency has a newer version // id \"com.github.ben-manes.versions\" version \"0.39.0\" // Code formatting id \"com.diffplug.spotless\" version \"7.0.3\" // EISOP Checker Framework id \"org.checkerframework\" version \"0.6.53\" id(\"org.sonarqube\") version \"6.1.0.5360\" } sonar { properties { property \"sonar.projectKey\", \"KeYProject_key\" property \"sonar.organization\", \"keyproject\" property \"sonar.host.url\", \"https://sonarcloud.io\" } } // Configure this project for use inside IntelliJ: idea { module { downloadJavadoc = false downloadSources = true } } static def getDate() { return new Date().format('yyyyMMdd') } // The $BUILD_NUMBER is an environment variable set by Jenkins. def build = System.env.BUILD_NUMBER == null ? \"-dev\" : \"-${System.env.BUILD_NUMBER}\" group = \"org.key-project\" version = \"2.12.4$build\" subprojects { apply plugin: \"java\" apply plugin: \"java-library\" apply plugin: \"maven-publish\" apply plugin: \"signing\" // GPG signing of artifacts, required by maven central apply plugin: \"idea\" apply plugin: \"eclipse\" apply plugin: \"com.diffplug.spotless\" apply plugin: \"checkstyle\" apply plugin: \"pmd\" apply plugin: \"org.checkerframework\" group = rootProject.group version = rootProject.version java { sourceCompatibility = 21 targetCompatibility = 21 } repositories { mavenCentral() maven { url 'https://git.key-project.org/api/v4/projects/35/packages/maven' } } dependencies { implementation(\"org.slf4j:slf4j-api:2.0.17\") implementation(\"org.slf4j:slf4j-api:2.0.17\") testImplementation(\"ch.qos.logback:logback-classic:1.5.18\") //compile group: 'org.apache.logging.log4j', name: 'log4j-api', version: '2.12.0' //compile group: 'org.apache.logging.log4j', name: 'log4j-core', version: '2.12.0' compileOnly(\"org.jspecify:jspecify:1.0.0\") testCompileOnly(\"org.jspecify:jspecify:1.0.0\") def eisop_version = \"3.49.1-eisop1\" compileOnly \"io.github.eisop:checker-qual:$eisop_version\" compileOnly \"io.github.eisop:checker-util:$eisop_version\" testCompileOnly \"io.github.eisop:checker-qual:$eisop_version\" checkerFramework \"io.github.eisop:checker-qual:$eisop_version\" checkerFramework \"io.github.eisop:checker:$eisop_version\" testImplementation(\"ch.qos.logback:logback-classic:1.5.18\") testImplementation(platform(\"org.junit:junit-bom:5.12.2\")) testImplementation (\"org.junit.jupiter:junit-jupiter-api\") testImplementation (\"org.junit.jupiter:junit-jupiter-api\") testImplementation (\"org.junit.jupiter:junit-jupiter-params\") testRuntimeOnly (\"org.junit.jupiter:junit-jupiter-engine\") testRuntimeOnly (\"org.junit.platform:junit-platform-launcher\") testImplementation project(':key.util') } tasks.withType(JavaCompile) { // Setting UTF-8 as the java source encoding. options.encoding = \"UTF-8\" // Setting the release to Java 21 options.release = 21 } tasks.withType(Javadoc) { failOnError = false options.addBooleanOption 'Xdoclint:none', true //options.verbose() options.encoding = 'UTF-8' options.addBooleanOption('html5', true) } tasks.withType(Test) {//Configure all tests systemProperty \"test-resources\", \"src/test/resources\" systemProperty \"testcases\", \"src/test/resources/testcase\" systemProperty \"TACLET_PROOFS\", \"tacletProofs\" systemProperty \"EXAMPLES_DIR\", file(\"$rootProject/key.ui/examples\") systemProperty \"RUNALLPROOFS_DIR\", \"$buildDir/report/runallproves\" systemProperty \"key.disregardSettings\", \"true\" maxHeapSize = \"4g\" forkEvery = 0 //default maxParallelForks = 1 // weigl: test on master } tasks.withType(Test) { useJUnitPlatform { includeEngines 'junit-vintage' includeEngines 'junit-jupiter' } } test { // Before we switched to JUnit 5, we used JUnit 4 with a customized discovery of test class. // This discovery called AutoSuite and searched in the compiled classes. AutoSuite was // necessary due to bugs caused in some execution order. // AutoSuites made the test order deterministic. A last known commit to find AutoSuite (for the case), // is 980294d04008f6b3798986bce218bac2753b4783. useJUnitPlatform { excludeTags \"owntest\", \"interactive\", \"performance\" } afterTest { desc, result -> logger.error \"${result.resultType}: ${desc.className}#${desc.name}\" } beforeTest { desc -> logger.error \"> ${desc.className}#${desc.name}\" } testLogging { outputs.upToDateWhen { false } showStandardStreams = true } } task testFast(type: Test) { group \"verification\" useJUnitPlatform { excludeTags \"slow\", \"performance\", \"interactive\" } testLogging { // set options for log level LIFECYCLE events \"failed\" exceptionFormat \"short\" // set options for log level DEBUG debug { events \"started\", \"skipped\", \"failed\" exceptionFormat \"full\" } // remove standard output/error logging from --info builds // by assigning only 'failed' and 'skipped' events info.events = [\"failed\", \"skipped\"] } } // The following two tasks can be used to execute main methods from the project // The main class is set via \"gradle -DmainClass=... execute --args ...\" // see https://stackoverflow.com/questions/21358466/gradle-to-execute-java-class-without-modifying-build-gradle task execute(type: JavaExec) { description 'Execute main method from the project. Set main class via \"gradle -DmainClass=... execute --args ...\"' group \"application\" mainClass.set(System.getProperty('mainClass')) classpath = sourceSets.main.runtimeClasspath } task executeInTests(type: JavaExec) { description 'Execute main method from the project (tests loaded). Set main class via \"gradle -DmainClass=... execute --args ...\"' group \"application\" mainClass.set(System.getProperty('mainClass')) classpath = sourceSets.test.runtimeClasspath } // findbugs { findbugsTest.enabled = false; ignoreFailures = true } pmd { pmdTest.enabled = false ignoreFailures = true toolVersion = \"6.53.0\" consoleOutput = false rulesMinimumPriority = 5 ruleSets = [\"category/java/errorprone.xml\", \"category/java/bestpractices.xml\"] } task pmdMainChanged(type: Pmd) { // Specify all files that should be checked def changedFiles = getChangedFiles() source pmdMain.source.filter { f -> f.getAbsoluteFile().toString() in changedFiles } classpath = checkstyleMain.classpath reports { html { enabled true outputLocation = file(\"build/reports/pmd/main_diff.html\") } xml { enabled true outputLocation = file(\"build/reports/pmd/main_diff.xml\") } } } checkstyle { toolVersion = \"10.6.0\" ignoreFailures = true configFile file(\"$rootDir/gradle/key_checks.xml\") showViolations = false // disable console output } task checkstyleMainChanged(type: Checkstyle) { // Specify all files that should be checked def changedFiles = getChangedFiles() source checkstyleMain.source.filter { f -> f.getAbsoluteFile().toString() in changedFiles } classpath = checkstyleMain.classpath //println(source.getFiles()) // Define the output folder of the generated reports reports { html { enabled true outputLocation = file(\"build/reports/checkstyle/main_diff.html\") } xml { enabled true outputLocation = file(\"build/reports/checkstyle/main_diff.xml\") } } } // tasks.withType(FindBugs) { // reports { // xml.enabled = false // html.enabled = true // } // } tasks.withType(Pmd) { reports { xml.getRequired().set(true) html.getRequired().set(true) } } task sourcesJar(type: Jar) { description = 'Create a jar file with the sources from this project' from sourceSets.main.allJava archiveClassifier = 'sources' } task javadocJar(type: Jar) { description = 'Create a jar file with the javadocs from this project' from javadoc archiveClassifier = 'javadoc' } license {//configures the license file header header = file(\"$rootDir/gradle/header\") mapping { //find styles here: // http://code.mycila.com/license-maven-plugin/#supported-comment-types java = \"SLASHSTAR_STYLE\" // DOUBLESLASH_STYLE javascript = \"SLASHSTAR_STYLE\" } mapping(\"key\", \"SLASHSTAR_STYLE\") } eclipse { //configures the generated .project and .classpath files. classpath { file { whenMerged { // This adds the exclude entry for every resource and antlr folder. //As eclipse is so stupid, that it does not distuinguish between resource and java folder correctly. entries.findAll { it.path.endsWith('src/test/antlr') }.each { it.excludes = [\"**/*.java\"] } entries.findAll { it.path.endsWith('/resources') }.each { it.excludes = [\"**/*.java\"] } } } } } spotless { // see https://github.com/diffplug/spotless/tree/main/plugin-gradle // optional: limit format enforcement to just the files changed by this feature branch // ratchetFrom 'origin/master' format('Key') { // define the files to apply `misc` to //target '*.gradle', '*.md', '.gitignore' target 'src/main/resources/**/*.key' trimTrailingWhitespace() //indentWithSpaces(4) // this does not really work endWithNewline() // TODO: license headers are problematic at the moment, // see https://git.key-project.org/key/key/-/wikis/KaKeY%202022-09-30 //licenseHeaderFile(\"$rootDir/gradle/header\", '\\\\s*\\\\\\\\\\\\w+') } antlr4 { target 'src/*/antlr4/**/*.g4' // default value, you can change if you want //licenseHeaderFile \"$rootDir/gradle/header\" } java { //target(\"*.java\") // don't need to set target, it is inferred from java // We ignore the build folder to avoid double checks and checks of generated code. targetExclude 'build/**' // allows us to use spotless:off / spotless:on to keep pre-formatted sections // MU: Only ... because of the eclipse(...) below, it is \"@formatter:off\" and \"@formatter:on\" // that must be used instead. toggleOffOn() removeUnusedImports() /* When new options are added in new versions of the Eclipse formatter, the easiest way is to export the new * style file from the Eclipse GUI and then use the CodeStyleMerger tool in * \"$rootDir/scripts/tools/checkstyle/CodeStyleMerger.java\" to merge the old and the new style files, * i.e. \"java CodeStyleMerger.java <oldStyleFile> <newStyleFile> keyCodeStyle.xml\". The tool adds all * entries with keys that were not present in the old file and optionally overwrites the old entries. The * file is output with ordered keys, such that the file can easily be diffed using git. */ eclipse().configFile(\"$rootDir/scripts/tools/checkstyle/keyCodeStyle.xml\") trimTrailingWhitespace() // not sure how to set this in the xml file ... //googleJavaFormat().aosp().reflowLongStrings() // note: you can use an empty string for all the imports you didn't specify explicitly, // '|' to join group without blank line, and '\\\\#` prefix for static imports importOrder('java|javax', 'de.uka', 'org.key_project', '', '\\\\#') // specific delimiter: normally just 'package', but spotless crashes for files in default package // (see https://github.com/diffplug/spotless/issues/30), therefore 'import' is needed. '//' is for files // with completely commented out code (which would probably better just be removed in future). if(project.name == 'recoder') { licenseHeaderFile(\"$rootDir/gradle/header-recoder\", '(package|import|//)') }else { licenseHeaderFile(\"$rootDir/gradle/header\", '(package|import|//)') } } } // checkerFramework { // checkers = [ // \"org.checkerframework.checker.nullness.NullnessChecker\", // ] // extraJavacArgs = [ // \"-AonlyDefs=^org\\\\.key_project\\\\.util\", // \"-Xmaxerrs\", \"10000\", // \"-Astubs=$projectDir/src/main/checkerframework\", // \"-Werror\", // \"-Aversion\", // ] // } afterEvaluate { // required so project.description is non-null as set by sub build.gradle publishing { publications { mavenJava(MavenPublication) { from components.java artifact sourcesJar artifact javadocJar pom { name = projects.name description = project.description url = 'https://key-project.org/' licenses { license { name = \"GNU General Public License (GPL), Version 2\" url = \"https://www.gnu.org/licenses/old-licenses/gpl-2.0.html\" } } developers { developer { id = 'key' name = 'KeY Developers' email = 'support@key-project.org' url = \"https://www.key-project.org/about/people/\" } } scm { connection = 'scm:git:git://github.com/keyproject/key.git' developerConnection = 'scm:git:git://github.com/keyproject/key.git' url = 'https://github.com/keyproject/key/' } } } } repositories { maven { /** * To be able to publish things on Maven Central, you need two things: * * (1) a JIRA account with permission on group-id `org.key-project` * (2) a keyserver-published GPG (w/o sub-keys) * * Your `$HOME/.gradle/gradle.properties` should like this: * ``` * signing.keyId=YourKeyId * signing.password=YourPublicKeyPassword * ossrhUsername=your-jira-id * ossrhPassword=your-jira-password * ``` * * You can test signing with `gradle sign`, and publish with `gradle publish`. * https://central.sonatype.org/publish/publish-guide/ */ if (project.version.endsWith(\"-SNAPSHOT\")) { name = \"mavenSnapshot\" url = \"https://s01.oss.sonatype.org/content/repositories/snapshots/\" credentials(PasswordCredentials) { username = project.properties.getOrDefault(\"ossrhUsername\", \"\") password = project.properties.getOrDefault(\"ossrhPassword\", \"\") } } else { name = \"mavenStaging\" url = \"https://s01.oss.sonatype.org/service/local/staging/deploy/maven2/\" credentials(PasswordCredentials) { username = project.properties.getOrDefault(\"ossrhUsername\", \"\") password = project.properties.getOrDefault(\"ossrhPassword\", \"\") } } } /* maven { // deployment to git.key-project.org name = \"GitlabPackages\" url \"https://git.key-project.org/api/v4/projects/35/packages/maven\" credentials(HttpHeaderCredentials) { if (System.getenv(\"TOKEN\") != null) { name = 'Private-Token' value = System.getenv(\"TOKEN\") } else { name = 'Job-Token' value = System.getenv(\"CI_JOB_TOKEN\") } } authentication { header(HttpHeaderAuthentication) } } */ } } signing { useGpgCmd() // works better than the Java implementation, which requires PGP keyrings. sign publishing.publications.mavenJava } } } task start { description \"Use :key.ui:run instead\" doFirst { println \"Use :key.ui:run instead\" } } // Generation of a JavaDoc across sub projects. task alldoc(type: Javadoc) { group \"documentation\" description \"Generate a JavaDoc across sub projects\" def projects = subprojects //key.ui javadoc is broken source projects.collect { it.sourceSets.main.allJava } classpath = files(projects.collect { it.sourceSets.main.compileClasspath }) destinationDir = file(\"${buildDir}/docs/javadoc\") if (JavaVersion.current().isJava9Compatible()) { //notworking on jenkins //options.addBooleanOption('html5', true) } configure(options) { //showFromPrivate() encoding = 'UTF-8' addBooleanOption 'Xdoclint:none', true // overview = new File( projectDir, 'src/javadoc/package.html' ) //stylesheetFile = new File( projectDir, 'src/javadoc/stylesheet.css' ) windowTitle = 'KeY API Documentation' docTitle = \"KeY JavaDoc ($project.version) -- ${getDate()}\" bottom = \"Copyright &copy; 2003-2023 <a href=\\\"http://key-project.org\\\">The KeY-Project</a>.\" use = true links += \"https://docs.oracle.com/en/java/javase/17/docs/api/\" links += \"http://www.antlr2.org/javadoc/\" links += \"http://www.antlr3.org/api/Java/\" links += \"https://www.antlr.org/api/Java/\" } } // Creates a jar file with the javadoc over all sub projects. task alldocJar(type: Zip) { dependsOn alldoc description 'Create a jar file with the javadoc over all sub projects' from alldoc archiveFileName = \"key-api-doc-${project.version}.zip\" destinationDirectory = file(\"$buildDir/distribution\") } //conditionally enable jacoco coverage when `-DjacocoEnabled=true` is given on CLI. def jacocoEnabled = System.properties.getProperty(\"jacocoEnabled\") ?: \"false\" if (jacocoEnabled.toBoolean()) { project.logger.lifecycle(\"Jacoco enabled. Test performance will be slower.\") apply from: rootProject.file(\"scripts/jacocokey.gradle\") } @Memoized def getChangedFiles() { // Get the target and source branch def anchor = \"git merge-base HEAD origin/main\".execute().getText() // Get list of all changed files including status def allFiles = \"git diff --name-status --diff-filter=dr $anchor\".execute().getText().split(\"\\n\") //println(\"Found ${allFiles.length} changed files\") // Remove the status prefix def files = new TreeSet<String>() for (file in allFiles) { if (file.length() > 1) { def a = file.substring(1).trim() if (!a.isBlank()) { files.add((\"$rootDir/\" + a).toString()) } } } // Return the list of touched files files }\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/keymaera-x",
            "repo_link": "https://github.com/LS-Lab/KeYmaeraX-release",
            "content": {
                "codemeta": "",
                "readme": "# KeYmaera X Theorem Prover for Hybrid Systems Self-driving cars, autonomous robots, modern airplanes, or robotic surgery: we increasingly entrust our lives to computers and therefore should strive for nothing but the highest safety standards - mathematical correctness proof. Proofs for such cyber-physical systems can be constructed with the KeYmaera X prover. As a _hybrid systems_ theorem prover, KeYmaera X analyzes the control program and the physical behavior of the controlled system together in _differential dynamic logic_. KeYmaera X features a minimal core of just about 2000 lines of code that isolates all soundness-critical reasoning. Such a small and simple prover core makes it much easier to trust verification results. Pre-defined and custom tactics built on top of the core drive automated proof search. KeYmaera X comes with a web-based front-end that provides a clean interface for both interactive and automated proving, highlighting the most crucial parts of a verification activity. Besides hybrid systems, KeYmaera X also supports the verification of _hybrid games_ in _differential game logic_. **More information** and precompiled binaries are available at [keymaerax.org](https://keymaerax.org/): * The [KeYmaera X Tutorial](https://keymaeraX.org/Xtutorial.html) * The [Logical Foundations of Cyber-Physical Systems](http://lfcps.org/lfcps/) textbook * The [KeYmaera X API Documentation](https://keymaerax.org/scaladoc) ## Installation 1. Install Java Runtime Environment version 11 or later, for example from [OpenJDK](https://openjdk.org/) 2. Optionally, install and set up [Wolfram Mathematica](https://www.wolfram.com/mathematica/) (version 10 or later) or [Wolfram Engine](http://www.wolfram.com/engine/). See below for more details on the different arithmetic solvers. 3. Download the file [keymaerax.jar](https://keymaerax.org/keymaerax.jar) 4. Configure KeYmaera X according to the **Configuration** section below. 5. Launch KeYmaera X by opening the console in the same directory as `keymaerax.jar` and running `java -jar keymaerax.jar`. You might need to specify additional arguments as explained in the **Configuration** section. For more details on installation, usage, and for troubleshooting steps, see the [Install section of the website](https://keymaerax.org/download.html). ## Configuration KeYmaera X requires a decision procedure for real arithmetic to finalize proofs. It is compatible with these arithmetic solvers: - Wolfram Mathematica - Wolfram Engine, a free alternative to Mathematica that requires an active internet connection. - The Z3 Theorem Prover, for which built-in binaries are included. This is the fallback when no other solver is configured. KeYmaera X is extensively tested with Mathematica and some features are only available when using Mathematica. After starting KeYmaera X you can configure arithmetic tools in the _KeYmaera X->Preferences_ menu. Depending on the operating system, Mathematica is installed in different locations. If KeYmaera X can't find your Mathematica installation, you need to manually specify the kernel and jlink paths when starting KeYmaera X using the `-mathkernel` and `-jlink` parameters. If you installed Mathematica at the [default path](https://reference.wolfram.com/language/tutorial/WolframSystemFileOrganization.html), the required values on the different platforms are: - Linux - `-mathkernel /usr/local/Wolfram/Mathematica/13.0/Executables/MathKernel` - `-jlink /usr/local/Wolfram/Mathematica/13.0/SystemFiles/Links/JLink/SystemFiles/Libraries/Linux-x86-64` - macOS - `-mathkernel /Applications/Mathematica.app/Contents/MacOS/MathKernel` - `-jlink /Applications/Mathematica.app/Contents/SystemFiles/Links/JLink/SystemFiles/Libraries/MacOSX-x86-64` - Windows - `-mathkernel \"C:\\Program Files\\Wolfram Research\\Mathematica\\13.0\\MathKernel.exe\"` - `-jlink \"C:\\Program Files\\Wolfram Research\\Mathematica\\13.0\\SystemFiles\\Links\\JLink\\SystemFiles\\Libraries\\Windows-x86-64\"` ## Building To compile KeYmaera X from source or set up a development environment, see [procedures.md](doc/procedures.md). More detailed but outdated instructions are available [in the wiki on GitHub](https://github.com/LS-Lab/KeYmaeraX-release/wiki/Building-Instructions). ## Publications KeYmaera X implements the uniform substitution calculus for differential dynamic logic in order to enable soundness assurance by way of a small trusted LCF-style kernel while still being amenable to automatic theorem proving. https://www.ls.cs.cmu.edu/publications.html 1. André Platzer. [A complete uniform substitution calculus for differential dynamic logic](https://doi.org/10.1007/s10817-016-9385-1). Journal of Automated Reasoning 59(2), pp. 219-266, 2017. Extended version of [CADE-25](https://doi.org/10.1007/978-3-319-21401-6_32). 2. André Platzer. [Logics of dynamical systems](https://doi.org/10.1109/LICS.2012.13). ACM/IEEE Symposium on Logic in Computer Science, LICS 2012, June 25-28, 2012, Dubrovnik, Croatia, pages 13-24. IEEE 2012. 3. Nathan Fulton, Stefan Mitsch, Jan-David Quesel, Marcus Völp and André Platzer. [KeYmaera X: An axiomatic tactical theorem prover for hybrid systems](https://doi.org/10.1007/978-3-319-21401-6_36). In Amy P. Felty and Aart Middeldorp, editors, International Conference on Automated Deduction, CADE-25, Berlin, Germany, Proceedings, LNCS. Springer, 2015. 4. Nathan Fulton, Stefan Mitsch, Brandon Bohrer and André Platzer. [Bellerophon: Tactical theorem proving for hybrid systems](https://doi.org/10.1007/978-3-319-66107-0_14). In Mauricio Ayala-Rincón and César Muñoz, editors, Interactive Theorem Proving, International Conference, ITP 2017, volume 10499 of LNCS, pp. 207-224. Springer, 2017. 5. André Platzer. [Logical Foundations of Cyber-Physical Systems](http://lfcps.org/lfcps/). Springer, Cham, 2018. [DOI](https://doi.org/10.1007/978-3-319-63588-0), [Videos](http://video.lfcps.org/) The soundness assurances provided by a small LCF-style kernel are further strengthened by a cross-verification of the soundness theorem for the uniform substitution calculus. 6. Brandon Bohrer, Vincent Rahli, Ivana Vukotic, Marcus Völp and André Platzer. [Formally verified differential dynamic logic](https://doi.org/10.1145/3018610.3018616). ACM SIGPLAN Conference on Certified Programs and Proofs, CPP 2017, Jan 16-17, 2017, Paris, France, pages 208-221, ACM, 2017. [Isabelle/HOL](https://github.com/LS-Lab/Isabelle-dL) and [Coq](https://github.com/LS-Lab/Coq-dL) A secondary goal of KeYmaera X is to also make it possible to implement extensions of differential dynamic logic, such as differential game logic for hybrid games as well as quantified differential dynamic logic for distributed hybrid systems: 7. André Platzer. [Differential game logic](https://doi.org/10.1145/2817824). ACM Trans. Comput. Log. 17(1), 2015. 8. André Platzer. [Differential hybrid games](https://doi.org/10.1145/3091123). ACM Trans. Comput. Log. 18(3), 2017. 9. André Platzer. [A complete axiomatization of quantified differential dynamic logic for distributed hybrid systems](https://doi.org/10.2168/LMCS-8(4:17)2012). Logical Methods in Computer Science 8(4), pages 1-44, 2012. KeYmaera X implements fast generalized uniform substitution algorithms, also cross-verified: 10. André Platzer. [Uniform substitution for differential game logic](https://doi.org/10.1007/978-3-319-94205-6_15). In Didier Galmiche, Stephan Schulz and Roberto Sebastiani, editors, Automated Reasoning, 9th International Joint Conference, IJCAR 2018, volume 10900 of LNCS, pp. 211-227. Springer 2018. 11. André Platzer. [Uniform substitution at one fell swoop](https://doi.org/10.1007/978-3-030-29436-6_25). In Pascal Fontaine, editor, International Conference on Automated Deduction, CADE-27, volume 11716 of LNCS, pp. 425-441. Springer, 2019. [Isabelle/HOL](http://isa-afp.org/entries/Differential_Game_Logic.html) Automatic proofs for differential equation invariants are based on: 12. André Platzer and Yong Kiam Tan. [Differential equation invariance axiomatization](https://doi.org/10.1145/3380825). J. ACM 67(1), 6:1-6:66, 2020. Extended version of [LICS'18](https://doi.org/10.1145/3209108.3209147). Liveness proofs for differential equations are based on: 13. Yong Kiam Tan and André Platzer. Yong Kiam Tan and André Platzer. [An axiomatic approach to existence and liveness for differential equations](https://doi.org/10.1007/s00165-020-00525-0). Formal Aspects of Computing 33(4), pp 461-518, 2021. Special issue for selected papers from [FM'19](https://doi.org/10.1007/978-3-030-30942-8_23). KeYmaera X uses the [Pegasus](http://pegasus.keymaeraX.org/) tool for invariant generation (which gets better when additional software is installed): 14. Andrew Sogokon, Stefan Mitsch, Yong Kiam Tan, Katherine Cordwell and André Platzer. [Pegasus: Sound continuous invariant generation](https://doi.org/10.1007/s10703-020-00355-z). Formal Methods in System Design, 58(1), pp. 5-41, 2022. Special issue for selected papers from [FM'19](https://doi.org/10.1007/978-3-030-30942-8_10). KeYmaera X implements the [ModelPlex](http://modelplex.net) method to ensure that verification results about models apply to cyber-physical system implementations. ModelPlex generates provably correct monitor conditions that, if checked to hold at runtime, are provably guaranteed to imply that the offline safety verification results about the CPS model apply to the present run of the actual CPS implementation. 15. Stefan Mitsch and André Platzer. [ModelPlex: Verified runtime validation of verified cyber-physical system models](https://doi.org/10.1007/s10703-016-0241-z). Formal Methods in System Design 49(1), pp. 33-74. 2016. Special issue for selected papers from [RV'14](https://doi.org/10.1007/978-3-319-11164-3_17). 16. Yong Kiam Tan, Stefan Mitsch and André Platzer. [Verifying switched system stability with logic](https://doi.org/10.1145/3501710.3519541) In Ezio Bartocci and Sylvie Putot, editors, Hybrid Systems: Computation and Control (part of CPS Week 2022), HSCC'22. Article No. 2, pp. 1-11. ACM, 2022. The design principles for the user interface of KeYmaera X are described in: 17. Stefan Mitsch and André Platzer. [The KeYmaera X proof IDE: Concepts on usability in hybrid systems theorem proving](https://doi.org/10.4204/EPTCS.240.5). In Catherine Dubois, Paolo Masci and Dominique Méry, editors, 3rd Workshop on Formal Integrated Development Environment F-IDE 2016, volume 240 of EPTCS, pp. 67-81, 2017. Model and proof management techniques are described in: 18. Stefan Mitsch. [Implicit and Explicit Proof Management in KeYmaera X](https://doi.org/10.4204/EPTCS.338.8) In José Proença and Andrei Paskevich, editors, 6th Workshop on Formal Integrated Development Environment F-IDE 2021, volume 338 of EPTCS 338, pp. 53-67, 2021. A comparison of KeYmaera X with its predecessor provers is described in: 19. Stefan Mitsch and André Platzer. [A Retrospective on Developing Hybrid System Provers in the KeYmaera Family: A Tale of Three Provers](https://doi.org/10.1007/978-3-030-64354-6_2). In Wolfgang Ahrendt et al., editors, Deductive Software Verification: Future Perspectives, volume 12345 of LNCS, pp. 21-64. Springer, 2020. ## Copyright and Licenses Copyright (C) 2014-2024 Carnegie Mellon University, Karlsruhe Institute of Technology Developed by Andre Platzer, Stefan Mitsch, Nathan Fulton, Brandon Bohrer, Yong Kiam Tan, Andrew Sogokon, Fabian Immler, Katherine Cordwell, Enguerrand Prebet, Joscha Mennicken, Tobias Erthal. With previous contributions by Nathan Fulton, Jan-David Quesel, Marcus Voelp, Ran Ji. See [COPYRIGHT.txt](COPYRIGHT.txt) for details. See [LICENSE.txt](LICENSE.txt) for the conditions of using this software. The KeYmaera X distribution contains external tools. A list of tools and their licenses can be found in [LICENSES_THIRD_PARTY.txt](LICENSES_THIRD_PARTY.txt). ## Contact KeYmaera X developers: keymaerax@keymaerax.org\n",
                "dependencies": "import java.io.{FileInputStream, InputStreamReader} import java.nio.charset.StandardCharsets import java.nio.file.{Files, Paths} import java.util.Properties ThisBuild / scalaVersion := \"2.13.13\" ThisBuild / version := \"5.1.1\" ThisBuild / scalacOptions ++= Seq( // Always show all non-suppressed warnings. See `scalac -Wconf:help` for more info. // https://www.scala-lang.org/2021/01/12/configuring-and-suppressing-warnings.html \"-Wconf:any:w\", \"-Ymacro-annotations\", ) ThisBuild / assemblyMergeStrategy := { // Multiple dependency jars have a module-info.class file in the same location. // Without custom rules, they cause merge conflicts with sbt-assembly. // Since we're building an uberjar, it should be safe to discard them (according to stackoverflow). // https://stackoverflow.com/a/55557287 case PathList(elements @ _*) if elements.last == \"module-info.class\" => MergeStrategy.discard // https://github.com/sbt/sbt-assembly#merge-strategy case path => val oldStrategy = (ThisBuild / assemblyMergeStrategy).value oldStrategy(path) } // Never execute tests in parallel across all sub-projects Global / concurrentRestrictions += Tags.limit(Tags.Test, 1) lazy val macros = project .in(file(\"keymaerax-macros\")) .disablePlugins(AssemblyPlugin) .settings( name := \"KeYmaeraX Macros\", libraryDependencies += \"org.scala-lang\" % \"scala-reflect\" % scalaVersion.value, ) lazy val core = project .in(file(\"keymaerax-core\")) .enablePlugins(BuildInfoPlugin) .dependsOn(macros) .settings( name := \"KeYmaeraX Core\", mainClass := Some(\"org.keymaerax.cli.KeymaeraxCore\"), libraryDependencies += \"org.scala-lang\" % \"scala-compiler\" % scalaVersion.value, libraryDependencies += \"cc.redberry\" %% \"rings.scaladsl\" % \"2.5.8\", libraryDependencies += \"com.github.scopt\" %% \"scopt\" % \"4.1.0\", libraryDependencies += \"com.lihaoyi\" %% \"fastparse\" % \"3.1.0\", libraryDependencies += \"io.github.classgraph\" % \"classgraph\" % \"4.8.174\", libraryDependencies += \"io.spray\" %% \"spray-json\" % \"1.3.6\", libraryDependencies += \"org.apache.commons\" % \"commons-configuration2\" % \"2.10.1\", libraryDependencies += \"org.apache.commons\" % \"commons-lang3\" % \"3.14.0\", libraryDependencies += \"org.reflections\" % \"reflections\" % \"0.10.2\", libraryDependencies += \"org.typelevel\" %% \"paiges-core\" % \"0.4.3\", libraryDependencies += \"org.typelevel\" %% \"spire\" % \"0.18.0\", // Logging // // KeYmaera X and some of its dependencies use slf4j for logging. // Slf4j is a facade for various logging frameworks called \"providers\". // Forcing slf4j 2 usually works fine for dependencies that create logs, // but forcing an slf4j 1 provider to use slf4j 2 will break things. // Since some dependencies updated to slf4j 2 already, we have to use an slf4j 2 provider // and force all other dependencies to use slf4j 2 as well via sbt's default version conflict resolution. // // https://github.com/jokade/slogging?tab=readme-ov-file#getting-started // https://www.baeldung.com/slf4j-with-log4j2-logback#Log4j2 libraryDependencies += \"biz.enef\" %% \"slogging-slf4j\" % \"0.6.2\", libraryDependencies += \"org.apache.logging.log4j\" % \"log4j-api\" % \"2.23.1\", libraryDependencies += \"org.apache.logging.log4j\" % \"log4j-core\" % \"2.23.1\", libraryDependencies += \"org.apache.logging.log4j\" % \"log4j-slf4j2-impl\" % \"2.23.1\", // A published version of scala-smtlib that works with Scala 2.13 // https://github.com/regb/scala-smtlib/issues/46#issuecomment-955691728 // https://mvnrepository.com/artifact/com.regblanc/scala-smtlib_2.13/0.2.1-42-gc68dbaa libraryDependencies += \"com.regblanc\" %% \"scala-smtlib\" % \"0.2.1-42-gc68dbaa\", Compile / run / mainClass := mainClass.value, assembly / mainClass := mainClass.value, assembly / assemblyJarName := s\"${normalizedName.value}-${version.value}.jar\", // Include version number as constant in source code buildInfoKeys := Seq[BuildInfoKey]( version, \"copyright\" -> Files.readString(Paths.get(\"COPYRIGHT.txt\")), \"license\" -> Files.readString(Paths.get(\"LICENSE.txt\")), \"licensesThirdParty\" -> Files.readString(Paths.get(\"LICENSES_THIRD_PARTY.txt\")), ), buildInfoPackage := \"org.keymaerax.info\", buildInfoOptions += BuildInfoOption.PackagePrivate, // Use Mathematica's JLink.jar as unmanaged dependency // The path is read from the property mathematica.jlink.path in the file local.properties Compile / unmanagedJars += { val properties = new Properties() try { val stream = new FileInputStream(\"local.properties\") val reader = new InputStreamReader(stream, StandardCharsets.UTF_8) properties.load(reader) } catch { case e: Throwable => throw new Exception(\"Failed to load file local.properties\", e) } val jlinkPath: String = properties.getProperty(\"mathematica.jlink.path\") if (jlinkPath == null) { throw new Exception(\"Property mathematica.jlink.path not found in file local.properties\") } file(jlinkPath) }, ) lazy val webui = project .in(file(\"keymaerax-webui\")) .dependsOn(macros, core) .settings( name := \"KeYmaeraX WebUI\", mainClass := Some(\"org.keymaerax.cli.KeymaeraxWebui\"), /// sqlite driver libraryDependencies += \"com.typesafe.slick\" %% \"slick\" % \"3.5.1\", libraryDependencies += \"com.typesafe.slick\" %% \"slick-codegen\" % \"3.5.1\", libraryDependencies += \"org.xerial\" % \"sqlite-jdbc\" % \"3.45.3.0\", // Akka libraryDependencies += \"com.typesafe.akka\" %% \"akka-http\" % \"10.5.3\", libraryDependencies += \"com.typesafe.akka\" %% \"akka-http-spray-json\" % \"10.5.3\", libraryDependencies += \"com.typesafe.akka\" %% \"akka-http-xml\" % \"10.5.3\", libraryDependencies += \"com.typesafe.akka\" %% \"akka-slf4j\" % \"2.8.5\", libraryDependencies += \"com.typesafe.akka\" %% \"akka-stream\" % \"2.8.5\", libraryDependencies += \"io.spray\" %% \"spray-json\" % \"1.3.6\", Compile / run / mainClass := mainClass.value, assembly / mainClass := mainClass.value, assembly / assemblyJarName := s\"${normalizedName.value}-${version.value}.jar\", // Include some resources as triggers for triggered execution (~) watchTriggers += baseDirectory.value.toGlob / \"src\" / \"main\" / \"resources\" / \"partials\" / \"*.html\", watchTriggers += baseDirectory.value.toGlob / \"src\" / \"main\" / \"resources\" / \"js\" / \"*.{js,map}\", watchTriggers += baseDirectory.value.toGlob / \"src\" / \"main\" / \"resources\" / \"*.html\", ///////////// // Testing // ///////////// libraryDependencies += \"org.scalatest\" %% \"scalatest\" % \"3.2.18\" % Test, libraryDependencies += \"org.scalamock\" %% \"scalamock\" % \"5.2.0\" % Test, // For generating HTML scalatest reports using the `-h <directory>` flag // See \"Using Reporters\" in https://www.scalatest.org/user_guide/using_scalatest_with_sbt // https://stackoverflow.com/a/59059383 // https://github.com/scalatest/scalatest/issues/1736 libraryDependencies += \"com.vladsch.flexmark\" % \"flexmark-all\" % \"0.64.8\" % Test, Test / parallelExecution := false, // set fork to true in order to run tests in their own Java process. // not forking avoids broken pipe exceptions in test reporter, but forking might become necessary in certain // multithreaded setups (see ScalaTest documentation) Test / fork := false, // record and report test durations Test / testOptions += Tests.Argument(TestFrameworks.ScalaTest, \"-oD\"), // report long-running tests (report every hour for tests that run longer than 1hr) Test / testOptions += Tests.Argument(TestFrameworks.ScalaTest, \"-W\", \"3600\", \"3600\"), resolvers ++= Resolver.sonatypeOssRepos(\"snapshots\"), // ScalaMeter testFrameworks += new TestFramework(\"org.scalameter.ScalaMeterFramework\"), logBuffered := false, ) // build KeYmaera X full jar with sbt clean assembly lazy val root = project .in(file(\".\")) .aggregate(macros, core, webui) .enablePlugins(ScalaUnidocPlugin) .disablePlugins(AssemblyPlugin) .settings( name := \"KeYmaeraX\", Compile / doc / scalacOptions ++= Seq(\"-doc-root-content\", \"rootdoc.txt\"), ScalaUnidoc / unidoc / scalacOptions += \"-Ymacro-expand:none\", ScalaUnidoc / unidoc / unidocProjectFilter := inAnyProject -- inProjects(macros), )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kinfit",
            "repo_link": "https://github.com/KinFit/KinFit.git",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/kramersmoyal",
            "repo_link": "https://github.com/LRydin/KramersMoyal",
            "content": {
                "codemeta": "",
                "readme": "[![DOI](https://joss.theoj.org/papers/10.21105/joss.01693/status.svg)](https://doi.org/10.21105/joss.01693) ![PyPI - License](https://img.shields.io/pypi/l/kramersmoyal) ![PyPI](https://img.shields.io/pypi/v/kramersmoyal) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/kramersmoyal) [![Build Status](https://github.com/LRydin/KramersMoyal/actions/workflows/CI.yml/badge.svg)](https://github.com/LRydin/KramersMoyal/actions/workflows/CI.yml) [![codecov](https://codecov.io/gh/LRydin/KramersMoyal/branch/master/graph/badge.svg)](https://codecov.io/gh/LRydin/KramersMoyal) [![Documentation Status](https://readthedocs.org/projects/kramersmoyal/badge/?version=latest)](https://kramersmoyal.readthedocs.io/en/latest/?badge=latest) # KramersMoyal `kramersmoyal` is a python package designed to obtain the Kramers-Moyal coefficients, or conditional moments, from stochastic data of any dimension. It employs kernel density estimations, instead of a histogram approach, to ensure better results for low number of points as well as allowing better fitting of the results. The [paper](https://doi.org/10.21105/joss.01693) is now officially published on [JOSS](https://joss.theoj.org/). The paper is also available [here](/paper/paper.pdf), or you can find it in the [ArXiv](https://arxiv.org/abs/1912.09737). # Installation To install `kramersmoyal`, just use `pip` ``` pip install kramersmoyal ``` Then on your favourite editor just use ```python from kramersmoyal import km ``` ## Dependencies The library depends on `numpy` and `scipy`. # A one-dimensional stochastic process A Jupyter notebook with this example can be found [here](/examples/kmc.ipynb) ## The theory Take, for example, the well-documented one-dimension Ornstein-Uhlenbeck process, also known as Va&#353;&#237;&#269;ek process, see [here](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process). This process is governed by two main parameters: the mean-reverting parameter &theta; and the diffusion parameter &sigma; <img src=\"/other/OU_eq.png\" title=\"Ornstein-Uhlenbeck process\" height=\"25\"/> which can be solved in various ways. For our purposes, recall that the drift coefficient, i.e., the first-order Kramers-Moyal coefficient, is given by ![](/other/inline_KM_1.png) and the second-order Kramers-Moyal coefficient is ![](/other/inline_KM_2.png), i.e., the diffusion. Generate an exemplary Ornstein-Uhlenbeck process with your favourite integrator, e.g., the [Euler-Maruyama](https://en.wikipedia.org/wiki/Euler%E2%80%93Maruyama_method) or with a more powerful tool from [`JiTCSDE`](https://github.com/neurophysik/jitcsde) found on GitHub. For this example let's take &theta;=.3 and &sigma;=.1, over a total time of 500 units, with a sampling of 1000 Hertz, and from the generated data series retrieve the two parameters, the drift -&theta;y(t) and diffusion &sigma;. ## Integrating an Ornstein-Uhlenbeck process Here is a short code on generating a Ornstein-Uhlenbeck stochastic trajectory with a simple Euler-Maruyama integration method ```python # integration time and time sampling t_final = 500 delta_t = 0.001 # The parameters theta and sigma theta = 0.3 sigma = 0.1 # The time array of the trajectory time = np.arange(0, t_final, delta_t) # Initialise the array y y = np.zeros(time.size) # Generate a Wiener process dw = np.random.normal(loc=0, scale=np.sqrt(delta_t), size=time.size) # Integrate the process for i in range(1,time.size): y[i] = y[i-1] - theta*y[i-1]*delta_t + sigma*dw[i] ``` From here we have a plain example of an Ornstein-Uhlenbeck process, always drifting back to zero, due to the mean-reverting drift &theta;. The effect of the noise can be seen across the whole trajectory. <img src=\"/other/fig1.png\" title=\"Ornstein-Uhlenbeck process\" height=\"200\"/> ## Using `kramersmoyal` Take the timeseries `y` and let's study the Kramers-Moyal coefficients. For this let's look at the drift and diffusion coefficients of the process, i.e., the first and second Kramers-Moyal coefficients, with an `epanechnikov` kernel ```python # The kmc holds the results, where edges holds the binning space kmc, edges = km(y, powers=2) ``` This results in <img src=\"/other/fig2.png\" title=\"Drift and diffusion terms of an Ornstein-Uhlenbeck process\" height=\"200\"/> Notice here that to obtain the Kramers-Moyal coefficients you need to divide `kmc` by the timestep `delta_t`. This normalisation stems from the Taylor-like approximation, i.e., the Kramers-Moyal expansion (`delta t` &rarr; 0). # A two-dimensional diffusion process A Jupyter notebook with this example can be found [here](/examples/kmc.ipynb) ## Theory A two-dimensional diffusion process is a stochastic process that comprises two ![](/other/inline_W.png) and allows for a mixing of these noise terms across its two dimensions. <img src=\"/other/2D-diffusion.png\" alt=\"2D-diffusion\" title=\"A 2-dimensional diffusion process\" height=\"60\" /> where we will select a set of state-dependent parameters obeying <img src=\"/other/parameters_2D-diffusion.png\" alt=\"2D-diffusion\" title=\"Specific parameters for the diffusion process\" height=\"70\" /> with ![](/other/inline_parameters_2D-diffusion_1.png) and ![](/other/inline_parameters_2D-diffusion_2.png). ## Choice of parameters As an example, let's take the following set of parameters for the drift vector and diffusion matrix ```python # integration time and time sampling t_final = 2000 delta_t = 0.001 # Define the drift vector N N = np.array([2.0, 1.0]) # Define the diffusion matrix g g = np.array([[0.5, 0.0], [0.0, 0.5]]) # The time array of the trajectory time = np.arange(0, t_final, delta_t) ``` ## Integrating a 2-dimensional process Integrating the previous stochastic trajectory with a simple Euler-Maruyama integration method ```python # Initialise the array y y = np.zeros([time.size, 2]) # Generate two Wiener processes with a scale of np.sqrt(delta_t) dW = np.random.normal(loc=0, scale=np.sqrt(delta_t), size=[time.size, 2]) # Integrate the process (takes about 20 secs) for i in range(1, time.size): y[i,0] = y[i-1,0] - N[0] * y[i-1,0] * delta_t + g[0,0]/(1 + np.exp(y[i-1,0]**2)) * dW[i,0] + g[0,1] * dW[i,1] y[i,1] = y[i-1,1] - N[1] * y[i-1,1] * delta_t + g[1,0] * dW[i,0] + g[1,1]/(1 + np.exp(y[i-1,1]**2)) * dW[i,1] ``` The stochastic trajectory in 2 dimensions for `10` time units (`10000` data points) <img src=\"/other/fig3.png\" alt=\"2D-diffusion\" title=\"2-dimensional trajectory\" height=\"280\" /> ## Back to `kramersmoyal` and the Kramers-Moyal coefficients First notice that all the results now will be two-dimensional surfaces, so we will need to plot them as such ```python # Choose the size of your target space in two dimensions bins = [100, 100] # Introduce the desired orders to calculate, but in 2 dimensions powers = np.array([[0,0], [1,0], [0,1], [1,1], [2,0], [0,2], [2,2]]) # insert into kmc: 0 1 2 3 4 5 6 # Notice that the first entry in [,] is for the first dimension, the # second for the second dimension... # Choose a desired bandwidth bw bw = 0.1 # Calculate the Kramers−Moyal coefficients kmc, edges = km(y, bw=bw, bins=bins, powers=powers) # The K−M coefficients are stacked along the first dim of the # kmc array, so kmc[1,...] is the first K−M coefficient, kmc[2,...] # is the second. These will be 2-dimensional matrices ``` Now one can visualise the Kramers-Moyal coefficients (surfaces) in green and the respective theoretical surfaces in black. (Don't forget to normalise: `kmc / delta_t`). <img src=\"/other/fig4.png\" alt=\"2D-diffusion\" title=\"2-dimensional Kramers-Moyal surfaces (green) and the theoretical surfaces (black)\" height=\"480\" /> # Contributions We welcome reviews and ideas from everyone. If you want to share your ideas or report a bug, open an [issue](https://github.com/LRydin/KramersMoyal/issues) here on GitHub, or contact us directly. If you need help with the code, the theory, or the implementation, do not hesitate to contact us, we are here to help. We abide to a [Conduct of Fairness](contributions.md). # TODOs Next on the list is - Include more kernels - Work through the documentation carefully # Changelog - Version 0.4.1 - Changing CI. Correcting `kmc[0,:]` normalisation. Various Simplifications. Bins as ints, powers as ints. - Version 0.4.0 - Added the documentation, first testers, and the Conduct of Fairness - Version 0.3.2 - Adding 2 kernels: `triagular` and `quartic` and extending the documentation and examples. - Version 0.3.1 - Corrections to the fft triming after convolution. - Version 0.3.0 - The major breakthrough: Calculates the Kramers-Moyal coefficients for data of any dimension. - Version 0.2.0 - Introducing convolutions and `gaussian` and `uniform` kernels. Major speed up in the calculations. - Version 0.1.0 - One and two dimensional Kramers-Moyal coefficients with an `epanechnikov` kernel. # Literature and Support ### Literature The study of stochastic processes from a data-driven approach is grounded in extensive mathematical work. From the applied perspective there are several references to understand stochastic processes, the Fokker-Planck equations, and the Kramers-Moyal expansion - Tabar, M. R. R. (2019). *Analysis and Data-Based Reconstruction of Complex Nonlinear Dynamical Systems.* Springer, International Publishing - Risken, H. (1989). *The Fokker-Planck equation.* Springer, Berlin, Heidelberg. - Gardiner, C.W. (1985). *Handbook of Stochastic Methods.* Springer, Berlin. You can find and extensive review on the subject [here](http://sharif.edu/~rahimitabar/pdfs/80.pdf)<sup>1</sup> ### History This project was started in 2017 at the [neurophysik](https://www.researchgate.net/lab/Klaus-Lehnertz-Lab-2) by Leonardo Rydin Gorjão, Jan Heysel, Klaus Lehnertz, and M. Reza Rahimi Tabar. Francisco Meirinhos later devised the hard coding to python. The project has had many supporters, such as Dirk Witthaut at the [Institute of Climate and Energy Systems (ICE)- Energiesystemtechnik (ICE-1), FZJ](https://www.fz-juelich.de/profile/witthaut_d), Benjamin Schäfer [Institute for Automation and Applied Informatics, KIT](https://www.iai.kit.edu/english/2154_4101.php), and Niklas Boers at [Technical University of Munich](https://www.professoren.tum.de/en/boers-niklas) & [Potsdam Institute for Climate Impact Research](https://www.pik-potsdam.de/members/boers), along with many others. ### Funding Helmholtz Association Initiative _Energy System 2050 - A Contribution of the Research Field Energy_ and the grant No. VH-NG-1025 and *STORM - Stochastics for Time-Space Risk Models* project of the Research Council of Norway (RCN) No. 274410. --- <sup>1</sup> Friedrich, R., Peinke, J., Sahimi, M., Tabar, M. R. R. *Approaching complexity by stochastic methods: From biological systems to turbulence,* [Phys. Rep. 506, 87-162 (2011)](https://doi.org/10.1016/j.physrep.2011.05.003).\n",
                "dependencies": "numpy scipy\nimport setuptools with open(\"README.md\", \"r\") as fh: long_description = fh.read() setuptools.setup( name=\"kramersmoyal\", version=\"0.4.1\", author=\"Leonardo Rydin Gorjão and Francisco Meirinhos\", author_email=\"leonardo.rydin@gmail.com\", description=\"Calculate Kramers-Moyal coefficients for stochastic process of any dimension, up to any order.\", long_description=long_description, long_description_content_type=\"text/markdown\", url=\"https://github.com/LRydin/KramersMoyal\", packages=setuptools.find_packages(), classifiers=[ \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: MIT License\", \"Operating System :: OS Independent\", ], python_requires='>=3.7', )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/lapy",
            "repo_link": "https://github.com/Deep-MI/lapy",
            "content": {
                "codemeta": "",
                "readme": "[![PyPI version](https://badge.fury.io/py/lapy.svg)](https://pypi.org/project/lapy/) # LaPy LaPy is an open-source Python package for differential geometry on triangle and tetrahedra meshes. It includes an FEM solver to estimate the Laplace, Poisson or Heat equations. Further functionality includes the computations of gradients, divergence, mean-curvature flow, conformal mappings, geodesics, ShapeDNA (Laplace spectra), and IO and plotting methods. LaPy is written purely in Python 3 without sacrificing speed as almost all loops are vectorized, drawing upon efficient and sparse mesh data structures. ## Contents: - **TriaMesh**: a class for triangle meshes offering various operations, such as fixing orientation, smoothing, curvature, boundary, quality, normals, and various efficient mesh datastructures (edges, adjacency matrices). IO from OFF, VTK and other formats. - **TetMesh**: a class for tetrahedral meshes (orientation, boundary, IO ...) - **Solver**: a class for linear FEM computation (Laplace stiffness and mass matrix, fast and sparse eigenvalue solver, anisotropic Laplace, Poisson) - **io**: module for IO of vertex functions and eigenvector files - **diffgeo**: module for gradients, divergence, mean curvature flow, etc. - **heat**: module for heat kernel and diffusion - **shapedna**: module for the ShapeDNA descriptor of surfaces and solids - **plot**: module for interactive visualizations (wrapping plotly) ## Usage: The LaPy package is a comprehensive collection of scripts, so we refer to the 'help' function and docstring of each module / function / class for usage info. For example: ``` import lapy as lp help(lp.TriaMesh) help(lp.Solver) ``` In the `examples` subdirectory, we provide several Jupyter notebooks that illustrate prototypical use cases of the toolbox. ## Installation: Use the following code to install the latest release of LaPy into your local Python package directory: `python3 -m pip install lapy` Use the following code to install the dev package in editable mode to a location of your choice: `python3 -m pip install --user --src /my/preferred/location --editable git+https://github.com/Deep-MI/Lapy.git#egg=lapy` Several functions, e.g. the Solver, require a sparse matrix decomposition, for which either the LU decomposition (from scipy sparse, default) or the faster Cholesky decomposition (from scikit-sparse cholmod, recommended) can be used. If the parameter flag use_cholmod is True, the code will try to import cholmod from the scikit-sparse package. If this fails, an error will be thrown. If you would like to use cholmod, you need to install scikit-sparse separately, as pip currently cannot install it (conda can). scikit-sparse requires numpy and scipy to be installed separately beforehand. ## API Documentation The API Documentation can be found at https://deep-mi.org/LaPy . ## References: If you use this software for a publication please cite both these papers: **[1]** Laplace-Beltrami spectra as 'Shape-DNA' of surfaces and solids. Reuter M, Wolter F-E, Peinecke N. Computer-Aided Design. 2006;38(4):342-366. http://dx.doi.org/10.1016/j.cad.2005.10.011 **[2]** BrainPrint: a discriminative characterization of brain morphology. Wachinger C, Golland P, Kremen W, Fischl B, Reuter M. Neuroimage. 2015;109:232-48. http://dx.doi.org/10.1016/j.neuroimage.2015.01.032 http://www.ncbi.nlm.nih.gov/pubmed/25613439 Shape-DNA [1] introduces the FEM methods and the Laplace spectra for shape analysis, while BrainPrint [2] focusses on medical applications. For Geodesics please also cite: [3] Crane K, Weischedel C, Wardetzky M. Geodesics in heat: A new approach to computing distance based on heat flow. ACM Transactions on Graphics. https://doi.org/10.1145/2516971.2516977 For non-singular mean curvature flow please cite: [4] Kazhdan M, Solomon J, Ben-Chen M. 2012. Can Mean-Curvature Flow be Modified to be Non-singular? Comput. Graph. Forum 31, 5, 1745-1754. https://doi.org/10.1111/j.1467-8659.2012.03179.x For conformal mapping please cite: [5] Choi PT, Lam KC, Lui LM. FLASH: Fast Landmark Aligned Spherical Harmonic Parameterization for Genus-0 Closed Brain Surfaces. SIAM Journal on Imaging Sciences, vol. 8, no. 1, pp. 67-94, 2015. https://doi.org/10.1137/130950008 We invite you to check out our lab webpage at https://deep-mi.org\n",
                "dependencies": "[build-system] requires = [ 'setuptools >= 61.0.0', 'numpy>=2', ] build-backend = 'setuptools.build_meta' [project] name = 'lapy' version = '1.2.0' description = 'A package for differential geometry on meshes (Laplace, FEM)' readme = 'README.md' license = {file = 'LICENSE'} requires-python = '>=3.9' authors = [ {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'}, ] maintainers = [ {name = 'Martin Reuter', email = 'martin.reuter@dzne.de'}, ] keywords = [ 'python', 'Laplace', 'FEM', 'ShapeDNA', 'BrainPrint', 'Triangle Mesh', 'Tetrahedra Mesh', 'Geodesics in Heat', 'Mean Curvature Flow', ] classifiers = [ 'Operating System :: Microsoft :: Windows', 'Operating System :: Unix', 'Operating System :: MacOS', 'Programming Language :: Python :: 3 :: Only', 'Programming Language :: Python :: 3.9', 'Programming Language :: Python :: 3.10', 'Programming Language :: Python :: 3.11', 'Programming Language :: Python :: 3.12', 'Natural Language :: English', 'License :: OSI Approved :: MIT License', 'Intended Audience :: Science/Research', ] dependencies = [ 'nibabel', 'numpy>=1.21', 'plotly', 'psutil', 'scipy!=1.13.0', ] [project.optional-dependencies] build = [ 'build', 'twine', ] chol = [ 'scikit-sparse', ] doc = [ 'furo!=2023.8.17', 'matplotlib', 'memory-profiler', 'numpydoc', 'sphinx!=7.2.*', 'sphinxcontrib-bibtex', 'sphinx-copybutton', 'sphinx-design', 'sphinx-gallery', 'sphinx-issues', 'pypandoc', 'nbsphinx', 'IPython', # For syntax highlighting in notebooks 'ipykernel', ] style = [ 'bibclean', 'codespell', 'pydocstyle[toml]', 'ruff', ] test = [ 'pytest', 'pytest-cov', 'pytest-timeout', ] all = [ 'lapy[build]', 'lapy[chol]', 'lapy[doc]', 'lapy[style]', 'lapy[test]', ] full = [ 'lapy[all]', ] [project.urls] homepage = 'https://Deep-MI.github.io/LaPy/dev/index.html' documentation = 'https://Deep-MI.github.io/LaPy/dev/index.html' source = 'https://github.com/Deep-MI/LaPy' tracker = 'https://github.com/Deep-MI/LaPy/issues' [project.scripts] lapy-sys_info = 'lapy.commands.sys_info:run' [tool.setuptools] include-package-data = false [tool.setuptools.packages.find] include = ['lapy*'] exclude = ['lapy*tests'] [tool.pydocstyle] convention = 'numpy' ignore-decorators = '(copy_doc|property|.*setter|.*getter|pyqtSlot|Slot)' match = '^(?!setup|__init__|test_).*\\.py' match-dir = '^lapy.*' add_ignore = 'D100,D104,D107' [tool.ruff] line-length = 100 extend-exclude = [ \"doc\", \".github\", \"data\", ] [tool.ruff.lint] # https://docs.astral.sh/ruff/linter/#rule-selection select = [ \"E\", # pycodestyle \"F\", # Pyflakes \"UP\", # pyupgrade \"B\", # flake8-bugbear \"I\", # isort # \"SIM\", # flake8-simplify ] [tool.ruff.lint.per-file-ignores] \"__init__.py\" = [\"F401\"] \"examples/*\" = [\"E501\"] # ignore too long lines in example ipynb [tool.pytest.ini_options] minversion = '6.0' addopts = '--durations 20 --junit-xml=junit-results.xml --verbose' filterwarnings = [] [tool.coverage.run] branch = true cover_pylib = false omit = [ '**/__init__.py', '**/lapy/_version.py', '**/lapy/commands/*', '**/tests/**', ] [tool.coverage.report] exclude_lines = [ 'pragma: no cover', 'if __name__ == .__main__.:', ] precision = 2\nfrom setuptools import setup setup()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/libertem",
            "repo_link": "https://github.com/LiberTEM/LiberTEM/",
            "content": {
                "codemeta": "",
                "readme": "|docs|_ |gitter|_ |azure|_ |github|_ |codeclimate|_ |precommit|_ |joss|_ |zenodo|_ |pypi|_ |condaforge|_ .. |docs| image:: https://img.shields.io/badge/%F0%9F%95%AE-docs-green.svg .. _docs: https://libertem.github.io/LiberTEM/ .. |gitter| image:: https://badges.gitter.im/join_chat.svg .. _gitter: https://gitter.im/LiberTEM/Lobby .. |azure| image:: https://dev.azure.com/LiberTEM/LiberTEM/_apis/build/status/LiberTEM.LiberTEM-data?branchName=master .. _azure: https://dev.azure.com/LiberTEM/LiberTEM/_build/latest?definitionId=4&branchName=master .. |zenodo| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.1477847.svg .. _zenodo: https://doi.org/10.5281/zenodo.1477847 .. |github| image:: https://img.shields.io/badge/GitHub-MIT-informational .. _github: https://github.com/LiberTEM/LiberTEM/ .. |codeclimate| image:: https://api.codeclimate.com/v1/badges/dee042f64380f64737e5/maintainability .. _codeclimate: https://codeclimate.com/github/LiberTEM/LiberTEM .. |joss| image:: https://joss.theoj.org/papers/10.21105/joss.02006/status.svg .. _joss: https://doi.org/10.21105/joss.02006 .. |precommit| image:: https://results.pre-commit.ci/badge/github/LiberTEM/LiberTEM/master.svg .. _precommit: https://results.pre-commit.ci/latest/github/LiberTEM/LiberTEM/master .. |pypi| image:: https://badge.fury.io/py/libertem.svg .. _pypi: https://pypi.org/project/libertem/ .. |condaforge| image:: https://anaconda.org/conda-forge/libertem/badges/version.svg .. _condaforge: https://anaconda.org/conda-forge/libertem LiberTEM is an open source platform for high-throughput distributed processing of large-scale binary data sets and live data streams using a modified `MapReduce programming model <https://en.wikipedia.org/wiki/MapReduce>`_. The current focus is `pixelated <https://en.wikipedia.org/wiki/Scanning_transmission_electron_microscopy#Universal_detectors>`_ scanning transmission electron microscopy (`STEM <https://en.wikipedia.org/wiki/Scanning_transmission_electron_microscopy>`_) \\[`MacLaren et al. (2016) <https://doi.org/10.1002/9783527808465.EMC2016.6284>`_, `Ophus (2019) <https://doi.org/10.1017/s1431927619000497>`_\\] and scanning electron beam diffraction data. MapReduce-like processing allows to specify an algorithm through two functions: One function that is mapped on portions of the input data, and another function that merges (reduces) a partial result from this mapping step into the complete result. A wide range of TEM and 4D STEM processing tasks can be expressed in this fashion, see `Applications`_. The UDF interface of LiberTEM offers a standardized, versatile API to decouple the mathematical core of an algorithm from details of data source, parallelism, and use of results. Mapping and merging can be performed in any order and with different subdivisions of the input data, including running parts of the calculation concurrently. That means the same implementation can be used in a wide range of modalities, including massive scaling on clusters. Since each merge step produces an intermediate result, this style of processing is suitable for displaying live results from a running calculation in a GUI application and for `processing live data streams <https://github.com/LiberTEM/LiberTEM-live>`_. A closed-loop feedback between processing and instrument control can be realized as well. See `User-defined functions <https://libertem.github.io/LiberTEM/udf.html>`_ for more details on the LiberTEM UDF interface. The LiberTEM back-end offers `high throughput and scalability <https://libertem.github.io/LiberTEM/architecture.html>`_ on PCs, single server nodes, clusters and cloud services. On clusters it can use fast distributed local storage on high-performance SSDs. That way it achieves `very high aggregate IO performance <https://libertem.github.io/LiberTEM/performance.html>`_ on a compact and cost-efficient system built from stock components. All CPU cores and CUDA devices in a system can be used in parallel. LiberTEM is supported on Linux, Mac OS X and Windows. Other platforms that allow installation of Python 3.7+ and the required packages will likely work as well. The GUI is running in a web browser. Installation ------------ The short version: .. code-block:: shell $ virtualenv -p python3 ~/libertem-venv/ $ source ~/libertem-venv/bin/activate (libertem-venv) $ python -m pip install \"libertem[torch]\" # optional for GPU support # See also https://docs.cupy.dev/en/stable/install.html (libertem-venv) $ python -m pip install cupy Please see `our documentation <https://libertem.github.io/LiberTEM/install.html>`_ for details! Alternatively, to run the `LiberTEM Docker image <https://libertem.github.io/LiberTEM/deployment/clustercontainer.html>`_: .. code-block:: shell $ docker run -p localhost:9000:9000 --mount type=bind,source=/path/to/your/data/,dst=/data/,ro ghcr.io/libertem/libertem or .. code-block:: shell $ singularity exec docker://ghcr.io/libertem/libertem /venv/bin/libertem-server Deployment for offline data processing on a single-node system for a local user is thoroughly tested and can be considered stable. Deployment on a cluster is experimental and still requires some additional work, see `Issue #105 <https://github.com/LiberTEM/LiberTEM/issues/105>`_. Back-end support for live data processing is still experimental as well, see https://github.com/LiberTEM/LiberTEM-live. Applications ------------ Since LiberTEM is programmable through `user-defined functions (UDFs) <https://libertem.github.io/LiberTEM/udf.html>`_, it can be used for a wide range of processing tasks on array-like data and data streams. The following applications have been implemented already: - Virtual detectors (virtual bright field, virtual HAADF, center of mass \\[`Krajnak et al. (2016) <https://doi.org/10.1016/j.ultramic.2016.03.006>`_\\], custom shapes via masks) - `Analysis of amorphous materials <https://libertem.github.io/LiberTEM/app/amorphous.html>`_ - `Strain mapping <https://libertem.github.io/LiberTEM-blobfinder/>`_ - `Off-axis electron holography reconstruction <https://libertem.github.io/LiberTEM-holo/>`_ - `Single Side Band ptychography <https://ptychography-4-0.github.io/ptychography/>`_ Some of these applications are available through an `interactive web GUI <https://libertem.github.io/LiberTEM/usage.html#gui-usage>`_. Please see `the applications section <https://libertem.github.io/LiberTEM/applications.html>`_ of our documentation for details! The Python API and user-defined functions (UDFs) can be used for complex operations such as arbitrary linear operations and other features like data export. Example Jupyter notebooks are available in the `examples directory <https://github.com/LiberTEM/LiberTEM/tree/master/examples>`_. If you are having trouble running the examples, please let us know by filing an issue or by `joining our Gitter chat <https://gitter.im/LiberTEM/Lobby>`_. LiberTEM is suitable as a high-performance processing backend for other applications, including live data streams. `Contact us <https://gitter.im/LiberTEM/Lobby>`_ if you are interested! LiberTEM is evolving rapidly and prioritizes features following user demand and contributions. Currently we are working on `live data processing <https://github.com/LiberTEM/LiberTEM-live>`_, improving application support for sparse data and event-based detectors, performance improvements for GPU processing, and implementing analysis methods for various applications of pixelated STEM and other large-scale detector data. If you like to influence the direction this project is taking, or if you'd like to `contribute <https://libertem.github.io/LiberTEM/contributing.html>`_, please join our `gitter chat <https://gitter.im/LiberTEM/Lobby>`_ and our `general mailing list <https://groups.google.com/forum/#!forum/libertem>`_. File formats ------------ LiberTEM currently opens most file formats used for pixelated STEM. See `our general information on loading data <https://libertem.github.io/LiberTEM/formats.html>`_ and `format-specific documentation <https://libertem.github.io/LiberTEM/reference/dataset.html#formats>`_ for more information! - Raw binary files - NumPy .npy binary files - Thermo Fisher EMPAD detector \\[`Tate et al. (2016) <https://doi.org/10.1017/S1431927615015664>`_\\] files - `Quantum Detectors MIB format <https://quantumdetectors.com/products/merlinem/>`_ - Nanomegas .blo block files - Direct Electron DE5 files (HDF5-based) and Norpix SEQ files for `DE-Series <https://directelectron.com/de-series-cameras/>`_ detectors - `Gatan K2 IS <https://web.archive.org/web/20180809021832/http://www.gatan.com/products/tem-imaging-spectroscopy/k2-camera>`_ raw format - Stacks of Gatan DM3 and DM4 files (via `openNCEM <https://github.com/ercius/openNCEM>`_) - Single-file Gatan DM4 scans when saved using C-ordering - FRMS6 from PNDetector pnCCD cameras \\[`Simson et al. (2015) <https://doi.org/10.1017/s1431927615011836>`_\\] (currently alpha, gain correction still needs UI changes) - FEI SER files (via `openNCEM <https://github.com/ercius/openNCEM>`_) - MRC (via `openNCEM <https://github.com/ercius/openNCEM>`_) - HDF5-based formats such as HyperSpy files, NeXus and EMD - TVIPS binary files - Sparse data in Raw CSR (compressed sparse row) format, as is possible to generate from event-based detectors - Please contact us if you are interested in support for an additional format! Live processing and detectors (experimental) -------------------------------------------- See `LiberTEM-live <https://libertem.github.io/LiberTEM-live/>`_! License ------- LiberTEM is licensed under the MIT license. Acknowledgements ---------------- We are very grateful for your continuing support for LiberTEM! See `the acknowledgement page <https://libertem.github.io/acknowledgements.html>`_ for a list of authors and contributors to LiberTEM and its subprojects. See also our info on `funding <https://libertem.github.io/#funding>`_ and `industry partners <https://libertem.github.io/#industry-partners>`_.\n",
                "dependencies": "[build-system] requires = [ \"hatchling\", \"hatch-fancy-pypi-readme\", ] build-backend = \"hatchling.build\" [project] name = \"libertem\" dynamic = [\"version\", \"readme\"] description = \"Open pixelated STEM framework\" license = { file = \"LICENSE\" } requires-python = \">=3.9.3\" authors = [ { name = \"the LiberTEM team\", email = \"libertem-dev@googlegroups.com\" }, ] keywords = [ \"electron\", \"microscopy\", ] classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Environment :: Console\", \"Environment :: Web Environment\", \"Intended Audience :: Developers\", \"Intended Audience :: End Users/Desktop\", \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: MIT License\", \"Natural Language :: English\", \"Operating System :: MacOS :: MacOS X\", \"Operating System :: Microsoft :: Windows\", \"Operating System :: POSIX :: Linux\", \"Programming Language :: JavaScript\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Programming Language :: Python :: 3.13\", \"Topic :: Scientific/Engineering\", \"Topic :: Scientific/Engineering :: Physics\", \"Topic :: Scientific/Engineering :: Visualization\", ] dependencies = [ \"autopep8\", \"click\", \"cloudpickle\", \"colorcet\", \"dask!=2023.6.1\", \"defusedxml\", \"distributed>=2.19.0\", \"h5py\", \"ipympl\", \"jsonschema\", \"jupyter_ui_poll\", \"matplotlib\", \"nbconvert\", \"nbformat\", \"ncempy>=1.10\", # Minimum constraints of numba for all Python versions we support # See https://numba.readthedocs.io/en/stable/release-notes-overview.html \"numba>=0.53;python_version < '3.10'\", \"numba>=0.55;python_version < '3.11'\", \"numba>=0.57;python_version < '3.12'\", \"numba>=0.59;python_version < '3.13'\", \"numba>=0.61;python_version < '3.14' and python_version >= '3.10'\", # for any future Python release, constrain numba to a recent version, # otherwise, version resolution might try to install an ancient version # that isn't constrained properly: \"numba>=0.61;python_version >= '3.14'\", \"numexpr!=2.8.6\", \"numpy\", \"opentelemetry-api\", \"pillow\", \"psutil\", \"pywin32!=226;platform_system==\\\"Windows\\\"\", \"scikit-image\", \"scikit-learn\", \"scipy>=1.4.1\", \"sparse\", \"sparseconverter>=0.4.0\", \"tblib\", \"threadpoolctl>=3.0\", \"tomli\", \"tornado>=5\", \"tqdm\", \"typing-extensions\", ] [project.optional-dependencies] bqplot = [ \"bqplot\", \"bqplot-image-gl\", \"ipython\", ] cupy = [ \"cupy\", ] hdbscan = [ \"hdbscan;( python_version!='3.11' or platform_system!='Windows')\", \"hdbscan<=0.8.30;( python_version=='3.11' and platform_system=='Windows')\", ] hdf5plugin = [ \"hdf5plugin\", ] torch = [ \"torch\", ] tracing = [ \"opentelemetry-distro\", \"opentelemetry-exporter-otlp\", ] [project.scripts] libertem-server = \"libertem.web.cli:main\" libertem-worker = \"libertem.executor.cli:main\" [project.urls] Repository = \"https://github.com/LiberTEM/LiberTEM\" Homepage = \"https://libertem.github.io/LiberTEM/\" [tool.hatch.version] path = \"src/libertem/__version__.py\" [tool.hatch.build.targets.wheel] artifacts = [ \"/src/libertem/_baked_revision.py\", ] [tool.hatch.build.targets.sdist] artifacts = [ \"/src/libertem/_baked_revision.py\", ] include = [ \"/src\", \"/tests\", \"/client/src\", \"/client/public\", \"/client/types\", \"/client/README.md\", \"/client/*.*js*\", \"/LICENSE\", \"/README.rst\", \"/pytest.ini\", \"/conftest.py\", \"/test_requirements.txt\", \"/override_requirements.txt\", \"/tox.ini\", \"/.flake8\", ] exclude = [ \"*.pyc\", \"*.nbi\", \"*.nbc\", \"__pycache__\", \".mypy_cache\", ] [tool.hatch.build.hooks.custom] # this enables hatch_build.py [tool.hatch.metadata.hooks.fancy-pypi-readme] \"content-type\" = \"text/x-rst\" [[tool.hatch.metadata.hooks.fancy-pypi-readme.fragments]] path = \"README.rst\" [[tool.hatch.metadata.hooks.fancy-pypi-readme.substitutions]] pattern = \":(cite|doc):`[^`]+` ?\" replacement = \"\" [tool.coverage.run] branch = true include = [ \"src/\" ] [tool.coverage.report] # Regexes for lines to exclude from consideration exclude_lines = [ # Have to re-enable the standard pragma \"pragma: no cover\", # Don't complain about missing debug-only code: \"def __repr__\", \"if self.debug\", # Don't complain about typing branches: \"if TYPE_CHECKING\", \"if typing.TYPE_CHECKING\", # Don't complain if tests don't hit defensive assertion code: \"raise AssertionError\", \"raise NotImplementedError\", # Don't complain if non-runnable code isn't run: \"if 0:\", \"if False:\", \"if __name__ == .__main__.:\", ]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/lightning-uq-box",
            "repo_link": "https://github.com/lightning-uq-box/lightning-uq-box",
            "content": {
                "codemeta": "",
                "readme": "<p align=\"center\"> <img src=\"https://github.com/lightning-uq-box/lightning-uq-box/blob/main/docs/_static/lettering.jpeg?raw=true\" alt=\"Lightning-UQ-Box logo\" width=\"600\" height=\"auto\" /> </p> [![docs](https://readthedocs.org/projects/lightning-uq-box/badge/?version=latest)](https://lightning-uq-box.readthedocs.io/en/latest/) [![style](https://github.com/lightning-uq-box/lightning-uq-box/actions/workflows/style.yaml/badge.svg)](https://github.com/lightning-uq-box/lightning-uq-box/actions/workflows/style.yaml) [![tests](https://github.com/lightning-uq-box/lightning-uq-box/actions/workflows/tests.yaml/badge.svg)](https://github.com/lightning-uq-box/lightning-uq-box/actions/workflows/tests.yaml) [![codecov](https://codecov.io/gh/lightning-uq-box/lightning-uq-box/branch/main/graph/badge.svg?token=oa3Z3PMVOg)](https://app.codecov.io/gh/lightning-uq-box/lightning-uq-box) [![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/lightning-uq-box/lightning-uq-box/blob/main/LICENSE) <a href=\"https://pytorch.org/get-started/locally/\"><img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white\"></a> <a href=\"https://pytorchlightning.ai/\"><img alt=\"Lightning\" src=\"https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white\"></a> &emsp; # lightning-uq-box The lightning-uq-box is a PyTorch library that provides various Uncertainty Quantification (UQ) techniques for modern neural network architectures. We hope to provide the starting point for a collaborative open source effort to make it easier for practitioners to include UQ in their workflows and remove possible barriers of entry. Additionally, we hope this can be a pathway to more easily compare methods across UQ frameworks and potentially enhance the development of new UQ methods for neural networks. *The project is currently under active development, but we nevertheless hope for early feedback, feature requests, or contributions. Please check the [Contribution Guide](https://lightning-uq-box.readthedocs.io/en/latest/contribute.html) for further information.* The goal of this library is threefold: 1. Provide implementations for a variety of Uncertainty Quantification methods for Modern Deep Neural Networks that work with a range of neural network architectures and have different theoretical underpinnings 2. Make it easy to compare UQ methods on a given dataset 3. Focus on reproducibility of experiments with minimum boiler plate code and standardized evaluation protocols To this end, each UQ-Method is essentially just a [Lightning Module](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) which can be used with a [Lightning Data Module](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) and a [Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html) to execute training, evaluation and inference for your desired task. The library also utilizes the [Lightning Command Line Interface (CLI)](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.cli.LightningCLI.html) for better reproducibility of experiments and setting up experiments at scale. # Theory Guide For a comprehensive document that provides more mathematical details for each method and generally forms the basis of our implementations, please see the [Theory Guide](./docs/api/Lightning_UQ_Box_Theory_Guide.pdf). As a living document, we plan to update it as the library encompasses more methods. If you have any questions, or find typos or errors, feel free to open an issue. # Installation The recommended way to install the latest released version is via pip, ```console pip install lightning-uq-box ``` For the latest development version you can run, ```console pip install git+https://github.com/lightning-uq-box/lightning-uq-box.git ``` The package is also available for installation via conda or spack. You can find instructions in the [documention](https://lightning-uq-box.readthedocs.io/en/latest/installation.html) # UQ-Methods In the tables that follow below, you can see what UQ-Method/Task combination is currently supported by the Lightning-UQ-Box via these indicators: - ✅ supported - ❌ not designed for this task - ⏳ in progress The implemented methods are of course not exhaustive, as the number of new methods keeps increasing. For an overview of methods that we are tracking or are planning to support, take a look at [this issue](https://github.com/lightning-uq-box/lightning-uq-box/issues/43). ## Classification of UQ-Methods The following sections aims to give an overview of different UQ-Methods by grouping them according to some commonalities. We agree that there could be other groupings as well and welcome suggestions to improve this overview. We also follow this grouping for the API documentation in the hopes to make navigation easier. ### Single Forward Pass Methods | UQ-Method | Regression | Classification | Segmentation | Pixel Wise Regression | |-----------------------------------------------|:----------:|:--------------:|:------------:|:---------------------:| | Quantile Regression (QR) | ✅ | ❌ | ❌ | ✅ | | Deep Evidential (DE) | ✅ | ⏳ | ⏳ | ✅ | | Mean Variance Estimation (MVE) | ✅ | ❌ | ❌ | ✅ | | ZigZag | ✅ | ✅ | ❌ | ❌ | | Mixture Density Networks | ✅ | ❌ | ❌ | ⏳ | ### Approximate Bayesian Methods | UQ-Method | Regression | Classification | Segmentation | Pixel Wise Regression | |-----------------------------------------------|:----------:|:--------------:|:------------:|:---------------------:| | Bayesian Neural Network VI ELBO (BNN_VI_ELBO) | ✅ | ✅ | ✅ | ⏳ | | Bayesian Neural Network VI (BNN_VI) | ✅ | ⏳ | ⏳ | ⏳ | | Deep Kernel Learning (DKL) | ✅ | ✅ | ❌ | ❌ | | Deterministic Uncertainty Estimation (DUE) | ✅ | ✅ | ❌ | ❌ | | Laplace Approximation (Laplace) | ✅ | ✅ | ❌ | ❌ | | Monte Carlo Dropout (MC-Dropout) | ✅ | ✅ | ✅ | ✅ | | Stochastic Gradient Langevin Dynamics (SGLD) | ✅ | ✅ | ⏳ | ⏳ | | Spectral Normalized Gaussian Process (SNGP) | ✅ | ✅ | ❌ | ❌ | | Stochastic Weight Averaging Gaussian (SWAG) | ✅ | ✅ | ✅ | ✅ | | Variational Bayesian Last Layer (VBLL) | ✅ | ✅ | ❌ | ❌ | | Deep Ensemble | ✅ | ✅ | ✅ | ✅ | | Masked Ensemble | ✅ | ✅ | ⏳ | ⏳ | | Density Uncertainty Layer | ✅ | ✅ | ❌ | ❌ | ### Generative Models | UQ-Method | Regression | Classification | Segmentation | Pixel Wise Regression | |-----------------------------------------------|:----------:|:--------------:|:------------:|:---------------------:| | Classification And Regression Diffusion (CARD)| ✅ | ✅ | ❌ | ❌ | | Probabilistic UNet | ❌ | ❌ | ✅ | ❌ | | Hierarchical Probabilistic UNet | ❌ | ❌ | ✅ | ❌ | | Variational Auto-Encoder (VAE) | ❌ | ❌ | ❌ | ✅ | ### Post-Hoc methods | UQ-Method | Regression | Classification | Segmentation | Pixel Wise Regression | |-----------------------------------------------|:----------:|:--------------:|:------------:|:---------------------:| | Test Time Augmentation (TTA) | ✅ | ✅ | ⏳ | ⏳ | | Temperature Scaling | ❌ | ✅ | ⏳ | ❌ | | Conformal Quantile Regression (Conformal QR) | ✅ | ❌ | ❌ | ⏳ | | Regularized Adaptive Prediction Sets (RAPS) | ❌ | ✅ | ❌ | ❌ | | Image to Image Conformal | ❌ | ❌ | ❌ | ✅ | # Tutorials We try to provide many different tutorials so that users can get a better understanding of implemented methods and get a feel for how they apply to different problems. Head over to the [tutorials](https://lightning-uq-box.readthedocs.io/en/latest/tutorial_overview.html) page to get started. These tutorials can also be launched in google colab if you navigate to the rocket icon at the top of a tutorial page. # Documentation We aim to provide an extensive documentation on all included UQ-methods that provide some theoretical background, as well as tutorials that illustrate these methods on toy datasets. # Citation If you use this software in your work, please cite our [paper](https://jmlr.org/papers/v26/24-2110.html): ```bibtex @article{JMLR:v26:24-2110, author = {Nils Lehmann and Nina Maria Gottschling and Jakob Gawlikowski and Adam J. Stewart and Stefan Depeweg and Eric Nalisnick}, title = {Lightning UQ Box: Uncertainty Quantification for Neural Networks}, journal = {Journal of Machine Learning Research}, year = {2025}, volume = {26}, number = {54}, pages = {1--7}, url = {http://jmlr.org/papers/v26/24-2110.html} } ```\n",
                "dependencies": "[build-system] requires = [ # setuptools 61+ required for pyproject.toml support \"setuptools>=61\", ] build-backend = \"setuptools.build_meta\" # https://packaging.python.org/en/latest/specifications/declaring-project-metadata/ [project] name = \"lightning-uq-box\" description = \"Lightning-UQ-Box: A toolbox for uncertainty quantification in deep learning\" readme = \"README.md\" requires-python = \">=3.10\" license = {file = \"LICENSE\"} authors = [ {name = \"Nils Lehmann\", email = \"n.lehmann@tum.de\"}, ] maintainers = [ {name = \"Nils Lehmann\", email = \"n.lehmann@tum.de\"}, ] keywords = [\"pytorch\", \"lightning\", \"uncertainty quantification\", \"conformal prediction\", \"bayesian deep learning\"] classifiers = [ \"Development Status :: 3 - Alpha\", \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: Apache Software License\", \"Operating System :: OS Independent\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Topic :: Scientific/Engineering :: Artificial Intelligence\", ] dependencies = [ # einops 0.3+ required for einops.repeat \"einops>=0.3\", # jsonargparse \"jsonargparse[signatures]>=4.28.0\", # lightning 2+ required for LightningCLI args + sys.argv support \"lightning>=2.4.0\", # matplotlib 3.5 required for Python 3.10 wheels \"matplotlib>=3.5\", # numpy 1.21.1+ required by Python 3.10 wheels \"numpy>=1.21.1\", # omegaconf \"omegaconf>=2.3.0\", # pandas 1.1.3+ required for Python 3.10 wheels \"pandas>=1.1.3\", # torch 1.12+ required by torchvision \"torch>=2.0\", # torchmetrics 0.10+ required for binary/multiclass/multilabel classification metrics \"torchmetrics>=1.2\", # torchvision 0.13+ required for torchvision.models._api.WeightsEnum \"torchvision>=0.16.1\", # scikit-learn \"scikit-learn>=1.3\", # for deep kernel learning and other GP models \"gpytorch>=1.11\", # Laplace Approximation \"laplace-torch>=0.2.1\", # Uncertainty toolbox metrics \"uncertainty-toolbox>=0.1.1\", # Kornia for Test Time Augmentations and other image processing \"kornia>=0.6.9\", # timm models for image classification and regression \"timm>=0.9.2\", # torchseg for segmentation, pixelwise regression \"torchseg>=0.0.1a1\", # saving pixelwise and segmentation predictions \"h5py>=3.12.1\", # exponential moving average for pytorch models \"ema-pytorch>=0.7.0\" ] dynamic = [\"version\"] [project.optional-dependencies] tests = [ ### Tests ### # pytest 7.3+ required for tmp_path_retention_policy \"pytest>=7.3\", # pytest-cov 4+ required for pytest 7.2+ compatibility \"pytest-cov>=4\", # pytest lazy fixture \"pytest-lazy-fixture>=0.6\", # hydra-core \"hydra-core>=1.3.2\", ## Style # Mypy \"mypy>=0.900\", # ruff 0.9+ required for 2025 style guide \"ruff>=0.9\", ] docs = [ ### Docs ### # sphinx \"sphinx>=4,<7\", # ipywidgets 7+ required by nbsphinx \"ipywidgets>=7\", # notebooks with sphinx \"nbsphinx>=0.8.5\", # reat the docs theme \"sphinx-book-theme>=1.0.1\", # Extension for markdown \"myst-parser>=2.0\", # toggle dropdowns \"sphinx-togglebutton>=0.3.2\", # jupytext notebook creation from .py files \"jupytext>=1.15.2\", # ipykernel to run notebooks \"ipykernel>=6.29.3\", ] [project.scripts] uq-box = \"lightning_uq_box.main:main\" [tool.pytest.ini_options] # Skip slow tests by default addopts = \"-m 'not slow'\" # https://docs.pytest.org/en/latest/how-to/capture-warnings.html filterwarnings = [ # Warnings raised by dependencies of dependencies, out of our control # Expected warnings # Lightning warns us about using num_workers=0, but it's faster on macOS \"ignore:The dataloader, .*, does not have many workers which may be a bottleneck:UserWarning\", # Lightning warns us about using the CPU when a GPU is available \"ignore:GPU available but not used.:UserWarning\", # https://github.com/Lightning-AI/lightning/issues/16756 \"ignore:Deprecated call to `pkg_resources.declare_namespace:DeprecationWarning\", \"ignore:pkg_resources is deprecated as an API.:DeprecationWarning:lightning_utilities.core.imports\", \"ignore:distutils Version classes are deprecated. Use packaging.version instead.\", # testing is run with num_workers=0 \"ignore:The .*dataloader.* does not have many workers which may be a bottleneck:UserWarning:lightning\", \"ignore:The .*dataloader.* does not have many workers which may be a bottleneck:lightning.fabric.utilities.warnings.PossibleUserWarning:lightning\", # Lightning CLI \"ignor:LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments\", # Lightning warns us if TensorBoard is not installed \"ignore:Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package:UserWarning\", # https://github.com/Lightning-AI/lightning/issues/18545 \"ignore:LightningCLI's args parameter is intended to run from within Python like if it were from the command line.:UserWarning\", # MLP has activation_fn as a module that can be changed, we use it in tests \"ignore:Unable to serialize instance ReLU()\", # Post Hoc methods TODO maybe more finegrained control in tests \"ignore:`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\", \"ignore:`training_step` returned `None`. If this was on purpose, ignore this warning...\", # MC-Dropout timm libraries implement dropout as functional an not module \"ignore:No dropout layers found in model, maybe dropout is implemented through nn.fucntional?\", # MacOS runner issues \"ignore:Skipping device Apple Paravirtual device that does not support Metal 2.0:UserWarning\", \"ignore:Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded:RuntimeWarning\", # Run tests on GPU \"ignore:The `srun` command is available on your system but is not used:lightning.fabric.utilities.warnings.PossibleUserWarning\", ] [tool.env] USE_IOMP = \"0\" # https://mypy.readthedocs.io/en/stable/config_file.html [tool.mypy] # Import discovery ignore_missing_imports = true exclude = [ \"(dist|docs|requirements|.*egg-info)/\", \"lightning_uq_box/uq_methods\" ] [tool.ruff] extend-include = [\"*.ipynb\"] fix = true [tool.ruff.format] skip-magic-trailing-comma = true [tool.ruff.lint] extend-select = [\"D\", \"I\", \"UP\"] [tool.ruff.lint.per-file-ignores] \"docs/**\" = [\"D\"] \"tests/**\" = [\"D\"] [tool.ruff.lint.isort] split-on-trailing-comma = false [tool.ruff.lint.pydocstyle] convention = \"google\" [tool.setuptools.dynamic] version = {attr = \"lightning_uq_box.__version__\"} [tool.setuptools.package-data] lightning_uq_box = [\"py.typed\"] [tool.setuptools.packages.find] include = [\"lightning_uq_box*\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/linmog",
            "repo_link": "https://jugit.fz-juelich.de/iek-10/public/optimization/linmog",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/llama",
            "repo_link": "https://github.com/alpaka-group/llama",
            "content": {
                "codemeta": "",
                "readme": "LLAMA - Low-Level Abstraction of Memory Access ============================================== [![ReadTheDocs](https://img.shields.io/badge/Docs-Read%20the%20Docs-blue.svg)](https://llama-doc.readthedocs.io) [![Doxygen](https://img.shields.io/badge/API-Doxygen-blue.svg)](https://alpaka-group.github.io/llama) [![Language](https://img.shields.io/badge/Language-C%2B%2B17-blue.svg)](https://isocpp.org/) [![Paper](https://img.shields.io/badge/Paper-Wiley%20Online%20Library-blue.svg)](https://doi.org/10.1002/spe.3077) [![Preprint](https://img.shields.io/badge/Preprint-arXiv-blue.svg)](https://arxiv.org/abs/2106.04284) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5901241.svg)](https://doi.org/10.5281/zenodo.5901241) [![codecov](https://codecov.io/gh/alpaka-group/llama/branch/develop/graph/badge.svg?token=B94D9G96FA)](https://codecov.io/gh/alpaka-group/llama) ![LLAMA](docs/images/logo_400x169.png) LLAMA is a cross-platform C\\++17/C\\++20 header-only template library for the abstraction of data layout and memory access. It separtes the view of the algorithm on the memory and the real data layout in the background. This allows for performance portability in applications running on heterogeneous hardware with the very same code. Documentation ------------- Our extensive user documentation is available on [Read the Docs](https://llama-doc.rtfd.io). It includes: * Installation instructions * Motivation and goals * Overview of concepts and ideas * Descriptions of LLAMA's constructs An API documentation is generated by [Doxygen](https://alpaka-group.github.io/llama/) from the C++ source. Please read the documentation on Read the Docs first! Supported compilers ------------------- LLAMA tries to stay close to recent developments in C++ and so requires fairly up-to-date compilers. The following compilers are supported by LLAMA and tested as part of our CI: | Linux | Windows | MacOS | |-----------------------------------------------------------------------------------------------|-----------------------------------------------------|----------------------------------| | g++ 10 - 13 </br> clang++ 12 - 17 </br> icpx (latest) </br> nvc++ 23.5 </br> nvcc 11.6 - 12.3 | Visual Studio 2022 </br> (latest on GitHub actions) | clang++ </br> (latest from brew) | Single header ------------- We create a single-header version of LLAMA on each commit, which you can find on the [single-header branch](https://github.com/alpaka-group/llama/tree/single-header). This also useful, if you would like to play with LLAMA on Compiler explorer: ```c++ #include <https://raw.githubusercontent.com/alpaka-group/llama/single-header/llama.hpp> ``` Contributing ------------ We greatly welcome contributions to LLAMA. Rules for contributions can be found in [CONTRIBUTING.md](CONTRIBUTING.md). Scientific publications ----------------------- We published an [article](https://doi.org/10.1002/spe.3077) on LLAMA in the journal of Software: Practice and Experience. We gave a talk on LLAMA at CERN's Compute Accelerator Forum on 2021-05-12. The video recording (starting at 40:00) and slides are available here on [CERN's Indico](https://indico.cern.ch/event/975010/). Mind that some of the presented LLAMA APIs have been renamed or redesigned in the meantime. We presented recently added features to LLAMA at the ACAT22 workshop as a [poster](https://indico.cern.ch/event/1106990/contributions/5096939/) and a contribution to the [proceedings](https://arxiv.org/abs/2302.08251). Additionally, we gave a [talk](https://indico.cern.ch/event/1106990/contributions/4991259/) at ACAT22 on LLAMA's instrumentation capabilities during a case study on [AdePT](https://github.com/apt-sim/AdePT), again, with a contribution to the [proceedings](https://arxiv.org/abs/2302.08252). Attribution ----------- If you use LLAMA for scientific work, please consider citing this project. We upload all releases to [Zenodo](https://zenodo.org/record/4911494), where you can export a citation in your preferred format. We provide a DOI for each release of LLAMA. Additionally, consider citing the [LLAMA paper](https://doi.org/10.1002/spe.3077). License ------- LLAMA is licensed under the [MPL-2.0](LICENSE).\n",
                "dependencies": "cmake_minimum_required (VERSION 3.18.3) project (llama CXX) # llama find_package(Boost 1.74.0 REQUIRED) find_package(fmt CONFIG QUIET) add_library(${PROJECT_NAME} INTERFACE) target_include_directories(${PROJECT_NAME} INTERFACE $<BUILD_INTERFACE:${PROJECT_SOURCE_DIR}/include> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>) target_compile_features(${PROJECT_NAME} INTERFACE cxx_std_17) target_link_libraries(${PROJECT_NAME} INTERFACE Boost::headers) add_compile_definitions(BOOST_ATOMIC_NO_LIB) # we don't need the compiled part in LLAMA or its examples if (fmt_FOUND) target_link_libraries(${PROJECT_NAME} INTERFACE fmt::fmt) if (CMAKE_CXX_COMPILER_ID STREQUAL \"NVHPC\") target_compile_definitions(${PROJECT_NAME} INTERFACE -DFMT_USE_NONTYPE_TEMPLATE_PARAMETERS=0 -DFMT_USE_NONTYPE_TEMPLATE_ARGS=0) endif() else() message(WARNING \"The fmt library was not found. You cannot use llama's dumping facilities.\") endif() # llama::llama to make subdirectory projects work add_library(${PROJECT_NAME}::${PROJECT_NAME} ALIAS ${PROJECT_NAME}) # llama IDE target to make source browsable/editable in IDEs file(GLOB_RECURSE llamaSources \"${CMAKE_CURRENT_SOURCE_DIR}/include/**\") add_custom_target(\"llamaIde\" SOURCES ${llamaSources}) source_group(TREE \"${CMAKE_CURRENT_LIST_DIR}/include/llama\" FILES ${llamaSources}) # default build type, see: https://www.kitware.com/cmake-and-the-default-build-type/ if(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES) message(STATUS \"Setting build type to 'Release' as none was specified.\") set_property(CACHE CMAKE_BUILD_TYPE PROPERTY VALUE \"Release\") endif() if (MSVC) # FIXME(bgruber): alpaka uses M_PI, so we need to make it available on MSVC. This may be fixed in alpaka 1.0.0. target_compile_definitions(llama INTERFACE _USE_MATH_DEFINES) target_compile_options(${PROJECT_NAME} INTERFACE /Zc:lambda) # needed in C++17 mode, remove when upgrading to C++20 endif() # CUDA include(CheckLanguage) check_language(CUDA) if (CMAKE_CUDA_COMPILER) enable_language(CUDA) set(CMAKE_CUDA_ARCHITECTURES \"35\" CACHE STRING \"CUDA architectures to compile for\") if (CMAKE_CUDA_COMPILER_ID STREQUAL \"Clang\") target_compile_definitions(${PROJECT_NAME} INTERFACE -DFMT_USE_FLOAT128=0) # Workaround for clang as CUDA compiler with libstdc++ 12 file(WRITE \"${CMAKE_CURRENT_BINARY_DIR}/clang_cuda_libstdc++12_workaround.hpp\" \"#include <__clang_cuda_runtime_wrapper.h>\\n\" \"#if defined(__clang__) && defined(__CUDA__) && defined(_GLIBCXX_RELEASE) && _GLIBCXX_RELEASE >= 12 && defined(__noinline__)\\n\" \"# undef __noinline__\\n\" \"#endif\\n\") target_compile_options(${PROJECT_NAME} INTERFACE -include \"${CMAKE_CURRENT_BINARY_DIR}/clang_cuda_libstdc++12_workaround.hpp\") endif() else() message(WARNING \"Could not find CUDA. Try setting CMAKE_CUDA_COMPILER. CUDA tests and examples are disabled.\") endif() # tests include(CMakeDependentOption) cmake_dependent_option(LLAMA_COMPILE_TESTS_AS_CUDA \"Sets the language of all test code to CUDA.\" OFF \"BUILD_TESTING;CMAKE_CUDA_COMPILER\" OFF) option(BUILD_TESTING \"\" OFF) include(CTest) if (BUILD_TESTING) option(LLAMA_SYSTEM_CATCH2 \"Use the system provided Catch2. This may result in a build failure, if Catch2 was compiled with a different C++ version as the LLAMA tests.\" ON) if (LLAMA_SYSTEM_CATCH2) find_package(Catch2 3.0.1 REQUIRED) include(Catch) else() # get Catch2 v3 and build it from source with the same C++ standard as the tests Include(FetchContent) FetchContent_Declare(Catch2 GIT_REPOSITORY https://github.com/catchorg/Catch2.git GIT_TAG v3.0.1) FetchContent_MakeAvailable(Catch2) target_compile_features(Catch2 PUBLIC cxx_std_20) include(Catch) # hide Catch2 cmake variables by default in cmake gui get_cmake_property(variables VARIABLES) foreach (var ${variables}) if (var MATCHES \"^CATCH_\") mark_as_advanced(${var}) endif() endforeach() endif() file(GLOB_RECURSE testSources \"${CMAKE_CURRENT_SOURCE_DIR}/tests/**\") add_executable(tests ${testSources}) catch_discover_tests(tests) source_group(TREE \"${CMAKE_CURRENT_LIST_DIR}/tests\" FILES ${testSources}) target_compile_features(tests PRIVATE cxx_std_20) if (LLAMA_COMPILE_TESTS_AS_CUDA) foreach(f ${testSources}) set_source_files_properties(${f} PROPERTIES LANGUAGE CUDA) endforeach() target_compile_options(tests PRIVATE --extended-lambda --expt-relaxed-constexpr) endif() if (MSVC) target_compile_options(tests PRIVATE /permissive- /constexpr:steps10000000 /diagnostics:caret) else() target_compile_options(tests PRIVATE -Wall -Wextra $<$<NOT:$<COMPILE_LANGUAGE:CUDA>>:-Werror=narrowing> -march=native) if (NOT CMAKE_CXX_COMPILER_ID STREQUAL \"NVHPC\") target_compile_options(tests PRIVATE -Wno-missing-braces) endif() endif() if (CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"AppleClang\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"IntelLLVM\") target_compile_options(tests PRIVATE -fconstexpr-steps=10000000) endif() if (CMAKE_CXX_COMPILER_ID STREQUAL \"IntelLLVM\") target_compile_options(tests PRIVATE -fp-model=precise) endif() if (CMAKE_CXX_COMPILER_ID STREQUAL \"NVHPC\") target_compile_options(tests PRIVATE --display_error_number -Wc,--pending_instantiations=0) target_compile_options(tests PRIVATE --diag_suppress=177) # disable: #177-D: variable \"<unnamed>::autoRegistrar72\" was declared but never referenced endif() if (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\") if (LLAMA_ENABLE_ASAN_FOR_TESTS AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 12.0 AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 14.0) target_compile_options(tests PRIVATE -Wno-maybe-uninitialized) # triggered inside std::function by std::regex endif() if (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 13.0 AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 14.0) target_compile_options(tests PRIVATE -Wno-dangling-reference) # triggered by an access involving RecordRef, so basically everywhere endif() endif() target_link_libraries(tests PRIVATE Catch2::Catch2WithMain llama::llama) option(LLAMA_ENABLE_ASAN_FOR_TESTS \"Enables address sanitizer for tests\" OFF) if (LLAMA_ENABLE_ASAN_FOR_TESTS) if (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\") target_compile_options(tests PRIVATE -fsanitize=address -fno-omit-frame-pointer) target_link_options (tests PRIVATE -fsanitize=address -fno-omit-frame-pointer) elseif(MSVC) target_compile_options(tests PRIVATE /fsanitize=address) target_link_options (tests PRIVATE /fsanitize=address) endif() endif() option(LLAMA_ENABLE_COVERAGE_FOR_TESTS \"Enables code coverage for tests\" OFF) if (LLAMA_ENABLE_COVERAGE_FOR_TESTS) if (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\") target_compile_options(tests PRIVATE --coverage) target_link_options(tests PRIVATE --coverage) endif() endif() endif() # examples option(LLAMA_BUILD_EXAMPLES \"Building (and installing) the examples\" OFF) if (LLAMA_BUILD_EXAMPLES) set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}) # general examples add_subdirectory(\"examples/vectoradd\") add_subdirectory(\"examples/nbody\") add_subdirectory(\"examples/nbody_code_comp\") add_subdirectory(\"examples/heatequation\") add_subdirectory(\"examples/viewcopy\") add_subdirectory(\"examples/bufferguard\") add_subdirectory(\"examples/raycast\") add_subdirectory(\"examples/bytesplit\") add_subdirectory(\"examples/bitpackint\") add_subdirectory(\"examples/bitpackfloat\") add_subdirectory(\"examples/memmap\") add_subdirectory(\"examples/stream\") add_subdirectory(\"examples/falsesharing\") add_subdirectory(\"examples/comptime\") # alpaka examples find_package(alpaka 1.0) if (_alpaka_FOUND) add_subdirectory(\"examples/alpaka/nbody\") add_subdirectory(\"examples/alpaka/vectoradd\") add_subdirectory(\"examples/alpaka/asyncblur\") add_subdirectory(\"examples/alpaka/pic\") add_subdirectory(\"examples/alpaka/daxpy\") add_subdirectory(\"examples/alpaka/babelstream\") else() message(WARNING \"Could not find alpaka. Alpaka examples are disabled.\") endif() # ROOT examples find_package(ROOT QUIET) if (ROOT_FOUND) add_subdirectory(\"examples/root/lhcb_analysis\") endif() # CUDA examples if (CMAKE_CUDA_COMPILER) add_subdirectory(\"examples/cuda/nbody\") add_subdirectory(\"examples/cuda/pitch\") add_subdirectory(\"examples/cuda/viewcopy\") endif() # SYCL examples find_package(IntelSYCL) if (IntelSYCL_FOUND) add_subdirectory(\"examples/sycl/nbody\") endif() endif() # install include(CMakePackageConfigHelpers) include(GNUInstallDirs) set(_llama_INSTALL_CMAKEDIR \"${CMAKE_INSTALL_LIBDIR}/cmake/llama\") configure_package_config_file ( \"${PROJECT_SOURCE_DIR}/cmake/llama-config.cmake.in\" \"${PROJECT_BINARY_DIR}/cmake/llama-config.cmake\" INSTALL_DESTINATION \"${_llama_INSTALL_CMAKEDIR}\") configure_file ( \"${PROJECT_SOURCE_DIR}/cmake/llama-config-version.cmake.in\" \"${PROJECT_BINARY_DIR}/cmake/llama-config-version.cmake\" @ONLY ) install( DIRECTORY \"${CMAKE_CURRENT_SOURCE_DIR}/include/llama\" DESTINATION \"include\" ) install( FILES \"${PROJECT_BINARY_DIR}/cmake/llama-config.cmake\" \"${PROJECT_BINARY_DIR}/cmake/llama-config-version.cmake\" DESTINATION \"${_llama_INSTALL_CMAKEDIR}\" )\n{\"$schema\":\"https://raw.githubusercontent.com/microsoft/vcpkg/master/scripts/vcpkg.schema.json\",\"name\":\"llama\",\"version\":\"0.6.0\",\"dependencies\":[\"alpaka\",\"boost-atomic\",\"boost-container\",\"boost-core\",\"boost-functional\",\"boost-mp11\",\"boost-smart-ptr\",{\"name\":\"boost-iostreams\",\"default-features\":false},\"catch2\",\"fmt\",\"tinyobjloader\",\"xsimd\"]}\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/llview",
            "repo_link": "https://github.com/FZJ-JSC/LLview",
            "content": {
                "codemeta": "",
                "readme": "<div align=\"left\"> <img src=\"docs/docs/images/LLview_logo.svg\" alt=\"LLview\" height=\"150em\"/> </div> [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10221407.svg)](https://doi.org/10.5281/zenodo.10221407) [![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0) # LLview <div align=\"center\"> <img src=\"docs/docs/images/LLview_thumbnail.png\" alt=\"LLview thumbnail\" width=\"100%\"/> </div> LLview is a set of software components to monitor clusters that are controlled by a resource manager and a scheduler system. Within its Job Reporting module, it provides detailed information of all the individual jobs running on the system. To achieve this, LLview connects to different sources in the system and collects data to present to the user via a web portal. For example, the resource manager provides information about the jobs, while additional daemons may be used to acquire extra information from the compute nodes, keeping the overhead at a minimum, as the metrics are obtained in the range of minutes apart. The LLview portal establishes a link between performance metrics and individual jobs to provide a comprehensive job reporting interface. ## Installation [Installation instructions](https://apps.fz-juelich.de/jsc/llview/docu/install/) can be found on LLview's [documentation page](https://llview.fz-juelich.de/docs). LLview presents its gathered data in a Web Portal created by [JURI](https://github.com/FZJ-JSC/JURI). ## Further Information For further information please see: http://llview.fz-juelich.de/docs Contact: [llview.jsc@fz-juelich.de](mailto:llview.jsc@fz-juelich.de) ## Copyright, License and CLA Copyright (c) 2023 Forschungszentrum Juelich GmbH, Juelich Supercomputing Centre http://llview.fz-juelich.de/ This is an open source software distributed under the GPLv3 license. More information see the LICENSE file at the top level. Contributions must follow the Contributor License Agreement. More information see the CONTRIBUTING.md file at the top level.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/lynx",
            "repo_link": "https://github.com/ajacquey/lynx",
            "content": {
                "codemeta": "",
                "readme": "<h1 align=\"center\"> <br> <a href=\"https://gitext.gfz-potsdam.de/ajacquey/lynx\">LYNX</a> <br> Lithosphere dYnamic Numerical toolboX <br> A MOOSE-based application <br> </h1> <h4 align=\"center\">A numerical simulator for modelling deformation of the lithosphere, based on <a href=\"http://mooseframework.org/\" target=\"blank\">MOOSE</a>.</h4> <p align=\"center\"> <a href=\"LICENSE\"> <img src=\"https://img.shields.io/badge/license-GPLv3-blue.svg\" alt=\"GPL License\"> </a> <a href=\"https://zenodo.org/record/3355376#.XUA2qi2Q1PU\"> <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3355376.svg\" alt=\"DOI\"> </a> </p> ## About LYNX (Lithosphere dYnamic Numerical toolboX) is a numerical simulator for modelling coupled Thermo-Hydro-Mechanical processes in the porous rocks of the lithosphere. The simulator is developed by [Antoine Jacquey](http://www.gfz-potsdam.de/en/staff/antoine-jacquey/) <a href=\"https://orcid.org/0000-0002-6259-4305\" target=\"orcid.widget\" rel=\"noopener noreferrer\" style=\"vertical-align:top;\"><img src=\"https://orcid.org/sites/default/files/images/orcid_16x16.png\" style=\"width:1em;margin-right:.5em;\" alt=\"ORCID iD icon\"></a> and [Mauro Cacace](http://www.gfz-potsdam.de/en/section/basin-modeling/staff/profil/mauro-cacace/) <a href=\"https://orcid.org/0000-0001-6101-9918\" target=\"orcid.widget\" rel=\"noopener noreferrer\" style=\"vertical-align:top;\"><img src=\"https://orcid.org/sites/default/files/images/orcid_16x16.png\" style=\"width:1em;margin-right:.5em;\" alt=\"ORCID iD icon\"></a> at the [GFZ Potsdam, German Research Centre for Geosciences](http://www.gfz-potsdam.de/en/home/) from the section [Basin Modelling](http://www.gfz-potsdam.de/en/section/basin-modeling/). LYNX is a MOOSE-based application. Visit the [MOOSE framework](http://mooseframework.org) page for more information. ## Licence LYNX is distributed under the [GNU GENERAL PUBLIC LICENSE v3](https://gitext.gfz-potsdam.de/ajacquey/lynx/blob/master/LICENSE). ## Getting Started #### Minimum System Requirements The following system requirements are from the MOOSE framework (see [Getting Started](http://mooseframework.org/getting-started/) for more information): * Compiler: C++11 Compliant GCC 4.8.4, Clang 3.4.0, Intel20130607 * Python 2.7+ * Memory: 16 GBs (debug builds) * Processor: 64-bit x86 * Disk: 30 GBs * OS: UNIX compatible (OS X, most flavors of Linux) #### 1. Setting Up a MOOSE Installation To install LYNX, you need first to have a working and up-to-date installation of the MOOSE framework. To do so, please visit the [Getting Started](http://mooseframework.org/getting-started/) page of the MOOSE framework and follow the instructions. If you encounter difficulties at this step, you can ask for help on the [MOOSE-users Google group](https://groups.google.com/forum/#!forum/moose-users). #### 2. Clone LYNX LYNX can be cloned directly from [GitLab](https://gitext.gfz-potsdam.de/ajacquey/lynx) using [Git](https://git-scm.com/). In the following, we refer to the directory `projects` which you created during the MOOSE installation (by default `~/projects`): cd ~/projects git clone https://gitext.gfz-potsdam.de/ajacquey/lynx.git cd ~/projects/lynx git checkout master *Note: the \"master\" branch of LYNX is the \"stable\" branch which is updated only if all tests are passing.* #### 3. Compile LYNX You can compile LYNX by following these instructions: cd ~/projects/lynx make -j4 #### 4. Test LYNX To make sure that everything was installed properly, you can run the tests suite of LYNX: cd ~/projects/lynx ./run_tests -j2 If all the tests passed, then your installation is working properly. You can now use the LYNX simulator! ## Usage To run LYNX from the command line with multiple processors, use the following command: mpiexec -n <nprocs> ~/projects/lynx/lynx-opt -i <input-file> Where `<nprocs>` is the number of processors you want to use and `<input-file>` is the path to your input file (extension `.i`). Information about the structure of the LYNX input files can be found in the documentation (link to follow). ## Cite If you use LYNX for your work please cite: * This repository: Jacquey, Antoine B., & Cacace, Mauro. (2019, July 30). LYNX: Lithosphere dYnamic Numerical toolboX, a MOOSE-based application (Version 1.0). Zenodo. http://doi.org/10.5281/zenodo.3355376 * The following research articles: Jacquey, Antoine B., & Cacace, Mauro. (2020). Multiphysics Modeling of a Brittle‐Ductile Lithosphere: 1. Explicit Visco‐Elasto‐Plastic Formulation and Its Numerical Implementation. Journal of Geophysical Research: Solid Earth. http://doi.org/10.1029/2019jb018474 Jacquey, Antoine B., & Cacace, Mauro. (2020). Multiphysics Modeling of a Brittle‐Ductile Lithosphere: 2. Semi‐brittle, Semi‐ductile Deformation and Damage Rheology. Journal of Geophysical Research: Solid Earth. http://doi.org/10.1029/2019jb018475 Please read the [CITATION](https://gitext.gfz-potsdam.de/ajacquey/lynx//blob/master/CITATION) file for more information. ## Publications using LYNX * Jacquey, Antoine B., & Cacace, Mauro. (2020). Multiphysics Modeling of a Brittle‐Ductile Lithosphere: 1. Explicit Visco‐Elasto‐Plastic Formulation and Its Numerical Implementation. Journal of Geophysical Research: Solid Earth. http://doi.org/10.1029/2019jb018474 * Jacquey, Antoine B., & Cacace, Mauro. (2020). Multiphysics Modeling of a Brittle‐Ductile Lithosphere: 2. Semi‐brittle, Semi‐ductile Deformation and Damage Rheology. Journal of Geophysical Research: Solid Earth. http://doi.org/10.1029/2019jb018475\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/maftools",
            "repo_link": "https://github.com/PoisonAlien/maftools",
            "content": {
                "codemeta": "",
                "readme": "<img src=\"vignettes/maftools_hex.svg\" align=\"left\" height=\"140\" /></a> ## maftools - An R package to summarize, analyze and visualize MAF files [![GitHub closed issues](https://img.shields.io/github/issues-closed-raw/poisonalien/maftools.svg)](https://github.com/poisonalien/maftools/issues) [![R-CMD-check](https://github.com/PoisonAlien/maftools/workflows/R-CMD-check/badge.svg)](https://github.com/PoisonAlien/maftools/actions) ## Introduction maftools is a comprehensive toolkit for processing somatic variants from cohort-based cancer genomic studies. maftools offers over 80 functions to perform the most commonly required tasks in cancer genomics, using [MAF](https://docs.gdc.cancer.gov/Data/File_Formats/MAF_Format/) as the only input file type. ## Installation ```{r} #Install from Bioconductor repository BiocManager::install(\"maftools\") #Install from GitHub repository BiocManager::install(\"PoisonAlien/maftools\") ``` ## Getting started: Vignette and a case study A complete documentation of maftools using [TCGA LAML](https://www.nejm.org/doi/full/10.1056/nejmoa1301689) as a case study can be found [here](http://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html). <p align=\"left\"> <img src=\"https://user-images.githubusercontent.com/8164062/97981605-d8a59500-1dd2-11eb-9f5e-cc808f7b3f91.gif\" height=\"320\" height=\"400\"> </p> ## Primary applications maftools is extremely easy to use, starting with importing an [MAF](https://docs.gdc.cancer.gov/Data/File_Formats/MAF_Format/) file along with the associated clinical data. Once the data is successfully imported, the resulting MAF object can be passed to various functions. Key applications include: - [Cohort summarization using oncoplots](https://bioconductor.org/packages/devel/bioc/vignettes/maftools/inst/doc/oncoplots.html#08_Combining_everything) - [Identify co-occurring and mutually exclusive events](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#91_Somatic_Interactions) - [Clinical enrichment analysis](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#96_Clinical_enrichment_analysis) - [Detect cancer driver genes](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#92_Detecting_cancer_driver_genes_based_on_positional_clustering) - [Infer tumor heterogeneity](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#99_Tumor_heterogeneity_and_MATH_scores) - [Analyze known cancer signaling pathways](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#98_Oncogenic_Signaling_Pathways) - [De-novo somatic signature analysis with NMF](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#9103_Signature_analysis) - [Compare two cohorts to identify differentially mutated genes](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#95_Comparing_two_cohorts_(MAFs)) - [Perform survival analysis and predict genesets associated with survival](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#942_Predict_genesets_associated_with_survival) - [Drug-gene interactions](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#97_Drug-Gene_Interactions) Besides the MAF files, maftools can handle sequencing alignment BAM files, copy number output from GISTIC and mosdepth. Please refer to the package documentation sections below to learn more. - [Generate personalized cancer report](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/cancer_hotspots.html) for known somatic [hotspots](https://www.cancerhotspots.org/) - [Sample mismatch and relatedness analysis](https://bioconductor.org/packages/devel/bioc/vignettes/maftools/inst/doc/maftools.html#12_Sample_swap_identification) - [Copy number analysis](https://bioconductor.org/packages/devel/bioc/vignettes/maftools/inst/doc/cnv_analysis.html) with [ASCAT](https://github.com/VanLoo-lab/ascat) and [mosdepth](https://github.com/brentp/mosdepth) Moreover, analyzing all 33 TCGA cohorts along with the harmonized clinical data is a breeze. - A single command [tcgaLoad](https://bioconductor.org/packages/release/bioc/vignettes/maftools/inst/doc/maftools.html#13_TCGA_cohorts) will import the desired TCGA cohort thereby avoiding costly time spent on data mining from public databases. - Please refer to an associated software package [TCGAmutations](https://github.com/PoisonAlien/TCGAmutations) that provides ready to use `MAF` objects for 33 TCGA cohorts and 2427 cell line profiles from CCLE - along with relevant clinical information for all sequenced samples. ## Citation **_Mayakonda A, Lin DC, Assenov Y, Plass C, Koeffler HP. 2018. Maftools: efficient and comprehensive analysis of somatic variants in cancer. [Genome Research](https://doi.org/10.1101/gr.239244.118). PMID: [30341162](https://www.ncbi.nlm.nih.gov/pubmed/?term=30341162)_** ## Useful links | File Fomats | Data portals | Annotation tools | |--------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------| | [Mutation Annotation Format](https://docs.gdc.cancer.gov/Data/File_Formats/MAF_Format/) | [TCGA](http://cancergenome.nih.gov) | [vcf2maf](https://github.com/mskcc/vcf2maf) - for converting your VCF files to MAF | | [Variant Call Format](https://en.wikipedia.org/wiki/Variant_Call_Format) | [ICGC](https://docs.icgc.org/) | [annovar2maf](https://github.com/PoisonAlien/annovar2maf) - for converting annovar output files to MAF | | ICGC [Simple Somatic Mutation Format](https://docs.icgc.org/submission/guide/icgc-simple-somatic-mutation-format/) | [Broad Firehose](https://gdac.broadinstitute.org/) | [bcftools csq](https://samtools.github.io/bcftools/howtos/csq-calling.html) - Rapid annotations of VCF files with variant consequences | | | [cBioPortal](https://www.cbioportal.org/) | [Annovar](https://annovar.openbioinformatics.org/en/latest/) | | | [PeCan](https://pecan.stjude.cloud/) | [Funcotator](https://gatk.broadinstitute.org/hc/en-us/articles/360037224432-Funcotator) | | | [CIViC](https://civicdb.org/home) - Clinical interpretation of variants in cancer | | | | [DGIdb](http://www.dgidb.org/) - Information on drug-gene interactions and the druggable genome | | ## Useful packages/tools Below are some more useful software packages for somatic variant analysis * [TRONCO](https://github.com/BIMIB-DISCo/TRONCO) - Repository of the TRanslational ONCOlogy library (R) * [dndscv](https://github.com/im3sanger/dndscv) - dN/dS methods to quantify selection in cancer and somatic evolution (R) * [cloneevol](https://github.com/hdng/clonevol) - Inferring and visualizing clonal evolution in multi-sample cancer sequencing (R) * [sigminer](https://github.com/ShixiangWang/sigminer) - Primarily for signature analysis and visualization in R. Supports `maftools` output (R) * [GenVisR](https://github.com/griffithlab/GenVisR) - Primarily for visualization (R) * [comut](https://github.com/vanallenlab/comut) - Primarily for visualization (Python) * [TCGAmutations](https://github.com/PoisonAlien/TCGAmutations) - pre-compiled curated somatic mutations from TCGA cohorts (from Broad Firehose and TCGA MC3 Project) that can be loaded into `maftools` (R) * [somaticfreq](<https://github.com/PoisonAlien/somaticfreq>) - rapid genotyping of known somatic hotspot variants from the tumor BAM files. Generates a browsable/sharable HTML report. (C) *** #### Powered By * [data.table](https://github.com/Rdatatable/data.table/wiki) at [warp speed](https://en.wikipedia.org/wiki/Warp_drive)\n",
                "dependencies": "Type: Package Package: maftools Title: Summarize, Analyze and Visualize MAF Files Version: 2.22.10 Date: 2021-04-30 Authors@R: person(given = \"Anand\", family = \"Mayakonda\", role = c(\"aut\", \"cre\"), email = \"anand_mt@hotmail.com\", comment = c(ORCID = \"0000-0003-1162-687X\")) Maintainer: Anand Mayakonda <anand_mt@hotmail.com> Description: Analyze and visualize Mutation Annotation Format (MAF) files from large scale sequencing studies. This package provides various functions to perform most commonly used analyses in cancer genomics and to create feature rich customizable visualzations with minimal effort. License: MIT + file LICENSE URL: https://github.com/PoisonAlien/maftools BugReports: https://github.com/PoisonAlien/maftools/issues Depends: R (>= 3.3) Imports: data.table, grDevices, methods, RColorBrewer, Rhtslib, survival, DNAcopy, pheatmap Suggests: berryFunctions, Biostrings, BSgenome, BSgenome.Hsapiens.UCSC.hg19, GenomicRanges, IRanges, knitr, mclust, MultiAssayExperiment, NMF, R.utils, RaggedExperiment, rmarkdown, S4Vectors LinkingTo: Rhtslib, zlibbioc VignetteBuilder: knitr biocViews: DataRepresentation, DNASeq, Visualization, DriverMutation, VariantAnnotation, FeatureExtraction, Classification, SomaticMutation, Sequencing, FunctionalGenomics, Survival Encoding: UTF-8 LazyData: TRUE NeedsCompilation: no Packaged: 2016-04-08 02:06:05 UTC; anand RoxygenNote: 7.3.1 SystemRequirements: GNU make, curl\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mallob",
            "repo_link": "https://github.com/domschrei/mallob",
            "content": {
                "codemeta": "",
                "readme": "[![status](https://joss.theoj.org/papers/700e9010c4080ffe8ae4df21cf1cc899/status.svg)](https://joss.theoj.org/papers/700e9010c4080ffe8ae4df21cf1cc899) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6890239.svg)](https://doi.org/10.5281/zenodo.6890239) # Mallob The platform **Mallob** (**Mal**leable **Lo**ad **B**alancer, or **Ma**ssively P**a**ra**ll**el **Lo**gic **B**ackend) is a distributed platform for processing automated reasoning tasks in modern large-scale HPC and cloud environments. Mallob primarily solves instances of the NP-complete _propositional satisfiability_ (SAT) problem - an essential building block at the core of automated reasoning and Symbolic AI. Mallob's flexible and decentralized approach to job scheduling allows to concurrently process many tasks of varying priority by different users. As such, Mallob can drastically improve your academic or industrial workflows tied to automated reasoning. Mallob's tightly integrated distributed general-purpose SAT solving engine, which we refer to as MallobSat, has received a large amount of attention, five gold medals of the International SAT Competition's Cloud Track in a row, and Amazon's proposition that our system is, \"by a _wide_ margin, the most powerful SAT solver on the planet\" (Byron Cook, [Amazon Science blog post](https://www.amazon.science/blog/automated-reasonings-scientific-frontiers)). Mallob is also the first distributed system that supports _incremental SAT solving_, i.e., interactive solving procedures over evolving formulas. Most recently, Mallob spearheaded the adoption of unsatisfiability proof checking in parallel and distributed SAT solving. Since proofs can serve as crucial witnesses for a result's correctness, Mallob is suitable even for the most critical use cases. <hr/> # Setup ## Prerequisites Note that we only support Linux as an operating system. (Some people have been developing and experimenting with Mallob within the WSL, but there seem to be issues related to inotify.) * CMake ≥ 3.11.4 * Open MPI (or another MPI implementation) * GDB * [jemalloc](https://github.com/jemalloc/jemalloc) ## Building ```bash # Only needed if building with MALLOB_APP_SAT (enabled by default). # For non-x86-64 architectures (ARM, POWER9, etc.), prepend `DISABLE_FPU=1` to \"bash\". ( cd lib && bash fetch_and_build_sat_solvers.sh ) # Build Mallob mkdir -p build cd build CC=$(which mpicc) CXX=$(which mpicxx) cmake -DCMAKE_BUILD_TYPE=RELEASE -DMALLOB_APP_SAT=1 -DMALLOB_USE_JEMALLOC=1 -DMALLOB_LOG_VERBOSITY=4 -DMALLOB_ASSERT=1 -DMALLOB_SUBPROC_DISPATCH_PATH=\\\"build/\\\" .. make; cd .. # Optional - only needed for on-the-fly LRAT checking ( cd lib && bash fetch_and_build_impcheck.sh && cp impcheck/build/impcheck_* ../build/ ) ``` Specify `-DCMAKE_BUILD_TYPE=RELEASE` for a release build or `-DCMAKE_BUILD_TYPE=DEBUG` for a debug build. You can use the following Mallob-specific build options: | Usage | Description | | ------------------------------------------- | ---------------------------------------------------------------------------------------------------------- | | -DMALLOB_ASSERT=<0/1> | Turn on assertions (even on release builds). Setting to 0 limits assertions to debug builds. | | -DMALLOB_JEMALLOC_DIR=path | If necessary, provide a path to a local installation of `jemalloc` where `libjemalloc.*` is located. | | -DMALLOB_LOG_VERBOSITY=<0..6> | Only compile logging messages of the provided maximum verbosity and discard more verbose log calls. | | -DMALLOB_SUBPROC_DISPATCH_PATH=\\\\\"path\\\\\" | Subprocess executables must be located under <path> for Mallob to find. (Use `\\\"build/\\\"` by default.) | | -DMALLOB_USE_ASAN=<0/1> | Compile with Address Sanitizer for debugging purposes. | | -DMALLOB_USE_GLUCOSE=<0/1> | Compile with support for Glucose SAT solver (disabled by default for licensing reasons, see below). | | -DMALLOB_USE_JEMALLOC=<0/1> | Compile with Scalable Memory Allocator `jemalloc` instead of default `malloc`. | | -DMALLOB_APP_KMEANS=<0/1> | Compile with K-Means clustering engine. | | -DMALLOB_APP_SAT=<0/1> | Compile with SAT solving engine. | | -DMALLOB_MAX_N_APPTHREADS_PER_PROCESS=<N> | Max. number of application threads (solver threads for SAT) per process to support. (max: 128) | | -DMALLOB_BUILD_LRAT_MODULES=<0/1> | Also build standalone LRAT checker | ## Docker We also provide a setup based on Docker containerization. Please consult the documentation in the `docker/` directory. ## Bash Autocompletion Mallob features bash auto-completion by pressing TAB. To enable this, execute this command from Mallob's base directory: source scripts/run/autocomplete.sh From this directory you can now autocomplete program options by pressing TAB once or twice. <hr/> # Testing **Note:** In its current state, the test suite expects that Mallob is built and run with OpenMPI, i.e., that `mpicc` and `mpicxx` (for building) and `mpirun` (for execution) link to OpenMPI executables on your system. For other MPI implementations, you may still be able to run the tests by removing or replacing the option `--oversubscribe` from the function `run()` in `scripts/run/systest_commons.sh`. In order to test that the system has been built and set up correctly, run the following command. ``` bash scripts/run/systest.sh mono drysched sched osc ``` This will locally run a suite of automated tests which cover the basic functionality of Mallob as a scheduler and as a SAT solving engine. To include Glucose in the tests, prepend the above command with \"GLUCOSE=1\". Running the tests takes a few minutes and in the end \"All tests done.\" should be output. <hr/> # Usage We first explain how to execute Mallob in general, then detail how to solve an isolated problem and then turn to solving many instances in a row or at the same time. ## General Given a single machine with two hardware threads per core, the following command executed in Mallob's base directory assigns one MPI process to each set of four physical cores (eight hardware threads) and then runs four solver threads on each MPI process. ``` RDMAV_FORK_SAFE=1; NPROCS=\"$(($(nproc)/8))\"; mpirun -np $NPROCS --bind-to core --map-by ppr:${NPROCS}:node:pe=4 build/mallob -t=4 $MALLOB_OPTIONS ``` Given a machine with `$nthreads` cores (and twice the number of hardware threads), the following command spawns a single process with one solver thread per core (per hardware thread): ``` RDMAV_FORK_SAFE=1; mpirun -np 1 --bind-to core --map-by ppr:1:node:pe=$nthreads build/mallob -t=$nthreads $MALLOB_OPTIONS RDMAV_FORK_SAFE=1; mpirun -np 1 --bind-to hwthread --map-by ppr:1:node:pe=$((2*$nthreads)) build/mallob -t=$((2*$nthreads)) $MALLOB_OPTIONS ``` Alternatively, only executing `build/mallob -t=$nthreads $MALLOB_OPTIONS` works as well in this case but does not pin threads to cores. You can always stop Mallob via Ctrl+C (interrupt signal) or by executing `killall mpirun` (or `killall build/mallob`). You can also specify the number of jobs to process (with `-J=$NUM_JOBS`) and/or the time to pass (with `-T=$TIME_LIMIT_SECS`) before Mallob terminates on its own. For exact and clean logging, you should not rely on a textfile in which you piped Mallob's output. Instead, specify a logging directory with `-log=<log-dir>` where separate sub-directories and files will be created for each worker / thread. This can be combined with the `-q` option to suppress Mallob's output to STDOUT. Verbosity of logging can be set with the `-v` option (as long as Mallob was compiled with the respective verbosity or higher, see `-DMALLOB_LOG_VERBOSITY` above). All further options of Mallob can be seen by executing Mallob with the `-h` option. (This also works without the `mpirun` prefix.) For running Mallob on distributed clusters, please also consult [our quickstart guide for clusters](docs/clusters.md) and/or the user documentation of your particular cluster. ## SAT Solving In general, in order to let Mallob process only a single instance, use option `-mono=$PROBLEM_FILE` where `$PROBLEM_FILE` is the path and file name of the problem to solve (DIMACS CNF format, possibly with .xz or .lzma compression, for SAT; whitespace-separated plain text file for K-Means). Specify the application of this instance with `-mono-app=sat` or `-mono-app=kmeans`. In this mode, all processes participate in solving, overhead is minimal, and Mallob terminates immediately after the job has been processed. Use option `-s2f=path/to/output.txt` (\"solution to file\") to write the result and (if applicable) the found satisfying assignment to a text file. ### Producing Proofs of Unsatisfiability To enable proof production, just set the option `-proof=path/to/final/compressed/prooffile.lrat` together with `-mono=path/to/input.cnf`. You also need to set a log directory with `-proof-dir=path/to/dir` where intermediate files will be written to on each machine. The final proof is output in compressed LRAT format. CaDiCaL is currently the only supported solver backend for parallel/distributed proof production. However, it is possible to employ other solvers as long as their only purpose is to find a satisfying assignment (i.e., exported clauses and unsatisfiability results from these solvers are discarded). See *Portfolio Tweaking* below. For instance, you can check the output proof with the standalone LRAT checker that comes with Mallob if you set `-DMALLOB_BUILD_LRAT_MODULES=1` at build time: ```bash build/standalone_lrat_checker path/to/input.cnf path/to/final/compressed/prooffile.lrat ``` Further synergies are possible; you can set `-uninvert=0` for Mallob and `--reversed` for the checker to avoid one entire I/O pass over the proof that \"uninverts\" its lines. You can use the [`drat-trim`](https://github.com/marijnheule/drat-trim) tool suite to decompress a proof; note that you need to `#define MODE 2` (1=DRAT, 2=LRAT) in `decompress.c` before building. ### Trusted solving *without* explicit proof production Proof production can be costly and bottlenecked by the I/O bandwidth of the single process which needs to write the entire proof. A more scalable approach is to check all proof information on-the-fly, without writing it to disk, and to transfer clause soundness guarantees across machines via cryptographic signatures. This is explained in detail in our [2024 SAT publication](https://dominikschreiber.de/papers/2024-sat-trusted-pre.pdf). Execute the command chain fetching and building [ImpCheck](https://github.com/domschrei/impcheck) in the above *Building* section. Then just use Mallob's option `-otfc=1` (without any `-proof*` options) to enable on-the-fly checking. Again, only CaDiCaL is supported for UNSAT whereas any solver can be employed for boosting satisfying assignments. By default, found satisfying assignments are also validated, which can be disabled via `-otfcm=0`. Log lines of the following shape are reporting a trusted result from a proof checking process: ``` c 0.851 0 <#11606> S0.0 TRUSTED checker reported UNSAT - sig c6c0a823f35ce38cdb31c9483dc98143 ``` ``` c 0.242 0 <#11607> S0.0 TRUSTED checker reported SAT - sig a43e47d81715035d79290d1a6acf05e8 ``` To be extra safe (e.g., if you are suspecting garbled or tampered-with logging output), execute the following command to validate the output signature: ```bash build/impcheck_confirm -formula-input=path/to/input.cnf -result=X -result-sig=SIG ``` where `X` is either 10 (for SAT) or 20 (for UNSAT), and `SIG` is the reported signature. **Note:** On-the-fly checking can also be used in Mallob's scheduled mode of operation. Globally unique clause IDs are ensured by adding a large offset times $x$ to a new solver thread's clause ID counter if the job has already experienced $x$ _balancing epochs_, i.e., received $x$ volume updates, since its initialization. The offset is chosen in such a way that 10,000 solvers each producing 10,000 clauses per second can run for 10,000 seconds before they may begin overlapping with clause IDs from the next balancing epoch. `ImpCheck` notices and reports any errors that would result from such a corner case. ### Portfolio Tweaking Mallob allows to customize the employed SAT solver backends and some of their flavors. This is done with the `-satsolver` option, which expects a string representing the solver backends to cycle over. The option also allows for a limited set of regular expression symbols. Here are some examples: ```bash ... -satsolver='c' # CaDiCaL only. ... -satsolver='kcl' # Kissat, CaDiCaL, Lingeling, Kissat, CaDiCaL, Lingeling, Kissat, ... ... -satsolver='k(c)*' # One Kissat, then only CaDiCaL (always put brackets around the argument of '*') ... -satsolver='kCLCLcl' # Capital letters indicate using truly incremental SAT solving for incremental jobs ... -satsolver='l+(c!){37}' # One Lingeling configured for satisfiable instances (+), then 37 LRAT-producing (!) CaDiCaLs, repeat ... -satsolver='(c!){37}k+((c!){37}l+)*' # As above, but replacing the 1st Lingeling with Kissat ``` ## Solve multiple instances in an orchestrated manner If you want to solve a fixed set of $n$ formulae or wish to evaluate Mallob's scheduling behavior with simulated jobs, follow these steps: * Write the set of formulae into a text file `$INSTANCE_FILE` (one line per path). * Configure the base properties of a job with a JSON file `$JOB_TEMPLATE`. For a plain job with default properties you can use `templates/job-template.json`. * Configure the behavior of each job-introducing process (\"client\") with a JSON file `$CLIENT_TEMPLATE`. You can find the simplest possible configuration in `templates/client-template.json` and a more complex randomized configuration in `templates/client-template-random.json`. Both files contain all necessary documentation to adjust them as desired. Then use these Mallob options: ``` -c=1 -ajpc=$MAX_PAR_JOBS -ljpc=$((2*$MAX_PAR_JOBS)) -J=$NUM_JOBS -job-desc-template=$INSTANCE_FILE -job-template=$JOB_TEMPLATE -client-template=$CLIENT_TEMPLATE -pls=0 ``` where `$NUM_JOBS` is set to $n$ (if it is larger than $n$, a client cycles through the provided job descriptions indefinitely). You can set `-sjd=1` to shuffle the provided job descriptions. You can also increase the number of client processes introducing jobs by increasing the value of `-c`. However, note that the provided configuration for active jobs in the system is applied to each of the clients independently, hence the formulae provided in the instance file are not split up among the clients but rather duplicated. ## Process jobs on demand This is the default and most general configuration of Mallob, i.e., without `-mono` or `-job-template` options. You can manually set the number of worker processes (`-w`) and the number of client processes introducing jobs (`-c`). By default, all processes are workers (`-w=-1`) and a single process is additionally a client (`-c=1`). The $k$ client processes are always the $k$ processes of the highest ranks, and they open up file system interfaces for introducing jobs and retrieving results at the directories `.api/jobs.0/` through `.api/jobs.`$k-1$`/`. ### Introducing a Job To introduce a job to the system, drop a JSON file in `.api/jobs.`$i$`/in/` (e.g., `.api/jobs.0/in/`) on the filesystem of the according PE structured like this: ``` { \"application\": \"SAT\", \"user\": \"admin\", \"name\": \"test-job-1\", \"files\": [\"/path/to/difficult/formula.cnf\"], \"priority\": 0.7, \"wallclock-limit\": \"5m\", \"cpu-limit\": \"10h\", \"arrival\": 10.3, \"dependencies\": [\"admin.prereq-job1\", \"admin.prereq-job2\"], \"incremental\": false } ``` Here is a brief overview of all required and optional fields in the JSON API: | Field name | Required? | Value type | Description | | ----------------- | :-------: | -----------: | -------------------------------------------------------------------------------------------------------------- | | user | **yes** | String | A string specifying the user who is submitting the job | | name | **yes** | String | A user-unique name for this job (increment) | | files | **yes*** | String array | File paths of the input to solve. For SAT, this must be a single (text file or compressed file or named pipe). | | priority | **yes*** | Float > 0 | Priority of the job (higher is more important) | | application | **yes** | String | Which kind of problem is being solved; currently either of \"SAT\" or \"DUMMY\" (default: DUMMY) | | wallclock-limit | no | String | Job wallclock limit: combination of a number and a unit (ms/s/m/h/d) | | cpu-limit | no | String | Job CPU time limit: combination of a number and a unit (ms/s/m/h/d) | | arrival | no | Float >= 0 | Job's arrival time (seconds) since program start; ignore job until then | | max-demand | no | Int >= 0 | Override the max. number of MPI processes this job should receive at any point in time (0: no limit) | | dependencies | no | String array | User-qualified job names (using \".\" as a separator) which must exit **before** this job is introduced | | interrupt | no | Bool | If `true`, the job given by \"user\" and \"name\" is interrupted (for incremental jobs, just the current revision).| | incremental | no | Bool | Whether this job has multiple _increments_ / _revisions_ and should be treated as such | | literals | no | Int array | You can specify the set of SAT literals (for this increment) directly in the JSON. | | precursor | no | String | _(Only for incremental jobs)_ User-qualified job name (`<user>.<jobname>`) of this job's previous increment | | assumptions | no | Int array | _(Only for incremental jobs)_ You can specify the set of assumptions for this increment directly in the JSON. | | done | no | Bool | _(Only for incremental jobs)_ If `true`, the incremental job given by \"precursor\" is finalized and cleaned up. | *) Not needed if `done` is set to `true`. In the above example, a job is introduced with priority 0.7, with a wallclock limit of five minutes and a CPU limit of 10 CPUh. For SAT solving, the input can be provided (a) as a plain file, (b) as a compressed (.lzma / .xz) file, or (c) as a named (UNIX) pipe. In each case, you have the option of providing the payload (i) in text form (i.e., a valid CNF description), or, with field `content-mode: \"raw\"`, in binary form (i.e., a sequence of bytes representing integers). For text files, Mallob uses the common iCNF extension for incremental formulae: The file may contain a single line of the form `a <lit1> <lit2> ... 0` where `<lit1>`, `<lit2>` etc. are assumption literals. For binary files, Mallob reads clauses as integer sequences with separation zeroes in between. Two zeroes in a row (i.e., an \"empty clause\") signal the end of clause literals, after which a number of assumption integers may be specified. Another zero signals that the description is complete. If providing a named pipe, make sure that (a) the named pipe is already created when submitting the job and (b) your application pipes the formula _after_ submitting the job (else it will hang indefinitely except if this is done in a separate thread). Assumptions can also be specified directly in the JSON describing the job via the `assumptions` field (without any trailing zero). This way, an incremental application could maintain a single text file with a monotonically growing set of clauses. The \"arrival\" and \"dependencies\" fields are useful to test a particular preset scenario of jobs: The \"arrival\" field ensures that the job will be scheduled only after Mallob ran for the specified amount of seconds. The \"dependencies\" field ensures that the job is scheduled only if all specified other jobs are already processed. Mallob is notified by the kernel as soon as a valid file is placed in `.api/jobs.0/in/` and will immediately remove the file and schedule the job. ### Retrieving a Job Result Upon completion of a job, Mallob writes a result JSON file under `.api/jobs.0/out/<user-name>.<job-name>.json` (you can repeatedly query the directory contents or employ a kernel-level mechanism like `inotify`). Such a file may look like this: ``` { \"application\": \"SAT\", \"cpu-limit\": \"10h\", \"file\": \"/path/to/difficult/formula.cnf\", \"name\": \"test-job-1\", \"priority\": 0.7, \"result\": { \"resultcode\": 10, \"resultstring\": \"SAT\", \"solution\": [0, 1, 2, 3, 4, 5] }, \"stats\": { \"time\": { \"parsing\": 0.03756427764892578, \"processing\": 0.07197785377502441, \"scheduling\": 0.0002980232238769531, \"total\": 0.11040472984313965 }, \"used_cpu_seconds\": 0.2633516788482666, \"used_wallclock_seconds\": 0.06638360023498535 }, \"user\": \"admin\", \"wallclock-limit\": \"5m\" } ``` The result code is 0 is unknown, 10 if SAT (solved successfully), and 20 if UNSAT (no solution exists). The `solution` field is application-dependent. For SAT solving, in case of SATISFIABLE, the solution field contains the found satisfying assignment; in case of UNSAT, the result for an incremental job contains the set of failed assumptions. Instead of the \"solution\" field, the response may also contain the fields \"solution-size\" and \"solution-file\" if the solution is large and if option `-pls` is set. In that case, your application has to read `solution-size` integers (as bytes) representing the solution from the named pipe located at `solution-file`. <hr/> # Debugging Debugging of distributed applications can be difficult, especially in Mallob's case where message passing goes hand in hand with multithreading and inter-process communication. Please take a look at [docs/debugging.md](docs/debugging.md) for some notes on how Mallob runs can be diagnosed and debugged appropriately. <hr/> # Programming Interfaces Mallob can be extended in the following ways: * New options for Mallob can be added in `src/optionslist.hpp`. - Options which are specific to a certain application can be found and edited in `src/app/$APPKEY/options.hpp`. * To add a new SAT solver to be used in a SAT solver engine, do the following: - Add a subclass of `PortfolioSolverInterface`. (You can use the existing implementation for any of the existing solvers and adapt it to your solver.) - Add your solver to the portfolio initialization in `src/app/sat/execution/engine.cpp`. * To extend Mallob by adding another kind of application (like combinatorial search, planning, SMT, ...), please read [docs/application_engines.md](docs/application_engines.md). * To add a unit test, create a class `test_*.cpp` in `src/test` and then add the test case to the bottom of `CMakeLists.txt`. * To add a system test, consult the files `scripts/systest_commons.sh` and/or `scripts/systest.sh`. <hr/> # Licensing and remarks The source code of Mallob can be used, changed and redistributed under the terms of the **Lesser General Public License (LGPLv3)**, one exception being the Glucose interface which is excluded from compilation by default (see below). **Please approach us if you require a deviating license.** The used versions of Lingeling, YalSAT, CaDiCaL, and Kissat are MIT-licensed, as is HordeSat (the massively parallel solver system our SAT engine was based on) and the proof-related tools which are included and/or fetched in the `tools/` directory. The Glucose interface of Mallob, unfortunately, is non-free software due to the [non-free license of (parallel-ready) Glucose](https://github.com/mi-ki/glucose-syrup/blob/master/LICENCE). Notably, its usage in competitive events is restricted. So when compiling Mallob with `-DMALLOB_USE_GLUCOSE=1` make sure that you have read and understood these restrictions. Within our codebase we make thankful use of the following liberally licensed projects: * [robin-map](https://github.com/Tessil/robin-map) by Thibaut Goetghebuer-Planchon, for efficient unordered maps and sets * [libcuckoo](https://github.com/efficient/libcuckoo) by Manu Goyal et al., for concurrent hash tables * [JSON for Modern C++](https://github.com/nlohmann/json) by Niels Lohmann, for reading and writing JSON files * [Compile Time Regular Expressions](https://github.com/hanickadot/compile-time-regular-expressions) by Hana Dusíková, for matching particular user inputs * [robin_hood hashing](https://github.com/martinus/robin-hood-hashing) by Martin Ankerl, for efficient unordered maps and sets ## Bibliography If you make use of Mallob in an academic setting or in a competitive event, please cite the most relevant among the following publications. #### SAT'21: Focus on SAT solving ```bibtex @inproceedings{schreiber2021scalable, title={Scalable SAT Solving in the Cloud}, author={Schreiber, Dominik and Sanders, Peter}, booktitle={International Conference on Theory and Applications of Satisfiability Testing}, pages={518--534}, year={2021}, organization={Springer}, doi={10.1007/978-3-030-80223-3_35} } ``` #### Euro-Par'22: Focus on decentralized scheduling ```bibtex @inproceedings{sanders2022decentralized, title={Decentralized Online Scheduling of Malleable {NP}-hard Jobs}, author={Sanders, Peter and Schreiber, Dominik}, booktitle={International European Conference on Parallel and Distributed Computing}, pages={119--135}, year={2022}, organization={Springer}, doi={10.1007/978-3-031-12597-3_8} } ``` #### TACAS'23: Proofs of unsatisfiability ```bibtex @InProceedings{michaelson2023unsatisfiability, author={Michaelson, Dawn and Schreiber, Dominik and Heule, Marijn J. H. and Kiesl-Reiter, Benjamin and Whalen, Michael W.}, title={Unsatisfiability proofs for distributed clause-sharing SAT solvers}, booktitle={Tools and Algorithms for the Construction and Analysis of Systems (TACAS)}, year={2023}, organization={Springer}, pages={348--366}, doi={10.1007/978-3-031-30823-9_18}, } ``` #### SAT Competition TRs ```bibtex @article{schreiber2020engineering, title={Engineering HordeSat Towards Malleability: mallob-mono in the {SAT} 2020 Cloud Track}, author={Schreiber, Dominik}, journal={SAT Competition 2020}, pages={45--46} } @article{schreiber2021mallob, title={Mallob in the {SAT} Competition 2021}, author={Schreiber, Dominik}, journal={SAT Competition 2021}, pages={38--39} } @article{schreiber2022mallob, title={Mallob in the {SAT} Competition 2022}, author={Schreiber, Dominik}, journal={SAT Competition 2022}, pages={46--47} } @article{schreiber2023mallob, title={Mallob\\{32,64,1600\\} in the {SAT} Competition 2023}, author={Schreiber, Dominik}, journal={SAT Competition 2023}, pages={46--47} } ``` #### Doctoral thesis (featuring all of the above + new content) ```bibtex @phdthesis{schreiber2023scalable, author={Dominik Schreiber}, title={Scalable {SAT} Solving and its Application}, year={2023}, school={Karlsruhe Institute of Technology}, doi={10.5445/IR/1000165224} } ``` Further references: * **[Mallob IPASIR Bridge for incremental SAT solving](https://github.com/domschrei/mallob-ipasir-bridge)** * **[ImpCheck - Immediate Massively Parallel Propositional Proof Checking](https://github.com/domschrei/impcheck)** * **[Experimental data at Zenodo](https://zenodo.org/doi/10.5281/zenodo.10184679)** * **[Mallob at Helmholtz Research Software Directory (RSD)](https://helmholtz.software/software/mallob)**\n",
                "dependencies": "cmake_minimum_required(VERSION 3.11.4) project (mallob) find_package(MPI REQUIRED) set(THREADS_PREFER_PTHREAD_FLAG ON) find_package(Threads REQUIRED) set(CMAKE_CXX_STANDARD 17) # Build-specific compile options set(BASE_LIBS Threads::Threads) set(MY_DEBUG_OPTIONS \"-rdynamic -g -ggdb\") if(CMAKE_BUILD_TYPE MATCHES DEBUG) add_definitions(-DMALLOB_VERSION=\\\"dbg\\\") if(MALLOB_ASSERT_HEAVY) add_definitions(-DMALLOB_ASSERT=2) else() add_definitions(-DMALLOB_ASSERT=1) endif() else() add_definitions(-DMALLOB_VERSION=\\\"rls\\\") if(MALLOB_ASSERT_HEAVY) add_definitions(-DMALLOB_ASSERT=2) elseif(MALLOB_ASSERT) add_definitions(-DMALLOB_ASSERT=1) endif() endif() if(MALLOB_LOG_VERBOSITY) add_definitions(-DLOGGER_STATIC_VERBOSITY=${MALLOB_LOG_VERBOSITY}) endif() if(MALLOB_MAX_N_APPTHREADS_PER_PROCESS) add_definitions(-DMALLOB_MAX_N_APPTHREADS_PER_PROCESS=${MALLOB_MAX_N_APPTHREADS_PER_PROCESS}) endif() if(MALLOB_SUBPROC_DISPATCH_PATH) add_definitions(-DMALLOB_SUBPROC_DISPATCH_PATH=${MALLOB_SUBPROC_DISPATCH_PATH}) endif() if(DEFINED MALLOB_TRUSTED_SUBPROCESSING) add_definitions(-DMALLOB_TRUSTED_SUBPROCESSING=${MALLOB_TRUSTED_SUBPROCESSING}) endif() if(MALLOB_USE_ASAN) set(MY_DEBUG_OPTIONS \"${MY_DEBUG_OPTIONS} -fno-omit-frame-pointer -fsanitize=address -static-libasan\") endif() if(MALLOB_USE_TBBMALLOC) set(BASE_LIBS tbbmalloc_proxy ${BASE_LIBS}) endif() if(MALLOB_USE_JEMALLOC) if(MALLOB_JEMALLOC_DIR) link_directories(${MALLOB_JEMALLOC_DIR}) endif() set(BASE_LIBS jemalloc ${BASE_LIBS}) endif() # Default application if(NOT DEFINED MALLOB_APP_SAT) set(MALLOB_APP_SAT 1) endif() # Libraries and includes set(BASE_LIBS ${BASE_LIBS} ${MPI_CXX_LIBRARIES} ${MPI_CXX_LINK_FLAGS} m z rt dl CACHE INTERNAL \"\") set(BASE_INCLUDES ${MPI_CXX_INCLUDE_PATH} src src/util/tsl lib CACHE INTERNAL \"\") set(BASE_COMPILEFLAGS CACHE INTERNAL \"\") # Base source files set(BASE_SOURCES ${BASE_SOURCES} src/app/job.cpp src/app/app_registry.cpp src/app/app_message_subscription.cpp src/balancing/event_driven_balancer.cpp src/balancing/request_matcher.cpp src/balancing/routing_tree_request_matcher.cpp src/comm/msg_queue/message_queue.cpp src/comm/mpi_base.cpp src/comm/mympi.cpp src/comm/sysstate_unresponsive_crash.cpp src/core/scheduling_manager.cpp src/data/job_description.cpp src/data/job_result.cpp src/data/job_transfer.cpp src/interface/json_interface.cpp src/interface/api/api_connector.cpp src/scheduling/job_scheduling_update.cpp src/util/logger.cpp src/util/option.cpp src/util/params.cpp src/util/permutation.cpp src/util/random.cpp src/util/sys/atomics.cpp src/util/sys/fileutils.cpp src/util/sys/process.cpp src/util/sys/proc.cpp src/util/sys/process_dispatcher.cpp src/util/sys/shared_memory.cpp src/util/sys/tmpdir.cpp src/util/sys/terminator.cpp src/util/sys/threading.cpp src/util/sys/thread_pool.cpp src/util/sys/timer.cpp src/util/sys/watchdog.cpp src/util/ringbuf/ringbuf.c CACHE INTERNAL \"\") # Use to debug #message(\"mallob_commons sources pre application registration: ${BASE_SOURCES}\") # Define test function function(new_test testname) message(\"Adding test: ${testname}\") add_executable(test_${testname} src/test/test_${testname}.cpp) target_include_directories(test_${testname} PRIVATE ${BASE_INCLUDES}) target_compile_options(test_${testname} PRIVATE ${BASE_COMPILEFLAGS}) target_link_libraries(test_${testname} mallob_commons) add_test(NAME test_${testname} COMMAND test_${testname}) endfunction() # Add application-specific build configuration # Setup (do not change) file(WRITE \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_includes.h~\" \"\") file(WRITE \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_options.h~\" \"\") file(WRITE \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_commands.h~\" \"\") function(register_mallob_app appkey) message(\"Registering application: ${appkey}\") file(APPEND \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_includes.h~\" \"#include \\\"app/${appkey}/register.hpp\\\"\\n\") file(APPEND \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_options.h~\" \"#include \\\"app/${appkey}/options.hpp\\\"\\n\") file(APPEND \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_commands.h~\" \"register_mallob_app_${appkey}();\\n\") include(${CMAKE_CURRENT_SOURCE_DIR}/src/app/${appkey}/setup.cmake) endfunction() function(publish_app_registry_changes) execute_process(COMMAND ${CMAKE_COMMAND} -E compare_files \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_options.h\" \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_options.h~\" RESULT_VARIABLE compare_result) if(compare_result EQUAL 0) message(\"No changes in register options.\") else() message(\"Moving changed app registry.\") file(RENAME \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_includes.h~\" \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_includes.h\") file(RENAME \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_options.h~\" \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_options.h\") file(RENAME \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_commands.h~\" \"${CMAKE_CURRENT_SOURCE_DIR}/src/app/.register_commands.h\") endif() endfunction() # Include applications register_mallob_app(\"dummy\") # always register the \"no-op\" application if(MALLOB_APP_SAT) register_mallob_app(\"sat\") endif() if(MALLOB_APP_KMEANS) register_mallob_app(\"kmeans\") endif() # Include further applications here: #if(MALLOB_APP_YOURAPP) # register_mallob_app(\"yourapp\") #endif() # ... publish_app_registry_changes() # Library with Mallob's common source files #message(\"mallob_commons sources post application registration: ${BASE_SOURCES}\") # Use to debug add_library(mallob_commons STATIC ${BASE_SOURCES} ) target_include_directories(mallob_commons PRIVATE ${BASE_INCLUDES}) target_compile_options(mallob_commons PRIVATE ${BASE_COMPILEFLAGS}) target_link_libraries(mallob_commons ${BASE_LIBS}) # Executables add_executable(mallob src/core/client.cpp src/core/worker.cpp src/main.cpp) target_include_directories(mallob PRIVATE ${BASE_INCLUDES}) target_compile_options(mallob PRIVATE ${BASE_COMPILEFLAGS}) target_link_libraries(mallob mallob_commons) add_executable(mallob_process_dispatcher src/app/sat/main_dispatch.cpp) target_include_directories(mallob_process_dispatcher PRIVATE ${BASE_INCLUDES}) target_compile_options(mallob_process_dispatcher PRIVATE ${BASE_COMPILEFLAGS}) target_link_libraries(mallob_process_dispatcher mallob_commons) # Debug flags to find line numbers in stack traces etc. add_definitions(\"${MY_DEBUG_OPTIONS}\") SET(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${MY_DEBUG_OPTIONS}\") # Tests enable_testing() new_test(bitsets) new_test(permutation) new_test(message_queue) new_test(volume_calculator) new_test(concurrent_malloc) new_test(hashing) new_test(async_collective) new_test(random) new_test(reverse_file_reader) new_test(categorized_external_memory) new_test(bidirectional_pipe)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mallocmc",
            "repo_link": "https://github.com/alpaka-group/mallocMC",
            "content": {
                "codemeta": "",
                "readme": "mallocMC ============= mallocMC: *Memory Allocator for Many Core Architectures* This project provides a framework for **fast memory managers** on **many core accelerators**. It is based on [alpaka](https://github.com/alpaka-group/alpaka) to run on many different accelerators and comes with multiple allocation algorithms out-of-the-box. Custom ones can be added easily due to the policy-based design. Usage ------- Follow the step-by-step instructions in [Usage.md](Usage.md) to replace your `new`/`malloc` calls with a *blacingly fast* mallocMC heap! :rocket: Install ------- mallocMC is header-only, but requires a few other C++ libraries to be available. Our installation notes can be found in [INSTALL.md](INSTALL.md). Contributing ------------ Rules for contributions are found in [CONTRIBUTING.md](./CONTRIBUTING.md). On the Algorithms ----------------------------- This library was originally inspired by the *ScatterAlloc* algorithm, [forked](https://en.wikipedia.org/wiki/Fork_%28software_development%29) from the **ScatterAlloc** project, developed by the [Managed Volume Processing](http://www.icg.tugraz.at/project/mvp) group at [Institute for Computer Graphics and Vision](http://www.icg.tugraz.at), TU Graz (kudos!). The currently shipped algorithms are using similar ideas but differ from the original one significantly. From the original project page (which is no longer existent to the best of our knowledge): ```quote ScatterAlloc is a dynamic memory allocator for the GPU. It is designed concerning the requirements of massively parallel execution. ScatterAlloc greatly reduces collisions and congestion by scattering memory requests based on hashing. It can deal with thousands of GPU-threads concurrently allocating memory and its execution time is almost independent of the thread count. ScatterAlloc is open source and easy to use in your CUDA projects. ``` Our Homepage: <https://www.hzdr.de/crp> Versions and Releases --------------------- Official releases can be found in the [Github releases](https://github.com/alpaka-group/mallocMC/releases). We try to stick to [semantic versioning](https://semver.org/) but we'll bump the major version number for major features. Development happens on the `dev` branch. Changes there have passed the CI and a code review but we make no guarantees about API or feature stability in this branch. Literature ---------- Just an incomplete link collection for now: - [Paper](https://doi.org/10.1109/InPar.2012.6339604) by Markus Steinberger, Michael Kenzel, Bernhard Kainz and Dieter Schmalstieg - 2012, May 5th: [Presentation](http://innovativeparallel.org/Presentations/inPar_kainz.pdf) at *Innovative Parallel Computing 2012* by *Bernhard Kainz* - Junior Thesis [![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.34461.svg)](http://dx.doi.org/10.5281/zenodo.34461) by Carlchristian Eckert (2014) License ------- We distribute the modified software under the same license as the original software from TU Graz (by using the [MIT License](https://en.wikipedia.org/wiki/MIT_License)). Please refer to the [LICENSE](LICENSE) file.\n",
                "dependencies": "cmake_minimum_required(VERSION 3.14...3.22) # ---- Project ---- project( mallocMC VERSION 3.0.0 LANGUAGES CXX ) # ---- Include guards ---- if(PROJECT_SOURCE_DIR STREQUAL PROJECT_BINARY_DIR) message( FATAL_ERROR \"In-source builds not allowed. Please make a new directory (called a build directory) and run CMake from there.\" ) endif() # ---- Options ---- option(mallocMC_BUILD_TESTING \"Turn on/off building the tests\" OFF) option(mallocMC_BUILD_EXAMPLES \"Turn on/off building the examples\" OFF) if (mallocMC_BUILD_TESTING OR mallocMC_BUILD_EXAMPLES) enable_testing() endif() if (mallocMC_BUILD_TESTING) set(alpaka_ACC_CPU_B_SEQ_T_SEQ_ENABLE ON CACHE BOOL \"\" FORCE) endif() # ---- Add dependencies via CPM ---- # see https://github.com/TheLartians/CPM.cmake for more info include(${CMAKE_CURRENT_LIST_DIR}/cmake/PackageProject.cmake) include(${CMAKE_CURRENT_LIST_DIR}/cmake/CPM_0.40.2.cmake) CPMUsePackageLock(${CMAKE_CURRENT_LIST_DIR}/cmake/package-lock.cmake) include(${CMAKE_CURRENT_LIST_DIR}/cmake/add_controlled.cmake) # PackageProject.cmake will be used to make our target installable add_controlled(\"alpaka\" REQUIRED) # ---- Create library ---- # Note: for header-only libraries change all PUBLIC flags to INTERFACE and create an interface add_library(${PROJECT_NAME} INTERFACE) set_target_properties(${PROJECT_NAME} PROPERTIES CXX_STANDARD 20) if(alpaka_ACC_GPU_CUDA_ENABLE) add_controlled(\"Gallatin\") if (TARGET gallatin::gallatin) set(mallocMC_HAS_Gallatin_AVAILABLE YES) else() set(mallocMC_HAS_Gallatin_AVAILABLE NO) endif() # Gallatin needs some fairly recent compute capability from CUDA. # CMake defaults to taking the oldest supported by the device # (https://cmake.org/cmake/help/latest/variable/CMAKE_CUDA_ARCHITECTURES.html) # which can be too old. This leads to compilation errors along the lines of # # error: no instance of overloaded function \"atomicCAS\" matches the argument list # argument types are: (unsigned short *, unsigned short, unsigned short) # # because this overload was only added later (apparently?). if (\"${CMAKE_CUDA_ARCHITECTURES}\" LESS 70) message( WARNING \"CUDA architecture detected is too old: ${CMAKE_CUDA_ARCHITECTURES}. \" \"If the architecture set is too old, this can lead to compilation errors with Gallatin. \" \"If Gallatin is needed, please set CMAKE_CUDA_ARCHITECTURES to the correct value >= 70.\" ) set(mallocMC_HAS_Gallatin_AVAILABLE NO) endif() if (mallocMC_HAS_Gallatin_AVAILABLE) target_link_libraries(${PROJECT_NAME} INTERFACE gallatin) target_compile_definitions(${PROJECT_NAME} INTERFACE mallocMC_HAS_Gallatin_AVAILABLE) endif() endif() # being a cross-platform target, we enforce standards conformance on MSVC target_compile_options(${PROJECT_NAME} INTERFACE \"$<$<COMPILE_LANG_AND_ID:CXX,MSVC>:/permissive->\") target_include_directories( ${PROJECT_NAME} INTERFACE $<BUILD_INTERFACE:${PROJECT_SOURCE_DIR}/include> $<INSTALL_INTERFACE:include/${PROJECT_NAME}-${PROJECT_VERSION}> ) target_link_libraries(${PROJECT_NAME} INTERFACE alpaka::alpaka) if(mallocMC_BUILD_TESTING) include(${CMAKE_CURRENT_LIST_DIR}/cmake/tools.cmake) add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/test ${CMAKE_BINARY_DIR}/test) endif() if(mallocMC_BUILD_EXAMPLES) include(${CMAKE_CURRENT_LIST_DIR}/cmake/tools.cmake) add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/examples ${CMAKE_BINARY_DIR}/examples) endif() # ---- Create an installable target ---- # this allows users to install and find the library via `find_package()`. # the location where the project's version header will be placed should match the project's regular # header paths string(TOLOWER ${PROJECT_NAME}/version.hpp VERSION_HEADER_LOCATION) packageProject( NAME ${PROJECT_NAME} VERSION ${PROJECT_VERSION} NAMESPACE ${PROJECT_NAME} BINARY_DIR ${PROJECT_BINARY_DIR} INCLUDE_DIR ${PROJECT_SOURCE_DIR}/include INCLUDE_DESTINATION include/${PROJECT_NAME}-${PROJECT_VERSION} VERSION_HEADER \"${VERSION_HEADER_LOCATION}\" COMPATIBILITY SameMajorVersion )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/materials-learning-algorithms",
            "repo_link": "https://github.com/mala-project/mala",
            "content": {
                "codemeta": "",
                "readme": "![image](./docs/source/img/logos/mala_horizontal.png) # MALA [![CPU](https://github.com/mala-project/mala/actions/workflows/cpu-tests.yml/badge.svg)](https://github.com/mala-project/mala/actions/workflows/cpu-tests.yml) [![image](https://github.com/mala-project/mala/actions/workflows/gh-pages.yml/badge.svg)](https://mala-project.github.io/mala/) [![image](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5557255.svg)](https://doi.org/10.5281/zenodo.5557255) MALA (Materials Learning Algorithms) is a data-driven framework to generate surrogate models of density functional theory calculations based on machine learning. Its purpose is to enable multiscale modeling by bypassing computationally expensive steps in state-of-the-art density functional simulations. MALA is designed as a modular and open-source python package. It enables users to perform the entire modeling toolchain using only a few lines of code. MALA is jointly developed by the Sandia National Laboratories (SNL) and the Center for Advanced Systems Understanding (CASUS). See [Contributing](docs/source/CONTRIBUTE.md) for contributing code to the repository. This repository is structured as follows: ``` ├── examples : contains useful examples to get you started with the package ├── install : contains scripts for setting up this package on your machine ├── mala : the source code itself ├── test : test scripts used during development, will hold tests for CI in the future └── docs : Sphinx documentation folder ``` ## Installation > **WARNING**: Even if you install MALA via PyPI, please consult the full installation instructions afterwards. External modules (like the QuantumESPRESSO bindings) are not distributed via PyPI! Please refer to [Installation of MALA](docs/source/install/installing_mala.rst). ## Running You can familiarize yourself with the usage of this package by running the examples in the `example/` folder. ## Contributors MALA is jointly maintained by - [Sandia National Laboratories](https://www.sandia.gov/) (SNL), USA. - Scientific supervisor: Sivasankaran Rajamanickam, code maintenance: Jon Vogel - [Center for Advanced Systems Understanding](https://www.casus.science/) (CASUS), Germany. - Scientific supervisor: Attila Cangi, code maintenance: Lenz Fiedler A full list of contributors can be found [here](docs/source/CONTRIBUTE.md). ## Citing MALA If you publish work which uses or mentions MALA, please cite the following paper: J. A. Ellis, L. Fiedler, G. A. Popoola, N. A. Modine, J. A. Stephens, A. P. Thompson, A. Cangi, S. Rajamanickam (2021). Accelerating Finite-temperature Kohn-Sham Density Functional Theory with Deep Neural Networks. [Phys. Rev. B 104, 035120 (2021)](https://doi.org/10.1103/PhysRevB.104.035120) alongside this repository.\n",
                "dependencies": "[tool.black] line-length = 79 [build-system] requires = [\"setuptools\"] build-backend = \"setuptools.build_meta\"\n# The requirement below makes sure we have torch installed, even if that will # pull the latest GPU torch in case the user's environment may be misconfigured # such that a pre-installed torch is not found. torch ase mpmath numpy optuna scipy pandas tensorboard openpmd-api scikit-spatial tqdm mendeleev sympy\nfrom setuptools import setup, find_packages # Doing it as suggested here: # https://packaging.python.org/guides/single-sourcing-package-version/ # (number 3) version = {} with open(\"mala/version.py\") as fp: exec(fp.read(), version) with open(\"README.md\") as f: readme = f.read() with open(\"LICENSE\") as f: license = f.read() extras = { \"dev\": [\"bump2version\"], \"opt\": [\"oapackage\", \"scikit-learn\"], \"mpi\": [\"mpi4py\"], \"test\": [\"pytest\", \"pytest-cov\"], \"doc\": open(\"docs/requirements.txt\").read().splitlines(), \"experimental\": [\"asap3\", \"dftpy\", \"minterpy\"], } setup( name=\"materials-learning-algorithms\", version=version[\"__version__\"], description=( \"Materials Learning Algorithms. \" \"A framework for machine learning materials properties from \" \"first-principles data.\" ), long_description=readme, long_description_content_type=\"text/markdown\", url=\"https://github.com/mala-project/mala\", author=\"MALA developers\", license=license, packages=find_packages( exclude=(\"test\", \"docs\", \"examples\", \"install\", \"ml-dft-sandia\") ), zip_safe=False, install_requires=open(\"requirements.txt\").read().splitlines(), extras_require=extras, python_requires=\">=3.10.4\", )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/matrad",
            "repo_link": "https://github.com/e0404/matRad",
            "content": {
                "codemeta": "",
                "readme": "[![Current Release](https://img.shields.io/github/v/release/e0404/matRad)](https://github.com/e0404/matRad/releases) [![Downloads](https://img.shields.io/github/downloads/e0404/matRad/total)](https://github.com/e0404/matRad/releases) [![Contributors](https://img.shields.io/github/contributors/e0404/matRad)](https://github.com/e0404/matRad/graphs/contributors) [![GitHub Build Status](https://github.com/e0404/matRad/actions/workflows/tests.yml/badge.svg)](https://github.com/e0404/matRad/actions/workflows/tests.yml) [![codecov](https://codecov.io/gh/e0404/matRad/graph/badge.svg?token=xQhUQLu4FK)](https://codecov.io/gh/e0404/matRad) Citable DOIs: - General DOI: [![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.3879615.svg)](https://doi.org/10.5281/zenodo.3879615) - Latest Release: [![DOI](https://zenodo.org/badge/29671667.svg)](https://zenodo.org/badge/latestdoi/29671667) # General information --- matRad is an open source treatment planning system for radiation therapy written in Matlab. It supports planning of intensity-modulated radiation therapy for mutliple modalities and is meant **for educational and research purposes**. **IT IS NOT SUITABLE FOR CLINICAL USE** (also see the no-warranty clause in the GPL license). The source code is maintained by a development team at the German Cancer Reserach Center - DKFZ in Heidelberg, Germany, and other contributors around the world. We are always looking for more people willing to help improve matRad. Do not hesitate and [get in touch](mailto:contact@matRad.org). More information can be found on the project page at <http://e0404.github.io/matRad/>; a wiki documentation is under constant development at <https://github.com/e0404/matRad/wiki>. # Getting Started If you want to quickly run matRad, start with the Quick Start below. Some information on the structure of matRad for more sustainable use is given afterwards. ## Quick Start It's the first time you want to use matRad? First, get a local copy of matRad by download or git cloning. Having done that, we recommend you navigate into the folder in Matlab and execute ``` matRad_rc ``` which will setup the path & configuration and tell you the current version. Then there're three options for a pleasant start with matRad. Choose one or try out each of them. ### Option 1: Using the GUI For an intuitive workflow with the graphical user interface, type ``` matRadGUI ``` in your command window. An empty GUI should be opened. Click the _*Load.mat_ data-Button in the Workflow-section to load a patient. Set the plan and optimization parameters, calculate the dose influence matrix and execute the fluence optimization in the GUI. ### Option 2: Using the main script If you prefer scripting, open the default script *matRad.m* from the main matRad folder: ``` edit matRad.m ``` Use it to learn something about the code structure and execute it section by section. You can also run the full script for an example photon plan by just typing ``` matRad ``` in your command window. ### Option 3: Using the examples The most time consuming but also most educational approach to matRad. When in the main matRad folder, navigate to the folder *examples*. Open one of the examples given there. Execute it section by section. Move on to the next example afterwards. ## Advanced information for new users ### Folder Structure #### Core Source Code Most of the source code of matRad is located in the \"matRad\" subfolder. Within the first level of matRad, you find the functions handling the basic workflow steps. These functions have simple interfaces relying on matRad's main data structures ct, cst, stf, dij, resultGUI, and pln. Additionally, it contains MatRad_Config.m which is a singleton class implementation to handle global configuration of matRad. Check out the infos further below. We try to keep the main workflow functions as consistent as possible, while the fine-grained implementation in the subfolders within matRad/* may undergo larger changes. #### User Directory By default, matRad adds the \"userdata\" folder to the path. It is the place to put your custom scripts, machine data, imported patients etc. Just follow the README files in the folders. Contents of this folder are added to the .gitignore and will thus be ignored during your development efforts, keeping your repository clean. #### Third-Party & Submodules Our ThirdParty-Tools used in matRad are stored in the thirdParty folder including licenses. Submodules contains references to used git repositories, and you might recognize that some dependencies appear both in submodules and thirdParty. This is mainly to maintain operation if the code is downloaded (and not cloned), and also helps us to maintain the build process of mex files built from source in the submodules (and then added to ThirdParty). #### Tests The \"test\" folder contains xUnit-Style tests based on the MOxUnit framework. You can run those tests by running matRad_runTests from the root directory. Check the README file within the test folder for more information. ### MatRad_Config / matRad_cfg matRad maintains its global configuration, including some default parameters, as well as a logging mechanism with different levels, in the MatRad_Config.m class serving as a \"singleton\" throughout matRad. You will see many functions using a call like `matRad_cfg = MatRad_Config.instance();`, which will get you the global configuration anywhere in the code or in the command window. Alternatively, `matRad_rc` will return matRad_cfg as well. # Need help? If you encounter problems with matRad, please consider the following guidelines **before** submitting issues on our github page. * Check you are using the newest version of matRad. * Please check the description of how to set up matRad and its technical documentation in the [wiki](https://github.com/e0404/matRad/wiki). * Go through the relevant examples and see if they answer your question (see *Option 3* above!) * Check open and closed issues for your question. Still having problems? Then create an issue, provide a **minimum example** of your attempted workflow / what causes the problems and be patient! # Citing matRad ### Scientific papers If you use matRad in a scientific publication, consider citing the following paper: Wieser, Hans-Peter, et al. \"Development of the open-source dose calculation and optimization toolkit matRad.\" Medical Physics 44.6 (2017): 2556-2568. [![DOI](https://img.shields.io/badge/DOI-10.1002%2Fmp.12251-blue)](https://doi.org/10.1002/mp.12251) BibTex entry: ``` @article{wieser2017development, title={Development of the open-source dose calculation and optimization toolkit matRad}, author={Wieser, Hans-Peter and Cisternas, Eduardo and Wahl, Niklas and Ulrich, Silke and Stadler, Alexander and Mescher, Henning and M{\\\"u}ller, Lucas-Raphael and Klinge, Thomas and Gabrys, Hubert and Burigo, Lucas and others}, journal={Medical Physics}, volume={44}, number={6}, pages={2556--2568}, year={2017}, publisher={Wiley Online Library}, doi={10.1002/mp.12251} } ``` ### Citing as Software matRad's code also has its own general DOI with Zenodo: [![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.3879615.svg)](https://doi.org/10.5281/zenodo.3879615) You can cite specific versions of matRad in your work! For example, Here is the badge that lead's to the latest release of matRad: [![DOI](https://zenodo.org/badge/29671667.svg)](https://zenodo.org/badge/latestdoi/29671667) # Funding Sources matRad developments (on this branch) were (in parts) funded by: - The German Research Foundation (DFG), Project No. 265744405 & 443188743 - The German Cancer Aid, Project No. 70113094 - The German Federal Ministry of Education and Research (BMBF), Project No. 01DN17048 - Mathworks Academic Research Support --- Copyright 2022 the matRad development team. matrad@dkfz.de All the elements of the compilation of matRad and Ipopt are free software. You can redistribute and/or modify matRad's source code version provided as files with .m and .mat extension under the terms of the GNU GENERAL PUBLIC LICENSE Version 3 (GPL v3). You can also add to matRad the Ipopt functionality by using the precompiled mex files of the Ipopt optimizer in object code version which are licensed under the Eclipse Public License Version 1.0 (EPL v1.0), also made available for download via https://projects.coin-or.org/Ipopt. matRad also contains interfaces to an open-source photon Monte Carlo dose calculation engine developed by Edgardo Dörner hosted on GitHub (http://github.com/edoerner/ompMC) and to the open-source proton Monte Carlo project MCsquare (www.openmcsquare.org) from UCLouvain, Louvain-la-Neuve, Belgium. Both interfaces are integrated into matRad as submodules. In addition, we provide a matlab standalone version of the compilation of matRad and Ipopt, where the files of matRad and Ipopt are licensed under GPL v3 and EPL v1.0 respectively. The matlab standalone version is meant to be used by students for learning and practicing scientific programming and does not yet contain the interfaces to the aforementioned Monte Carlo dose calculation engines. matRad is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. Please note that we treat the compilation of matRad and Ipopt as separate and independent works (or modules, components, programs). Therefore, to the best of our understanding, the compilation of matRad and Ipopt is subject to the \"Mere Aggregation\" exception in section 5 of the GNU v3 and the exemption from \"Contributions\" in section 1. b) ii) of the EPL v1.0. Should this interpretation turn out to be not in compliance with the applicable laws in force, we have provided you with an additional permission under GNU GPL version 3 section 7 to allow you to use the work resulting from combining matRad with Ipopt. You will receive a copy of the GPL v3 and a copy of the EPL v1.0 in the file LICENSE.md along with the compilation. If not, see http://www.gnu.org/licenses/ and/or http://opensource.org/licenses/EPL-1.0/. ---\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mcodac",
            "repo_link": "https://gitlab.com/dlr-sy/mcodac",
            "content": {
                "codemeta": "",
                "readme": "[![doi](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.13383097-red.svg)](https://zenodo.org/records/13383097) [![doc](https://img.shields.io/static/v1?label=Pages&message=Reference%20Guide&color=blue&style=flat&logo=gitlab)](https://dlr-sy.gitlab.io/mcodac) [![PyPi](https://img.shields.io/static/v1?label=PyPi&message=1.2.0&color=informational&logo=pypi)](https://pypi.org/project/mcodac/) # MCODAC MCODAC (Modular COmposite Damage Analysis Code) is a Fortran library for the evaluation of pristine and damaged composite structures. In addition to basic mathematical tools for tensor manipulation, it contains multidimensional interpolation methods, numerical optimization routines and common utility algorithms used in continuum mechanics. Furthermore, the library contains analysis methods specifically tailored to composites, from micromechanical homogenization approaches to macroscopic fatigue models of orthotropic multilayer composites. This project is compiled for Python using [f2py](https://numpy.org/doc/stable/f2py). > Installation from source requires an active Fortran compiler (ifort, gfortran). ## Downloading Use GIT to get the latest code base. From the command line, use ``` git clone https://gitlab.com/dlr-sy/mcodac mcodac ``` If you check out the repository for the first time, you have to initialize all submodule dependencies first. Execute the following from within the repository. ``` git submodule update --init --recursive ``` To update all refererenced submodules to the latest production level, use ``` git submodule foreach --recursive 'git pull origin $(git config -f $toplevel/.gitmodules submodule.$name.branch || echo master)' ``` ## Installation MCODAC can be installed from source using [poetry](https://python-poetry.org). If you don't have [poetry](https://python-poetry.org) installed, run ``` pip install poetry --pre --upgrade ``` to install the latest version of [poetry](https://python-poetry.org) within your python environment. Use ``` poetry update ``` to update all dependencies in the lock file or directly execute ``` poetry install ``` to install all dependencies from the lock file. Last, you should be able to import MCODAC as a python package. ```python import mcodac ``` ## Example Please refer to the linked [repository](https://gitlab.com/dlr-sy/mcodac) for specific application examples. ## Contact * [Marc Garbade](mailto:marc.garbade@dlr.de) ## Support * [List of Contributors](CONTRIBUTING.md)\n",
                "dependencies": "# TOML file to create MCODAC # # @note: TOML file # Created on 29.12.2022 # # @version: 1.0 # ---------------------------------------------------------------------------------------------- # @requires: # - # # @change: # - # # @author: garb_ma [DLR-SY,STM Braunschweig] # ---------------------------------------------------------------------------------------------- [build-system] requires = [\"poetry-core>=1.0\"] build-backend = \"poetry.core.masonry.api\" [tool.poetry] name = \"mcodac\" version = \"1.2.0\" description = \"Calculation of pristine and damaged composite structures\" authors = [\"Garbade, Marc <marc.garbade@dlr.de>\"] maintainers = [\"Garbade, Marc <marc.garbade@dlr.de>\", \"Lüders, Caroline <caroline.lüders@dlr.de>\", \"Marripelli, Mrudula <mrudula.marripelli@dlr.de>\", \"Kakumoto de Melo, Lucas Yuzo <lucas.kakumotodemelo@dlr.de>\"] license = \"GPL-3.0-or-later\" packages = [{include=\"**/*\", from=\"bin\"}] exclude = [\"bin/_*\", \"bin/**/.*\", \"bin/linux/*\", \"bin/windows/*\"] repository = \"https://gitlab.com/dlr-sy/mcodac\" documentation = \"https://gitlab.com/dlr-sy/mcodac/-/blob/mcd_development/README.md\" keywords = [\"analysis\",\"damage\",\"composite\"] readme = \"README.md\" classifiers = [ \"Development Status :: 4 - Beta\", \"Topic :: Scientific/Engineering\", \"Programming Language :: Python :: 2\", \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)\", \"Operating System :: OS Independent\" ] [tool.poetry.urls] Changelog = \"https://gitlab.com/dlr-sy/mcodac/-/blob/mcd_development/CHANGELOG.md\" [[tool.poetry.source]] name = \"dlr-pypi\" url = \"https://pypi.python.org/simple\" priority = \"primary\" [[tool.poetry.source]] name = \"dlr-sy\" url = \"https://gitlab.dlr.de/api/v4/groups/541/-/packages/pypi/simple\" priority = \"supplemental\" [tool.poetry.build] script = \"config/build.py\" generate-setup-file = false [tool.poetry.dependencies] python = \"~2.7 || ^3.5\" numpy = [{version = \"^1.16\", python = \"~2.7 || ~3.5\"}, {version = \"^1.18\", python = \"~3.6\"}, {version = \"^1.21\", python = \"~3.7\"}, {version = \"^1.22\", python = \"~3.8\"}, {version = \">=1.22,<2\", python = \"^3.9\"}] # All mandatory development dependencies [tool.poetry.group.dev.dependencies] mkl = [{version = \"~2022.2\", platform = \"win32\"}] mkl-devel = [{version = \"~2022.2\", platform = \"win32\"}] tbb = [{version = \"~2021.7\", platform = \"win32\"}] pyx-core = [{version = \"^1.17\", python = \"~2.7 || ^3.5,<3.7\"}, {git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake.git\", develop=true, python=\"^3.7\"}] [tool.pyxmake.abaqus] name = \"standardU\" source = \"solver\" files = \"mcd_astandard\" libs = [\"mcd_core{arch}\",\"bbeam{arch}\", \"muesli{arch}\",\"dispmodule{arch}\", \"interp{arch}\",\"pchip{arch}\",\"toms{arch}\"] include = [\"include/{platform}/{arch}\", \"include/{platform}/{arch}/boxbeam\", \"include/{platform}/{arch}/dispmodule\", \"include/{platform}/{arch}/toms\"] dependency = \"lib/{platform}/{arch}\" output = \"bin/{platform}/{arch}\" [tool.pyxmake.cmake] name = \"mcodac\" source = \"config\" [tool.pyxmake.doxygen] name = \"MCODAC\" title = [\"MCODAC\", \"MCODAC Reference Guide\"] source = \"src\" output = \"doc/mcd_core\" [tool.pyxmake.f2py] name = \"mcodac\" source = \"src\" files = [\"mcd_data\", \"mcd_error\", \"mcd_plugins\", \"mcd_tools\", \"mcd_contact\", \"mcd_material\", \"mcd_load\", \"mcd_element\", \"mcd_subbuckling\", \"mcd_dmginitiation\", \"mcd_dmgevolution\", \"mcd_fracture\", \"mcd_dmginfluence\", \"mcd_degradation\", \"mcd_dg8\", \"mcd_dmgtolerance\",\"mcd_iostream\", \"mcd_fatigue\", \"mcd_prony\", \"mcd_wrapper\", \"mcd_toplevel\", \"mcd_main\"] libs = [\"bbeam{arch}\",\"muesli{arch}\",\"dispmodule{arch}\", \"interp{arch}\",\"pchip{arch}\",\"toms{arch}\"] include = [\"include/{platform}/{arch}/boxbeam\", \"include/{platform}/{arch}/dispmodule\", \"include/{platform}/{arch}/toms\"] dependency = \"lib/{platform}/{arch}\" output = \"build\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/me-compute",
            "repo_link": "https://github.com/rizac/me-compute",
            "content": {
                "codemeta": "",
                "readme": "# <img align=\"left\" height=\"30\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/medien_kommunikation/Infothek/Mediathek/Bilder/GFZ/GFZ_Logo/GFZ-Logo_eng_RGB.svg\"> Me-compute <img align=\"right\" height=\"50\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/GFZ_Wortmarke_SVG_klein_en_edit.svg\"> |Jump to: | [Installation](#installation) | [Usage](#usage) | [Citation](#citation) | | - | - | - | - | Program to compute energy Magnitude (Me) from downloaded seismic events. The download is performed via [stream2segment](https://github.com/rizac/stream2segment) (included in this package) into a custom SQLite or Postgres database (in this case, the database has to be setup beforehand). Once downloaded, events and data within a customizable time window can be fetched from the database in order to compute each event Me (Me = mean of all stations energy magnitudes in the 5th-95th percentiles). The computed Me are available in several formats: CSV, HDF, HTML and QuakeMl (see Usage below for details). The download + processing routines can be chained and scheduled on a server to compute the energy magnitude in semi-realtime (e.g. daily or weekly. See instructions below) ## Installation Make virtualenv `python3 -m venv [PYPATH]` and activate it: `source [PYPATH]/bin/activate`. **Remember that any command of the program must be done with the virtual env activated** Update required packages for installing Python stuff: ```console pip install --upgrade pip setuptools ``` Install the program: From the directory where you cloned `mecompute`: 1. [Optional] If you want to be safer and install **exactly** the dependencies with the tested versions, and you don't have conflicts with existing dependencies (e.g., your virtualenv is empty and not supposed to have other packages installed), then you can run: `pip install -r ./requirements.txt` or `pip install -r ./requirements.dev.txt` (the latter if you want to run tests) 2. Install the program: ```bash pip install . ``` or (if you want to run tests): ```bash pip install \".[dev]\" ``` (add the `-e` option if you want to install in [editable mode](https://stackoverflow.com/a/35064498)) **The installation creates a new terminal command `me-compute` within your virtualenv, that you can inspect via**: ```bash me-compute --help ``` ## Usage First of all, you should configure your download routine. The repository contains a `config` directory (git-ignored), with several configuration files that you can copy and modify. Most of them are for experienced users and are already filled with default values: the only routine that has to be customized is the download routine (file `download.yaml`, see below) ### Events and data Download: The download routine downloads data and metadata from the configured FDSN event and dataselect web services into a custom database (Sqlite or Postgres using [stream2segment](https://github.com/rizac/stream2segment). With Postgres, the db has to be setup beforehand) . Open `download.yaml` (or a copy of it) and configure `dburl` (ideally, you might want to setup also `start`, `end`, `events_url` and `data_url`). Then run stream2segment with the `s2s` command: ```commandline s2s download -c download.yaml ``` ### Me computation To compute the energy magnitude of the events saved on the db, you run the `me-compute` command with customized options, e.g.: ```bash me-compute -s [START] -e [END] -d download.yaml ... [OUTPUT_DIR] ``` START and END are the start and end time of the events to consider, in ISO format (e.g. \"2016-03-31\"). If omitted, they will be inferred (Type `me-compute --help` for more details) OUTPUT_DIR is the destination directory. You can use the special characters `%S%` and `%E%` that will be replaced with the start and end time strings (see above). The output directory and its parents will be created if they do not exist. In the output directory, the following files will be saved: - **station-energy-magnitude.hdf** A tabular file where each row represents a station(^) and each column the station computed data and metadata, including the station energy magnitude. (^) Note: technically speaking, a single HDF row represents a waveform. We talk about station because by default we download a single channel per station (the vertical component `BHZ`, see `download.yaml` for details) - **energy-magnitude.csv** A tabular file where each row represents a seismic event, aggregating the result of the previous file into the final event energy magnitude. The event Me is the mean of all station energy magnitudes within the 5-95 percentiles. Empty or non-numeric Me values indicate that the energy magnitude could not be computed or resulted in invalid values (NaN, null, +-inf) - **energy-magnitude.html** A report that can be opened in the user browser to visualize the computed energy magnitudes on maps and HTML tables - **[eventid1].xml, ..., [eventid1].xml** All processed events saved in QuakeML format, updated with the information of their energy magnitude. Only events with valid Me will be saved - **energy-magnitude.log** the log file where the info, errors and warnings of the routine are stored. The core energy magnitude computation at station level (performed via `stream2segment` utilities) has a separated and more detailed log file (see below) - **station-energy-magnitude.log** the log file where the info, errors and warnings of the station energy magnitude computation have been stored ### Cron job (schedule downloads+ Me computation) Assuming your Python virtualenv is at `[VEN_PATH]`, with your python virtualenv activated (`source [VENV_PATH]/bin/activate`), type `which me-compute`. You should see something like `[VENV_PATH]/bin/me-compute` (same for `which s2s`). With the paths above, you can set up cron jobs to schedule all above routines. For instance, below an example file that can be edited via `crontab -e` (https://linux.die.net/man/1/crontab): It downloads every day shortly after midnight (00:05) events and data of the previous day (the download time window must be configured in the download.yaml file). Afterwards, it computes the energy magnitude at 5:00 AM (5 hours are a more than sufficient time to complete the download of all data): ```bash # For more information see the manual pages of crontab(5) and cron(8) # # m h dom mon dow command 5 0 * * * [VENV_PATH]/bin/python [VENV_PATH]/bin/s2s download -c /home/download.private.yaml 0 5 * * * [VENV_PATH]/bin/python [VENV_PATH]/bin/me-compute -d [DOWNLOAD_YAML] -s [START] -e [END] \"[ROOT_DIR]/me-result_%S%_%E%\" ``` ### Misc #### Run tests and generate test data Run: ```commandline pytest ./me-compute/test ``` Note that there is only one test routine generating files in a `test/tmp` directory (git-ignored). The directory is **not** deleted automatically in order to leave developers the ability to perform an additional visual test on the generated output (e.g. HTML report) ## Citation > Zaccarelli, Riccardo (2023): me-compute: a Python software to download events and data from FDSN web services and compute their energy magnitude (Me). GFZ Data Services. https://doi.org/10.5880/GFZ.2.6.2023.008\n",
                "dependencies": "blinker==1.6.2 blosc2==2.0.0 certifi==2023.5.7 charset-normalizer==3.1.0 click==8.1.3 contourpy==1.1.0 cycler==0.11.0 Cython==0.29.35 decorator==5.1.1 Flask==2.3.2 fonttools==4.40.0 greenlet==2.0.2 idna==3.4 itsdangerous==2.1.2 Jinja2==3.1.2 kiwisolver==1.4.4 lxml==4.9.2 MarkupSafe==2.1.3 matplotlib==3.7.1 # -e git+https://github.com/rizac/me-compute.git@5b34509404d4de9be0c1656f865c0d900da1b105#egg=me_compute msgpack==1.0.5 numexpr==2.8.4 numpy==1.25.0 obspy==1.4.0 packaging==23.1 pandas==2.0.2 Pillow==9.5.0 psutil==5.9.5 psycopg2==2.9.6 py-cpuinfo==9.0.0 pyparsing==3.1.0 python-dateutil==2.8.2 pytz==2023.3 PyYAML==6.0 requests==2.31.0 scipy==1.10.1 sdaas @ git+https://git@github.com/rizac/sdaas.git@6647d5dd3e802019a5fcdaf9758f912242a105c9 six==1.16.0 SQLAlchemy==2.0.16 stream2segment @ git+https://git@github.com/rizac/stream2segment.git@a7a9a24af9b29eb71f3a592bfc76b9ef5a947dc0 tables==3.8.0 typing_extensions==4.6.3 tzdata==2023.3 urllib3==2.0.3 Werkzeug==2.3.6\n\"\"\"A setuptools based setup module. Taken from: https://github.com/pypa/sampleproject/blob/master/setup.py See also: http://python-packaging-user-guide.readthedocs.org/en/latest/distributing/ Additional links: https://packaging.python.org/en/latest/distributing.html https://github.com/pypa/sampleproject \"\"\" from __future__ import print_function # Always prefer setuptools over distutils from setuptools import setup, find_packages # To use a consistent encoding from codecs import open from os import path here = path.abspath(path.dirname(__file__)) # Get the long description from the README file with open(path.join(here, 'README.md'), encoding='utf-8') as f: long_description = f.read() # http://stackoverflow.com/questions/2058802/how-can-i-get-the-version-defined-in-setup-py-setuptools-in-my-package version = \"1.0\" # with open(path.join(here, 'program_version')) as version_file: # version = version_file.read().strip() # copy config directory: config_src = path.abspath(path.join(here, 'mecompute', 'base-config')) config_dest = path.abspath(path.join(here, 'config')) if not path.isdir(config_dest): import shutil shutil.copytree(config_src, config_dest) setup( name='me-compute', # Versions should comply with PEP440. For a discussion on single-sourcing # the version across setup.py and the project code, see # https://packaging.python.org/en/latest/single_source_version.html version=version, description='A Python project to compute energy agnitude (Me) from downloaded seismic events', long_description=long_description, # The project's main homepage. url='https://github.com/rizac/me-compute', # Author details author='riccardo zaccarelli', author_email='rizac@gfz-potsdam.de', # FIXME: what to provide? # Choose your license license='GPL', python_requires='>=3.8', # See https://pypi.python.org/pypi?%3Aaction=list_classifiers classifiers=[ # How mature is this project? Common values are # 3 - Alpha # 4 - Beta # 5 - Production/Stable 'Development Status :: 3 - Alpha', # Indicate who your project is intended for 'Intended Audience :: Science/Research', 'Topic :: Scientific/Engineering', # Pick your license as you wish (should match \"license\" above) 'License :: OSI Approved :: GNU License', # Specify the Python versions you support here. # 'Programming Language :: Python :: 3.5', # 'Programming Language :: Python :: 3.6', # 'Programming Language :: Python :: 3.7', # 'Programming Language :: Python :: 3.8', 'Programming Language :: Python :: 3.9', 'Programming Language :: Python :: 3.10', 'Programming Language :: Python :: 3.11', ], # What does your project relate to? keywords='compute Magnitude Energy (Me)', # You can just specify the packages manually here if your project is # simple. Or you can use find_packages(). packages=find_packages(exclude=['contrib', 'docs', 'tests', 'htmlcov']), # Alternatively, if you want to distribute just a my_module.py, uncomment # this: # py_modules=[\"my_module\"], # List run-time dependencies here. These will be installed by pip when # your project is installed. For info see: # https://packaging.python.org/en/latest/requirements.html install_requires=[ 'stream2segment @ git+https://git@github.com/rizac/stream2segment.git', 'sdaas @ git+https://git@github.com/rizac/sdaas.git' # 'git+https://github.com/rizac/stream2segment.git#egg=stream2segment', # 'git+https://github.com/rizac/sdaas.git#egg=sdaas' ], # List additional groups of dependencies here (e.g. development # dependencies). You can install these using the following syntax, # for example: # $ pip install -e .[dev,test] (pip install -e \".[dev,test]\" in zsh) extras_require={ # use latest versions. Without boundaries 'dev': [# 'pep8>=1.7.0', 'pylint', 'pytest', # 'pytest-cov>=2.5.1', # 'pytest-mock>=1.6.2' ], # 'jupyter': ['jupyter>=1.0.0'] }, # If there are data files included in your packages that need to be # installed, specify them here. If using Python 2.6 or less, then these # have to be included in MANIFEST.in as well. # # package_data={ # 'sample': ['package_data.dat'], # }, # make the installation process copy also the package data (see MANIFEST.in) # for info see https://python-packaging.readthedocs.io/en/latest/non-code-files.html include_package_data=True, # zip_safe=False, # Although 'package_data' is the preferred approach, in some case you may # need to place data files outside of your packages. See: # http://docs.python.org/3/distutils/setupscript.html#installing-additional-files # noqa # In this case, 'data_file' will be installed into '<sys.prefix>/my_data' # # data_files=[('my_data', ['data/data_file'])], # To provide executable scripts, use entry points in preference to the # \"scripts\" keyword. Entry points provide cross-platform support and allow # pip to create the appropriate form of executable for the target platform. entry_points={ 'console_scripts': [ 'me-compute=mecompute.cli:cli', ], }, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mitk",
            "repo_link": "https://github.com/MITK/MITK",
            "content": {
                "codemeta": "",
                "readme": "![MITK Logo][logo] The [Medical Imaging Interaction Toolkit][mitk] (MITK) is a free open-source software system for development of interactive medical image processing software. MITK combines the [Insight Toolkit][itk] (ITK) and the [Visualization Toolkit][vtk] (VTK) with an application framework. The links below provide high-level and reference documentation targeting different usage scenarios: - Get a [high-level overview][mitk-overview] about MITK with pointers to further documentation - End-users looking for help with MITK applications should read the [MITK User Manual][mitk-usermanual] - Developers contributing to or using MITK, please see the [MITK Developer Manual][mitk-devmanual] as well as the [MITK API Reference][mitk-apiref] See the [MITK homepage][mitk] for details. Supported platforms ------------------- MITK is a cross-platform C++ toolkit and officially supports: - Windows - Linux - macOS For details, please read the [Supported Platforms][platforms] page. License ------- Copyright (c) [German Cancer Research Center (DKFZ)][dkfz]. All rights reserved. MITK is available as free open-source software under a [3-clause BSD license][license]. Download -------- The *MitkWorkbench* application and a bunch of command-line apps are released twice per year on our [Download][download] page and the [GitHub Releases][releases] page. The official MITK source code is available in the [MITK Git repository][git_repo]. The Git clone command is git clone https://github.com/MITK/MITK.git Active development takes place in the MITK develop branch and its usage is advised for advanced users only. How to contribute ----------------- Contributions are encouraged. To make the contribution process as smooth as possible, please read [Contributing to MITK][contribute] before. Build instructions ------------------ MITK uses [CMake][cmake] to configure a build tree. The following is a crash course about cloning, configuring, and building MITK with Ninja on Linux or macOS when all [prerequisites][prerequisites] are met: git clone https://github.com/MITK/MITK.git mkdir MITK-superbuild cmake -S MITK -B MITK-superbuild -G \"Ninja\" -D CMAKE_BUILD_TYPE=Release cmake --build MITK-superbuild On Windows, configuring and building with Visual Studio/MSBuild would look something like this: cmake -S MITK -B MITK-superbuild -G \"Visual Studio 17 2022\" cmake --build MITK-superbuild --config Release -- -m Read the comprehensive [build instructions][build] page for details. Useful links ------------ - [Homepage][mitk] - [Download][download] - [Create an issue/ask for help][issues] [logo]: https://github.com/MITK/MITK/raw/master/mitk.png [mitk]: https://www.mitk.org [itk]: https://itk.org [vtk]: https://vtk.org [mitk-overview]: https://docs.mitk.org/nightly/ [mitk-usermanual]: https://docs.mitk.org/nightly/UserManualPortal.html [mitk-devmanual]: https://docs.mitk.org/nightly/DeveloperManualPortal.html [mitk-apiref]: https://docs.mitk.org/nightly/usergroup0.html [platforms]: https://docs.mitk.org/nightly/SupportedPlatformsPage.html [prerequisites]: https://docs.mitk.org/nightly/BuildInstructionsPage.html#BuildInstructions_Prerequisites [build]: https://docs.mitk.org/nightly/BuildInstructionsPage.html [dkfz]: https://www.dkfz.de [license]: https://github.com/MITK/MITK/blob/master/LICENSE [download]: https://www.mitk.org/Download [releases]: https://github.com/MITK/MITK/releases [git_repo]: https://github.com/MITK/MITK [contribute]: https://github.com/MITK/MITK/blob/master/CONTRIBUTING.md [cmake]: https://www.cmake.org [issues]: https://github.com/MITK/MITK/issues\n",
                "dependencies": "#[[ When increasing the minimum required version, check if Boost_ADDITIONAL_VERSIONS in CMake/PackageDepends/MITK_Boost_Config.cmake can be removed. See the first long comment in CMakeExternals/Boost.cmake for details. ]] set(MITK_CMAKE_MINIMUM_REQUIRED_VERSION 3.18) cmake_minimum_required(VERSION ${MITK_CMAKE_MINIMUM_REQUIRED_VERSION}) if(CMAKE_VERSION VERSION_GREATER_EQUAL 3.19 AND CMAKE_VERSION VERSION_LESS 3.19.2) message(FATAL_ERROR \"\\ CMake v${CMAKE_VERSION} is defective [1]. \\ Please either downgrade to v3.18 or upgrade to at least v3.19.2.\\n\\ [1] https://gitlab.kitware.com/cmake/cmake/-/issues/21529\") endif() #----------------------------------------------------------------------------- # Policies #----------------------------------------------------------------------------- #[[ T28060 https://cmake.org/cmake/help/v3.18/policy/CMP0091.html https://cmake.org/cmake/help/v3.18/variable/CMAKE_MSVC_RUNTIME_LIBRARY.html We pass CMP0091 to all external projects as command-line argument: -DCMAKE_POLICY_DEFAULT_CMP0091:STRING=OLD ]] cmake_policy(SET CMP0091 OLD) if(POLICY CMP0135) # https://cmake.org/cmake/help/v3.24/policy/CMP0135.html # Set timestamps of extracted ExternalProject_Add() downloads to time of extraction. cmake_policy(SET CMP0135 NEW) endif() if(POLICY CMP0167) # https://cmake.org/cmake/help/v3.30/policy/CMP0167.html # The FindBoost module is removed. cmake_policy(SET CMP0167 OLD) endif() #----------------------------------------------------------------------------- # Superbuild Option - Enabled by default #----------------------------------------------------------------------------- option(MITK_USE_SUPERBUILD \"Build MITK and the projects it depends on via SuperBuild.cmake.\" ON) if(MITK_USE_SUPERBUILD) project(MITK-superbuild) set(MITK_SOURCE_DIR ${PROJECT_SOURCE_DIR}) set(MITK_BINARY_DIR ${PROJECT_BINARY_DIR}) else() project(MITK VERSION 2024.12.99) include_directories(SYSTEM ${MITK_SUPERBUILD_BINARY_DIR}) endif() #----------------------------------------------------------------------------- # MITK Extension Feature #----------------------------------------------------------------------------- set(MITK_EXTENSION_DIRS \"\" CACHE STRING \"\") unset(MITK_ABSOLUTE_EXTENSION_DIRS) foreach(MITK_EXTENSION_DIR ${MITK_EXTENSION_DIRS}) get_filename_component(MITK_ABSOLUTE_EXTENSION_DIR \"${MITK_EXTENSION_DIR}\" ABSOLUTE) list(APPEND MITK_ABSOLUTE_EXTENSION_DIRS \"${MITK_ABSOLUTE_EXTENSION_DIR}\") endforeach() set(MITK_DIR_PLUS_EXTENSION_DIRS \"${MITK_SOURCE_DIR}\" ${MITK_ABSOLUTE_EXTENSION_DIRS}) #----------------------------------------------------------------------------- # Update CMake module path #----------------------------------------------------------------------------- set(MITK_CMAKE_DIR ${MITK_SOURCE_DIR}/CMake) set(CMAKE_MODULE_PATH ${MITK_CMAKE_DIR}) foreach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS}) set(MITK_CMAKE_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/CMake\") if(EXISTS \"${MITK_CMAKE_EXTENSION_DIR}\") list(APPEND CMAKE_MODULE_PATH \"${MITK_CMAKE_EXTENSION_DIR}\") endif() endforeach() #----------------------------------------------------------------------------- # CMake function(s) and macro(s) #----------------------------------------------------------------------------- # Standard CMake macros include(FeatureSummary) include(CTest) include(CMakeParseArguments) include(FindPackageHandleStandardArgs) # MITK macros include(mitkFunctionGetGccVersion) include(mitkFunctionCheckCompilerFlags) include(mitkFunctionSuppressWarnings) # includes several functions include(mitkMacroEmptyExternalProject) include(mitkFunctionEnableBuildConfiguration) include(mitkFunctionWhitelists) include(mitkFunctionAddExternalProject) include(mitkFunctionAddLibrarySearchPaths) SUPPRESS_VC_DEPRECATED_WARNINGS() #----------------------------------------------------------------------------- # Set a default build type if none was specified #----------------------------------------------------------------------------- if(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES) message(STATUS \"Setting build type to 'Debug' as none was specified.\") set(CMAKE_BUILD_TYPE Debug CACHE STRING \"Choose the type of build.\" FORCE) # Set the possible values of build type for cmake-gui set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\") endif() if(CMAKE_COMPILER_IS_GNUCXX) mitkFunctionGetGccVersion(${CMAKE_CXX_COMPILER} GCC_VERSION) else() set(GCC_VERSION 0) endif() set(MITK_CXX_STANDARD 17) set(CMAKE_CXX_EXTENSIONS 0) set(CMAKE_CXX_STANDARD ${MITK_CXX_STANDARD}) set(CMAKE_CXX_STANDARD_REQUIRED 1) # This is necessary to avoid problems with compile feature checks. # CMAKE_CXX_STANDARD seems to only set the -std=c++<std> flag for targets. # However, compile flag checks also need to be done with -std=c++<std>. # The MITK_CXX<std>_FLAG variable is also used for external projects # build during the MITK super-build. mitkFunctionCheckCompilerFlags(\"-std=c++${MITK_CXX_STANDARD}\" MITK_CXX${MITK_CXX_STANDARD}_FLAG) #----------------------------------------------------------------------------- # Warn if source or build path is too long #----------------------------------------------------------------------------- if(WIN32) set(_src_dir_length_max 50) set(_bin_dir_length_max 50) if(MITK_USE_SUPERBUILD) set(_src_dir_length_max 34) # _src_dir_length_max - strlen(ep/src/ITK-build) set(_bin_dir_length_max 40) # _bin_dir_length_max - strlen(MITK-build) endif() string(LENGTH \"${MITK_SOURCE_DIR}\" _src_n) string(LENGTH \"${MITK_BINARY_DIR}\" _bin_n) # The warnings should be converted to errors if(_src_n GREATER _src_dir_length_max) message(WARNING \"MITK source code directory path length is too long (${_src_n} > ${_src_dir_length_max}).\" \"Please move the MITK source code directory to a directory with a shorter path.\" ) endif() if(_bin_n GREATER _bin_dir_length_max) message(WARNING \"MITK build directory path length is too long (${_bin_n} > ${_bin_dir_length_max}).\" \"Please move the MITK build directory to a directory with a shorter path.\" ) endif() endif() #----------------------------------------------------------------------------- # Switch on parallel builds in Visual Studio #----------------------------------------------------------------------------- # Since Visual Studio 2019 v16.9 a new Multi-ToolTask scheduler is available, # which schedules and limits tasks both within (was /MP) and across targets (-m). # Make sure to add the -m option in build scripts when using MSBuild or CMake # to build everything: # # msbuild -m # # resp. # # cmake --build <path> -- -m if(MSVC_VERSION VERSION_GREATER_EQUAL 1928) if(NOT CMAKE_VS_GLOBALS MATCHES \"(^|;)UseMultiToolTask=\") list(APPEND CMAKE_VS_GLOBALS UseMultiToolTask=true) endif() if(NOT CMAKE_VS_GLOBALS MATCHES \"(^|;)EnforceProcessCountAcrossBuilds=\") list(APPEND CMAKE_VS_GLOBALS EnforceProcessCountAcrossBuilds=true) endif() endif() #----------------------------------------------------------------------------- # Additional MITK Options (also shown during superbuild) #----------------------------------------------------------------------------- # ----------------------------------------- # General build options option(BUILD_SHARED_LIBS \"Build MITK with shared libraries\" ON) option(WITH_COVERAGE \"Enable/Disable coverage\" OFF) option(BUILD_TESTING \"Test the project\" ON) option(MITK_FAST_TESTING \"Disable long-running tests like packaging\" OFF) option(MITK_XVFB_TESTING \"Execute test drivers through xvfb-run\" OFF) option(MITK_PCH \"Enable precompiled headers\" ON) option(MITK_BUILD_ALL_APPS \"Build all MITK applications\" OFF) option(MITK_BUILD_EXAMPLES \"Build the MITK Examples\" OFF) mark_as_advanced( MITK_XVFB_TESTING MITK_FAST_TESTING MITK_BUILD_ALL_APPS MITK_PCH ) #----------------------------------------------------------------------------- # Set UI testing flags #----------------------------------------------------------------------------- if(MITK_XVFB_TESTING) set(MITK_XVFB_TESTING_COMMAND \"xvfb-run\" \"--auto-servernum\" CACHE STRING \"Command and options to test through Xvfb\") mark_as_advanced(MITK_XVFB_TESTING_COMMAND) endif(MITK_XVFB_TESTING) # ----------------------------------------- # Other options set(MITK_CUSTOM_REVISION_DESC \"\" CACHE STRING \"Override MITK revision description\") mark_as_advanced(MITK_CUSTOM_REVISION_DESC) set_property(GLOBAL PROPERTY MITK_EXTERNAL_PROJECTS \"\") include(CMakeExternals/ExternalProjectList.cmake) foreach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS}) set(MITK_CMAKE_EXTERNALS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/CMakeExternals\") if(EXISTS \"${MITK_CMAKE_EXTERNALS_EXTENSION_DIR}/ExternalProjectList.cmake\") include(\"${MITK_CMAKE_EXTERNALS_EXTENSION_DIR}/ExternalProjectList.cmake\") endif() endforeach() # ----------------------------------------- # Other MITK_USE_* options not related to # external projects build via the # MITK superbuild option(MITK_USE_BLUEBERRY \"Build the BlueBerry platform\" ON) option(MITK_USE_OpenMP \"Use OpenMP\" OFF) option(MITK_USE_Python3 \"Use Python 3\" OFF) #----------------------------------------------------------------------------- # Build configurations #----------------------------------------------------------------------------- set(_buildConfigs \"Custom\") file(GLOB _buildConfigFiles CMake/BuildConfigurations/*.cmake) foreach(_buildConfigFile ${_buildConfigFiles}) get_filename_component(_buildConfigFile ${_buildConfigFile} NAME_WE) list(APPEND _buildConfigs ${_buildConfigFile}) endforeach() foreach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS}) file(GLOB _extBuildConfigFiles \"${MITK_EXTENSION_DIR}/CMake/BuildConfigurations/*.cmake\") foreach(_extBuildConfigFile ${_extBuildConfigFiles}) get_filename_component(_extBuildConfigFile \"${_extBuildConfigFile}\" NAME_WE) list(APPEND _buildConfigs \"${_extBuildConfigFile}\") endforeach() list(REMOVE_DUPLICATES _buildConfigs) endforeach() set(MITK_BUILD_CONFIGURATION \"WorkbenchRelease\" CACHE STRING \"Use pre-defined MITK configurations\") set_property(CACHE MITK_BUILD_CONFIGURATION PROPERTY STRINGS ${_buildConfigs}) mitkFunctionEnableBuildConfiguration() mitkFunctionCreateWhitelistPaths(MITK) mitkFunctionFindWhitelists(MITK) # ----------------------------------------- # Qt version related variables option(MITK_USE_Qt6 \"Use Qt 6 library\" ON) if(MITK_USE_Qt6) set(MITK_QT6_MINIMUM_VERSION 6.6) set(MITK_QT6_COMPONENTS Concurrent Core Core5Compat CoreTools Designer DesignerComponentsPrivate Gui Help LinguistTools Network OpenGL OpenGLWidgets Qml Sql StateMachine Svg ToolsTools UiTools WebEngineCore WebEngineWidgets Widgets Xml ) if(APPLE) list(APPEND MITK_QT6_COMPONENTS DBus) endif() # Hint at default install locations of Qt if(NOT Qt6_DIR) if(MSVC) set(_dir_candidates \"C:/Qt\") if(CMAKE_GENERATOR MATCHES \"^Visual Studio [0-9]+ ([0-9]+)\") set(_compilers \"msvc${CMAKE_MATCH_1}\") elseif(CMAKE_GENERATOR MATCHES \"Ninja\") include(mitkFunctionGetMSVCVersion) mitkFunctionGetMSVCVersion() if(VISUAL_STUDIO_PRODUCT_NAME MATCHES \"^Visual Studio ([0-9]+)\") set(_compilers \"msvc${CMAKE_MATCH_1}\") endif() endif() if(_compilers MATCHES \"[0-9]+\") if (CMAKE_MATCH_0 EQUAL 2022) list(APPEND _compilers \"msvc2019\") # Binary compatible endif() endif() else() set(_dir_candidates ~/Qt) if(APPLE) set(_compilers clang) else() list(APPEND _dir_candidates /opt/Qt) set(_compilers gcc) endif() endif() if(CMAKE_SIZEOF_VOID_P EQUAL 8) foreach(_compiler ${_compilers}) list(APPEND _compilers64 \"${_compiler}_64\") endforeach() set(_compilers ${_compilers64}) endif() if(APPLE) list(APPEND _compilers macos) endif() foreach(_dir_candidate ${_dir_candidates}) get_filename_component(_dir_candidate ${_dir_candidate} REALPATH) foreach(_compiler ${_compilers}) set(_glob_expression \"${_dir_candidate}/6.*/${_compiler}\") file(GLOB _hints ${_glob_expression}) list(SORT _hints) list(APPEND MITK_QT6_HINTS ${_hints}) endforeach() endforeach() endif() find_package(Qt6 ${MITK_QT6_MINIMUM_VERSION} COMPONENTS ${MITK_QT6_COMPONENTS} REQUIRED HINTS ${MITK_QT6_HINTS}) get_target_property(QT_QMAKE_EXECUTABLE Qt6::qmake LOCATION) get_target_property(QT_HELPGENERATOR_EXECUTABLE Qt6::qhelpgenerator LOCATION) endif() if(Qt6_DIR) list(APPEND CMAKE_PREFIX_PATH \"${Qt6_DIR}/../../..\") list(REMOVE_DUPLICATES CMAKE_PREFIX_PATH) endif() # ----------------------------------------- # Custom dependency logic if(WIN32 AND Qt6_DIR) set(_dir_candidate \"${Qt6_DIR}/../../../../../Tools/OpenSSLv3/Win_x64\") get_filename_component(_dir_candidate ${_dir_candidate} ABSOLUTE) if(EXISTS \"${_dir_candidate}\") set(OPENSSL_ROOT_DIR \"${_dir_candidate}\") endif() endif() find_package(OpenSSL 3) if(NOT OpenSSL_FOUND) find_package(OpenSSL 1.1.1) endif() option(MITK_USE_SYSTEM_Boost \"Use the system Boost\" OFF) set(MITK_USE_Boost_LIBRARIES \"\" CACHE STRING \"A semi-colon separated list of required Boost libraries\") if(MITK_USE_httplib AND NOT OpenSSL_FOUND) set(openssl_message \"Could not find OpenSSL (dependency of cpp-httplib).\\n\") if(UNIX) if(APPLE) set(openssl_message \"${openssl_message}Please install it using your favorite package management \" \"system (i.e. Homebrew or MacPorts).\\n\") else() set(openssl_message \"${openssl_message}Please install the dev package of OpenSSL (i.e. libssl-dev).\\n\") endif() else() set(openssl_message \"${openssl_message}Please either install Win32 OpenSSL:\\n\" \" https://slproweb.com/products/Win32OpenSSL.html\\n\" \"Or use the Qt Maintenance tool to install (recommended):\\n\" \" Developer and Designer Tools > OpenSSL Toolkit > OpenSSL 64-bit binaries\\n\") endif() set(openssl_message \"${openssl_message}If it still cannot be found, you can hint CMake to find OpenSSL by \" \"adding/setting the OPENSSL_ROOT_DIR variable to the root directory of an \" \"OpenSSL installation. Make sure to clear variables of partly found \" \"versions of OpenSSL before, or they will be mixed up.\") message(FATAL_ERROR ${openssl_message}) endif() if(MITK_USE_Python3) set(MITK_USE_ZLIB ON CACHE BOOL \"\" FORCE) if(APPLE) set(python3_mininum_version 3.11) else() set(python3_mininum_version 3.8) endif() find_package(Python3 ${python3_mininum_version} REQUIRED COMPONENTS Interpreter Development NumPy) if(WIN32) string(REPLACE \"\\\\\" \"/\" Python3_STDARCH \"${Python3_STDARCH}\") string(REPLACE \"\\\\\" \"/\" Python3_STDLIB \"${Python3_STDLIB}\") string(REPLACE \"\\\\\" \"/\" Python3_SITELIB \"${Python3_SITELIB}\") endif() endif() if(BUILD_TESTING AND NOT MITK_USE_CppUnit) message(\"> Forcing MITK_USE_CppUnit to ON because BUILD_TESTING=ON\") set(MITK_USE_CppUnit ON CACHE BOOL \"Use CppUnit for unit tests\" FORCE) endif() if(MITK_USE_BLUEBERRY) option(MITK_BUILD_ALL_PLUGINS \"Build all MITK plugins\" OFF) mark_as_advanced(MITK_BUILD_ALL_PLUGINS) if(NOT MITK_USE_CTK) message(\"> Forcing MITK_USE_CTK to ON because of MITK_USE_BLUEBERRY\") set(MITK_USE_CTK ON CACHE BOOL \"Use CTK in MITK\" FORCE) endif() endif() #----------------------------------------------------------------------------- # Pixel type multiplexing #----------------------------------------------------------------------------- # Customize the default pixel types for multiplex macros set(MITK_ACCESSBYITK_INTEGRAL_PIXEL_TYPES \"int, unsigned int, short, unsigned short, char, unsigned char\" CACHE STRING \"List of integral pixel types used in AccessByItk and InstantiateAccessFunction macros\") set(MITK_ACCESSBYITK_FLOATING_PIXEL_TYPES \"double, float\" CACHE STRING \"List of floating pixel types used in AccessByItk and InstantiateAccessFunction macros\") set(MITK_ACCESSBYITK_COMPOSITE_PIXEL_TYPES \"itk::RGBPixel<unsigned char>, itk::RGBAPixel<unsigned char>\" CACHE STRING \"List of composite pixel types used in AccessByItk and InstantiateAccessFunction macros\") set(MITK_ACCESSBYITK_DIMENSIONS \"2,3\" CACHE STRING \"List of dimensions used in AccessByItk and InstantiateAccessFunction macros\") mark_as_advanced(MITK_ACCESSBYITK_INTEGRAL_PIXEL_TYPES MITK_ACCESSBYITK_FLOATING_PIXEL_TYPES MITK_ACCESSBYITK_COMPOSITE_PIXEL_TYPES MITK_ACCESSBYITK_DIMENSIONS ) # consistency checks if(NOT MITK_ACCESSBYITK_INTEGRAL_PIXEL_TYPES) set(MITK_ACCESSBYITK_INTEGRAL_PIXEL_TYPES \"int, unsigned int, short, unsigned short, char, unsigned char\" CACHE STRING \"List of integral pixel types used in AccessByItk and InstantiateAccessFunction macros\" FORCE) endif() if(NOT MITK_ACCESSBYITK_FLOATING_PIXEL_TYPES) set(MITK_ACCESSBYITK_FLOATING_PIXEL_TYPES \"double, float\" CACHE STRING \"List of floating pixel types used in AccessByItk and InstantiateAccessFunction macros\" FORCE) endif() if(NOT MITK_ACCESSBYITK_COMPOSITE_PIXEL_TYPES) set(MITK_ACCESSBYITK_COMPOSITE_PIXEL_TYPES \"itk::RGBPixel<unsigned char>, itk::RGBAPixel<unsigned char>\" CACHE STRING \"List of composite pixel types used in AccessByItk and InstantiateAccessFunction macros\" FORCE) endif() if(NOT MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES) string(REPLACE \",\" \";\" _integral_types ${MITK_ACCESSBYITK_INTEGRAL_PIXEL_TYPES}) string(REPLACE \",\" \";\" _floating_types ${MITK_ACCESSBYITK_FLOATING_PIXEL_TYPES}) foreach(_scalar_type ${_integral_types} ${_floating_types}) set(MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES \"${MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES}itk::VariableLengthVector<${_scalar_type}>,\") endforeach() string(LENGTH \"${MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES}\" _length) math(EXPR _length \"${_length} - 1\") string(SUBSTRING \"${MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES}\" 0 ${_length} MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES) set(MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES ${MITK_ACCESSBYITK_VECTOR_PIXEL_TYPES} CACHE STRING \"List of vector pixel types used in AccessByItk and InstantiateAccessFunction macros for itk::VectorImage types\" FORCE) endif() if(NOT MITK_ACCESSBYITK_DIMENSIONS) set(MITK_ACCESSBYITK_DIMENSIONS \"2,3\" CACHE STRING \"List of dimensions used in AccessByItk and InstantiateAccessFunction macros\") endif() find_package(Git REQUIRED) #----------------------------------------------------------------------------- # Superbuild script #----------------------------------------------------------------------------- if(MITK_USE_SUPERBUILD) include(\"${CMAKE_CURRENT_SOURCE_DIR}/SuperBuild.cmake\") # Print configuration summary message(\"\\n\\n\") feature_summary( DESCRIPTION \"------- FEATURE SUMMARY FOR ${PROJECT_NAME} -------\" WHAT ALL) return() endif() #***************************************************************************** #**************************** END OF SUPERBUILD **************************** #***************************************************************************** #----------------------------------------------------------------------------- # Organize MITK targets in folders #----------------------------------------------------------------------------- set_property(GLOBAL PROPERTY USE_FOLDERS ON) set(MITK_ROOT_FOLDER \"MITK\" CACHE STRING \"\") mark_as_advanced(MITK_ROOT_FOLDER) #----------------------------------------------------------------------------- # CMake function(s) and macro(s) #----------------------------------------------------------------------------- include(WriteBasicConfigVersionFile) include(CheckCXXSourceCompiles) include(GenerateExportHeader) include(mitkFunctionAddManifest) include(mitkFunctionAddCustomModuleTest) include(mitkFunctionCheckModuleDependencies) include(mitkFunctionCompileSnippets) include(mitkFunctionConfigureVisualStudioUserProjectFile) include(mitkFunctionCreateBlueBerryApplication) include(mitkFunctionCreateCommandLineApp) include(mitkFunctionCreateModule) include(mitkFunctionCreatePlugin) include(mitkFunctionCreateProvisioningFile) include(mitkFunctionGetLibrarySearchPaths) include(mitkFunctionGetVersion) include(mitkFunctionGetVersionDescription) include(mitkFunctionInstallAutoLoadModules) include(mitkFunctionInstallCTKPlugin) include(mitkFunctionInstallProvisioningFiles) include(mitkFunctionInstallThirdPartyCTKPlugins) include(mitkFunctionOrganizeSources) include(mitkFunctionUseModules) if( ${MITK_USE_MatchPoint} ) include(mitkFunctionCreateMatchPointDeployedAlgorithm) endif() include(mitkMacroConfigureItkPixelTypes) include(mitkMacroCreateExecutable) include(mitkMacroCreateModuleTests) include(mitkMacroGenerateToolsLibrary) include(mitkMacroGetLinuxDistribution) include(mitkMacroGetPMDPlatformString) include(mitkMacroInstall) include(mitkMacroInstallHelperApp) include(mitkMacroInstallTargets) include(mitkMacroMultiplexPicType) #----------------------------------------------------------------------------- # Global CMake variables #----------------------------------------------------------------------------- if(NOT DEFINED CMAKE_DEBUG_POSTFIX) # We can't do this yet because the CTK Plugin Framework # cannot cope with a postfix yet. #set(CMAKE_DEBUG_POSTFIX d) endif() #----------------------------------------------------------------------------- # Output directories. #----------------------------------------------------------------------------- set(_default_LIBRARY_output_dir lib) set(_default_RUNTIME_output_dir bin) set(_default_ARCHIVE_output_dir lib) foreach(type LIBRARY RUNTIME ARCHIVE) # Make sure the directory exists if(MITK_CMAKE_${type}_OUTPUT_DIRECTORY AND NOT EXISTS ${MITK_CMAKE_${type}_OUTPUT_DIRECTORY}) message(\"Creating directory MITK_CMAKE_${type}_OUTPUT_DIRECTORY: ${MITK_CMAKE_${type}_OUTPUT_DIRECTORY}\") file(MAKE_DIRECTORY \"${MITK_CMAKE_${type}_OUTPUT_DIRECTORY}\") endif() if(MITK_CMAKE_${type}_OUTPUT_DIRECTORY) set(CMAKE_${type}_OUTPUT_DIRECTORY ${MITK_CMAKE_${type}_OUTPUT_DIRECTORY}) else() set(CMAKE_${type}_OUTPUT_DIRECTORY ${PROJECT_BINARY_DIR}/${_default_${type}_output_dir}) set(MITK_CMAKE_${type}_OUTPUT_DIRECTORY ${CMAKE_${type}_OUTPUT_DIRECTORY}) endif() set(CMAKE_${type}_OUTPUT_DIRECTORY ${CMAKE_${type}_OUTPUT_DIRECTORY} CACHE INTERNAL \"Output directory for ${type} files.\") mark_as_advanced(CMAKE_${type}_OUTPUT_DIRECTORY) endforeach() #----------------------------------------------------------------------------- # Set MITK specific options and variables (NOT available during superbuild) #----------------------------------------------------------------------------- if(OpenSSL_FOUND AND WIN32) #[[ On Windows, CMake is able to locate the link libraries for OpenSSL but it does not look for the corresponding DLLs that we need to copy to our binary directories and include in packaging. Setting these paths manually is cumbersome so we try to use a simple heuristic to automatically set them: - Based on the link libraries (usually located in a lib folder), try to find the \"../bin\" binary directory. - Use the base file names of the link libraries to find corresponding DLLs like \"<base name>*.dll\", that usually are named like \"<base name>-1_1-x64.dll\" or similar. ]] set(openssl_ssl_dll \"\") set(openssl_crypto_dll \"\") if(OPENSSL_SSL_LIBRARY) set(openssl_ssl_lib \"\") list(LENGTH OPENSSL_SSL_LIBRARY num_items) if(num_items GREATER 1) list(FIND OPENSSL_SSL_LIBRARY \"optimized\" optimized) if(NOT optimized EQUAL -1) math(EXPR optimized \"${optimized}+1\") if(optimized LESS num_items) list(GET OPENSSL_SSL_LIBRARY ${optimized} openssl_ssl_lib) endif() endif() else() set(openssl_ssl_lib \"${OPENSSL_SSL_LIBRARY}\") endif() if(EXISTS \"${openssl_ssl_lib}\") get_filename_component(openssl_bin_dir \"${openssl_ssl_lib}\" DIRECTORY) get_filename_component(openssl_bin_dir \"${openssl_bin_dir}\" DIRECTORY) set(openssl_bin_dir \"${openssl_bin_dir}/bin\") if(EXISTS \"${openssl_bin_dir}\") get_filename_component(openssl_ssl_basename \"${openssl_ssl_lib}\" NAME_WE) file(GLOB openssl_ssl_dll \"${openssl_bin_dir}/${openssl_ssl_basename}*.dll\") list(LENGTH openssl_ssl_dll num_findings) if(num_findings GREATER 1) set(openssl_ssl_dll \"\") endif() set(openssl_crypto_lib \"\") list(LENGTH OPENSSL_CRYPTO_LIBRARY num_items) if(num_items GREATER 1) list(FIND OPENSSL_CRYPTO_LIBRARY \"optimized\" optimized) if(NOT optimized EQUAL -1) math(EXPR optimized \"${optimized}+1\") if(optimized LESS num_items) list(GET OPENSSL_CRYPTO_LIBRARY ${optimized} openssl_crypto_lib) endif() endif() else() set(openssl_crypto_lib \"${OPENSSL_CRYPTO_LIBRARY}\") endif() get_filename_component(openssl_crypto_basename \"${openssl_crypto_lib}\" NAME_WE) file(GLOB openssl_crypto_dll \"${openssl_bin_dir}/${openssl_crypto_basename}*.dll\") list(LENGTH openssl_crypto_dll num_findings) if(num_findings GREATER 1) set(openssl_crypto_dll \"\") endif() endif() endif() endif() set(MITK_OPENSSL_SSL_DLL \"${openssl_ssl_dll}\" CACHE FILEPATH \"\") if(DEFINED CACHE{MITK_OPENSSL_SSL_DLL} AND NOT MITK_OPENSSL_SSL_DLL AND openssl_ssl_dll) set(MITK_OPENSSL_SSL_DLL \"${openssl_ssl_dll}\" CACHE FILEPATH \"\" FORCE) endif() set(MITK_OPENSSL_CRYPTO_DLL \"${openssl_crypto_dll}\" CACHE FILEPATH \"\") if(DEFINED CACHE{MITK_OPENSSL_CRYPTO_DLL} AND NOT MITK_OPENSSL_CRYPTO_DLL AND openssl_crypto_dll) set(MITK_OPENSSL_CRYPTO_DLL \"${openssl_crypto_dll}\" CACHE FILEPATH \"\" FORCE) endif() if(MITK_OPENSSL_SSL_DLL AND EXISTS \"${MITK_OPENSSL_SSL_DLL}\" AND MITK_OPENSSL_CRYPTO_DLL AND EXISTS \"${MITK_OPENSSL_CRYPTO_DLL}\") foreach(config_type ${CMAKE_CONFIGURATION_TYPES}) execute_process(COMMAND \"${CMAKE_COMMAND}\" -E make_directory \"${MITK_BINARY_DIR}/bin/${config_type}\") configure_file(\"${MITK_OPENSSL_SSL_DLL}\" \"${MITK_BINARY_DIR}/bin/${config_type}/\" COPYONLY) configure_file(\"${MITK_OPENSSL_CRYPTO_DLL}\" \"${MITK_BINARY_DIR}/bin/${config_type}/\" COPYONLY) endforeach() MITK_INSTALL(FILES \"${MITK_OPENSSL_SSL_DLL}\" \"${MITK_OPENSSL_CRYPTO_DLL}\" ) endif() endif() # Look for optional Doxygen package find_package(Doxygen) option(BLUEBERRY_DEBUG_SMARTPOINTER \"Enable code for debugging smart pointers\" OFF) mark_as_advanced(BLUEBERRY_DEBUG_SMARTPOINTER) # Ask the user to show the console window for applications option(MITK_SHOW_CONSOLE_WINDOW \"Use this to enable or disable the console window when starting MITK GUI Applications\" ON) mark_as_advanced(MITK_SHOW_CONSOLE_WINDOW) if(NOT MITK_FAST_TESTING) if(MITK_CTEST_SCRIPT_MODE STREQUAL \"Continuous\" OR MITK_CTEST_SCRIPT_MODE STREQUAL \"Experimental\") set(MITK_FAST_TESTING ON) endif() endif() if(NOT UNIX) set(MITK_WIN32_FORCE_STATIC \"STATIC\" CACHE INTERNAL \"Use this variable to always build static libraries on non-unix platforms\") endif() if(MITK_BUILD_ALL_PLUGINS) set(MITK_BUILD_ALL_PLUGINS_OPTION \"FORCE_BUILD_ALL\") endif() # Configure pixel types used for ITK image access multiplexing mitkMacroConfigureItkPixelTypes() # Configure module naming conventions set(MITK_MODULE_NAME_REGEX_MATCH \"^[A-Z].*$\") set(MITK_MODULE_NAME_REGEX_NOT_MATCH \"^[Mm][Ii][Tt][Kk].*$\") set(MITK_DEFAULT_MODULE_NAME_PREFIX \"Mitk\") set(MITK_MODULE_NAME_PREFIX ${MITK_DEFAULT_MODULE_NAME_PREFIX}) set(MITK_MODULE_NAME_DEFAULTS_TO_DIRECTORY_NAME 1) #----------------------------------------------------------------------------- # Get MITK version info #----------------------------------------------------------------------------- mitkFunctionGetVersion(${MITK_SOURCE_DIR} MITK) mitkFunctionGetVersionDescription(${MITK_SOURCE_DIR} MITK) # MITK_VERSION set(MITK_VERSION_STRING \"${MITK_VERSION_MAJOR}.${MITK_VERSION_MINOR}.${MITK_VERSION_PATCH}\") if(MITK_VERSION_PATCH STREQUAL \"99\") set(MITK_VERSION_STRING \"${MITK_VERSION_STRING}-${MITK_REVISION_SHORTID}\") endif() #----------------------------------------------------------------------------- # Installation preparation # # These should be set before any MITK install macros are used #----------------------------------------------------------------------------- # on macOS all BlueBerry plugins get copied into every # application bundle (.app directory) specified here if(MITK_USE_BLUEBERRY AND APPLE) foreach(MITK_EXTENSION_DIR ${MITK_DIR_PLUS_EXTENSION_DIRS}) set(MITK_APPLICATIONS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Applications\") if(EXISTS \"${MITK_APPLICATIONS_EXTENSION_DIR}/AppList.cmake\") set(MITK_APPS \"\") include(\"${MITK_APPLICATIONS_EXTENSION_DIR}/AppList.cmake\") foreach(mitk_app ${MITK_APPS}) # extract option_name string(REPLACE \"^^\" \"\\\\;\" target_info ${mitk_app}) set(target_info_list ${target_info}) list(GET target_info_list 1 option_name) list(GET target_info_list 0 app_name) # check if the application is enabled if(${option_name} OR MITK_BUILD_ALL_APPS) set(MACOSX_BUNDLE_NAMES ${MACOSX_BUNDLE_NAMES} Mitk${app_name}) endif() endforeach() endif() endforeach() endif() #----------------------------------------------------------------------------- # Set coverage Flags #----------------------------------------------------------------------------- if(WITH_COVERAGE) if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\") set(coverage_flags \"-g -fprofile-arcs -ftest-coverage -O0 -DNDEBUG\") set(COVERAGE_CXX_FLAGS ${coverage_flags}) set(COVERAGE_C_FLAGS ${coverage_flags}) endif() endif() #----------------------------------------------------------------------------- # MITK C/CXX Flags #----------------------------------------------------------------------------- set(MITK_C_FLAGS \"${COVERAGE_C_FLAGS}\") set(MITK_C_FLAGS_DEBUG ) set(MITK_C_FLAGS_RELEASE ) set(MITK_CXX_FLAGS \"${COVERAGE_CXX_FLAGS} ${MITK_CXX${MITK_CXX_STANDARD}_FLAG}\") set(MITK_CXX_FLAGS_DEBUG ) set(MITK_CXX_FLAGS_RELEASE ) set(MITK_EXE_LINKER_FLAGS ) set(MITK_SHARED_LINKER_FLAGS ) if(WIN32) set(MITK_CXX_FLAGS \"${MITK_CXX_FLAGS} -DWIN32_LEAN_AND_MEAN -DNOMINMAX\") mitkFunctionCheckCompilerFlags(\"/wd4005\" MITK_CXX_FLAGS) # warning C4005: macro redefinition mitkFunctionCheckCompilerFlags(\"/wd4231\" MITK_CXX_FLAGS) # warning C4231: nonstandard extension used : 'extern' before template explicit instantiation # the following line should be removed after fixing bug 17637 mitkFunctionCheckCompilerFlags(\"/wd4316\" MITK_CXX_FLAGS) # warning C4316: object alignment on heap mitkFunctionCheckCompilerFlags(\"/wd4180\" MITK_CXX_FLAGS) # warning C4180: qualifier applied to function type has no meaning mitkFunctionCheckCompilerFlags(\"/wd4251\" MITK_CXX_FLAGS) # warning C4251: 'identifier' : class 'type' needs to have dll-interface to be used by clients of class 'type2' endif() if(APPLE) set(MITK_CXX_FLAGS \"${MITK_CXX_FLAGS} -DGL_SILENCE_DEPRECATION\") # Apple deprecated OpenGL in macOS 10.14 endif() if(NOT MSVC_VERSION) foreach(_flag -Wall -Wextra -Wpointer-arith -Winvalid-pch -Wcast-align -Wwrite-strings -Wno-error=gnu -Wno-error=unknown-pragmas # The strict-overflow warning is generated by ITK template code -Wno-error=strict-overflow -Woverloaded-virtual -Wstrict-null-sentinel #-Wold-style-cast #-Wsign-promo -Wno-deprecated-copy -Wno-array-bounds -Wno-cast-function-type -Wno-maybe-uninitialized -Wno-error=stringop-overread -fdiagnostics-show-option ) mitkFunctionCheckCAndCXXCompilerFlags(${_flag} MITK_C_FLAGS MITK_CXX_FLAGS) endforeach() endif() if(CMAKE_COMPILER_IS_GNUCXX AND NOT APPLE) mitkFunctionCheckCompilerFlags(\"-Wl,--no-undefined\" MITK_SHARED_LINKER_FLAGS) mitkFunctionCheckCompilerFlags(\"-Wl,--as-needed\" MITK_SHARED_LINKER_FLAGS) endif() if(CMAKE_COMPILER_IS_GNUCXX) mitkFunctionCheckCAndCXXCompilerFlags(\"-fstack-protector-all\" MITK_C_FLAGS MITK_CXX_FLAGS) set(MITK_CXX_FLAGS_RELEASE \"-U_FORTIFY_SOURCES -D_FORTIFY_SOURCE=2 ${MITK_CXX_FLAGS_RELEASE}\") endif() set(MITK_MODULE_LINKER_FLAGS ${MITK_SHARED_LINKER_FLAGS}) set(MITK_EXE_LINKER_FLAGS ${MITK_SHARED_LINKER_FLAGS}) #----------------------------------------------------------------------------- # MITK Packages #----------------------------------------------------------------------------- set(MITK_MODULES_PACKAGE_DEPENDS_DIR ${MITK_SOURCE_DIR}/CMake/PackageDepends) set(MODULES_PACKAGE_DEPENDS_DIRS ${MITK_MODULES_PACKAGE_DEPENDS_DIR}) foreach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS}) set(MITK_PACKAGE_DEPENDS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/CMake/PackageDepends\") if(EXISTS \"${MITK_PACKAGE_DEPENDS_EXTENSION_DIR}\") list(APPEND MODULES_PACKAGE_DEPENDS_DIRS \"${MITK_PACKAGE_DEPENDS_EXTENSION_DIR}\") endif() endforeach() if(NOT MITK_USE_SYSTEM_Boost) set(Boost_NO_SYSTEM_PATHS 1) endif() set(Boost_USE_MULTITHREADED 1) set(Boost_USE_STATIC_LIBS 0) set(Boost_USE_STATIC_RUNTIME 0) set(Boost_ADDITIONAL_VERSIONS 1.74 1.74.0) # We need this later for a DCMTK workaround set(_dcmtk_dir_orig ${DCMTK_DIR}) # This property is populated at the top half of this file get_property(MITK_EXTERNAL_PROJECTS GLOBAL PROPERTY MITK_EXTERNAL_PROJECTS) foreach(ep ${MITK_EXTERNAL_PROJECTS}) get_property(_package GLOBAL PROPERTY MITK_${ep}_PACKAGE) get_property(_components GLOBAL PROPERTY MITK_${ep}_COMPONENTS) if(MITK_USE_${ep} AND _package) if(_components) find_package(${_package} COMPONENTS ${_components} REQUIRED CONFIG) else() # Prefer config mode first because it finds external # <proj>Config.cmake files pointed at by <proj>_DIR variables. # Otherwise, existing Find<proj>.cmake files could fail. if(DEFINED ${_package}_DIR) #we store the information because it will be overwritten by find_package #and would get lost for all EPs that use on Find<proj>.cmake instead of config #files. set(_temp_EP_${_package}_dir ${${_package}_DIR}) endif(DEFINED ${_package}_DIR) find_package(${_package} QUIET CONFIG) string(TOUPPER \"${_package}\" _package_uc) if(NOT (${_package}_FOUND OR ${_package_uc}_FOUND)) if(DEFINED _temp_EP_${_package}_dir) set(${_package}_DIR ${_temp_EP_${_package}_dir} CACHE PATH \"externally set dir of the package ${_package}\" FORCE) endif(DEFINED _temp_EP_${_package}_dir) find_package(${_package} REQUIRED) endif() endif() endif() endforeach() # Ensure that the MITK CMake module path comes first set(CMAKE_MODULE_PATH ${MITK_CMAKE_DIR} ${CMAKE_MODULE_PATH} ) if(MITK_USE_DCMTK) if(${_dcmtk_dir_orig} MATCHES \"${MITK_EXTERNAL_PROJECT_PREFIX}.*\") # Help our FindDCMTK.cmake script find our super-build DCMTK set(DCMTK_DIR ${MITK_EXTERNAL_PROJECT_PREFIX}) else() # Use the original value set(DCMTK_DIR ${_dcmtk_dir_orig}) endif() endif() if(MITK_USE_DCMQI) # Due to the preferred CONFIG mode in find_package calls above, # the DCMQIConfig.cmake file is read, which does not provide useful # package information. We explicitly need MODULE mode to find DCMQI. # Help our FindDCMQI.cmake script find our super-build DCMQI set(DCMQI_DIR ${MITK_EXTERNAL_PROJECT_PREFIX}) find_package(DCMQI REQUIRED) endif() if(MITK_USE_OpenMP) find_package(OpenMP REQUIRED COMPONENTS CXX) else() find_package(OpenMP QUIET COMPONENTS CXX) if(OpenMP_FOUND) set(MITK_USE_OpenMP ON CACHE BOOL \"\" FORCE) elseif(APPLE AND OpenMP_libomp_LIBRARY AND NOT OpenMP_CXX_LIB_NAMES) set(OpenMP_CXX_LIB_NAMES libomp CACHE STRING \"\" FORCE) get_filename_component(openmp_lib_dir \"${OpenMP_libomp_LIBRARY}\" DIRECTORY) set(openmp_include_dir \"${openmp_lib_dir}/../include\") if(EXISTS \"${openmp_include_dir}\") get_filename_component(openmp_include_dir \"${openmp_include_dir}\" REALPATH) set(OpenMP_CXX_FLAGS \"-Xpreprocessor -fopenmp -I${openmp_include_dir}\" CACHE STRING \"\" FORCE) find_package(OpenMP QUIET COMPONENTS CXX) if(OpenMP_FOUND) set(MITK_USE_OpenMP ON CACHE BOOL \"\" FORCE) endif() endif() endif() endif() # Qt support if(MITK_USE_Qt6) if(MITK_USE_BLUEBERRY) option(BLUEBERRY_USE_QT_HELP \"Enable support for integrating plugin documentation into Qt Help\" ${DOXYGEN_FOUND}) mark_as_advanced(BLUEBERRY_USE_QT_HELP) # Sanity checks for in-application BlueBerry plug-in help generation if(BLUEBERRY_USE_QT_HELP) set(_force_blueberry_use_qt_help_to_off 0) if(NOT DOXYGEN_FOUND) message(\"> Forcing BLUEBERRY_USE_QT_HELP to OFF because Doxygen was not found.\") set(_force_blueberry_use_qt_help_to_off 1) endif() if(DOXYGEN_FOUND AND DOXYGEN_VERSION VERSION_LESS 1.8.7) message(\"> Forcing BLUEBERRY_USE_QT_HELP to OFF because Doxygen version 1.8.7 or newer not found.\") set(_force_blueberry_use_qt_help_to_off 1) endif() if(NOT QT_HELPGENERATOR_EXECUTABLE) message(\"> Forcing BLUEBERRY_USE_QT_HELP to OFF because QT_HELPGENERATOR_EXECUTABLE is empty.\") set(_force_blueberry_use_qt_help_to_off 1) endif() if(NOT MITK_USE_Qt6) message(\"> Forcing BLUEBERRY_USE_QT_HELP to OFF because MITK_USE_Qt6 is OFF.\") set(_force_blueberry_use_qt_help_to_off 1) endif() if(_force_blueberry_use_qt_help_to_off) set(BLUEBERRY_USE_QT_HELP OFF CACHE BOOL \"Enable support for integrating plugin documentation into Qt Help\" FORCE) endif() endif() if(BLUEBERRY_QT_HELP_REQUIRED AND NOT BLUEBERRY_USE_QT_HELP) message(FATAL_ERROR \"BLUEBERRY_USE_QT_HELP is required to be set to ON\") endif() endif() endif() #----------------------------------------------------------------------------- # Testing #----------------------------------------------------------------------------- if(BUILD_TESTING) # Configuration for the CMake-generated test driver set(CMAKE_TESTDRIVER_EXTRA_INCLUDES \"#include <stdexcept>\") set(CMAKE_TESTDRIVER_BEFORE_TESTMAIN \" try {\") set(CMAKE_TESTDRIVER_AFTER_TESTMAIN \" } catch (const std::exception& e) { fprintf(stderr, \\\"%s\\\\n\\\", e.what()); return EXIT_FAILURE; } catch (...) { printf(\\\"Exception caught in the test driver\\\\n\\\"); return EXIT_FAILURE; }\") set(MITK_TEST_OUTPUT_DIR \"${MITK_BINARY_DIR}/test_output\") if(NOT EXISTS ${MITK_TEST_OUTPUT_DIR}) file(MAKE_DIRECTORY ${MITK_TEST_OUTPUT_DIR}) endif() # Test the package target include(mitkPackageTest) endif() configure_file(mitkTestingConfig.h.in ${MITK_BINARY_DIR}/mitkTestingConfig.h) #----------------------------------------------------------------------------- # MITK_SUPERBUILD_BINARY_DIR #----------------------------------------------------------------------------- # If MITK_SUPERBUILD_BINARY_DIR isn't defined, it means MITK is *NOT* build using Superbuild. # In that specific case, MITK_SUPERBUILD_BINARY_DIR should default to MITK_BINARY_DIR if(NOT DEFINED MITK_SUPERBUILD_BINARY_DIR) set(MITK_SUPERBUILD_BINARY_DIR ${MITK_BINARY_DIR}) endif() #----------------------------------------------------------------------------- # Set C/CXX and linker flags for MITK code #----------------------------------------------------------------------------- set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${MITK_CXX_FLAGS}\") set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} ${MITK_CXX_FLAGS_DEBUG}\") set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} ${MITK_CXX_FLAGS_RELEASE}\") set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${MITK_C_FLAGS}\") set(CMAKE_C_FLAGS_DEBUG \"${CMAKE_C_FLAGS_DEBUG} ${MITK_C_FLAGS_DEBUG}\") set(CMAKE_C_FLAGS_RELEASE \"${CMAKE_C_FLAGS_RELEASE} ${MITK_C_FLAGS_RELEASE}\") set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${MITK_EXE_LINKER_FLAGS}\") set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} ${MITK_SHARED_LINKER_FLAGS}\") set(CMAKE_MODULE_LINKER_FLAGS \"${CMAKE_MODULE_LINKER_FLAGS} ${MITK_MODULE_LINKER_FLAGS}\") #----------------------------------------------------------------------------- # Add subdirectories #----------------------------------------------------------------------------- add_subdirectory(Utilities) add_subdirectory(Modules) include(\"${CMAKE_CURRENT_SOURCE_DIR}/Modules/ModuleList.cmake\") mitkFunctionWhitelistModules(MITK MITK_MODULES) set(MITK_ROOT_FOLDER_BACKUP \"${MITK_ROOT_FOLDER}\") foreach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS}) get_filename_component(MITK_ROOT_FOLDER \"${MITK_EXTENSION_DIR}\" NAME) set(MITK_MODULES_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Modules\") if(EXISTS \"${MITK_MODULES_EXTENSION_DIR}/ModuleList.cmake\") set(MITK_MODULES \"\") include(\"${MITK_MODULES_EXTENSION_DIR}/ModuleList.cmake\") foreach(mitk_module ${MITK_MODULES}) add_subdirectory(\"${MITK_MODULES_EXTENSION_DIR}/${mitk_module}\" \"Modules/${mitk_module}\") endforeach() endif() set(MITK_MODULE_NAME_PREFIX ${MITK_DEFAULT_MODULE_NAME_PREFIX}) endforeach() set(MITK_ROOT_FOLDER \"${MITK_ROOT_FOLDER_BACKUP}\") add_subdirectory(Wrapping) set(MITK_DOXYGEN_OUTPUT_DIR \"${PROJECT_BINARY_DIR}/Documentation/Doxygen\" CACHE PATH \"Output directory for doxygen generated documentation.\") if(MITK_USE_BLUEBERRY) include(\"${CMAKE_CURRENT_SOURCE_DIR}/Plugins/PluginList.cmake\") mitkFunctionWhitelistPlugins(MITK MITK_PLUGINS) set(mitk_plugins_fullpath \"\") foreach(mitk_plugin ${MITK_PLUGINS}) list(APPEND mitk_plugins_fullpath Plugins/${mitk_plugin}) endforeach() set(MITK_PLUGIN_REGEX_LIST \"\") foreach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS}) set(MITK_PLUGINS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Plugins\") if(EXISTS \"${MITK_PLUGINS_EXTENSION_DIR}/PluginList.cmake\") set(MITK_PLUGINS \"\") include(\"${MITK_PLUGINS_EXTENSION_DIR}/PluginList.cmake\") foreach(mitk_plugin ${MITK_PLUGINS}) list(APPEND mitk_plugins_fullpath \"${MITK_PLUGINS_EXTENSION_DIR}/${mitk_plugin}\") endforeach() endif() endforeach() if(EXISTS ${MITK_PRIVATE_MODULES}/PluginList.cmake) include(${MITK_PRIVATE_MODULES}/PluginList.cmake) foreach(mitk_plugin ${MITK_PRIVATE_PLUGINS}) list(APPEND mitk_plugins_fullpath ${MITK_PRIVATE_MODULES}/${mitk_plugin}) endforeach() endif() if(MITK_BUILD_EXAMPLES) include(\"${CMAKE_CURRENT_SOURCE_DIR}/Examples/Plugins/PluginList.cmake\") set(mitk_example_plugins_fullpath ) foreach(mitk_example_plugin ${MITK_EXAMPLE_PLUGINS}) list(APPEND mitk_example_plugins_fullpath Examples/Plugins/${mitk_example_plugin}) list(APPEND mitk_plugins_fullpath Examples/Plugins/${mitk_example_plugin}) endforeach() endif() # Specify which plug-ins belong to this project macro(GetMyTargetLibraries all_target_libraries varname) set(re_ctkplugin_mitk \"^org_mitk_[a-zA-Z0-9_]+$\") set(re_ctkplugin_bb \"^org_blueberry_[a-zA-Z0-9_]+$\") set(_tmp_list) list(APPEND _tmp_list ${all_target_libraries}) ctkMacroListFilter(_tmp_list re_ctkplugin_mitk re_ctkplugin_bb MITK_PLUGIN_REGEX_LIST OUTPUT_VARIABLE ${varname}) endmacro() # Get infos about application directories and build options set(mitk_apps_fullpath \"\") foreach(MITK_EXTENSION_DIR ${MITK_DIR_PLUS_EXTENSION_DIRS}) set(MITK_APPLICATIONS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Applications\") if(EXISTS \"${MITK_APPLICATIONS_EXTENSION_DIR}/AppList.cmake\") set(MITK_APPS \"\") include(\"${MITK_APPLICATIONS_EXTENSION_DIR}/AppList.cmake\") foreach(mitk_app ${MITK_APPS}) # extract option_name string(REPLACE \"^^\" \"\\\\;\" target_info ${mitk_app}) set(target_info_list ${target_info}) list(GET target_info_list 0 directory_name) list(GET target_info_list 1 option_name) if(${option_name}) list(APPEND mitk_apps_fullpath \"${MITK_APPLICATIONS_EXTENSION_DIR}/${directory_name}^^${option_name}\") endif() endforeach() endif() endforeach() if (mitk_plugins_fullpath) ctkMacroSetupPlugins(${mitk_plugins_fullpath} BUILD_OPTION_PREFIX MITK_BUILD_ APPS ${mitk_apps_fullpath} BUILD_ALL ${MITK_BUILD_ALL_PLUGINS} COMPACT_OPTIONS) endif() set(MITK_PLUGIN_USE_FILE \"${MITK_BINARY_DIR}/MitkPluginUseFile.cmake\") if(${PROJECT_NAME}_PLUGIN_LIBRARIES) ctkFunctionGeneratePluginUseFile(${MITK_PLUGIN_USE_FILE}) else() file(REMOVE ${MITK_PLUGIN_USE_FILE}) set(MITK_PLUGIN_USE_FILE ) endif() endif() #----------------------------------------------------------------------------- # Documentation #----------------------------------------------------------------------------- set(MITK_DOXYGEN_ADDITIONAL_INPUT_DIRS) set(MITK_DOXYGEN_ADDITIONAL_IMAGE_PATHS) set(MITK_DOXYGEN_ADDITIONAL_EXAMPLE_PATHS) foreach(MITK_EXTENSION_DIR ${MITK_DIR_PLUS_EXTENSION_DIRS}) set(MITK_DOXYGEN_ADDITIONAL_INPUT_DIRS \"${MITK_DOXYGEN_ADDITIONAL_INPUT_DIRS} \\\"${MITK_EXTENSION_DIR}\\\"\") set(MITK_DOXYGEN_ADDITIONAL_IMAGE_PATHS \"${MITK_DOXYGEN_ADDITIONAL_IMAGE_PATHS} \\\"${MITK_EXTENSION_DIR}\\\"\") # MITK_DOXYGEN_ADDITIONAL_EXAMPLE_PATHS should be modified by MITK extensions as needed endforeach() if(DOXYGEN_FOUND) foreach(MITK_EXTENSION_DIR ${MITK_DIR_PLUS_EXTENSION_DIRS}) set(MITK_DOCUMENTATION_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Documentation\") file(GLOB MITK_DOCUMENTATION_EXTENSION_FILES CONFIGURE_DEPENDS \"${MITK_DOCUMENTATION_EXTENSION_DIR}/*.cmake\") foreach(doc_file ${MITK_DOCUMENTATION_EXTENSION_FILES}) include(\"${doc_file}\") endforeach() endforeach() # Transform list of example paths into space-separated string of quoted paths unset(example_paths) foreach(example_path ${MITK_DOXYGEN_ADDITIONAL_EXAMPLE_PATHS}) set(example_paths \"${example_paths} \\\"${example_path}\\\"\") endforeach() set(MITK_DOXYGEN_ADDITIONAL_EXAMPLE_PATHS \"${example_paths}\") add_subdirectory(Documentation) endif() #----------------------------------------------------------------------------- # Installation #----------------------------------------------------------------------------- # set MITK cpack variables # These are the default variables, which can be overwritten ( see below ) include(mitkSetupCPack) set(use_default_config ON) set(ALL_MITK_APPS \"\") set(activated_apps_no 0) foreach(MITK_EXTENSION_DIR ${MITK_DIR_PLUS_EXTENSION_DIRS}) set(MITK_APPLICATIONS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Applications\") if(EXISTS \"${MITK_APPLICATIONS_EXTENSION_DIR}/AppList.cmake\") set(MITK_APPS \"\") include(\"${MITK_APPLICATIONS_EXTENSION_DIR}/AppList.cmake\") foreach(mitk_app ${MITK_APPS}) string(REPLACE \"^^\" \"\\\\;\" target_info ${mitk_app}) set(target_info_list ${target_info}) list(GET target_info_list 0 directory_name) list(GET target_info_list 1 option_name) list(GET target_info_list 2 executable_name) list(APPEND ALL_MITK_APPS \"${MITK_EXTENSION_DIR}/Applications/${directory_name}^^${option_name}^^${executable_name}\") if(${option_name} OR MITK_BUILD_ALL_APPS) MATH(EXPR activated_apps_no \"${activated_apps_no} + 1\") endif() endforeach() endif() endforeach() list(LENGTH ALL_MITK_APPS app_count) if(app_count EQUAL 1 AND (activated_apps_no EQUAL 1 OR MITK_BUILD_ALL_APPS)) # Corner case if there is only one app in total set(use_project_cpack ON) elseif(activated_apps_no EQUAL 1 AND NOT MITK_BUILD_ALL_APPS) # Only one app is enabled (no \"build all\" flag set) set(use_project_cpack ON) else() # Less or more then one app is enabled set(use_project_cpack OFF) endif() foreach(mitk_app ${ALL_MITK_APPS}) # extract target_dir and option_name string(REPLACE \"^^\" \"\\\\;\" target_info ${mitk_app}) set(target_info_list ${target_info}) list(GET target_info_list 0 target_dir) list(GET target_info_list 1 option_name) list(GET target_info_list 2 executable_name) # check if the application is enabled if(${option_name} OR MITK_BUILD_ALL_APPS) # check whether application specific configuration files will be used if(use_project_cpack) # use files if they exist if(EXISTS \"${target_dir}/CPackOptions.cmake\") include(\"${target_dir}/CPackOptions.cmake\") endif() if(EXISTS \"${target_dir}/CPackConfig.cmake.in\") set(CPACK_PROJECT_CONFIG_FILE \"${target_dir}/CPackConfig.cmake\") configure_file(${target_dir}/CPackConfig.cmake.in ${CPACK_PROJECT_CONFIG_FILE} @ONLY) set(use_default_config OFF) endif() endif() # add link to the list list(APPEND CPACK_CREATE_DESKTOP_LINKS \"${executable_name}\") endif() endforeach() # if no application specific configuration file was used, use default if(use_default_config) configure_file(${MITK_SOURCE_DIR}/MITKCPackOptions.cmake.in ${MITK_BINARY_DIR}/MITKCPackOptions.cmake @ONLY) set(CPACK_PROJECT_CONFIG_FILE \"${MITK_BINARY_DIR}/MITKCPackOptions.cmake\") endif() # include CPack model once all variables are set include(CPack) # Additional installation rules include(mitkInstallRules) #----------------------------------------------------------------------------- # Last configuration steps #----------------------------------------------------------------------------- # ---------------- Export targets ----------------- set(MITK_EXPORTS_FILE \"${MITK_BINARY_DIR}/MitkExports.cmake\") file(REMOVE ${MITK_EXPORTS_FILE}) set(targets_to_export) get_property(module_targets GLOBAL PROPERTY MITK_MODULE_TARGETS) if(module_targets) list(APPEND targets_to_export ${module_targets}) endif() if(MITK_USE_BLUEBERRY) if(MITK_PLUGIN_LIBRARIES) list(APPEND targets_to_export ${MITK_PLUGIN_LIBRARIES}) endif() endif() export(TARGETS ${targets_to_export} APPEND FILE ${MITK_EXPORTS_FILE}) set(MITK_EXPORTED_TARGET_PROPERTIES ) foreach(target_to_export ${targets_to_export}) get_target_property(autoload_targets ${target_to_export} MITK_AUTOLOAD_TARGETS) if(autoload_targets) set(MITK_EXPORTED_TARGET_PROPERTIES \"${MITK_EXPORTED_TARGET_PROPERTIES} set_target_properties(${target_to_export} PROPERTIES MITK_AUTOLOAD_TARGETS \\\"${autoload_targets}\\\")\") endif() get_target_property(autoload_dir ${target_to_export} MITK_AUTOLOAD_DIRECTORY) if(autoload_dir) set(MITK_EXPORTED_TARGET_PROPERTIES \"${MITK_EXPORTED_TARGET_PROPERTIES} set_target_properties(${target_to_export} PROPERTIES MITK_AUTOLOAD_DIRECTORY \\\"${autoload_dir}\\\")\") endif() get_target_property(deprecated_module ${target_to_export} MITK_MODULE_DEPRECATED_SINCE) if(deprecated_module) set(MITK_EXPORTED_TARGET_PROPERTIES \"${MITK_EXPORTED_TARGET_PROPERTIES} set_target_properties(${target_to_export} PROPERTIES MITK_MODULE_DEPRECATED_SINCE \\\"${deprecated_module}\\\")\") endif() endforeach() # ---------------- External projects ----------------- get_property(MITK_ADDITIONAL_LIBRARY_SEARCH_PATHS_CONFIG GLOBAL PROPERTY MITK_ADDITIONAL_LIBRARY_SEARCH_PATHS) set(MITK_CONFIG_EXTERNAL_PROJECTS ) #string(REPLACE \"^^\" \";\" _mitk_external_projects ${MITK_EXTERNAL_PROJECTS}) foreach(ep ${MITK_EXTERNAL_PROJECTS}) get_property(_components GLOBAL PROPERTY MITK_${ep}_COMPONENTS) set(MITK_CONFIG_EXTERNAL_PROJECTS \"${MITK_CONFIG_EXTERNAL_PROJECTS} set(MITK_USE_${ep} ${MITK_USE_${ep}}) set(MITK_${ep}_DIR \\\"${${ep}_DIR}\\\") set(MITK_${ep}_COMPONENTS ${_components}) \") endforeach() foreach(ep ${MITK_EXTERNAL_PROJECTS}) get_property(_package GLOBAL PROPERTY MITK_${ep}_PACKAGE) get_property(_components GLOBAL PROPERTY MITK_${ep}_COMPONENTS) if(_components) set(_components_arg COMPONENTS \\${_components}) else() set(_components_arg) endif() if(_package) set(MITK_CONFIG_EXTERNAL_PROJECTS \"${MITK_CONFIG_EXTERNAL_PROJECTS} if(MITK_USE_${ep}) set(${ep}_DIR \\${MITK_${ep}_DIR}) if(MITK_${ep}_COMPONENTS) mitkMacroFindDependency(${_package} COMPONENTS \\${MITK_${ep}_COMPONENTS}) else() mitkMacroFindDependency(${_package}) endif() endif()\") endif() endforeach() # ---------------- Tools ----------------- configure_file(${MITK_SOURCE_DIR}/CMake/ToolExtensionITKFactory.cpp.in ${MITK_BINARY_DIR}/ToolExtensionITKFactory.cpp.in COPYONLY) configure_file(${MITK_SOURCE_DIR}/CMake/ToolExtensionITKFactoryLoader.cpp.in ${MITK_BINARY_DIR}/ToolExtensionITKFactoryLoader.cpp.in COPYONLY) configure_file(${MITK_SOURCE_DIR}/CMake/ToolGUIExtensionITKFactory.cpp.in ${MITK_BINARY_DIR}/ToolGUIExtensionITKFactory.cpp.in COPYONLY) # ---------------- Configure files ----------------- configure_file(mitkVersion.h.in ${MITK_BINARY_DIR}/mitkVersion.h) configure_file(mitkConfig.h.in ${MITK_BINARY_DIR}/mitkConfig.h) set(IPFUNC_INCLUDE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/Utilities/ipFunc) set(UTILITIES_DIR ${CMAKE_CURRENT_SOURCE_DIR}/Utilities) configure_file(mitkConfig.h.in ${MITK_BINARY_DIR}/mitkConfig.h) configure_file(MITKConfig.cmake.in ${MITK_BINARY_DIR}/MITKConfig.cmake @ONLY) write_basic_config_version_file(${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}ConfigVersion.cmake VERSION ${MITK_VERSION_STRING} COMPATIBILITY AnyNewerVersion) #----------------------------------------------------------------------------- # MITK Applications #----------------------------------------------------------------------------- # This must come after MITKConfig.h was generated, since applications # might do a find_package(MITK REQUIRED). add_subdirectory(Applications) if(MSVC AND TARGET MitkWorkbench) set_directory_properties(PROPERTIES VS_STARTUP_PROJECT MitkWorkbench) endif() foreach(MITK_EXTENSION_DIR ${MITK_ABSOLUTE_EXTENSION_DIRS}) set(MITK_APPLICATIONS_EXTENSION_DIR \"${MITK_EXTENSION_DIR}/Applications\") if(EXISTS \"${MITK_APPLICATIONS_EXTENSION_DIR}/CMakeLists.txt\") add_subdirectory(\"${MITK_APPLICATIONS_EXTENSION_DIR}\" \"Applications\") endif() endforeach() #----------------------------------------------------------------------------- # MITK Examples #----------------------------------------------------------------------------- if(MITK_BUILD_EXAMPLES) # This must come after MITKConfig.h was generated, since applications # might do a find_package(MITK REQUIRED). add_subdirectory(Examples) endif() #----------------------------------------------------------------------------- # Print configuration summary #----------------------------------------------------------------------------- message(\"\\n\\n\") feature_summary( DESCRIPTION \"------- FEATURE SUMMARY FOR ${PROJECT_NAME} -------\" WHAT ALL )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/membrain-v2",
            "repo_link": "https://github.com/teamtomo/membrain-seg",
            "content": {
                "codemeta": "",
                "readme": "# MemBrain-Seg [![License](https://img.shields.io/pypi/l/membrain-seg.svg?color=green)](https://github.com/teamtomo/membrain-seg/raw/main/LICENSE) [![PyPI](https://img.shields.io/pypi/v/membrain-seg.svg?color=green)](https://pypi.org/project/membrain-seg) [![Python Version](https://img.shields.io/pypi/pyversions/membrain-seg.svg?color=green)](https://python.org) [![CI](https://github.com/teamtomo/membrain-seg/actions/workflows/ci.yml/badge.svg)](https://github.com/teamtomo/membrain-seg/actions/workflows/ci.yml) [![codecov](https://codecov.io/gh/teamtomo/membrain-seg/branch/main/graph/badge.svg)](https://codecov.io/gh/teamtomo/membrain-seg) Membrain-Seg<sup>1</sup> is a Python project developed by [teamtomo](https://github.com/teamtomo) for membrane segmentation in 3D for cryo-electron tomography (cryo-ET). This tool aims to provide researchers with an efficient and reliable method for segmenting membranes in 3D microscopic images. Membrain-Seg is currently under early development, so we may make breaking changes between releases. ## Example notebook For a quick start, you can walk through our example notebook. You can easily run it on Google Colab by clicking on the badge below: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CellArchLab/membrain_tutorial_scripts/blob/main/MemBrain_seg_tutorial.ipynb) ## Publication: Membrain-seg's current functionalities are described on more detail in our [preprint](https://www.biorxiv.org/content/10.1101/2024.01.05.574336v1). <p align=\"center\" width=\"100%\"> <img width=\"100%\" src=\"https://user-images.githubusercontent.com/34575029/248259282-ee622267-77fa-4c88-ad38-ad0cfd76b810.png\"> </p> Membrain-Seg is currently under early development, so we may make breaking changes between releases. # Version Updates For a detailed history of changes and updates, please refer to our [CHANGELOG.md](./CHANGELOG.md). # Overview MemBrain-seg is a practical tool for membrane segmentation in cryo-electron tomograms. It's built on the U-Net architecture and makes use of a pre-trained model for efficient performance. The U-Net architecture and training parameters are largely inspired by nnUNet<sup>2</sup>. Our current best model is available for download [here](https://drive.google.com/file/d/1hruug1GbO4V8C4bkE5DZJeybDyOxZ7PX/view?usp=sharing). Please let us know how it works for you. If the given model does not work properly, you may want to try one of our experimental or previous versions: Experimental models: - [v10_beta_FAaug -- model traing with Fourier amplitude augmentation for better generalization](https://drive.google.com/file/d/1kaN9ihB62OfHLFnyI2_t6Ya3kJm7Wun9/view?usp=sharing) - [v10_beta_MWaug -- model traing with missing wedge augmentation for better missing wedge restoration](https://drive.google.com/file/d/1-i836rU-wfuClsqPRbKqJ-eW2jCUlwJm/view?usp=sharing) Other (older) model versions: - [v10_alpha -- standard model until 24th April 2025](https://drive.google.com/file/d/1tSQIz_UCsQZNfyHg0RxD-4meFgolszo8/view?usp=sharing) - [v9 -- best model until 10th Aug 2023](https://drive.google.com/file/d/15ZL5Ao7EnPwMHa8yq5CIkanuNyENrDeK/view?usp=sharing) - [v9b -- model for non-denoised data until 10th Aug 2023](https://drive.google.com/file/d/1TGpQ1WyLHgXQIdZ8w4KFZo_Kkoj0vIt7/view?usp=sharing) If you wish, you can also train a new model using your own data, or combine it with our (soon to come!) publicly-available dataset. To enhance segmentation, MemBrain-seg includes preprocessing functions. These help to adjust your tomograms so they're similar to the data our network was trained on, making the process smoother and more efficient. Explore MemBrain-seg, use it for your needs, and let us know how it works for you! Preliminary [documentation](https://teamtomo.github.io/membrain-seg/) is available, but far from perfect. Please let us know if you encounter any issues, and we are more than happy to help (and get feedback what does not work yet). ``` [1] Lamm, L., Zufferey, S., Righetto, R.D., Wietrzynski, W., Yamauchi, K.A., Burt, A., Liu, Y., Zhang, H., Martinez-Sanchez, A., Ziegler, S., Isensee, F., Schnabel, J.A., Engel, B.D., and Peng, T, 2024. MemBrain v2: an end-to-end tool for the analysis of membranes in cryo-electron tomography. bioRxiv, https://doi.org/10.1101/2024.01.05.574336 [2] Isensee, F., Jaeger, P.F., Kohl, S.A.A., Petersen, J., Maier-Hein, K.H., 2021. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods 18, 203-211. https://doi.org/10.1038/s41592-020-01008-z ``` # Installation For detailed installation instructions, please look [here](https://teamtomo.github.io/membrain-seg/installation/). # Features ## Segmentation Segmenting the membranes in your tomograms is the main feature of this repository. Please find more detailed instructions [here](https://teamtomo.github.io/membrain-seg/Usage/Segmentation/). ## Preprocessing Currently, we provide the following two [preprocessing](https://github.com/teamtomo/membrain-seg/tree/main/src/membrain_seg/tomo_preprocessing) options: - Pixel size matching: Rescale your tomogram to match the training pixel sizes - Fourier amplitude matching: Scale Fourier components to match the \"style\" of different tomograms - Deconvolution: denoises the tomogram by applying the deconvolution filter from Warp For more information, see the [Preprocessing](https://teamtomo.github.io/membrain-seg/Usage/Preprocessing/) subsection. ## Model training It is also possible to use this package to train your own model. Instructions can be found [here](https://teamtomo.github.io/membrain-seg/Usage/Training/). ## Patch annotations In case you would like to train a model that works better for your tomograms, it may be beneficial to add some more patches from your tomograms to the training dataset. Recommendations on how to to this can be found [here](https://teamtomo.github.io/membrain-seg/Usage/Annotations/).\n",
                "dependencies": "# https://peps.python.org/pep-0517/ [build-system] requires = [\"hatchling\", \"hatch-vcs\"] build-backend = \"hatchling.build\" # https://peps.python.org/pep-0621/ [project] name = \"membrain-seg\" description = \"membrane segmentation in 3D for cryo-ET\" readme = \"README.md\" requires-python = \">=3.8\" license = { text = \"BSD 3-Clause License\" } authors = [ { email = \"lorenz.lamm@helmholtz-munich.de\", name = \"Lorenz Lamm\" }, ] classifiers = [ \"Development Status :: 3 - Alpha\", \"License :: OSI Approved :: BSD License\", \"Natural Language :: English\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.8\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Typing :: Typed\", ] dynamic = [\"version\"] dependencies = [ \"imageio\", \"mrcfile\", \"monai\", \"numpy<2.0.0\", \"pandas\", \"pytorch-lightning\", \"scikit-image\", \"scipy\", \"simpleitk\", \"torch\", \"typer[all]\", ] # extras # https://peps.python.org/pep-0621/#dependencies-optional-dependencies [project.optional-dependencies] test = [\"pytest>=6.0\", \"pytest-cov\"] dev = [ \"black\", \"ipython\", \"pdbpp\", \"pre-commit\", \"pytest-cov\", \"pytest\", \"rich\", \"ruff\", \"mkdocs-material\", ] [project.urls] homepage = \"https://github.com/teamtomo/membrain-seg\" repository = \"https://github.com/teamtomo/membrain-seg\" # same as console_scripts entry point # [project.scripts] # spam-cli = \"spam:main_cli\" [project.scripts] tomo_preprocessing = \"membrain_seg.tomo_preprocessing:cli\" membrain = \"membrain_seg.segmentation.cli:cli\" patch_corrections = \"membrain_seg.annotations:cli\" # Entry points # https://peps.python.org/pep-0621/#entry-points # [project.entry-points.\"spam.magical\"] # tomatoes = \"spam:main_tomatoes\" # https://hatch.pypa.io/latest/config/metadata/ [tool.hatch.version] source = \"vcs\" # https://hatch.pypa.io/latest/config/build/#file-selection # [tool.hatch.build.targets.sdist] # include = [\"/src\", \"/tests\"] # [tool.hatch.build.targets.wheel] # only-include = [\"src\"] # sources = [\"src\"] # https://github.com/charliermarsh/ruff [tool.ruff] line-length = 88 target-version = \"py38\" # https://beta.ruff.rs/docs/rules/ extend-select = [ \"E\", # style errors \"W\", # style warnings \"F\", # flakes \"D\", # pydocstyle \"I\", # isort \"U\", # pyupgrade # \"S\", # bandit \"C\", # flake8-comprehensions \"B\", # flake8-bugbear \"A001\", # flake8-builtins \"RUF\", # ruff-specific rules ] # I do this to get numpy-style docstrings AND retain # D417 (Missing argument descriptions in the docstring) # otherwise, see: # https://beta.ruff.rs/docs/faq/#does-ruff-support-numpy-or-google-style-docstrings # https://github.com/charliermarsh/ruff/issues/2606 extend-ignore = [ \"D100\", # Missing docstring in public module \"D107\", # Missing docstring in __init__ \"D203\", # 1 blank line required before class docstring \"D212\", # Multi-line docstring summary should start at the first line \"D213\", # Multi-line docstring summary should start at the second line \"D401\", # First line should be in imperative mood \"D413\", # Missing blank line after last section \"D416\", # Section name should end with a colon ] [tool.ruff.per-file-ignores] \"tests/*.py\" = [\"D\", \"S\"] \"setup.py\" = [\"D\"] # https://docs.pytest.org/en/6.2.x/customize.html [tool.pytest.ini_options] minversion = \"6.0\" testpaths = [\"tests\"] filterwarnings = [\"error\"] # https://mypy.readthedocs.io/en/stable/config_file.html [tool.mypy] files = \"src/**/\" strict = true disallow_any_generics = false disallow_subclassing_any = false show_error_codes = true pretty = true # # module specific overrides # [[tool.mypy.overrides]] # module = [\"numpy.*\",] # ignore_errors = true # https://coverage.readthedocs.io/en/6.4/config.html [tool.coverage.report] exclude_lines = [ \"pragma: no cover\", \"if TYPE_CHECKING:\", \"@overload\", \"except ImportError\", \"\\\\.\\\\.\\\\.\", \"raise NotImplementedError()\", ] [tool.coverage.run] source = [\"src\"] # https://github.com/mgedmin/check-manifest#configuration [tool.check-manifest] ignore = [ \".github_changelog_generator\", \".pre-commit-config.yaml\", \".ruff_cache/**/*\", \"setup.py\", \"tests/**/*\", ] # https://python-semantic-release.readthedocs.io/en/latest/configuration.html [tool.semantic_release] version_source = \"tag_only\" branch = \"main\" changelog_sections = \"feature,fix,breaking,documentation,performance,chore,:boom:,:sparkles:,:children_crossing:,:lipstick:,:iphone:,:egg:,:chart_with_upwards_trend:,:ambulance:,:lock:,:bug:,:zap:,:goal_net:,:alien:,:wheelchair:,:speech_balloon:,:mag:,:apple:,:penguin:,:checkered_flag:,:robot:,:green_apple:,Other\" # commit_parser=semantic_release.history.angular_parser build_command = \"pip install build && python -m build\" # # for things that require compilation # # https://cibuildwheel.readthedocs.io/en/stable/options/ # [tool.cibuildwheel] # # Skip 32-bit builds & PyPy wheels on all platforms # skip = [\"*-manylinux_i686\", \"*-musllinux_i686\", \"*-win32\", \"pp*\"] # test-extras = [\"test\"] # test-command = \"pytest {project}/tests -v\" # test-skip = \"*-musllinux*\" # [tool.cibuildwheel.environment] # HATCH_BUILD_HOOKS_ENABLE = \"1\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/merkle-dag-matlab",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Merkle-DAG-Matlab",
            "content": {
                "codemeta": "",
                "readme": "![MATLAB](https://img.shields.io/badge/MATLAB-%23D00000.svg?style=plastic&logo=mathworks&logoColor=white) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/Merkle-DAG-Matlab?style=plastic) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808889.svg)](https://doi.org/10.5281/zenodo.12808889) # Merkle-DAG Implementation in MATLAB This repository contains MATLAB scripts for implementing and using a Merkle Directed Acyclic Graph (DAG) data structure. The Merkle-DAG is a cryptographic data structure used to efficiently verify the integrity and consistency of data blocks. ### About > [IPFS Link](https://docs.ipfs.tech/concepts/merkle-dag/) ### Overview - Construct a Merkle-DAG manually or from data blocks. - Traverse the graph structure and verify the integrity of data blocks. - Multiple hash algorithms for computing node hashes. Supported algorithms from Java Security (via MATLAB) ```matlab import java.security.MessageDigest; java.security.Security.getAlgorithms('MessageDigest') ``` ### Scripts 1. `MerkleDAGNode.m` > Represents a node in the Merkle-DAG, holds data, compute hashes, and manage child nodes. 2. `MerkleDAG.m` > Constructs the Merkle-DAG from data blocks, performs integrity verification, and provides traversal methods (DFS & BFS). ### Example Usages #### Manually Create the Merkle-DAG (Adding Nodes) Use case: for scenarios where the DAG structure is not strictly determined by the data itself (no specific relationships/dependencies). ```matlab node1 = MerkleDAGNode([1 2 3]); % default: SHA-256, if no hash algorithm specified node2 = MerkleDAGNode([4 5 6]); node3 = MerkleDAGNode([7 8 9]); % Add children to node1 (hash recursively updated) node1.addChild(node2); node1.addChild(node3); % Add another child to node2 (hash recursively updated) node4 = MerkleDAGNode([10 11 12]); node2.addChild(node4); % Display the Merkle-DAG structure DAGGraph = MerkleDAG(); DAGGraph.setRoot(node1) DAGGraph.traverseDFS(); % Depth-First (DFS) traversal DAGGraph.traverseBFS(); % Breadth-First (BFS) traversal ``` #### Build DAG from data blocks Use case: automated, allowing constructing a Merkle-DAG from a matrix of data blocks. Each row of the matrix represents a data block. The DAG is built by hashing these blocks into the graph. For scenarios where the relationships between data blocks are determined by their positions in the matrix. ```matlab dataBlocks = [ // compose data of the MerkleDAG as a matrix 1 2 3; 4 5 6; 7 8 9; 10 11 12; 13 14 15 ]; merkleDAG = MerkleDAG(dataBlocks, 'SHA-384'); % Display the Merkle-DAG structure merkleDAG.traverseDFS(); % Depth-First (DFS) traversal merkleDAG.traverseBFS(); % Breadth-First (BFS) traversal ``` #### Integrity Verification The verifyBlock method checks whether a specific data block is part of the Merkle-DAG. Computes the hash of the data block and verifies it against the hashes in the DAG. ```matlab % Verify a specific data block dataBlockToVerify = [4 5 6]; merkleDAG.verifyBlock(dataBlockToVerify); ```\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/meshit",
            "repo_link": "https://github.com/bloech/MeshIt",
            "content": {
                "codemeta": "",
                "readme": "MeshIt 2010-2020 ================ The tool MeshIT uses TRIANGLE <http://www.cs.cmu.edu/~quake/triangle.html> and TETGEN <http://wias-berlin.de/software/tetgen> to generate a quality tetrahedral mesh based on structural geological information. This procedure is fully automatized and needs at least scattered data points as input! Main developers: Mauro Cacace (<mailto:cacace@gfz-potsdam.de>) and Guido Blöcher (<mailto:bloech@gfz-potsdam.de>). Some extensions were added by PERFACCT (www.perfacct.eu) by the following developers: Johannes Spazier Nihed Boussaidi Danny Puhan The source can be compiled as it comes on Windows, Linux and MacOS by running the building options below. For exporting the exodus file format used by Moose (https://github.com/idaholab/moose) the user has to specify some internal flags in the project file meshit.pro: * For Linux and Mac we suggest to link the static exodus library which comes along with libMesh (https://libmesh.github.io/) provided by Moose framework installation: * Set the EXODUS_LIBMESH variable to `true` * Define the path to the root directory of the `libmesh` installation using variable LIBMESH * For Windows you have to link the dynamic exodus library which will be provided by the package mingw-w64-ucrt-x86_64-libexodus provided by the MSYS2 (https://www.msys2.org/) installation: * Set the EXODUS_LIBRARY variable to `true` * Define the path the rootdirectory of 'exodusII' installation On all platforms the following requirements are suggested and tested: Windows: Qt 5.15.2 for Windows 64-bit Linux: Qt 5.9.9 for Linux 64-bit Mac: Qt 5.14.1 for OS X Preparations: * For Windows users, please follow the following steps for a proper `exodusII` dynamic library installation: * Download the `msys2-x86_64` installer (https://www.msys2.org/) * Run the installer. Installing MSYS2 requires 64 bit Windows 10 or newer. * Enter your desired Installation Folder (short ASCII-only path on a NTFS volume, no accents, no spaces, no symlinks, no subst or network drives, no FAT). * When done, click Finish. * Now MSYS2 is ready for you and a terminal for the UCRT64 environment will launch. * Install `mingw-w64-ucrt-x86_64-libexodus`. Run the following command: pacman -S mingw-w64-ucrt-x86_64-libexodus * To enable the dynamic dependencies of the `exodusII` library add the root folder to your environment variable path: * Open the Start Search, type in `env`, and choose `Edit the system environment variables`. * Click the `Environment Variables...` button. * Under the `User Variables` section, find the row with \"Path\" in the first column, and click edit. * The `Edit environment variable` UI will appear. Here, you can click \"New\" and type in the new path (default installation path C:\\msys64\\ucrt64) you want to add. * Dismiss all of the dialogs by choosing \"OK\". Your changes are saved! * You will probably need to restart apps for them to pick up the change. Restarting the machine would ensure all apps are run with the PATH change. * For macOS users, please check the version of macOS specified in the `meshit.pro` file (line `QMAKE_MACOSX_DEPLOYMENT_TARGET = 10.15`). By default, the version of macOS specified is 10.15 (macOS Catalina) Building: qmake meshit.pro make/nmake/mingw32-make Cite: * https://doi.org/10.5281/zenodo.4327281 * Cacace, M., Blöcher, G. MeshIt - a software for three dimensional volumetric meshing of complex faulted reservoirs. Environ Earth Sci 74, 5191-5209 (2015). https://doi.org/10.1007/s12665-015-4537-x\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/metabolator",
            "repo_link": "https://codebase.helmholtz.cloud/metabolator/metabolator",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/methylkit",
            "repo_link": "https://github.com/al2na/methylKit",
            "content": {
                "codemeta": "",
                "readme": "<a name=\"logo\"/> <div align=\"center\"> <img src=\"https://raw.githubusercontent.com/al2na/methylKit/master/inst/methylKit_logo.png\" alt=\"methylKit Logo\" ></img> </a> </div> methylKit ======== Build Status | | | | - | - | | Github | [![Build Status](https://github.com/al2na/methylKit/actions/workflows/check-standard.yaml/badge.svg)](https://github.com/al2na/methylKit/actions/workflows/check-standard.yaml) | | Bioc Release | [![Bioc release status](https://www.bioconductor.org/shields/build/release/bioc/methylKit.svg)](https://bioconductor.org/checkResults/release/bioc-LATEST/methylKit) | Bioc Devel | [![Bioc devel status](https://www.bioconductor.org/shields/build/devel/bioc/methylKit.svg?text=esfes&)](https://bioconductor.org/checkResults/devel/bioc-LATEST/methylKit) | [![GitHub R package version](https://img.shields.io/github/r-package/v/al2na/methylKit?label=version&)](https://github.com/al2na/methylKit/blob/master/NEWS) [![codecov](https://codecov.io/github/al2na/methylKit/branch/master/graphs/badge.svg)](https://codecov.io/github/al2na/methylKit) # Introduction *methylKit* is an [R](http://en.wikipedia.org/wiki/R_%28programming_language%29) package for DNA methylation analysis and annotation from high-throughput bisulfite sequencing. The package is designed to deal with sequencing data from [RRBS](http://www.nature.com/nprot/journal/v6/n4/abs/nprot.2010.190.html) and its variants, but also target-capture methods such as [Agilent SureSelect methyl-seq](http://www.halogenomics.com/sureselect/methyl-seq). In addition, methylKit can deal with base-pair resolution data for 5hmC obtained from Tab-seq or oxBS-seq. It can also handle whole-genome bisulfite sequencing data if proper input format is provided. ## Current Features * Coverage statistics * Methylation statistics * Sample correlation and clustering * Differential methylation analysis * Feature annotation and accessor/coercion functions * Multiple visualization options * Regional and tiling windows analysis * (Almost) proper [documentation](https://bioconductor.org/packages/release/bioc/vignettes/methylKit/inst/doc/methylKit.html) * Reading methylation calls directly from [Bismark(Bowtie/Bowtie2](http://www.bioinformatics.bbsrc.ac.uk/projects/bismark/) alignment files * Batch effect control * Multithreading support (for faster differential methylation calculations) * Coercion to objects from Bioconductor package GenomicRanges * Reading methylation percentage data from generic text files ## Staying up-to-date You can subscribe to our googlegroups page to get the latest information about new releases and features (low-frequency, only updates are posted) - https://groups.google.com/forum/#!forum/methylkit To ask questions please use methylKit_discussion forum - https://groups.google.com/forum/#!forum/methylkit_discussion You can also check out the blogposts we make on using methylKit - http://zvfak.blogspot.de/search/label/methylKit ------- ## Installation in R console, ```r library(devtools) install_github(\"al2na/methylKit\", build_vignettes=FALSE, repos=BiocManager::repositories(), dependencies=TRUE) ``` if this doesn't work, you might need to add `type=\"source\"` argument. ### Install the development version ```r library(devtools) install_github(\"al2na/methylKit\", build_vignettes=FALSE, repos=BiocManager::repositories(),ref=\"development\", dependencies=TRUE) ``` if this doesn't work, you might need to add `type=\"source\"` argument. ------- # How to Use Typically, bisulfite converted reads are aligned to the genome and % methylation value per base is calculated by processing alignments. *`methylKit`* takes that % methylation value per base information as input. Such input file may be obtained from [AMP pipeline](http://code.google.com/p/amp-errbs/) for aligning RRBS reads. A typical input file looks like this: ``` chrBase chr base strand coverage freqC freqT chr21.9764539 chr21 9764539 R 12 25.00 75.00 chr21.9764513 chr21 9764513 R 12 0.00 100.00 chr21.9820622 chr21 9820622 F 13 0.00 100.00 chr21.9837545 chr21 9837545 F 11 0.00 100.00 chr21.9849022 chr21 9849022 F 124 72.58 27.42 chr21.9853326 chr21 9853326 F 17 70.59 29.41 ``` *`methylKit`* reads in those files and performs basic statistical analysis and annotation for differentially methylated regions/bases. Also a tab separated text file with a generic format can be read in, such as methylation ratio files from [BSMAP](http://code.google.com/p/bsmap/), see [here](http://zvfak.blogspot.com/2012/10/how-to-read-bsmap-methylation-ratio.html) for an example. Alternatively, `read.bismark` function can read SAM file(s) output by [Bismark](http://www.bioinformatics.bbsrc.ac.uk/projects/bismark/)(using bowtie/bowtie2) aligner (the SAM file must be sorted based on chromosome and read start). The sorting must be done by unix sort or samtools, sorting using other tools may change the column order of the SAM file and that will cause an error. Below, there are several options showing how to do basic analysis with *`methylKit`*. ## Documentation ## * You can look at the vignette [here](https://bioconductor.org/packages/release/bioc/vignettes/methylKit/inst/doc/methylKit.html). This is the primary source of documentation. It includes detailed examples. * You can check out the [slides](https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/methylkit/methylKitTutorialSlides_2013.pdf ) for a tutorial at EpiWorkshop 2013. This works with older versions of methylKit, you may need to update the function names. * You can check out the [tutorial](https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/methylkit/methylKitTutorial_feb2012.pdf) prepared for EpiWorkshop 2012. This works with older versions of methylKit, you may need to update the function names. * You can check out the [slides](https://www.slideshare.net/AlexanderGosdschan/eurobioc-2018-metyhlkit-overview) prepared for EuroBioc 2018. This also includes more recent features of methylKit and is meant to give you a quick overview about what you can do with the package. ## Downloading Annotation Files Annotation files in BED format are needed for annotating your differentially methylated regions. You can download annotation files from UCSC table browser for your genome of interest. Go to [http://genome.ucsc.edu/cgi-bin/hgGateway]. On the top menu click on \"tools\" then \"table browser\". Select your \"genome\" of interest and \"assembly\" of interest from the drop down menus. Make sure you select the correct genome and assembly. Selecting wrong genome and/or assembly will return unintelligible results in downstream analysis. From here on you can either download *gene annotation* or *CpG island annotation*. 1. For gene annotation, select _\"Genes and Gene prediction tracks\"_ from the *\"group\"* drop-down menu. Following that, select _\"Refseq Genes\"_ from the *\"track\"* drop-down menu. Select _\"BED- browser extensible data\"_ for the *\"output format\"*. Click *\"get output\"* and on the following page click *\"get BED\"* without changing any options. save the output as a text file. 2. For CpG island annotation, select _\"Regulation\"_ from the *\"group\"* drop-down menu. Following that, select _\"CpG islands\"_ from the *\"track\"* drop-down menu. Select _\"BED- browser extensible data\"_ for the *\"output format\"*. Click *\"get output\"* and on the following page click *\"get BED\"* without changing any options. save the output as a text file. In addition, you can check this tutorial to learn how to download any track from UCSC in BED format (http://www.openhelix.com/cgi/tutorialInfo.cgi?id=28) ------- # R script for Genome Biology publication The most recent version of the R script in the Genome Biology manuscript is [here](http://code.google.com/p/methylkit/downloads/list?q=label:AdditionalFile4 ). ------- # Citing methylKit If you used methylKit please cite: * Altuna Akalin, Matthias Kormaksson, Sheng Li, Francine E. Garrett-Bakelman, Maria E. Figueroa, Ari Melnick, Christopher E. Mason. _(2012)_. *\"[methylKit: A comprehensive R package for the analysis of genome-wide DNA methylation profiles.](http://genomebiology.com/2012/13/10/R87/)\"* _Genome Biology_ , 13:R87. If you used flat-file objects or over-dispersion corrected tests please consider citing: * Wreczycka K, Gosdschan A, Yusuf D, Grüning B, Assenov Y, Akalin A. *\"[Strategies for analyzing bisulfite sequencing data.](https://linkinghub.elsevier.com/retrieve/pii/S0168-1656(17)31593-6)\"* J Biotechnol., 2017 and also consider citing the following publication as a use-case with specific cutoffs: * Altuna Akalin, Francine E. Garrett-Bakelman, Matthias Kormaksson, Jennifer Busuttil, Lu Zhang, Irina Khrebtukova, Thomas A. Milne, Yongsheng Huang, Debabrata Biswas, Jay L. Hess, C. David Allis, Robert G. Roeder, Peter J. M. Valk, Bob Löwenberg, Ruud Delwel, Hugo F. Fernandez, Elisabeth Paietta, Martin S. Tallman, Gary P. Schroth, Christopher E. Mason, Ari Melnick, Maria E. Figueroa. _(2012)_. *\"[Base-Pair Resolution DNA Methylation Sequencing Reveals Profoundly Divergent Epigenetic Landscapes in Acute Myeloid Leukemia.](http://www.plosgenetics.org/article/info%3Adoi%2F10.1371%2Fjournal.pgen.1002781)\"* _PLoS Genetics_ 8(6). ------- # Contact & Questions e-mail to [methylkit_discussion@googlegroups.com](mailto:methylkit_discussion@googlegroups.com ) or post a question using [the web interface](https://groups.google.com/forum/#!forum/methylkit_discussion). if you are going to submit bug reports or ask questions, please send sessionInfo() output from R console as well. Questions are very welcome, although we suggest you read the paper, documentation(function help pages and the vignette) and [ blog entries](http://zvfak.blogspot.com/search/label/methylKit) first. The answer to your question might be there already. ------- # Contribute to the development See the [trello board](https://trello.com/b/k2kv1Od7/methylkit) for methylKit development. You can contribute to the methylKit development via github ([http://github.com/al2na/methylKit/]) by opening an issue and discussing what you want to contribute, we will guide you from there. In addition, you should: * Bump up the version in the DESCRIPTION file on the 3rd number. For example, the master branch has the version numbering as in \"X.Y.1\". If you make a change to master branch you should bump up the version in the DESCRIPTION file to \"X.Y.2\". * Add your changes to the NEWS file as well under the correct version and appropriate section. Attribute the changes to yourself, such as \"Contributed by X\" License --------- Artistic License/GPL\n",
                "dependencies": "Package: methylKit Type: Package Title: DNA methylation analysis from high-throughput bisulfite sequencing results Version: 1.33.3 Author: Altuna Akalin [aut, cre], Matthias Kormaksson [aut], Sheng Li [aut], Arsene Wabo [ctb], Adrian Bierling [aut], Alexander Blume [aut], Katarzyna Wreczycka [ctb] Maintainer: Altuna Akalin <aakalin@gmail.com>, Alexander Blume <alex.gos90@gmail.com> Description: methylKit is an R package for DNA methylation analysis and annotation from high-throughput bisulfite sequencing. The package is designed to deal with sequencing data from RRBS and its variants, but also target-capture methods and whole genome bisulfite sequencing. It also has functions to analyze base-pair resolution 5hmC data from experimental protocols such as oxBS-Seq and TAB-Seq. Methylation calling can be performed directly from Bismark aligned BAM files. License: Artistic-2.0 URL: https://github.com/al2na/methylKit BugReports: https://github.com/al2na/methylKit/issues LazyLoad: yes NeedsCompilation: yes LinkingTo: Rcpp, Rhtslib (>= 1.13.1) SystemRequirements: GNU make biocViews: DNAMethylation, Sequencing, MethylSeq Depends: R (>= 3.5.0), GenomicRanges (>= 1.18.1), methods Imports: IRanges, data.table (>= 1.9.6), parallel, S4Vectors (>= 0.13.13), GenomeInfoDb, KernSmooth, qvalue, emdbook, Rsamtools, gtools, fastseg, rtracklayer, mclust, mgcv, Rcpp, R.utils, limma, grDevices, graphics, stats, utils Suggests: testthat (>= 2.1.0), knitr, rmarkdown, genomation, BiocManager VignetteBuilder: knitr Collate: 'methylKit.R' 'backbone.R' 'diffMeth.R' 'clusterSamples.R' 'regionalize.R' 'processBismarkAln.R' 'RcppExports.R' 'document_data.R' 'bedgraph.R' 'reorganize.R' 'percMethylation.R' 'normalizeCoverage.R' 'pool.R' 'adjustMethylC.R' 'updateMethObject.R' 'batchControl.R' 'dataSim.R' 'methylDBClasses.R' 'methylDBFunctions.R' 'tabix.functions.R' 'methSeg.R' 'diffMethDSS.R' 'deprecated_defunct.R' 'onUnload.R' RoxygenNote: 7.1.1\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mfdfa",
            "repo_link": "https://github.com/LRydin/MFDFA",
            "content": {
                "codemeta": "",
                "readme": "[![DOI:10.1016/j.cpc.2021.108254](http://img.shields.io/badge/DOI-10.1016/j.cpc.2021.108254-00ff00.svg)](https://doi.org/10.1016/j.cpc.2021.108254) [![arXiv](https://img.shields.io/badge/arXiv-2104.10470-00ff00.svg)](https://arxiv.org/abs/2104.10470) [![zenodo](https://zenodo.org/badge/224135077.svg)](https://zenodo.org/badge/latestdoi/224135077) ![PyPI - License](https://img.shields.io/pypi/l/MFDFA) ![PyPI](https://img.shields.io/pypi/v/MFDFA) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/MFDFA) [![Build Status](https://github.com/LRydin/MFDFA/actions/workflows/CI.yml/badge.svg)](https://github.com/LRydin/MFDFA/actions/workflows/CI.yml) [![codecov](https://codecov.io/gh/LRydin/MFDFA/branch/master/graph/badge.svg)](https://codecov.io/gh/LRydin/MFDFA) [![Documentation Status](https://readthedocs.org/projects/mfdfa/badge/?version=latest)](https://mfdfa.readthedocs.io/en/latest/?badge=latest) # MFDFA Multifractal Detrended Fluctuation Analysis `MFDFA` is a model-independent method to uncover the self-similarity of a stochastic process or auto-regressive model. `DFA` was first developed by Peng *et al.*<sup>1</sup> and later extended to study multifractality `MFDFA` by Kandelhardt *et al.*<sup>2</sup>. In the latest release there is as well added a moving window system, especially useful for short timeseries, a recent extension to DFA called *extended DFA*, and the extra feature of Empirical Mode Decomposition as detrending method. # Installation To install MFDFA you can simply use ``` pip install MFDFA ``` And on your favourite editor simply import `MFDFA` as ```python from MFDFA import MFDFA ``` There is an added library `fgn` to generate fractional Gaussian noise. You can find the latest published paper of this library in Computer Physics Communications [L. Rydin Gorjão, G. Hassan, J. Kurths, and D. Witthaut, _MFDFA: Efficient multifractal detrended fluctuation analysis in python_, Computer Physics Communications *273*, 108254 2022](https://doi.org/10.1016/j.cpc.2021.108254). You can find the paper [here](https://github.com/LRydin/MFDFA/blob/master/paper/paper.pdf). # The `MFDFA` library `MFDFA` basis is solely dependent on `numpy`, especially `numpy`'s `polynomial`. In version 0.3 a [Empirical Mode Decomposition](https://en.wikipedia.org/wiki/Hilbert%E2%80%93Huang_transform) method was added for an alternative method of detrending timeseries, relying on [Dawid Laszuk's](https://github.com/laszukdawid/PyEMD) `PyEMD`. # Employing the `MFDFA` library ## An exemplary one-dimensional fractional Ornstein-Uhlenbeck process The rationale here is simple: Numerically integrate a stochastic process in which we know exactly the fractal properties, characterised by the Hurst coefficient, and recover this with MFDFA. We will use a fractional Ornstein-Uhlenbeck, a commonly employ stochastic process with mean-reverting properties. For a more detailed explanation on how to integrate an Ornstein-Uhlenbeck process, see the [kramersmoyal's package](https://github.com/LRydin/KramersMoyal#a-one-dimensional-stochastic-process). You can also follow the [fOU.ipynb](/examples/fOU.ipynb) ### Generating a fractional Ornstein-Uhlenbeck process This is one method of generating a (fractional) Ornstein-Uhlenbeck process with *H=0.7*, employing a simple Euler-Maruyama integration method ```python # Imports from MFDFA import MFDFA from MFDFA import fgn # where this second library is to generate fractional Gaussian noises # integration time and time sampling t_final = 2000 delta_t = 0.001 # Some drift theta and diffusion sigma parameters theta = 0.3 sigma = 0.1 # The time array of the trajectory time = np.arange(0, t_final, delta_t) # The fractional Gaussian noise H = 0.7 dB = (t_final ** H) * fgn(N = time.size, H = H) # Initialise the array y y = np.zeros([time.size]) # Integrate the process for i in range(1, time.size): y[i] = y[i-1] - theta * y[i-1] * delta_t + sigma * dB[i] ``` And now you have a fractional process with a self-similarity exponent *H=0.7* ### Using the `MFDFA` To now utilise the `MFDFA`, we take this exemplary process and run the (multifractal) detrended fluctuation analysis. For now lets consider only the monofractal case, so we need only `q=2`. ```python # Select a band of lags, which usually ranges from # very small segments of data, to very long ones, as lag = np.unique(np.logspace(0.5, 3, 100).astype(int)) # Notice these must be ints, since these will segment # the data into chucks of lag size # Select the power q q = 2 # The order of the polynomial fitting order = 1 # Obtain the (MF)DFA as lag, dfa = MFDFA(y, lag = lag, q = q, order = order) ``` Now we need to visualise the results, which can be understood in a log-log scale. To find *H* we need to fit a line to the results in the log-log plot ```python # To uncover the Hurst index, lets get some log-log plots plt.loglog(lag, dfa, 'o', label='fOU: MFDFA q=2') # And now we need to fit the line to find the slope. Don't # forget that since you are plotting in a double logarithmic # scales, you need to fit the logs of the results H_hat = np.polyfit(np.log(lag)[4:20],np.log(dfa[4:20]),1)[0] # Now what you should obtain is: slope = H + 1 print('Estimated H = '+'{:.3f}'.format(H_hat[0])) ``` <img src=\"docs/_static/fig1.png\" title=\"MFDFA of a fractional Ornstein-Uhlenbeck process\" height=\"250\"/> ## Uncovering multifractality in stochastic processes You can find more about multifractality in the [documentation](https://mfdfa.readthedocs.io/en/latest/1dLevy.html). # Changelog - Version 0.4.3 - Reverting negative values in the estimation of the singularity strenght α. - Version 0.4.2 - Corrected spectral plots. Added [examples](https://github.com/LRydin/MFDFA/tree/master/examples) from the paper. - Version 0.4.1 - Added conventional spectral plots as _h(q)_ vs _q_, _τ(q)_ vs _q_, and _f(α)_ vs _α_. - Version 0.4 - EMD is now optional. Restored back compatibility: py3.3 to py3.9. For EMD py3.6 or larger is needed. - Version 0.3 - Adding EMD detrending. First release. PyPI code. - Version 0.2 - Removed experimental features. Added documentation - Version 0.1 - Uploaded initial working code # Contributions I welcome reviews and ideas from everyone. If you want to share your ideas or report a bug, open an [issue](https://github.com/LRydin/KramersMoyal/issues) here on GitHub, or contact me directly. If you need help with the code, the theory, or the implementation, do not hesitate to reach out, I am here to help. This package abides to a [Conduct of Fairness](contributions.md). # Literature and Support ### Submission history This library has been submitted for publication at [The Journal of Open Source Software](https://joss.theoj.org/) in December 2019. It was rejected. The review process can be found [here on GitHub](https://github.com/openjournals/joss-reviews/issues/1966). The plan is to extend the library and find another publisher. ### History This project was started in 2019 at the [Faculty of Mathematics, University of Oslo](https://www.mn.uio.no/math/english/research/groups/risk-stochastics/) in the Risk and Stochastics section by Leonardo Rydin Gorjão and is supported by Dirk Witthaut and the [Institute of Energy and Climate Research Systems Analysis and Technology Evaluation](https://www.fz-juelich.de/iek/iek-ste/EN/Home/home_node.html). I'm very thankful to all the folk in Section 3 in the Faculty of Mathematics, University of Oslo, for helping me getting around the world of stochastic processes: Dennis, Anton, Michele, Fabian, Marc, Prof. Benth and Prof. di Nunno. In April 2020 Galib Hassan joined in extending `MFDFA`, particularly the implementation of `EMD`. ### Funding Helmholtz Association Initiative *Energy System 2050 - A Contribution of the Research Field Energy* and the grant No. VH-NG-1025; *STORM - Stochastics for Time-Space Risk Models* project of the Research Council of Norway (RCN) No. 274410, and the *E-ON Stipendienfonds*. ### References <sup>1</sup>Peng, C.-K., Buldyrev, S. V., Havlin, S., Simons, M., Stanley, H. E., & Goldberger, A. L. (1994). *Mosaic organization of DNA nucleotides*. [Physical Review E, 49(2), 1685-1689](https://doi.org/10.1103/PhysRevE.49.1685)\\ <sup>2</sup>Kantelhardt, J. W., Zschiegner, S. A., Koscielny-Bunde, E., Havlin, S., Bunde, A., & Stanley, H. E. (2002). *Multifractal detrended fluctuation analysis of nonstationary time series*. [Physica A: Statistical Mechanics and Its Applications, 316(1-4), 87-114](https://doi.org/10.1016/S0378-4371(02)01383-3)\n",
                "dependencies": "numpy\nimport setuptools with open(\"README.md\", \"r\") as fh: long_description = fh.read() setuptools.setup( name=\"MFDFA\", version=\"0.4.3\", author=\"Leonardo Rydin Gorjao\", author_email=\"leonardo.rydin@gmail.com\", description=\"Multifractal Detrended Fluctuation Analysis in Python\", long_description=long_description, long_description_content_type=\"text/markdown\", url=\"https://github.com/LRydin/MFDFA\", packages=setuptools.find_packages(), install_requires = [\"numpy\"], extras_require = {\"EMD-signal\": [\"EMD-signal\"], \"matplotlib\": [\"matplotlib\"]}, classifiers=[ \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: MIT License\", \"Operating System :: OS Independent\", ], license=\"MIT License\", python_requires='>=3.6', )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mhm",
            "repo_link": "https://git.ufz.de/mhm/mhm",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mibianto",
            "repo_link": "https://github.com/ccb-sb/mibianto",
            "content": {
                "codemeta": "",
                "readme": "# MiBiAnTo: An Online Microbiome Analysis Tool [Mibianto Webpage](https://www.ccb.uni-saarland.de/mibianto) ## Issues, queries and request We have created this GitHub repository to handle communication with users. Please feel free to make use of it to get in touch with questions or remarks regarding the database website or content.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/micromechanics-indentationgui",
            "repo_link": "https://github.com/micromechanics/indentationGUI",
            "content": {
                "codemeta": "",
                "readme": "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.11563149.svg)](https://doi.org/10.5281/zenodo.11563149) <h2 align = \"center\"> micromechanics-indentationGUI </h2> <p align=\"center\"> <img src=\"https://raw.githubusercontent.com/micromechanics/indentationGUI/main/micromechanics_indentationGUI/pic/logo.png\" width=\"314\" title=\"micromechanics-indentationGUI\" > </p> # Install - Create a new environment (python >= 3.8) using **anaconda-navigator** (https://www.anaconda.com/). In Anaconda's documentation (https://docs.anaconda.com/free/navigator/) you will learn: - how to **install** anaconda-navigator in the section of \"*Anaconda Navigator* => *Installation*\" (reading takes ~5 min), - how to **create and activate** a new environment in the section of \"*Anaconda Navigator* => *Tutorials* => *How to create a Python 3.5 environment from Anaconda2 or Anaconda3*\" (reading takes ~5 min). - In the terminal of the environment created by Anaconda Navigator, keyboard type the following command and press Enter ``` bash pip install micromechanics-indentationGUI ``` # Upgrade In the terminal, keyboard type the following command and press Enter ``` bash pip install --upgrade micromechanics-indentationGUI ``` # Usage Users need to know: - For fast reading using **HDF5** files: - Using the given original file (e.g. the XLSX file for G200X), the HDF5 file will be automatically generated at the first calibration/calculation (or when an HDF5 file with the same name as the XLSX file does not exist). - The automatically generated HDF5 file has the same file name as the original file (e.g. the XLSX file for G200X) except the file extension of \".h5\" and locates in the same folder as the original file. - The original file extension (e.g. '.xlsx' for G200X) should be given in the path instead of the file extension of the HDF5 file (\".h5\"). - **[Important]** If you changed the content of the original file, please delete the correspoding HDF5 file. Running by keyboard typing the following command and pressing Enter in the terminal ``` bash micromechanics-indentationGUI ``` # Uninstall In the terminal, keyboard type the following command and press Enter ``` bash pip uninstall micromechanics-indentationGUI ``` # More detailed descriptions for developers # Prepare and create a new version - Delete RecentFiles.txt in /indentationGUI/micromechanics_indentationGUI - Delete *.hf in /indentationGUI/micromechanics_indentationGUI/Examples - Set \"# pylint: skip-file\" for all files named \"***_ui.py\" - Test the code: linting, documentation and then the tests from the project's main directory ``` bash pylint micromechanics_indentationGUI/ make -C docs html # python tests/testVerification.py ``` Then upload/create-pull-request to GitHub, via ``` bash ./commit.py 'my message' ```\n",
                "dependencies": "[build-system] requires = [\"setuptools >= 43.0.0\", \"wheel\"]\n#This file is autogenerated by commit.py from setup.cfg. Change content there pyside6 ==6.4.2 numpy matplotlib >= 3.6.3 micromechanics == 1.1.14 pandas ==1.4.2 openpyxl >= 3.1.1 scikit-learn tables xlsxwriter == 3.1.2\nfrom setuptools import setup from setuptools.command.install import install import os import platform import commit import sys class PostInstallCommand(install): \"\"\"Post-installation for installation mode.\"\"\" def run(self): install.run(self) if platform.system() == \"Linux\" and ('generic' in platform.platform()): self.create_desktop_file() def create_desktop_file(self): \"\"\"create .desktop file for Linux-Ubuntu user\"\"\" pythonVersion = sys.version[:3] installation_path = os.path.join(sys.prefix, 'lib', f\"python{pythonVersion}\", 'site-packages') desktop_file_content = f\"\"\" [Desktop Entry] Type=Application Name=indentationGUI Exec={os.path.join(sys.prefix, 'bin', 'indentationGUI')} Icon={os.path.join(installation_path, 'micromechanics_indentationGUI', 'pic', 'indentationGUI.png')} Terminal=false \"\"\" desktop_file_path = os.path.expanduser(\"~/.local/share/applications/indentationGUI.desktop\") with open(desktop_file_path, \"w\") as desktop_file: desktop_file.write(desktop_file_content) if __name__ == '__main__': setup( name='micromechanics-indentationGUI', version=commit.get_version()[1:], cmdclass ={'install':PostInstallCommand} )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/millepede-ii",
            "repo_link": "https://gitlab.desy.de/claus.kleinwort/millepede-ii",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/minterpy",
            "repo_link": "https://github.com/casus/minterpy",
            "content": {
                "codemeta": "",
                "readme": "![](./docs/assets/Wordmark-color.png) [![Code style: black][black-badge]][black-link] # minterpy <figure class=\"quote\"> <blockquote> to minterpy *sth.* (transitive verb) -- to produce a multivariate polynomial representation of *sth.* . </blockquote> <figcaption> &mdash; The minterpy developers in <cite>[\"Lifting the curse of dimensionality\"](https://interpol.pages.hzdr.de/minterpy/fundamentals/introduction.html)</cite> </figcaption> </figure> --- `minterpy` is an open-source Python package for a multivariate generalization of the classical Newton and Lagrange interpolation schemes as well as related tasks. It is based on an optimized re-implementation of the multivariate interpolation prototype algorithm (*MIP*) by Hecht et al.[^1] and thereby provides software solutions that lift the curse of dimensionality from interpolation tasks. While interpolation occurs as the bottleneck of most computational challenges, `minterpy` aims to free empirical sciences from their computational limitations. `minterpy` is continuously extended and improved by adding further functionality and modules that provide novel digital solutions to a broad field of computational challenges, including but not limited to: - multivariate interpolation - non-linear polynomial regression - numerical integration - global (black-box) optimization - surface level-set methods - non-periodic spectral partial differential equations (PDE) solvers on flat and complex geometries - machine learning regularization - data reconstruction - computational solutions in algebraic geometry ## Installation Since this implementation is a prototype, we currently only provide the installation by self-building from source. We recommend to using `git` to get the `minterpy` source: ```bash git clone https://gitlab.hzdr.de/interpol/minterpy.git ``` Within the source directory, you may use the following package manager to install ``minterpy``. A best practice is to create a virtual environment for `minterpy`. You can do this with the help of [conda] and the ``environment.yaml`` by: ```bash conda env create -f environment.yaml ``` A new conda environment called `minterpy` is created. Activate the new environment by: ```bash conda activate minterpy ``` From within the environment, install the `minterpy` using [pip], ```bash pip install [-e] .[all,dev,docs] ``` where the flag `-e` means the package is directly linked into the python site-packages of your Python version. The options `[all,dev,docs]` refer to the requirements defined in the `options.extras_require` section in `setup.cfg`. You **must not** use the command `python setup.py install` to install `minterpy`, as you cannot always assume the files `setup.py` will always be present in the further development of `minterpy`. Finally, if you want to deactivate the conda environment, type: ```bash conda deactivate ``` Alternative to conda, you can create a new virtual environment via [venv], [virtualenv], or [pyenv-virtualenv]. See [CONTRIBUTING.md](./CONTRIBUTING.md) for details. ## Quickstart With `minterpy` one can easily interpolate a given function. For instance, take the function `f(x) = x\\sin(10x)` in one dimension: ```python import numpy as np def test_function(x): return x * np.sin(10*x) ``` In order to `minterpy` the function `test_function` one can use the top-level function `interpolate`: ```python import minterpy as mp interpolant = mp.interpolate(test_function,spatial_dimension=1, poly_degree=64) ``` Here, interpolant is a callable function, which can be used as a representation of `test_function`. `interpolate` takes as arguments the function to interpolate, the number of dimensions (`spatial_dimension`), and the degree of the underlying polynomial (`poly_degree`). You may adjust this parameter in order to get higher accuracy. For the example above, a degree of 64 produces an interpolant that reproduces the `test_function` almost up to machine precision: ```python import matplotlib.pylab as plt x = np.linspace(-1,1,100) plt.plot(x,interpolant(x),label=\"interpolant\") plt.plot(x,test_function(x),\"k.\",label=\"test function\") plt.legend() plt.show() ``` <img src=\"./docs/assets/images/test-function1D.png\" alt=\"Compare test function with its interpolant\" width=\"400\"/> For more comprehensive examples, see the [getting started guides](https://interpol.pages.hzdr.de/minterpy/getting-started/index.html) section of the ``minterpy`` docs. ## Testing After installation, we encourage you to at least run the unit tests of `minterpy`, where we use [`pytest`](https://docs.pytest.org/en/6.2.x/) to run the tests. If you want to run all tests, type: ```bash pytest [-vvv] ``` from within the `minterpy` source directory. ## Contributing to `minterpy` Contributions to the `minterpy` packages are highly welcome. We recommend you have a look at the [CONTRIBUTING.md](./CONTRIBUTING.md) first. For a more comprehensive contribution guide visit the [Contributors section](link-to-developer-section) of the documentation. ## Credits and contributors This work was partly funded by the Center for Advanced Systems Understanding (CASUS) that is financed by Germany's Federal Ministry of Education and Research (BMBF) and by the Saxony Ministry for Science, Culture and Tourism (SMWK) with tax funds on the basis of the budget approved by the Saxony State Parliament. ### The minterpy development team The core development of the `minterpy` is currently done by a small team at the Center for Advanced Systems Understanding ([CASUS]), namely - Uwe Hernandez Acosta ([HZDR]/[CASUS]) (u.hernandez@hzdr.de) - Sachin Krishnan Thekke Veettil ([HZDR]/[CASUS]) (s.thekke-veettil@hzdr.de) - Damar Wicaksono ([HZDR]/[CASUS]) (d.wicaksono@hzdr.de) - Janina Schreiber ([HZDR]/[CASUS]) (j.schreiber@hzdr.de) ### Mathematical foundation - Michael Hecht ([HZDR]/[CASUS]) (m.hecht@hzdr.de) ### Former Members and Contributions - Jannik Michelfeit - Nico Hoffman ([HZDR]) - Steve Schmerler ([HZDR]) - Vidya Chandrashekar (TU Dresden) ### Acknowledgement - Klaus Steiniger ([HZDR]) - Patrick Stiller ([HZDR]) - Matthias Werner ([HZDR]) - Krzysztof Gonciarz ([MPI-CBG],[CSBD]) - Attila Cangi ([HZDR]/[CASUS]) - Michael Bussmann ([HZDR]/[CASUS]) ### Community This package would not be possible without many contributions done from the community as well. For that, we want to send big thanks to: - the guy who will show me how to include a list of contributors on github/gitlab ## License [MIT](LICENSE) © minterpy development team [^1]: [arXiv:2010.10824](https://arxiv.org/abs/2010.10824) [conda]: https://docs.conda.io/ [pip]: https://pip.pypa.io/en/stable/ [venv]: https://docs.python.org/3/tutorial/venv.html [virtualenv]: https://virtualenv.pypa.io/en/latest/ [pyenv-virtualenv]: https://github.com/pyenv/pyenv-virtualenv [pre-commit]: https://pre-commit.com/ [Jupyter]: https://jupyter.org/ [nbstripout]: https://github.com/kynan/nbstripout [Google style]: http://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings [virtualenv]: https://virtualenv.pypa.io/en/latest/index.html [pytest]: https://docs.pytest.org/en/6.2.x/ [CASUS]: https://www.casus.science [HZDR]: https://www.hzdr.de [MPI-CBG]: https://www.mpi-cbg.de [CSBD]: https://www.csbdresden.de [black-badge]: https://img.shields.io/badge/code%20style-black-000000.svg [black-link]: https://github.com/psf/black\n",
                "dependencies": "[build-system] requires = [\"wheel\", \"setuptools>=42\", \"setuptools_scm[toml]>=3.4\"] build-backend = \"setuptools.build_meta\" [tool.setuptools_scm] write_to = \"src/minterpy/version.py\" [tool.isort] profile = \"black\"\n#!/usr/bin/env python # Copyright (c) 2021, Uwe Hernandez Acosta # # Distributed under the 3-clause BSD license, see accompanying file LICENSE # or https://gitlab.hzdr.de/interpol/minterpy for details. from setuptools import setup setup()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mirp",
            "repo_link": "https://github.com/oncoray/mirp",
            "content": {
                "codemeta": "",
                "readme": "<img src=\"https://raw.githubusercontent.com/oncoray/mirp/master/icon/mirp.svg\" align=\"right\" width=\"120\"/> ![GitHub License](https://img.shields.io/github/license/oncoray/mirp) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mirp) [![PyPI - Version](https://img.shields.io/pypi/v/mirp)](https://pypi.org/project/mirp/) ![GitHub Actions Workflow Status](https://img.shields.io/github/actions/workflow/status/oncoray/mirp/auto-test-dependencies_timed.yml) [![JOSS](https://joss.theoj.org/papers/165c85b1ecad891550a21b12c8b2e577/status.svg)](https://joss.theoj.org/papers/165c85b1ecad891550a21b12c8b2e577) # Medical Image Radiomics Processor MIRP is a python package for quantitative analysis of medical images. It focuses on processing images for integration with radiomics workflows. These workflows either use quantitative features computed using MIRP, or directly use MIRP to process images as input for neural networks and other deep learning models. MIRP offers the following main functionality: - [Extract and collect metadata](https://oncoray.github.io/mirp/image_metadata.html) from medical images. - [Find and collect labels or names](https://oncoray.github.io/mirp/mask_labels.html) of regions of interest from image segmentations. - [Compute quantitative features](https://oncoray.github.io/mirp/quantitative_image_analysis.html) from regions of interest in medical images. - [Process images for deep learning](https://oncoray.github.io/mirp/deep_learning.html). ## Tutorials We currently offer the following tutorials: - [Computing quantitative features from MR images](https://oncoray.github.io/mirp/tutorial_compute_radiomics_features_mr.html) - [Applying filters to images](https://oncoray.github.io/mirp/tutorial_apply_image_filter.html) ## Documentation Documentation can be found here: https://oncoray.github.io/mirp/ ## Supported Python and OS MIRP currently supports the following Python versions and operating systems: | Python | Linux | Win | OSX | |--------|-----------|-----------|-----------| | 3.10 | Supported | Supported | Supported | | 3.11 | Supported | Supported | Supported | | 3.12 | Supported | Supported | Supported | ## Supported imaging and mask modalities MIRP currently supports the following image modalities: | File format | File type | Supported modality | |-------------|-----------|-------------------------------------------------| | DICOM | image | CT, MR (incl. ADC, DCE), PT, RTDOSE, CR, DX, MG | | DICOM | mask | RTSTRUCT, SEG | | NIfTI | any | any | | NRRD | any | any | | numpy | any | any | NIfTI, NRRD, and numpy files support any kind of (single-channel) image. MIRP cannot process RGB or 4D images. ## Installing MIRP MIRP is available from PyPI and can be installed using `pip`, or other installer tools: ```commandline pip install mirp ``` ## Examples - Computing Radiomics Features MIRP can be used to compute quantitative features from regions of interest in images in an IBSI-compliant manner using a standardized workflow This requires both images and masks. MIRP can process DICOM, NIfTI, NRRD and numpy images. Masks are DICOM radiotherapy structure sets (RTSTRUCT), DICOM segmentation (SEG) or volumetric data with integer labels (e.g. 1, 2, etc.). Below is a minimal working example for extracting features from a single image file and its mask. ```python from mirp import extract_features feature_data = extract_features( image=\"path to image\", mask=\"path to mask\", base_discretisation_method=\"fixed_bin_number\", base_discretisation_n_bins=32 ) ``` Instead of providing the path to the image (`\"path_to_image\"`), a numpy image can be provided, and the same goes for `\"path to mask\"`. The disadvantage of doing so is that voxel spacing cannot be determined. MIRP also supports processing images and masks for multiple samples (e.g., patients). The syntax is much the same, but depending on the file type and directory structure, additional arguments need to be specified. For example, assume that files are organised in subfolders for each sample, i.e. `main_folder / sample_name / subfolder`. The minimal working example is then: ```python from mirp import extract_features feature_data = extract_features( image=\"path to main image directory\", mask=\"path to main mask directory\", image_sub_folder=\"image subdirectory structure relative to main image directory\", mask_sub_folder=\"mask subdirectory structure relative to main mask directory\", base_discretisation_method=\"fixed_bin_number\", base_discretisation_n_bins=32 ) ``` The above example will compute features sequentially. MIRP supports parallel processing using the `ray` package. Feature computation can be parallelized by specifying the `num_cpus` argument, e.g. `num_cpus=2` for two CPU threads. ## Examples - Image Preprocessing for Deep Learning Deep learning-based radiomics is an alternative to using predefined quantitative features. MIRP supports preprocessing of images and masks using the same standardized workflow that is used for computing features. Below is a minimal working example for preprocessing deep learning images. Note that MIRP uses the numpy notation for indexing, i.e. indices are ordered [*z*, *y*, *x*]. ```python from mirp import deep_learning_preprocessing processed_images = deep_learning_preprocessing( image=\"path to image\", mask=\"path to mask\", crop_size=[50, 224, 224] ) ``` ## Examples - Summarising Image Metadata MIRP can also summarise image metadata. This is particularly relevant for DICOM files that have considerable metadata. Other files, e.g. NIfTI, only have metadata related to position and spacing of the image. Below is a minimal working example for extracting metadata from a single image file. ```python from mirp import extract_image_parameters image_parameters = extract_image_parameters( image=\"path to image\" ) ``` MIRP also supports extracting metadata from multiple files. For example, assume that files are organised in subfolders for each sample, i.e. `main_folder / sample_name / subfolder`. The minimal working example is then: ```python from mirp import extract_image_parameters image_parameters = extract_image_parameters( image=\"path to main image directory\", image_sub_folder=\"image subdirectory structure relative to main image directory\" ) ``` ## Examples - Finding labels MIRP can identify which labels are present in masks. For a single mask file, labels can be retrieved as follows: ```python from mirp import extract_mask_labels mask_labels = extract_mask_labels( mask=\"path to mask\" ) ``` MIRP supports extracting labels from multiple masks. For example, assume that files are organised in subfolders for each sample, i.e. `main_folder / sample_name / subfolder`. The minimal working example is then: ```python from mirp import extract_mask_labels mask_labels = extract_mask_labels( mask=\"path to main mask directory\", mask_sub_folder=\"mask subdirectory structure relative to main mask directory\" ) ``` ## Transitioning to version 2 Version 2 is a major refactoring of the previous code base. For users this brings the following noticeable changes: - MIRP was previously configured using two `xml` files: [`config_data.xml`](mirp/config_data.xml) for configuring directories, data to be read, etc., and [`config_settings.xml`](mirp/config_settings.xml) for configuring experiments. While these two files can still be used, MIRP can now be configured directly, without using these files. - The main functions of MIRP (`mainFunctions.py`) have all been re-implemented. - `mainFunctions.extract_features` is now `extract_features` (functional form) or `extract_features_generator` (generator). The replacements allow for both writing feature values to a directory and returning them as function output. - `mainFunctions.extract_images_to_nifti` is now `extract_images` (functional form) or `extract_images_generator` (generator). The replacements allow for both writing images to a directory (e.g., in NIfTI or numpy format) and returning them as function output. - `mainFunctions.extract_images_for_deep_learning` has been replaced by `deep_learning_preprocessing` (functional form) and `deep_learning_preprocessing_generator` (generator). - `mainFunctions.get_file_structure_parameters` and `mainFunctions.parse_file_structure` are deprecated, as the the file import system used in version 2 no longer requires a rigid directory structure. - `mainFunctions.get_roi_labels` is now `extract_mask_labels`. - `mainFunctions.get_image_acquisition_parameters` is now `extract_image_parameters`. For advanced users and developers, the following changes are relevant: - MIRP previously relied on `ImageClass` and `RoiClass` objects. These have been completely replaced by `GenericImage` (and its subclasses, e.g. `CTImage`) and `BaseMask` objects, respectively. New image modalities can be added as subclass of `GenericImage` in the `mirp.images` submodule. - File import, e.g. from DICOM or NIfTI files, in version 1 was implemented in an ad-hoc manner, and required a rigid directory structure. Since version 2, file import is implemented using an object-oriented approach, and directory structures are more flexible. File import of new modalities can be implemented as a relevant subclass of `ImageFile`. - MIRP now uses the `ray` package for parallel processing. # Citation info MIRP has been published in *Journal of Open Source Software*: ```Zwanenburg A, Löck S. MIRP: A Python package for standardised radiomics. J Open Source Softw. 2024;9: 6413. doi:10.21105/joss.06413``` # Contributing If you have ideas for improving MIRP, please read the short [contribution guide](./CONTRIBUTING.md). # Developers and contributors MIRP is developed by: * Alex Zwanenburg We would like thank the following contributors: * Stefan Leger * Sebastian Starke\n",
                "dependencies": "[build-system] requires = [\"setuptools\"] build-backend = \"setuptools.build_meta\" [project] name = \"mirp\" version = \"2.3.2\" description = \"A package for standardised processing of medical imaging and computation of quantitative features.\" authors = [ {name = \"Alex Zwanenburg\", email = \"alexander.zwanenburg@nct-dresden.de\"} ] license = {file = \"LICENSE.txt\"} readme = \"README.md\" classifiers = [ \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"License :: OSI Approved :: European Union Public Licence 1.2 (EUPL 1.2)\", \"Operating System :: OS Independent\", \"Topic :: Scientific/Engineering :: Image Processing\", \"Topic :: Scientific/Engineering :: Medical Science Apps.\" ] requires-python = \">=3.10\" dependencies = [ \"itk>=5.3.0\", \"matplotlib>=3.7.0\", \"numpy>=1.25\", \"pandas>=2.0.0\", \"pydicom>=2.4.0\", \"pywavelets>=1.4.0\", \"scikit-image>=0.20.0\", \"scipy>=1.11\", \"ray>=2.34.0\", \"typing-extensions>=4.10; python_version<'3.11'\" ] [project.urls] Repository = \"https://github.com/oncoray/mirp\" Documentation = \"https://oncoray.github.io/mirp/\" Changelog = \"https://github.com/oncoray/mirp/blob/master/NEWS.md\" [project.optional-dependencies] test = [\"pytest>=7.4.0\"] docs = [\"sphinx>=5.0.0\", \"sphinx_rtd_theme>=2.0.0\", \"nbsphinx\"] [tool.pytest.ini_options] addopts = [\"--strict-markers\"] markers = [ \"ci: marks tests for continuous integration\" ] #filterwarnings = [ # \"error:Arrays of 2-dimensional vectors are deprecated\" #] [tool.setuptools.packages.find] where = [\".\"] include = [\"mirp*\"] exclude = [\"docs*\"] [tool.setuptools.package-data] mirp = [\"config_settings.xml\", \"config_data.xml\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mlair",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/esde/machine-learning/mlair",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mmpxrt",
            "repo_link": "https://codebase.helmholtz.cloud/smid55/mmpxrt",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mode-behave",
            "repo_link": "https://github.com/julianreul/mode_behave",
            "content": {
                "codemeta": "",
                "readme": "Model Purpose and General Information ===================================== MO|DE.behave is a Python-based software package for the estimation and simulation of discrete choice models. The purpose of this software is to enable the rapid quantitative analysis of survey data on choice behavior, utilizing advanced discrete choice methods. Therefore, MO|DE.behave incorporates estimation routines for conventional multinomial logit models, as well as for mixed logit models with nonparametric distributions. Furthermore, MO|DE.behave contains a set of post-processing tools for visualizing estimation and simulation results. Additionally, pre-estimated discrete choice simulation methods for transportation research are included to enrich the software package for this specific community. On mixed logit models: In recent years, a new modeling approach in the field of discrete choice theory became popular - the mixed logit model (see Train, K. (2009): \"Mixed logit\", in Discrete choice methods with simulation (pp. 76-93), Cambridge University Press). Conventional discrete choice models only have a limited capability to describe the heterogeneity of choice preferences within a base population, i.e., the divergent choice behavior of different individuals or consumer groups can only be studied to a limited degree. Mixed logit models overcome this deficiency and allow for the analysis of preference distributions across base populations. Communication and contribution: We encourage active participation in the software development process to adapt it to user needs. If you would like to contribute to the project or report any bugs, please refer to the contribution-file or simply create an issue in the repository. For any other interests (e.g. potential research collaborations), please directly contact the project maintainers via email, as indicated and updated on GitHub. Documentation on GitHub Pages: https://fzj-iek3-vsa.github.io/mode_behave/ Installation ============ 1. Download or clone the repository to a local folder. #. Open (Anaconda) Prompt. #. Create a new environment from reference_environment.yml file (recommended):: conda env create -f reference_environment.yml #. Activate the environment with:: conda activate env_mode_behave #. cd to the directory, where you stored the repository and where the setup.py file is located. #. In this folder run:: pip install -e . #. Alternatively, run:: pip install mode-behave Workflow ======== This section explains an exemplary workflow from model setup to estimation and post-processing for a sub-sample of survey data on household decisions on the type of propulsion technology when purchasing a new car. The propulsion types are differentiated into \"ICEV: Internal combustion engine vehicle\", \"PHEV: Plug-in Hybrid Electric Vehicle\", \"BEV: Battery Electric Vehicle\", and \"FCEV: Fuel Cell Electric Vehicle\". The data was collected in the year 2021 among German households and the respective sub-sample is provided with the model (./mode_behave_public/InputData/example_data.csv). The complete script accessible as well (./mode_behave_public/Deployments/example_estimation.py) 1. Import model and required modules with:: import numpy as np import pandas as pd import mode_behave_public as mb 2. Load data with (PATH_TO_DATA requires individual definition. See below for further documentation on required data formats.):: example_data = pd.read_csv(PATH_TO_DATA + \"example_data.csv\") 3. Definition of model parameters \"PURCHASE_PRICE\", \"RANGE\", and \"CHARGING_FUELING_TIME\". See section \"Structure of Parameters and Input Data\" for further information:: param_fixed = [] param_random = ['PURCHASE_PRICE', 'RANGE', 'CHARGING_FUELING_TIME'] param_temp = {'constant': { 'fixed':[], 'random':[] }, 'variable': { 'fixed': [], 'random':[] } } param_temp['variable']['fixed'] = param_fixed param_temp['variable']['random'] = param_random 4. Initialize a model with:: model = mb.Core( param=param_temp, data_in=example_data, alt=4, equal_alt=1, include_weights=False ) The structure of the input data and the parameter-input are given below. 5. Estimate the model with:: model.estimate_mixed_logit( min_iter=10, max_iter=1000, tol=0.01, space_method = 'std_value', scale_space = 2, max_shares = 1000, bits_64=True, t_stats_out=False ) The estimation of the mixed logit model can be modified by definition of keyword-arguments during instantiation and within the estimation-method itself. Arguments for instantiation (ov.Core(...)):: dict param: Indicates the names of the model attributes. The attribute-names shall be derived from the column names of the input data. str data_name: Indicates the name of the input data-file. int alt: Indicates the number of considered choice alternatives. int equal_alt: Indicates the maximum number of equal choice alternatives per choice set. Keyword-arguments for instantiation (ov.Core(...)):: boolean include_weights: If this is set to True, the model will search for a column in the input-data, called \"weight\", which indicates the weight for each observation. Defaults to True. Keyword-arguments for estimation-method (model.estimate_mixed_logit(...)):: int min_inter: Min. iterations for EM-algorithm. int max_iter: Max. iterations for EM-algorithm. float tol: Numerical tolerance of EM-algorithm. bool bit_64: Defaults to False. If set to True, all numbers are calculated in 64-bit format, which increases precision, but also runtime. str space_method: Defines the chosen method to span the parameter space for the mixed logit estimation. int scale_space: Defines the size of the space, relative to the chosen space_method. int max_shares: Defines the maximum number of points to be observed in the parameter space. 6. Visualize the estimated preferences:: model.visualize_space( k=2, scale_individual=True, cluster_method='kmeans', external_points=np.array([model.initial_point]), bw_adjust=0.03, names_choice_options={0: \"ICEV\", 1: \"PHEV\", 2: \"BEV\", 3: \"FCEV\"} ) Keyword-arguments:: int k: Number of preference clusters to be analyzed. boolean scale_individual: Scales the visualized preferences to fit the bounds (-1, 1), to ease the comparability of preferences between different model attributes. str cluster_method: Defines the clustering algorithm for the identification of diverging preference groups. array external_points: An array of preferences to be visualized in the figure as a reference point. In this case, the mean preferences of the base population are visualized with \"model.initial_point\" float bw_adjust: Smoothing parameter for the displayed preference distribution. dict names_choice_options: This dictionary can be used to define the names of the choice options. 7. Simulate the choice probabilities for each choice options in diverging scenarios (more exemplary use cases of this method can be found in the script example_estimation.py):: model.forecast(method='MNL', sense_scenarios={\"Cheap_EV\": { \"PURCHASE_PRICE\": [[1.1], [1.1], [0.5], [0.5]]} }, names_choice_options={0: \"ICEV\", 1: \"PHEV\", 2: \"BEV\", 3: \"FCEV\"}, name_scenario='sensitivity' ) Keyword-arguments:: str method: Defines the type of choice model to be used among \"MNL\" (Multinomial logit), \"LC\" (Latent class), and \"MXL\" (Mixed logit) dict sense_scenarios: Can be used to define diverging scenarios from the base scenario, which is defined by the mean values in the base data. The values indicate scaling factors by which the attributes are changed. E.g., a value of 1.1 for the attribute \"PURCHASE_PRICE\" indicates a 10% increase in the purchase price for the respective choice option. dict names_choice_options: This dictionary can be used to define the names of the choice options. str name_scenario: This string can be defined to declare the scenario name. It is used to store the generated visualization under this name in the output folder \"./mode_behave_public/Visualizations/\" Testing ======= The software includes testing routines, written with the package *unittest*, to ensure its functionality throughout the development process. The first test-routine checks the functionality of the estimation routines (PATH: *./test/test_estimation.py*), while the second test routine checks the functionality of simulation routines (PATH: *./test/test_simulation.py*) These testing routines can be activated in two ways: 1. Via GitHub Actions: Whenever a new commit is pushed to the repository, GitHub Actions are automatically triggered, which execute the test routines. The test results are displayed in the GitHub Actions tab in the software's repository online. 2. Via manual execution: Alternatively, the test routines can be called manually. You might chose this option, if you develop the software locally and want to validate your changes before pushing a new commit. To execute the existing test routines manually, open the (Anaconda) prompt and enter these commands:: cd \"PATH_TO_MODULE/test/\" python -m unittest test_estimation.py python -m unittest test_simulation.py These commands execute the two test routines for estimation and simulation. Substitute *PATH_TO_MODULE* with the path to the repository's home directory on your local machine. If new features are added to the software, there should also be new test routines added, which check their sustained functionality thoughout the development process (test-driven development). Structure of Parameters and Input Data ====================================== 1. Input data The input dataset contains the observations with which the model is calibrated. The input data is called with the specified string of the keyword-argument *data_in*. The input data must be loaded from .csv- or .pickle-format before model initialization. The data shall follow the structure below:: Rows: Observations. Columns: One column per parameter of the utility function AND per alternative AND per equal alternative. Specified as: **'Attribute_name_' + str(no_alternative) + str(no_equal_alternative)** One column for the choice-indication of EACH alternative AND per equal alternative. Specified as: **choice_' + str(no_alternative) + str(no_equal_alternative)** One column per alternative AND per equal alternative, indicating the availability. Specified as: **'av_' + str(no_alternative) + str(no_equal_alternative)** If a parameter is constant across alternatives or equal alternatives, then let the columns be equal. Furthermore, the observations can be given a weight. Therefore, an additional column needs to be provided, named 'weight'. - Without any further suffix. Index: The index shall start from '0'. 2. Initialization argument 'param': 'param' is specified as a dictionary containing the attribute names of the utility function, sorted by type:: param['constant']['fixed']: Attributes, which are constant over choice options and fixed within the parameter space. param['constant']['random']: Attributes, which are constant over choice options and randomly distributed over the parameter space. param['variable']['fixed']: Attributes, which vary over choice options and are fixed within the parameter space. param['variable']['random']: Attributes, which vary over choice options and are randomly distributed over the parameter space. 3. The vector x, containing the initial estimates for the logit coefficients. The coefficients in vector x (solution vector of maximum likelihood optimization) follow a certain structure (alternatives=alt):: x[:(alt-1)]: ASC-constants for the alternatives 1-#of alternatives. ASC for choice option 0 defaults to 0. x[(alt-1):(alt-1)+no_constant_fixed]: Coefficients of constant and fixed attributes. x[(alt-1)+no_constant_fixed:(alt-1)+(no_constant_fixed+no_constant_random)]: Coefficients of constant and fixed attributes. x[(alt-1)+(no_constant_fixed+no_constant_random):(alt-1)+(no_constant_fixed+no_constant_random)+no_variable_fixed*alt]: Coefficients of variable (thus multiplication with alternatives) and fixed attributes. x[(alt-1)+(no_constant_fixed+no_constant_random)+no_variable_fixed*alt:(alt-1)+(no_constant_fixed+no_constant_random)+(no_variable_fixed+no_variable_random)*alt]: Coefficients of variable and random attributes. Theoretical Background ====================== A mixed logit model is a multinomial logit model (MNL), in which the coefficients do not take a single value, but are distributed over a parameter space. Within this package, the mixed logit models are estimated on a discrete parameter space, which is specified by the researcher (nonparametric design). The discrete subsets of the parameter space are called classes, analogously to latent class models (LCM). The goal of the estimation procedure is to estimate the optimal share, i.e. weight, of each class within the discrete parameter space. The algorithm roughly follows the procedure below: 1. Estimate initial coefficients of a standard multinomial logit model. 2. Specify a continuous parameter space for the random coefficients with the mean and the standard deviation of each initially calculated random coefficient. (The standard deviation can be calculated from a k-fold cross-validation.) Alternatively, the parameter space can be defined via the absolute values of the parameters. 3. Draw points (maximum number of point = -max_shares-) from the parameter space via latin hypercube sampling. 3. Estimate the optimal share for each drawn point with an expectation-maximization (EM) algorithm. (see Train, 2009) Further reading: * Train, K. (2009): \"Mixed logit\", in Discrete choice methods with simulation (pp. 76-93), Cambridge University Press * Train, K. (2008): \"EM algorithms for nonparametric estimation of mixing distributions\", in Journal of Choice Modelling, 1(1), 40-69, https://doi.org/10.1016/S1755-5345(13)70022-8 * Train, K. (2016): \"Mixed logit with a flexible mixing distribution\", in Journal of Choice Modelling, 19, 40-53, https://doi.org/10.1016/j.jocm.2016.07.004 * McFadden, D. and Train, K. (2000): \"Mixed MNL models for discrete response\", in Journal of Applied Econometrics, 15(5), 447-470, https://www.jstor.org/stable/2678603 Post-Analysis ============= 1. Access of estimated coefficients and summary statistics:: model.shares: Estimated shares of discrete classes within parameter space. model.points: Parameter space of random coefficients. model.initial_point: Coefficients of initially estimated logit model. 2. Visualization of parameter space:: model.visualize_space(**kwargs) int k: k incidates the number of cluster centers, to which the estimated random parameters of the mixed logit model shall be attributed. The cluster centers indicate different potential choice or consumer groups. This method clusters the estimated random preferences and shows the position of the cluster centers as well as the overall distribution of estimated random parameters across the whole parameter space. 3. Forecast with cluster centers:: model.forecast(method, **kwargs) str method: \"method\" indicates the type of the discrete choice model (\"MNL\", \"MXL\", or \"LC\" for latent class). int k: Also \"k\" can be given to indicate the number of cluster centers which shall be analyzed. dict sense_scenarios: Indicates the relative change in the value of selected model attributes. This keyword is useful for conducting sensitivity analyses. list av_external: This parameter is used to externally define the availabilities of certain choice options. E.g., if a choice option shall be excluded from the simulation. This method forecasts the mean choice, based on the estimated parameters of each cluster center and the attribute values of the base data. It is a good reference point to study the diverging choice behavior of each cluster center. 4. Cluster the drawn points from the parameter space to similar preference groups (e.g. consumer groups):: model.cluster_space(method, k, **kwargs) str method: Indicates the clustering algorithm, e.g. kmeans. int k: Indicates the number of cluster centers. The output of this method is the classification of the drawn points from the parameter space into clusters. The second output are the calculated cluster centers. The clusters can be interpreted as consumer groups. 5. Assignment of observations to cluster centers:: model.assign_to_cluster(**kwargs) This method calculates probabilities for each observation in the base data, which indicate the likelihood with which an observation belongs to a cluster center (the method internally calls self.cluster_space to determine the cluster centers). This method is useful to characterize the consumer groups. Simulation ========== The model incorporates a class **Simulation**, which contains customized methods to simulate previously estimated choice models. In order to simulate choice probabilities, the model must be instantiated as follows:: model = mb.Core(model_type = 'simulation', simulation_type = 'mode_choice') str simulation_type: Specifies which kind of simulation shall be conducted. Currently only MNL-simulations are implemented. The following MNL-simulations are currently available: **MNL-Model for Mode-Choice (simulation_type = 'mode_choice')**:: model.simulate_mode_choice(agegroup, occupation, regiontype, distance, av) The method simulates the probability of mode choice for ten different modes (Walking, Biking, MIV-self, MIV-co, bus_near, train_near, train_city, bus_far, train_far, carsharing). Input parameters are the agegroup of the simulated agent (1: <18, 2: 18-65, 3: >65), the occupation (1: full-time work, 2: part-time, 3: education, 4: no occupation), the regiontype of residence (according to RegioStaR7 - BMVI classification), distance (travel cost and time are derived from this variable, based on cost-assumptions for the year 2020. Also, the regiontype for the calculation of average speeds is assumed to be identical with the specified regiontype of the home location of the agent), as well as the availability of each mode in numpy-array format. Filename of pre-estimated model parameters: 'initial_point_mode' **MNL-model for the probability of the number of cars per households (simulation_type = 'car_ownership')**:: model.simulate_hh_cars(regiontype, hh_size, adults_working, children, htype, quali_opnv, sharing, relative_cost_per_car, age_adults) The method simulates the probability, that a household owns 0-3+ cars (4 discrete alternatives). Input parameters are the regiontype of residence in I/O-format according to RegioStaR7 BMVI classification (e.g.: regiontype = 1 for \"Metropolis\"), the household size (hh_size), the number of working adults (adults_working), the number of children in the household (children), the housing type (htype) in I/O-format (e.g.: 1, if individual house, 0, if multi-apartment house), the quality of public transport in the residence area (1: Very Bad, 2: Bad, 3: Good, 4: Very Good), whether the household holds a carsharing-membership (sharing), the ratio of the average car price divided by net monthly household income (relative_cost_per_car). Average market prices can be derived from Kraus' vehicle cost model. Last input parameter is the average age of the adults, living in the household, scaled by *0.1!\n",
                "dependencies": "Pillow>=9.5.0 matplotlib>=3.7.1 numba>=0.56.4 numpy>=1.23.5 pandas>=2.0.1 pickleshare>=0.7.5 scikit-learn>=1.2.2 scikit-optimize>=0.9.0 scipy>=1.10.1 seaborn>=0.12.2\nfrom setuptools import setup setup(name='mode_behave', version='1.1.2', description='Estimation and simulation of discrete choice models', author='Julian Reul', author_email='j.reul@fz-juelich.de', url='https://github.com/julianreul/mode_behave', license='MIT', packages=['mode_behave_public'], install_requires=[ 'Pillow>=9.5.0', 'matplotlib>=3.7.1', 'numba>=0.56.4', 'numpy>=1.23.5', 'pandas>=2.0.1', 'pickleshare>=0.7.5', 'scikit-learn>=1.2.2', 'scikit-optimize>=0.9.0', 'scipy>=1.10.1', 'seaborn>=0.12.2' ], include_package_data=True, zip_safe=False)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/moovie",
            "repo_link": "https://jugit.fz-juelich.de/IBG-1/ModSim/MooViE",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mptrac",
            "repo_link": "https://github.com/slcs-jsc/mptrac",
            "content": {
                "codemeta": "",
                "readme": "# Massive-Parallel Trajectory Calculations Massive-Parallel Trajectory Calculations (MPTRAC) is a Lagrangian particle dispersion model for the analysis of atmospheric transport processes in the free troposphere and stratosphere. ![logo](https://github.com/slcs-jsc/mptrac/blob/master/docs/logo/MPTRAC_320px.png) [![release (latest by date)](https://img.shields.io/github/v/release/slcs-jsc/mptrac)](https://github.com/slcs-jsc/mptrac/releases) [![commits since latest release (by SemVer)](https://img.shields.io/github/commits-since/slcs-jsc/mptrac/latest)](https://github.com/slcs-jsc/mptrac/commits/master) [![last commit](https://img.shields.io/github/last-commit/slcs-jsc/mptrac.svg)](https://github.com/slcs-jsc/mptrac/commits/master) [![top language](https://img.shields.io/github/languages/top/slcs-jsc/mptrac.svg)](https://github.com/slcs-jsc/mptrac/tree/master/src) [![code size](https://img.shields.io/github/languages/code-size/slcs-jsc/mptrac.svg)](https://github.com/slcs-jsc/mptrac/tree/master/src) [![repo size](https://img.shields.io/github/repo-size/slcs-jsc/mptrac.svg)](https://github.com/slcs-jsc/mptrac/tree/master/src) [![codacy](https://api.codacy.com/project/badge/Grade/a9de7b2239f843b884d2a4eb583726c9)](https://app.codacy.com/gh/slcs-jsc/mptrac?utm_source=github.com&utm_medium=referral&utm_content=slcs-jsc/mptrac&utm_campaign=Badge_Grade_Settings) [![codecov](https://codecov.io/gh/slcs-jsc/mptrac/branch/master/graph/badge.svg?token=4X6IEHWUBJ)](https://codecov.io/gh/slcs-jsc/mptrac) [![tests](https://img.shields.io/github/actions/workflow/status/slcs-jsc/mptrac/tests.yml?branch=master&label=tests)](https://github.com/slcs-jsc/mptrac/actions) [![docs](https://img.shields.io/github/actions/workflow/status/slcs-jsc/mptrac/docs.yml?branch=master&label=docs)](https://slcs-jsc.github.io/mptrac) [![status](https://joss.theoj.org/papers/dc49cd7be52f2a47c41cc20143f3719a/status.svg)](https://joss.theoj.org/papers/dc49cd7be52f2a47c41cc20143f3719a) [![license](https://img.shields.io/github/license/slcs-jsc/mptrac.svg)](https://github.com/slcs-jsc/mptrac/blob/master/COPYING) [![doi](https://zenodo.org/badge/DOI/10.5281/zenodo.4400597.svg)](https://doi.org/10.5281/zenodo.4400597) ## Introduction Massive-Parallel Trajectory Calculations (MPTRAC) is a high-performance Lagrangian particle dispersion model designed to simulate and analyze atmospheric transport processes in the free troposphere and stratosphere. MPTRAC leverages massive parallelization to efficiently handle large-scale trajectory calculations, making it a helpful tool for researchers and scientists studying atmospheric dynamics, pollutant dispersion, and stratospheric transport events. The model's primary focus is on accurately tracking the movement of air parcels and particles over extended periods of time and large spatial domains. It accounts for complex atmospheric processes, including advection, turbulent diffusion, and various meteorological influences, by integrating high-resolution meteorological data. MPTRAC is used in applications such as: - Tracking long-range transport of pollutants, including aerosol particles and trace gases. - Volcanic ash dispersion modeling to predict ash cloud movement and impact. - Stratospheric tracer experiments to investigate stratosphere-troposphere exchange processes. - Atmospheric chemistry studies, focusing on the distribution and transformation of trace gases. By offering scalability and accuracy, MPTRAC empowers atmospheric scientists and environmental researchers to gain deeper insights into complex transport phenomena, enhancing our understanding of atmospheric dynamics and improving prediction capabilities. ## Features MPTRAC is a powerful tool for atmospheric trajectory calculations, offering a wide range of features to enhance accuracy, performance, and usability: - **Advanced Trajectory Calculations**: MPTRAC calculates air parcel trajectories by solving the kinematic equation of motion using horizontal wind and vertical velocity fields from global reanalyses or forecast datasets, enabling precise tracking of atmospheric transport processes in the free troposphere and stratosphere. - **Stochastic Perturbation and Mixing**: Mesoscale diffusion and subgrid-scale wind fluctuations are simulated using the Langevin equation, introducing stochastic perturbations to trajectories. An inter-parcel exchange module represents mixing of air between neighboring particles, capturing realistic atmospheric dispersion. - **Comprehensive Process Modeling**: MPTRAC includes modules to simulate convection, sedimentation, exponential decay, gas and aqueous phase chemistry, and wet and dry deposition, allowing for accurate modeling of physical and chemical transformations. - **Meteorological Data Pre-Processing**: The model pre-processes meteorological data to estimate variables such as boundary layer height, convective available potential energy (CAPE), geopotential heights, potential vorticity, and tropopause data, ensuring seamless integration with diverse datasets. - **Flexible Output and Visualization**: MPTRAC supports various output formats for particle trajectories, gridded fields, ensemble statistics, vertical profiles, point samples, and station data. Visualization interfaces with Gnuplot and ParaView make it easy to analyze complex data. - **High-Performance Computing**: The model employs hybrid parallelization using MPI, OpenMP, and OpenACC, allowing efficient utilization of resources from single workstations to HPC clusters and GPU systems. - **Open Source and Community Driven**: MPTRAC is distributed as open-source software under the GNU General Public License (GPL), promoting collaborative development and ensuring transparency. ## Getting started ### Prerequisites To build and run MPTRAC, you will need some basic tools and libraries, including Git, GNU Make, GCC, GSL, HDF5, and netCDF. For additional features such as HPC and GPU support, optional dependencies like OpenMPI and the NVIDIA HPC SDK are required. Some of the required dependencies are included with the MPTRAC repository. See the next section for more details. For a complete list of dependencies, including specific versions and installation instructions, refer to the [dependencies file](https://github.com/slcs-jsc/mptrac/blob/master/DEPENDENCIES.md). ### Installation To install MPTRAC, follow these steps: **1. Download MPTRAC** Get the latest or a previous version from the [MPTRAC releases](https://github.com/slcs-jsc/mptrac/releases) page. After downloading, extract the release file: unzip mptrac-x.y.zip Alternatively, to get the latest development version, clone the GitHub repository: git clone https://github.com/slcs-jsc/mptrac.git **2. Install required libraries** The MPTRAC git repository includes several libraries that can be compiled and installed using a build script: cd [mptrac_directory]/libs ./build.sh -a Alternatively, if you prefer to use existing system libraries, install the dependencies manually. **3. Configure the Makefile** Navigate to the source directory and adjust the `Makefile` as needed: cd [mptrac_directory]/src emacs Makefile Pay special attention to the following settings: * Edit the `LIBDIR` and `INCDIR` paths to point to the directories where the GSL, netCDF, and other required libraries are located on your system. * By default, the MPTRAC binaries are linked dynamically. Ensure that the `LD_LIBRARY_PATH` is properly configured to include the paths to the shared libraries. If you prefer static linking, you can enable it by setting the `STATIC` flag, which allows you to copy and use the binaries on other machines. However, in some cases, either static or dynamic linking may not be feasible or could cause specific issues. * To enable MPI parallelization in MPTRAC, you must set the `MPI` flag. Additionally, an MPI library, such as OpenMPI, must be installed on your system. To utilize OpenACC parallelization, enable the `GPU` flag, and ensure the NVIDIA HPC SDK is installed to compile the GPU code. OpenMP parallelization is always enabled. * Some options in the Makefile are labeled as experimental. These features are still under development and may not be fully functional or tested. Use them at your own risk. **4. Compile and test the installation** Once the Makefile is configured, compile the code using: make [-j] To verify the installation, run the test suite: make check This will execute a series of tests sequentially. If any test fails, check the log messages for further details. ### Running an example simulation To demonstrate how MPTRAC can simulate the dispersion of volcanic ash, we provide an example based on the eruption of the Puyehue-Cordón Caulle volcano in Chile in June 2011. This example illustrates the setup and execution of a typical simulation. The example is located in the `projects/example/` subdirectory. You can also use the `projects/` folder to store results from your own simulation experiments with MPTRAC. Follow these steps to run the example simulation: 1. Navigate to the example directory: ``` cd [mptrac_directory]/projects/example ``` 2. Execute the `run.sh` script to start the simulation: ``` ./run.sh ``` The `run.sh` script outlines the necessary steps to invoke MPTRAC programs such as `atm_init` (for initializing trajectory seeds), `atm_split` (for splitting particle groups), and `trac` (for computing the trajectories). This script automates the simulation process, ensuring a smooth workflow. The output of the simulation will be stored in the `projects/example/data` subdirectory. For comparison, reference data is available in the `projects/example/data.ref` folder. Additionally, the simulation generates several plots at different time steps, which are stored in `projects/example/plots`. These plots show the results of the volcanic plume dispersion over time and can be visualized using the gnuplot plotting tool. For example, particle positions and grid outputs at different time steps (June 6th and June 8th, 2011) are shown below: <p align=\"center\"><img src=\"projects/example/plots.ref/atm_2011_06_06_00_00.tab.png\" width=\"45%\"/> &emsp; <img src=\"projects/example/plots.ref/grid_2011_06_06_00_00.tab.png\" width=\"45%\"/></p> <p align=\"center\"><img src=\"projects/example/plots.ref/atm_2011_06_08_00_00.tab.png\" width=\"45%\"/> &emsp; <img src=\"projects/example/plots.ref/grid_2011_06_08_00_00.tab.png\" width=\"45%\"/></p> ### Additional project subdirectories In addition to the example simulation, MPTRAC provides several utility scripts and resources in the `projects/` directory to support your work: - `projects/meteo/`: Contains scripts for downloading meteorological input data from various data centers (e.g., ECMWF, NOAA), which are required for trajectory simulations. - `projects/python/`: Includes Python scripts to read, analyze, and visualize MPTRAC output data, such as air parcel trajectories and gridded fields. These can be helpful for creating custom plots and diagnostics. - `projects/paraview/`: Provides examples and guidelines for using ParaView to visualize MPTRAC air parcel data in an interactive 3D environment. These directories offer helpful tools and examples for extending your use of MPTRAC beyond the basic workflow and adapting it to your specific research needs. ## Further information To learn more about MPTRAC and its scientific background, please refer to the following key publications: * Hoffmann, L., Baumeister, P. F., Cai, Z., Clemens, J., Griessbach, S., Günther, G., Heng, Y., Liu, M., Haghighi Mood, K., Stein, O., Thomas, N., Vogel, B., Wu, X., and Zou, L.: Massive-Parallel Trajectory Calculations version 2.2 (MPTRAC-2.2): Lagrangian transport simulations on graphics processing units (GPUs), Geosci. Model Dev., 15, 2731-2762, https://doi.org/10.5194/gmd-15-2731-2022, 2022. * Hoffmann, L., T. Rößler, S. Griessbach, Y. Heng, and O. Stein, Lagrangian transport simulations of volcanic sulfur dioxide emissions: Impact of meteorological data products, J. Geophys. Res. Atmos., 121, 4651-4673, https://doi.org/10.1002/2015JD023749, 2016. For a complete list of related publications and references, please visit the [references page](https://slcs-jsc.github.io/mptrac/references/). The [user manual](https://slcs-jsc.github.io/mptrac) provides detailed instructions on how to install, configure, and run MPTRAC, along with example workflows and practical tips. Developer-oriented documentation, including code structure and API references, can be found in the [Doxygen manual](https://slcs-jsc.github.io/mptrac/doxygen). For additional information, please also refer to the [MPTRAC Wiki](https://github.com/slcs-jsc/mptrac/wiki). ## Contributing We welcome contributions from the community to help improve and expand MPTRAC, supporting both operational and research applications. If you encounter any issues, bugs, or have suggestions for new features, please let us know by submitting a report on the [issue tracker](https://github.com/slcs-jsc/mptrac/issues). Your feedback helps us make MPTRAC better for everyone. If you would like to contribute code, fix bugs, or enhance existing features, feel free to submit a [pull request](https://github.com/slcs-jsc/mptrac/pulls). We appreciate well-documented and tested contributions. Before getting started, please take a moment to read the [contributing guidelines](https://github.com/slcs-jsc/mptrac/blob/master/CONTRIBUTING.md). They include helpful information on setting up your development environment and best practices for submitting contributions. If you have any questions or need assistance during the process, don't hesitate to reach out. We are happy to help and collaborate! ## License MPTRAC is developed at the Jülich Supercomputing Centre, Forschungszentrum Jülich, Germany. The project is made available as open-source software to promote transparency, collaboration, and innovation in atmospheric transport modeling. MPTRAC is distributed under the terms of the [GNU General Public License v3.0](https://github.com/slcs-jsc/mptrac/blob/master/COPYING). This license ensures that the software remains free and open, while also allowing for modifications and redistribution under the same terms. If you use MPTRAC in scientific publications or research work, please make sure to cite it appropriately. Detailed citation information can be found in the [citation file](https://github.com/slcs-jsc/mptrac/blob/master/CITATION.cff). Thank you for supporting open science and helping to advance MPTRAC! ## Contact For inquiries, support, or collaboration opportunities, please reach out to: Dr. Lars Hoffmann Jülich Supercomputing Centre, Forschungszentrum Jülich, Germany e-mail: l.hoffmann@fz-juelich.de Feel free to get in touch if you have questions about MPTRAC, need assistance, or are interested in contributing to the project.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mss",
            "repo_link": "https://github.com/Open-MSS/MSS",
            "content": {
                "codemeta": "",
                "readme": "**Chat:** [![IRC: #mss-general on libera.chat](https://img.shields.io/badge/libera.chat-%23MSS_General-blue)](https://web.libera.chat/?channels=#mss-general) [![IRC: #mss-gsoc on libera.chat](https://img.shields.io/badge/libera.chat-%23MSS_GSoC-brightgreen)](https://web.libera.chat/?channels=#mss-gsoc) Mission Support System Usage Guidelines ======================================= Welcome to the Mission Support System software for planning atmospheric research flights. This document is intended to point you into the right direction in order to get the software working on your computer. ## Installing MSS We distinguish between Developer and User installations. ### Developer Installation Please read our [contributing](https://open-mss.github.io/contributing/) pages. and [development](https://mss.readthedocs.io/en/stable/development.html) guidelines ### User Installation Get **pixi** from https://pixi.sh/latest/ for your operation system. You can now decide if you want to install **mss** as global or a project. #### Global installation You can install **mss** global without defining a project first. This method is practical when you are interested in starting the client and don't need server configurations. pixi global install mss #### Global Usage msui mswms -h mscolab -h mssautoplot -h ##### Global Updating Updates environments in the global environment. This makes a release upgrade. pixi global update mss #### Project installation Initialize a new project and navigate to the project directory. pixi init MSS cd MSS Use the shell command to activate the environment and start a new shell in there. pixi shell Add the **mss** dependencies from conda-forge. (MSS) pixi add mss ##### Project Usage Always when you want to start **mss** programs you have after its installation to activate the environment by pixi shell in the project dir. On the very first start of **msui** it takes a bit longer because it setups fonts. cd MSS pixi shell (MSS) msui (MSS) mswms -h (MSS) mscolab -h (MSS) mssautoplot -h ##### Project Updating Update dependencies as recorded in the local lock file. cd MSS pixi shell (MSS) pixi update mss ##### Project Upgrading Update the version of packages to the latest possible version, disregarding the manifest version constraints This makes a release upgrade. cd MSS pixi shell (MSS) pixi update mss Current release info ==================== [![Conda Version](https://img.shields.io/conda/vn/conda-forge/mss.svg)](https://anaconda.org/conda-forge/mss) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6572620.svg)](https://doi.org/10.5281/zenodo.6572620) [![JuRSE Code Pick](https://img.shields.io/badge/JuRSE_Code_Pick-July_2024-blue)](https://www.fz-juelich.de/en/rse/jurse-community/jurse-code-of-the-month/july-2024) [![Conda Platforms](https://img.shields.io/conda/pn/conda-forge/mss.svg)](https://anaconda.org/conda-forge/mss) [![DOCS](https://img.shields.io/badge/%F0%9F%95%AE-docs-green.svg)](https://mss.rtfd.io) [![Conda Recipe](https://img.shields.io/badge/recipe-mss-green.svg)](https://anaconda.org/conda-forge/mss) [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/mss.svg)](https://anaconda.org/conda-forge/mss) [![Coverage Status](https://coveralls.io/repos/github/Open-MSS/MSS/badge.svg?branch=develop)](https://coveralls.io/github/Open-MSS/MSS?branch=develop) Publications ============ Please read the reference documentation Bauer, R., Grooß, J.-U., Ungermann, J., Bär, M., Geldenhuys, M., and Hoffmann, L.: The Mission Support System (MSS v7.0.4) and its use in planning for the SouthTRAC aircraft campaign, Geosci. Model Dev., 15, 8983-8997, https://doi.org/10.5194/gmd-15-8983-2022, 2022. Rautenhaus, M., Bauer, G., and Doernbrack, A.: A web service based tool to plan atmospheric research flights, Geosci. Model Dev., 5, 55-71, https://doi.org/10.5194/gmd-5-55-2012, 2012. and the paper's Supplement (which includes a tutorial) before using the application. The documents are available at: - http://www.geosci-model-dev.net/5/55/2012/gmd-5-55-2012.pdf - http://www.geosci-model-dev.net/5/55/2012/gmd-5-55-2012-supplement.pdf For copyright information, please see the files NOTICE and LICENSE, located in the same directory as this README file. When using this software, please be so kind and acknowledge its use by citing the above mentioned reference documentation in publications, presentations, reports, etc. that you create. Thank you very much. Acknowledgements ---------------- We are very grateful for your continuing support for MSS! See our [Contributors page](https://mss.readthedocs.io/en/stable/authors.html) for a list of authors. See also our info on [funding]( https://mss.readthedocs.io/en/stable/funding.html).\n",
                "dependencies": "[build-system] requires = [\"setuptools >= 40.8.0\", \"future\"] build-backend = \"setuptools.build_meta:__legacy__\" [tool.codespell] exclude-file = \"codespell-ignored-lines.txt\" ignore-words-list = [ \"PRES\", \"degreeE\", \"doubleClick\", \"indexIn\", \"socio-economic\", \"EMAC\", \"emac\" ]\n# -*- coding: utf-8 -*- \"\"\" mslib.setup ~~~~~~~~~~~ setuptools script This file is part of MSS. :copyright: Copyright 2016-2017 Reimar Bauer :copyright: Copyright 2016-2025 by the MSS team, see AUTHORS. :license: APACHE-2.0, see LICENSE for details. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. \"\"\" # The README.txt file should be written in reST so that PyPI can use # it to generate your project's PyPI page. import os from past.builtins import execfile from setuptools import setup, find_namespace_packages long_description = open('README.md').read() execfile('mslib/version.py') console_scripts = [ \"mscolab = mslib.mscolab.mscolab:main\", \"mss = mslib.msui.mss:main\", \"mssautoplot = mslib.utils.mssautoplot:main\", \"msui = mslib.msui.msui:main\", \"mswms = mslib.mswms.mswms:main\", \"mswms_demodata = mslib.mswms.demodata:main\"] if os.name != 'nt': console_scripts.append('msidp = mslib.msidp.idp:main') setup( name=\"open-mss\", version=__version__, # noqa description=\"MSS - Mission Support System\", long_description=long_description, classifiers=[\"Development Status :: 5 - Production/Stable\",], keywords=\"mslib\", maintainer=\"Reimar Bauer\", maintainer_email=\"rb.proj@gmail.com\", author=\"Marc Rautenhaus\", author_email=\"wxmetvis@posteo.de\", license=\"Apache-2.0\", url=\"https://github.com/Open-MSS/MSS\", platforms=\"any\", packages=find_namespace_packages(include=[\"mslib\", \"mslib.*\"]), namespace_packages=[], include_package_data=True, zip_safe=False, install_requires=[], # we use pixi entry_points=dict( console_scripts=console_scripts, ), )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/mtress",
            "repo_link": "https://github.com/mtress/mtress",
            "content": {
                "codemeta": "",
                "readme": "# Model Template for Renewable Energy Supply Systems (MTRESS) ## Introduction The DLR Institute of Networked Energy Systems has developed the MTRESS tool that can be used to optimise energy supply systems for new and existing projects at any location. MTRESS facilitates the creation of energy system optimisation models for individual residential and commercial buildings, as well as for neighbourhoods and entire industrial properties. It enables users to include a wide range of influencing factors and energy options in the simulation and minimises the planning effort. This is a generic model for community-based open source [oemof.solph](https://github.com/oemof/oemof-solph/) tool. MTRESS offers a variety of possible technology combinations for energy supply systems. It includes pre-built technologies that are commonly considered in energy systems, such as: - Photovoltaic (PV) and/or Renewable Energy Source - Grid Connection (Electricity, Heat and Gas) - Heat Pumps - Heat Exchangers - Electrolysers (PEM, Alkaline, AEM) - Fuel Cells (PEM, Alkaline, AEM) - Compressors - Combined Heat and Power (CHP) with various gas types and gas-mixtures (e.g., H2 + Natural Gas) as input fuels - Storages (Battery, Heat Storage, Gas Storage (Hydrogen)) - Resistive Heater and Gas Boiler It covers different sectors including Electricity, Heat and Gas (e.g., H2, Natural Gas, Biogas, etc.). It is tailored for optimising control strategies fulfilling fixed demand time series for electricity, heat, gas (including hydrogen), and domestic hot water using any selected combination of the implemented supply technologies. MTRESS requires appropriately prepared initial data on the boundary conditions of the respective project. A wide range of data sources can be used, including historical energy consumption data for the project, but also higher-level data on the location, for example from climate models or the solar cadastre. Moreover, the forecasted demands and renewable generations could also be used for scheduling optimized operation for next days. It could be used for long-term planning and the assumptions about the development of costs and the CO2 impact of the future energy mix can also be incorporated into the modelling. As an open-source model, MTRESS is available to users in a wide range of areas. It can be utilized for both research and commercial purposes. Researchers, utility owners, and policymakers can all benefit from this tool for energy system planning and operation. Applications are not just limited to municipal heating plans, home automation offerings, hydrogen infrastructure planning, and optimized operation of sector-coupled energy systems, but extend to any scenario requiring comprehensive energy optimization and management. ## Installation MTRESS depends on solph, which is automatically installed using pip ```bash pip install mtress ``` However, pip will not install a solver, to perform the actual optimisation. Please refer to the [documentation of solph](https://oemof-solph.readthedocs.io/en/stable/readme.html#installing-a-solver) to learn how to install a solver. ## Documentation The auto-generated documentation can be found on the [GitLab pages](https://mtress-ecosystem.pages.gitlab.dlr.de/mtress/api). <br> The coverage report can be found on the [GitLab pages](https://mtress-ecosystem.pages.gitlab.dlr.de/mtress/coverage) as well. ## Usage and Tutorials Please refer to the examples folder in this repository to get acquainted with building and optimizing energy systems in MTRESS. These examples will help you understand the basics and guide you through the process before you start creating your own energy system. Please feel free to contact us if you have any questions or need further assistance. Contact information can be found below. ## Acknowledgements The development of Version 2 was funded by the Federal Ministry for Economic Affairs and Energy (BMWi) and the Federal Ministry of Education and Research (BMBF) of Germany in the project [ENaQ](https://www.enaq-fliegerhorst.de/) (project number 03SBE111). The development of the heat sector formulations in Version 3 was funded by the Federal Ministry of Education and Research (BMBF) of Germany in the project [Wärmewende Nordwest](https://www.waermewende-nordwest.de/) (project number 03SF0624). The development of the gas sector formulations in Version 3 was funded by the Federal Ministry of Education and Research (BMBF) of Germany in the project [H2Giga-Systogen100](https://www.region-heide.de/projekte/systogen100.html) (project number 03HY115E). ## Contributing You are welcome to contribute to MTRESS. We use [Black code style](https://black.readthedocs.io/) with 79 characters a line, and put our code under [MIT license](LICENSE). When contributing, you need to do the same. For smaller changes, you can just open a merge request. If you plan something bigger, please open an issue first, so that we can discuss beforehand and avoid double work. Also, please report bugs by opening an issue. ## Citation If you use MTRESS Software for your research, please consider citation as follows using the [Zenodo record, doi:10.5281/zenodo.6395909](https://zenodo.org/doi/10.5281/zenodo.6395909). ## Reference <a id=\"1\">[1]</a> Schönfeldt, Patrik and Schlüters, Sunke and Oltmanns, Keno,\"MTRESS 3.0--Modell Template for Residential Energy Supply Systems\", arXiv preprint, 2022, [arXiv:2211.14080](https://arxiv.org/abs/2211.14080). ## Contact The software development is administrated by [Patrik Schönfeldt](mailto:patrik.schoenfeldt@dlr.de), for general questions please contact him. Individual authors may leave their contact information in the [citation.cff](CITATION.cff).\n",
                "dependencies": "[build-system] requires = [\"flit_core >=3.2,<4\"] build-backend = \"flit_core.buildapi\" [tool.flit.sdist] include = [ \"CITATION.cff\", \"LICENSE\", \"README.md\", \"doc/\", \"examples/\", \"mtress/\", \"tests/\", ] [project] name = \"mtress\" dynamic = [\"version\"] description = \"Energy System Model Template\" readme = \"README.md\" authors = [ {name = \"Deutsches Zentrum für Luft- und Raumfahrt e.V. (DLR)\", email = \"patrik.schoenfeldt@dlr.de\"}, ] classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: MIT License\", \"Operating System :: Unix\", \"Operating System :: POSIX\", \"Operating System :: Microsoft :: Windows\", \"Operating System :: MacOS\", \"Operating System :: OS Independent\", \"Programming Language :: Python\", \"Programming Language :: Python :: 3\", \"Topic :: Utilities\", ] requires-python = \">=3.10\" dependencies = [ \"graphviz\", \"imageio\", \"oemof.solph>=0.5.4\", \"oemof.thermal>=0.0.6.dev1\", \"pandas>=1.3.4\", \"PyYAML>=6.0\", \"numpy>=2.2.0\", \"setuptools>=59.1.1\", \"pvlib\", \"tox\", ] license = {text = \"MIT\"} [tool.ruff] target-version = \"py310\" # Same as for core python (and oemof.solph) line-length = 79 [tool.ruff.lint] # E501 is the code for line length errors select = [\"E501\"] [tool.black] # needs to align with ruff line-length = 79 [tool.tox] legacy_tox_ini = \"\"\" [tox] envlist = py3 [gh-actions] python = 3: py3 [testenv] setenv = PYTHONPATH={toxinidir}/tests PYTHONUNBUFFERED=yes passenv = * deps = pytest commands = {posargs:pytest -vv --ignore=src} [testenv:py3] basepython = {env:TOXPYTHON:python3} setenv = {[testenv]setenv} usedevelop = true commands = # runs tests on the documentation {posargs:pytest --doctest-modules --doctest-ignore-import-errors --cov-report=term-missing --cov=examples/ --cov=src/} # runs tests on the installed package and produces an html report specifying the test gaps # the report is placed in this folder by default, can be changed by adding ':DEST' after 'html' where DEST is the target folder {posargs:pytest --cov-report=term-missing --cov-report=html --cov=mtress} deps = {[testenv]deps} pytest-cov \"\"\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/nest",
            "repo_link": "https://github.com/nest/nest-simulator",
            "content": {
                "codemeta": "",
                "readme": "# The Neural Simulation Tool - NEST [![Documentation](https://img.shields.io/readthedocs/nest-simulator?logo=readthedocs&logo=Read%20the%20Docs&label=Documentation)](https://nest-simulator.org/documentation) [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/2218/badge)](https://bestpractices.coreinfrastructure.org/projects/2218) [![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/nest/nest-simulator/badge)](https://scorecard.dev/viewer/?uri=github.com/nest/nest-simulator) [![License](http://img.shields.io/:license-GPLv2+-green.svg)](http://www.gnu.org/licenses/gpl-2.0.html) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10834751.svg)](https://doi.org/10.5281/zenodo.10834751) [![Latest release](https://img.shields.io/github/release/nest/nest-simulator.svg?color=brightgreen&label=latest%20release)](https://github.com/nest/nest-simulator/releases) [![GitHub contributors](https://img.shields.io/github/contributors/nest/nest-simulator?logo=github)](https://github.com/nest/nest-simulator) [![GitHub commit activity](https://img.shields.io/github/commit-activity/y/nest/nest-simulator?logo=github&color=%23ff6633)](https://github.com/nest/nest-simulator) [![Ubuntu version](https://img.shields.io/badge/ubuntu-(PPA)-blue?logo=debian)](https://nest-simulator.readthedocs.io/en/latest/installation/) [![Fedora package](https://img.shields.io/fedora/v/nest?logo=fedora)](https://src.fedoraproject.org/rpms/nest) [![conda-forge version](https://img.shields.io/conda/vn/conda-forge/nest-simulator.svg?logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/nest-simulator) [![Homebrew version](https://img.shields.io/homebrew/v/nest.svg?logo=apple)](https://formulae.brew.sh/formula/nest) [![Docker Image Version](https://img.shields.io/docker/v/nest/nest-simulator?color=blue&label=docker&logo=docker&logoColor=white&sort=semver)](https://hub.docker.com/r/nest/nest-simulator) [![Virtual applicance](https://img.shields.io/badge/VM-v3.7-blue?logo=CodeSandbox)](https://nest-simulator.readthedocs.io/en/latest/installation/livemedia.html#live-media) [![YouTube Video Views](https://img.shields.io/youtube/views/K7KXmIv6ROY?style=social)](https://www.youtube.com/results?search_query=nest-simulator+neurons) [![Twitter Follow](https://img.shields.io/twitter/follow/nestsimulator?style=social)](https://twitter.com/nestsimulator) NEST is a simulator for spiking neural network models that focuses on the dynamics, size and structure of neural systems rather than on the exact morphology of individual neurons. A NEST simulation tries to follow the logic of an electrophysiological experiment that takes place inside a computer with the difference that the neural system to be investigated must be defined by the experimenter. NEST is ideal for networks of spiking neurons of any size, for example: - Models of information processing, e.g., in the visual or auditory cortex of mammals, - Models of network activity dynamics, e.g., laminar cortical networks or balanced random networks, - Models of learning and plasticity. ## Key features of NEST * NEST provides a Python interface or a stand-alone application * NEST provides a large collection of [neurons and synapse models](https://nest-simulator.org/documentation/models/index.html) * NEST provides numerous [example network scripts](https://nest-simulator.org/documentation/examples/index.html) along with [tutorials and guides](https://nest-simulator.org/documentation/get-started_index.html) to help you develop your simulation * NEST has a large community of experienced developers and users; NEST was first released in 1994 under the name SYNOD, and has been extended and improved ever since * NEST is extensible: you can extend NEST by adding your own modules * NEST is scalable: Use NEST on your laptop or the largest supercomputers * NEST is memory efficient: It makes the best use of your multi-core computer and compute clusters with minimal user intervention * NEST is an open source project and is licensed under the GNU General Public License v2 or later * NEST employs continuous integration workflows in order to maintain high code quality standards for correct and reproducible simulations ## Documentation Please visit our [online documentation](https://nest-simulator.org/documentation) for details on installing and using NEST. ## Cite NEST If you use NEST as part of your research, please cite the *version* of NEST you used. The full citation for each release can be found on [Zenodo](https://zenodo.org/search?q=title%3ANEST%20AND%20-description%3Agraphical%20AND%20simulator&l=list&p=1&s=10&sort=publication-desc) For general citations, please use `Gewaltig M-O & Diesmann M (2007) NEST (Neural Simulation Tool) Scholarpedia 2(4):1430.` ## Contact If you need help or would like to discuss an idea or issue, join our [maling list](https://nest-simulator.org/documentation/developer_space/guidelines/mailing_list_guidelines.html), where we encourage active participation from our developers and users to share their knowledge and experience with NEST. You can find other [ways to get in touch here](https://nest-simulator.org/documentation/community.html). ## Contribute NEST is built on an active community and we welcome contributions to our code and documentation. For bug reports, feature requests, documentation improvements, or other issues, you can create a [GitHub issue](https://github.com/nest/nest-simulator/issues/new/choose), For working with NEST code and documentation, you can find guidelines for contributions [in our documentation](https://nest-simulator.org/documentation/developer_space/index.html#contribute-to-nest) ## Publications You can find a list of NEST [related publications here](https://www.nest-simulator.org/publications/). ## License NEST is open source software and is licensed under the [GNU General Public License v2](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html) or later. General information on the NEST Initiative can be found at its homepage at https://www.nest-initiative.org.\n",
                "dependencies": "# CMakeLists.txt # # This file is part of NEST. # # Copyright (C) 2004 The NEST Initiative # # NEST is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 2 of the License, or # (at your option) any later version. # # NEST is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with NEST. If not, see <http://www.gnu.org/licenses/> cmake_minimum_required( VERSION 3.19 ) # add cmake modules: for all `include(...)` first look here list( APPEND CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ) project( nest CXX C ) set( NEST_USER_EMAIL \"users@nest-simulator.org\" ) include( ColorMessages ) # check if the given CMAKE_INSTALL_PREFIX is not empty if(\"${CMAKE_INSTALL_PREFIX}\" STREQUAL \"\") printError(\"CMAKE_INSTALL_PREFIX cannot be an empty string\") endif() # handle relative installation prefixes if( NOT IS_ABSOLUTE ${CMAKE_INSTALL_PREFIX}) # convert relative path to absolute path get_filename_component(absPath ${CMAKE_INSTALL_PREFIX} ABSOLUTE BASE_DIR ${CMAKE_BINARY_DIR}) set(CMAKE_INSTALL_PREFIX ${absPath}) printInfo(\"Relative CMAKE_INSTALL_PREFIX has been converted to absolute path ${CMAKE_INSTALL_PREFIX}\") endif() ################################################################################ ################## All User Defined options ################## ################################################################################ # use Python to build PyNEST set( with-python ON CACHE STRING \"Build PyNEST [default=ON].\" ) option( cythonize-pynest \"Use Cython to cythonize pynestkernel.pyx [default=ON]. If OFF, PyNEST has to be build from a pre-cythonized pynestkernel.pyx.\" ON ) # select parallelization scheme set( with-mpi OFF CACHE STRING \"Build with MPI parallelization [default=OFF].\" ) set( with-openmp ON CACHE BOOL \"Build with OpenMP multi-threading [default=ON]. Optionally set OMP compiler flags.\" ) # external libraries set( with-libneurosim OFF CACHE STRING \"Build with libneurosim [default=OFF]. Optionally give the directory where libneurosim is installed.\" ) set( with-music OFF CACHE STRING \"Build with MUSIC [default=OFF]. Optionally give the directory where MUSIC is installed.\" ) set( with-sionlib OFF CACHE STRING \"Build with SIONlib [default=OFF]. Optionally give the directory where sionlib is installed.\" ) set( with-boost ON CACHE STRING \"Build with Boost [default=ON]. To set a specific Boost installation, give the install path.\" ) set( with-hdf5 OFF CACHE STRING \"Find a HDF5 library. To set a specific HDF5 installation, set install path. [default=ON]\" ) set( with-readline ON CACHE STRING \"Build with GNU Readline library [default=ON]. To set a specific library, give the install path.\" ) set( with-ltdl ON CACHE STRING \"Build with ltdl library [default=ON]. To set a specific ltdl, give the install path. NEST uses ltdl for dynamic loading of external user modules.\" ) set( with-gsl ON CACHE STRING \"Build with the GSL library [default=ON]. To set a specific library, give the install path.\" ) # NEST properties set( with-modelset \"full\" CACHE STRING \"The modelset to include. Sample configurations are in the modelsets directory. This option is mutually exclusive with -Dwith-models. [default=full].\" ) set( with-models OFF CACHE STRING \"The models to include as a semicolon-separated list of model headers (without the .h extension). This option is mutually exclusive with -Dwith-modelset. [default=OFF].\" ) set( tics_per_ms \"1000.0\" CACHE STRING \"Specify elementary unit of time [default=1000 tics per ms].\" ) set( tics_per_step \"100\" CACHE STRING \"Specify resolution [default=100 tics per step].\" ) set( with-detailed-timers OFF CACHE STRING \"Build with detailed internal time measurements [default=OFF]. Detailed timers can affect the performance.\" ) set( with-mpi-sync-timer OFF CACHE STRING \"Build with mpi synchronization barrier and timer [default=OFF]. Can affect the performance.\" ) set( with-threaded-timers ON CACHE STRING \"Build with one internal timer per thread [default=ON]. Multi-threaded timers can affect the performance.\" ) set( target-bits-split \"standard\" CACHE STRING \"Split of the 64-bit target neuron identifier type [default='standard']. 'standard' is recommended for most users. If running on more than 262144 MPI processes or more than 512 threads, change to 'hpc'.\" ) # generic build configuration option( static-libraries \"Build static executable and libraries [default=OFF]\" OFF ) set( with-optimize ON CACHE STRING \"Enable user defined optimizations [default=ON (uses '-O2')]. When OFF, no '-O' flag is passed to the compiler. Explicit compiler flags can be given; separate multiple flags by ';'.\" ) set( with-warning ON CACHE STRING \"Enable user defined warnings [default=ON (uses '-Wall')]. Separate multiple flags by ';'.\" ) set( with-debug OFF CACHE STRING \"Enable user defined debug flags [default=OFF]. When ON, '-g' is used. Separate multiple flags by ';'.\" ) set( with-cpp-std \"c++17\" CACHE STRING \"C++ standard to use for compilation [default='c++17'].\" ) set( with-intel-compiler-flags OFF CACHE STRING \"User defined flags for the Intel compiler [default='-fp-model strict']. Separate multiple flags by ';'.\" ) set( with-libraries OFF CACHE STRING \"Link additional libraries [default=OFF]. Give full path. Separate multiple libraries by ';'.\" ) set( with-includes OFF CACHE STRING \"Add additional include paths [default=OFF]. Give full path without '-I'. Separate multiple include paths by ';'.\" ) set( with-defines OFF CACHE STRING \"Additional defines, e.g. '-DXYZ=1' [default=OFF]. Separate multiple defines by ';'.\" ) # documentation build configuration set( with-userdoc OFF CACHE STRING \"Build user documentation [default=OFF]\") set( with-devdoc OFF CACHE STRING \"Build developer documentation [default=OFF]\") set( with-full-logging OFF CACHE STRING \"Write debug output to 'dump_<num_ranks>_<rank>.log' file [default=OFF]\") ################################################################################ ################## Project Directory variables ################## ################################################################################ # In general use the CMAKE_INSTALL_<dir> and CMAKE_INSTALL_FULL_<dir> vars from # GNUInstallDirs (included after calling nest_process_with_python()), but the # CMAKE_INSTALL_DATADIR is usually just CMAKE_INSTALL_DATAROOTDIR # and we want it to be CMAKE_INSTALL_DATAROOTDIR/PROJECT_NAME set( CMAKE_INSTALL_DATADIR \"share/${PROJECT_NAME}\" CACHE STRING \"Relative directory, where NEST installs its data (share/nest)\" ) ################################################################################ ################## Find utility programs ################## ################################################################################ find_program( SED NAMES sed gsed ) ################################################################################ ################## Load includes ################## ################################################################################ # This include checks the symbols, etc. include( CheckIncludesSymbols ) # These includes publish function names. include( ProcessOptions ) include( CheckExtraCompilerFeatures ) include( ConfigureSummary ) include( GetTriple ) # get triples arch-vendor-os get_host_triple( NEST_HOST_TRIPLE NEST_HOST_ARCH NEST_HOST_VENDOR NEST_HOST_OS ) get_target_triple( NEST_TARGET_TRIPLE NEST_TARGET_ARCH NEST_TARGET_VENDOR NEST_TARGET_OS ) # Process the command line arguments # IMPORTANT: Do not change the order of nest_process_with_python() and include( GNUInstallDirs )! # If NEST is built with Python, nest_process_with_python() defaults CMAKE_INSTALL_PREFIX # to the active virtual Python environment. This effects the inclusion # of GNUInstallDirs defining CMAKE_INSTALL_<dir> and CMAKE_INSTALL_FULL_<dir>. nest_process_with_python() include( GNUInstallDirs ) nest_post_process_with_python() nest_process_with_std() nest_process_with_intel_compiler_flags() nest_process_with_warning() nest_process_with_libraries() nest_process_with_includes() nest_process_with_defines() nest_process_static_libraries() nest_process_tics_per_ms() nest_process_tics_per_step() nest_process_with_libltdl() nest_process_with_readline() nest_process_with_gsl() nest_process_with_openmp() nest_process_with_mpi() nest_process_with_detailed_timers() nest_process_with_threaded_timers() nest_process_with_mpi_sync_timer() nest_process_with_libneurosim() nest_process_with_music() nest_process_with_sionlib() nest_process_with_mpi4py() nest_process_with_boost() nest_process_with_hdf5() nest_process_target_bits_split() nest_process_userdoc() nest_process_devdoc() nest_process_full_logging() nest_process_models() # These two function calls must come last, as to prevent unwanted interactions of the newly set flags # with detection/compilation operations carried out in earlier functions. The optimize/debug flags set # using these functions should only apply to the compilation of NEST, not to that of test programs # generated by CMake when it tries to detect compiler options or such. nest_process_with_optimize() nest_process_with_debug() nest_get_color_flags() set( CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${NEST_C_COLOR_FLAGS}\" ) set( CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${NEST_CXX_COLOR_FLAGS}\" ) # check additionals nest_check_exitcode_abort() nest_check_exitcode_segfault() nest_check_have_cmath_makros_ignored() nest_check_have_alpha_cxx_std_bug() nest_check_have_sigusr_ignored() nest_check_have_static_template_declaration_fail() nest_check_have_stl_vector_capacity_base_unity() nest_check_have_stl_vector_capacity_doubling() nest_check_have_xlc_ice_on_using() nest_check_have_std_nan() nest_check_have_std_isnan() nest_check_random123() include( NestVersionInfo ) get_version_info() printInfo(\"Done configuring NEST version: ${NEST_VERSION}\") enable_testing() set( TEST_OPTS \"\" ) if ( HAVE_PYTHON ) set( TEST_OPTS \"${TEST_OPTS};--with-python=${PYTHON}\" ) endif () if ( HAVE_MUSIC ) set( TEST_OPTS \"${TEST_OPTS};--with-music=${MUSIC_EXECUTABLE}\" ) endif () add_custom_target( installcheck COMMAND ${CMAKE_COMMAND} -E env ${CMAKE_INSTALL_FULL_DATADIR}/testsuite/do_tests.sh --prefix=${CMAKE_INSTALL_PREFIX} ${TEST_OPTS} WORKING_DIRECTORY \"${PROJECT_BINARY_DIR}\" COMMENT \"Executing NEST's testsuite...\" ) # N.B. to ensure \"make install\" is always run before \"make installcheck\", we # would ideally like to add: # add_dependencies( installcheck install ) # However, an issue in CMake at time of writing (May 2020, see # https://gitlab.kitware.com/cmake/cmake/-/issues/8438) precludes us from doing # so. ################################################################################ ################## Define Subdirectories here ################## ################################################################################ add_subdirectory( doc ) add_subdirectory( bin ) add_subdirectory( examples ) add_subdirectory( build_support ) add_subdirectory( lib ) add_subdirectory( libnestutil ) add_subdirectory( models ) add_subdirectory( sli ) add_subdirectory( nest ) add_subdirectory( nestkernel ) add_subdirectory( thirdparty ) add_subdirectory( testsuite ) if ( HAVE_PYTHON ) add_subdirectory( pynest ) endif () ################################################################################ ################## Summary of flags ################## ################################################################################ # used in nest-config # all compiler flags if ( NOT CMAKE_BUILD_TYPE OR \"${CMAKE_BUILD_TYPE}\" STREQUAL \"None\" ) set( ALL_CFLAGS \"${CMAKE_C_FLAGS}\" ) set( ALL_CXXFLAGS \"${CMAKE_CXX_FLAGS}\" ) elseif ( ${CMAKE_BUILD_TYPE} STREQUAL \"Debug\" ) set( ALL_CFLAGS \"${CMAKE_C_FLAGS} ${CMAKE_C_FLAGS_DEBUG}\" ) set( ALL_CXXFLAGS \"${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_DEBUG}\" ) elseif ( ${CMAKE_BUILD_TYPE} STREQUAL \"Release\" ) set( ALL_CFLAGS \"${CMAKE_C_FLAGS} ${CMAKE_C_FLAGS_RELEASE}\" ) set( ALL_CXXFLAGS \"${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_RELEASE}\" ) elseif ( ${CMAKE_BUILD_TYPE} STREQUAL \"RelWithDebInfo\" ) set( ALL_CFLAGS \"${CMAKE_C_FLAGS} ${CMAKE_C_FLAGS_RELWITHDEBINFO}\" ) set( ALL_CXXFLAGS \"${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_RELWITHDEBINFO}\" ) elseif ( ${CMAKE_BUILD_TYPE} STREQUAL \"MinSizeRel\" ) set( ALL_CFLAGS \"${CMAKE_C_FLAGS} ${CMAKE_C_FLAGS_MINSIZEREL}\" ) set( ALL_CXXFLAGS \"${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_MINSIZEREL}\" ) else () printError( \"Unknown build type: '${CMAKE_BUILD_TYPE}'\" ) endif () if ( with-defines ) foreach ( def ${with-defines} ) set( ALL_CFLAGS \"${def} ${ALL_CFLAGS}\" ) set( ALL_CXXFLAGS \"${def} ${ALL_CXXFLAGS}\" ) endforeach () endif () # add sionlib defines foreach ( def ${SIONLIB_DEFINES} ) set( ALL_CFLAGS \"${ALL_CFLAGS} ${def}\" ) set( ALL_CXXFLAGS \"${ALL_CXXFLAGS} ${def}\" ) endforeach () # libraries required to link extension modules set( MODULE_LINK_LIBS \"-lnest\" \"-lsli\" \"${LTDL_LIBRARIES}\" \"${READLINE_LIBRARIES}\" \"${GSL_LIBRARIES}\" \"${LIBNEUROSIM_LIBRARIES}\" \"${MUSIC_LIBRARIES}\" \"${MPI_CXX_LIBRARIES}\" \"${OpenMP_CXX_LIBRARIES}\" \"${SIONLIB_LIBRARIES}\" \"${BOOST_LIBRARIES}\" ) if ( with-libraries ) set( MODULE_LINK_LIBS \"${MODULE_LINK_LIBS};${with-libraries}\" ) endif () string( REPLACE \";\" \" \" MODULE_LINK_LIBS \"${MODULE_LINK_LIBS}\" ) # libraries requied to link NEST set( ALL_LIBS \"-lnest\" ${MODULE_LINK_LIBS} ) # all includes set( ALL_INCLUDES_tmp \"${CMAKE_INSTALL_FULL_INCLUDEDIR}/nest\" \"${LTDL_INCLUDE_DIRS}\" \"${READLINE_INCLUDE_DIRS}\" \"${GSL_INCLUDE_DIRS}\" \"${LIBNEUROSIM_INCLUDE_DIRS}\" \"${MUSIC_INCLUDE_DIRS}\" \"${MPI_CXX_INCLUDE_PATH}\" \"${BOOST_INCLUDE_DIR}\" ) set( ALL_INCLUDES \"\" ) foreach ( INC ${ALL_INCLUDES_tmp} ${with-includes} ) if ( INC AND NOT INC STREQUAL \"\" ) set( ALL_INCLUDES \"${ALL_INCLUDES} -I${INC}\" ) endif () endforeach () set( ALL_INCLUDES \"${ALL_INCLUDES} ${SIONLIB_INCLUDE}\" ) ################################################################################ ################## File generation here ################## ################################################################################ configure_file( \"${PROJECT_SOURCE_DIR}/libnestutil/config.h.in\" \"${PROJECT_BINARY_DIR}/libnestutil/config.h\" @ONLY ) configure_file( \"${PROJECT_SOURCE_DIR}/pynest/setup.py.in\" \"${PROJECT_BINARY_DIR}/pynest/setup.py\" @ONLY ) configure_file( \"${PROJECT_SOURCE_DIR}/bin/nest-config.in\" \"${PROJECT_BINARY_DIR}/bin/nest-config\" @ONLY ) configure_file( \"${PROJECT_SOURCE_DIR}/bin/nest_vars.sh.in\" \"${PROJECT_BINARY_DIR}/bin/nest_vars.sh\" @ONLY ) configure_file( \"${PROJECT_SOURCE_DIR}/doc/fulldoc.conf.in\" \"${PROJECT_BINARY_DIR}/doc/fulldoc.conf\" @ONLY ) configure_file( \"${PROJECT_SOURCE_DIR}/pynest/nest/versionchecker.py.in\" \"${PROJECT_BINARY_DIR}/pynest/nest/versionchecker.py\" @ONLY ) ################################################################################ ################## Install Extra Files ################## ################################################################################ install( FILES LICENSE README.md DESTINATION ${CMAKE_INSTALL_DOCDIR} ) nest_print_config_summary()\n[tool.pytest.ini_options] markers = [ \"skipif_missing_gsl: skip test if NEST was built without GSL support\", \"skipif_missing_hdf5: skip test if NEST was built without HDF5 support\", \"skipif_missing_mpi: skip test if NEST was built without MPI support\", \"skipif_missing_threads: skip test if NEST was built without multithreading support\", \"skipif_incompatible_mpi: skip if NEST with built with MPI that does not work with subprocess\", \"simulation: the simulation class to use. Always pass a 2nd dummy argument\" ] [tool.isort] profile = \"black\" known_third_party = \"nest\" [tool.black] line-length = 120\n# Required Python packages for NEST Simulator. # # This file specifies the required Python packages for those who would # like to compile NEST or build NEST documentation themselves. # If you just want to execute NEST, you should install NEST # directly as described in https://www.nest-simulator.org/installation. # # Note that this file only specifies the required Python packages and # not the entire software stack needed to build and work with NEST Simulator. # To create a mamba environment with the entire software stack for NEST # Simulator, use the the environment yaml file. # # The requirements from this file can be installed by # # pip install -r requirements.txt # To build and work with PyNEST -r requirements_pynest.txt # To run NEST testsuite -r requirements_testing.txt # To build NEST documentation -r requirements_docs.txt # To run NEST Server -r requirements_nest_server.txt\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/nest-ml",
            "repo_link": "https://github.com/nest/nestml",
            "content": {
                "codemeta": "{\"@context\":\"https://w3id.org/codemeta/3.0\",\"type\":\"SoftwareSourceCode\",\"applicationCategory\":\"computational neuroscience\",\"codeRepository\":\"https://github.com/nest/nestml/\",\"description\":\"NESTML is a domain-specific language for neuron and synapse models. These dynamical models can be used in simulations of brain activity on several platforms, in particular NEST Simulator.\\n\\nNESTML combines an easy to understand, yet powerful syntax, with good simulation performance by means of code generation, through a flexible and user-friendly toolchain.\",\"funder\":{\"type\":\"Organization\",\"name\":\"J\\u00fclich Research Centre GmbH, J\\u00fclich, Germany\"},\"keywords\":[\"neuron\",\"synapse\",\"neuroscience\",\"neural network\",\"spiking\"],\"license\":\"https://spdx.org/licenses/GPL-2.0-only\",\"name\":\"NESTML\",\"programmingLanguage\":\"Python\",\"codemeta:contIntegration\":{\"id\":\"https://github.com/nest/nestml/\"},\"continuousIntegration\":\"https://github.com/nest/nestml/\",\"issueTracker\":\"https://github.com/nest/nestml/issues\",\"version\":\"8.0.1-post-dev\"}\n",
                "readme": "[![astropy](http://img.shields.io/badge/powered%20by-AstroPy-orange.svg?style=flat)](http://www.astropy.org/) [![NESTML build](https://github.com/nest/nestml/actions/workflows/nestml-build.yml/badge.svg)](https://github.com/nest/nestml/actions/) # NESTML: The NEST Modelling Language NESTML is a domain-specific language for neuron and synapse models. These dynamical models can be used in simulations of brain activity on several platforms, in particular [NEST Simulator](https://nest-simulator.readthedocs.org/). NESTML combines an easy to understand, yet powerful syntax, with good simulation performance by means of code generation, through a flexible and user-friendly toolchain. ## Documentation Full documentation can be found at: <pre><p align=\"center\"><a href=\"https://nestml.readthedocs.io/\">https://nestml.readthedocs.io/</a></p></pre> ## Directory structure `models` - Example neuron models in NESTML format. `pynestml` - The source code of the PyNESTML toolchain. `tests` - A collection of tests for testing of the toolchain's behavior. `doc` - The documentation of the modeling language NESTML as well as processing toolchain PyNESTML. `extras` - Miscellaneous development tools, editor syntax highlighting rules, etc. ## License Copyright (C) 2017 The NEST Initiative NESTML is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 2 of the License, or (at your option) any later version. NESTML is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with NESTML. If not, see <http://www.gnu.org/licenses/>. ## Acknowledgements This software was initially supported by the JARA-HPC Seed Fund *NESTML - A modeling language for spiking neuron and synapse models for NEST* and the Initiative and Networking Fund of the Helmholtz Association and the Helmholtz Portfolio Theme *Simulation and Modeling for the Human Brain*. This software was developed in part or in whole in the Human Brain Project, funded from the European Union's Horizon 2020 Framework Programme for Research and Innovation under Specific Grant Agreements No. 720270, No. 785907 and No. 945539 (Human Brain Project SGA1, SGA2 and SGA3).\n",
                "dependencies": "# note: copy any changes to doc/requirements.txt numpy >= 1.8.2 scipy sympy antlr4-python3-runtime == 4.13.2 setuptools Jinja2 >= 2.10 typing;python_version<\"3.5\" astropy odetoolbox >= 2.5.8\n# -*- coding: utf-8 -*- # # setup.py # # This file is part of NEST. # # Copyright (C) 2004 The NEST Initiative # # NEST is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 2 of the License, or # (at your option) any later version. # # NEST is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with NEST. If not, see <http://www.gnu.org/licenses/>. import os import sys from setuptools import setup, find_packages assert sys.version_info.major >= 3 and sys.version_info.minor >= 9, \"Python 3.9 or higher is required to run NESTML\" with open(\"requirements.txt\") as f: requirements = f.read().splitlines() data_files = [] for dir_to_include in [\"doc\", \"models\", \"extras\"]: for dirname, dirnames, filenames in os.walk(dir_to_include): fileslist = [] for filename in filenames: fullname = os.path.join(dirname, filename) fileslist.append(fullname) data_files.append((dirname, fileslist)) setup( name=\"NESTML\", version=\"8.0.1-post-dev\", description=\"NESTML is a domain specific language that supports the specification of neuron models in a\" \" precise and concise syntax, based on the syntax of Python. Model equations can either be given\" \" as a simple string of mathematical notation or as an algorithm written in the built-in procedural\" \" language. The equations are analyzed by NESTML to compute an exact solution if possible or use an \" \" appropriate numeric solver otherwise.\", license=\"GNU General Public License v2.0\", url=\"https://github.com/nest/nestml\", packages=find_packages(), package_data={\"pynestml\": [\"codegeneration/resources_autodoc/*.jinja2\", \"codegeneration/resources_nest/point_neuron/*.jinja2\", \"codegeneration/resources_nest/point_neuron/common/*.jinja2\", \"codegeneration/resources_nest/point_neuron/directives_cpp/*.jinja2\", \"codegeneration/resources_nest/point_neuron/setup/*.jinja2\", \"codegeneration/resources_nest/point_neuron/setup/common/*.jinja2\", \"codegeneration/resources_nest_compartmental/cm_neuron/*.jinja2\", \"codegeneration/resources_nest_compartmental/cm_neuron/directives_cpp/*.jinja2\", \"codegeneration/resources_nest_compartmental/cm_neuron/setup/*.jinja2\", \"codegeneration/resources_nest_compartmental/cm_neuron/setup/common/*.jinja2\", \"codegeneration/resources_python_standalone/point_neuron/*.jinja2\", \"codegeneration/resources_python_standalone/point_neuron/directives_py/*.jinja2\", \"codegeneration/resources_spinnaker/*.jinja2\", \"codegeneration/resources_spinnaker/directives_py/*\", \"codegeneration/resources_spinnaker/directives_cpp/*\", \"codegeneration/resources_nest_desktop/*.jinja2\"]}, data_files=data_files, entry_points={ \"console_scripts\": [ \"nestml = pynestml.frontend.pynestml_frontend:main\", ], }, install_requires=requirements, test_suite=\"tests\", )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/nuclear-nexus",
            "repo_link": "https://gitlab.desy.de/fs-mcp/nuclear-nexus",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/nnu-net",
            "repo_link": "https://github.com/MIC-DKFZ/nnUNet",
            "content": {
                "codemeta": "",
                "readme": "# Welcome to the new nnU-Net! Click [here](https://github.com/MIC-DKFZ/nnUNet/tree/nnunetv1) if you were looking for the old one instead. Coming from V1? Check out the [TLDR Migration Guide](documentation/tldr_migration_guide_from_v1.md). Reading the rest of the documentation is still strongly recommended ;-) ## **2024-04-18 UPDATE: New residual encoder UNet presets available!** Residual encoder UNet presets substantially improve segmentation performance. They ship for a variety of GPU memory targets. It's all awesome stuff, promised! Read more :point_right: [here](documentation/resenc_presets.md) :point_left: Also check out our [new paper](https://arxiv.org/pdf/2404.09556.pdf) on systematically benchmarking recent developments in medical image segmentation. You might be surprised! # What is nnU-Net? Image datasets are enormously diverse: image dimensionality (2D, 3D), modalities/input channels (RGB image, CT, MRI, microscopy, ...), image sizes, voxel sizes, class ratio, target structure properties and more change substantially between datasets. Traditionally, given a new problem, a tailored solution needs to be manually designed and optimized - a process that is prone to errors, not scalable and where success is overwhelmingly determined by the skill of the experimenter. Even for experts, this process is anything but simple: there are not only many design choices and data properties that need to be considered, but they are also tightly interconnected, rendering reliable manual pipeline optimization all but impossible! ![nnU-Net overview](documentation/assets/nnU-Net_overview.png) **nnU-Net is a semantic segmentation method that automatically adapts to a given dataset. It will analyze the provided training cases and automatically configure a matching U-Net-based segmentation pipeline. No expertise required on your end! You can simply train the models and use them for your application**. Upon release, nnU-Net was evaluated on 23 datasets belonging to competitions from the biomedical domain. Despite competing with handcrafted solutions for each respective dataset, nnU-Net's fully automated pipeline scored several first places on open leaderboards! Since then nnU-Net has stood the test of time: it continues to be used as a baseline and method development framework ([9 out of 10 challenge winners at MICCAI 2020](https://arxiv.org/abs/2101.00232) and 5 out of 7 in MICCAI 2021 built their methods on top of nnU-Net, [we won AMOS2022 with nnU-Net](https://amos22.grand-challenge.org/final-ranking/))! Please cite the [following paper](https://www.google.com/url?q=https://www.nature.com/articles/s41592-020-01008-z&sa=D&source=docs&ust=1677235958581755&usg=AOvVaw3dWL0SrITLhCJUBiNIHCQO) when using nnU-Net: Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211. ## What can nnU-Net do for you? If you are a **domain scientist** (biologist, radiologist, ...) looking to analyze your own images, nnU-Net provides an out-of-the-box solution that is all but guaranteed to provide excellent results on your individual dataset. Simply convert your dataset into the nnU-Net format and enjoy the power of AI - no expertise required! If you are an **AI researcher** developing segmentation methods, nnU-Net: - offers a fantastic out-of-the-box applicable baseline algorithm to compete against - can act as a method development framework to test your contribution on a large number of datasets without having to tune individual pipelines (for example evaluating a new loss function) - provides a strong starting point for further dataset-specific optimizations. This is particularly used when competing in segmentation challenges - provides a new perspective on the design of segmentation methods: maybe you can find better connections between dataset properties and best-fitting segmentation pipelines? ## What is the scope of nnU-Net? nnU-Net is built for semantic segmentation. It can handle 2D and 3D images with arbitrary input modalities/channels. It can understand voxel spacings, anisotropies and is robust even when classes are highly imbalanced. nnU-Net relies on supervised learning, which means that you need to provide training cases for your application. The number of required training cases varies heavily depending on the complexity of the segmentation problem. No one-fits-all number can be provided here! nnU-Net does not require more training cases than other solutions - maybe even less due to our extensive use of data augmentation. nnU-Net expects to be able to process entire images at once during preprocessing and postprocessing, so it cannot handle enormous images. As a reference: we tested images from 40x40x40 pixels all the way up to 1500x1500x1500 in 3D and 40x40 up to ~30000x30000 in 2D! If your RAM allows it, larger is always possible. ## How does nnU-Net work? Given a new dataset, nnU-Net will systematically analyze the provided training cases and create a 'dataset fingerprint'. nnU-Net then creates several U-Net configurations for each dataset: - `2d`: a 2D U-Net (for 2D and 3D datasets) - `3d_fullres`: a 3D U-Net that operates on a high image resolution (for 3D datasets only) - `3d_lowres` → `3d_cascade_fullres`: a 3D U-Net cascade where first a 3D U-Net operates on low resolution images and then a second high-resolution 3D U-Net refined the predictions of the former (for 3D datasets with large image sizes only) **Note that not all U-Net configurations are created for all datasets. In datasets with small image sizes, the U-Net cascade (and with it the 3d_lowres configuration) is omitted because the patch size of the full resolution U-Net already covers a large part of the input images.** nnU-Net configures its segmentation pipelines based on a three-step recipe: - **Fixed parameters** are not adapted. During development of nnU-Net we identified a robust configuration (that is, certain architecture and training properties) that can simply be used all the time. This includes, for example, nnU-Net's loss function, (most of the) data augmentation strategy and learning rate. - **Rule-based parameters** use the dataset fingerprint to adapt certain segmentation pipeline properties by following hard-coded heuristic rules. For example, the network topology (pooling behavior and depth of the network architecture) are adapted to the patch size; the patch size, network topology and batch size are optimized jointly given some GPU memory constraint. - **Empirical parameters** are essentially trial-and-error. For example the selection of the best U-net configuration for the given dataset (2D, 3D full resolution, 3D low resolution, 3D cascade) and the optimization of the postprocessing strategy. ## How to get started? Read these: - [Installation instructions](documentation/installation_instructions.md) - [Dataset conversion](documentation/dataset_format.md) - [Usage instructions](documentation/how_to_use_nnunet.md) Additional information: - [Learning from sparse annotations (scribbles, slices)](documentation/ignore_label.md) - [Region-based training](documentation/region_based_training.md) - [Manual data splits](documentation/manual_data_splits.md) - [Pretraining and finetuning](documentation/pretraining_and_finetuning.md) - [Intensity Normalization in nnU-Net](documentation/explanation_normalization.md) - [Manually editing nnU-Net configurations](documentation/explanation_plans_files.md) - [Extending nnU-Net](documentation/extending_nnunet.md) - [What is different in V2?](documentation/changelog.md) Competitions: - [AutoPET II](documentation/competitions/AutoPETII.md) [//]: # (- [Ignore label]&#40;documentation/ignore_label.md&#41;) ## Where does nnU-Net perform well and where does it not perform? nnU-Net excels in segmentation problems that need to be solved by training from scratch, for example: research applications that feature non-standard image modalities and input channels, challenge datasets from the biomedical domain, majority of 3D segmentation problems, etc . We have yet to find a dataset for which nnU-Net's working principle fails! Note: On standard segmentation problems, such as 2D RGB images in ADE20k and Cityscapes, fine-tuning a foundation model (that was pretrained on a large corpus of similar images, e.g. Imagenet 22k, JFT-300M) will provide better performance than nnU-Net! That is simply because these models allow much better initialization. Foundation models are not supported by nnU-Net as they 1) are not useful for segmentation problems that deviate from the standard setting (see above mentioned datasets), 2) would typically only support 2D architectures and 3) conflict with our core design principle of carefully adapting the network topology for each dataset (if the topology is changed one can no longer transfer pretrained weights!) ## What happened to the old nnU-Net? The core of the old nnU-Net was hacked together in a short time period while participating in the Medical Segmentation Decathlon challenge in 2018. Consequently, code structure and quality were not the best. Many features were added later on and didn't quite fit into the nnU-Net design principles. Overall quite messy, really. And annoying to work with. nnU-Net V2 is a complete overhaul. The \"delete everything and start again\" kind. So everything is better (in the author's opinion haha). While the segmentation performance [remains the same](https://docs.google.com/spreadsheets/d/13gqjIKEMPFPyMMMwA1EML57IyoBjfC3-QCTn4zRN_Mg/edit?usp=sharing), a lot of cool stuff has been added. It is now also much easier to use it as a development framework and to manually fine-tune its configuration to new datasets. A big driver for the reimplementation was also the emergence of [Helmholtz Imaging](http://helmholtz-imaging.de), prompting us to extend nnU-Net to more image formats and domains. Take a look [here](documentation/changelog.md) for some highlights. # Acknowledgements <img src=\"documentation/assets/HI_Logo.png\" height=\"100px\" /> <img src=\"documentation/assets/dkfz_logo.png\" height=\"100px\" /> nnU-Net is developed and maintained by the Applied Computer Vision Lab (ACVL) of [Helmholtz Imaging](http://helmholtz-imaging.de) and the [Division of Medical Image Computing](https://www.dkfz.de/en/mic/index.php) at the [German Cancer Research Center (DKFZ)](https://www.dkfz.de/en/index.html).\n",
                "dependencies": "[project] name = \"nnunetv2\" version = \"2.6.1\" requires-python = \">=3.10\" description = \"nnU-Net is a framework for out-of-the box image segmentation.\" readme = \"readme.md\" license = { file = \"LICENSE\" } authors = [ { name = \"Fabian Isensee\", email = \"f.isensee@dkfz-heidelberg.de\"}, { name = \"Helmholtz Imaging Applied Computer Vision Lab\" } ] classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", \"Intended Audience :: Healthcare Industry\", \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: Apache Software License\", \"Topic :: Scientific/Engineering :: Artificial Intelligence\", \"Topic :: Scientific/Engineering :: Image Recognition\", \"Topic :: Scientific/Engineering :: Medical Science Apps.\", ] keywords = [ 'deep learning', 'image segmentation', 'semantic segmentation', 'medical image analysis', 'medical image segmentation', 'nnU-Net', 'nnunet' ] dependencies = [ \"torch>=2.1.2\", \"acvl-utils>=0.2.3,<0.3\", # 0.3 may bring breaking changes. Careful! \"dynamic-network-architectures>=0.3.1,<0.4\", # 0.3.1 and lower are supported, 0.4 may have breaking changes. Let's be careful here \"tqdm\", \"dicom2nifti\", \"scipy\", \"batchgenerators>=0.25.1\", \"numpy>=1.24\", \"scikit-learn\", \"scikit-image>=0.19.3\", \"SimpleITK>=2.2.1\", \"pandas\", \"graphviz\", 'tifffile', 'requests', \"nibabel\", \"matplotlib\", \"seaborn\", \"imagecodecs\", \"yacs\", \"batchgeneratorsv2>=0.2\", \"einops\", \"blosc2>=3.0.0b1\" ] [project.urls] homepage = \"https://github.com/MIC-DKFZ/nnUNet\" repository = \"https://github.com/MIC-DKFZ/nnUNet\" [project.scripts] nnUNetv2_plan_and_preprocess = \"nnunetv2.experiment_planning.plan_and_preprocess_entrypoints:plan_and_preprocess_entry\" nnUNetv2_extract_fingerprint = \"nnunetv2.experiment_planning.plan_and_preprocess_entrypoints:extract_fingerprint_entry\" nnUNetv2_plan_experiment = \"nnunetv2.experiment_planning.plan_and_preprocess_entrypoints:plan_experiment_entry\" nnUNetv2_preprocess = \"nnunetv2.experiment_planning.plan_and_preprocess_entrypoints:preprocess_entry\" nnUNetv2_train = \"nnunetv2.run.run_training:run_training_entry\" nnUNetv2_predict_from_modelfolder = \"nnunetv2.inference.predict_from_raw_data:predict_entry_point_modelfolder\" nnUNetv2_predict = \"nnunetv2.inference.predict_from_raw_data:predict_entry_point\" nnUNetv2_convert_old_nnUNet_dataset = \"nnunetv2.dataset_conversion.convert_raw_dataset_from_old_nnunet_format:convert_entry_point\" nnUNetv2_find_best_configuration = \"nnunetv2.evaluation.find_best_configuration:find_best_configuration_entry_point\" nnUNetv2_determine_postprocessing = \"nnunetv2.postprocessing.remove_connected_components:entry_point_determine_postprocessing_folder\" nnUNetv2_apply_postprocessing = \"nnunetv2.postprocessing.remove_connected_components:entry_point_apply_postprocessing\" nnUNetv2_ensemble = \"nnunetv2.ensembling.ensemble:entry_point_ensemble_folders\" nnUNetv2_accumulate_crossval_results = \"nnunetv2.evaluation.find_best_configuration:accumulate_crossval_results_entry_point\" nnUNetv2_plot_overlay_pngs = \"nnunetv2.utilities.overlay_plots:entry_point_generate_overlay\" nnUNetv2_download_pretrained_model_by_url = \"nnunetv2.model_sharing.entry_points:download_by_url\" nnUNetv2_install_pretrained_model_from_zip = \"nnunetv2.model_sharing.entry_points:install_from_zip_entry_point\" nnUNetv2_export_model_to_zip = \"nnunetv2.model_sharing.entry_points:export_pretrained_model_entry\" nnUNetv2_move_plans_between_datasets = \"nnunetv2.experiment_planning.plans_for_pretraining.move_plans_between_datasets:entry_point_move_plans_between_datasets\" nnUNetv2_evaluate_folder = \"nnunetv2.evaluation.evaluate_predictions:evaluate_folder_entry_point\" nnUNetv2_evaluate_simple = \"nnunetv2.evaluation.evaluate_predictions:evaluate_simple_entry_point\" nnUNetv2_convert_MSD_dataset = \"nnunetv2.dataset_conversion.convert_MSD_dataset:entry_point\" [project.optional-dependencies] dev = [ \"black\", \"ruff\", \"pre-commit\" ] [build-system] requires = [\"setuptools>=67.8.0\"] build-backend = \"setuptools.build_meta\" [tool.codespell] skip = '.git,*.pdf,*.svg' # # ignore-words-list = ''\nimport setuptools if __name__ == \"__main__\": setuptools.setup()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/nodejs-tcp-server-client",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Nodejs-TCP-Server-Client",
            "content": {
                "codemeta": "",
                "readme": "![NODE.JS](https://img.shields.io/badge/NODE.JS-%2343853D.svg?&style=plastic&logo=node.js&logoColor=white) ![JavaScript](https://img.shields.io/badge/JavaScript-323330?style=plastic&logo=javascript&logoColor=f7df1e) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/nodejs-tcp_server-client?&color=green&style=plastic) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808924.svg)](https://doi.org/10.5281/zenodo.12808924) [![SWH](https://archive.softwareheritage.org/badge/swh:1:dir:3022219080921ea266808592fa83f3afc5242282/)](https://archive.softwareheritage.org/swh:1:dir:3022219080921ea266808592fa83f3afc5242282;origin=https://github.com/Ramy-Badr-Ahmed/node-tcp;visit=swh:1:snp:e0d42b65bc06365c756247a55b21ca431f17a53a;anchor=swh:1:rev:d9e2239926489996936d18aa5804ce1fc2503181) # TCP Communication with Node.js A TCP server and client implementation using Node.js's `net` module, operating at the Transport Layer of the OSI Model. This implementation focuses on direct communication without additional overhead such as data compression or encryption/decryption, suitable for safe and trusted networks. The TLS variant (@Presentation-Layer of the OSI Model) is located here: [Node-TLS](https://github.com/Ramy-Badr-Ahmed/node-tls) #### Some Use Cases: - Network Diagnostics > Test network connectivity and latency between network segments. - Embedded Systems Communication > Integrate TCP client with embedded devices (e.g. Raspberry Pi, Arduino with Ethernet/Wi-Fi shields) to send data to a central server (for monitoring and control). - Time Synchronization > Use the server to provide a timestamp service for devices on a network (ensure synchronized time across various systems). - IoT Apps > Use the TCP server as a central hub to collect data from various IoT devices. > Set up the TCP client to send sensor data periodically from remote IoT devices to the server for analysis (centralized data receiver/logger). #### Quick Start: Server: ```shell npm install node tcpServer.js # Runs Server ``` Client: ```shell npm install node tcpClient.js # Runs Client ``` Logs and Outputs: The server and client will log various server/client events and actions, such as connection establishment, data transmission, and encountered errors. References - [Net Module](https://nodejs.org/api/net.html)\n",
                "dependencies": "{\"name\":\"tcp-server-client\",\"version\":\"1.0.0\",\"main\":\"tcpServer.js\",\"scripts\":{\"start-server\":\"node tcpServer.js\",\"start-client\":\"node tcpClient.js\"},\"keywords\":[],\"author\":\"Ramy-Badr-Ahmed\",\"description\":\"A simple TCP server and client implementation using Node.js.\",\"license\":\"Apache-2.0\",\"dependencies\":{\"uuid\":\"^10.0.0\"}}\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/nodejs-tls-server-client",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Nodejs-TLS-Server-Client",
            "content": {
                "codemeta": "",
                "readme": "![NODE.JS](https://img.shields.io/badge/NODE.JS-%2343853D.svg?&style=plastic&logo=node.js&logoColor=white) ![JavaScript](https://img.shields.io/badge/JavaScript-323330?style=plastic&logo=javascript&logoColor=f7df1e) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/nodejs-tls_server-client?color=green&style=plastic) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808908.svg)](https://doi.org/10.5281/zenodo.12808908) [![SWH](https://archive.softwareheritage.org/badge/swh:1:dir:8017c373f704257957a1cc9b5044c7347651b899/)](https://archive.softwareheritage.org/swh:1:dir:8017c373f704257957a1cc9b5044c7347651b899;origin=https://github.com/Ramy-Badr-Ahmed/node-tls;visit=swh:1:snp:eec57a10aaa0a231ac22e6c8a476c167a0669b66;anchor=swh:1:rev:0b48c4c274fb30ea4c7913f1d77083f9e2baa888) # TLS Communication with Node.js A TLS server and client implementation using Node.js's tls module, operating at the Presentation Layer of the OSI Model. This implementation focuses on encrypted and authenticated communication, suitable for secure and trusted networks. The TCP variant (@Transport-Layer of the OSI Model) is located here: [Node-TCP](https://github.com/Ramy-Badr-Ahmed/node-tcp) #### Use Cases: - Network Security > Ensure secure communication between network segments with TLS encryption and certificate-based authentication. - Secure Embedded Systems Communication > Integrate TLS client with embedded devices (e.g., Raspberry Pi, Arduino with Ethernet/Wi-Fi shields) to send data securely to a central server (for monitoring and control). - Secure Time Synchronization > Use the server to provide a timestamp service for devices on a network (ensure synchronized time across various systems with TLS security). - Secure IoT Applications > Use the TLS server as a central hub to collect data from various IoT devices securely. > Set up the TLS client to send sensor data periodically from remote IoT devices to the server for analysis (centralized secure data receiver/logger). ### Quick Start: Prerequisites: - Node.js (v14.x or later) - OpenSSL (for generating certificates) Server: Place server certificate and key under the `Certs\\server` directory ```shell npm install node tlsServer.js # Runs Server ``` Client: Place client certificate and key under the `Certs\\client` directory ```shell npm install node tlsClient.js # Runs Client ``` Logs and Outputs: The server and client will log various events and actions, such as connection establishment, data transmission, handshake report, session management, and encountered errors. References - [TLS (SSL) Module](https://nodejs.org/api/tls.html)\n",
                "dependencies": "{\"name\":\"tls-server-client\",\"version\":\"1.0.0\",\"main\":\"tlsServer.js\",\"scripts\":{\"start-server\":\"node tlsServer.js\",\"start-client\":\"node tlsClient.js\"},\"keywords\":[],\"author\":\"Ramy-Badr-Ahmed\",\"description\":\"A TLS server and client implementation using Node.js.\",\"license\":\"Apache-2.0\",\"dependencies\":{\"uuid\":\"^10.0.0\"}}\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/novosparc",
            "repo_link": "https://github.com/rajewsky-lab/novosparc",
            "content": {
                "codemeta": "",
                "readme": "|PyPI| |Docs| |PePy| .. |PyPI| image:: https://img.shields.io/pypi/v/novosparc.svg :target: https://pypi.org/project/novosparc/ .. |Docs| image:: https://readthedocs.org/projects/novosparc/badge/?version=latest :target: https://novosparc.readthedocs.io/ .. |PePy| image:: https://static.pepy.tech/badge/novosparc :target: https://pepy.tech/project/novosparc novoSpaRc - *de novo* Spatial Reconstruction of Single-Cell Gene Expression =========================================================================== .. image:: https://raw.githubusercontent.com/nukappa/nukappa.github.io/master/images/novosparc.png :width: 90px :align: left ``novoSpaRc`` predicts locations of single cells in space by solely using single-cell RNA sequencing data. An existing reference database of marker genes is not required, but significantly enhances performance if available. ``novoSpaRc`` accompanies the following publications: | *Gene Expression Cartography* | M Nitzan*, N Karaiskos*, N Friedman†, N Rajewsky† | `Nature (2019) <https://www.nature.com/articles/s41586-019-1773-3>`_ and | *novoSpaRc: flexible spatial reconstruction of single-cell gene expression with optimal transport* | N Moriel*, E Senel*, N Friedman, N Rajewsky, N Karaiskos†, M Nitzan† | `Nature Protocols (2021) <https://www.nature.com/articles/s41596-021-00573-7>`_ Read the `documentation <https://novosparc.readthedocs.io>`_ and the `tutorial <https://github.com/rajewsky-lab/novosparc/blob/master/reconstruct_drosophila_embryo_tutorial.ipynb>`_ for more information.\n",
                "dependencies": "POT>=0.7.0 numpy anndata>=0.7.5 matplotlib>=3.3.2 scipy pandas recommonmark scanpy>=1.6.0 scikit_learn\nfrom setuptools import setup, find_packages with open(\"README.rst\", \"r\") as fh: long_description = fh.read() try: from novosparc import __author__, __email__, __version__ except ImportError: # Deps not yet installed __author__ = __email__ = __version__ = '' with open('requirements.txt', 'r') as f: required_packages = f.read().splitlines() setup( name=\"novosparc\", version=__version__, author=__author__, author_email=__email__, description=\"De novo spatial reconstruction of single-cell gene expression.\", long_description=long_description, url=\"https://github.com/rajewsky-lab/novosparc\", license='MIT', install_requires=required_packages, packages=find_packages(), classifiers=[ \"Programming Language :: Python :: 3\", \"Operating System :: POSIX :: Linux\", \"Operating System :: MacOS :: MacOS X\" ] )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/odm2sms",
            "repo_link": "https://jugit.fz-juelich.de/sms/odm2sms",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/oemof-solph",
            "repo_link": "https://github.com/oemof/oemof-solph/",
            "content": {
                "codemeta": "",
                "readme": "|tox-pytest| |tox-checks| |appveyor| |coveralls| |codecov| |scrutinizer| |codacy| |codeclimate| |wheel| |packaging| |supported-versions| |docs| |zenodo| |version| |commits-since| |chat| ------------------------------ .. |tox-pytest| image:: https://github.com/oemof/oemof-solph/actions/workflows/tox_pytests.yml/badge.svg?branch=dev :target: https://github.com/oemof/oemof-solph/actions?query=workflow%3A%22tox+checks%22 .. |tox-checks| image:: https://github.com/oemof/oemof-solph/actions/workflows/tox_checks.yml/badge.svg?branch=dev :target: https://github.com/oemof/oemof-solph/actions?query=workflow%3A%22tox+checks%22 .. |packaging| image:: https://github.com/oemof/oemof-solph/actions/workflows/packaging.yml/badge.svg :target: https://github.com/oemof/oemof-solph/actions?query=workflow%3Apackaging .. |docs| image:: https://readthedocs.org/projects/oemof-solph/badge/?style=flat :target: https://readthedocs.org/projects/oemof-solph :alt: Documentation Status .. |appveyor| image:: https://ci.appveyor.com/api/projects/status/github/oemof/oemof-solph?branch=dev&svg=true :alt: AppVeyor Build Status :target: https://ci.appveyor.com/project/oemof-developer/oemof-solph .. |coveralls| image:: https://coveralls.io/repos/oemof/oemof-solph/badge.svg?branch=dev&service=github :alt: Coverage Status :target: https://coveralls.io/github/oemof/oemof-solph .. |codecov| image:: https://codecov.io/gh/oemof/oemof-solph/branch/dev/graphs/badge.svg?branch=dev :alt: Coverage Status :target: https://codecov.io/gh/oemof/oemof-solph .. |codacy| image:: https://api.codacy.com/project/badge/Grade/a6e5cb2dd2694c73895e142e4cf680d5 :target: https://app.codacy.com/gh/oemof/oemof-solph/dashboard :alt: Codacy Code Quality Status .. |codeclimate| image:: https://codeclimate.com/github/oemof/oemof-solph/badges/gpa.svg :target: https://codeclimate.com/github/oemof/oemof-solph :alt: CodeClimate Quality Status .. |version| image:: https://img.shields.io/pypi/v/oemof.solph.svg :alt: PyPI Package latest release :target: https://pypi.org/project/oemof.solph .. |wheel| image:: https://img.shields.io/pypi/wheel/oemof.solph.svg :alt: PyPI Wheel :target: https://pypi.org/project/oemof.solph .. |supported-versions| image:: https://img.shields.io/pypi/pyversions/oemof.solph.svg :alt: Supported versions :target: https://pypi.org/project/oemof.solph .. |supported-implementations| image:: https://img.shields.io/pypi/implementation/oemof.solph.svg :alt: Supported implementations :target: https://pypi.org/project/oemof.solph .. |commits-since| image:: https://img.shields.io/github/commits-since/oemof/oemof-solph/latest/dev :alt: Commits since latest release :target: https://github.com/oemof/oemof-solph/compare/master...dev .. |zenodo| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.596235.svg :alt: Zenodo DOI :target: https://doi.org/10.5281/zenodo.596235 .. |scrutinizer| image:: https://img.shields.io/scrutinizer/quality/g/oemof/oemof-solph/dev.svg :alt: Scrutinizer Status :target: https://scrutinizer-ci.com/g/oemof/oemof-solph/ .. |chat| image:: https://img.shields.io/badge/chat-oemof:matrix.org-%238ADCF7 :alt: matrix-chat :target: https://matrix.to/#/#oemof:matrix.org .. figure:: https://raw.githubusercontent.com/oemof/oemof-solph/492e3f5a0dda7065be30d33a37b0625027847518/docs/_logo/logo_oemof_solph_FULL.svg :align: center ------------------------------ =========== oemof.solph =========== **A model generator for energy system modelling and optimisation (LP/MILP)** .. contents:: :depth: 2 :local: :backlinks: top Introduction ============ The oemof.solph package is part of the `Open energy modelling framework (oemof) <https://github.com/oemof/oemof>`_. This is an organisational framework to bundle tools for energy (system) modelling. oemof-solph is a model generator for energy system modelling and optimisation. The package ``oemof.solph`` is very often called just ``oemof``. This is because installing the ``oemof`` meta package was once the best way to get ``oemof.solph``. Notice that you should prefeably install ``oemof.solph`` instead of ``oemof`` if you want to use ``solph``. Everybody is welcome to use and/or develop oemof.solph. Read our `contribution <https://oemof.readthedocs.io/en/latest/contributing.html>`_ section. Contribution is already possible on a low level by simply fixing typos in oemof's documentation or rephrasing sections which are unclear. If you want to support us that way please fork the oemof-solph repository to your own GitHub account and make changes as described in the `github guidelines <https://docs.github.com/en/get-started/quickstart/hello-world>`_ If you have questions regarding the use of oemof including oemof.solph you can visit the openmod forum (`tag oemof <https://forum.openmod-initiative.org/tags/c/qa/oemof>`_ or `tag oemof-solph <https://forum.openmod-initiative.org/tags/c/qa/oemof-solph>`_) and open a new thread if your questions hasn't been already answered. Keep in touch! - You can become a watcher at our `github site <https://github.com/oemof/oemof>`_, but this will bring you quite a few mails and might be more interesting for developers. If you just want to get the latest news, like when is the next oemof meeting, you can follow our news-blog at `oemof.org <https://oemof.org/>`_. Documentation ============= The `oemof.solph documentation <https://oemof-solph.readthedocs.io/>`_ is powered by readthedocs. Use the `project site <https://readthedocs.org/projects/oemof>`_ of oemof.solph to choose the version of the documentation. Go to the `download page <https://readthedocs.org/projects/oemof/downloads/>`_ to download different versions and formats (pdf, html, epub) of the documentation. .. _installation_label: Installation ============ If you have a working Python installation, use pypi to install the latest version of oemof.solph. Python >= 3.8 is recommended. Lower versions may work but are not tested. We highly recommend to use virtual environments. Please refer to the documentation of your Python distribution (e.g. Anaconda, Micromamba, or the version of Python that came with your Linux installation) to learn how to set up and use virtual environments. :: (venv) pip install oemof.solph If you want to use the latest features, you might want to install the **developer version**. The developer version is not recommended for productive use:: (venv) pip install https://github.com/oemof/oemof-solph/archive/dev.zip For running an oemof-solph optimisation model, you need to install a solver. Following you will find guidelines for the installation process for different operating systems. .. _windows_solver_label: .. _linux_solver_label: Installing a solver ------------------- There are several solvers that can work with oemof, both open source and commercial. Two open source solvers are widely used (CBC and GLPK), but oemof suggests CBC (Coin-or branch and cut). It may be useful to compare results of different solvers to see which performs best. Other commercial solvers, like Gurobi or Cplex, are also options. Have a look at the `pyomo docs <https://pyomo.readthedocs.io/en/stable/api/pyomo.solvers.plugins.solvers.html>`_ to learn about which solvers are supported. Check the solver installation by executing the test_installation example below (see section Installation Test). **Linux** To install the solvers have a look at the package repository of your Linux distribution or search for precompiled packages. GLPK and CBC ares available at Debian, Feodora, Ubuntu and others. **Windows** 1. Download `CBC <https://github.com/coin-or/Cbc/releases>`_ 2. Download `GLPK (64/32 bit) <https://sourceforge.net/projects/winglpk/>`_ 3. Unpack CBC/GLPK to any folder (e.g. C:/Users/Somebody/my_programs) 4. Add the path of the executable files of both solvers to the PATH variable (cf. `setting environment variables as user <https://learn.microsoft.com/en-us/troubleshoot/windows-client/performance/cannot-modify-user-environment-variables-system-properties>`_) 5. Restart Windows Check the solver installation by executing the test_installation example (see the `Installation test` section). **Mac OSX** Please follow the installation instructions on the respective homepages for details. CBC-solver: https://github.com/coin-or/Cbc GLPK-solver: http://arnab-deka.com/posts/2010/02/installing-glpk-on-a-mac/ If you install the CBC solver via brew (highly recommended), it should work without additional configuration. **conda** The CBC-solver can also be installed in a `conda` environment. Please note, that it is highly recommended to `use pip after conda <https://www.anaconda.com/blog/using-pip-in-a-conda-environment>`_, so: .. code:: console (venv) conda install -c conda-forge coincbc (venv) pip install oemof.solph .. _check_installation_label: Installation test ----------------- Test the installation and the installed solver by running the installation test in your virtual environment: .. code:: console (venv) oemof_installation_test If the installation was successful, you will receive something like this: .. code:: console ********* Solver installed with oemof: glpk: working cplex: not working cbc: working gurobi: not working ********* oemof.solph successfully installed. as an output. Contributing ============ A warm welcome to all who want to join the developers and contribute to oemof.solph. Information on the details and how to approach us can be found `in the oemof documentation <https://oemof.readthedocs.io/en/latest/contributing.html>`_ . Citing ====== For explicitly citing solph, you might want to refer to `DOI:10.1016/j.simpa.2020.100028 <https://doi.org/10.1016/j.simpa.2020.100028>`_, which gives an overview over the capabilities of solph. The core ideas of oemof as a whole are described in `DOI:10.1016/j.esr.2018.07.001 <https://doi.org/10.1016/j.esr.2018.07.001>`_ (preprint at `arXiv:1808.0807 <https://arxiv.org/abs/1808.08070v1>`_). To allow citing specific versions, we use the zenodo project to get a DOI for each version. Example Applications ==================== The combination of specific modules (often including other packages) is called an application (app). For example, it can depict a concrete energy system model. You can find a large variety of helpful examples in the documentation. The examples show the optimisation of different energy systems and are supposed to help new users to understand the framework's structure. Please make sure the example you are looking at is created for the version of solph you have installed. You are welcome to contribute your own examples via a `pull request <https://github.com/oemof/oemof-solph/pulls>`_ or by e-mailing us (see `here <https://oemof.org/contact/>`_ for contact information). License ======= Copyright (c) oemof developer group Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
                "dependencies": "[build-system] requires = [\"flit_core >=3.2,<4\"] build-backend = \"flit_core.buildapi\" [tool.flit.sdist] include = [ \"AUTHORS.rst\", \"CITATION.cff\", \"CODE_OF_CONDUCT.md\", \"CONTRIBUTING.rst\", \"LICENSE\", \"README.rst\", \"VERSION\", \"tox.ini\", \"ci/\", \"docs/\", \"examples/\", \"src/\", \"tests/\", \".bumpversion.cfg\", \".coveragerc\", \".editorconfig\", \".flake8\", \".pep8speaks.yml\", \".readthedocs.yaml\", \".scrutinizer.yml\", ] exclude = [\"docs/_build\"] [project] name = \"oemof.solph\" dynamic = [\"version\"] description = \"A model generator for energy system modelling and optimisation.\" readme = \"README.rst\" authors = [ {name = \"oemof developer group\", email = \"contact@oemof.org\"}, ] classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: MIT License\", \"Operating System :: Unix\", \"Operating System :: POSIX\", \"Operating System :: Microsoft :: Windows\", \"Operating System :: MacOS\", \"Operating System :: OS Independent\", \"Programming Language :: Python\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Programming Language :: Python :: 3.13\", \"Programming Language :: Python :: Implementation :: CPython\", \"Topic :: Utilities\", ] requires-python = \">=3.10\" dependencies = [ \"blinker\", \"dill\", \"numpy >= 2.0.0\", \"pandas >= 2.2.0\", \"pyomo >= 6.8.0\", \"networkx\", \"oemof.tools >= 0.4.3\", \"oemof.network >= 0.5.0\", ] license = {text = \"MIT\"} [project.urls] Homepage = \"https://github.com/oemof/oemof-solph\" Documentation = \"https://oemof-solph.readthedocs.io/\" Changelog = \"https://oemof-solph.readthedocs.io/en/latest/changelog.html\" \"Issue Tracker\" = \"https://github.com/oemof/oemof-solph/issues/\" [project.optional-dependencies] dev = [ \"flit\", \"matplotlib\", \"nbformat\", \"pytest\", \"sphinx\", \"sphinx_rtd_theme\", \"sphinx-copybutton\", \"sphinx-design\", \"termcolor\", \"tox\", ] [project.scripts] oemof_installation_test = \"oemof.solph._console_scripts:check_oemof_installation\" [tool.black] line-length = 79 target-version = ['py39', 'py310', 'py311'] include = '\\.pyi?$' exclude = ''' /( \\.eggs | \\.git | \\.hg | \\.mypy_cache | \\.tox | \\.venv | _build | buck-out | build | dist | ci )/ ''' [tool.pytest.ini_options] norecursedirs = [ \".git\", \".tox\", \".env\", \"dist\", \"build\", \"docs/_build\", \"migrations\", \"examples\", ] python_files = [ \"test_*.py\", \"*_test.py\", \"*_tests.py\", \"tests.py\", ] addopts = \"\"\" -ra --strict-markers --ignore=docs/conf.py --ignore=setup.py --ignore=ci --ignore=.eggs --doctest-modules --doctest-glob=\\\\*.rst --tb=short --pyargs \"\"\" testpaths = [ \"src/oemof/solph/\", \"tests/\", \"docs/\", ] [tool.isort] force_single_line = true line_length = 79 known_first_party = \"oemof-solph\" default_section = \"THIRDPARTY\" forced_separate = \"test_oemof-solph\" skip = \"migrations\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/opencarp",
            "repo_link": "https://git.opencarp.org/openCARP/openCARP",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/openfuelcell2",
            "repo_link": "https://github.com/openFuelCell2/openFuelCell2",
            "content": {
                "codemeta": "",
                "readme": "# openFuelCell2 [openFuelCell2](https://openfuelcell2.github.io/) is a computational fluid dynamics (CFD) toolbox for simulating electrochemical devices such as fuel cells and electrolysis. The solver is based on the open-source library, OpenFOAM®. ## About the code The source code was developed from a previous open-source repository called [openFuelCell](http://openfuelcell.sourceforge.net/). It was also inspired by the standard solver \"reactingTwoPhaseEulerFoam\" in OpenFOAM®. The solver can consider coupled problems with multi-region and multi-physics issues, including single and two phase flows, multiple species components, charge transfer, and electrochemical reactions in different regions. More applications and solvers will be available in the future. ## How to use The code is compiled with the OpenFOAM libraries, either [ORG](https://openfoam.org/) or [COM](https://www.openfoam.com/) versions. The default branch is compatable with the COM version, while the other branches are also provided for different OpenFOAM environments. The available environments will include: OpenFOAM-v2012, OpenFOAM-v2106, OpenFOAM-v2306, OpenFOAM-v6, OpenFOAM-v8. Note: the main branch is only compatible with OpenFOAM-v2306, while the others are under preparation. ```bash # Download the source code # Setup the corresponding openfoam environment # Switch to the corresponding branch # Change dictionary to the repository cd openFuelCell2/src # Compile the source code with ./Allwmake # Or compile in parallel ./Allwmake -j n ``` You can also clear the libraries and executable files with ```bash cd openFuelCell2/src ./Allwclean ``` ## Computational domains --- Take the cross-section of a fuel cell as an example. The computational domain gives, <div align=\"center\"> <img src=\"images/computationDomain.jpg\" height=\"70%\" width=\"70%\"> </div> In a PEM fuel cell or other types, there are several domains/regions: air, fuel, electrolyte, and interconnect. This can be found from the repository [openFuelCell](http://openfuelcell.sourceforge.net/). However, additional domains/regions, e.g. phiEA, phiEC, and phiI are also necessary to account for electron/ion and dissolved water transfer. To consider the coupling transfer problems in a PEM fuel cell, a global region, also called as parent mesh, and several local regions, also called as child meshes, are used. In the global region, only the energy equation is solved. In the local regions, corresponding partial differential equations will be discretized and solved. During the simulation, material properties, e.g. density, thermal conductivity, etc., are mapped from local regions to the global region, while the temperature field is mapped from the global region to the local regions. The local regions can be classified as three different types, namely fluid, solid, and electric regions. See the [code](src/libSrc/fuelCellSystems/regions). - Fluid region: This region represents the space where fluid flows by. In a fuel cell or electrolyzer, it consists with gas channels and/or porous regions. In this region, the following processes are addressed: - Fluid flow (single/two phase) - Species transfer - Electrochemical reaction - Heat and mass transfer For example, in a fuel cell, the following parts apply to this region, - Air flow paths + porous electrodes - Fuel flow paths + porous electrodes - Cooling channels - Solid region: This represents the solid components in fuel cell or electrolyzer. In the current solver, no equations will be solved here. However, stress analysis during assembly and thermal effects may be implemented in future applications. For example. in a fuel cell, the following components apply to this region, - Electrolyte/membrane - Interconnect/Bipolar-plate - Endplate - Electric region: This region accounts for the electric-conductive components. It is designed to consider electron/ion transfer specifically. However, it is found that the proton transfer region is the same as the region where dissolved water transfer takes place. Therefore, a switcher is enabled in the code to turn on/off the dissolved water transfer model. The following equations will be solved, - Potential equations (Poisson equations) - Dissolved water transfer equations (diffusion and electro-osmotic-drag) For example, in a fuel cell, the following components belong to this region: - Bipolar-plate, GDL, MPL, CL -> electron transfer regions - Catalyst coated membrane, CCM, -> proton transfer and dissolved water transfer region - Global region: The heat source/sinks in local regions will be mapped to this region. And the obtained temperature is mapped back to the local regions. The heat source/sink include: - Joule heat from the electron/proton regions. - Condensation/evaporation in the fluid regions. - Electrochemical reactions in the fluid regions. ## Recent updates --- - [Oct. 2020] A new branch for openFOAM-ESI > The majority part of this update was conducted by Mr. Steffen Hess. The code is able to compile in the OpenFOAM-ESI environment. - [Nov. 2020] The new branch for openFOAM-ESI > Some bugs were found and fixed: 1. The compiling sequence. 2. The locations of gravity fields, g. The files \"g\" move to constant/. 3. The functionObjects library is missing. 4. Remove some warnings: apply new functions in OpenFOAM-ESI. - [Nov. 2021] The new branch for openFOAM-2106 > Some bugs were found and fixed: 1. The method **heatTransfer(T, cellListIO)** in class \"TwoResistanceHeatTransferPhaseSystem\" is fixed. > Dictionary structure is rearranged: 1. src: source files 2. appSrc: application source files 3. libSrc: libraries source files 4. run: test cases - [Jun. 2022] Clean the source code for releasing > Bugs are found and fixed: 1. The previous solver might predict results with singularities in phiI region. This is fixed. > The source code is updated: 1. Change the phase name to none if single-phase flow is simulated. This makes the variable names change from _A.air_ to _A_. 2. Moving the correction of diffusivity from MultiComponentPhaseModel to **diffusivityModelList**. 3. Moving the definition of phase properties from phaseProperties to regionProperties. 4. Avoiding redundant output of diffusivity coefficients. 5. Applying a different method to Update the value of phi in phiI region --> use setReference of phiEqn. This seems to make the solution more stable. 6. Making the \"porousZone\" flexible to read. If the file doesn't exist, no porous zones are applied. 7. Update the test cases: rewrite the scripts. 8. Change the header of each file. openFuelCell is included. - [Dec. 2022] Update the repository 1. Fixed a bug in diffusivityList 2. Update the tutorial - [Sep. 2023] Update the repository for public release 1. A new branch is included for OpenFOAM-v2306 2. The prescribed mean current density and voltage are now defined as a function of time. (Assailable functions can be found in OpenFOAM/primitives/functions/Function1). 3. Include the radiation model in solid region. 4. Copy thermoTools to the repo. (need to remove this in next update.) 5. In test cases, when it comes to two-phase flow, a steadyState scheme is now used, specially for ddt term of species transfer. 6. Update the preprocessing script for an easier usage. ## Related publications - Journal - Zhang, Shidong, Steffen Hess, Holger Marschall, Uwe Reimer, Steven Beale, and Werner Lehnert. \"openFuelCell2: A New Computational Tool for Fuel Cells, Electrolyzers, and other Electrochemical Devices and Processes.\" Computer Physics Communications, Forthcoming (2023). - Zhang, Shidong, Shangzhe Yu, Roland Peters, Steven B. Beale, Holger Marschall, Felix Kunz, and Rüdiger-A. Eichel. \"A new procedure for rapid convergence in numerical performance calculations of electrochemical cells.\" Electrochimica Acta (2023): 143275. - Yu, Shangzhe, Shidong Zhang, Dominik Schäfer, Roland Peters, Felix Kunz, and Rüdiger-A. Eichel. \"Numerical Modeling and Simulation of the Solid Oxide Cell Stacks and Metal Interconnect Oxidation with OpenFOAM.\" Energies 16, no. 9 (2023): 3827. - Zhang, Shidong, Steven B. Beale, Uwe Reimer, Martine Andersson, and Werner Lehnert. \"Polymer electrolyte fuel cell modeling-A comparison of two models with different levels of complexity.\" International Journal of Hydrogen Energy 45, no. 38 (2020): 19761-19777. - Zhang, Shidong. \"Low-Temperature Polymer Electrolyte Fuel Cells.\" In Electrochemical Cell Calculations with OpenFOAM, pp. 59-85. Springer, Cham, 2022. - Conference - Hess, Steffen, Shidong Zhang, Thomas Kadyk, Werner Lehnert, Michael Eikerling, and Steven B. Beale. \"Numerical Two-Phase Simulations of Alkaline Water Electrolyzers.\" ECS Transactions 112, no. 4 (2023): 419. - Zhang, Shidong, Kai Wang, Shangzhe Yu, Nicolas Kruse, Roland Peters, Felix Kunz, and Rudiger-A. Eichel. \"Multiscale and Multiphysical Numerical Simulations of Solid Oxide Cell (SOC).\" ECS Transactions 111, no. 6 (2023): 937. - Zhang, Shidong, Steven B. Beale, Yan Shi, Holger Janßen, Uwe Reimer, and Werner Lehnert. \"Development of an Open-Source Solver for Polymer Electrolyte Fuel Cells.\" ECS Transactions 98, no. 9 (2020): 317. - Thesis - Zhang, Shidong. Modeling and Simulation of Polymer Electrolyte Fuel Cells. No. FZJ-2020-02318. Elektrochemische Verfahrenstechnik, 2020. ## Developers --- The code is firstly developed by [Shidong Zhang](s.zhang@fz-juelich.de) for the PhD thesis, supervised by Prof. [Werner Lehnert](w.lehnert@fz-juelich.de) and Prof. [Steven Beale](s.beale@fz-juelich.de). The detailed model description and simulation results can be found in the thesis, `Modeling and Simulation of Polymer Electrolyte Fuel Cells` by FZJ. The following individuals also contribute to the optimization of the code, - Steffen Hess (s.hess@fz-juelich.de), Forschungszentrum Juelich, IEK-14 - Prof. Steven B. Beale (s.beale@fz-juelich.de), Forschungszentrum Juelich, IEK-13 To be continued...\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/opengeosys",
            "repo_link": "https://gitlab.opengeosys.org/ogs/ogs",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/openpmd-api",
            "repo_link": "https://github.com/openPMD/openPMD-api/",
            "content": {
                "codemeta": "",
                "readme": "C++ & Python API for Scientific I/O with openPMD ================================================ [![Supported openPMD Standard](https://img.shields.io/badge/openPMD-1.0.0--1.1.0-blue)](https://github.com/openPMD/openPMD-standard/releases) [![Doxygen](https://img.shields.io/badge/API-Doxygen-blue)](https://www.openpmd.org/openPMD-api) [![Gitter chat](https://img.shields.io/gitter/room/openPMD/API)](https://gitter.im/openPMD/API) ![Supported Platforms][api-platforms] [![License](https://img.shields.io/badge/license-LGPLv3-blue)](https://www.gnu.org/licenses/lgpl-3.0.html) [![DOI](https://rodare.hzdr.de/badge/DOI/10.14278/rodare.27.svg)](https://doi.org/10.14278/rodare.27) [![CodeFactor](https://www.codefactor.io/repository/github/openpmd/openpmd-api/badge)](https://www.codefactor.io/repository/github/openpmd/openpmd-api) [![Coverage Status](https://coveralls.io/repos/github/openPMD/openPMD-api/badge)](https://coveralls.io/github/openPMD/openPMD-api) [![Documentation Status](https://readthedocs.org/projects/openpmd-api/badge/?version=latest)](https://openpmd-api.readthedocs.io/en/latest/?badge=latest) [![Linux/OSX Build Status dev](https://travis-ci.com/openPMD/openPMD-api.svg?branch=dev)](https://travis-ci.com/openPMD/openPMD-api) [![Windows Build Status dev](https://ci.appveyor.com/api/projects/status/x95q4n620pqk0e0t/branch/dev?svg=true)](https://ci.appveyor.com/project/ax3l/openpmd-api/branch/dev) [![PyPI Wheel Release](https://github.com/openPMD/openPMD-api/workflows/wheels/badge.svg?branch=wheels&event=push)](https://github.com/openPMD/openPMD-api/actions?query=workflow%3Awheels) [![Nightly Packages Status](https://dev.azure.com/axelhuebl/openPMD-api/_apis/build/status/openPMD.openPMD-api?branchName=azure_install&label=nightly%20packages)](https://dev.azure.com/axelhuebl/openPMD-api/_build/latest?definitionId=1&branchName=azure_install) [![Coverity Scan Build Status](https://scan.coverity.com/projects/17602/badge.svg)](https://scan.coverity.com/projects/openpmd-openpmd-api) [api-platforms]: https://img.shields.io/badge/platforms-linux%20|%20osx%20|%20win-blue \"Supported Platforms\" openPMD is an open meta-data schema that provides meaning and self-description for data sets in science and engineering. See [the openPMD standard](https://github.com/openPMD/openPMD-standard) for details of this schema. This library provides a reference API for openPMD data handling. Since openPMD is a schema (or markup) on top of portable, hierarchical file formats, this library implements various backends such as HDF5, ADIOS2 and JSON. Writing & reading through those backends and their associated files are supported for serial and [MPI-parallel](https://www.mpi-forum.org/docs/) workflows. ## Usage ### C++ [![C++17][api-cpp]](https://isocpp.org/) ![C++17 API: Beta][dev-beta] [api-cpp]: https://img.shields.io/badge/language-C%2B%2B17-yellowgreen \"C++17 API\" [dev-beta]: https://img.shields.io/badge/phase-beta-yellowgreen \"Status: Beta\" ```cpp #include <openPMD/openPMD.hpp> #include <iostream> // ... auto s = openPMD::Series(\"samples/git-sample/data%T.h5\", openPMD::Access::READ_ONLY); for( auto const & [step, it] : s.iterations ) { std::cout << \"Iteration: \" << step << \"\\n\"; for( auto const & [name, mesh] : it.meshes ) { std::cout << \" Mesh '\" << name << \"' attributes:\\n\"; for( auto const& val : mesh.attributes() ) std::cout << \" \" << val << '\\n'; } for( auto const & [name, species] : it.particles ) { std::cout << \" Particle species '\" << name << \"' attributes:\\n\"; for( auto const& val : species.attributes() ) std::cout << \" \" << val << '\\n'; } } ``` ### Python [![Python3][api-py3]](https://www.python.org/) ![Python3 API: Beta][dev-beta] [api-py3]: https://img.shields.io/badge/language-Python3-yellowgreen \"Python3 API\" ```py import openpmd_api as io # ... series = io.Series(\"samples/git-sample/data%T.h5\", io.Access.read_only) for k_i, i in series.iterations.items(): print(\"Iteration: {0}\".format(k_i)) for k_m, m in i.meshes.items(): print(\" Mesh '{0}' attributes:\".format(k_m)) for a in m.attributes: print(\" {0}\".format(a)) for k_p, p in i.particles.items(): print(\" Particle species '{0}' attributes:\".format(k_p)) for a in p.attributes: print(\" {0}\".format(a)) ``` ### More! Curious? Our manual shows full [read & write examples](https://openpmd-api.readthedocs.io/en/latest/usage/firstwrite.html), both serial and MPI-parallel! ## Dependencies Required: * CMake 3.22.0+ * C++17 capable compiler, e.g., g++ 7+, clang 7+, MSVC 19.15+, icpc 19+, icpx Shipped internally (downloaded by CMake unless `openPMD_SUPERBUILD=OFF` is set): * [Catch2](https://github.com/catchorg/Catch2) 2.13.10+ ([BSL-1.0](https://github.com/catchorg/Catch2/blob/master/LICENSE.txt)) * [pybind11](https://github.com/pybind/pybind11) 2.13.0+ ([new BSD](https://github.com/pybind/pybind11/blob/master/LICENSE)) * [NLohmann-JSON](https://github.com/nlohmann/json) 3.9.1+ ([MIT](https://github.com/nlohmann/json/blob/develop/LICENSE.MIT)) * [toml11](https://github.com/ToruNiina/toml11) 3.7.1+ ([MIT](https://github.com/ToruNiina/toml11/blob/master/LICENSE)) I/O backends: * [JSON](https://en.wikipedia.org/wiki/JSON) * [HDF5](https://support.hdfgroup.org/HDF5) 1.8.13+ (optional) * [ADIOS2](https://github.com/ornladios/ADIOS2) 2.9.0+ (optional) while those can be built either with or without: * MPI 2.1+, e.g. OpenMPI 1.6.5+ or MPICH2 Optional language bindings: * Python: * Python 3.8 - 3.13 * pybind11 2.13.0+ * numpy 1.15+ * mpi4py 2.1+ (optional, for MPI) * pandas 1.0+ (optional, for dataframes) * dask 2021+ (optional, for dask dataframes) * CUDA C++ (optional, currently used only in tests) ## Installation [![Spack Package](https://img.shields.io/badge/spack.io-openpmd--api-brightgreen)](https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api) [![Conda Package](https://img.shields.io/badge/conda.io-openpmd--api-brightgreen)](https://anaconda.org/conda-forge/openpmd-api) [![Brew Package](https://img.shields.io/badge/brew.sh-openpmd--api-brightgreen)](https://github.com/openPMD/homebrew-openPMD) [![PyPI Package](https://img.shields.io/badge/pypi.org-openpmd--api-brightgreen)](https://pypi.org/project/openPMD-api) [![From Source](https://img.shields.io/badge/from_source-CMake-brightgreen)](https://cmake.org) Our community loves to help each other. Please [report installation problems](https://github.com/openPMD/openPMD-api/issues/new?labels=install&template=install_problem.md) in case you should get stuck. Choose *one* of the install methods below to get started: ### [Spack](https://spack.io) [![Spack Version](https://img.shields.io/spack/v/openpmd-api)](https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api) [![Spack Platforms](https://img.shields.io/badge/platforms-linux%20|%20osx%20-blue)](https://spack.io) [![Spack Use Case](https://img.shields.io/badge/use_case-desktop_%28C%2B%2B,_py%29,_development,_HPC-brightgreen)](https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api) ```bash # optional: +python -adios2 -hdf5 -mpi spack install openpmd-api spack load openpmd-api ``` ### [Conda](https://conda.io) [![Conda Version](https://img.shields.io/conda/vn/conda-forge/openpmd-api)](https://anaconda.org/conda-forge/openpmd-api) [![Conda Platforms](https://img.shields.io/badge/platforms-linux%20|%20osx%20|%20win-blue)](https://anaconda.org/conda-forge/openpmd-api) [![Conda Use Case](https://img.shields.io/badge/use_case-desktop_%28py%29-brightgreen)](https://anaconda.org/conda-forge/openpmd-api) [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/openpmd-api)](https://anaconda.org/conda-forge/openpmd-api) ```bash # optional: OpenMPI support =*=mpi_openmpi* # optional: MPICH support =*=mpi_mpich* conda create -n openpmd -c conda-forge openpmd-api conda activate openpmd ``` ### [Brew](https://brew.sh) [![Brew Version](https://img.shields.io/badge/brew-latest_version-orange)](https://github.com/openPMD/homebrew-openPMD) [![Brew Platforms](https://img.shields.io/badge/platforms-linux%20|%20osx%20-blue)](https://docs.brew.sh/Homebrew-on-Linux) [![Brew Use Case](https://img.shields.io/badge/use_case-desktop_%28C%2B%2B,_py%29-brightgreen)](https://brew.sh) ```bash brew tap openpmd/openpmd brew install openpmd-api ``` ### [PyPI](https://pypi.org) [![PyPI Version](https://img.shields.io/pypi/v/openPMD-api)](https://pypi.org/project/openPMD-api) [![PyPI Platforms](https://img.shields.io/badge/platforms-linux%20|%20osx%20|%20win-blue)](https://pypi.org/project/openPMD-api/#files) [![PyPI Use Case](https://img.shields.io/badge/use_case-desktop_%28py%29-brightgreen)](https://pypi.org/project/openPMD-api) [![PyPI Format](https://img.shields.io/pypi/format/openPMD-api)](https://pypi.org/project/openPMD-api) [![PyPI Downloads](https://img.shields.io/pypi/dm/openPMD-api)](https://pypi.org/project/openPMD-api) On very old macOS versions (<10.9) or on exotic processor architectures, this install method *compiles from source* against the found installations of HDF5, ADIOS2, and/or MPI (in system paths, from other package managers, or loaded via a module system, ...). ```bash # we need pip 19 or newer # optional: --user python3 -m pip install -U pip # optional: --user python3 -m pip install openpmd-api ``` If MPI-support shall be enabled, we always have to recompile: ```bash # optional: --user python3 -m pip install -U pip packaging setuptools wheel python3 -m pip install -U cmake # optional: --user openPMD_USE_MPI=ON python3 -m pip install openpmd-api --no-binary openpmd-api ``` For some exotic architectures and compilers, you might need to disable a compiler feature called [link-time/interprocedural optimization](https://en.wikipedia.org/wiki/Interprocedural_optimization) if you encounter linking problems: ```bash export CMAKE_INTERPROCEDURAL_OPTIMIZATION=OFF # optional: --user python3 -m pip install openpmd-api --no-binary openpmd-api ``` Additional CMake options can be passed via individual environment variables, which need to be prefixed with `openPMD_CMAKE_`. ### From Source [![Source Use Case](https://img.shields.io/badge/use_case-development-brightgreen)](https://cmake.org) openPMD-api can also be built and installed from source using [CMake](https://cmake.org/): ```bash git clone https://github.com/openPMD/openPMD-api.git mkdir openPMD-api-build cd openPMD-api-build # optional: for full tests, with unzip ../openPMD-api/share/openPMD/download_samples.sh # for own install prefix append: # -DCMAKE_INSTALL_PREFIX=$HOME/somepath # for options append: # -DopenPMD_USE_...=... # e.g. for python support add: # -DopenPMD_USE_PYTHON=ON -DPython_EXECUTABLE=$(which python3) cmake ../openPMD-api cmake --build . # optional ctest # sudo might be required for system paths cmake --build . --target install ``` The following options can be added to the `cmake` call to control features. CMake controls options with prefixed `-D`, e.g. `-DopenPMD_USE_MPI=OFF`: | CMake Option | Values | Description | |------------------------------|------------------|------------------------------------------------------------------------------| | `openPMD_USE_MPI` | **AUTO**/ON/OFF | Parallel, Multi-Node I/O for clusters | | `openPMD_USE_HDF5` | **AUTO**/ON/OFF | HDF5 backend (`.h5` files) | | `openPMD_USE_ADIOS2` | **AUTO**/ON/OFF | ADIOS2 backend (`.bp` files in BP3, BP4 or higher) | | `openPMD_USE_PYTHON` | **AUTO**/ON/OFF | Enable Python bindings | | `openPMD_USE_INVASIVE_TESTS` | ON/**OFF** | Enable unit tests that modify source code <sup>1</sup> | | `openPMD_USE_VERIFY` | **ON**/OFF | Enable internal VERIFY (assert) macro independent of build type <sup>2</sup> | | `openPMD_INSTALL` | **ON**/OFF | Add installation targets | | `openPMD_INSTALL_RPATH` | **ON**/OFF | Add RPATHs to installed binaries | | `Python_EXECUTABLE` | (newest found) | Path to Python executable | <sup>1</sup> *e.g. changes C++ visibility keywords, breaks MSVC* <sup>2</sup> *this includes most pre-/post-condition checks, disabling without specific cause is highly discouraged* Additionally, the following libraries are downloaded via [FetchContent](https://cmake.org/cmake/help/latest/module/FetchContent.html) during the configuration of the project or, if the corresponding `<PACKAGENAME>_ROOT` variable is provided, can be provided externally: * [Catch2](https://github.com/catchorg/Catch2) (2.13.10+) * [PyBind11](https://github.com/pybind/pybind11) (2.13.0+) * [NLohmann-JSON](https://github.com/nlohmann/json) (3.9.1+) * [toml11](https://github.com/ToruNiina/toml11) (3.7.1+) By default, this will build as a shared library (`libopenPMD.[so|dylib|dll]`) and installs also its headers. In order to build a static library, append `-DBUILD_SHARED_LIBS=OFF` to the `cmake` command. You can only build a static or a shared library at a time. By default, the `Release` version is built. In order to build with debug symbols, pass `-DCMAKE_BUILD_TYPE=Debug` to your `cmake` command. By default, tests, examples and command line tools are built. In order to skip building those, pass ``OFF`` to these ``cmake`` options: | CMake Option | Values | Description | |-------------------------------|------------|--------------------------| | `openPMD_BUILD_TESTING` | **ON**/OFF | Build tests | | `openPMD_BUILD_EXAMPLES` | **ON**/OFF | Build examples | | `openPMD_BUILD_CLI_TOOLS` | **ON**/OFF | Build command-line tools | | `openPMD_USE_CUDA_EXAMPLES` | ON/**OFF** | Use CUDA in examples | ## Linking to your project The install will contain header files and libraries in the path set with `-DCMAKE_INSTALL_PREFIX`. ### CMake If your project is using CMake for its build, one can conveniently use our provided `openPMDConfig.cmake` package, which is installed alongside the library. First set the following environment hint if openPMD-api was *not* installed in a system path: ```bash # optional: only needed if installed outside of system paths export CMAKE_PREFIX_PATH=$HOME/somepath:$CMAKE_PREFIX_PATH ``` Use the following lines in your project's `CMakeLists.txt`: ```cmake # supports: COMPONENTS MPI NOMPI HDF5 ADIOS2 find_package(openPMD 0.17.0 CONFIG) if(openPMD_FOUND) target_link_libraries(YourTarget PRIVATE openPMD::openPMD) endif() ``` *Alternatively*, add the openPMD-api repository source directly to your project and use it via: ```cmake add_subdirectory(\"path/to/source/of/openPMD-api\") target_link_libraries(YourTarget PRIVATE openPMD::openPMD) ``` For development workflows, you can even automatically download and build openPMD-api from within a depending CMake project. Just replace the `add_subdirectory` call with: ```cmake include(FetchContent) set(CMAKE_POLICY_DEFAULT_CMP0077 NEW) set(openPMD_BUILD_CLI_TOOLS OFF) set(openPMD_BUILD_EXAMPLES OFF) set(openPMD_BUILD_TESTING OFF) set(openPMD_BUILD_SHARED_LIBS OFF) # precedence over BUILD_SHARED_LIBS if needed set(openPMD_INSTALL OFF) # or instead use: # set(openPMD_INSTALL ${BUILD_SHARED_LIBS}) # only install if used as a shared library set(openPMD_USE_PYTHON OFF) FetchContent_Declare(openPMD GIT_REPOSITORY \"https://github.com/openPMD/openPMD-api.git\" GIT_TAG \"0.17.0\") FetchContent_MakeAvailable(openPMD) ``` ### Manually If your (Linux/OSX) project is build by calling the compiler directly or uses a manually written `Makefile`, consider using our `openPMD.pc` helper file for `pkg-config`, which are installed alongside the library. First set the following environment hint if openPMD-api was *not* installed in a system path: ```bash # optional: only needed if installed outside of system paths export PKG_CONFIG_PATH=$HOME/somepath/lib/pkgconfig:$PKG_CONFIG_PATH ``` Additional linker and compiler flags for your project are available via: ```bash # switch to check if openPMD-api was build as static library # (via BUILD_SHARED_LIBS=OFF) or as shared library (default) if [ \"$(pkg-config --variable=static openPMD)\" == \"true\" ] then pkg-config --libs --static openPMD # -L/usr/local/lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lopenPMD -pthread /usr/lib/libmpi.so -pthread /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so /usr/lib/libmpi.so /usr/lib/x86_64-linux-gnu/hdf5/openmpi/libhdf5.so /usr/lib/x86_64-linux-gnu/libsz.so /usr/lib/x86_64-linux-gnu/libz.so /usr/lib/x86_64-linux-gnu/libdl.so /usr/lib/x86_64-linux-gnu/libm.so -pthread /usr/lib/libmpi.so -pthread /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so /usr/lib/libmpi.so else pkg-config --libs openPMD # -L${HOME}/somepath/lib -lopenPMD fi pkg-config --cflags openPMD # -I${HOME}/somepath/include ``` ## Author Contributions openPMD-api is developed by many people. It was initially started by the [Computational Radiation Physics Group](https://hzdr.de/crp) at [HZDR](https://www.hzdr.de/) as successor to [libSplash](https://github.com/ComputationalRadiationPhysics/libSplash/), generalizing the [successful HDF5 & ADIOS1 implementations](https://arxiv.org/abs/1706.00522) in [PIConGPU](https://github.com/ComputationalRadiationPhysics/picongpu). The following people and institutions [contributed](https://github.com/openPMD/openPMD-api/graphs/contributors) to openPMD-api: * [Axel Huebl (LBNL, previously HZDR)](https://github.com/ax3l): project lead, releases, documentation, automated CI/CD, Python bindings, Dask, installation & packaging, prior reference implementations * [Franz Poeschel (CASUS)](https://github.com/franzpoeschel): JSON & ADIOS2 backend, data staging/streaming, reworked class design * [Fabian Koller (HZDR)](https://github.com/C0nsultant): initial library design and implementation with HDF5 & ADIOS1 backend * [Junmin Gu (LBNL)](https://github.com/guj): non-collective parallel I/O fixes, ADIOS improvements, benchmarks Maintained by the following research groups: * [Computational Radiation Physics (CRD)](https://www.casus.science/casus/team/) at CASUS/HZDR, led by [Michael Bussmann](https://github.com/bussmann) * [Accelerator Modeling Program (AMP)](https://atap.lbl.gov/accelerator-modeling-program/) at LBNL, led by [Jean-Luc Vay](https://github.com/jlvay) * [Scientific Data Management (SDM)](https://crd.lbl.gov/divisions/scidata/sdm/) at LBNL, led by [Kesheng (John) Wu](https://github.com/john18) Further thanks go to improvements and contributions from: * [Carsten Fortmann-Grote (EU XFEL GmbH, now MPI-EvolBio)](https://github.com/CFGrote): draft of our Python unit tests * [Dominik Stańczak (Warsaw University of Technology)](https://github.com/StanczakDominik): documentation improvements * [Ray Donnelly (Anaconda, Inc.)](https://github.com/mingwandroid): support on conda packaging and libc++ quirks * [James Amundson (FNAL)](https://github.com/amundson): compile fix for newer compilers * [René Widera (HZDR)](https://github.com/psychocoderHPC): design improvements for initial API design * [Erik Zenker (HZDR)](https://github.com/erikzenker): design improvements for initial API design * [Sergei Bastrakov (HZDR)](https://github.com/sbastrakov): documentation improvements (windows) * [Rémi Lehe (LBNL)](https://github.com/RemiLehe): package integration testing on macOS and Linux * [Lígia Diana Amorim (LBNL)](https://github.com/LDAmorim): package integration testing on macOS * [Kseniia Bastrakova (HZDR)](https://github.com/KseniaBastrakova): compatibility testing * [Richard Pausch (HZDR)](https://github.com/PrometheusPi): compatibility testing, documentation improvements * [Paweł Ordyna (HZDR)](https://github.com/pordyna): report on NVCC warnings * [Dmitry Ganyushin (ORNL)](https://github.com/dmitry-ganyushin): Dask prototyping & ADIOS2 benchmarking * [John Kirkham (NVIDIA)](https://github.com/jakirkham): Dask guidance & reviews * [Erik Schnetter (PITP)](https://github.com/eschnett): C++ API bug fixes * [Jean Luca Bez (LBNL)](https://github.com/jeanbez): HDF5 performance tuning * [Bernhard Manfred Gruber (CERN)](https://github.com/bernhardmgruber): CMake fix for parallel HDF5 * [Nils Schild (IPP)](https://github.com/DerNils-git): CMake improvements for subprojects ### Grants The openPMD-api authors acknowledge support via the following programs. Supported by the CAMPA collaboration, a project of the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research and Office of High Energy Physics, Scientific Discovery through Advanced Computing (SciDAC) program. Previously supported by the Consortium for Advanced Modeling of Particles Accelerators (CAMPA), funded by the U.S. DOE Office of Science under Contract No. DE-AC02-05CH11231. Supported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort of two U.S. Department of Energy organizations (Office of Science and the National Nuclear Security Administration). This project has received funding from the European Unions Horizon 2020 research and innovation programme under grant agreement No 654220. This work was partially funded by the Center of Advanced Systems Understanding (CASUS), which is financed by Germany's Federal Ministry of Education and Research (BMBF) and by the Saxon Ministry for Science, Culture and Tourism (SMWK) with tax funds on the basis of the budget approved by the Saxon State Parliament. Supported by the HElmholtz Laser Plasma Metadata Initiative (HELPMI) project (ZT-I-PF-3-066), funded by the \"Initiative and Networking Fund\" of the Helmholtz Association in the framework of the \"Helmholtz Metadata Collaboration\" project call 2022. ### Transitive Contributions openPMD-api stands on the shoulders of giants and we are grateful for the following projects included as direct dependencies: * [ADIOS2](https://github.com/ornladios/ADIOS2) by [S. Klasky, N. Podhorszki, W.F. Godoy (ORNL), team, collaborators](https://csmd.ornl.gov/adios) and [contributors](https://github.com/ornladios/ADIOS2/graphs/contributors) * [Catch2](https://github.com/catchorg/Catch2) by [Phil Nash](https://github.com/philsquared), [Martin Hořeňovský](https://github.com/horenmar) and [contributors](https://github.com/catchorg/Catch2/graphs/contributors) * HDF5 by [the HDF group](https://www.hdfgroup.org) and community * [json](https://github.com/nlohmann/json) by [Niels Lohmann](https://github.com/nlohmann) and [contributors](https://github.com/nlohmann/json/graphs/contributors) * [toml11](https://github.com/ToruNiina/toml11) by [Toru Niina](https://github.com/ToruNiina) and [contributors](https://github.com/ToruNiina/toml11#Contributors) * [pybind11](https://github.com/pybind/pybind11) by [Wenzel Jakob (EPFL)](https://github.com/wjakob) and [contributors](https://github.com/pybind/pybind11/graphs/contributors) * all contributors to the evolution of modern C++ and early library preview developers, e.g. [Michael Park (Facebook)](https://github.com/mpark) * the [CMake build system](https://cmake.org) and [contributors](https://github.com/Kitware/CMake/blob/master/Copyright.txt) * packaging support by the [conda-forge](https://conda-forge.org), [PyPI](https://pypi.org) and [Spack](https://spack.io) communities, among others * the [openPMD-standard](https://github.com/openPMD/openPMD-standard) by [Axel Huebl (HZDR, now LBNL)](https://github.com/ax3l) and [contributors](https://github.com/openPMD/openPMD-standard/blob/latest/AUTHORS.md)\n",
                "dependencies": "# Preamble #################################################################### # cmake_minimum_required(VERSION 3.22.0) project(openPMD VERSION 0.17.0) # LANGUAGES CXX # the openPMD \"markup\"/\"schema\" standard version set(openPMD_STANDARD_VERSION 1.1.0) include(${openPMD_SOURCE_DIR}/cmake/openPMDFunctions.cmake) # CMake policies ############################################################## # # CMake 3.24+ tarball download robustness # https://cmake.org/cmake/help/latest/module/ExternalProject.html#url if(POLICY CMP0135) cmake_policy(SET CMP0135 NEW) endif() # No in-Source builds ######################################################### # # In-source builds clutter up the source directory and lead to mistakes with # generated includes if(openPMD_SOURCE_DIR STREQUAL openPMD_BINARY_DIR) message(FATAL_ERROR \"In-source builds are not possible. \" \"Please remove the CMakeFiles/ directory and CMakeCache.txt file. \" \"Then run CMake in a temporary build directory. \" \"Learn more: https://hsf-training.github.io/hsf-training-cmake-webpage/02-building/index.html\") endif() # Project structure ########################################################### # get_property(isMultiConfig GLOBAL PROPERTY GENERATOR_IS_MULTI_CONFIG) # temporary build directories if(NOT openPMD_ARCHIVE_OUTPUT_DIRECTORY) if(CMAKE_ARCHIVE_OUTPUT_DIRECTORY) set(openPMD_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_ARCHIVE_OUTPUT_DIRECTORY}) else() set(openPMD_ARCHIVE_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/lib\") endif() endif() if(NOT openPMD_LIBRARY_OUTPUT_DIRECTORY) if(CMAKE_LIBRARY_OUTPUT_DIRECTORY) set(openPMD_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_LIBRARY_OUTPUT_DIRECTORY}) else() set(openPMD_LIBRARY_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/lib\") endif() endif() if(NOT openPMD_RUNTIME_OUTPUT_DIRECTORY) if(CMAKE_RUNTIME_OUTPUT_DIRECTORY) set(openPMD_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}) else() set(openPMD_RUNTIME_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/bin\") endif() endif() if(NOT openPMD_PDB_OUTPUT_DIRECTORY) if(CMAKE_PDB_OUTPUT_DIRECTORY) set(openPMD_PDB_OUTPUT_DIRECTORY ${CMAKE_PDB_OUTPUT_DIRECTORY}) else() set(openPMD_PDB_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY}) endif() endif() if(NOT openPMD_COMPILE_PDB_OUTPUT_DIRECTORY) if(CMAKE_COMPILE_PDB_OUTPUT_DIRECTORY) set(openPMD_COMPILE_PDB_OUTPUT_DIRECTORY ${CMAKE_COMPILE_PDB_OUTPUT_DIRECTORY}) else() set(openPMD_COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY}) endif() endif() # install directories if(NOT CMAKE_INSTALL_LIBDIR AND NOT WIN32) include(GNUInstallDirs) endif() if(NOT openPMD_INSTALL_PREFIX) if(CMAKE_INSTALL_PREFIX) set(openPMD_INSTALL_PREFIX \"${CMAKE_INSTALL_PREFIX}\") else() message(FATAL_ERROR \"openPMD_INSTALL_PREFIX / CMAKE_INSTALL_PREFIX not set.\") endif() endif() if(NOT openPMD_INSTALL_BINDIR) if(CMAKE_INSTALL_BINDIR) set(openPMD_INSTALL_BINDIR \"${CMAKE_INSTALL_BINDIR}\") else() set(openPMD_INSTALL_BINDIR bin) endif() endif() if(NOT openPMD_INSTALL_INCLUDEDIR) if(CMAKE_INSTALL_INCLUDEDIR) set(openPMD_INSTALL_INCLUDEDIR \"${CMAKE_INSTALL_INCLUDEDIR}\") else() set(openPMD_INSTALL_INCLUDEDIR include) endif() endif() if(NOT openPMD_INSTALL_LIBDIR) if(CMAKE_INSTALL_LIBDIR) set(openPMD_INSTALL_LIBDIR \"${CMAKE_INSTALL_LIBDIR}\") else() if(WIN32) set(openPMD_INSTALL_LIBDIR Lib) else() set(openPMD_INSTALL_LIBDIR lib) endif() endif() endif() if(NOT openPMD_INSTALL_CMAKEDIR) if(CMAKE_INSTALL_CMAKEDIR) set(openPMD_INSTALL_CMAKEDIR \"${CMAKE_INSTALL_CMAKEDIR}/openPMD\") else() if(WIN32) set(openPMD_INSTALL_CMAKEDIR \"cmake\") else() set(openPMD_INSTALL_CMAKEDIR \"${CMAKE_INSTALL_LIBDIR}/cmake/openPMD\") endif() endif() endif() # Options and Variants ######################################################## # function(openpmd_option name description default) set(openPMD_USE_${name} ${default} CACHE STRING \"${description}\") set_property(CACHE openPMD_USE_${name} PROPERTY STRINGS \"ON;TRUE;AUTO;OFF;FALSE\" ) # list of all possible options set(openPMD_CONFIG_OPTIONS ${openPMD_CONFIG_OPTIONS} ${name} PARENT_SCOPE) endfunction() openpmd_option(MPI \"Parallel, Multi-Node I/O for clusters\" AUTO) openpmd_option(HDF5 \"HDF5 backend (.h5 files)\" AUTO) openpmd_option(ADIOS2 \"ADIOS2 backend (.bp files)\" AUTO) openpmd_option(PYTHON \"Enable Python bindings\" AUTO) option(openPMD_INSTALL \"Add installation targets\" ON) option(openPMD_INSTALL_RPATH \"Add RPATHs to installed binaries\" ON) option(openPMD_HAVE_PKGCONFIG \"Generate a .pc file for pkg-config\" ON) # superbuild control option(openPMD_SUPERBUILD \"Download & build extra dependencies\" ON) option(openPMD_USE_INTERNAL_CATCH \"Use internally shipped Catch2\" ${openPMD_SUPERBUILD}) option(openPMD_USE_INTERNAL_PYBIND11 \"Use internally shipped pybind11\" ${openPMD_SUPERBUILD}) option(openPMD_USE_INTERNAL_JSON \"Use internally shipped nlohmann-json\" ${openPMD_SUPERBUILD}) option(openPMD_USE_INTERNAL_TOML11 \"Use internally shipped toml11\" ${openPMD_SUPERBUILD}) option(openPMD_USE_INVASIVE_TESTS \"Enable unit tests that modify source code\" OFF) option(openPMD_USE_VERIFY \"Enable internal VERIFY (assert) macro independent of build type\" ON) option(openPMD_USE_FILESYSTEM_HEADER \"Enable filesystem header. May be disabled for old compiler versions.\" OFF) mark_as_advanced(openPMD_USE_FILESYSTEM_HEADER) set(CMAKE_CONFIGURATION_TYPES \"Release;Debug;MinSizeRel;RelWithDebInfo\") if(NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE \"Release\") endif() include(CMakeDependentOption) # change CMake default (static libs): # build shared libs if supported by target platform get_property(SHARED_LIBS_SUPPORTED GLOBAL PROPERTY TARGET_SUPPORTS_SHARED_LIBS) if(DEFINED BUILD_SHARED_LIBS) set(openPMD_BUILD_SHARED_LIBS_DEFAULT ${BUILD_SHARED_LIBS}) else() set(openPMD_BUILD_SHARED_LIBS_DEFAULT ${SHARED_LIBS_SUPPORTED}) endif() option(openPMD_BUILD_SHARED_LIBS \"Build shared libraries (so/dylib/dll).\" ${openPMD_BUILD_SHARED_LIBS_DEFAULT}) if(openPMD_BUILD_SHARED_LIBS AND NOT SHARED_LIBS_SUPPORTED) message(FATAL_ERROR \"openPMD_BUILD_SHARED_LIBS requested but not supported by platform\") endif() # Testing logic with possibility to overwrite on a project basis in superbuilds include(CTest) mark_as_advanced(BUILD_TESTING) # automatically defined, default: ON if(DEFINED BUILD_TESTING) set(openPMD_BUILD_TESTING_DEFAULT ${BUILD_TESTING}) else() set(openPMD_BUILD_TESTING_DEFAULT ON) endif() option(openPMD_BUILD_TESTING \"Build the openPMD tests\" ${openPMD_BUILD_TESTING_DEFAULT}) # deprecated: backwards compatibility to <= 0.13.* if(NOT DEFINED BUILD_CLI_TOOLS) set(BUILD_CLI_TOOLS ON) endif() if(NOT DEFINED BUILD_EXAMPLES) set(BUILD_EXAMPLES ON) endif() option(openPMD_BUILD_CLI_TOOLS \"Build the command line tools\" ${BUILD_CLI_TOOLS}) option(openPMD_BUILD_EXAMPLES \"Build the examples\" ${BUILD_EXAMPLES}) openpmd_option(CUDA_EXAMPLES \"Use CUDA in examples\" OFF) # Helper Functions ############################################################ # # C++ standard: requirements for a target function(openpmd_cxx_required target) target_compile_features(${target} PUBLIC cxx_std_17) set_target_properties(${target} PROPERTIES CXX_EXTENSIONS OFF CXX_STANDARD_REQUIRED ON ) endfunction() # CUDA C++ standard: requirements for a target function(openpmd_cuda_required target) target_compile_features(${target} PUBLIC cuda_std_17) set_target_properties(${target} PROPERTIES CUDA_SEPARABLE_COMPILATION ON CUDA_EXTENSIONS OFF CUDA_STANDARD_REQUIRED ON) endfunction() # Dependencies ################################################################ # message(STATUS \"openPMD-api superbuild: ${openPMD_SUPERBUILD}\") # external library: MPI (optional) # Implementation quirks for BullMPI, Clang+MPI and Brew's MPICH # definitely w/o MPI::MPI_C: # brew's MPICH with C-flag work-arounds - errors AppleClang for CXX targets # https://github.com/Homebrew/homebrew-core/issues/80465 # https://lists.mpich.org/pipermail/discuss/2020-January/005863.html # sometimes needed MPI::MPI_C in the past: # Clang+MPI: Potentially needed MPI::MPI_C targets in the past # (exact MPI flavor & Clang version lost) # BullMPI: PUBLIC dependency to MPI::MPI_CXX is missing in MPI::MPI_C target set(openPMD_MPI_LINK_C_DEFAULT OFF) option(openPMD_MPI_LINK_C \"Also link the MPI C targets\" ${openPMD_MPI_LINK_C_DEFAULT}) mark_as_advanced(openPMD_MPI_LINK_C) set(openPMD_MPI_NEED_COMPONENTS CXX) set(openPMD_MPI_TARGETS MPI::MPI_CXX) if(openPMD_MPI_LINK_C) set(openPMD_MPI_NEED_COMPONENTS C ${openPMD_MPI_NEED_COMPONENTS}) set(openPMD_MPI_TARGETS MPI::MPI_C ${openPMD_MPI_TARGETS}) endif() if(openPMD_USE_MPI STREQUAL AUTO) find_package(MPI COMPONENTS ${openPMD_MPI_NEED_COMPONENTS}) if(MPI_FOUND) set(openPMD_HAVE_MPI TRUE) else() set(openPMD_HAVE_MPI FALSE) endif() elseif(openPMD_USE_MPI) find_package(MPI REQUIRED COMPONENTS ${openPMD_MPI_NEED_COMPONENTS}) set(openPMD_HAVE_MPI TRUE) else() set(openPMD_HAVE_MPI FALSE) endif() # external library: nlohmann-json (required) include(${openPMD_SOURCE_DIR}/cmake/dependencies/json.cmake) add_library(openPMD::thirdparty::nlohmann_json INTERFACE IMPORTED) target_link_libraries(openPMD::thirdparty::nlohmann_json INTERFACE nlohmann_json::nlohmann_json) # external library: toml11 (required) include(${openPMD_SOURCE_DIR}/cmake/dependencies/toml11.cmake) add_library(openPMD::thirdparty::toml11 INTERFACE IMPORTED) target_link_libraries(openPMD::thirdparty::toml11 INTERFACE toml11::toml11) # external: CUDA (optional) if(openPMD_BUILD_EXAMPLES) # currently only used in examples if(openPMD_USE_CUDA_EXAMPLES STREQUAL AUTO) find_package(CUDAToolkit) elseif(openPMD_USE_CUDA_EXAMPLES) find_package(CUDAToolkit REQUIRED) endif() endif() if(CUDAToolkit_FOUND) enable_language(CUDA) set(openPMD_HAVE_CUDA_EXAMPLES TRUE) else() set(openPMD_HAVE_CUDA_EXAMPLES FALSE) endif() # external library: HDF5 (optional) # note: in the new hdf5-cmake.config files, major releases like # 1.8, 1.10 and 1.12 are not marked compatible versions # We could use CMake 3.19.0+ version ranges, but: # - this issues a Wdev warning with FindHDF5.cmake # - does not work at least with HDF5 1.10: # Could not find a configuration file for package \"HDF5\" that is compatible # with requested version range \"1.8.13...1.12\". # The following configuration files were considered but not accepted: # ../share/cmake/hdf5/hdf5-config.cmake, version: 1.10.7 # - thus, we do our own HDF5_VERSION check... if(openPMD_USE_HDF5 STREQUAL AUTO) set(HDF5_PREFER_PARALLEL ${openPMD_HAVE_MPI}) find_package(HDF5 COMPONENTS C) if(HDF5_FOUND) set(openPMD_HAVE_HDF5 TRUE) else() set(openPMD_HAVE_HDF5 FALSE) endif() elseif(openPMD_USE_HDF5) set(HDF5_PREFER_PARALLEL ${openPMD_HAVE_MPI}) find_package(HDF5 REQUIRED COMPONENTS C) set(openPMD_HAVE_HDF5 TRUE) else() set(openPMD_HAVE_HDF5 FALSE) endif() # HDF5 checks string(CONCAT openPMD_HDF5_STATUS \"\") # version: lower limit if(openPMD_HAVE_HDF5) if(\"${HDF5_VERSION}\" STREQUAL \"\") message(WARNING \"HDF5_VERSION is empty. Now assuming it is 1.8.13 or newer.\") else() if(HDF5_VERSION VERSION_LESS 1.8.13) string(CONCAT openPMD_HDF5_STATUS \"Found HDF5 version ${HDF5_VERSION} is too old. At least \" \"version 1.8.13 is required.\\n\") endif() endif() endif() # we imply support for parallel I/O if MPI variant is ON if(openPMD_HAVE_MPI AND openPMD_HAVE_HDF5 AND NOT HDF5_IS_PARALLEL # FindHDF5.cmake AND NOT HDF5_ENABLE_PARALLEL # hdf5-config.cmake ) string(CONCAT openPMD_HDF5_STATUS \"Found MPI but only serial version of HDF5. Either set \" \"openPMD_USE_MPI=OFF to disable MPI or set openPMD_USE_HDF5=OFF \" \"to disable HDF5 or provide a parallel install of HDF5.\\n\") endif() # HDF5 includes mpi.h in the public header H5public.h if parallel if(openPMD_HAVE_HDF5 AND (HDF5_IS_PARALLEL OR HDF5_ENABLE_PARALLEL) AND NOT openPMD_HAVE_MPI) string(CONCAT openPMD_HDF5_STATUS \"Found only parallel version of HDF5 but no MPI. Either set \" \"openPMD_USE_MPI=ON to force using MPI or set openPMD_USE_HDF5=OFF \" \"to disable HDF5 or provide a serial install of HDF5.\\n\") endif() if(openPMD_HDF5_STATUS) string(CONCAT openPMD_HDF5_STATUS ${openPMD_HDF5_STATUS} \"If you manually installed a version of HDF5 in \" \"a non-default path, add its installation prefix to the \" \"environment variable CMAKE_PREFIX_PATH to find it: \" \"https://cmake.org/cmake/help/latest/envvar/CMAKE_PREFIX_PATH.html\") if(openPMD_USE_HDF5 STREQUAL AUTO) message(WARNING \"${openPMD_HDF5_STATUS}\") set(openPMD_HAVE_HDF5 FALSE) elseif(openPMD_USE_HDF5) message(FATAL_ERROR \"${openPMD_HDF5_STATUS}\") endif() endif() # external library: ADIOS2 (optional) set(openPMD_REQUIRED_ADIOS2_COMPONENTS CXX) if(openPMD_HAVE_MPI) list(APPEND openPMD_REQUIRED_ADIOS2_COMPONENTS MPI) endif() if(openPMD_USE_ADIOS2 STREQUAL AUTO) find_package(ADIOS2 2.9.0 CONFIG COMPONENTS ${openPMD_REQUIRED_ADIOS2_COMPONENTS}) if(ADIOS2_FOUND) set(openPMD_HAVE_ADIOS2 TRUE) else() set(openPMD_HAVE_ADIOS2 FALSE) endif() elseif(openPMD_USE_ADIOS2) find_package(ADIOS2 2.9.0 REQUIRED CONFIG COMPONENTS ${openPMD_REQUIRED_ADIOS2_COMPONENTS}) set(openPMD_HAVE_ADIOS2 TRUE) else() set(openPMD_HAVE_ADIOS2 FALSE) endif() unset(openPMD_REQUIRED_ADIOS2_COMPONENTS) # external library: pybind11 (optional) include(${openPMD_SOURCE_DIR}/cmake/dependencies/pybind11.cmake) # Targets ##################################################################### # set(CORE_SOURCE src/config.cpp src/ChunkInfo.cpp src/Dataset.cpp src/Datatype.cpp src/Error.cpp src/Format.cpp src/Iteration.cpp src/IterationEncoding.cpp src/Mesh.cpp src/ParticlePatches.cpp src/ParticleSpecies.cpp src/Record.cpp src/ReadIterations.cpp src/RecordComponent.cpp src/Series.cpp src/UnitDimension.cpp src/version.cpp src/auxiliary/Date.cpp src/auxiliary/Filesystem.cpp src/auxiliary/JSON.cpp src/auxiliary/JSONMatcher.cpp src/auxiliary/Mpi.cpp src/backend/Attributable.cpp src/backend/BaseRecordComponent.cpp src/backend/MeshRecordComponent.cpp src/backend/PatchRecord.cpp src/backend/PatchRecordComponent.cpp src/backend/Writable.cpp src/benchmark/mpi/OneDimensionalBlockSlicer.cpp src/helper/list_series.cpp src/snapshots/ContainerImpls.cpp src/snapshots/ContainerTraits.cpp src/snapshots/IteratorHelpers.cpp src/snapshots/IteratorTraits.cpp src/snapshots/RandomAccessIterator.cpp src/snapshots/Snapshots.cpp src/snapshots/StatefulIterator.cpp) set(IO_SOURCE src/IO/AbstractIOHandler.cpp src/IO/AbstractIOHandlerImpl.cpp src/IO/AbstractIOHandlerHelper.cpp src/IO/Access.cpp src/IO/DummyIOHandler.cpp src/IO/IOTask.cpp src/IO/FlushParams.cpp src/IO/HDF5/HDF5IOHandler.cpp src/IO/HDF5/ParallelHDF5IOHandler.cpp src/IO/HDF5/HDF5Auxiliary.cpp src/IO/JSON/JSONIOHandler.cpp src/IO/JSON/JSONIOHandlerImpl.cpp src/IO/JSON/JSONFilePosition.cpp src/IO/ADIOS/ADIOS2IOHandler.cpp src/IO/ADIOS/ADIOS2PreloadAttributes.cpp src/IO/ADIOS/ADIOS2PreloadVariables.cpp src/IO/ADIOS/ADIOS2File.cpp src/IO/ADIOS/ADIOS2Auxiliary.cpp src/IO/InvalidatableFile.cpp) # library if(openPMD_BUILD_SHARED_LIBS) set(_openpmd_lib_type SHARED) else() set(_openpmd_lib_type STATIC) endif() add_library(openPMD ${_openpmd_lib_type} ${CORE_SOURCE} ${IO_SOURCE}) add_library(openPMD::openPMD ALIAS openPMD) # properties openpmd_cxx_required(openPMD) set_target_properties(openPMD PROPERTIES COMPILE_PDB_NAME openPMD ARCHIVE_OUTPUT_DIRECTORY ${openPMD_ARCHIVE_OUTPUT_DIRECTORY} LIBRARY_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY} RUNTIME_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} PDB_OUTPUT_DIRECTORY ${openPMD_PDB_OUTPUT_DIRECTORY} COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_COMPILE_PDB_OUTPUT_DIRECTORY} POSITION_INDEPENDENT_CODE ON WINDOWS_EXPORT_ALL_SYMBOLS ON ) # note: same as above, but for Multi-Config generators if(isMultiConfig) # this is a tweak for setup.py to pick up our libs & pybind module properly # this assumes there will only be one config built option(openPMD_BUILD_NO_CFG_SUBPATH \"For multi-config builds, do not appends the config to build dir\" OFF) mark_as_advanced(openPMD_BUILD_NO_CFG_SUBPATH) foreach(CFG IN LISTS CMAKE_CONFIGURATION_TYPES) string(TOUPPER \"${CFG}\" CFG_UPPER) if(openPMD_BUILD_NO_CFG_SUBPATH) # for setup.py set(CFG_PATH \"\") else() set(CFG_PATH \"/${CFG}\") endif() set_target_properties(openPMD PROPERTIES COMPILE_PDB_NAME_${CFG_UPPER} openPMD ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}${CFG_PATH} LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_LIBRARY_OUTPUT_DIRECTORY}${CFG_PATH} RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}${CFG_PATH} PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PDB_OUTPUT_DIRECTORY}${CFG_PATH} COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_COMPILE_PDB_OUTPUT_DIRECTORY}${CFG_PATH} ) endforeach() endif() set(_cxx_msvc \"$<AND:$<COMPILE_LANGUAGE:CXX>,$<CXX_COMPILER_ID:MSVC>>\") set(_msvc_1914 \"$<VERSION_GREATER_EQUAL:$<CXX_COMPILER_VERSION>,19.14>\") set(_msvc_options) list(APPEND _msvc_options $<${_cxx_msvc}:/bigobj> $<${_cxx_msvc}:$<${_msvc_1914}:/Zc:__cplusplus>> ) target_compile_options(openPMD PUBLIC ${_msvc_options}) # own headers target_include_directories(openPMD PUBLIC $<BUILD_INTERFACE:${openPMD_SOURCE_DIR}/include> $<BUILD_INTERFACE:${openPMD_BINARY_DIR}/include> $<INSTALL_INTERFACE:include> ) # Catch2 for unit tests if(openPMD_BUILD_TESTING) include(${openPMD_SOURCE_DIR}/cmake/dependencies/catch.cmake) add_library(openPMD::thirdparty::Catch2 INTERFACE IMPORTED) target_link_libraries(openPMD::thirdparty::Catch2 INTERFACE Catch2::Catch2) endif() if(openPMD_HAVE_MPI) target_link_libraries(openPMD PUBLIC ${openPMD_MPI_TARGETS}) endif() # JSON Backend and User-Facing Runtime Options #target_link_libraries(openPMD PRIVATE openPMD::thirdparty::nlohmann_json) target_include_directories(openPMD SYSTEM PRIVATE $<TARGET_PROPERTY:openPMD::thirdparty::nlohmann_json,INTERFACE_INCLUDE_DIRECTORIES> $<TARGET_PROPERTY:openPMD::thirdparty::toml11,INTERFACE_INCLUDE_DIRECTORIES>) # HDF5 Backend # TODO: Once we require CMake 3.20+, simply link hdf5::hdf5 C lib target if(openPMD_HAVE_HDF5) target_link_libraries(openPMD PUBLIC ${HDF5_LIBRARIES}) target_include_directories(openPMD SYSTEM PRIVATE ${HDF5_INCLUDE_DIRS}) target_compile_definitions(openPMD PRIVATE ${HDF5_DEFINITIONS}) endif() # ADIOS2 Backend if(openPMD_HAVE_ADIOS2) if(openPMD_HAVE_MPI) target_link_libraries(openPMD PUBLIC adios2::cxx11_mpi) else() target_link_libraries(openPMD PUBLIC adios2::cxx11) endif() endif() # Runtime parameter and API status checks (\"asserts\") if(openPMD_USE_VERIFY) target_compile_definitions(openPMD PRIVATE openPMD_USE_VERIFY=1) else() target_compile_definitions(openPMD PRIVATE openPMD_USE_VERIFY=0) endif() # python bindings if(openPMD_HAVE_PYTHON) add_library(openPMD.py MODULE src/binding/python/openPMD.cpp src/binding/python/auxiliary.cpp src/binding/python/Access.cpp src/binding/python/Attributable.cpp src/binding/python/BaseRecordComponent.cpp src/binding/python/ChunkInfo.cpp src/binding/python/Dataset.cpp src/binding/python/Datatype.cpp src/binding/python/Error.cpp src/binding/python/Helper.cpp src/binding/python/Iteration.cpp src/binding/python/IterationEncoding.cpp src/binding/python/Mesh.cpp src/binding/python/ParticlePatches.cpp src/binding/python/ParticleSpecies.cpp src/binding/python/PatchRecord.cpp src/binding/python/PatchRecordComponent.cpp src/binding/python/Record.cpp src/binding/python/RecordComponent.cpp src/binding/python/MeshRecordComponent.cpp src/binding/python/Series.cpp src/binding/python/UnitDimension.cpp ) target_link_libraries(openPMD.py PRIVATE openPMD) target_link_libraries(openPMD.py PRIVATE pybind11::module pybind11::windows_extras) # LTO/IPO: CMake target properties work well for 3.18+ and are buggy before set(_USE_PY_LTO ON) # default shall be ON if(DEFINED CMAKE_INTERPROCEDURAL_OPTIMIZATION) # overwrite default if defined if(NOT CMAKE_INTERPROCEDURAL_OPTIMIZATION) set(_USE_PY_LTO OFF) endif() endif() message(STATUS \"Python LTO/IPO: ${_USE_PY_LTO}\") set_target_properties(openPMD.py PROPERTIES INTERPROCEDURAL_OPTIMIZATION ${_USE_PY_LTO}) unset(_USE_PY_LTO) if(EMSCRIPTEN) set_target_properties(openPMD.py PROPERTIES PREFIX \"\") else() pybind11_extension(openPMD.py) endif() if(NOT MSVC AND NOT ${CMAKE_BUILD_TYPE} MATCHES Debug|RelWithDebInfo) pybind11_strip(openPMD.py) endif() set_target_properties(openPMD.py PROPERTIES CXX_VISIBILITY_PRESET \"hidden\" CUDA_VISIBILITY_PRESET \"hidden\") # ancient Clang releases # https://github.com/openPMD/openPMD-api/issues/542 # https://pybind11.readthedocs.io/en/stable/faq.html#recursive-template-instantiation-exceeded-maximum-depth-of-256 # https://bugs.llvm.org/show_bug.cgi?id=18417 # https://github.com/llvm/llvm-project/commit/e55b4737c026ea2e0b44829e4115d208577a67b2 if((\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"AppleClang\" AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.1) OR (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\" AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 4.0)) message(STATUS \"Clang: Passing -ftemplate-depth=1024\") target_compile_options(openPMD.py PRIVATE -ftemplate-depth=1024) endif() if(WIN32) set(openPMD_INSTALL_PYTHONDIR_DEFAULT \"${CMAKE_INSTALL_LIBDIR}/site-packages\") else() set(openPMD_INSTALL_PYTHONDIR_DEFAULT \"${CMAKE_INSTALL_LIBDIR}/python${Python_VERSION_MAJOR}.${Python_VERSION_MINOR}/site-packages\" ) endif() set(openPMD_INSTALL_PYTHONDIR \"${openPMD_INSTALL_PYTHONDIR_DEFAULT}\" CACHE STRING \"Location for installed python package\") set(openPMD_PYTHON_OUTPUT_DIRECTORY \"${openPMD_BINARY_DIR}/${openPMD_INSTALL_PYTHONDIR}\" CACHE STRING \"Build directory for python modules\") set_target_properties(openPMD.py PROPERTIES ARCHIVE_OUTPUT_NAME openpmd_api_cxx LIBRARY_OUTPUT_NAME openpmd_api_cxx COMPILE_PDB_NAME openpmd_api_cxx ARCHIVE_OUTPUT_DIRECTORY ${openPMD_PYTHON_OUTPUT_DIRECTORY}/openpmd_api LIBRARY_OUTPUT_DIRECTORY ${openPMD_PYTHON_OUTPUT_DIRECTORY}/openpmd_api RUNTIME_OUTPUT_DIRECTORY ${openPMD_PYTHON_OUTPUT_DIRECTORY}/openpmd_api PDB_OUTPUT_DIRECTORY ${openPMD_PYTHON_OUTPUT_DIRECTORY}/openpmd_api COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_PYTHON_OUTPUT_DIRECTORY}/openpmd_api ) # note: same as above, but for Multi-Config generators if(isMultiConfig) foreach(CFG IN LISTS CMAKE_CONFIGURATION_TYPES) string(TOUPPER \"${CFG}\" CFG_UPPER) if(openPMD_BUILD_NO_CFG_SUBPATH) # for setup.py set(CFG_PATH \"\") else() set(CFG_PATH \"/${CFG}\") endif() set_target_properties(openPMD.py PROPERTIES COMPILE_PDB_NAME_${CFG_UPPER} openpmd_api_cxx ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PYTHON_OUTPUT_DIRECTORY}${CFG_PATH}/openpmd_api LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PYTHON_OUTPUT_DIRECTORY}${CFG_PATH}/openpmd_api RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PYTHON_OUTPUT_DIRECTORY}${CFG_PATH}/openpmd_api PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PYTHON_OUTPUT_DIRECTORY}${CFG_PATH}/openpmd_api COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PYTHON_OUTPUT_DIRECTORY}${CFG_PATH}/openpmd_api ) endforeach() endif() function(copy_aux_py) set(AUX_PY_SRC_DIR ${openPMD_SOURCE_DIR}/src/binding/python/openpmd_api/) set(AUX_PY_DSR_DIR ${openPMD_PYTHON_OUTPUT_DIRECTORY}/openpmd_api/) foreach(src_name IN LISTS ARGN) configure_file(${AUX_PY_SRC_DIR}/${src_name} ${AUX_PY_DSR_DIR}/${src_name} COPYONLY) endforeach() endfunction() copy_aux_py( __init__.py DaskArray.py DaskDataFrame.py DataFrame.py ls/__init__.py ls/__main__.py pipe/__init__.py pipe/__main__.py ) endif() # tests set(openPMD_TEST_NAMES Core Auxiliary SerialIO ParallelIO JSON ) # command line tools set(openPMD_CLI_TOOL_NAMES ls ) set(openPMD_PYTHON_CLI_TOOL_NAMES pipe ) set(openPMD_PYTHON_CLI_MODULE_NAMES ${openPMD_CLI_TOOL_NAMES}) # examples set(openPMD_EXAMPLE_NAMES 1_structure 2_read_serial 2a_read_thetaMode_serial 3_write_serial 3a_write_thetaMode_serial 3b_write_resizable_particles 4_read_parallel 5_write_parallel 6_dump_filebased_series 7_extended_write_serial 8_benchmark_parallel 8a_benchmark_write_parallel 8b_benchmark_read_parallel 10_streaming_write 10_streaming_read 12_span_write 13_write_dynamic_configuration 14_toml_template ) set(openPMD_PYTHON_EXAMPLE_NAMES 2_read_serial 2a_read_thetaMode_serial 3_write_serial 3a_write_thetaMode_serial 3b_write_resizable_particles 4_read_parallel 5_write_parallel 7_extended_write_serial 9_particle_write_serial 10_streaming_write 10_streaming_read 11_particle_dataframe 12_span_write 13_write_dynamic_configuration ) if(openPMD_USE_INVASIVE_TESTS) if(WIN32) message(WARNING \"Invasive tests that redefine class signatures are \" \"known to fail on Windows!\") endif() target_compile_definitions(openPMD PRIVATE openPMD_USE_INVASIVE_TESTS=1) endif() function(set_filesystem_header_for_target target) if(openPMD_USE_FILESYSTEM_HEADER) target_compile_definitions(${target} PRIVATE openPMD_USE_FILESYSTEM_HEADER=1) else() target_compile_definitions(${target} PRIVATE openPMD_USE_FILESYSTEM_HEADER=0) endif() endfunction() set_filesystem_header_for_target(openPMD) if(openPMD_HAVE_PYTHON) set_filesystem_header_for_target(openPMD.py) endif() if(openPMD_BUILD_TESTING) # compile Catch2 implementation part separately add_library(CatchRunner ${_openpmd_lib_type} test/CatchRunner.cpp) # Always MPI_Init with Serial Fallback add_library(CatchMain ${_openpmd_lib_type} test/CatchMain.cpp) # Serial only openpmd_cxx_required(CatchRunner) openpmd_cxx_required(CatchMain) set_target_properties(CatchRunner CatchMain PROPERTIES ARCHIVE_OUTPUT_DIRECTORY ${openPMD_ARCHIVE_OUTPUT_DIRECTORY} LIBRARY_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY} RUNTIME_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} PDB_OUTPUT_DIRECTORY ${openPMD_PDB_OUTPUT_DIRECTORY} COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_COMPILE_PDB_OUTPUT_DIRECTORY} POSITION_INDEPENDENT_CODE ON WINDOWS_EXPORT_ALL_SYMBOLS ON ) set_target_properties(CatchRunner PROPERTIES COMPILE_PDB_NAME CatchRunner) set_target_properties(CatchMain PROPERTIES COMPILE_PDB_NAME CatchMain) # note: same as above, but for Multi-Config generators if(isMultiConfig) foreach(CFG IN LISTS CMAKE_CONFIGURATION_TYPES) string(TOUPPER \"${CFG}\" CFG_UPPER) set_target_properties(CatchRunner PROPERTIES COMPILE_PDB_NAME_${CFG_UPPER} CatchRunner ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}/${CFG} LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_LIBRARY_OUTPUT_DIRECTORY}/${CFG} RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG} PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PDB_OUTPUT_DIRECTORY}/${CFG} COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_COMPILE_PDB_OUTPUT_DIRECTORY}/${CFG} ) set_target_properties(CatchMain PROPERTIES COMPILE_PDB_NAME_${CFG_UPPER} CatchMain ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}/${CFG} LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_LIBRARY_OUTPUT_DIRECTORY}/${CFG} RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG} PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_PDB_OUTPUT_DIRECTORY}/${CFG} COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_COMPILE_PDB_OUTPUT_DIRECTORY}/${CFG} ) endforeach() endif() target_compile_options(CatchRunner PUBLIC ${_msvc_options}) target_compile_options(CatchMain PUBLIC ${_msvc_options}) target_link_libraries(CatchRunner PUBLIC openPMD::thirdparty::Catch2) target_link_libraries(CatchMain PUBLIC openPMD::thirdparty::Catch2) if(openPMD_HAVE_MPI) target_link_libraries(CatchRunner PUBLIC ${openPMD_MPI_TARGETS}) target_compile_definitions(CatchRunner PUBLIC openPMD_HAVE_MPI=1) endif() macro(additional_testing_sources test_name out_list) if(${test_name} STREQUAL \"SerialIO\") list(APPEND ${out_list} test/Files_SerialIO/close_and_reopen_test.cpp test/Files_SerialIO/filebased_write_test.cpp test/Files_SerialIO/issue_1744_unique_ptrs_at_close_time.cpp ) elseif(${test_name} STREQUAL \"ParallelIO\" AND openPMD_HAVE_MPI) list(APPEND ${out_list} test/Files_ParallelIO/read_variablebased_randomaccess.cpp test/Files_ParallelIO/iterate_nonstreaming_series.cpp test/Files_ParallelIO/bug_1655_bp5_writer_hangup.cpp ) elseif(${test_name} STREQUAL \"Core\") list(APPEND ${out_list} test/Files_Core/automatic_variable_encoding.cpp ) endif() endmacro() foreach(testname ${openPMD_TEST_NAMES}) set(ADDITIONAL_SOURCE_FILES \"\") additional_testing_sources(${testname} ADDITIONAL_SOURCE_FILES) add_executable(${testname}Tests test/${testname}Test.cpp ${ADDITIONAL_SOURCE_FILES}) target_include_directories(${testname}Tests PRIVATE test/Files_${testname}/) openpmd_cxx_required(${testname}Tests) set_target_properties(${testname}Tests PROPERTIES COMPILE_PDB_NAME ${testname}Tests ARCHIVE_OUTPUT_DIRECTORY ${openPMD_ARCHIVE_OUTPUT_DIRECTORY} LIBRARY_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY} RUNTIME_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} PDB_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) # note: same as above, but for Multi-Config generators if(isMultiConfig) foreach(CFG IN LISTS CMAKE_CONFIGURATION_TYPES) string(TOUPPER \"${CFG}\" CFG_UPPER) set_target_properties(${testname}Tests PROPERTIES COMPILE_PDB_NAME_${CFG_UPPER} ${testname}Tests ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}/${CFG} LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_LIBRARY_OUTPUT_DIRECTORY}/${CFG} RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG} PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG} COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG} ) endforeach() endif() if(openPMD_USE_INVASIVE_TESTS) target_compile_definitions(${testname}Tests PRIVATE openPMD_USE_INVASIVE_TESTS=1) endif() target_link_libraries(${testname}Tests PRIVATE openPMD) if(${testname} MATCHES \"Parallel.+$\") target_link_libraries(${testname}Tests PRIVATE CatchRunner) else() target_link_libraries(${testname}Tests PRIVATE CatchMain) endif() if(${testname} STREQUAL JSON) target_include_directories(${testname}Tests SYSTEM PRIVATE $<TARGET_PROPERTY:openPMD::thirdparty::nlohmann_json,INTERFACE_INCLUDE_DIRECTORIES> $<TARGET_PROPERTY:openPMD::thirdparty::toml11,INTERFACE_INCLUDE_DIRECTORIES>) endif() endforeach() endif() if(openPMD_BUILD_CLI_TOOLS) foreach(toolname ${openPMD_CLI_TOOL_NAMES}) add_executable(openpmd-${toolname} src/cli/${toolname}.cpp) openpmd_cxx_required(openpmd-${toolname}) set_target_properties(openpmd-${toolname} PROPERTIES COMPILE_PDB_NAME openpmd-${toolname} ARCHIVE_OUTPUT_DIRECTORY ${openPMD_ARCHIVE_OUTPUT_DIRECTORY} LIBRARY_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY} RUNTIME_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} PDB_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) # note: same as above, but for Multi-Config generators if(isMultiConfig) foreach(CFG IN LISTS CMAKE_CONFIGURATION_TYPES) string(TOUPPER \"${CFG}\" CFG_UPPER) set_target_properties(openpmd-${toolname} PROPERTIES COMPILE_PDB_NAME_${CFG_UPPER} openpmd-${toolname} ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}/${CFG} LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_LIBRARY_OUTPUT_DIRECTORY}/${CFG} RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG} PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG} COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG} ) endforeach() endif() target_link_libraries(openpmd-${toolname} PRIVATE openPMD) endforeach() endif() if(openPMD_BUILD_EXAMPLES) foreach(examplename ${openPMD_EXAMPLE_NAMES}) if(${examplename} MATCHES \".+parallel$\" AND NOT openPMD_HAVE_MPI) # skip parallel test continue() endif() add_executable(${examplename} examples/${examplename}.cpp) if (openPMD_HAVE_CUDA_EXAMPLES) set_source_files_properties(examples/${examplename}.cpp PROPERTIES LANGUAGE CUDA) openpmd_cuda_required(${examplename}) else() openpmd_cxx_required(${examplename}) endif() set_target_properties(${examplename} PROPERTIES COMPILE_PDB_NAME ${examplename} ARCHIVE_OUTPUT_DIRECTORY ${openPMD_ARCHIVE_OUTPUT_DIRECTORY} LIBRARY_OUTPUT_DIRECTORY ${openPMD_LIBRARY_OUTPUT_DIRECTORY} RUNTIME_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} PDB_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} COMPILE_PDB_OUTPUT_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) # note: same as above, but for Multi-Config generators if(isMultiConfig) foreach(CFG IN LISTS CMAKE_CONFIGURATION_TYPES) string(TOUPPER \"${CFG}\" CFG_UPPER) set_target_properties(${examplename} PROPERTIES COMPILE_PDB_NAME_${CFG_UPPER} ${examplename} ARCHIVE_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_ARCHIVE_OUTPUT_DIRECTORY}/${CFG} LIBRARY_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_LIBRARY_OUTPUT_DIRECTORY}/${CFG} RUNTIME_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG} PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG} COMPILE_PDB_OUTPUT_DIRECTORY_${CFG_UPPER} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${CFG} ) endforeach() endif() target_link_libraries(${examplename} PRIVATE openPMD) endforeach() endif() # Warnings #################################################################### # # TODO: LEGACY! Use CMake TOOLCHAINS instead! if(CMAKE_SOURCE_DIR STREQUAL PROJECT_SOURCE_DIR) # On Windows, Clang -Wall aliases -Weverything; default is /W3 if (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\" AND NOT WIN32) # list(APPEND CMAKE_CXX_FLAGS \"-fsanitize=address\") # address, memory, undefined # set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -fsanitize=address\") # set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -fsanitize=address\") # set(CMAKE_MODULE_LINKER_FLAGS \"${CMAKE_MODULE_LINKER_FLAGS} -fsanitize=address\") # note: might still need a # export LD_PRELOAD=libclang_rt.asan.so # or on Debian 9 with Clang 6.0 # export LD_PRELOAD=/usr/lib/llvm-6.0/lib/clang/6.0.0/lib/linux/libclang_rt.asan-x86_64.so: # /usr/lib/llvm-6.0/lib/clang/6.0.0/lib/linux/libclang_rt.ubsan_minimal-x86_64.so # at runtime when used with symbol-hidden code (e.g. pybind11 module) set(CMAKE_CXX_FLAGS \"-Wall -Wextra -Wpedantic -Wshadow -Woverloaded-virtual -Wextra-semi -Wunreachable-code -Wsign-compare ${CMAKE_CXX_FLAGS}\") elseif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"AppleClang\") set(CMAKE_CXX_FLAGS \"-Wall -Wextra -Wpedantic -Wshadow -Woverloaded-virtual -Wextra-semi -Wunreachable-code -Wsign-compare ${CMAKE_CXX_FLAGS}\") elseif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Intel\") set(CMAKE_CXX_FLAGS \"-w3 -wd193,383,1572 ${CMAKE_CXX_FLAGS}\") elseif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\") set(CMAKE_CXX_FLAGS \"-Wall -Wextra -Wpedantic -Wshadow -Woverloaded-virtual -Wunreachable-code -Wsign-compare ${CMAKE_CXX_FLAGS}\") elseif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\") # Warning C4503: \"decorated name length exceeded, name was truncated\" # Symbols longer than 4096 chars are truncated (and hashed instead) set(CMAKE_CXX_FLAGS \"-wd4503 ${CMAKE_CXX_FLAGS}\") # Warning C4244: \"conversion from 'X' to 'Y', possible loss of data\" set(CMAKE_CXX_FLAGS \"-wd4244 ${CMAKE_CXX_FLAGS}\") # Yes, you should build against the same C++ runtime and with same # configuration (Debug/Release). MSVC does inconvenient choices for their # developers, so be it. (Our Windows-users use conda-forge builds, which # are consistent.) set(CMAKE_CXX_FLAGS \"-wd4251 ${CMAKE_CXX_FLAGS}\") endif() endif() # Generate Files with Configuration Options ################################### # # TODO configure a version.hpp configure_file( ${openPMD_SOURCE_DIR}/include/openPMD/config.hpp.in ${openPMD_BINARY_DIR}/include/openPMD/config.hpp @ONLY ) configure_file( ${openPMD_SOURCE_DIR}/openPMDConfig.cmake.in ${openPMD_BINARY_DIR}/openPMDConfig.cmake @ONLY ) # get absolute paths to linked libraries function(openpmdreclibs tgtname outname) get_target_property(PC_PRIVATE_LIBS_TGT ${tgtname} INTERFACE_LINK_LIBRARIES) foreach(PC_LIB IN LISTS PC_PRIVATE_LIBS_TGT) if(TARGET ${PC_LIB}) openpmdreclibs(${PC_LIB} ${outname}) else() if(PC_LIB) string(APPEND ${outname} \" ${PC_LIB}\") endif() endif() endforeach() set(${outname} ${${outname}} PARENT_SCOPE) endfunction() if(openPMD_HAVE_PKGCONFIG) openpmdreclibs(openPMD openPMD_PC_PRIVATE_LIBS) if(openPMD_BUILD_SHARED_LIBS) set(openPMD_PC_STATIC false) else() set(openPMD_PC_STATIC true) endif() configure_file( ${openPMD_SOURCE_DIR}/openPMD.pc.in ${openPMD_BINARY_DIR}/openPMD.pc @ONLY ) endif() include(CMakePackageConfigHelpers) write_basic_package_version_file(\"openPMDConfigVersion.cmake\" VERSION ${openPMD_VERSION} COMPATIBILITY SameMajorVersion ) # Installs #################################################################### # # headers, libraries and executables if(openPMD_INSTALL) set(openPMD_INSTALL_TARGET_NAMES openPMD) if(openPMD_BUILD_CLI_TOOLS) foreach(toolname ${openPMD_CLI_TOOL_NAMES}) list(APPEND openPMD_INSTALL_TARGET_NAMES openpmd-${toolname}) endforeach() endif() if(openPMD_INSTALL_RPATH) set(openPMD_INSTALL_RPATH_TARGET_NAMES ${openPMD_INSTALL_TARGET_NAMES}) if(openPMD_HAVE_PYTHON) list(APPEND openPMD_INSTALL_RPATH_TARGET_NAMES openPMD.py) endif() if(NOT DEFINED CMAKE_INSTALL_RPATH) if(APPLE) set_target_properties(${openPMD_INSTALL_RPATH_TARGET_NAMES} PROPERTIES INSTALL_RPATH \"@loader_path\" ) elseif(CMAKE_SYSTEM_NAME MATCHES \"Linux\") set_target_properties(${openPMD_INSTALL_RPATH_TARGET_NAMES} PROPERTIES INSTALL_RPATH \"$ORIGIN\" ) endif() # Windows: has no RPath concept, all interdependent `.dll`s must be in # %PATH% or in the same dir as the calling executable endif() if(NOT DEFINED CMAKE_INSTALL_RPATH_USE_LINK_PATH) # those are appended AFTER the paths in INSTALL_RPATH set_target_properties(${openPMD_INSTALL_RPATH_TARGET_NAMES} PROPERTIES INSTALL_RPATH_USE_LINK_PATH ON ) endif() endif() install(TARGETS ${openPMD_INSTALL_TARGET_NAMES} EXPORT openPMDTargets LIBRARY DESTINATION ${openPMD_INSTALL_LIBDIR} ARCHIVE DESTINATION ${openPMD_INSTALL_LIBDIR} RUNTIME DESTINATION ${openPMD_INSTALL_BINDIR} INCLUDES DESTINATION ${openPMD_INSTALL_INCLUDEDIR} ) if(openPMD_HAVE_PYTHON) install( DIRECTORY ${openPMD_SOURCE_DIR}/src/binding/python/openpmd_api DESTINATION ${openPMD_INSTALL_PYTHONDIR} PATTERN \"*pyc\" EXCLUDE PATTERN \"__pycache__\" EXCLUDE ) install(TARGETS openPMD.py DESTINATION ${openPMD_INSTALL_PYTHONDIR}/openpmd_api ) if(openPMD_BUILD_CLI_TOOLS) foreach(toolname ${openPMD_PYTHON_CLI_TOOL_NAMES}) install( PROGRAMS ${openPMD_SOURCE_DIR}/src/cli/${toolname}.py DESTINATION ${openPMD_INSTALL_BINDIR} RENAME openpmd-${toolname} ) endforeach() endif() endif() install(DIRECTORY \"${openPMD_SOURCE_DIR}/include/openPMD\" DESTINATION ${openPMD_INSTALL_INCLUDEDIR} FILES_MATCHING PATTERN \"*.hpp\" PATTERN \"*.tpp\" ) install( FILES ${openPMD_BINARY_DIR}/include/openPMD/config.hpp DESTINATION ${openPMD_INSTALL_INCLUDEDIR}/openPMD ) # CMake package file for find_package(openPMD::openPMD) in depending projects install(EXPORT openPMDTargets FILE openPMDTargets.cmake NAMESPACE openPMD:: DESTINATION ${openPMD_INSTALL_CMAKEDIR} ) install( FILES ${openPMD_BINARY_DIR}/openPMDConfig.cmake ${openPMD_BINARY_DIR}/openPMDConfigVersion.cmake DESTINATION ${openPMD_INSTALL_CMAKEDIR} ) # pkg-config .pc file for depending legacy projects # This is for projects that do not use a build file generator, e.g. # because they compile manually on the command line or write their # Makefiles by hand. if(openPMD_HAVE_PKGCONFIG) install( FILES ${openPMD_BINARY_DIR}/openPMD.pc DESTINATION ${openPMD_INSTALL_LIBDIR}/pkgconfig ) endif() endif() # Tests ####################################################################### # if(openPMD_BUILD_TESTING) enable_testing() # OpenMPI root guard: https://github.com/open-mpi/ompi/issues/4451 if(\"$ENV{USER}\" STREQUAL \"root\") # calling even --help as root will abort and warn on stderr execute_process(COMMAND ${MPIEXEC_EXECUTABLE} --help ERROR_VARIABLE MPIEXEC_HELP_TEXT OUTPUT_STRIP_TRAILING_WHITESPACE) if(${MPIEXEC_HELP_TEXT} MATCHES \"^.*allow-run-as-root.*$\") set(MPI_ALLOW_ROOT --allow-run-as-root) endif() endif() set(MPI_TEST_EXE ${MPIEXEC_EXECUTABLE} ${MPI_ALLOW_ROOT} ${MPIEXEC_NUMPROC_FLAG} 2 ) # do we have openPMD-example-datasets? if(EXISTS \"${openPMD_BINARY_DIR}/samples/git-sample/\") set(EXAMPLE_DATA_FOUND ON) message(STATUS \"Found openPMD-example-datasets: TRUE\") else() message(STATUS \"Note: Skipping example and tool runs (missing openPMD-example-datasets)\") if(WIN32) message(STATUS \"Note: run\\n\" \" Powershell.exe -File ${openPMD_SOURCE_DIR}/share/openPMD/download_samples.ps1\\n\" \"to add example files to samples/git-sample/ directory!\") else() message(STATUS \"Note: run\\n\" \" ${openPMD_SOURCE_DIR}/share/openPMD/download_samples.sh\\n\" \"to add example files to samples/git-sample/ directory!\") endif() endif() if(openPMD_HAVE_PYTHON) # do we have mpi4py for MPI-parallel Python tests? if(openPMD_HAVE_MPI) execute_process(COMMAND ${Python_EXECUTABLE} -m mpi4py -c \"import mpi4py.MPI\" RESULT_VARIABLE MPI4PY_RETURN OUTPUT_QUIET ERROR_QUIET) if(MPI4PY_RETURN EQUAL 0) message(STATUS \"Found mpi4py: TRUE\") else() message(STATUS \"Could NOT find mpi4py (will NOT run MPI-parallel Python examples)\") endif() endif() endif() # C++ Unit tests foreach(testname ${openPMD_TEST_NAMES}) if(${testname} MATCHES \"^Parallel.*$\") if(openPMD_HAVE_MPI) add_test(NAME MPI.${testname} COMMAND ${MPI_TEST_EXE} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${testname}Tests WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) endif() else() add_test(NAME Serial.${testname} COMMAND ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${testname}Tests WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) endif() endforeach() # Python Unit tests if(openPMD_HAVE_PYTHON) function(test_set_pythonpath test_name) if(WIN32) if(isMultiConfig) string(REGEX REPLACE \"/\" \"\\\\\\\\\" WIN_BUILD_BASEDIR ${openPMD_BINARY_DIR}/$<CONFIG>) string(REGEX REPLACE \"/\" \"\\\\\\\\\" WIN_BUILD_BINDIR ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/$<CONFIG>) else() string(REGEX REPLACE \"/\" \"\\\\\\\\\" WIN_BUILD_BASEDIR ${openPMD_BINARY_DIR}) string(REGEX REPLACE \"/\" \"\\\\\\\\\" WIN_BUILD_BINDIR ${openPMD_RUNTIME_OUTPUT_DIRECTORY}) endif() string(REPLACE \";\" \"\\\\;\" WIN_PATH \"$ENV{PATH}\") string(REPLACE \";\" \"\\\\;\" WIN_PYTHONPATH \"$ENV{PYTHONPATH}\") set_property(TEST ${test_name} PROPERTY ENVIRONMENT \"PATH=${WIN_BUILD_BINDIR}\\\\${CMAKE_BUILD_TYPE}\\;${WIN_PATH}\\n\" \"PYTHONPATH=${WIN_BUILD_BASEDIR}\\\\${openPMD_INSTALL_PYTHONDIR}\\\\${CMAKE_BUILD_TYPE}\\;${WIN_PYTHONPATH}\" ) else() set_tests_properties(${test_name} PROPERTIES ENVIRONMENT \"PYTHONPATH=${openPMD_BINARY_DIR}/${openPMD_INSTALL_PYTHONDIR}:$ENV{PYTHONPATH}\" ) endif() endfunction() if(openPMD_HAVE_HDF5) if(EXAMPLE_DATA_FOUND) add_test(NAME Unittest.py COMMAND ${Python_EXECUTABLE} ${openPMD_SOURCE_DIR}/test/python/unittest/Test.py -v WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) test_set_pythonpath(Unittest.py) endif() endif() endif() # Examples if(openPMD_BUILD_EXAMPLES) # C++ Examples # Current examples all use HDF5, elaborate if other backends are used if(openPMD_HAVE_HDF5) if(EXAMPLE_DATA_FOUND) foreach(examplename ${openPMD_EXAMPLE_NAMES}) if(${examplename} MATCHES \"^10.*$\") # streaming examples are done separately elseif(${examplename} MATCHES \"^.*_parallel$\") if(openPMD_HAVE_MPI) add_test(NAME MPI.${examplename} COMMAND ${MPI_TEST_EXE} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${examplename} WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) endif() else() add_test(NAME Serial.${examplename} COMMAND ${examplename} WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) endif() endforeach() endif() endif() if(openPMD_HAVE_ADIOS2) add_test(NAME Asynchronous.10_streaming COMMAND sh -c \"$<TARGET_FILE:10_streaming_write> & sleep 1; $<TARGET_FILE:10_streaming_read>\" WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}) endif() endif() # Command Line Tools if(openPMD_BUILD_CLI_TOOLS) # all tools must provide a \"--help\" foreach(toolname ${openPMD_CLI_TOOL_NAMES}) add_test(NAME CLI.help.${toolname} COMMAND openpmd-${toolname} --help WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) endforeach() if(openPMD_HAVE_HDF5 AND EXAMPLE_DATA_FOUND) add_test(NAME CLI.ls COMMAND openpmd-ls ../samples/git-sample/data%08T.h5 WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) endif() endif() # Python CLI Modules if(openPMD_HAVE_PYTHON) # (Note that during setuptools install, these are furthermore installed as # console scripts and replace the all-binary CLI tools.) foreach(pymodulename ${openPMD_PYTHON_CLI_MODULE_NAMES}) add_test(NAME CLI.py.help.${pymodulename} COMMAND ${Python_EXECUTABLE} -m openpmd_api.${pymodulename} --help WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) test_set_pythonpath(CLI.py.help.${pymodulename}) endforeach() endif() # Python-based command line tools if(openPMD_BUILD_CLI_TOOLS AND openPMD_HAVE_PYTHON) # all tools must provide a \"--help\" foreach(toolname ${openPMD_PYTHON_CLI_TOOL_NAMES}) configure_file( ${openPMD_SOURCE_DIR}/src/cli/${toolname}.py ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-${toolname} COPYONLY ) add_test(NAME CLI.help.${toolname}.py COMMAND ${Python_EXECUTABLE} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-${toolname} --help WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) test_set_pythonpath(CLI.help.${toolname}.py) endforeach() # openpmd-pipe (python) test if( NOT WIN32 AND openPMD_HAVE_HDF5 AND openPMD_HAVE_ADIOS2 AND EXAMPLE_DATA_FOUND ) if( openPMD_HAVE_MPI ) set(MPI_TEST_EXE ${MPIEXEC_EXECUTABLE} ${MPI_ALLOW_ROOT} #${MPIEXEC_NUMPROC_FLAG} 2 ) add_test(NAME CLI.pipe.py COMMAND sh -c \"${MPI_TEST_EXE} ${Python_EXECUTABLE} \\ ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe \\ --infile ../samples/git-sample/data%T.h5 \\ --outfile ../samples/git-sample/data%T.bp && \\ \\ ${MPI_TEST_EXE} ${Python_EXECUTABLE} \\ ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe \\ --infile ../samples/git-sample/data00000100.h5 \\ --outfile \\ ../samples/git-sample/single_iteration_%T.bp && \\ \\ ${MPI_TEST_EXE} ${Python_EXECUTABLE} \\ ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe \\ --infile ../samples/git-sample/thetaMode/data%T.h5 \\ --outfile \\ ../samples/git-sample/thetaMode/data_%T.bp && \\ \\ ${MPI_TEST_EXE} ${Python_EXECUTABLE} \\ ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe \\ --infile ../samples/git-sample/thetaMode/data_%T.bp \\ --outfile ../samples/git-sample/thetaMode/data%T.json \\ --outconfig ' \\ json.attribute.mode = \\\"short\\\" \\n\\ json.dataset.mode = \\\"template_no_warn\\\"' \\ \" WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) else() add_test(NAME CLI.pipe.py COMMAND sh -c \"${Python_EXECUTABLE} \\ ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe \\ --infile ../samples/git-sample/data%T.h5 \\ --outfile ../samples/git-sample/data%T.bp && \\ \\ ${Python_EXECUTABLE} \\ ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe \\ --infile ../samples/git-sample/thetaMode/data%T.h5 \\ --outfile ../samples/git-sample/thetaMode/data%T.bp && \\ \\ ${Python_EXECUTABLE} \\ ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/openpmd-pipe \\ --infile ../samples/git-sample/thetaMode/data%T.bp \\ --outfile ../samples/git-sample/thetaMode/data%T.json \\ \" WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) endif() test_set_pythonpath(CLI.pipe.py) endif() endif() # Python Examples # Current examples all use HDF5, elaborate if other backends are used if(openPMD_HAVE_PYTHON AND openPMD_HAVE_HDF5) if(EXAMPLE_DATA_FOUND) foreach(examplename ${openPMD_PYTHON_EXAMPLE_NAMES}) configure_file( ${openPMD_SOURCE_DIR}/examples/${examplename}.py ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${examplename}.py COPYONLY ) if(openPMD_BUILD_TESTING) if(${examplename} MATCHES \"^10.*$\") # streaming examples are done separately continue() elseif(${examplename} MATCHES \"^.*_parallel$\") if(openPMD_HAVE_MPI AND MPI4PY_RETURN EQUAL 0) # see https://mpi4py.readthedocs.io/en/stable/mpi4py.run.html add_test(NAME Example.py.${examplename} COMMAND ${MPI_TEST_EXE} ${Python_EXECUTABLE} -m mpi4py ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${examplename}.py WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) else() continue() endif() else() add_test(NAME Example.py.${examplename} COMMAND ${Python_EXECUTABLE} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/${examplename}.py WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY} ) endif() test_set_pythonpath(Example.py.${examplename}) endif() endforeach() if(openPMD_HAVE_ADIOS2 AND openPMD_BUILD_TESTING AND NOT WIN32) add_test(NAME Asynchronous.10_streaming.py COMMAND sh -c \"${Python_EXECUTABLE} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/10_streaming_write.py & sleep 1; ${Python_EXECUTABLE} ${openPMD_RUNTIME_OUTPUT_DIRECTORY}/10_streaming_read.py\" WORKING_DIRECTORY ${openPMD_RUNTIME_OUTPUT_DIRECTORY}) test_set_pythonpath(Asynchronous.10_streaming.py) endif() endif() endif() endif() # Status Message for Build Options ############################################ # openpmd_print_summary()\n[build-system] requires = [ \"setuptools>=42\", \"wheel\", \"cmake>=3.22.0,<4.0.0\", \"packaging>=23\", \"pybind11>=2.13.0,<3.0.0\" ] build-backend = \"setuptools.build_meta\"\nnumpy>=1.15.0\nimport os import platform import re import subprocess import sys from setuptools import Extension, setup from setuptools.command.build_ext import build_ext class CMakeExtension(Extension): def __init__(self, name, sourcedir=''): Extension.__init__(self, name, sources=[]) self.sourcedir = os.path.abspath(sourcedir) class CMakeBuild(build_ext): def run(self): from packaging.version import parse try: out = subprocess.check_output(['cmake', '--version']) except OSError: raise RuntimeError( \"CMake 3.22.0+ must be installed to build the following \" + \"extensions: \" + \", \".join(e.name for e in self.extensions)) cmake_version = parse(re.search( r'version\\s*([\\d.]+)', out.decode() ).group(1)) if cmake_version < parse('3.22.0'): raise RuntimeError(\"CMake >= 3.22.0 is required\") for ext in self.extensions: self.build_extension(ext) def build_extension(self, ext): extdir = os.path.abspath(os.path.dirname( self.get_ext_fullpath(ext.name) )) # required for auto-detection of auxiliary \"native\" libs if not extdir.endswith(os.path.sep): extdir += os.path.sep pyv = sys.version_info cmake_args = [ # Python: use the calling interpreter in CMake # https://cmake.org/cmake/help/latest/module/FindPython.html#hints # https://cmake.org/cmake/help/latest/command/find_package.html#config-mode-version-selection '-DPython_ROOT_DIR=' + sys.prefix, f'-DPython_FIND_VERSION={pyv.major}.{pyv.minor}.{pyv.micro}', '-DPython_FIND_VERSION_EXACT=TRUE', '-DPython_FIND_STRATEGY=LOCATION', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=' + os.path.join(extdir, \"openpmd_api\"), # '-DCMAKE_RUNTIME_OUTPUT_DIRECTORY=' + extdir, '-DopenPMD_PYTHON_OUTPUT_DIRECTORY=' + extdir, '-DopenPMD_USE_PYTHON:BOOL=ON', # variants '-DopenPMD_USE_MPI:BOOL=' + openPMD_USE_MPI, # skip building cli tools, examples & tests # note: CLI tools provided as console scripts '-DopenPMD_BUILD_CLI_TOOLS:BOOL=OFF', '-DopenPMD_BUILD_EXAMPLES:BOOL=' + BUILD_EXAMPLES, '-DopenPMD_BUILD_TESTING:BOOL=' + BUILD_TESTING, # static/shared libs '-DopenPMD_BUILD_SHARED_LIBS:BOOL=' + BUILD_SHARED_LIBS, # Unix: rpath to current dir when packaged # needed for shared (here non-default) builds '-DCMAKE_BUILD_WITH_INSTALL_RPATH:BOOL=ON', '-DCMAKE_INSTALL_RPATH_USE_LINK_PATH:BOOL=OFF', # Windows: has no RPath concept, all `.dll`s must be in %PATH% # or same dir as calling executable ] if HDF5_USE_STATIC_LIBRARIES is not None: cmake_args.append('-DHDF5_USE_STATIC_LIBRARIES:BOOL=' + HDF5_USE_STATIC_LIBRARIES) if ZLIB_USE_STATIC_LIBS is not None: cmake_args.append('-DZLIB_USE_STATIC_LIBS:BOOL=' + ZLIB_USE_STATIC_LIBS) if CMAKE_INTERPROCEDURAL_OPTIMIZATION is not None: cmake_args.append('-DCMAKE_INTERPROCEDURAL_OPTIMIZATION=' + CMAKE_INTERPROCEDURAL_OPTIMIZATION) if sys.platform == \"darwin\": cmake_args.append('-DCMAKE_INSTALL_RPATH=@loader_path') else: # values: linux*, aix, freebsd, ... # just as well win32 & cygwin (although Windows has no RPaths) cmake_args.append('-DCMAKE_INSTALL_RPATH=$ORIGIN') cmake_args += extra_cmake_args cfg = 'Debug' if self.debug else 'Release' build_args = ['--config', cfg] # Assumption: Windows builds are always multi-config (MSVC VS) if platform.system() == \"Windows\": cmake_args += [ '-DopenPMD_BUILD_NO_CFG_SUBPATH:BOOL=ON', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_{}={}'.format( cfg.upper(), os.path.join(extdir, \"openpmd_api\") ) ] if sys.maxsize > 2**32: cmake_args += ['-A', 'x64'] build_args += ['--', '/m'] else: cmake_args += ['-DCMAKE_BUILD_TYPE=' + cfg] build_args += ['--', '-j2'] env = os.environ.copy() env['CXXFLAGS'] = '{} -DVERSION_INFO=\\\\\"{}\\\\\"'.format( env.get('CXXFLAGS', ''), self.distribution.get_version() ) if not os.path.exists(self.build_temp): os.makedirs(self.build_temp) subprocess.check_call( ['cmake', ext.sourcedir] + cmake_args, cwd=self.build_temp, env=env ) subprocess.check_call( ['cmake', '--build', '.'] + build_args, cwd=self.build_temp ) # note that this does not call install; # we pick up artifacts directly from the build output dirs with open('./README.md', encoding='utf-8') as f: long_description = f.read() # Allow to control options via environment vars. # Work-around for https://github.com/pypa/setuptools/issues/1712 # note: changed default for SHARED, MPI, TESTING and EXAMPLES openPMD_USE_MPI = os.environ.get('openPMD_USE_MPI', 'OFF') HDF5_USE_STATIC_LIBRARIES = os.environ.get('HDF5_USE_STATIC_LIBRARIES', None) ZLIB_USE_STATIC_LIBS = os.environ.get('ZLIB_USE_STATIC_LIBS', None) # deprecated: backwards compatibility to <= 0.13.* BUILD_SHARED_LIBS = os.environ.get('BUILD_SHARED_LIBS', 'OFF') BUILD_TESTING = os.environ.get('BUILD_TESTING', 'OFF') BUILD_EXAMPLES = os.environ.get('BUILD_EXAMPLES', 'OFF') # end deprecated BUILD_SHARED_LIBS = os.environ.get('openPMD_BUILD_SHARED_LIBS', BUILD_SHARED_LIBS) BUILD_TESTING = os.environ.get('openPMD_BUILD_TESTING', BUILD_TESTING) BUILD_EXAMPLES = os.environ.get('openPMD_BUILD_EXAMPLES', BUILD_EXAMPLES) CMAKE_INTERPROCEDURAL_OPTIMIZATION = os.environ.get( 'CMAKE_INTERPROCEDURAL_OPTIMIZATION', None) # extra CMake arguments extra_cmake_args = [] for k, v in os.environ.items(): extra_cmake_args_prefix = \"openPMD_CMAKE_\" if k.startswith(extra_cmake_args_prefix) and \\ len(k) > len(extra_cmake_args_prefix): extra_cmake_args.append(\"-D{0}={1}\".format( k[len(extra_cmake_args_prefix):], v)) # https://cmake.org/cmake/help/v3.0/command/if.html if openPMD_USE_MPI.upper() in ['1', 'ON', 'TRUE', 'YES']: openPMD_USE_MPI = \"ON\" else: openPMD_USE_MPI = \"OFF\" # Get the package requirements from the requirements.txt file with open('./requirements.txt') as f: install_requires = [line.strip('\\n') for line in f.readlines()] if openPMD_USE_MPI == \"ON\": install_requires.append('mpi4py>=2.1.0') # keyword reference: # https://packaging.python.org/guides/distributing-packages-using-setuptools setup( name='openPMD-api', # note PEP-440 syntax: x.y.zaN but x.y.z.devN version='0.17.0.dev', author='Axel Huebl, Franz Poeschel, Fabian Koller, Junmin Gu', author_email='axelhuebl@lbl.gov, f.poeschel@hzdr.de', maintainer='Axel Huebl', maintainer_email='axelhuebl@lbl.gov', description='C++ & Python API for Scientific I/O with openPMD', long_description=long_description, long_description_content_type='text/markdown', keywords=('openPMD openscience hdf5 adios mpi hpc research ' 'file-format file-handling'), url='https://www.openPMD.org', project_urls={ 'Documentation': 'https://openpmd-api.readthedocs.io', 'Doxygen': 'https://www.openpmd.org/openPMD-api', 'Reference': 'https://doi.org/10.14278/rodare.27', 'Source': 'https://github.com/openPMD/openPMD-api', 'Tracker': 'https://github.com/openPMD/openPMD-api/issues', }, ext_modules=[CMakeExtension('openpmd_api_cxx')], cmdclass=dict(build_ext=CMakeBuild), # scripts=['openpmd-ls'], zip_safe=False, python_requires='>=3.8', # tests_require=['pytest'], install_requires=install_requires, # see: src/bindings/python/cli entry_points={ 'console_scripts': [ 'openpmd-ls = openpmd_api.ls.__main__:main', 'openpmd-pipe = openpmd_api.pipe.__main__:main' ] }, # we would like to use this mechanism, but pip / setuptools do not # influence the build and build_ext with it. # therefore, we use environment vars to control. # ref: https://github.com/pypa/setuptools/issues/1712 # extras_require={ # 'mpi': ['mpi4py>=2.1.0'], # }, # cmdclass={'test': PyTest}, # platforms='any', classifiers=[ 'Development Status :: 4 - Beta', 'Natural Language :: English', 'Environment :: Console', 'Intended Audience :: Science/Research', 'Operating System :: OS Independent', 'Topic :: Scientific/Engineering', 'Topic :: Database :: Front-Ends', 'Programming Language :: C++', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.8', 'Programming Language :: Python :: 3.9', 'Programming Language :: Python :: 3.10', 'Programming Language :: Python :: 3.11', 'Programming Language :: Python :: 3.12', 'Programming Language :: Python :: 3.13', ('License :: OSI Approved :: ' 'GNU Lesser General Public License v3 or later (LGPLv3+)'), ], )\n# This is a Spack environment file. # # Activating and installing this environment will provide all dependencies # that are needed for full-feature development. # https//spack.readthedocs.io/en/latest/environments.html#anonymous-environments # spack: specs: - adios~sz - adios2 - cmake - hdf5 - mpi - python - py-mpi4py - py-numpy - py-pandas - py-dask - py-pre-commit\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/osadcp-toolbox",
            "repo_link": "https://git.geomar.de/dam/osadcp_toolbox/",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/otter",
            "repo_link": "https://gitlab.com/qtb-hhu/marine/otter",
            "content": {
                "codemeta": "",
                "readme": "[![DOI](https://img.shields.io/badge/DOI-10.1038/s43247--024--01782--0-blue)](https://doi.org/10.1038/s43247-024-01782-0) [![Latest Release](https://gitlab.com/qtb-hhu/marine/otter/-/badges/release.svg)](https://gitlab.com/qtb-hhu/marine/otter/-/releases) # otter <div style=\"float: left;\"> <img src=\"img/otter_logo-removebg.png\" alt=\"Otter Logo\" width=\"200\" title=\"This images was created using GENAI (DALL-E 3).\"/> </div> <div style=\"text-align: right;\"> <br> <br> <br> The otter Framework combines community network analysis using Co-Occurrence networks, Louvain Clustering and Convergent Cross Mapping networks. <br> <br> <br> <br> </div> ### Installation Python version 3.10 (Python 3.10.5) ```bash pip install -r requirements.txt ``` Using MacOS when clang error: ``` brew install llvm ``` ### Pytest ```bash pytest ``` ## Usage: ### Run the app ```bash streamlit run Otter_APP.py ``` ## Getting Started ### 1. Set Prefix Path Fill in the **Prefix Path** where the output tables will be saved. - **Default**: `Tutorial/tables` - This is the directory where all your results will be stored. ### 2. Set Run ID Input the **Run ID**, which is the name for your experiment. - **Default**: `MyExperimentRun` - This name will be used to track the output of your experiment. - Now press check the checkbox: Start Caclulations. ### 3. Upload Abundance Table Upload the `abundance.csv` file, which contains the abundance time series. - **Format**: The `abundance.csv` should follow the example provided in `tests/table/abundance.csv`. - This table should include your time-series data for each ASV (Amplicon Sequence Variant). ### 4. Upload Taxa Table Upload the `taxa.csv` file, which contains the taxonomic hierarchy for each ASV present in the abundance file. - **Format**: Ensure this table includes the corresponding taxonomy information for each ASV. ### 5. (Optional) Upload Environmental Table You can optionally upload the `environmental.csv` file. This table should contain environmental parameters for each sample. - **Format**: Ensure this file matches the sample names in the `abundance.csv`. ### 6. Adjust Analysis Parameters You can now adjust the parameters for the following calculations using the **expanding parameter cockpit**: - **CON**: Co-Occurrence Network - **CCM**: Convergent Cross Mapping - **Permu**: Permutation tests Make sure to fine-tune these parameters based on your analysis needs. ### 7. Run Calculations Once parameters are set, you can create each network one by one or all at once: - Use the **Create Network Buttons** from left to right to calculate each network (CON, CCM, etc.). - Alternatively, click the **Run All** button to generate all networks in one go. OR ```bash streamlit run Otter_APP.py ``` ## Visualizing Results After the network calculations are complete, you can switch scroll down to visualize your results. 1. Environmental_Data 2. Environmental_Cluster_Correlation_Heatmap 3. Cluster Distribution 4. Latentspace Embedding for Cluster Distances 5. Alpha and Beta Diversity # Run it on a linux cluster Make sure the correct python version is installed and you are allowed to install python packages. You can use argparse to parse parameters or adjust the run_on_terminal_main.py defaults. This python script creates a virtual environment and install all packages. First clone the repo onto the cluster /server. Note for CON and CCM is number of notes is number of cpu -2 . 1. Install python <br> ```bash sudo apt-get update sudo apt-get install python3.10.5 ``` 2. Clone gitlab <br> ```bash git clone https://gitlab.com/qtb-hhu/marine/otter.git ``` 3. Run the Script (from project root use cd to navigate), that install the requirements and start the Analysis <br> ```bash python -m run_on_terminal_main --num_cores 10 ``` ### Convergent Cross Mapping: The CCM modul is based on https://github.com/PrinceJavier/causal_ccm. ### Acknowledgements Thanks to Prince Joseph Erneszer Javier for developing CCM package in python. Thanks the Alfred-Wegener-Institute for funding parts of the development of the project. ### Citation Please cite Beyond blooms: the winter ecosystem reset determines microeukaryotic community dynamics in the Fram Strait. when using otter: https://www.nature.com/articles/s43247-024-01782-0 and the software https://zenodo.org/records/13840807. ``` @article{oldenburg2024beyond, title={Beyond blooms: the winter ecosystem reset determines microeukaryotic community dynamics in the Fram Strait}, author={Oldenburg, Ellen and Kronberg, Raphael M and Metfies, Katja and Wietz, Matthias and von Appen, Wilken-Jon and Bienhold, Christina and Popa, Ovidiu and Ebenh{\\\"o}h, Oliver}, journal={Communications Earth \\& Environment}, volume={5}, number={1}, pages={643}, year={2024}, publisher={Nature Publishing Group UK London} } ```\n",
                "dependencies": "altair==4.0 numpy==1.26.4 pandas==2.2.3 networkx==3.1 jupyter==1.0.0 ennemi==1.3.0 scipy==1.10.1 matplotlib==3.9.4 statsmodels==0.14.0 tqdm==4.65.0 pytest==7.1.1 black==24.8.0 pylint openpyxl scikit-learn==1.3.0 scikit-bio==0.5.9 streamlit==1.17.0 seaborn==0.13.2 umap-learn==0.5.6 plotly kaleido==0.2.1 matplotlib_venn dask\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/training-catalogue-for-photon-neutron",
            "repo_link": "https://github.com/pan-training/training-catalogue",
            "content": {
                "codemeta": "{\"@context\":\"https://doi.org/10.5063/schema/codemeta-2.0\",\"@type\":\"SoftwareSourceCode\",\"identifier\":\"PaN training-catalogue\",\"description\":\"Browsing, discovering and organising PaN sciences training resources.\",\"name\":\"PaN Training Catalogue\",\"codeRepository\":\"https://github.com/pan-training/training-catalogue\",\"issueTracker\":\"https://github.com/pan-training/training-catalogue/issues\",\"license\":\"https://opensource.org/licenses/BSD-3-Clause\",\"version\":\"1.0\",\"author\":[],\"contIntegration\":\"\",\"developmentStatus\":\"active\",\"downloadUrl\":\"https://github.com/pan-training/training-catalogue/archive/master.zip\",\"keywords\":[\"training\",\"software\"],\"softwareVersion\":\"1.1\",\"dateCreated\":\"2021-06-11\",\"datePublished\":\"2021-06-11\",\"programmingLanguage\":\"Ruby\"}\n",
                "readme": "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7015078.svg)](https://doi.org/10.5281/zenodo.7015078) # PaN Training Catalogue (based on the TeSS Trainning Catalogue from ELIXIR) This repository contains the sourcecode of our [PaN Training Catalogue](https://pan-training.eu). This catalogue is based on the [TeSS Trainning Catalogue](https://github.com/ElixirTeSS/TeSS) from the [ELIXIR](https://elixir-europe.org) project and is used in our Photon and Neutron (PaN) projects [ExPaNDS](https://expands.eu) and [PaNOSC](https://panosc.eu). ## Setup ``` sudo apt-get install git postgresql libpq-dev imagemagick openjdk-8-jre nodejs redis-server ``` Clone the TeSS source code via git: ``` git clone https://github.com/ElixirTeSS/TeSS.git ``` ### RVM, Ruby, Gems It is typically recommended to install Ruby with RVM. With RVM, you can specify the version of Ruby you want installed, plus a whole lot more (e.g. gemsets). Full installation instructions for RVM are available online. In short: ``` sudo apt-get install software-properties-common sudo apt-add-repository -y ppa:rael-gc/rvm sudo apt-get update sudo apt-get install rvm rvm user gemsets ``` TeSS was developed using Ruby 2.4.5. We recommend using version 2.7.4 for our PaN catalague. To install recommended version of ruby and create a gemset, you can do something like the following: ``` rvm install `cat .ruby-version` /bin/bash --login rvm get stable --auto-dotfiles rvm use --create `cat .ruby-version`@`cat .ruby-gemset` ``` Bundler provides a consistent environment for Ruby projects by tracking and installing the exact gems and versions that are needed for your Ruby application. ``` gem install bundler ``` ### Redis/Sidekiq We installed Redis before... but start Sidekiq! ``` bundle exec sidekiq ``` ...or as a daemon in the background for production: ``` bundle exec sidekiq -d -L log/sidekiq.log -e production ``` Note that program 'gem' (a package management framework for Ruby called RubyGems) gets installed when you install RVM so you do not have to install it separately. Once you have Ruby, RVM and bundler installed, from the root folder of the app do: ``` bundle install ``` Follow the steps on the official GitHub and setup PostgrSQL [repo](https://github.com/ElixirTeSS/TeSS), Solr, ... In a first development instance is is necessary to add the database login information in `secrets.yml`. ### Set up environment `bin/rails db:environment:set RAILS_ENV=development or production!!!` ### Solr TeSS uses Apache Solr to power its search and filtering system. To start solr, run: ``` bundle exec rake sunspot:solr:start ``` You can replace start with stop or restart to stop or restart solr. You can use reindex to reindex all records. ``` bundle exec rake sunspot:solr:reindex ``` ### Database and Config From the app's root directory, create several config files by copying the example files. ``` cp config/tess.example.yml config/tess.yml cp config/sunspot.example.yml config/sunspot.yml cp config/secrets.example.yml config/secrets.yml ``` Create Postgres DB with user `tess_user` and edit `config/secrets.yml` to configure the database name, user and password defined before. Edit `config/secrets.yml` to configure the app's secret_key_base which you can generate with: ``` bundle exec rake secret ``` Create the databases: ``` bundle exec rake db:create:all ``` Start Solr: ``` bundle exec rake sunspot:solr:start bundle exec rake sunspot:solr:reindex ``` Create the database structure and load in seed data: Note: Ensure you have started Solr before running this command! ``` $ bundle exec rake db:setup ``` ### Dev Server The dev server can evaluated with ``` bundle exec sidekiq ``` and ``` bundle exec rails server ``` and accessed via: http://localhost:3000 #### Setup Administrators Once you have a local TeSS succesfully running, you may want to setup administrative users. To do this register a new account in TeSS through the registration page. Then go to the applications Rails console: ``` bundle exec rails c ``` Find the user and assign them the administrative role. This can be completed by running this (where myemail@domain.co is the email address you used to register with): ``` 2.2.6 :001 > User.find_by_email('myemail@domain.co').update_attributes(role: Role.find_by_name('admin')) ``` ## Deployment: Providing TeSS using an Application Server After setting up TeSS, the configuration of an application server (**Phusion Passenger** is an application server and it is often used to power Ruby sites) is required. Or my prefered setup with Nginx: https://www.phusionpassenger.com/library/config/nginx/intro.html We need additinal packages: ``` apt-get install apache2-dev apt-get install libcurl4-gnutls-dev ``` After successfull development deployment add the Passenger Gem with: ``` sudo apt-get install -y dirmngr gnupg sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 561F9B9CAC40B2F7 sudo apt-get install -y apt-transport-https ca-certificates # Add our APT repository sudo sh -c 'echo deb https://oss-binaries.phusionpassenger.com/apt/passenger bionic main > /etc/apt/sources.list.d/passenger.list' sudo apt-get update # Install Passenger + Nginx module sudo apt-get install -y libnginx-mod-http-passenger ``` Check the installation with: ``` sudo /usr/bin/passenger-config validate-install sudo /usr/sbin/passenger-memory-stats ``` ...and add the recommended lines to your Nginx configuration file and finish the Passenger setup. ``` server { # SSL configuration listen 443; ssl on; proxy_set_header X_FORWARDED_PROTO https; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-Url-Scheme $scheme; proxy_redirect off; proxy_max_temp_file_size 0; server_name pan-training.hzdr.de; ssl_certificate /etc/ssl/certs/pan.cert; ssl_certificate_key /etc/ssl/private/pan.key; ssl_session_timeout 1d; ssl_session_cache shared:MozSSL:10m; # about 40000 sessions ssl_session_tickets off; ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384; ssl_prefer_server_ciphers off; root /var/www/catalogue/public; passenger_enabled on; passenger_ruby /usr/share/rvm/gems/ruby-2.4.5@tess/wrappers/ruby; passenger_document_root /var/www/catalogue/public/; passenger_sticky_sessions on; } ``` Then we initialize the production environment: ``` bundle exec rake db:setup RAILS_ENV=production ``` or clean init: ``` bundle exec rake db:reset RAILS_ENV=production ``` ...and reindex Solr: ``` bundle exec rake sunspot:solr:start RAILS_ENV=production bundle exec rake sunspot:solr:reindex RAILS_ENV=production ``` Create an admin user and assign it appropriate 'admin' role bu looking up that role in console in model Role (default roles should be created automatically): ``` bundle exec rails c -e production ``` The first time and each time a css or js file is updated: ``` bundle exec rake assets:clean RAILS_ENV=production bundle exec rake assets:precompile RAILS_ENV=production ``` and reindexing the matadata: ``` bundle exec rake sunspot:solr:reindex RAILS_ENV=production ``` Status Check and restart: ``` bundle exec rake sunspot:solr:start RAILS_ENV=production service nginx restart bundle exec sidekiq -d -L log/sidekiq.log -C config/sidekiq.yml -e production service redis-server restart passenger-memory-stats passenger-status ``` Logfiles: ``` /var/log/redis/redis-server.log /var/log/nginx/error.log /var/log/catalogue/passenger.log /var/log/catalogue/sidekiq.log /var/log/catalogue/production.log /var/log/catalogue/sunspot-solr-production.log ```\n",
                "dependencies": "source 'https://rubygems.org' #ruby \"2.4.5\" # Bundle edge Rails instead: gem 'rails', github: 'rails/rails' gem 'rails', '~> 5.2' #see here https://github.com/rails/rails/issues/36970 #gem \"bootsnap\", \">= 1.1.0\", require: false # New Rails 5.2 default gem gem \"bootsnap\", \">= 1.4.6\" # Use postgresql as the database for Active Record gem 'pg' # For installing PG on macs: gem 'lunchy' # Use SCSS for stylesheets gem 'sass-rails', '~> 5.0' # Use Uglifier as compressor for JavaScript assets gem 'uglifier', '>= 1.3.0' # See https://github.com/rails/execjs#readme for more supported runtimes gem 'therubyracer'#, platforms: :ruby # CRUD of resources via a UI gem 'rails_admin' gem 'haml', '~> 5.0.4' # Rails admin needs this, but doesn't fix the version to one that works with Rails 5.2 # Authentication gem 'devise' gem 'omniauth', '~> 1.9.1' gem 'omniauth_openid_connect', '0.3.3' gem 'openid_connect', '~> 1.1' gem 'json-jwt', '~>1.11.0' # Activity logging gem 'public_activity', '~> 1.6.1' gem 'simple_token_authentication', '~> 1.17' gem 'bootstrap-sass', '>= 3.4.1' gem 'font-awesome-sass', '~> 4.7.0' gem 'friendly_id', '~> 5.5.0' gem 'sunspot_rails', '~> 2.2.7' gem 'sunspot_solr', '= 2.2.0' gem 'progress_bar', '~> 1.1.0' gem 'activerecord-session_store' gem 'gravtastic', '~> 3.2.6' gem 'dynamic_sitemaps', github: 'lassebunk/dynamic_sitemaps', branch: 'master' gem 'whenever' # These are required for Sidekiq, to look up scientific topics gem 'httparty' gem 'sidekiq' gem 'slim' # Use jquery as the JavaScript library gem 'jquery-rails' # Turbolinks makes following links in your web application faster. Read more: https://github.com/rails/turbolinks gem 'turbolinks' gem 'jquery-turbolinks' # Build JSON APIs with ease. Read more: https://github.com/rails/jbuilder gem 'jbuilder' # bundle exec rake doc:rails generates the API under doc/api. gem 'sdoc', '~> 0.4.0', group: :doc # Gem for creating before_validation callbacks for stripping whitespace gem 'auto_strip_attributes', '~> 2.0' # Gem for validating URLs gem 'validate_url', '~> 1.0.15' gem 'simple_form' # Gem for rendering Markdown gem 'redcarpet', '~> 3.5.1' # Gem for paginating search results gem 'will_paginate' #gem 'will_paginate-bootstrap', '~> 1.0.1' # Gem for authorisation gem 'pundit', '~> 1.1.0' # Simple colour picker from a predefined list gem 'jquery-simplecolorpicker-rails' # For getting date of materials for the home page #gem 'by_star', '~> 2.2.1', git: 'git://github.com/radar/by_star' gem 'by_star', '~> 4.0.0', git: 'https://github.com/radar/by_star' # Use ActiveModel has_secure_password # gem 'bcrypt', '~> 3.1.7' # Use Unicorn as the app server # gem 'unicorn' # Use Capistrano for deployment # gem 'capistrano-rails', group: :development gem 'handlebars_assets' gem 'paperclip', '~> 5.2.1' gem 'icalendar', '~> 2.4.1' gem 'bootstrap-datepicker-rails', '~> 1.6.4.1' gem 'rack-cors', require: 'rack/cors' gem 'recaptcha', require: 'recaptcha/rails' gem 'linkeddata' # Used for lat/lon rake task gem 'geocoder' gem 'redis' gem 'active_model_serializers' gem 'private_address_check' # For the link monitor rake taks gem 'time_diff' #passenger gem \"passenger\", \">= 5.0.25\", require: \"phusion_passenger/rack_handler\" source 'https://rails-assets.org' do gem 'rails-assets-markdown-it', '~> 7.0.1' gem 'rails-assets-moment', '~> 2.15.0' gem 'rails-assets-eonasdan-bootstrap-datetimepicker', '~> 4.17.42' gem 'rails-assets-devbridge-autocomplete', '~> 1.2.26' gem 'rails-assets-clipboard', '~> 1.5.12' end group :test do gem 'minitest', '5.10.3' gem 'fakeredis' gem 'rails-controller-testing' end group :development, :test do # Call 'byebug' anywhere in the code to stop execution and get a debugger console gem 'webmock', '~> 3.4.2' gem 'byebug' gem 'simplecov' gem 'rubocop' gem 'codacy-coverage', :require => false end group :development do # Access an IRB console on exception pages or by using <%= console %> in views gem 'web-console' # Spring speeds up development by keeping your application running in the background. Read more: https://github.com/rails/spring #gem 'spring' gem 'listen' end group :production do #gem 'passenger', '~> 5.0.25' end gem 'mimemagic', github: 'mimemagicrb/mimemagic', ref: '01f92d86d15d85cfd0f20dabd025dcbd36a8a60f' gem 'tess_api_client', :git => 'https://github.com/pan-training/training-catalogue-api-client.git' #analytics (gdpr compliant) gem 'ahoy_matey' gem 'blazer' gem 'faker', :git => 'https://github.com/faker-ruby/faker.git', :branch => 'master' gem 'rake_text' gem 'language_list', '~> 1.1'\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pasta-bit-vector",
            "repo_link": "https://github.com/pasta-toolbox/bit_vector/",
            "content": {
                "codemeta": "",
                "readme": "# pasta::bit_vector <p align=\"center\"> <img width=250 height=175 src=\"https://raw.githubusercontent.com/pasta-toolbox/bit_vector/main/docs/images/logo_pasta_bit_vector.svg\" /> </p> [![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0) [![DOI](https://zenodo.org/badge/419381885.svg)](https://zenodo.org/badge/latestdoi/419381885) [![pasta::bit_vector CI](https://github.com/pasta-toolbox/bit_vector/actions/workflows/ctest.yml/badge.svg)](https://github.com/pasta-toolbox/bit_vector/actions/workflows/ctest.yml) [![codecov](https://codecov.io/gh/pasta-toolbox/bit_vector/branch/main/graph/badge.svg?token=2QD6ME44SU)](https://codecov.io/gh/pasta-toolbox/bit_vector) This header-only library contains a highly tuned (uncompressed) bit vector implementation with multiple space efficient rank and select support data structures. Our fastest rank and select support has a space overhead of only ~3.51% and makes use of data level parallelism via SIMD instructions. If you use this code in a scientific context, please cite our paper. ```bibtex @inproceedings{Kurpicz2022CompactRankSelect, author = {Florian Kurpicz}, title = {Engineering Compact Data Structures for Rank and Select Queries on Bit Vectors}, booktitle = {{SPIRE}}, series = {Lecture Notes in Computer Science}, volume = {13617}, pages = {257--272}, publisher = {Springer}, year = {2022}, doi = {10.1007/978-3-031-20643-6\\_19} } ``` ## Contents This repository contains the following algorithms and data structures. Our [documentation][] contains in-depth on the usage of all these algorithms and data structures including easy to follow examples. You can find the example in the screenshot below as text, too. [![Screenshot Documentation](https://raw.githubusercontent.com/pasta-toolbox/bit_vector/main/docs/images/screenshot_documentation_v1.0.0.png)](https://www.pasta-toolbox.org/bit_vector/) ### Bit Vectors Bit vectors play an important role in many compressed text indices, e.g., the FM-index. This repository contains the following bit vector implementations: - highly tuned [uncompressed bit vector][] with access operator - compact [rank](include/pasta/bit_vector/support/rank.hpp) and [select](include/pasta/bit_vector/support/rank_select.hpp) support for the uncompressed bit vector based on > Dong Zhou and David G. Andersen and Michael Kaminsky, > Space-Efficient, High-Performance Rank and Select Structures on Uncompressed Bit Sequences, > SEA 2013. - improved [rank](include/pasta/bit_vector/support/flat_rank.hpp) and [select](include/pasta/bit_vector/support/flat_rank_select.hpp) support requiring the same amount of memory but providing faster rank (up to 8% speedup) and select (up to 16.5% speedup) queries, and - a very fast [rank](include/pasta/bit_vector/support/wide_rank.hpp) support that can also answer [select](include/pasta/bit_vector/support/wide_rank_select.hpp) queries. [uncompressed bit vector]: include/pasta/bit_vector/bit_vector.hpp ### Easy to Use Since this is a header-only library, you have to simply add it to your projects include path to use it. A small example can be found below. We refer to the [documentation][] for more information. ```cpp #include <pasta/bit_vector/bit_vector.hpp> #include <pasta/bit_vector/support/flat_rank_select.hpp> // Create a bit vector of size 1000 containing only zeros and flip every other bit. pasta::BitVector bv(1000, 0); for (size_t i = 0; i < bv.size(); ++i) { if (i % 2 == 0) { bv[i] = 1; } } // Print the bit vector to see that everything worked ;-) for (auto it = bv.begin(); it != bv.end(); ++it) { std::cout << ((*it == true) ? '1' : '0'); } std::cout << std::endl; // Create rank and select support and print the result of some queries. pasta::FlatRankSelect rs(bv); std::cout << rs.rank0(250) << \", \" << rs.rank1(250) << \", \" << rs.select0(250) << \", \" << rs.rank1(250) << std::endl; ``` ### Benchmarks and Tests There exist an easy to use [benchmark][], which helps to compare the implementations in this repository. To build the benchmark, run the CMake command with `-DPASTA_BIT_VECTOR_BUILD_BENCHMARKS=On`. Our tests are contained in the folder [tests][]. To build the tests, run the CMake command with `-DPASTA_BIT_VECTOR_BUILD_TESTS=On`. We also conducted an extensive experimental evaluation. To this end, we use our [rank and select benchmark][] where we compare our implementations with many other compact rank and select data structures. We refer to our paper for a full description of the results, i.e., hardware, inputs, and competitors. Below, you can find some of the figures we present in the paper. ![Screenshot Documentation](https://raw.githubusercontent.com/pasta-toolbox/bit_vector/main/docs/images/space_requirements_v1.0.0.png) ![Screenshot Documentation](https://raw.githubusercontent.com/pasta-toolbox/bit_vector/main/docs/images/rank_times_v1.0.0.png) ![Screenshot Documentation](https://raw.githubusercontent.com/pasta-toolbox/bit_vector/main/docs/images/select_times_v1.0.0.png) ![Screenshot Documentation](https://raw.githubusercontent.com/pasta-toolbox/bit_vector/main/docs/images/select_times_pasta_only_v1.0.0.png) [benchmark]: benchmarks/bit_vector_benchmark.cpp [rank and select benchmark]: https://github.com/pasta-toolbox/bit_vector_experiments [tests]: tests/ ## How to Get This Below, we list all commands that are required to build the code in this repository. To this end, we provide three CMake presets (_debug_, _release_, and _release with debug information_). - The debug preset creates a `debug` folder and uses the compiler flags `-DDEBUG -O0 -g -ggdb -fsanitize=address`. - The release preset creates a `build` folder and uses the compiler flags `-DNDEBUG -march=native -O3`. - The release with debug information preset creates a `build_with_debug_info` folder and uses the compiler flags `-DDEBUG -g -march=native -O3`. Per default, we use the following compiler flags: `-Wall -Wextra -Wpedantic -fdiagnostics-color=always`. ### Requirements pasta::bit_vector is written in C++20 and requires a compiler that [supports][] it. We use [Ninja][] as build system. For more information on how to use this library, please refer to our [documentation][]. [supports]: https://en.cppreference.com/w/cpp/compiler_support [Ninja]: https://ninja-build.org/ ### tl;dr To just clone the source code, use the following. ```bash git clone git@github.com:pasta-toolbox/bit_vector cd bit_vector git submodule update --init --recursive ``` If you also want to build the test, please continue with the following commands. ```bash cmake --preset=[debug|build|relwithdeb]-DPASTA_BIT_VECTOR_BUILD_TESTS=On cmake --build --preset=[debug|release|relwithdeb] ctest --test-dir [debug|build|relwithdeb] ``` [documentation]: https://www.pasta-toolbox.org/bit_vector/\n",
                "dependencies": "# ############################################################################## # CMakeLists.txt # # Copyright (C) 2021-2024 Florian Kurpicz <florian@kurpicz.org> # # pasta::bit_vector is free software: you can redistribute it and/or modify it # under the terms of the GNU General Public License as published by the Free # Software Foundation, either version 3 of the License, or (at your option) any # later version. # # pasta::bit_vector is distributed in the hope that it will be useful, but # WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or # FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more # details. # # You should have received a copy of the GNU General Public License along with # pasta::bit_vector. If not, see <http://www.gnu.org/licenses/>. # # ############################################################################## cmake_minimum_required(VERSION 3.25) project( pasta_bit_vector DESCRIPTION \"Bit Vector with Compact and Fast Rank and Select Support\" LANGUAGES CXX VERSION 1.0.1 ) set(CMAKE_CXX_STANDARD 20) set(CMAKE_CXX_STANDARD_REQUIRED ON) include(FetchContent) if (PROJECT_IS_TOP_LEVEL) # Enable easy formatting FetchContent_Declare( Format.cmake GIT_REPOSITORY https://github.com/TheLartians/Format.cmake GIT_TAG v1.8.3 ) FetchContent_MakeAvailable(Format.cmake) add_subdirectory(docs) endif () # Options when compiling pasta::bit_vector Build tests option(PASTA_BIT_VECTOR_BUILD_TESTS \"Build pasta::bit_vector's tests.\" OFF) # Build benchmark tools option(PASTA_BIT_VECTOR_BUILD_BENCHMARKS \"Build pasta::bit_vector's benchmarks.\" OFF ) # Build pasta::bit_vector with code coverage options option(PASTA_BIT_VECTOR_COVERAGE_REPORTING \"Enable coverage reporting for pasta::bit_vector\" OFF ) # Optional code coverage (library compile options are only set if coverage # reporting is enabled add_library(pasta_bit_vector_coverage_config INTERFACE) if (PASTA_BIT_VECTOR_COVERAGE_REPORTING) target_compile_options( pasta_bit_vector_coverage_config INTERFACE -fprofile-arcs -ftest-coverage ) target_link_libraries(pasta_bit_vector_coverage_config INTERFACE gcov) endif () # pasta::bit_vector interface definitions add_library(pasta_bit_vector INTERFACE) target_include_directories( pasta_bit_vector INTERFACE ${CMAKE_CURRENT_SOURCE_DIR}/include ) target_link_libraries( pasta_bit_vector INTERFACE pasta_utils pasta_bit_vector_coverage_config tlx ) # Use FetchContent to load dependencies FetchContent_Declare( pasta_utils GIT_REPOSITORY https://github.com/pasta-toolbox/utils.git GIT_TAG origin/main ) FetchContent_MakeAvailable(pasta_utils) FetchContent_Declare( tlx GIT_REPOSITORY https://github.com/tlx/tlx.git GIT_TAG origin/main ) FetchContent_MakeAvailable(tlx) # Optional test if (PASTA_BIT_VECTOR_BUILD_TESTS) enable_testing() add_subdirectory(tests) endif () # Optional benchmarks if (PASTA_BIT_VECTOR_BUILD_BENCHMARKS) add_executable(bit_vector_benchmark benchmarks/bit_vector_benchmark.cpp) target_link_libraries( bit_vector_benchmark PUBLIC pasta_bit_vector tlx pasta_utils optimized pasta_memory_monitor ) endif () # ##############################################################################\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pdaf",
            "repo_link": "https://github.com/PDAF/PDAF",
            "content": {
                "codemeta": "",
                "readme": "# PDAF (Parallel Data Assimilation Framework) Copyright 2004-2024, Lars Nerger, Alfred Wegener Institute, Helmholtz Center for Polar and Marine Research, Bremerhaven, Germany. For license information, please see the file LICENSE.txt. For full documentation and tutorial, see: http://pdaf.awi.de ## Introduction PDAF is a framework for data assimilation. PDAF can be used to assess data assimilation methods with small models, to perform real data assimilation with high-dimensional models, and to teach ensemble data assimilation. PDAF provides - A parallel infrastructure, using MPI and OpenMP, to implement a parallel data assimilation system based on existing numerical models (typically of components of the Earth system). - A selection of ensemble data assimilation algorithms based on the Kalman filter or nonlinear filters (see list below) - A selection of 3D variational methods, both with parameterized and ensemble covariance matrix - Functions for ensemble diagnostics - Functionality to generate synthetic observations for data assimilation studies (e.g. OSSEs) The PDAF release provides also - Tutorial codes demontrating the implementation - Code templates to assist in the implementation - Toy models fully implemented with PDAF for the study of data assimilation methods. - Model bindings for using PDAF with different models ## First Steps with PDAF A good starting point for using PDAF is to run a tutorial example. The directory /tutorial contains files demonstrating the application of PDAF with a simple 2-dimensional example. The web site http://pdaf.awi.de/trac/wiki/FirstSteps provides hints on getting started with PDAF and https://pdaf.awi.de/trac/wiki/PdafTutorial holds the tutorial slide sets that explain the implementation steps and how to compile and run the tutorial examples. ## Models The directory models/ contains toy models that are fully implemented with PDAF. These models can be used to assess the behavior of different assimilation algorithms. - **lorenz96/** This directory contains the Lorenz-96 model, which is a widely used model to assess data assimilation methods. Provided is a full implementation of the data assimilation with various filters and options. This model can be configured to have a sufficiently large state dimension to test low-rank filter algorithms. - **lorenz63/** This directory contains the Lorenz-63 model, which is a classical 3-variable model with chaotic dynamics. Provided is a full implementation of the data assimilation with various filters and options. The small state dimension and nonlinear dynamics make it a suitable test case for the standard particle filter (PF). - **lorenz05b/** This directory contains the model Lorenz-2005 model II. Provided is a full implementation of the data assimilation with various filters and options. - **lorenz05c/** This directory contains the two-scale model Lorenz-2005 model III. Provided is a full implementation of the data assimilation with various filters and options. Instructions on how to run data assimilation experiments with these models are provided on the PDAF web site. ## Installation of the PDAF library The PDAF library will be automatically built when compiling a tutorial case or one of the models. However, one can also separately build the library. In order to build the PDAF library you need a Fortran 90 compiler, and 'make' 1. Choose a suitable include file for the make process and/or edit one. See the directory make.arch/ for several provided include files. There are include files for compilation with and without MPI. Note: PDAF is generally intended for parallel computing using MPI. However, it can be compiled for serial computing. To compile PDAF for this case, a simplified MPI header file is included und should be in the include path. In addition, a dummy implementation of MPI, which behaves like MPI in the single-process case, is provided in the directory nullmpi/. For the serial case, this file should also be compiled and linked when PDAF is linked to a program. 2. Set the environment variable $PDAF_ARCH to the name of the include file (without ending .h). Alternatively you can specify PDAF_ARCH on the command line when running 'make' in step 3. 3. cd into the directory src/ and execute 'make' at the prompt. This will compile the sources and generate a library file that includes the ensemble filter methods in the directory lib/. To generate the PDAF library including the 3D-Var methods and the solvers from the external libraries in /external/ execute 'make pdaf-var' at the prompt. ## Test suite The directory testsuite/ contains another set of example implementations. This is more for 'internal use'. We use these implementations to validate PDAF. The model is trivial: At each time step simply the time step size is added to the state vector. In this example all available filters are implemented. ## Verifying your installation The tutorial implementations can be verified as follows: You can run the script **runtests.sh** in the main tutorial directory **tutorial**. This script will compile and run all tutorial implementations. Afterwards the outputs at the final time step are checked against reference outputs from the directory verification using a Python script. You can also compare the output files like out.online_2D_parallelmodel with reference files. (Note: The script runtests.sh uses the generic compile definitions for Linux with the gfotran compiler. For other systems, you might need to change the settings for the make definitions files). The testsuite also provides a functionality for verification: Using 'make' one can run test cases for the verification which are compared to reference outputs provided in the sub-directories of the directory testsuite/tests_dummy1D for different computers and compilers. In particular the online case dummymodel_1D and the offline test offline_1D can be run. Scripts for serial (non-parallel) execution as well as example scripts for running parallel test jobs on computers with SLURM or PBS batch systems are provided. An installation of PDAF can be verified using the test suite as follows: 1. prepare the include file in make.arch 2. cd to testsuite/src 3. Build and execute the online experiments: 'make pdaf_dummy_online' and 'make test_pdaf_online > out.test_pdaf_online' 4. Build and execute the offline experiments: 'make pdaf_dummy_online' and 'make test_pdaf_offline > out.test_pdaf_offline' 6. Check the files out.test_pdaf_online and out.test_pdaf_offline At the end of the file, you see a list of Checks done using a Python script. Here the outputs are compared with reference outputs produced with gfortran and MacOS. You can also diff the files to corresponding files in one of the example-directories in ../tests_dummy1D. Here, also reference output files, like output_lestkf0.dat are stored. ## Data Assimilation Algorithms The filter algorithms in PDAF are: **Filters with global analysis step** - **EnKF** (The classical perturbed-observations Ensemble Kalman filter) [G. Evensen, J. Geophys. Res. 99 C5 (1994) 10143-10162, G. Burgers et al., Mon. Wea. Rev. 126 (1998) 1719-1724] - **ESTKF** (Error Subspace Transform Kalman filter) [L. Nerger et al. Mon. Wea. Rev. 140 (2012) 2335-2345, doi:10.1175/MWR-D-11-00102.1] - **ETKF** (Ensemble Transform Kalman filter) [C. H. Bishop et al. Mon. Wea. Rev. 129 (2001) 420-436] The implementation in PDAF follows that described for the LETKF, but as a global filter. - **SEIK** (Singular \"Evolutive\" Interpolated Kalman) filter This is the full ensemble variant of the SEIK (Singular \"Interpolated\" Extended Kalman) filter. [SEIK: D.-T. Pham et al., C. R. Acad Sci., Ser. III, 326 (1009) 255-260, for the SEIK variant in PDAF see L. Nerger et al., Tellus 57A (2005) 715-735, doi:10.3402/tellusa.v57i5.14732] - **SEEK** (Singular \"Evolutive\" Extended Kalman) filter [D.-T. Pham et al., J. Mar. Syst. 16 (1998) 323-340] - **NETF** (Nonlinear Ensemble Transform Filter) [J. Toedter, B. Ahrens, Mon. Wea. Rev. 143 (2015) 1347-1367, doi:10.1175/MWR-D-14-00108.1] - **PF** (Particle filter with importance resampling) [see S. Vetra-Carvalho et al., Tellus A 70 (2018) 1445364, doi:10.1080/16000870.2018.1445364] **Filters with localized analysis step** - **LESTKF** (Local Error Subspace Transform Kalman filter) [L. Nerger et al. Mon. Wea. Rev. 140 (2012) 2335-2345, doi:10.1175/MWR-D-11-00102.1] - **LETKF** (Local Ensemble Transform Kalman filter) [B. R. Hunt et al., Physica D 230 (2007) 112-126, doi:10.1016/j.physd.2006.11.008] - **LSEIK** (Local Singular \"Evolutive\" Interpolated Kalman) filter [L. Nerger et al., Oce. Dyn. 56 (2006) 634-649, doi:10.1007/s10236-006-0083-0] - **LEnKF** (The classical perturbed-observations Ensemble Kalman filter with localization) [G. Evensen, J. Geophys. Res. 99 C5 (1994) 10143-10162, G. Burgers et al., Mon. Wea. Rev. 126 (1998) 1719-1724] - **LNETF** (Nonlinear Ensemble Transform Filter with observation localization) [J. Toedter, B. Ahrens, Mon. Wea. Rev. 143 (2015) 1347-1367, doi:10.1175/MWR-D-14-00108.1] - **LKNETF** (Local Kalman-nonlinear ensemble transform filter) [L. Nerger, Q. J. R. Meteorol Soc., 148 (2022) 620-640, doi:10.1002/qj.4221] All filter algorithms are fully parallelized with MPI and optimized. The local filters (LSEIK, LETKF, LESTKF, LNETF, LKNETF) are in addition parallelized using OpenMP. **Smoother extensions** are included for the filters ESTKF/LESTKF, ETKF/LETKF, EnKF, NETF/LNETF. **3D-Var methods** Next to the ensemble filter methods, different 3D-Var methods are provided: - **parameterized 3D-Var** - **3D ensemble** Var with ensemble transformation by ESTKF or LESTKF - **hybrid 3D-Var** with ensemble transformation by ESTKF or LESTKF The 3D-Var methods are implemented as incremental 3D-Var schemes following Bannister, Q. J. Royal Meteorol. Soc., 143 (2017) 607-633, doi:10.1002/qj.2982. PDAF is written in Fortran (mainly Fortran 90 with some features from Fortran 2003). The compilation and execution has been tested on the different systems ranging from notebook computers to supercomputers, e.g.: - Linux - MacOS - Cray CLE - NEC Super-UX - Microsoft Windows 10 with Cygwin ## Contact Information Please send comments, suggestions, or bug reports to pdaf@awi.de\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/peakperformance",
            "repo_link": "https://github.com/JuBiotech/peak-performance",
            "content": {
                "codemeta": "",
                "readme": "[![PyPI version](https://img.shields.io/pypi/v/peak-performance)](https://pypi.org/project/peak-performance/) [![pipeline](https://github.com/jubiotech/peak-performance/workflows/pipeline/badge.svg)](https://github.com/JuBiotech/peak-performance/actions) [![coverage](https://codecov.io/gh/jubiotech/peak-performance/branch/main/graph/badge.svg)](https://app.codecov.io/gh/JuBiotech/peak-performance) [![documentation](https://readthedocs.org/projects/peak-performance/badge/?version=latest)](https://peak-performance.readthedocs.io/en/latest) [![DOI](https://joss.theoj.org/papers/10.21105/joss.07313/status.svg)](https://doi.org/10.21105/joss.07313) [![DOI](https://zenodo.org/badge/713469041.svg)](https://zenodo.org/doi/10.5281/zenodo.10255543) # About PeakPerformance PeakPerformance employs Bayesian modeling for chromatographic peak data fitting. This has the innate advantage of providing uncertainty quantification while jointly estimating all peak parameters united in a single peak model. As Markov Chain Monte Carlo (MCMC) methods are utilized to infer the posterior probability distribution, convergence checks and the aformentioned uncertainty quantification are applied as novel quality metrics for a robust peak recognition. # Installation It is highly recommended to follow the following steps and install ``PeakPerformance`` in a fresh Python environment: 1. Install the package manager [Mamba](https://github.com/conda-forge/miniforge/releases). Choose the latest installer at the top of the page, click on \"show all assets\", and download an installer denominated by \"Mambaforge-version number-name of your OS.exe\", so e.g. \"Mambaforge-23.3.1-1-Windows-x86_64.exe\" for a Windows 64 bit operating system. Then, execute the installer to install mamba and activate the option \"Add Mambaforge to my PATH environment variable\". ⚠ If you have already installed Miniconda, you can install Mamba on top of it but there are compatibility issues with Anaconda. ℹ The newest conda version should also work, just replace `mamba` with `conda` in step 2. 2. Create a new Python environment in the command line using the provided [`environment.yml`](https://github.com/JuBiotech/peak-performance/blob/main/environment.yml) file from the repo. Download `environment.yml` first, then navigate to its location on the command line interface and run the following command: ``` mamba env create -f environment.yml ``` Naturally, it is alternatively possible to just install ``PeakPerformance`` via pip: ```bash pip install peak-performance ``` # First steps Be sure to check out our thorough [documentation](https://peak-performance.readthedocs.io/en/latest). It contains not only information on how to install PeakPerformance and prepare raw data for its application but also detailed treatises about the implemented model structures, validation with both synthetic and experimental data against a commercially available vendor software, exemplary usage of diagnostic plots and investigation of various effects. Furthermore, you will find example notebooks and data sets showcasing different aspects of PeakPerformance. # How to contribute If you encounter bugs while using PeakPerformance, please bring them to our attention by opening an issue. When doing so, describe the problem in detail and add screenshots/code snippets and whatever other helpful material you can provide. When contributing code, create a local clone of PeakPerformance, create a new branch, and open a pull request (PR). # How to cite Head over to Zenodo to [generate a BibTeX citation](https://doi.org/10.5281/zenodo.10255543) for the latest release. In addition to the utilized software version, please cite our scientific publication over at the Journal of Open Source Software (JOSS). A detailed citation can be found in CITATION.cff and in the sidebar.\n",
                "dependencies": "[build-system] requires = [\"setuptools\", \"setuptools-scm\"] build-backend = \"setuptools.build_meta\" [project] name = \"peak_performance\" version = \"0.7.2\" authors = [ {name = \"Jochen Nießer\", email = \"j.niesser@fz-juelich.de\"}, {name = \"Michael Osthege\", email = \"m.osthege@fz-juelich.de\"}, ] description = \"A Python toolbox to fit chromatography peaks with uncertainty.\" readme = \"README.md\" requires-python = \">=3.9\" dynamic = [\"dependencies\"] keywords = [\"hplc\", \"mass-spectrometry\", \"uncertainty quantification\"] license = {text = \"AGPLv3\"} classifiers = [ \"Programming Language :: Python :: 3\", \"Operating System :: OS Independent\", \"License :: OSI Approved :: GNU Affero General Public License v3\", \"Intended Audience :: Science/Research\", ] [tool.setuptools.dynamic] dependencies = {file = [\"requirements.txt\"]} optional-dependencies = {dev = { file = [\"requirements-dev.txt\"] }} [tool.setuptools.packages.find] include = [\"peak_performance*\"] [project.urls] homepage = \"https://jugit.fz-juelich.de/IBG-1/micropro/peak-performance\" documentation = \"https://jugit.fz-juelich.de/IBG-1/micropro/peak-performance\" repository = \"https://jugit.fz-juelich.de/IBG-1/micropro/peak-performance\" [tool.pytest.ini_options] xfail_strict=true [tool.black] line-length = 100 [tool.ruff] line-length = 140 ignore-init-module-imports = true [tool.coverage.run] omit = [ # exclude tests files from coverage calculation \"**/test*.py\", \"**/example\", \"**/notebooks\", ] [tool.mypy] ignore_missing_imports = true exclude = [ 'test_.*?\\.py$', ]\narviz matplotlib numpy<2 pandas pymc>=5.9.1 pytensor scipy openpyxl numpy\nfrom setuptools import setup setup()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pedpy",
            "repo_link": "https://github.com/PedestrianDynamics/PedPy",
            "content": {
                "codemeta": "",
                "readme": "<div align=\"center\"> <img src=\"docs/source/_static/logo_text.svg\" height=\"100px\" alt=\"PedPy Logo\"> </div> ----------------- [![PyPI Latest Release](https://img.shields.io/pypi/v/pedpy.svg)](https://pypi.org/project/pedpy/) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pedpy) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7194992.svg)](https://doi.org/10.5281/zenodo.7194992) [![License](https://img.shields.io/pypi/l/pedpy.svg)](https://github.com/PedestrianDynamics/pedpy/blob/main/LICENSE) ![ci workflow](https://github.com/PedestrianDynamics/pedestrian-trajectory-analyzer/actions/workflows/ci.yml/badge.svg) [![codecov](https://codecov.io/gh/PedestrianDynamics/PedPy/graph/badge.svg?token=X5C9NTKAVK)](https://codecov.io/gh/PedestrianDynamics/PedPy) [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff) [![Documentation Status](https://readthedocs.org/projects/pedpy/badge/?version=latest)](http://pedpy.readthedocs.io/?badge=latest) [![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7046/badge)](https://bestpractices.coreinfrastructure.org/projects/7046) [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu) # PedPy: Analysis of pedestrian dynamics based on trajectory files. *PedPy* is a python module for pedestrian movement analysis. It implements different measurement methods for density, velocity and flow. If you use *PedPy* in your work, please cite it using the following information from zenodo: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7194992.svg)](https://doi.org/10.5281/zenodo.7194992) ## Getting started ### Setup Python For setting up your Python Environment a Python version >= 3.11 is recommended (our code is tested with 3.11, 3.12, and 3.13). To avoid conflicts with other libraries/applications the usage of virtual environments is recommended, see [Python Documentation](https://docs.python.org/3/library/venv.html) for more detail. ### Installing PedPy To install the latest **stable** version of *PedPy* and its dependencies from PyPI: ```bash python3 -m pip install pedpy ``` You can also install the latest version of *PedPy* directly from the repository, by following these steps: 1. Uninstall an installed version of *PedPy*: ```bash python3 -m pip uninstall pedpy ``` 2. Install latest version of *PedPy* from repository: ``` python3 -m pip install git+https://github.com/PedestrianDynamics/PedPy.git ``` ### Usage For first time users, have a look at the [getting started notebook](notebooks/getting_started.ipynb), as it shows the first steps to start an analysis with *PedPy*. A more detailed overview of *PedPy* is demonstrated in the [user guide notebook](notebooks/user_guide.ipynb). The [fundamental diagram notebook](notebooks/fundamental_diagram.ipynb) shows how to use *PedPy* for computing the fundamental diagram of a series of experiments. #### Interactive online session If you want to try out *PedPy* for the first time, you can find an interactive online environments for both notebooks here: - Getting started: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/PedestrianDynamics/PedPy/main?labpath=notebooks%2Fgetting_started.ipynb) - User guide: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/PedestrianDynamics/PedPy/main?labpath=notebooks%2Fuser_guide.ipynb) - Fundamental diagram: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/PedestrianDynamics/PedPy/main?labpath=notebooks%2Ffundamental_diagram.ipynb) **Note:** The execution might be slower compared to a local usage, as only limited resources are available. It is possible to also upload different trajectory files and run the analysis completely online, but this might not be advisable for long computations. #### Local usage of the notebooks For local usage of the notebooks, you can either download the notebooks and [demo files](notebooks/demo-data) from the GitHub repository or clone the whole repository with: ```bash git clone https://github.com/PedestrianDynamics/pedpy.git ``` For using either of the notebook some additional libraries need to be installed, mainly for plotting. You can install the needed libraries with: ```bash python3 -m pip install jupyter matplotlib ``` Afterward, you can start a jupyter server with: ```bash jupyter notebook ``` After navigating to one of the notebooks, you can see how the library can be used for different kinds of analysis. Some examples how the computed values can be visualized are also shown in the notebooks, e.g., density/velocity profiles, fundamental diagrams, N-T-diagrams, etc. ![voronoi](figs/voronoi_diagrams.png) ![density](figs/density_comparison.png)\n",
                "dependencies": "[project] name = \"PedPy\" dynamic = ['version'] authors = [{ name = \"T. Schrödter\" }] description = \"PedPy is a Python module for pedestrian movement analysis.\" readme = \"README.md\" license = { file = \"LICENSE\" } classifiers = [ \"Development Status :: 5 - Production/Stable\", \"License :: OSI Approved :: MIT License\", \"Operating System :: OS Independent\", \"Natural Language :: English\", \"Programming Language :: Python :: 3 :: Only\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Programming Language :: Python :: 3.13\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", ] dependencies = [ \"numpy~=2.1\", \"pandas~=2.2.3\", \"Shapely~=2.0.6\", \"scipy~=1.14\", \"matplotlib~=3.9\", \"h5py~=3.12\", ] requires-python = \">=3.11\" [project.urls] homepage = \"https://pedpy.readthedocs.io/\" [build-system] requires = [\"setuptools>=75\", \"wheel\", \"versioningit\"] build-backend = \"setuptools.build_meta\" [tool.versioningit.vcs] method = \"git\" default-tag = \"0.0.0\" [tool.versioningit.format] distance = \"{next_version}.dev{distance}\" dirty = \"{next_version}.dev{distance}\" distance-dirty = \"{next_version}.dev{distance}\" [tool.versioningit.write] file = \"pedpy/_version.py\" default-tag = \"0.0.0\" template = \"\"\" __version__ = \"{version}\" __commit_hash__ = \"{rev}\" \"\"\" [tool.setuptools] packages = [ \"pedpy\", \"pedpy.data\", \"pedpy.internal\", \"pedpy.io\", \"pedpy.methods\", \"pedpy.plotting\", ] [tool.ruff] exclude = [ \".bzr\", \".direnv\", \".eggs\", \".git\", \".git-rewrite\", \".hg\", \".ipynb_checkpoints\", \".mypy_cache\", \".nox\", \".pants.d\", \".pyenv\", \".pytest_cache\", \".pytype\", \".ruff_cache\", \".svn\", \".tox\", \".venv\", \".vscode\", \"__pypackages__\", \"_build\", \"buck-out\", \"build\", \"dist\", \"node_modules\", \"site-packages\", \"venv\", ] line-length = 80 src = [\"pedpy\"] extend-include = [\"*.ipynb\"] target-version = \"py310\" [tool.ruff.format] quote-style = \"double\" indent-style = \"space\" skip-magic-trailing-comma = false line-ending = \"auto\" docstring-code-format = true docstring-code-line-length = \"dynamic\" [tool.ruff.lint.isort] case-sensitive = true known-first-party = [\"pedpy\"] [tool.ruff.lint] select = [ # pyflakes \"F\", # pep-8-naming \"N\", # pycodestyle \"E\", \"W\", \"D\", # flake8-2020 \"YTT\", # flake8-bugbear \"B\", # flake8-quotes \"Q\", # flake8-debugger \"T10\", # flake8-gettext \"INT\", # pylint \"PL\", # flake8-pytest-style \"PT\", # misc lints \"PIE\", # flake8-pyi \"PYI\", # tidy imports \"TID\", # type-checking imports \"TCH\", # comprehensions \"C4\", # pygrep-hooks \"PGH\", # Ruff-specific rules \"RUF\", # flake8-bandit: exec-builtin \"S102\", # numpy \"NPY\", # Perflint \"PERF\", # flynt \"FLY\", # flake8-logging-format \"G\", # flake8-future-annotations \"FA\", # unconventional-import-alias \"ICN001\", # flake8-slots \"SLOT\", # flake8-raise \"RSE\", # pandas-vet \"PD\", \"RUF\" ] ignore = [] # Allow unused variables when underscore-prefixed. dummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\" [tool.ruff.lint.pydocstyle] convention = \"google\" [tool.ruff.lint.pylint] ## Maximum number of arguments for function / method max-args = 10 ## Maximum number of branch for function / method body. max-branches = 15 ## Maximum number of locals for function / method body. max-locals = 20 ## Maximum number of statements in function / method body. max-statements = 50 ## Constant types to ignore when used as \"magic values\" (see: PLR2004). allow-magic-value-types = [\"int\", \"str\"] [tool.mypy] python_version = \"3.10\" namespace_packages = true ignore_missing_imports = false check_untyped_defs = true disallow_any_generics = true disallow_incomplete_defs = true no_implicit_optional = true no_implicit_reexport = true strict_equality = true warn_redundant_casts = true warn_unused_ignores = false plugins = [\"numpy.typing.mypy_plugin\"] exclude = \"^(helper|docs|scripts|tests)(/|$)\" [[tool.mypy.overrides]] module = [ \"matplotlib.*\", \"mpl_toolkits.*\", \"pandas.*\", \"shapely.*\", \"numpy.*\", \"scipy.*\", \"setuptools.*\", \"h5py.*\", ] ignore_missing_imports = true\nnumpy~=2.1 pandas~=2.2.3 Shapely~=2.0.6 scipy~=1.14 matplotlib~=3.9 h5py~=3.12 # testing pytest~=8.3 pytest-mock~=3.14 pytest-cov~=5.0 # build tools setuptools~=75.1 click~=8.1 build~=1.1 # ci mypy==1.13 mypy-extensions==1.0 ruff==0.8 pre-commit==4.0\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pepc",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/SLPP/pepc/pepc",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/perihub",
            "repo_link": "https://github.com/PeriHub/PeriHub",
            "content": {
                "codemeta": "",
                "readme": "<!-- SPDX-FileCopyrightText: 2023 PeriHub <https://github.com/PeriHub/PeriHub> SPDX-License-Identifier: Apache-2.0 --> # PeriHub - Empowering Research with Peridynamic Modeling [![Pipeline Status](https://img.shields.io/github/actions/workflow/status/PeriHub/PeriHub/CI.yml?branch=main)](https://github.com/PeriHub/PeriLab.jl/actions) [![docs](https://img.shields.io/badge/docs-v1-blue.svg)](https://perihub.github.io/PeriHub/) [![License](https://img.shields.io/badge/License-Apache-blue.svg)](https://github.com/PeriHub/PeriHub/blob/main/LICENSE.md) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8159334.svg)](https://doi.org/10.5281/zenodo.8159334) [![Docker Image](https://img.shields.io/docker/pulls/perihub/frontend)](https://hub.docker.com/r/perihub/frontend) [![YouTube](https://img.shields.io/youtube/channel/subscribers/UCeky7HtUGlOJ2OKknvl6YnQ)](https://www.youtube.com/@PeriHub) PeriHub is a powerful software solution that can significantly benefit research in various fields. It is an extension of the open-source PeriLab software, providing a numerical implementation of the peridynamic theory. With PeriHub, researchers gain access to a valuable tool for addressing specific challenges and exploring diverse use cases in materials science, engineering, and related disciplines. ## Key Features - **Peridynamic Modeling:** PeriHub excels at facilitating peridynamic modeling, enabling researchers to analyze material behavior and complex systems. Its unique approach empowers users to explore new frontiers and deepen their understanding of material behavior. - **User-Friendly Interface:** PeriHub offers a user-friendly interface, making it accessible to both experienced researchers and newcomers in the field. The platform's ease of use ensures efficient simulations, analysis of results, and gaining valuable insights into material behavior. - **REST API and GUI Support:** Researchers can seamlessly interact with PeriHub using its REST API and GUI support, providing flexibility and convenience in conducting simulations and research tasks. - **High-Quality and Reliable:** Developed collaboratively by a dedicated group of experts, PeriHub adheres to high standards of quality, reliability, and FAIRness (Findability, Accessibility, Interoperability, and Reusability). The German Aerospace Center (DLR) has played a significant role in fostering an environment that encourages innovation and interdisciplinary collaboration throughout the software's development process. - **Portability and Scalability:** PeriHub utilizes Docker containers, ensuring seamless integration and deployment across various computing environments. This approach enhances the software's portability, scalability, and ease of use, making it even more practical for research purposes. ### Overview ![](docs/assets/images/PeriHub.svg) ### Generate model ![](docs/assets/gif/generateModel.gif) ### View generated mesh ![](docs/assets/gif/viewMesh.gif) ### Edit input deck ![](docs/assets/gif/editInputDeck.gif) ### Submit model ![](docs/assets/gif/runModel.gif) ### Analyse results ![](docs/assets/gif/analyseResults.gif) ### Plot results ![](docs/assets/gif/plotResults.gif) ### Analyse fracture ![](docs/assets/gif/analyseFracture.gif) # Getting Started with PeriHub Services To get started with PeriHub, you can use Docker Compose to easily set up the required services. Here's a step-by-step guide: - Clone the repository ``` git clone https://github.com/PeriHub/PeriHub.git ``` - Go into the PeriHub folder. ``` cd PeriHub ``` - Copy the .env file and edit its contents. ``` cp .env.example .env ``` - Run docker-compose. ``` docker-compose up ``` - If docker finished starting PeriHub, go to http://localhost:8080 ## Contact - [Jan-Timo Hesse](mailto:Jan-Timo.Hesse@dlr.de) ## License Please see the file [LICENSE.md](LICENSE.md) for further information about how the content is licensed.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/perilab",
            "repo_link": "https://github.com/PeriHub/PeriLab.jl",
            "content": {
                "codemeta": "",
                "readme": "<!-- SPDX-FileCopyrightText: 2023 Christian Willberg <christian.willberg@dlr.de>, Jan-Timo Hesse <jan-timo.hesse@dlr.de> SPDX-License-Identifier: BSD-3-Clause --> # `PeriLab` - Peridynamic Laboratory [docs-dev-img]: https://img.shields.io/badge/docs-dev-blue.svg [docs-dev-url]: https://perihub.github.io/PeriLab.jl/dev [docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg [docs-stable-url]: https://perihub.github.io/PeriLab.jl/stable [ci-img]: https://github.com/perihub/PeriLab.jl/actions/workflows/CI.yml/badge.svg [ci-url]: https://github.com/perihub/PeriLab.jl/actions/workflows/CI.yml [cov-img]: https://codecov.io/gh/perihub/PeriLab.jl/branch/main/graph/badge.svg [cov-url]: https://codecov.io/gh/perihub/PeriLab.jl [code-style-img]: https://img.shields.io/badge/code%20style-blue-4495d1.svg [code-style-url]: https://github.com/invenia/BlueStyle [aqua-img]: https://raw.githubusercontent.com/JuliaTesting/Aqua.jl/master/badge.svg [aqua-url]: https://github.com/JuliaTesting/Aqua.jl [sciml-img]: https://img.shields.io/static/v1?label=code%20style&message=SciML&color=9558b2&labelColor=389826 [sciml-url]: https://github.com/SciML/SciMLStyle [doi-img]: https://zenodo.org/badge/DOI/10.1016/j.softx.2024.101700.svg [doi-url]: https://doi.org/10.1016/j.softx.2024.101700 [release-img]: https://img.shields.io/github/v/release/PeriHub/PeriLab.jl [release-url]: https://github.com/PeriHub/PeriLab.jl/releases [docker-img]: https://img.shields.io/docker/pulls/perihub/perilab [docker-url]: https://hub.docker.com/r/perihub/perilab [license-img]: https://img.shields.io/badge/License-BSD-blue.svg [license-url]: https://github.com/PeriHub/PeriLab.jl/LICENSE [youtube-img]: https://img.shields.io/youtube/channel/subscribers/UCeky7HtUGlOJ2OKknvl6YnQ [youtube-url]: https://www.youtube.com/@PeriHub | **Documentation** | **Build Status** | **Quality** | |:----:|:----:|:----:| | [![][docs-stable-img]][docs-stable-url] [![][docs-dev-img]][docs-dev-url] | [![][ci-img]][ci-url] [![][cov-img]][cov-url] | [![][aqua-img]][aqua-url] [![][sciml-img]][sciml-url] | | **Deployment** | **License** | **Socials** | | [![][release-img]][release-url] [![][docker-img]][docker-url] | [![][license-img]][license-url] [![][doi-img]][doi-url] | [![][youtube-img]][youtube-url] | Welcome to `PeriLab`, a powerful software solution designed for tackling Peridynamic problems. <p align=\"center\" style=\"font-size:0;\"><!-- PeriLab_crack --><img align=\"middle\" src=\"https://raw.githubusercontent.com/PeriHub/PeriLab.jl/main/assets/PeriLab_crack.gif\" width=\"50%\"><!-- PeriLab_additive --><img align=\"middle\" src=\"https://raw.githubusercontent.com/PeriHub/PeriLab.jl/main/assets/PeriLab_additive.gif\" width=\"50%\"> </p> ## Documentation Explore the comprehensive [documentation](https://perihub.github.io/PeriLab.jl/) for `PeriLab`. ## Examples A few basic examples of `PeriLab` can be found in the [examples](https://github.com/PeriHub/PeriLab.jl/tree/main/examples) directory, or if you want to have a look at results go to our growing [PeriLab-Results service](https://perilab-results.nimbus-extern.dlr.de). ## Features ⭐ - 🚀 **Easy Installation**: PeriLab's straightforward installation process makes it accessible for researchers and engineers without extensive computational expertise. - ✒️ **Modularization**: The software is designed with a modular architecture that allows users to easily integrate their own material and damage models. - 🎨 **Formulations**: Bond-based, bond-associated, as well as oridnary and non-ordinary state-based peridynamic formulations can be used with PeriLab. - 🔩 **Material models**: PeriLab supports various material models, such as elastic, plastic, and more, enabling simulation of complex materials and structures. - 🔨 **Damage models**: Damage models such as critical stretch or an energy based criterium are included to simulate different types of damage, such as crack propagation or delamination, in their peridynamic simulations. - 👬 **FEM/PD coupling**: Coupling between Peridynamics and the Finite element method is supported. - 🔥 **Additive Manufacturing**: PeriLab supports additive manufacturing, allowing users to create custom additive models for their simulations. - 🔑 **Solver**: Different solvers like `Verlet` or `Static` can be utilized, swiching between those is also supported. - 🧲 **Multimodels**: PeriLab supports multimodels simulations, combining different types of peridynamics and damage models to create a comprehensive simulation environment. - 🔌 **Subroutine Interfaces**: Subroutines, such as UMAT, VUMAT and HETVAL are usable as material models - ⚡ **MPI**: PeriLab supports parallel computing using Message Passing Interface (MPI) technology to improve simulation performance on high-performance clusters. - 🔁 **Multistep simulations**: PeriLab supports the definition of multiple solver steps, allowing to combine different enviromental conditions in a single run. - 📐 **Surface Correction**: PeriLab provides tools for surface correction, such as the `Volume correction` method. - 💻 **HPC capabilities**: PeriLab is designed for high-performance computing (HPC) environments, allowing users to run large-scale simulations efficiently. - 📤📥 **Exodus Input/Output**: PeriLab uses the Exodus II data format for input and output, enabling easy transfer of data between simulation tools. - 🧮 **Abaqus Input**: PeriLab supports Abaqus input files, allowing users to create custom Abaqus models for their simulations. - ➗ **Bond filter**: The bond filter feature allows users to apply specific conditions to the bonds between particles in a simulation, influencing their behavior and interaction with other particles. - 🔧 **User specified Input/Output**: PeriLab provides flexible options for users to specify custom input and output files, enabling easy data manipulation and analysis. - 🧪 **Test Pipeline**: The PeriLab Source Code will be tested in a test pipeline to ensure its correctness and performance. ## Installation The `PeriLab` package is available through the Julia package system and can be installed using the following commands: ```julia using Pkg Pkg.add(\"PeriLab\") ``` Throughout the rest of this tutorial, we will assume that you have installed the PeriLab package and have already typed `using PeriLab` to bring all of the relevant variables into your current namespace. ## Getting Started with `PeriLab` Jumpstart your exploration of the PeriLab simulation core with provided examples. Run the following commands in Julia: ```julia PeriLab using PeriLab PeriLab.get_examples() PeriLab.main(\"examples/DCB/DCBmodel.yaml\") ``` >Note: More details about the main functionalities in the yaml input deck [here](https://github.com/PeriHub/PeriLab.jl/blob/main/src/Support/Parameters/parameter_handling.jl). ## Parallel Processing with `PeriLab` (MPI) To handle large-scale problems efficiently, install [MPI](https://juliaparallel.org/MPI.jl/stable/usage/). Run PeriLab with two processors on a **Linux** system using the following commands: ```sh $ julia julia> using MPI julia> MPI.install_mpiexecjl() ``` >Note: If you work with **Windows 10 or higher** you can use the [WSL](https://learn.microsoft.com/en-us/windows/wsl/install) environment. Run PeriLab with two processors: ```sh $ mpiexecjl -n 2 julia -e 'using PeriLab; PeriLab.main(\"examples/DCB/DCBmodel.yaml\")' ``` >Note: For HPC configurations please refer to [here](https://juliaparallel.org/MPI.jl/stable/configuration/#configure_jll_binarys). ## Installing with Docker 🐳 To install PeriLab using the official Perihub/Perilab Docker image, follow these steps: 1. **Install Docker**: Before you begin, ensure that you have Docker installed on your system. You can download and install Docker from the official website (https://www.docker.com/). Make sure your system meets the minimum requirements for running Docker. 2. **Pull the Perihub/Perilab Docker image**: Use the following command in a terminal or command prompt to pull the latest Perihub/Perilab Docker image from the Docker Hub repository: ```bash docker pull perihub/perilab ``` 3. **Run the Docker container**: Once the image has been downloaded, create a new directory for your PeriLab simulations and navigate to it in the terminal or command prompt. Run the following command to start the Docker container: ```bash docker run -it --rm -v <path_to_local_simulations_directory>:/app/simulations perihub/perilab bash ``` Replace `<path_to_local_simulations_directory>` with the absolute path to a local directory where you want to store your PeriLab simulations. This command will open a new terminal session inside the Docker container. Now, you've successfully installed PeriLab using the official Perihub/Perilab Docker image. You can start running your own peridynamic simulations within the container. ## Web Framework `PeriHub` 🖥️ PeriLab is also included as a ready to use application in the [PeriHub](https://github.com/PeriHub/PeriHub) web framework. ## `PeriLab` on `JuliaHub` Experience the convenience of using PeriLab as a ready-to-use application on JuliaHub. Simply create an [account](https://juliahub.com), navigate to the [applications page](https://juliahub.com/ui/Applications), and add the repository URL: https://github.com/PeriHub/PeriLab.jl. Configure advanced options, such as _filename_, _dryrun_, _verbosity_, _debug_, and _silence_. Click __Start__ and monitor the job progress. Results will be available in a zipped folder. Hit the __Start__ button and wait for the job to finish, the results will be available in a zipped folder. >Note: The free tier on `JuliaHub` offers 20 hours of computational time per month. ## What's Next? 🚀 Here are some exciting tasks on our roadmap: - 💧 **Degradation**: The simulation of degrative materials will be added to PeriLab - 👊 **Contact**: An upcoming feature in PeriLab is enhancing contact modeling to support advanced features like friction, adhesion, and contact forces based on temperature or other variables. - ➕ **More material and damage models**: PeriLab's future development plans include adding more sophisticated material models (e.g., viscoelastic-plastic) and damage models, expanding the software's applicability to a wider range of real-world phenomena. - ✂️ **Distribution logic**: As part of its ongoing development, PeriLab will continue to incorporate new distribution logic for improved performance and reduced computational resources. - 🏎️ **Optimizations**: As part of its ongoing development, PeriLab will continue to focus on optimizing the simulation process by incorporating new techniques like parallel optimization algorithms for improved efficiency and reduced computational resources. Feel free to contribute and help us make PeriLab even better! 🙌 ## Contributing We welcome contributions in various forms, including bug reports, documentation improvements, feature suggestions, and more. To get started, follow these steps and have a look at the [Contribution Guidelines](CONTRIBUTING.md): ### Development 1. **Clone the repository:** ```sh git clone https://github.com/PeriHub/PeriLab.jl cd PeriLab.jl ``` 2. **Activate the environment and install dependencies:** ```sh $ julia julia> ] pkg> activate . pkg> up ``` 3. **Run the script:** ```sh $ julia --project=. src/main.jl examples/DCB/DCBmodel.yaml ``` ## Questions For any questions or inquiries about PeriLab.jl, feel free to reach out to the authors via email. ## Authors and acknowledgment <p> <a href=\"https://orcid.org/0000-0003-2433-9183\"><img src=\"https://orcid.org/assets/vectors/orcid.logo.icon.svg\" style=\"height:15px;width:auto;vertical-align: top;background-color:transparent;\"> </a>[Prof. Dr.-Ing. Christian Willberg](mailto::christian.willberg@h2.de) </p> <p> <a href=\"https://orcid.org/0000-0002-3006-1520\"><img src=\"https://orcid.org/assets/vectors/orcid.logo.icon.svg\" style=\"height:15px;width:auto;vertical-align: top;background-color:transparent;\"> [M.Sc. Jan-Timo Hesse](mailto::jan-timo.hesse@dlr.de) </p> ## Project status `PeriLab` is currently in development. ## How to cite To cite PeriLab in your publications please use the following [paper](https://doi.org/10.1016/j.softx.2024.101700). ```s @Article{WillbergC2024, author={Willberg, Christian and Hesse, Jan-Timo and Pernatii, Anna}, title={{PeriLab - Peridynamic Laboratory}}, journal={SoftwareX}, year={2024}, publisher={Elsevier}, volume={26}, issn={2352-7110}, doi={10.1016/j.softx.2024.101700}, url={https://doi.org/10.1016/j.softx.2024.101700} } ``` ## Partner | <img src=\"https://raw.githubusercontent.com/PeriHub/PeriLab.jl/main/assets/dlr.jpg\" height=\"200\" title=\"German Aerospace Center\"> | :------------------------------------------------------------------------------------------------------------------------------:| | [German Aerospace Center](http://www.dlr.de/sy) | |<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Logo_h2.svg/1280px-Logo_h2.svg.png\" height=\"200\" title=\"Magdeburg-Stendal University of Applied Science\"> | [Magdeburg-Stendal University of Applied Science](http://www.h2.de) | <img src=\"https://www.cd.ovgu.de/cd_media/CD_OVGU/Downloads/Logo_jpg_png_svg_EPS_pdf/Logodownload/OVGU_Logo-download-1-p-1960.jpeg\" height=\"200\" title=\"Otto von Guericke University Magdeburg\"> | |[Otto von Guericke University Magdeburg](http://www.ovgu.de)| ## Acknowledgments <p align=\"center\" style=\"font-size:0;\"><!-- <!-- DFG --><img align=\"middle\" src=\"https://raw.githubusercontent.com/PeriHub/PeriLab.jl/main/assets/dfg.jpg\" height=\"120\"> </p> This project has benefited from funding by the [Deutsche Forschungsgemeinschaft](https://www.dfg.de/) (DFG, German Research Foundation) through the following grant ''Gekoppelte Peridynamik-Finite-Elemente-Simulationen zur Schädigungsanalyse von Faserverbundstrukturen''. <br/><br/>Grant number: [WI 4835/5-1](https://gepris.dfg.de/gepris/projekt/456427423) <p align=\"center\" style=\"font-size:0;\"><!-- SACHSEN --><img align=\"middle\" src=\"https://raw.githubusercontent.com/PeriHub/PeriLab.jl/main/assets/sachsen.jpg\" height=\"120\"> </p> [M-ERA.NET](https://www.m-era.net/) funded project ''Exploring Multi-Method Analysis of composite structures and joints under consideration of uncertainties engineering and processing (EMMA)'' This measure is co-financed with tax funds on the basis of the budget passed by the [Saxon state parlament](https://www.landtag.sachsen.de/de). <br/><br/>Grant number: [3028223](https://www.m-era.net/materipedia/2020/emma). <p align=\"center\" style=\"font-size:0;\"><!-- HyTank --><img align=\"middle\" src=\"https://raw.githubusercontent.com/PeriHub/PeriLab.jl/main/assets/hytank.jpg\" height=\"120\"><!-- --> </p> [Federal Ministry for Economic Affairs and Climate Action](https://www.bmwk.de/Navigation/DE/Home/home.html) funded project ''Virtuelle Kennwertermittlung, Schadensprädiktion und Simulationsmethoden für geklebte Fügestellen eines LH2-Tanks in Faserverbundbauweise für die kommerzielle Luftfahrt (HYTANK)''.<br/><br/> Grant number: 20W2214G.\n",
                "dependencies": "name = \"PeriLab\" uuid = \"2ecefcea-59f0-46dd-9cfd-1d2b8cc5f112\" authors = [\"Christian Willberg <christian.willberg@dlr.de>\", \"Jan-Timo Hesse <jan-timo.hesse@dlr.de>\"] version = \"1.3.8\" [deps] AbaqusReader = \"bc6b9049-e460-56d6-94b4-a597b2c0390d\" ArgParse = \"c7e460c6-2fb9-53a9-8c5b-16f535851c63\" CSV = \"336ed68f-0bac-5ca0-87d4-7b16caf5d00b\" Combinatorics = \"861a8166-3701-5b0c-9a16-15d98fcdc6aa\" DataFrames = \"a93c6f00-e57d-5684-b7b6-d8193f3e46c0\" DataStructures = \"864edb3b-99cc-5e75-8d2d-829cb0a9cfe8\" Dates = \"ade2ca70-3891-5945-98fb-dc099432e06a\" Dierckx = \"39dd38d3-220a-591b-8e3c-4c3a8c710a94\" Exodus = \"f57ae99e-f805-4780-bdca-96e224be1e5a\" FastGaussQuadrature = \"442a2c76-b920-505d-bb47-c5924d526838\" JSON3 = \"0f8b85d8-7281-11e9-16c2-39a750bddbf1\" LazyGrids = \"7031d0ef-c40d-4431-b2f8-61a8d2f650db\" LibGit2 = \"76f85450-5226-5b5a-8eaa-529ad045b433\" LinearAlgebra = \"37e2e46d-f89d-539d-b4ee-838fcccc9c8e\" Logging = \"56ddb016-857b-54e1-b83d-db4d58db5568\" LoggingExtras = \"e6f89c97-d47a-5376-807f-9c37f3926c36\" LoopVectorization = \"bdcacae8-1622-11e9-2a5c-532679323890\" MPI = \"da04e1cc-30fd-572f-bb4f-1f8673147195\" MPIPreferences = \"3da0fdf6-3ccc-4f1b-acd9-58baa6c99267\" Meshes = \"eacbb407-ea5a-433e-ab97-5258b1ca43fa\" NLsolve = \"2774e3e8-f4cf-5e23-947b-6d7e65073b56\" NearestNeighbors = \"b8a86587-4115-5ab1-83bc-aa920d37bbce\" OrderedCollections = \"bac558e1-5e72-5ebc-8fee-abe8a469f55d\" Pkg = \"44cfe95a-1eb2-52ea-b672-e2afdf69b78f\" PointNeighbors = \"1c4d5385-0a27-49de-8e2c-43b175c8985c\" PrettyTables = \"08abe8d2-0d0c-5749-adfa-8a2ac140af0d\" Printf = \"de0858da-6303-5e67-8744-51eddeeeb8d7\" ProgressBars = \"49802e3a-d2f1-5c88-81d8-b72133a6f568\" Random = \"9a3f8284-a2c9-5f02-9a11-845980a1fd5c\" Rotations = \"6038ab10-8711-5258-84ad-4b1120ba62dc\" StaticArrays = \"90137ffa-7385-5640-81b9-e52037218182\" Statistics = \"10745b16-79ce-11e8-11f9-7d13ad32a3b2\" StyledStrings = \"f489334b-da3d-4c2e-b8f0-e476e12c162b\" Tensors = \"48a634ad-e948-5137-8d70-aa71f2a747f4\" TimerOutputs = \"a759f4b9-e2f1-59dc-863e-4aeb61b1ea8f\" Unitful = \"1986cc42-f94f-5a68-af5c-568840ba703d\" YAML = \"ddb6d928-2868-570f-bddf-ab3f9cf99eb6\" ZipArchives = \"49080126-0e18-4c2a-b176-c102e4b3760c\" [compat] AbaqusReader = \"0.2.6\" Aqua = \"0.8\" ArgParse = \"1\" CSV = \"0.10\" Combinatorics = \"1\" DataFrames = \"1\" DataStructures = \"0.18\" Dates = \"1\" Dierckx = \"0.5\" Exodus = \"0.13\" FastGaussQuadrature = \"1\" JSON3 = \"1\" LazyGrids = \"1\" LibGit2 = \"1\" LinearAlgebra = \"1\" Logging = \"1\" LoggingExtras = \"1\" LoopVectorization = \"0.12\" MPI = \"0.20\" MPIPreferences = \"0.1\" Meshes = \"0.51\" NLsolve = \"4.5\" NearestNeighbors = \"0.4\" OrderedCollections = \"1\" Pkg = \"1\" PointNeighbors = \"0.4\" PrettyTables = \"2\" Printf = \"1\" ProgressBars = \"1\" Random = \"1\" Rotations = \"1\" StaticArrays = \"1\" Statistics = \"1\" StyledStrings = \"1\" Tensors = \"1\" Test = \"1\" TestSetExtensions = \"3\" TimerOutputs = \"0.5\" Unitful = \"1.22\" YAML = \"0.4\" ZipArchives = \"1, 2\" julia = \"1.6\" [extras] Aqua = \"4c88cf16-eb10-579e-8560-4a9242c79595\" JSON3 = \"0f8b85d8-7281-11e9-16c2-39a750bddbf1\" Logging = \"56ddb016-857b-54e1-b83d-db4d58db5568\" MPI = \"da04e1cc-30fd-572f-bb4f-1f8673147195\" Test = \"8dfed614-e22c-5e08-85e1-65c5234f0b40\" TestSetExtensions = \"98d24dd4-01ad-11ea-1b02-c9a08f80db04\" [targets] test = [\"Aqua\", \"JSON3\", \"Logging\", \"MPI\", \"Test\", \"TestSetExtensions\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/perun",
            "repo_link": "https://github.com/Helmholtz-AI-Energy/perun",
            "content": {
                "codemeta": "",
                "readme": "<div align=\"center\"> <img src=\"https://raw.githubusercontent.com/Helmholtz-AI-Energy/perun/main/docs/images/full_logo.svg\"> </div> &nbsp; &nbsp; [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu) [![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7253/badge)](https://bestpractices.coreinfrastructure.org/projects/7253) [![DOI](https://zenodo.org/badge/523363424.svg)](https://zenodo.org/badge/latestdoi/523363424) ![PyPI](https://img.shields.io/pypi/v/perun) ![PyPI - Downloads](https://img.shields.io/pypi/dm/perun) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![codecov](https://codecov.io/gh/Helmholtz-AI-Energy/perun/graph/badge.svg?token=9O6FSJ6I3G)](https://codecov.io/gh/Helmholtz-AI-Energy/perun) [![](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/) [![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause) [![Documentation Status](https://readthedocs.org/projects/perun/badge/?version=latest)](https://perun.readthedocs.io/en/latest/?badge=latest) perun is a Python package that calculates the energy consumption of Python scripts by sampling usage statistics from your Intel, Nvidia or AMD hardware components. It can handle MPI applications, gather data from hundreds of nodes, and accumulate it efficiently. perun can be used as a command-line tool or as a function decorator in Python scripts. Check out the [docs](https://perun.readthedocs.io/en/latest/) or a working [example](https://github.com/Helmholtz-AI-Energy/perun/blob/main/examples/torch_mnist/README.md)! ## Key Features - Measures energy consumption of Python scripts using Intel RAPL, ROCM-SMI, Nvidia-NVML, and psutil - Capable of handling MPI application, gathering data from hundreds of nodes efficiently - Monitor individual functions using decorators - Tracks energy usage of the application over multiple executions - Easy to benchmark applications and functions - Experimental!: Can monitor any non-distributed command line application ## Installation From PyPI: ```console pip install perun ``` > Extra dependencies like nvidia-smi, rocm-smi and mpi can be installed using pip as well: ```console pip install perun[nvidia, rocm, mpi] ``` From Github: ```console pip install git+https://github.com/Helmholtz-AI-Energy/perun ``` ## Quick Start ### Command Line To use perun as a command-line tool, run the monitor subcommand followed by the path to your Python script and its arguments: ```console $ perun monitor path/to/your/script.py [args] ``` perun will output two files, an HDF5 style containing all the raw data that was gathered, and a text file with a summary of the results. ```text PERUN REPORT App name: finetune_qa_accelerate First run: 2023-08-15T18:56:11.202060 Last run: 2023-08-17T13:29:29.969779 RUN ID: 2023-08-17T13:29:29.969779 | Round # | Host | RUNTIME | ENERGY | CPU_POWER | CPU_UTIL | GPU_POWER | GPU_MEM | DRAM_POWER | MEM_UTIL | |----------:|:--------------------|:----------|:-----------|:------------|:-----------|:------------|:-----------|:-------------|:-----------| | 0 | hkn0432.localdomain | 995.967 s | 960.506 kJ | 231.819 W | 3.240 % | 702.327 W | 55.258 GB | 29.315 W | 0.062 % | | 0 | hkn0436.localdomain | 994.847 s | 960.469 kJ | 235.162 W | 3.239 % | 701.588 W | 56.934 GB | 27.830 W | 0.061 % | | 0 | All | 995.967 s | 1.921 MJ | 466.981 W | 3.240 % | 1.404 kW | 112.192 GB | 57.145 W | 0.061 % | The application has been run 7 times. In total, it has used 3.128 kWh, released a total of 1.307 kgCO2e into the atmosphere, and you paid 1.02 € in electricity for it. ``` Perun will keep track of the energy of your application over multiple runs. #### Binary support (experimental) perun is capable of monitoring simple applications written in other languages, as long as they don't make use of MPI or are distributed over multiple computational nodes. ```console $ perun monitor --binary path/to/your/executable [args] ``` ### Function Monitoring Using a function decorator, information can be calculated about the runtime, power draw and component utilization while the function is executing. ```python import time from perun import monitor @monitor() def main(n: int): time.sleep(n) ``` After running the script with ```perun monitor```, the text report will add information about the monitored functions. ```text Monitored Functions | Round # | Function | Avg Calls / Rank | Avg Runtime | Avg Power | Avg CPU Util | Avg GPU Mem Util | |----------:|:----------------------------|-------------------:|:----------------|:-----------------|:---------------|:-------------------| | 0 | main | 1 | 993.323±0.587 s | 964.732±0.499 W | 3.244±0.003 % | 35.091±0.526 % | | 0 | prepare_train_features | 88 | 0.383±0.048 s | 262.305±19.251 W | 4.541±0.320 % | 3.937±0.013 % | | 0 | prepare_validation_features | 11 | 0.372±0.079 s | 272.161±19.404 W | 4.524±0.225 % | 4.490±0.907 % | ``` ### MPI Perun is compatible with MPI applications that make use of ```mpi4py```, and requires no changes in the code or in the perun configuration. Simply replace the ```python``` command with ```perun monitor```. ```console mpirun -n 8 perun monitor path/to/your/script.py ``` ## Docs To get more information, check out our [docs page](https://perun.readthedocs.io/en/latest/) or check the [examples](https://github.com/Helmholtz-AI-Energy/perun/tree/main/examples). ## Citing perun If you found perun usefull, please consider citing the conference paper: * Gutiérrez Hermosillo Muriedas, J.P., Flügel, K., Debus, C., Obermaier, H., Streit, A., Götz, M.: perun: Benchmarking Energy Consumption of High-Performance Computing Applications. In: Cano, J., Dikaiakos, M.D., Papadopoulos, G.A., Pericàs, M., and Sakellariou, R. (eds.) Euro-Par 2023: Parallel Processing. pp. 17-31. Springer Nature Switzerland, Cham (2023). https://doi.org/10.1007/978-3-031-39698-4_2. ```bibtex @InProceedings{10.1007/978-3-031-39698-4_2, author=\"Guti{\\'e}rrez Hermosillo Muriedas, Juan Pedro and Fl{\\\"u}gel, Katharina and Debus, Charlotte and Obermaier, Holger and Streit, Achim and G{\\\"o}tz, Markus\", editor=\"Cano, Jos{\\'e} and Dikaiakos, Marios D. and Papadopoulos, George A. and Peric{\\`a}s, Miquel and Sakellariou, Rizos\", title=\"perun: Benchmarking Energy Consumption of High-Performance Computing Applications\", booktitle=\"Euro-Par 2023: Parallel Processing\", year=\"2023\", publisher=\"Springer Nature Switzerland\", address=\"Cham\", pages=\"17--31\", abstract=\"Looking closely at the Top500 list of high-performance computers (HPC) in the world, it becomes clear that computing power is not the only number that has been growing in the last three decades. The amount of power required to operate such massive computing machines has been steadily increasing, earning HPC users a higher than usual carbon footprint. While the problem is well known in academia, the exact energy requirements of hardware, software and how to optimize it are hard to quantify. To tackle this issue, we need tools to understand the software and its relationship with power consumption in today's high performance computers. With that in mind, we present perun, a Python package and command line interface to measure energy consumption based on hardware performance counters and selected physical measurement sensors. This enables accurate energy measurements on various scales of computing, from a single laptop to an MPI-distributed HPC application. We include an analysis of the discrepancies between these sensor readings and hardware performance counters, with particular focus on the power draw of the usually overlooked non-compute components such as memory. One of our major insights is their significant share of the total energy consumption. We have equally analyzed the runtime and energy overhead perun generates when monitoring common HPC applications, and found it to be minimal. Finally, an analysis on the accuracy of different measuring methodologies when applied at large scales is presented.\", isbn=\"978-3-031-39698-4\" } ```\n",
                "dependencies": "[build-system] requires = [\"hatchling\"] build-backend = \"hatchling.build\" [project] name = \"perun\" version = \"0.8.10\" description = \"Measure the energy used by your MPI+Python applications.\" authors = [ {name = \"Gutiérrez Hermosillo Muriedas, Juan Pedro\", email=\"juan.muriedas@kit.edu\"} ] maintainers = [ {name = \"Gutiérrez Hermosillo Muriedas, Juan Pedro\", email=\"juan.muriedas@kit.edu\"} ] readme = \"README.md\" license = {file = \"LICENSE\"} keywords = [\"energy\", \"monitoring\", \"mpi\", \"python\", \"hpc\", \"command-line\", \"benchmarking\"] classifiers = [ \"Development Status :: 4 - Beta\", \"Environment :: Console\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: BSD License\", \"Programming Language :: Python :: 3\", \"Topic :: Scientific/Engineering\", \"Topic :: System :: Monitoring\", \"Topic :: System :: Benchmark\", \"Topic :: System :: Hardware\", \"Topic :: System :: Distributed Computing\", \"Typing :: Typed\", ] requires-python = \">=3.9\" dependencies = [ \"py-cpuinfo>=5.0.0\", \"numpy>=1.20.0\", \"psutil>=5.9.0\", \"h5py>=3.5.9\", \"pandas>=1.3\", \"tabulate>=0.9\", ] [project.optional-dependencies] nvidia = [\"nvidia-ml-py>=12.535.77\"] mpi = [\"mpi4py>=3.1\"] rocm = [\"amdsmi>=6.2.0\"] docs = [ \"sphinx\", \"sphinx-autoapi\", \"furo\", ] dev = [ # Testing \"pytest\", \"coverage\", \"hypothesis~=6.100\", # QA \"pre-commit\", # Releases and changelogs \"python-semantic-release>=8.1.1\", \"build\" ] [project.urls] Homepage = \"https://github.com/Helmholtz-AI-Energy/perun\" Documentation = \"https://perun.readthedocs.io\" Repository = \"https://github.com/Helmholtz-AI-Energy/perun\" Issues = \"https://github.com/Helmholtz-AI-Energy/perun/issues\" Changelog = \"https://github.com/Helmholtz-AI-Energy/perun/blob/main/CHANGELOG.md\" [project.scripts] perun = \"perun.api.cli:cli\" # Semantic release [tool.semantic_release] build_command = \"python -m build\" version_variables = [ \"perun/__init__.py:__version__\", \"docs/conf.py:release\" ] version_toml = [\"pyproject.toml:project.version\"] commit_message = \"chore: {version}\\n\\nAutomatically generated by python-semantic-release\" commit_parser = \"angular\" tag_format = \"v{version}\" [tool.semantic_release.branches.main] match = \"main\" prerelease_token = \"rc\" prerelease = false [tool.semantic_release.branches.release-test] match = \"release-test\" prerelease_token = \"rc\" prerelease = true [tool.semantic_release.changelog] template_dir = \"templates\" chagelog_file = \"CHANGELOG.md\" exclude_commit_patterns = [] [tool.semantic_release.commit_parser_options] allowed_tags = [\"docs\", \"feat\", \"fix\", \"perf\", \"refactor\"] minor_tags = [\"feat\"] patch_tags = [\"fix\", \"perf\"] default_bump_level = 0 [tool.semantic_release.commit_author] env = \"GIT_COMMIT_AUTHOR\" default = \"semantic-release <semantic-release>\" [tool.mypy] python_version=\"3.9\" packages=[\"perun\"] exclude=[ 'tests/', 'examples/' ] plugins = ['numpy.typing.mypy_plugin'] disallow_untyped_defs = true disallow_any_unimported = true no_implicit_optional = true check_untyped_defs=true warn_return_any=true show_error_codes =true warn_unused_ignores=true follow_imports = \"normal\" follow_untyped_imports = true [[tool.mypy.overrides]] module=[ \"perun.comm\", \"perun.backend.rocmsmi\", \"perun.backend.psutil\", \"perun.backend.powercap_rapl\", \"perun.backend.nvml\" ] warn_return_any= false # ignore_missing_imports = true # Ruff [tool.ruff] target-version = \"py39\" exclude = [\"docs\", \"examples\"] [tool.ruff.lint] select = [\"E\", \"F\", \"I\", \"D\", \"D417\"] ignore = [\"E501\"] [tool.ruff.lint.per-file-ignores] \"tests/*\" = [\"D\"] [tool.ruff.lint.pydocstyle] convention = \"numpy\" [tool.ruff.format] docstring-code-format = true # Testing [tool.pytest.ini_options] testpaths = [\"tests\"] [tool.coverage.run] source = [\"perun\"] command_line = \"-m pytest\" [tool.coverage.report] format = \"markdown\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/petrack",
            "repo_link": "https://jugit.fz-juelich.de/ped-dyn-emp/petrack",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/php-codemeta-crosswalk",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/php-codemeta-crosswalk",
            "content": {
                "codemeta": "{\"@context\":\"https://doi.org/10.5063/schema/codemeta-2.0\",\"@type\":\"SoftwareSourceCode\",\"datePublished\":\"2023-10-06\",\"dateCreated\":\"2023-07-31\",\"description\":\" Codemeta conversions \",\"name\":\"codemeta-crosswalk\",\"license\":\"https://spdx.org/licenses/Apache-2.0.html\",\"dateModified\":\"2024-05-06\",\"softwareVersion\":\"1.0-Beta\",\"developmentStatus\":\"active\",\"programmingLanguage\":\"PHP\",\"operatingSystem\":\"cross-platform\",\"runtimePlatform\":\"PHP Interpreter\",\"relatedLink\":\"https://1959e979-c58a-4d3c-86bb-09ec2dfcec8a.ka.bw-cloud-instance.org\",\"codeRepository\":\"https://github.com/Ramy-Badr-Ahmed/codemetaCrosswalk\",\"readme\":\"https://github.com/Ramy-Badr-Ahmed/codemetaCrosswalk/blob/main/README.md\",\"identifier\":\"https://archive.softwareheritage.org/swh:1:dir:b00c5e6af70451deb1960215c2a4b34e53586820;origin=https://github.com/Ramy-Badr-Ahmed/codemetaCrosswalk;visit=swh:1:snp:9e3fdc7155f39bf4fdd94c0af65efa2c31178b15;anchor=swh:1:rev:8224c09eefbf9c1df87de7c0edbeae18e849ca32\",\"author\":[{\"@type\":\"Person\",\"email\":\"126559907+Ramy-Badr-Ahmed@users.noreply.github.com\",\"familyName\":\"Ahmed\",\"givenName\":\"Ramy-Badr\"}],\"keywords\":[\"bibtex\",\"biblatex\",\"datacite\",\"codemeta-json\",\"codemeta-crosswalk\"],\"softwareRequirements\":[\"PHP >=8.0\",\"illuminate/validation>=10.43\",\"composer/spdx-licenses>=1.5\",\"illuminate/support>=10.39\"],\"maintainer\":{\"@type\":\"Person\",\"email\":\"126559907+Ramy-Badr-Ahmed@users.noreply.github.com\",\"familyName\":\"Ahmed\",\"givenName\":\"Ramy-Badr\"},\"applicationCategory\":[\"codemeta.json\",\"Metadata\",\"Conversions of Metadata\"],\"funder\":{\"@type\":\"Organization\",\"funding\":\"EU-Horizon: 101057264\",\"@id\":\"https://doi.org/10.3030/101057264\",\"name\":\"European Comission\"}}\n",
                "readme": "![GitHub top language](https://img.shields.io/github/languages/top/Ramy-Badr-Ahmed/codemetaCrosswalk) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/codemetaCrosswalk) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808884.svg)](https://doi.org/10.5281/zenodo.12808884) [![SWH](https://archive.softwareheritage.org/badge/swh:1:dir:8fe28098cf799244158d8204fe80c471fcf04a66/)](https://archive.softwareheritage.org/swh:1:dir:8fe28098cf799244158d8204fe80c471fcf04a66;origin=https://github.com/Ramy-Badr-Ahmed/codemetaCrosswalk;visit=swh:1:snp:7129240170329390ce30cc6819cde370f3595473;anchor=swh:1:rev:9a6b4f039e24bb96503eb8179f810ed6a4c8ed4c) # Codemeta.json Crosswalk This repository has been developed as part of the [FAIR4CoreEOSC project](https://faircore4eosc.eu/eosc-core-components/eosc-research-software-apis-and-connectors-rsac) to address two project's pillars (Describe and Cite). > [!Note] > A demonstrable version can be accessed here: <a href=\"https://1959e979-c58a-4d3c-86bb-09ec2dfcec8a.ka.bw-cloud-instance.org/\" target=\"_blank\">**Demo Version**</a> Sample snapshot of the codemeta generator and converter demo: ![snap.PNG](snap.PNG) It currently addresses metadata conversions for the following use cases: | From | To | |----------|---------------| | CodeMeta | DataCite [^1] | | CodeMeta | BibLatex [^2] | | CodeMeta | BibTex [^3] | The codemeta conversion pattern to the above schemes is extendable to other metadata schemes as template classes located under `Schemes` directory. The initial keys correspondence is defined in this repository [^4]. > [!Note] > There's a scheme template class that can help see this pattern. Please consult the crosswalk [^4] and scheme documentations to properly construct this class. ## Installation Steps: 1) Clone this project. 2) Open a console session and navigate to the cloned directory: Run \"composer install\" This should involve installing the PHP REPL, PsySH 3) (optional) Add psysh to PATH Example, Bash: echo 'export PATH=\"$PATH:/The_Cloned_Directory/vendor/bin\"' >> ~/.bashrc source ~/.bashrc 4) (Optional) Create your local branch. ## Usage: - In a console session inside the cloned directory, start the php REPL: ```php $ psysh // if not added to PATH replace with: vendor/bin/psysh Psy Shell v0.12.0 (PHP 8.2.0 -- cli) by Justin Hileman ``` - Define namespace: ```php > namespace Conversions; > use Conversions; ``` - Specify codemeta.json path: ```php > $codeMetaPath = 'CodeMeta/codeMeta.json' ``` > [!Note] > By default, codemeta.json is located under 'CodeMeta' directory where an example already exists. > `$codeMetaPath` can _also_ directly take codemeta.json as an array - Specify target scheme (as fully qualified class name) ```php > $dataCite = \\Schemes\\DataCite::class > $bibLatex = \\Schemes\\BibLatex::class > $bibTex = \\Schemes\\BibTex::class ``` > [!Note] > By default, scheme classes are located under 'Schemes' directory. - Get the conversion from the specified Codemeta.json: ```php > $errors = NULL; // initialise errors variable > $dataCiteFromCodeMeta = CodeMetaConversion::To($dataCite, $codeMetaPath, $errors) // array-formatted > $bibLatexFromCodeMeta = CodeMetaConversion::To($bibLatex, $codeMetaPath, $errors) // string-formatted > $bibTexFromCodeMeta = CodeMetaConversion::To($bibTex, $codeMetaPath, $errors) // string-formatted ``` - Retrieve errors (if occurred) from the `Illuminate\\Support\\MessageBag()` instance: ```php > $errors->messages() // gets error messages as specified in CodeMeta/conversionsValidations.php > $errors->keys() // gets codemeta keys where errors occurred > $errors->first() // gets the first occurred error message > $errors->has('identifier') // checks whether an error has occurred in the codemeta `identifier` key ``` > [!Note] > Validations use the `Illuminate\\Validation\\Validator` package. > Error messages and rules can be customised in `CodeMeta/conversionsValidations.php` as per the package syntax. ### References [^1]: [DataCite Metadata Schema](https://schema.datacite.org/meta/kernel-4.3/doc/DataCite-MetadataKernel_v4.3.pdf). [^2]: [BibLATEX style extension for Software](https://ctan.math.washington.edu/tex-archive/macros/latex/contrib/biblatex-contrib/biblatex-software/software-biblatex.pdf). [^3]: [BibTex](https://en.wikipedia.org/wiki/BibTeX). [^4]: [Codemeta Crosswalk](https://github.com/codemeta/codemeta/blob/master/crosswalk.csv).\n",
                "dependencies": "{\"name\":\"codemeta/crosswalk\",\"type\":\"project\",\"description\":\"Codemeta conversions to other metadata schemes/styles. Part of the FAIRCORE4EOSC project for LZI-Dagstuhl\",\"keywords\":[\"codemeta\",\"crosswalk\",\"bibtex\",\"datacite\",\"biblatex\"],\"license\":\"MIT\",\"require\":{\"php\":\"^8.2\",\"psy/psysh\":\"^0.12.0\",\"illuminate/support\":\"^10.39\",\"composer/spdx-licenses\":\"^1.5\",\"illuminate/validation\":\"^10.43\"},\"autoload\":{\"psr-4\":{\"Conversions\\\\\":\"Conversions\",\"Schemes\\\\\":\"Schemes\"}},\"minimum-stability\":\"dev\",\"prefer-stable\":true}\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/picongpu",
            "repo_link": "https://github.com/ComputationalRadiationPhysics/picongpu",
            "content": {
                "codemeta": "",
                "readme": "PIConGPU - Particle-in-Cell Simulations for the Exascale Era ============================================================ [![Code Status dev](https://gitlab.com/hzdr/crp/picongpu/badges/dev/pipeline.svg?key_text=dev)](https://gitlab.com/hzdr/crp/picongpu/pipelines/dev/latest) [![Documentation Status](https://readthedocs.org/projects/picongpu/badge/?version=latest)](http://picongpu.readthedocs.io) [![Doxygen](https://img.shields.io/badge/API-Doxygen-blue.svg)](http://computationalradiationphysics.github.io/picongpu) [![Language](https://img.shields.io/badge/language-C%2B%2B20-orange.svg)](https://isocpp.org/) [![License PIConGPU](https://img.shields.io/badge/license-GPLv3-blue.svg?label=PIConGPU)](https://www.gnu.org/licenses/gpl-3.0.html) [![License PMacc](https://img.shields.io/badge/license-LGPLv3-blue.svg?label=PMacc)](https://www.gnu.org/licenses/lgpl-3.0.html) [![PIConGPU Presentation Video](http://img.youtube.com/vi/nwZuG-XtUDE/0.jpg)](http://www.youtube.com/watch?v=nwZuG-XtUDE) [![PIConGPU Release](docs/logo/pic_logo_vert_158x360.png)](http://www.youtube.com/watch?v=nwZuG-XtUDE) Introduction ------------ PIConGPU is a fully relativistic, [manycore](https://en.wikipedia.org/wiki/Manycore_processor), 3D3V particle-in-cell ([PIC](http://en.wikipedia.org/wiki/Particle-in-cell)) code. The Particle-in-Cell algorithm is a central tool in plasma physics. It describes the dynamics of a plasma by computing the motion of electrons and ions in the plasma based on [Maxwell's equations](http://en.wikipedia.org/wiki/Maxwell%27s_equations). PIConGPU implements various numerical schemes to solve the PIC cycle. Its features for the electro-magnetic PIC algorithm include: - a central or Yee-lattice for fields - particle pushers that solve the equation of motion for charged and neutral particles, e.g., the *Boris-* and the [*Vay-Pusher*](http://dx.doi.org/10.1063/1.2837054) - Maxwell field solvers, e.g. [*Yee's*](http://dx.doi.org/10.1109/TAP.1966.1138693) and [*Lehe's*](http://dx.doi.org/10.1103/PhysRevSTAB.16.021301) scheme - rigorously charge conserving current deposition schemes, such as [*Esirkepov*](http://dx.doi.org/10.1016/S0010-4655%2800%2900228-9) and *EZ* (Esirkepov meets ZigZag) - macro-particle form factors ranging from NGP (0th order), CIC (1st), TSC (2nd), PQS (3rd) to PCS (4th) and the electro-magnetic PIC algorithm is further self-consistently coupled to: - classical radiation reaction ([DOI:10.1016/j.cpc.2016.04.002](http://dx.doi.org/10.1016/j.cpc.2016.04.002)) - advanced field ionization methods ([DOI:10.1103/PhysRevA.59.569](http://dx.doi.org/10.1103/PhysRevA.59.569), [LV Keldysh](http://www.jetp.ac.ru/cgi-bin/dn/e_020_05_1307.pdf), BSI) Besides the electro-magnetic PIC algorithm and extensions to it, we developed a wide range of tools and diagnostics, e.g.: - online, far-field radiation diagnostics for coherent and incoherent radiation emitted by charged particles - full restart and output capabilities via [openPMD](http://openPMD.org), including [parallel HDF5](http://hdfgroup.org/) - 2D and 3D live view and diagnostics tools - a large selection of extensible [online-plugins](http://picongpu.readthedocs.io/en/latest/usage/plugins.html) As one of our supported compute platforms, GPUs provide a computational performance of several [TFLOP/s](http://en.wikipedia.org/wiki/FLOPS) at considerable lower invest and maintenance costs compared to multi CPU-based compute architectures of similar performance. The latest high-performance systems ([TOP500](http://www.top500.org/)) are enhanced by accelerator hardware that boost their peak performance up to the multi-PFLOP/s level. With its outstanding performance and scalability to more than 18'000 GPUs, PIConGPU was one of the **finalists** of the 2013 [Gordon Bell Prize](http://sc13.supercomputing.org/content/acm-gordon-bell-prize). PIConGPU is developed and maintained by the [Computational Radiation Physics Group](https://www.hzdr.de/db/Cms?pNid=2097) at the [Institute for Radiation Physics](http://www.hzdr.de/db/Cms?pNid=132) at [HZDR](http://www.hzdr.de/) in close collaboration with the Center for Information Services and High Performance Computing ([ZIH](http://tu-dresden.de/die_tu_dresden/zentrale_einrichtungen/zih)) of the Technical University Dresden ([TUD](http://www.tu-dresden.de)). We are a member of the [Dresden GPU Center of Excellence](http://ccoe-dresden.de/) that cooperates on a broad range of scientific GPU and manycore applications, workshops and teaching efforts. Attribution ----------- PIConGPU is a *scientific project*. If you **present and/or publish** scientific results that used PIConGPU, you should set a **reference** to show your support. Our according **up-to-date publication** at **the time of your publication** should be inquired from: - [REFERENCE.md](https://raw.githubusercontent.com/ComputationalRadiationPhysics/picongpu/master/REFERENCE.md) Please also consider adding yourself to our [community map](https://github.com/ComputationalRadiationPhysics/picongpu-communitymap). We would love to hear from you! Oral Presentations ------------------ The following slide should be part of **oral presentations**. It is intended to acknowledge the team maintaining PIConGPU and to support our community: (*coming soon*) presentation_picongpu.pdf (svg version, key note version, png version: 1920x1080 and 1024x768) Software License ---------------- *PIConGPU* is licensed under the **GPLv3+**. Furthermore, you can develop your own particle-mesh algorithms based on our general library *PMacc* that is shipped alongside PIConGPU. *PMacc* is *dual licensed* under both the **GPLv3+ and LGPLv3+**. For a detailed description, please refer to [LICENSE.md](LICENSE.md) ******************************************************************************** Install ------- See our notes in [INSTALL.rst](INSTALL.rst). Users ----- Dear User, we hereby emphasize that we are still actively developing PIConGPU at great speed and do, from time to time, break backwards compatibility. When using this software, please stick to the latest release or use the `dev` branch containing the latest changes. It also contains a file `CHANGELOG.md` with the latest changes (and how to update your simulations). Read it first before updating between two versions! Also, we add a git `tag` according to a version number for each release. For any questions regarding the usage of PIConGPU please **do not** contact the developers and maintainers directly. Instead, please [open an issue on GitHub](https://github.com/ComputationalRadiationPhysics/picongpu/issues/new). Before you post a question, browse the PIConGPU [documentation](https://github.com/ComputationalRadiationPhysics/picongpu/search?l=markdown), [wiki](https://github.com/ComputationalRadiationPhysics/picongpu/wiki) and the [issue tracker](https://github.com/ComputationalRadiationPhysics/picongpu/issues) to see if your question has been answered, already. PIConGPU is a collaborative project. We thus encourage users to engage in answering questions of other users and post solutions to problems to the list. A problem you have encountered might be the future problem of another user. In addition, please consider using the collaborative features of GitHub if you have questions or comments on code or documentation. This will allow other users to see the piece of code or documentation you are referring to. Main ressources are in our [online manual](https://picongpu.readthedocs.io), the [user section](https://github.com/ComputationalRadiationPhysics/picongpu/wiki) of our wiki, documentation files in [`.md` (Markdown)](http://commonmark.org/help/) and [`.rst` (reStructuredText)](http://www.sphinx-doc.org/en/stable/rest.html) format in this repository and a [getting started video](http://www.youtube.com/watch?v=7ybsD8G4Rsk). Feel free to visit [picongpu.hzdr.de](http://picongpu.hzdr.de) to learn more about the PIC algorithm. Software Upgrades ----------------- PIConGPU ships new and frequent changes to the code in the development branch `dev`. From time to time we publish a new release of PIConGPU. Before you pull the changes in, please read our [ChangeLog](CHANGELOG.md)! You may have to update some of your simulation `.param` and `.cfg` files by hand since PIConGPU is an active project and new features often require changes in input files. Additionally, a full description of new features and fixed bugs in comparison to the previous release is provided in that file. In case you decide to use *new, potentially buggy and experimental* features from our `dev` branch, be aware that you must participate or at least follow the development yourself. Syntax changes and in-development bugs will *not* be announced outside of their according pull requests and issues. Before drafting a new release, we open a new `release-*` branch from `dev` with the `*` being the version number of the upcoming release. This branch only receives bug fixes (feature freeze) and users are welcome to try it out (however, the change log and a detailed announcement might still be missing in it). Developers ---------- ### How to participate See [CONTRIBUTING.md](CONTRIBUTING.md) If you like to jump in right away, see [![open \"good first issue\" issues](https://img.shields.io/github/issues-raw/ComputationalRadiationPhysics/picongpu/good%20first%20issue.svg?color=56cbef)](https://github.com/ComputationalRadiationPhysics/picongpu/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) Active Team ----------- ### Scientific Supervision - Dr. Michael Bussmann - Dr. Thomas Kluge - Dr. Richard Pausch - Dr. Klaus Steiniger ### Maintainers* and core developers - Finn-Ole Carstens - Dr. Alexander Debus - Dr. Marco Garten* - Dr. Axel Huebl* - Dr. Jeffrey Kelling* - Dr. Julian Lenz* - Brian Edward Marre - Tapish Narwal* - Pawel Ordyna - Dr. Richard Pausch* - Franz Poeschel - Dr. Klaus Steiniger* - Rene Widera* ### Former Members, Contributions and Thanks The PIConGPU team expresses its gratitude to: Dr. Sergei Bastrakov, Florian Berninger, Heiko Burau, Fabia Dietrich, Robert Dietrich, Carlchristian Eckert, Simeon Ehrig, Ph.D. Wen Fu, Alexander Grund, Sebastian Hahn, Anton Helm, Wolfgang Hoehnig, Dr.-Ing. Guido Juckeland, Jeffrey Kelling, Maximilian Knespel, Dr. Remi Lehe, Felix Schmitt, Frank Winkler, Benjamin Schneider, Joseph Schuchart, Conrad Schumann, Stefan Tietze, Ph.D. Marija Vranic, Benjamin Worpitz, Erik Zenker, Sophie Rudat, Sebastian Starke, Alexander Matthes, Kseniia Bastrakova, Bernhard Manfred Gruber, Jakob Trojok, Anton Lebedev, Nils Prinz, Felix Meyer, Lennert Sprenger, Manhui Wang, Maxence Thevenet, Ilja Goethel, Mika Soren Voß, Lei Bifeng, Andrei Berceanu, Felix Meyer, Lennert Sprenger and Nico Wrobel. Kudos to everyone, mentioned or unmentioned, who contributed further in any way! ******************************************************************************** ![image of an lwfa](docs/images/lwfa_iso.png \"LWFA\") ![image of our strong scaling](docs/images/StrongScalingPIConGPU_log.png \"Strong Scaling\")\n",
                "dependencies": "-r lib/python/picongpu/requirements.txt\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pigx",
            "repo_link": "https://github.com/BIMSBbioinfo/pigx",
            "content": {
                "codemeta": "",
                "readme": "[![install with Guix badge](https://img.shields.io/badge/install%20with-guix-F4BB15.svg)](https://gnu.org/s/guix) <div align=\"center\"> <img src=\"logo.svg\" alt=\"PiGx logo\" width=\"30%\" height=\"30%\" /> </div> # What is PiGx? PiGx is a collection of genomics pipelines. More information can be found in [PiGx website](https://bioinformatics.mdc-berlin.de/pigx/) It includes the following pipelines: - [PiGx BSseq](https://github.com/BIMSBbioinfo/pigx_bsseq) for raw fastq read data of bisulfite experiments - [PiGx RNAseq](https://github.com/BIMSBbioinfo/pigx_rnaseq) for RNAseq samples - [PiGx scRNAseq](https://github.com/BIMSBbioinfo/pigx_scrnaseq) for single cell dropseq analysis - [PiGx ChIPseq](https://github.com/BIMSBbioinfo/pigx_chipseq) for reads from ChIPseq experiments - [PiGx CRISPR](https://github.com/BIMSBbioinfo/pigx_crispr) *(work in progress)* for the analysis of sequence mutations in CRISPR-CAS9 targeted amplicon sequencing data All pipelines are easily configured with a sample sheet (in CSV format) and a descriptive settings file (in YAML format). For more detailed information see the README.md file for each of the pipelines in the `pipelines` directory. ## Publication **Wurmus R, Uyar B, Osberg B, Franke V, Gosdschan A, Wreczycka K, Ronen J, Akalin A**. [PiGx: Reproducible genomics analysis pipelines with GNU Guix.](https://www.ncbi.nlm.nih.gov/pubmed/30277498) **Gigascience**. 2018 Oct 2. doi: 10.1093/gigascience/giy123. PubMed PMID: 30277498. # Getting started To run PiGx on your experimental data, describe your samples in a CSV file `sample_sheet.csv`, provide a `settings.yaml` to override the defaults defaults, and select the pipeline. To generate a settings file template for any pipeline: ```sh pigx [pipeline] --init=settings ``` To generate a sample sheet template for any pipeline: ```sh pigx [pipeline] --init=sample-sheet ``` Here's a simple example to run the RNAseq pipeline: ```sh pigx rnaseq my-sample-sheet.csv --settings my-settings.yaml ``` To see all available options run `pigx --help`. # Install Pre-built binaries for PiGx are available through GNU Guix, the functional package manager for reproducible, user-controlled software management. Install the complete pipeline bundle with the following command: ```sh guix install pigx ``` If you want to install PiGx from source, please make sure that all required dependencies are installed and then follow the common GNU build system steps after unpacking the [latest release tarball](https://github.com/BIMSBbioinfo/pigx/releases/latest): ```sh ./configure --prefix=/some/where make install ``` You can enable or disable each of the pipelines with the `--enable-PIPELINE` and `--disable-PIPELINE` arguments to the `configure` script. `PIPELINE` is one of `bsseq`, `rnaseq`, `scrnaseq`, `chipseq`, and `crispr`. For more options run `./configure --help`. # License All PiGx pipelines are free software: you can redistribute PiGx and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. See `LICENSE` for the full license text.\n",
                "dependencies": "#!/bin/sh autoreconf -vif\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pkgndep",
            "repo_link": "https://github.com/jokergoo/pkgndep",
            "content": {
                "codemeta": "",
                "readme": "# Analyzing Dependency Heaviness of R Packages [![R-CMD-check](https://github.com/jokergoo/pkgndep/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/pkgndep/actions) [![CRAN](https://www.r-pkg.org/badges/version/pkgndep)](https://cran.r-project.org/web/packages/pkgndep/index.html) When developing R packages, we should try to avoid directly setting dependencies on \"heavy packages\". The \"heaviness\" for a package means, the number of additional dependency packages it brings to. If your package directly depends on a heavy package, it would bring several consequences: 1. Users need to install a lot of additional packages when installing your package which brings the risk that installation of some packages may fail and it makes your package cannot be installed. 2. The namespaces that are loaded into your R session after loading your package will be huge (you can see the loaded namespaces by `sessionInfo()`). 3. You package will be \"heavy\" as well and it may take long time to load your package. In the DESCRIPTION file of your package, there are \"direct dependency pakcages\" listed in the `Depends`, `Imports` and `LinkingTo` fields. There are also \"indirect dependency packages\" that can be found recursively for each of the direct dependency packages. Here what we called \"dependency packages\" are the union of the direct and indirect dependency packages. There are also packages listed in `Suggests` and `Enhances` fields in DESCRIPTION file, but they are not enforced to be installed when installing your package. Of course, they also have \"indirect dependency packages\". To get rid of the heavy packages that are not often used in your package, it is better to move them into the `Suggests`/`Enhances` fields and to load/install them only when they are needed. Here the **pkgndep** package checks the heaviness of the dependency packages of your package. For each package listed in the `Depends`, `Imports`, `LinkingTo` and `Suggests`/`Enhances` fields in the DESCRIPTION file, **pkgndep** checks how many additional packages your package requires. The summary of the dependency is visualized by a customized heatmap. As an example, I am developing a package called [**cola**](https://github.com/jokergoo/cola/) which depends on [a lot of other packages](https://github.com/jokergoo/ComplexHeatmap/blob/master/DESCRIPTION). The dependency heatmap looks like follows: ![](https://user-images.githubusercontent.com/449218/140655626-f2062b6e-c11f-4dc0-b6b9-d954feffc4ad.png) In the heatmap, rows are the packages listed in `Depends`, `Imports` and `Suggests` fields, columns are the additional dependency packages required for each row package. The barplots on the right show the number of required package, the number of imported functions/methods/classes (parsed from NAMESPACE file) and the quantitative measure \"heaviness\" (the definition of heaviness will be introduced later). We can see if all the packages are put in the `Depends` or `Imports` field (i.e. movig all suggsted packages to `Imports`), in total 248 packages are required, which are really a lot. Actually some of the heavy packages such as **WGCNA**, **clusterProfiler** and **ReactomePA** (the last three packages in the heatmap rows) are not very frequently used in **cola**, moving them to `Suggests` field and using them only when they are needed greatly helps to reduce the heaviness of **cola**. Now the number of required packages are reduced to only 64. ## Citation Gu Z. et al., pkgndep: a tool for analyzing dependency heaviness of R packages. Bioinformatics 2022. https://doi.org/10.1093/bioinformatics/btac449 Gu Z, On the Dependency Heaviness of CRAN/Bioconductor Ecosystem. Journal of Systems and Software 2023. https://doi.org/10.1016/j.jss.2023.111610 ## Installation The **pkgndep** package can be installed from CRAN by ```r install.packages(\"pkgndep\") ``` ## Usage To use this package: ```r library(pkgndep) pkg = pkgndep(\"package-name\") dependency_heatmap(pkg) ``` or ```r pkg = pkgndep(\"path-of-the-package\") dependency_heatmap(pkg) ``` An executable example: ```r library(pkgndep) pkg = pkgndep(\"ComplexHeatmap\") pkg ``` ``` ## ComplexHeatmap, version 2.9.4 ## 30 additional packages are required for installing 'ComplexHeatmap' ## 117 additional packages are required if installing packages listed in all fields in DESCRIPTION ``` ```r dependency_heatmap(pkg) ``` ![](https://user-images.githubusercontent.com/449218/140655659-2ca142c5-067f-4f76-a0d2-00d0aea49c96.png) ## Heaviness database There is an integrated dependency heaviness database for all R packages for a lot of R/Bioc versions. The database can be accessed by: ```r heaviness_database() ``` ## License MIT @ Zuguang Gu\n",
                "dependencies": "Package: pkgndep Type: Package Title: Analyze Dependency Heaviness of R Packages Version: 2.0.0 Date: 2023-01-11 Authors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"), comment = c('ORCID'=\"0000-0002-7395-8709\")) Depends: R (>= 4.0.0) Imports: ComplexHeatmap (>= 2.6.0), GetoptLong, GlobalOptions, utils, grid, hash, methods, BiocManager, brew, BiocVersion Suggests: knitr, rmarkdown, svglite, callr, rjson, Rook, igraph, ggplot2, ggrepel, base64, testthat, cowplot Description: A new metric named 'dependency heaviness' is proposed that measures the number of additional dependency packages that a parent package brings to its child package and are unique to the dependency packages imported by all other parents. The dependency heaviness analysis is visualized by a customized heatmap. The package is described in <doi:10.1093/bioinformatics/btac449>. We have also performed the dependency heaviness analysis on the CRAN/Bioconductor package ecosystem and the results are implemented as a web-based database which provides comprehensive tools for querying dependencies of individual R packages. The systematic analysis on the CRAN/Bioconductor ecosystem is described in <doi:10.1016/j.jss.2023.111610>. From 'pkgndep' version 2.0.0, the heaviness database includes snapshots of the CRAN/Bioconductor ecosystems for many old R versions. URL: https://github.com/jokergoo/pkgndep, https://jokergoo.github.io/pkgndep/ VignetteBuilder: knitr License: MIT + file LICENSE\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pointneighborsjl",
            "repo_link": "https://github.com/trixi-framework/PointNeighbors.jl",
            "content": {
                "codemeta": "",
                "readme": "# PointNeighbors.jl [![Docs-stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://trixi-framework.github.io/PointNeighbors.jl/stable) [![Docs-dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://trixi-framework.github.io/PointNeighbors.jl/dev) [![Slack](https://img.shields.io/badge/chat-slack-e01e5a)](https://join.slack.com/t/trixi-framework/shared_invite/zt-sgkc6ppw-6OXJqZAD5SPjBYqLd8MU~g) [![Youtube](https://img.shields.io/youtube/channel/views/UCpd92vU2HjjTPup-AIN0pkg?style=social)](https://www.youtube.com/@trixi-framework) [![Build Status](https://github.com/trixi-framework/PointNeighbors.jl/workflows/CI/badge.svg)](https://github.com/trixi-framework/PointNeighbors.jl/actions?query=workflow%3ACI) [![Codecov](https://codecov.io/gh/trixi-framework/PointNeighbors.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/trixi-framework/PointNeighbors.jl) [![SciML Code Style](https://img.shields.io/static/v1?label=code%20style&message=SciML&color=9558b2&labelColor=389826)](https://github.com/SciML/SciMLStyle) [![License: MIT](https://img.shields.io/badge/License-MIT-success.svg)](https://opensource.org/license/mit/) [![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.12702157.svg)](https://zenodo.org/doi/10.5281/zenodo.12702157) **PointNeighbors.jl** is a package for neighborhood search with fixed search radius in 1D, 2D and 3D point clouds. ## Features - Several implementations of neighborhood search with fixed search radius - Focus on fast incremental updates to be usable for particle-based simulations with frequent updates - Designed as a \"playground\" to easily switch between different implementations and data structures - Common API over all implementations - Extensive benchmark suite to study different implementations (work in progress) - GPU compatibility (work in progress) | Implementation | Description | Features | Query | Update | GPU-compatible | | ------------- | ------------- | --- | :--: | :--: | :--: | | `GridNeighborhoodSearch` with `DictionaryCellList` | Grid-based NHS with Julia `Dict` backend | Infinite domain | Fast | Fast | ❌ | | `GridNeighborhoodSearch` with `FullGridCellList` | Grid-based NHS allocating all cells of the domain | Finite domain, but efficient memory layout for densely filled domain. | Faster | Fastest | ✅ | | `PrecomputedNeighborhoodSearch` | Precompute neighbor lists | Best for [TLSPH](https://trixi-framework.github.io/TrixiParticles.jl/stable/systems/total_lagrangian_sph/) without NHS updates. Not suitable for updates in every time step. | Fastest | Very slow | ❌ | ## Benchmarks The following benchmarks were conducted on an AMD Ryzen Threadripper 3990X using 128 threads. Benchmark of a single force computation step of a Weakly Compressible SPH (WCSPH) simulation: ![wcsph](https://github.com/trixi-framework/PointNeighbors.jl/assets/44124897/ad5c378b-9ce2-4e6f-91dc-1e0da379b91f) Benchmark of an incremental update similar to a WCSPH simulation (note the log scale): ![update](https://github.com/trixi-framework/PointNeighbors.jl/assets/44124897/71eac5c9-6aa5-4267-bc0b-4057c89f8b12) Benchmark of a full right-hand side evaluation of a WCSPH simulation (note the log scale): ![rhs](https://github.com/trixi-framework/PointNeighbors.jl/assets/44124897/ac328a96-1b9f-4319-a785-dce9d862fd70) ## Packages using PointNeighbors.jl - [TrixiParticles.jl](https://github.com/trixi-framework/TrixiParticles.jl) - [Peridynamics.jl](https://github.com/kaipartmann/Peridynamics.jl) - [PeriLab.jl](https://github.com/PeriHub/PeriLab.jl) If you're using PointNeighbors.jl in your package, please feel free to open a PR adding it to this list. ## Cite Us If you use PointNeighbors.jl in your own research or write a paper using results obtained with the help of PointNeighbors.jl, please cite it as ```bibtex @misc{pointneighbors, title={{P}oint{N}eighbors.jl: {N}eighborhood search with fixed search radius in {J}ulia}, author={Erik Faulhaber and Niklas Neher and Sven Berger and Michael Schlottke-Lakemper and Gregor Gassner}, year={2024}, howpublished={\\url{https://github.com/trixi-framework/PointNeighbors.jl}}, doi={10.5281/zenodo.12702157} } ```\n",
                "dependencies": "name = \"PointNeighbors\" uuid = \"1c4d5385-0a27-49de-8e2c-43b175c8985c\" authors = [\"Erik Faulhaber <erik.faulhaber@uni-koeln.de>\"] version = \"0.4.12-pre\" [deps] Adapt = \"79e6a3ab-5dfb-504d-930d-738a2a938a0e\" Atomix = \"a9b6321e-bd34-4604-b9c9-b65b8de01458\" GPUArraysCore = \"46192b85-c4d5-4398-a991-12ede77f4527\" KernelAbstractions = \"63c18a36-062a-441e-b654-da1e3ab1ce7c\" LinearAlgebra = \"37e2e46d-f89d-539d-b4ee-838fcccc9c8e\" Polyester = \"f517fe37-dbe3-4b94-8317-1923a5111588\" Reexport = \"189a3867-3050-52da-a836-e630ba90ab69\" StaticArrays = \"90137ffa-7385-5640-81b9-e52037218182\" [compat] Adapt = \"4\" Atomix = \"1\" GPUArraysCore = \"0.2\" KernelAbstractions = \"0.9\" LinearAlgebra = \"1\" Polyester = \"0.7.5\" Reexport = \"1\" StaticArrays = \"1\" julia = \"1.10\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/postwrf",
            "repo_link": "https://github.com/anikfal/PostWRF",
            "content": {
                "codemeta": "",
                "readme": "# PostWRF [![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.8191714.svg)](https://zenodo.org/record/8191714) ### Visualization and postprocessing of the WRF and ERA5 data **Plot the WRF and ERA5 data, in the same simple way as you run the WRF model!** PostWRF is a bunch of interactive tools, written in NCL and Bash scripts, to visualize and post-process the WRF model outputs (as well as ERA5 and RTTOV data, to some extent). PostWRF is useful for both the expert and less-experienced users. Students can plot the WRF and ERA5 outputs whithout struggling with coding and syntax errors. Expert users can also carry out common postprocessing tasks in a simple and straightforward way. ## Main capabilities: - WRF Data extraction - WRF horizontal contour plot - WRF cross-section plot - WRF statistical diagrams - RTTOV input (from WRF) and output data generation - WRF data conversion to Geotiff - WRF Skew-T and windrose diagrams - ERA5 horizontal contour plot - ERA5 data extraction ## Sample visualizations and postprocessing ![github_postwrf](https://github.com/anikfal/PostWRF/assets/11738727/16be89c3-1bb1-4245-a430-1d07876563dd) ## Installation: Install NCL on a Linux machine (e.g. Fedora): ```bash sudo dnf install ncl ``` That's it! Enough for most of the PostWRF's capabilities! ## Run PostWRF: 1. ``` git clone git@github.com:anikfal/PostWRF.git ``` 2. ``` cd PostWRF ``` 3. ``` chmod +x postwrf.sh modules/*.sh modules_era/*.sh ``` 4. Copy or link your WRF or ERA5 files in the PostWRF directory 5. ``` ./postwrf.sh ``` 6. Follow the instructions and give relevant information to visualize/postprocess your data ## HTML Documentations: Documentations with practical examples: https://postwrf.readthedocs.io/en/master ## YouTube Training Videos: https://youtube.com/playlist?list=PL93HaRiv5QkCOWQ4E_Oeszi9DBOYrdNXD ## Paper: For more detailed information about the backend structure of the software, please read https://doi.org/10.1016/j.envsoft.2022.105591 If you find PostWRF helpful, please kindly cite it in your works.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/potsdam-open-source-radio-interferometry-tool",
            "repo_link": "https://git.gfz-potsdam.de/vlbi-data-analysis/port",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/profasi",
            "repo_link": "https://gitlab.jsc.fz-juelich.de/slbio/profasi",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/propulate",
            "repo_link": "https://github.com/Helmholtz-AI-Energy/propulate",
            "content": {
                "codemeta": "",
                "readme": "![Propulate Logo](https://raw.githubusercontent.com/Helmholtz-AI-Energy/propulate/refs/heads/main/LOGO_light.svg#gh-light-mode-only) ![Propulate Logo](https://raw.githubusercontent.com/Helmholtz-AI-Energy/propulate/refs/heads/main/LOGO_dark.svg#gh-dark-mode-only) # Parallel Propagator of Populations [![DOI](https://zenodo.org/badge/495731357.svg)](https://zenodo.org/badge/latestdoi/495731357) [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu) [![License: BSD-3](https://img.shields.io/badge/License-BSD--3-blue)](https://opensource.org/licenses/BSD-3-Clause) ![PyPI](https://img.shields.io/pypi/v/propulate) ![PyPI - Downloads](https://img.shields.io/pypi/dm/propulate) [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)[![](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/) [![OpenSSF Best Practices](https://www.bestpractices.dev/projects/7785/badge)](https://www.bestpractices.dev/projects/7785) [![](https://img.shields.io/badge/Contact-propulate%40lists.kit.edu-orange)](mailto:propulate@lists.kit.edu) [![Documentation Status](https://readthedocs.org/projects/propulate/badge/?version=latest)](https://propulate.readthedocs.io/en/latest/?badge=latest) [![codecov](https://codecov.io/gh/Helmholtz-AI-Energy/propulate/graph/badge.svg?token=ZG6PEXJOIO)](https://codecov.io/gh/Helmholtz-AI-Energy/propulate)[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/Helmholtz-AI-Energy/propulate/main.svg)](https://results.pre-commit.ci/latest/github/Helmholtz-AI-Energy/propulate/main) # **Click [here](https://www.scc.kit.edu/en/aboutus/16956.php) to watch our 3 min introduction video!** ## What `Propulate` can do for you `Propulate` is an HPC-tailored software for solving optimization problems in parallel. It is openly accessible and easy to use. Compared to a widely used competitor, `Propulate` is consistently faster - at least an order of magnitude for a set of typical benchmarks - and in some cases even more accurate. Inspired by biology, `Propulate` borrows mechanisms from biological evolution, such as selection, recombination, and mutation. Evolution begins with a population of solution candidates, each with randomly initialized genes. It is an iterative \"survival of the fittest\" process where the population at each iteration can be viewed as a generation. For each generation, the fitness of each candidate in the population is evaluated. The genes of the fittest candidates are incorporated in the next generation. Like in nature, `Propulate` does not wait for all compute units to finish the evaluation of the current generation. Instead, the compute units communicate the currently available information and use that to breed the next candidate immediately. This avoids waiting idly for other units and thus a load imbalance. Each unit is responsible for evaluating a single candidate. The result is a fitness level corresponding with that candidate's genes, allowing us to compare and rank all candidates. This information is sent to other compute units as soon as it becomes available. When a unit is finished evaluating a candidate and communicating the resulting fitness, it breeds the candidate for the next generation using the fitness values of all candidates it evaluated and received from other units so far. `Propulate` can be used for hyperparameter optimization and neural architecture search at scale. It was already successfully applied in several accepted scientific publications. Applications include grid load forecasting, remote sensing, and structural molecular biology: > J. Debus, C. Debus, G. Dissertori, et al. **PETNet-Coincident Particle Event Detection using Spiking Neural Networks**. > 2024 Neuro Inspired Computational Elements Conference (NICE), La Jolla, CA, USA, pp. 1-9 ( 2024). > https://doi.org/10.1109/NICE61972.2024.10549584 > D. Coquelin, K. Flügel, M. Weiel, et al. **AB-Training: A Communication-Efficient Approach for Distributed Low-Rank > Learning**. arXiv preprint (2024). https://doi.org/10.48550/arXiv.2405.01067 > D. Coquelin, K. Flügel, M. Weiel, et al. **Harnessing Orthogonality to Train Low-Rank Neural Networks**. arXiv > preprint (2024). https://doi.org/10.48550/arXiv.2401.08505 > A. Weyrauch, T. Steens, O. Taubert, et al. **ReCycle: Fast and Efficient Long Time Series Forecasting with Residual > Cyclic Transformers**. 2024 IEEE Conference on Artificial Intelligence (CAI), Singapore, pp. 1187-1194 (2024). > https://doi.org/10.1109/CAI59869.2024.00212 > O. Taubert, F. von der Lehr, A. Bazarova, et al. **RNA contact prediction by data efficient deep learning**. > Communications Biology 6(1), 913 (2023). https://doi.org/10.1038/s42003-023-05244-9 > D. Coquelin, K. Flügel, M. Weiel, et al. **Harnessing Orthogonality to Train Low-Rank Neural Networks**. arXiv > preprint (2023). https://doi.org/10.48550/arXiv.2401.08505 > Y. Funk, M. Götz, and H. Anzt. **Prediction of optimal solvers for sparse linear systems using deep learning**. > Proceedings of the 2022 SIAM Conference on Parallel Processing for Scientific Computing (pp. 14-24). Society for > Industrial and Applied Mathematics (2022). https://doi.org/10.1137/1.9781611977141.2 > D. Coquelin, R. Sedona, M. Riedel, and M. Götz. **Evolutionary Optimization of Neural Architectures in Remote Sensing > Classification Problems**. IEEE International Geoscience and Remote Sensing Symposium IGARSS, Brussels, Belgium, > pp. 1587-1590 (2021). https://doi.org/10.1109/IGARSS47720.2021.9554309 ## In more technical terms ``Propulate`` is a massively parallel evolutionary hyperparameter optimizer based on the island model with asynchronous propagation of populations and asynchronous migration. In contrast to classical GAs, ``Propulate`` maintains a continuous population of already evaluated individuals with a softened notion of the typically strictly separated, discrete generations. Our contributions include: - A novel parallel genetic algorithm based on a fully asynchronized island model with independently processing workers. - Massive parallelism by asynchronous propagation of continuous populations and migration via efficient communication using the message passing interface. - Optimized use efficiency of parallel hardware by minimizing idle times in distributed computing environments. To be more efficient, the generations are less well separated than they usually are in evolutionary algorithms. New individuals are generated from a pool of currently active, already evaluated individuals that may be from any generation. Individuals may be removed from the breeding population based on different criteria. You can find the corresponding publication [here](https://doi.org/10.1007/978-3-031-32041-5_6): > Taubert, O. *et al.* (2023). Massively Parallel Genetic Optimization Through Asynchronous Propagation of Populations. > In: Bhatele, A., Hammond, J., Baboulin, M., Kruse, C. (eds) High Performance Computing. ISC High Performance 2023. > Lecture Notes in Computer Science, vol 13948. Springer, Cham. > [doi.org/10.1007/978-3-031-32041-5_6](https://doi.org/10.1007/978-3-031-32041-5_6) ## Documentation Check out the full documentation at [https://propulate.readthedocs.io/](https://propulate.readthedocs.io/) :rocket:! Here you can find installation instructions, tutorials, theoretical background, and API references. **:point_right: If you have any questions or run into any challenges while using `Propulate`, don't hesitate to post an [issue](https://github.com/Helmholtz-AI-Energy/propulate/issues) :bookmark:, reach out via [GitHub discussions](https://github.com/Helmholtz-AI-Energy/propulate/discussions) :octocat:, or contact us directly via e-mail :email: to [propulate@lists.kit.edu](mailto:propulate@lists.kit.edu).** ## Installation - You can install the **latest stable release** from PyPI: ``pip install propulate`` - If you need the **latest updates**, you can also install ``Propulate`` directly from the master branch. Pull and run ``pip install .``. - If you want to run the **tutorials**, you can install the required dependencies via: ``pip install .\"[tutorials]\"`` - If you want to **contribute** to ``Propulate`` as a developer, you need to install the required dependencies with the package: ``pip install -e .\"[dev]\"``. ``Propulate`` depends on [``mpi4py``](https://mpi4py.readthedocs.io/en/stable/) and requires an MPI implementation under the hood. Currently, it is only tested with [OpenMPI](https://www.open-mpi.org/). ## Quickstart *Below, you can find a quick recipe for how to use `Propulate` in general. Check out the official [ReadTheDocs](https://propulate.readthedocs.io/en/latest/tut_propulator.html) documentation for more detailed tutorials and explanations.* Let's minimize the sphere function $f_\\text{sphere}\\left(x,y\\right)=x^2 +y^2$ with `Propulate` as a quick example. The minimum is at $\\left(x, y\\right)=\\left(0,0\\right)$ at the orange star. ![](./docs/images/sphere.png) First, we need to define the key ingredients that define our optimization problem: - The **search space** of the parameters to be optimized as a `Python` dictionary. `Propulate` can handle three different parameter types: - A tuple of `float` for a continuous parameter, e.g., `{\"learning_rate\": (0.0001, 0.01)}` - A tuple of `int` for an ordinal parameter, e.g., `{\"conv_layers\": (2, 10)}` - A tuple of `str` for a categorical parameter, e.g., `{\"activation\": (\"relu\", \"sigmoid\", \"tanh\")}` Thus, an exemplary search space might look like this: ```python search_space = { \"learning_rate\": (0.0001, 0.01), # Search a continuous space between 0.0001 and 0.01. \"num_layers\": (2, 10), # Search the integer space between 2 and 10 (inclusive). \"activation\": (\"relu\", \"sigmoid\", \"tanh\"), # Search the categorical space with the specified possibilities. } ``` The sphere function has two continuous parameters, $x$ and $y$, and we consider $x,y\\in\\left[-5.12,5.12\\right]$. The search space in our example thus looks like this: ```python limits = { \"x\": (-5.12, 5.12), \"y\": (-5.12, 5.12) } ``` - The **loss function**. This is the function we want to minimize in order to find the best parameters. It can be any `Python` function that - takes a set of parameters as a `Python` dictionary as an input. - returns a scalar loss value that determines how good the tested parameter set is. In this example, the loss function whose minimum we want to find is the sphere function: ```python def sphere(params: Dict[str, float]) -> float: \"\"\" Sphere function: continuous, convex, separable, differentiable, unimodal Input domain: -5.12 <= x, y <= 5.12 Global minimum 0 at (x, y) = (0, 0) Parameters ---------- params: Dict[str, float] The function parameters. Returns ------- float The function value. \"\"\" return numpy.sum(numpy.array(list(params.values())) ** 2).item() ``` Next, we need to define the **evolutionary operator** or propagator that we want to use to breed new individuals during the optimization process. `Propulate` provides a reasonable default propagator via a utility function: ```python # Set up logger for Propulate optimization. propulate.set_logger_config() # Set up separate random number generator for Propulate optimization. DO NOT USE SOMEWHERE ELSE! rng = random.Random( <your-random-seed> + mpi4py.MPI.COMM_WORLD.rank ) # Set up evolutionary operator. propagator = propulate.get_default_propagator( pop_size=config.pop_size, # The breeding population size limits=limits, # The search-space limits rng=rng, # Random number generator ) ``` We also need to set up the asynchronous parallel evolutionary **optimizer**, that is a so-called ``Propulator`` instance: ```python # Set up Propulator performing actual optimization. propulator = propulate.Propulator( loss_fn=sphere, propagator=propagator, rng=rng, generations=config.generations, checkpoint_path=config.checkpoint, ) ``` Now we can run the actual optimization. Overall, ``generations * mpi4py.MPI.COMM_WORLD.size`` evaluations will be performed: ```python # Run optimization and print summary of results. propulator.propulate() propulator.summarize() ``` The output should look something like this: ```text ################################################# # PROPULATE: Parallel Propagator of Populations # ################################################# [2024-03-12 14:37:01,374][propulate.propulator][INFO] - No valid checkpoint file given. Initializing population randomly... [2024-03-12 14:37:01,374][propulate.propulator][INFO] - Island 0 has 4 workers. [2024-03-12 14:37:01,374][propulate.propulator][INFO] - Island 0 Worker 0: In generation 0... [2024-03-12 14:37:01,374][propulate.propulator][INFO] - Island 0 Worker 3: In generation 0... [2024-03-12 14:37:01,374][propulate.propulator][INFO] - Island 0 Worker 2: In generation 0... [2024-03-12 14:37:01,374][propulate.propulator][INFO] - Island 0 Worker 1: In generation 0... [2024-03-12 14:37:01,377][propulate.propulator][INFO] - Island 0 Worker 3: In generation 10... [2024-03-12 14:37:01,377][propulate.propulator][INFO] - Island 0 Worker 1: In generation 10... [2024-03-12 14:37:01,378][propulate.propulator][INFO] - Island 0 Worker 0: In generation 10... [2024-03-12 14:37:01,378][propulate.propulator][INFO] - Island 0 Worker 2: In generation 10... ... [2024-03-12 14:37:02,197][propulate.propulator][INFO] - Island 0 Worker 1: In generation 960... [2024-03-12 14:37:02,206][propulate.propulator][INFO] - Island 0 Worker 2: In generation 990... [2024-03-12 14:37:02,206][propulate.propulator][INFO] - Island 0 Worker 1: In generation 970... [2024-03-12 14:37:02,215][propulate.propulator][INFO] - Island 0 Worker 1: In generation 980... [2024-03-12 14:37:02,224][propulate.propulator][INFO] - Island 0 Worker 1: In generation 990... [2024-03-12 14:37:02,232][propulate.propulator][INFO] - OPTIMIZATION DONE. NEXT: Final checks for incoming messages... [2024-03-12 14:37:02,244][propulate.propulator][INFO] - ########### # SUMMARY # ########### Number of currently active individuals is 4000. Expected overall number of evaluations is 4000. [2024-03-12 14:37:03,703][propulate.propulator][INFO] - Top 1 result(s) on island 0: (1): [{'a': '2.91E-3', 'b': '-3.05E-3'}, loss 1.78E-5, island 0, worker 0, generation 956] ``` ### Let's get your hands dirty Do the following to run the [example script](https://github.com/Helmholtz-AI-Energy/propulate/blob/master/tutorials/propulator_example.py): - Make sure you have a working MPI installation on your machine. - If you have not already done this, create a fresh virtual environment with ``Python``: ``$ python3 -m venv best-venv-ever`` - Activate it: ``$ source best-venv-ever/bin/activate`` - Upgrade ``pip``: ``$ pip install --upgrade pip`` - Install ``Propulate``: ``$ pip install propulate`` - Run the example script ``propulator_example.py``: ``$ mpirun --use-hwthread-cpus python propulator_example.py`` ## Acknowledgments *This work is supported by the Helmholtz AI platform grant.* ![](./.figs/hai_kit_logos.svg)\n",
                "dependencies": "[build-system] requires = [\"setuptools >= 61.0\"] build-backend = \"setuptools.build_meta\" [tool.setuptools] packages = [\"propulate\", \"propulate.propagators\", \"propulate.utils\"] [project] name = \"propulate\" version = \"1.2.2\" authors = [ { name=\"Marie Weiel, Oskar Taubert, Helmholtz AI\", email=\"propulate@lists.kit.edu\" }, ] description = \"Massively parallel genetic optimization through asynchronous propagation of populations\" readme = \"README.md\" requires-python = \">=3.9\" classifiers = [ \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: BSD License\", \"Development Status :: 3 - Alpha\", ] dependencies = [ \t\"deepdiff\", \"matplotlib ~= 3.8.4\", \t\"mpi4py\", \t\"numpy\", \"colorlog\", \"Gpy ~= 1.13.1\", \"h5py\", ] [project.optional-dependencies] dev = [ \"pre-commit\", \"coverage\", \"ruff\", \"mypy\", \"pytest\", \"pytest-cov\", \"pytest-mpi\", \"sphinx-autoapi\", \"sphinx-rtd-theme\", \"sphinxcontrib-napoleon\", \"sphinxemoji\", \"torch\", \"torchvision\", \"torchmetrics\", ] test = [ \"coverage\", \"pytest\", \"pytest-cov\", \"pytest-mpi\", ] tutorials = [ \"torch\", \"torchvision\", \"torchmetrics\", \"lightning\", ] [project.urls] Homepage = \"https://github.com/Helmholtz-AI-Energy/propulate\" Issues = \"https://github.com/Helmholtz-AI-Energy/propulate/issues\" [tool.mypy] python_version = \"3.9\" modules = [\"propulate\"] disallow_untyped_defs = true disallow_incomplete_defs = true files = [ \"tutorials/\" ] [tool.ruff] # Exclude a variety of commonly ignored directories. exclude = [ \".bzr\", \".direnv\", \".eggs\", \".git\", \".git-rewrite\", \".hg\", \".ipynb_checkpoints\", \".mypy_cache\", \".nox\", \".pants.d\", \".pyenv\", \".pytest_cache\", \".pytype\", \".ruff_cache\", \".svn\", \".tox\", \".venv\", \".vscode\", \"__pypackages__\", \"_build\", \"buck-out\", \"build\", \"dist\", \"node_modules\", \"site-packages\", \"venv\", ] # Same as Black. line-length = 132 indent-width = 4 # Assume Python 3.9. target-version = \"py39\" [tool.ruff.lint] # Enable Pyflakes (`F`) and a subset of the pycodestyle (`E`) codes by default. # Unlike Flake8, Ruff doesn't enable pycodestyle warnings (`W`) or # McCabe complexity (`C901`) by default. select = [\"N\", \"E4\", \"E7\", \"E9\", \"F\", \"D\"] ignore = [\"D100\", \"D104\", \"D404\"] # Enable import sorting extend-select = [\"I\"] # Allow fix for all enabled rules (when `--fix`) is provided. fixable = [\"ALL\"] unfixable = [] # Allow unused variables when underscore-prefixed. dummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\" [tool.ruff.format] # Like Black, use double quotes for strings. quote-style = \"double\" # Like Black, indent with spaces, rather than tabs. indent-style = \"space\" # Like Black, respect magic trailing commas. skip-magic-trailing-comma = false # Like Black, automatically detect the appropriate line ending. line-ending = \"auto\" # Enable auto-formatting of code examples in docstrings. Markdown, # reStructuredText code/literal blocks and doctests are all supported. # # This is currently disabled by default, but it is planned for this # to be opt-out in the future. docstring-code-format = false # Set the line length limit used when formatting code snippets in # docstrings. # # This only has an effect when the `docstring-code-format` setting is # enabled. docstring-code-line-length = \"dynamic\" [tool.ruff.lint.pydocstyle] convention = \"numpy\" [tool.pytest.ini_options] testpaths = [ \"tests\", ] [tool.coverage.run] parallel = true omit = [\"*/tests/*\"] [tool.coverage.report] omit = [\"*/tests/*\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pia",
            "repo_link": "https://github.com/hzi-braunschweig/pia-system",
            "content": {
                "codemeta": "",
                "readme": "# PIA-System ![logo](psa.app.web/src/assets/images/pia_logo.png) [![code style: prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg?style=flat-square)](https://github.com/prettier/prettier) [![DOI](https://zenodo.org/badge/319654384.svg)](https://zenodo.org/badge/latestdoi/319654384) [**P**rospective Mon**i**toring and Management - **A**pp](https://info-pia.de/) (PIA). PIA facilitates the data acquisition in health studies and takes into account the wishes of the participants, the study center and the research institution and thereby also supports the long-term motivation of the participants. PIA consists of a web application as well as mobile apps for Android and iOS that enables the participants to conduct health studies and study management, as well it can be used as a symptom diary for contact tracing. The main goals of this project are: - Simplify the data collection process - (Long-term) motivation of users through persuasive technology - Focus on usability and user centered design - Focus on data protection and security ### Built with In the backend PIA is composed of [Node.js](https://nodejs.org/) microservices that are using [PostgreSQL](https://www.postgresql.org/) as a database. The microservices are containerized using [Docker](https://www.docker.com/) and deployed with [Kubernetes](https://kubernetes.io/). As frontends an [Angular](https://angular.io/) web app and a [Ionic](https://ionicframework.com/) powered iOS and Android mobile app are provided. ## Getting started ### Local development To set up PIA for local development, please follow the [development guide](./docs/development.md). ### Deployment To deploy PIA to a (production) Kubernetes cluster, please follow the [deployment guide](./docs/deployment.md). <!-- ## Roadmap *TODO* --> ## Contributing Any contributions you make are **greatly appreciated**. Please fork the [gitlab repository](https://gitlab.com/pia-eresearch-system/pia). 1. Fork the [PIA GitLab repository](https://gitlab.com/pia-eresearch-system/pia) 2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`) 3. Make sure your Changes are formatted using [prettier](https://github.com/prettier/prettier) (`npx prettier --write .`) 4. Commit your Changes (`git commit -m 'Add some AmazingFeature'`) 5. Push to the Branch (`git push origin feature/AmazingFeature`) 6. Open a Pull Request ## Licence Distributed under the AGPL-3.0 license. See [LICENSE](./LICENSES/AGPL-3.0-or-later.txt) for more information. ## Contact [PiaPost@helmholtz-hzi.de](mailto:PiaPost@helmholtz-hzi.de) ![HZI](psa.app.web/src/assets/images/hzi_logo.jpg)\n",
                "dependencies": "{\"name\":\"root\",\"private\":true,\"license\":\"AGPL-3.0-or-later\",\"scripts\":{\"start\":\"skaffold dev\",\"update-packages\":\"ncu --packageFile 'psa.*/package.json' -i -x '/@types/node$|@types/node-fetch|node-fetch|typeorm|parse5|chalk|ngx-.*|ng-.*|rxjs|jspdf.*|.*angular.*/'\",\"update-packagelock\":\"rm -rf psa.*/node_modules psa.*/package-lock.json && lerna exec \\\"npm install --ignore-scripts --package-lock-only --no-audit\\\"\",\"update-openapi\":\"REBUILD_OPENAPI=1 ./psa.utils.scripts/openapi/generate-merge-config.sh && openapi-merge-cli\",\"update-third-party-licenses\":\"docker build --target raw -f psa.utils.ci-thirdparty-license-collector/Dockerfile -o . .\",\"format\":\"prettier -w .\"},\"devDependencies\":{\"lerna\":\"^4.0.0\",\"npm-check-updates\":\"^12.0.2\",\"openapi-merge-cli\":\"^1.3.1\",\"prettier\":\"^2.5.1\"}}\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ptylab",
            "repo_link": "https://github.com/PtyLab/PtyLab.py",
            "content": {
                "codemeta": "",
                "readme": "# PtyLab.py ![Python 3.9+](https://img.shields.io/badge/python-3.9+-green.svg) PtyLab is an inverse modeling toolbox for Conventional (CP) and Fourier (FP) ptychography in a unified framework. For more information please check the [paper](https://opg.optica.org/oe/fulltext.cfm?uri=oe-31-9-13763&id=529026). ## Getting started The simplest way to get started is to check the below demo in Google Colab. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PtyLab/PtyLab.py/blob/main/demo.ipynb) ![demo](assets/recon.gif) To explore more use cases of PtyLab, check the [example_scripts](example_scripts) and [jupyter_tutorials](jupyter_tutorials) directories. However, please install the package first as described in the below sections. ## Installation To install the package from source, ```bash pip install git+https://github.com/PtyLab/PtyLab.py.git ``` This package uses `cupy` to utilize GPU for faster reconstruction. Please check their [instructions](https://docs.cupy.dev/en/stable/install.html) for installing this dependency. ### Development Please clone this repository and navigate to the root folder ```bash git clone git@github.com:PtyLab/PtyLab.py.git cd PtyLab.py ``` Inside a virtual environment (preferably with conda), please install `ptylab` and its dependencies from the pinned versions specified under `requirements.txt`: ```bash conda create --name ptylab_venv python=3.11.5 # or python version satisfying \">=3.9, <3.12\" conda activate ptylab_venv pip install -e . -r requirements.txt ``` To use the GPU, `cupy` can be additionally installed in this environment. ```bash conda install -c conda-forge cupy ``` If you would like to contribute to this package, especially if it involves modifying dependencies, please checkout the [`CONTRIBUTING.md`](CONTRIBUTING.md) file. ## Citation If you use this package in your work, cite us as below. ```tex @article{Loetgering:23, author = {Lars Loetgering and Mengqi Du and Dirk Boonzajer Flaes and Tomas Aidukas and Felix Wechsler and Daniel S. Penagos Molina and Max Rose and Antonios Pelekanidis and Wilhelm Eschen and J\\\"{u}rgen Hess and Thomas Wilhein and Rainer Heintzmann and Jan Rothhardt and Stefan Witte}, journal = {Opt. Express}, number = {9}, pages = {13763--13797}, publisher = {Optica Publishing Group}, title = {PtyLab.m/py/jl: a cross-platform, open-source inverse modeling toolbox for conventional and Fourier ptychography}, volume = {31}, month = {Apr}, year = {2023}, doi = {10.1364/OE.485370}, } ```\n",
                "dependencies": "[tool.poetry] name = \"ptylab\" version = \"0.2.1\" description = \"A cross-platform, open-source inverse modeling toolbox for conventional and Fourier ptychography\" authors = [\"Lars Loetgering <lars.loetgering@fulbrightmail.org>\", \"PtyLab Team\"] readme = \"README.md\" packages = [ { include = \"PtyLab\" } ] [tool.poetry.dependencies] python = \">=3.9, <3.13\" numpy = \">=1.22, <2.0.0\" matplotlib = \"^3.7.2\" h5py = \"^3.9.0\" scipy = \"^1.11.1\" scikit-image = \"^0.21.0\" scikit-learn = \"^1.3.0\" tqdm = \"^4.65.0\" pyqtgraph = \"^0.13.3\" tables = \"^3.8.0\" bokeh = \"^3.2.1\" tensorflow = \"^2.14.0\" PyQt5 = \"^5.15.10\" PyQt5-Qt5 = \"*\" black = { version = \"^23.7.0\", optional = true } ipykernel = { version = \"^6.25.0\", optional = true } pre-commit = { version = \"^4.0.1\", optional = true } [tool.poetry.extras] dev = [\"black\", \"ipykernel\", \"pre-commit\"] [build-system] requires = [\"poetry-core\"] build-backend = \"poetry.core.masonry.api\"\nabsl-py==2.1.0 ; python_version >= \"3.9\" and python_version < \"3.13\" appnope==0.1.4 ; python_version >= \"3.9\" and python_version < \"3.13\" and platform_system == \"Darwin\" asttokens==2.4.1 ; python_version >= \"3.9\" and python_version < \"3.13\" astunparse==1.6.3 ; python_version >= \"3.9\" and python_version < \"3.13\" black==23.12.1 ; python_version >= \"3.9\" and python_version < \"3.13\" blosc2==2.5.1 ; python_version >= \"3.9\" and python_version < \"3.13\" bokeh==3.4.1 ; python_version >= \"3.9\" and python_version < \"3.13\" certifi==2024.6.2 ; python_version >= \"3.9\" and python_version < \"3.13\" cffi==1.17.0 ; python_version >= \"3.9\" and python_version < \"3.13\" and implementation_name == \"pypy\" cfgv==3.4.0 ; python_version >= \"3.9\" and python_version < \"3.13\" charset-normalizer==3.3.2 ; python_version >= \"3.9\" and python_version < \"3.13\" click==8.1.7 ; python_version >= \"3.9\" and python_version < \"3.13\" colorama==0.4.6 ; python_version >= \"3.9\" and python_version < \"3.13\" and (platform_system == \"Windows\" or sys_platform == \"win32\") comm==0.2.2 ; python_version >= \"3.9\" and python_version < \"3.13\" contourpy==1.2.1 ; python_version >= \"3.9\" and python_version < \"3.13\" cycler==0.12.1 ; python_version >= \"3.9\" and python_version < \"3.13\" debugpy==1.8.5 ; python_version >= \"3.9\" and python_version < \"3.13\" decorator==5.1.1 ; python_version >= \"3.9\" and python_version < \"3.13\" distlib==0.3.9 ; python_version >= \"3.9\" and python_version < \"3.13\" exceptiongroup==1.2.2 ; python_version >= \"3.9\" and python_version < \"3.11\" executing==2.0.1 ; python_version >= \"3.9\" and python_version < \"3.13\" filelock==3.16.1 ; python_version >= \"3.9\" and python_version < \"3.13\" flatbuffers==24.3.25 ; python_version >= \"3.9\" and python_version < \"3.13\" fonttools==4.53.0 ; python_version >= \"3.9\" and python_version < \"3.13\" gast==0.4.0 ; python_version >= \"3.9\" and python_version < \"3.13\" google-pasta==0.2.0 ; python_version >= \"3.9\" and python_version < \"3.13\" grpcio==1.64.1 ; python_version >= \"3.9\" and python_version < \"3.13\" h5py==3.11.0 ; python_version >= \"3.9\" and python_version < \"3.13\" identify==2.6.1 ; python_version >= \"3.9\" and python_version < \"3.13\" idna==3.7 ; python_version >= \"3.9\" and python_version < \"3.13\" imageio==2.34.1 ; python_version >= \"3.9\" and python_version < \"3.13\" importlib-metadata==7.1.0 ; python_version >= \"3.9\" and python_version < \"3.10\" importlib-resources==6.4.0 ; python_version >= \"3.9\" and python_version < \"3.10\" ipykernel==6.29.5 ; python_version >= \"3.9\" and python_version < \"3.13\" ipython==8.18.1 ; python_version >= \"3.9\" and python_version < \"3.13\" jedi==0.19.1 ; python_version >= \"3.9\" and python_version < \"3.13\" jinja2==3.1.4 ; python_version >= \"3.9\" and python_version < \"3.13\" joblib==1.4.2 ; python_version >= \"3.9\" and python_version < \"3.13\" jupyter-client==8.6.2 ; python_version >= \"3.9\" and python_version < \"3.13\" jupyter-core==5.7.2 ; python_version >= \"3.9\" and python_version < \"3.13\" keras==3.3.3 ; python_version >= \"3.9\" and python_version < \"3.13\" kiwisolver==1.4.5 ; python_version >= \"3.9\" and python_version < \"3.13\" lazy-loader==0.4 ; python_version >= \"3.9\" and python_version < \"3.13\" libclang==18.1.1 ; python_version >= \"3.9\" and python_version < \"3.13\" markdown-it-py==3.0.0 ; python_version >= \"3.9\" and python_version < \"3.13\" markdown==3.6 ; python_version >= \"3.9\" and python_version < \"3.13\" markupsafe==2.1.5 ; python_version >= \"3.9\" and python_version < \"3.13\" matplotlib-inline==0.1.7 ; python_version >= \"3.9\" and python_version < \"3.13\" matplotlib==3.9.0 ; python_version >= \"3.9\" and python_version < \"3.13\" mdurl==0.1.2 ; python_version >= \"3.9\" and python_version < \"3.13\" ml-dtypes==0.3.2 ; python_version >= \"3.9\" and python_version < \"3.13\" msgpack==1.0.8 ; python_version >= \"3.9\" and python_version < \"3.13\" mypy-extensions==1.0.0 ; python_version >= \"3.9\" and python_version < \"3.13\" namex==0.0.8 ; python_version >= \"3.9\" and python_version < \"3.13\" ndindex==1.8 ; python_version >= \"3.9\" and python_version < \"3.13\" nest-asyncio==1.6.0 ; python_version >= \"3.9\" and python_version < \"3.13\" networkx==3.2.1 ; python_version >= \"3.9\" and python_version < \"3.13\" nodeenv==1.9.1 ; python_version >= \"3.9\" and python_version < \"3.13\" numexpr==2.10.0 ; python_version >= \"3.9\" and python_version < \"3.13\" numpy==1.26.4 ; python_version >= \"3.9\" and python_version < \"3.13\" opt-einsum==3.3.0 ; python_version >= \"3.9\" and python_version < \"3.13\" optree==0.11.0 ; python_version >= \"3.9\" and python_version < \"3.13\" packaging==24.0 ; python_version >= \"3.9\" and python_version < \"3.13\" pandas==2.2.2 ; python_version >= \"3.9\" and python_version < \"3.13\" parso==0.8.4 ; python_version >= \"3.9\" and python_version < \"3.13\" pathspec==0.12.1 ; python_version >= \"3.9\" and python_version < \"3.13\" pexpect==4.9.0 ; python_version >= \"3.9\" and python_version < \"3.13\" and sys_platform != \"win32\" pillow==10.3.0 ; python_version >= \"3.9\" and python_version < \"3.13\" platformdirs==4.2.2 ; python_version >= \"3.9\" and python_version < \"3.13\" pre-commit==4.0.1 ; python_version >= \"3.9\" and python_version < \"3.13\" prompt-toolkit==3.0.47 ; python_version >= \"3.9\" and python_version < \"3.13\" protobuf==4.25.3 ; python_version >= \"3.9\" and python_version < \"3.13\" psutil==6.0.0 ; python_version >= \"3.9\" and python_version < \"3.13\" ptyprocess==0.7.0 ; python_version >= \"3.9\" and python_version < \"3.13\" and sys_platform != \"win32\" pure-eval==0.2.3 ; python_version >= \"3.9\" and python_version < \"3.13\" py-cpuinfo==9.0.0 ; python_version >= \"3.9\" and python_version < \"3.13\" pycparser==2.22 ; python_version >= \"3.9\" and python_version < \"3.13\" and implementation_name == \"pypy\" pygments==2.18.0 ; python_version >= \"3.9\" and python_version < \"3.13\" pyparsing==3.1.2 ; python_version >= \"3.9\" and python_version < \"3.13\" pyqt5-qt5==5.15.2 ; python_version >= \"3.9\" and python_version < \"3.13\" pyqt5-sip==12.13.0 ; python_version >= \"3.9\" and python_version < \"3.13\" pyqt5==5.15.10 ; python_version >= \"3.9\" and python_version < \"3.13\" pyqtgraph==0.13.7 ; python_version >= \"3.9\" and python_version < \"3.13\" python-dateutil==2.9.0.post0 ; python_version >= \"3.9\" and python_version < \"3.13\" pytz==2024.1 ; python_version >= \"3.9\" and python_version < \"3.13\" pywavelets==1.6.0 ; python_version >= \"3.9\" and python_version < \"3.13\" pywin32==306 ; sys_platform == \"win32\" and platform_python_implementation != \"PyPy\" and python_version >= \"3.9\" and python_version < \"3.13\" pyyaml==6.0.1 ; python_version >= \"3.9\" and python_version < \"3.13\" pyzmq==26.1.0 ; python_version >= \"3.9\" and python_version < \"3.13\" requests==2.32.3 ; python_version >= \"3.9\" and python_version < \"3.13\" rich==13.7.1 ; python_version >= \"3.9\" and python_version < \"3.13\" scikit-image==0.21.0 ; python_version >= \"3.9\" and python_version < \"3.13\" scikit-learn==1.5.0 ; python_version >= \"3.9\" and python_version < \"3.13\" scipy==1.13.1 ; python_version >= \"3.9\" and python_version < \"3.13\" setuptools==70.0.0 ; python_version >= \"3.9\" and python_version < \"3.13\" six==1.16.0 ; python_version >= \"3.9\" and python_version < \"3.13\" stack-data==0.6.3 ; python_version >= \"3.9\" and python_version < \"3.13\" tables==3.9.2 ; python_version >= \"3.9\" and python_version < \"3.13\" tensorboard-data-server==0.7.2 ; python_version >= \"3.9\" and python_version < \"3.13\" tensorboard==2.16.2 ; python_version >= \"3.9\" and python_version < \"3.13\" tensorflow-io-gcs-filesystem==0.37.0 ; python_version >= \"3.9\" and python_version < \"3.12\" tensorflow==2.16.1 ; python_version >= \"3.9\" and python_version < \"3.13\" termcolor==2.4.0 ; python_version >= \"3.9\" and python_version < \"3.13\" threadpoolctl==3.5.0 ; python_version >= \"3.9\" and python_version < \"3.13\" tifffile==2024.5.22 ; python_version >= \"3.9\" and python_version < \"3.13\" tomli==2.0.1 ; python_version >= \"3.9\" and python_version < \"3.11\" tornado==6.4 ; python_version >= \"3.9\" and python_version < \"3.13\" tqdm==4.66.4 ; python_version >= \"3.9\" and python_version < \"3.13\" traitlets==5.14.3 ; python_version >= \"3.9\" and python_version < \"3.13\" typing-extensions==4.5.0 ; python_version >= \"3.9\" and python_version < \"3.13\" tzdata==2024.1 ; python_version >= \"3.9\" and python_version < \"3.13\" urllib3==2.2.1 ; python_version >= \"3.9\" and python_version < \"3.13\" virtualenv==20.27.0 ; python_version >= \"3.9\" and python_version < \"3.13\" wcwidth==0.2.13 ; python_version >= \"3.9\" and python_version < \"3.13\" werkzeug==3.0.3 ; python_version >= \"3.9\" and python_version < \"3.13\" wheel==0.43.0 ; python_version >= \"3.9\" and python_version < \"3.13\" wrapt==1.16.0 ; python_version >= \"3.9\" and python_version < \"3.13\" xyzservices==2024.4.0 ; python_version >= \"3.9\" and python_version < \"3.13\" zipp==3.19.2 ; python_version >= \"3.9\" and python_version < \"3.10\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pyapi-rts",
            "repo_link": "https://github.com/KIT-IAI/PyAPI-RTS",
            "content": {
                "codemeta": "",
                "readme": "# PyAPI-RTS **A Python library to read and manipulate RSCAD draft files.** See <a href=\"examples/simple_example/simple_example.ipynb\">examples/simple_example/simple_example.ipynb</a> for a short preview of the API or take a look into our <a href=\"docs/pyapi_rts.pdf\">documentation</a>. ## Installation To install this project, perform the following steps: 1. Clone the project 2. `cd` into the cloned directory 3. `pip install poetry` 4. `poetry install` ## Generate classes from RSCAD components Before the first use of the project, the classes for the components in the RSCAD master library need to be generated. 1. Copy the files from the `COMPONENTS` directory into `pyapi_rts/pyapi_rts/class_extractor/COMPONENTS`. 2. Run `poetry run python ./pyapi_rts/class_extractor/main.py` Other options for the class generation: - \\-d: Set to delete the output folder before new classes are generated - \\-o: Set to include the OBSOLETE folder in the generation. Recommended if you use .dfx files converted from older versions - \\-p: Set path to COMPONENTS folder - \\-t: Set thread count used to parse the files. Default: 8 ! The progress bar is not accurate due to optimizations applied during generation. ## Run tests `poetry run pytest` ## Citing > M. Weber, J. Enzinger, H. K. Çakmak, U. Kühnapfel and V. Hagenmeyer, \"PyAPI-RTS: A Python-API for RSCAD Modeling,\" 2023 Open Source Modelling and Simulation of Energy Systems (OSMSES), Aachen, Germany, 2023, pp. 1-7, doi: [10.1109/OSMSES58477.2023.10089671](https://doi.org/10.1109/OSMSES58477.2023.10089671).\n",
                "dependencies": "[tool.poetry] name = \"pyapi_rts\" version = \"0.1.0\" description = \"\" authors = [\"Moritz Weber <moritz.weber@kit.edu>\"] exclude = [\"pyapi_rts/class_extractor\"] [tool.poetry.dependencies] python = \">=3.10,<3.12\" networkx = \"^2.8\" progress = \"^1.6\" lark = \"^1.1.2\" ipykernel = \"^6.15.1\" [tool.poetry.dev-dependencies] pytest = \"^7.2.0\" pytest-xdist = \"^2.5.0\" pytest-cov = \"^3.0.0\" coverage = \"^6.3.3\" Sphinx = \"^4.5.0\" sphinx-rtd-theme = \"^1.0.0\" pytest-profiling = \"^1.7.0\" black = {version = \"^24.3.0\", allow-prereleases = true} matplotlib = \"^3.5.2\" numpy = \"^1.23.1\" vprof = \"^0.38\" scipy = \"^1.10.0\" [build-system] requires = [\"poetry-core>=1.0.0\"] build-backend = \"poetry.core.masonry.api\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pycomlink",
            "repo_link": "https://github.com/pycomlink/pycomlink",
            "content": {
                "codemeta": "",
                "readme": "[![CI](https://github.com/pycomlink/pycomlink/actions/workflows/main.yml/badge.svg?branch=master)](https://github.com/pycomlink/pycomlink/actions/workflows/main.yml) [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/pycomlink/pycomlink/master) [![Documentation Status](https://readthedocs.org/projects/pycomlink/badge/?version=latest)](https://pycomlink.readthedocs.io/en/latest/) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4810169.svg)](https://doi.org/10.5281/zenodo.4810169) Anaconda Version [![Anaconda Version](https://anaconda.org/conda-forge/pycomlink/badges/version.svg)](https://anaconda.org/conda-forge/pycomlink) [![Anaconda-Server Badge](https://anaconda.org/conda-forge/pycomlink/badges/latest_release_date.svg)](https://anaconda.org/conda-forge/pycomlink) pycomlink ========= A python toolbox for deriving rainfall information from commercial microwave link (CML) data. Installation ------------ `pycomlink` is tested with Python 3.9, 3.10 and 3.11. There have been problems with Python 3.8, see https://github.com/pycomlink/pycomlink/pull/120. Many things might work with older version, but there is no support for this. It can be installed via [`conda-forge`](https://conda-forge.org/): $ conda install -c conda-forge pycomlink If you are new to `conda` or if you are unsure, it is recommended to [create a new conda environment, activate it](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands), [add the conda-forge channel](https://conda-forge.org/) and then install. Installation via `pip` is also possible: $ pip install pycomlink At the time of writing, with `pycomlink v0.4.0` which dropped `tensorflow` as dependency, `pip` install works fine. But, if we add new dependencies in the future, we might again run into issues with `pip` install. To run the example notebooks you will also need the [Jupyter Notebook](https://jupyter.org/) and `ipython`, both also available via `conda` or `pip`. If you want to clone the repository for developing purposes follow these steps (installation of Jupyter Notebook included): $ git clone https://github.com/pycomlink/pycomlink.git $ cd pycomlink $ conda env create -f environment_dev.yml $ conda activate pycomlink-dev $ cd .. $ pip install -e pycomlink Usage ----- The following jupyter notebooks showcase some use cases of `pycomlink` * [Basic example CML processing workflow](http://nbviewer.jupyter.org/github/pycomlink/pycomlink/blob/master/notebooks/Basic%20CML%20processing%20workflow.ipynb) * [Compare interpolation methods](https://nbviewer.org/github/pycomlink/pycomlink/blob/master/notebooks/Compare%20interpolation%20methods.ipynb) * [Get radar data along CML paths](https://nbviewer.org/github/pycomlink/pycomlink/blob/master/notebooks/Get%20radar%20rainfall%20along%20CML%20paths.ipynb) * [Nearby-link approach for rain event detection from RAINLINK](https://nbviewer.org/github/pycomlink/pycomlink/blob/master/notebooks/Nearby%20link%20approach%20processing%20example.ipynb) * [Compare different WAA methods](https://nbviewer.org/github/pycomlink/pycomlink/blob/master/notebooks/Wet%20antenna%20attenuation.ipynb) * [Detect data gaps stemming from heavy rainfall events that cause a loss of connection along a CML](https://nbviewer.org/github/pycomlink/pycomlink/blob/master/notebooks/Blackout%20gap%20detection%20examples.ipynb) Note that the links point to static versions of the example notebooks. You can run all these notebook online via mybinder if you click on the \"launch binder\" buttom at the top. Features -------- * Perform all required CML data processing steps to derive rainfall information from raw signal levels: * data sanity checks * ~~anomaly detection~~ (removed because using outdated `tensorflow` code) * wet/dry classification * baseline calculation * wet antenna correction * transformation from attenuation to rain rate * Generate rainfall maps from the data of a CML network * Validate you results against gridded rainfall data or rain gauges networks Documentation ------------- The documentation is hosted by readthedocs.org: [https://pycomlink.readthedocs.io/en/latest/](https://pycomlink.readthedocs.io/en/latest/)\n",
                "dependencies": "numpy scipy pandas matplotlib numba h5py xarray sparse netcdf4 shapely pyproj tqdm pykrige scikit-learn future bottleneck poligrain\n# -*- coding: utf-8 -*- \"\"\" Created on Tue Dec 2 13:20:35 2014 @author: chwala-c \"\"\" import os from setuptools import setup, find_packages with open(\"requirements.txt\", \"r\") as f: INSTALL_REQUIRES = [rq for rq in f.read().split(\"\\n\") if rq != \"\"] # Utility function to read the README file. # Used for the long_description. It's nice, because now 1) we have a top level # README file and 2) it's easier to type in the README file than to put a raw # string in below ... def read(fname): return open(os.path.join(os.path.dirname(__file__), fname)).read() setup( name = \"pycomlink\", version = \"0.4.1\", author = \"Christian Chwala\", author_email = \"christian.chwala@kit.edu\", description = (\"Python tools for CML (commercial microwave link) data processing\"), license = \"BSD\", keywords = \"microwave links precipitation radar cml\", url = \"https://github.com/pycomlink/pycomlink\", download_url = ( \"https://github.com/pycomlink/pycomlink/archive/0.4.1.tar.gz\"), packages=find_packages(exclude=['test']), include_package_data=True, long_description=read('README.md'), classifiers=[ \"Development Status :: 3 - Alpha\", \"Topic :: Scientific/Engineering :: Atmospheric Science\", \"License :: OSI Approved :: BSD License\", 'Programming Language :: Python :: 3.9', 'Programming Language :: Python :: 3.10', 'Programming Language :: Python :: 3.11', ], # A list of all available classifiers can be found at # https://pypi.python.org/pypi?%3Aaction=list_classifiers install_requires=INSTALL_REQUIRES, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pygms",
            "repo_link": "https://github.com/cmeessen/pyGMS",
            "content": {
                "codemeta": "",
                "readme": "# pyGMS [![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0) [![DOI](https://zenodo.org/badge/194238991.svg)](https://zenodo.org/badge/latestdoi/194238991) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/50e5df33317949d58e8d7bf7c40a336b)](https://www.codacy.com?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=cmeessen/pyGMS&amp;utm_campaign=Badge_Grade) [![Codacy Badge](https://api.codacy.com/project/badge/Coverage/50e5df33317949d58e8d7bf7c40a336b)](https://www.codacy.com?utm_source=github.com&utm_medium=referral&utm_content=cmeessen/pyGMS&utm_campaign=Badge_Coverage) `pyGMS` is a Python 3 module designed to analyse the rheological behaviour of lithosphere-scale 3D structural models that were created with the [GeoModellingSystem](https://www.gfz-potsdam.de/en/section/basin-modeling/infrastructure/gms/) (GMS, GFZ Potsdam). `pyGMS` was originally written for the purpose of plotting yield strength envelope cross sections for my PhD thesis. ## Installation This is a short version of the installation instructions. For a more detailed version visit the [documentation](https://cmeessen.github.io/pyGMS/installation.html). ```bash # Clone the repository git clone git@github.com:cmeessen/pyGMS.git # Create an Anaconda environment cd pyGMS conda env create -f environment.yml # Install with pip conda activate pygms pip install -e . # Install some dependencies to be able to see the kernel in Jupyter notebooks conda install -c conda-forge nb_conda_kernels ``` ## Documentation Please have a look at the [documentation](https://cmeessen.github.io/pyGMS/index.html) for information on how to install and use pyGMS. ## Contributing If you find bugs, have a feature wish or a pull request, please open an [issue](https://github.com/cmeessen/pyGMS/issues). ### Preparing a pull request Before preparing a pull request make sure to - comment the code - update CHANGELOG - check code style (`make pycodestyle`) - add a test if the contribution adds a new feature or fixes a bug - update the documentation (`cd docs/sphinx && make html && make gh-pages`) - run `make coverage` (maintainers only)\n",
                "dependencies": "from setuptools import setup from setuptools import find_packages from pkg_resources import resource_filename from pyGMS import __version__ # METADATA NAME = 'pygms-cmeessen' MODULE = 'pyGMS' VERSION = __version__ AUTHOR = 'Christian Meeßen' AUTHOR_EMAIL = 'christian.meessen@gfz-potsdam.de' MAINTAINER = 'Christian Meeßen' MAINTAINER_EMAIL = 'christian.meessen@gfz-potsdam.de' URL = 'https://github.com/cmeessen/pyGMS' DESCRIPTION = 'A Python module to analyse models created with the GeoModellingSystem' try: with open(resource_filename(MODULE, '../README.md'), 'r') as fh: LONG_DESCRIPTION = fh.read() except ImportError: with open('README.md') as fh: LONG_DESCRIPTION = fh.read() LONG_DESCRIPTION_TYPE = 'text/markdown' PACKAGE_DATA = find_packages() CLASSIFIERS = [ 'Natural Language :: English', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.7', 'License :: OSI Approved :: GNU GPL-3.0', 'Operating System :: OS Independent', 'Topic :: Geophysics', ] # DEPENDENCIES INSTALL_REQUIRES = [ 'numpy', 'matplotlib', 'pandas', 'scipy' ] if __name__ == '__main__': setup( name=NAME, version=VERSION, author=AUTHOR, author_email=AUTHOR_EMAIL, maintainer=MAINTAINER, maintainer_email=MAINTAINER_EMAIL, description=DESCRIPTION, long_description=LONG_DESCRIPTION, long_description_content_type=LONG_DESCRIPTION_TYPE, url=URL, packages=PACKAGE_DATA, classifiers=CLASSIFIERS, install_requires=INSTALL_REQUIRES, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pyquickmaps",
            "repo_link": "https://git.geomar.de/open-source/pyquickmaps",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pysdc",
            "repo_link": "https://github.com/Parallel-in-Time/pySDC",
            "content": {
                "codemeta": "",
                "readme": "[![badge-ga](https://github.com/Parallel-in-Time/pySDC/actions/workflows/ci_pipeline.yml/badge.svg?branch=master)](https://github.com/Parallel-in-Time/pySDC/actions/workflows/ci_pipeline.yml) [![badge-ossf](https://bestpractices.coreinfrastructure.org/projects/6909/badge)](https://bestpractices.coreinfrastructure.org/projects/6909) [![badge-cc](https://codecov.io/gh/Parallel-in-Time/pySDC/branch/master/graph/badge.svg?token=hpP18dmtgS)](https://codecov.io/gh/Parallel-in-Time/pySDC) [![zenodo](https://zenodo.org/badge/26165004.svg)](https://zenodo.org/badge/latestdoi/26165004) [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu) [![SQAaaS badge shields.io](https://img.shields.io/badge/sqaaas%20software-silver-lightgrey)](https://api.eu.badgr.io/public/assertions/aS8J0NDTTjCyYP6iVufviQ \"SQAaaS silver badge achieved\") [![PyPI - Downloads](https://img.shields.io/pypi/dm/pySDC?logo=pypi)](https://pypistats.org/packages/pysdc) # Welcome to pySDC! The `pySDC` project is a Python implementation of the spectral deferred correction (SDC) approach and its flavors, esp. the multilevel extension MLSDC and PFASST. It is intended for rapid prototyping and educational purposes. New ideas like e.g. sweepers or predictors can be tested and first toy problems can be easily implemented. ## Features - Variants of SDC: explicit, implicit, IMEX, multi-implicit, Verlet, multi-level, diagonal, multi-step - Variants of PFASST: virtually parallel or MPI-based parallel, classical or multigrid perspective - 8 tutorials: from setting up a first collocation problem to SDC, PFASST and advanced topics - Projects: many documented projects with defined and tested outcomes - Many different examples, collocation types, data types already implemented - Works with [FEniCS](https://fenicsproject.org/), [mpi4py-fft](https://mpi4py-fft.readthedocs.io/en/latest/) and [PETSc](http://www.mcs.anl.gov/petsc/) (through [petsc4py](https://bitbucket.org/petsc/petsc4py)) - Continuous integration via [GitHub Actions](https://github.com/Parallel-in-Time/pySDC/actions) and [Gitlab CI](https://gitlab.hzdr.de/r.speck/pysdc/-/pipelines) (through the [GitHub2Gitlab Action](https://github.com/jakob-fritz/github2lab_action)) - Fully compatible with Python 3.9 - 3.12, runs at least on Ubuntu ## Getting started The code is hosted on GitHub, see <https://github.com/Parallel-in-Time/pySDC>, and PyPI, see <https://pypi.python.org/pypi/pySDC>. While using `pip install pySDC` will give you a core version of `pySDC` to work with, working with the developer version is most often the better choice. We thus recommend to checkout the code from GitHub and install the dependencies e.g. by using [micromamba](https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html). For this, `pySDC` ships with environment files which can be found in the folder `etc/` or within the projects. Use these as e.g. ``` bash micromamba create -f etc/environment-base.yml ``` If you want to install the developer version using `pip` directly from the GitHub repository, use this: ``` # optionally use venv python3 -m venv name_of_pySDC_env . ./name_of_pySDC_env/bin/activate # drop @5.5.0 if you want to install the develop version pip install git+https://github.com/Parallel-in-Time/pySDC@5.5.0 ``` To check your installation, run ``` bash pytest pySDC/tests -m NAME ``` where `NAME` corresponds to the environment you chose (`base` in the example above). You may need to update your `PYTHONPATH` by running ``` bash export PYTHONPATH=$PYTHONPATH:/path/to/pySDC/root/folder ``` in particular if you want to run any of the playgrounds, projects or tutorials. All `import` statements there assume that the `pySDC`\\'s base directory is part of `PYTHONPATH`. For many examples, `LaTeX` is used for the plots, i.e. a decent installation of this is needed in order to run those examples. When using `fenics` or `petsc4py`, a C++ compiler is required (although installation may go through at first). For more details on `pySDC`, check out http://www.parallel-in-time.org/pySDC. ## How to cite If you use pySDC or parts of it for your work, great! Let us know if we can help you with this. Also, we would greatly appreciate a citation of [this paper](https://doi.org/10.1145/3310410): > Robert Speck, **Algorithm 997: pySDC - Prototyping Spectral Deferred > Corrections**, ACM Transactions on Mathematical Software (TOMS), > Volume 45 Issue 3, August 2019, <https://doi.org/10.1145/3310410> The current software release can be cited using Zenodo: [![zenodo](https://zenodo.org/badge/26165004.svg)](https://zenodo.org/badge/latestdoi/26165004) ## Contributing `pySDC` code was originally developed by [Robert Speck (@pancetta)](https://github.com/pancetta), and is now maintained and developed by a small community of scientists interested in SDC methods. Checkout the [Changelog](./CHANGELOG.md) to see pySDC's evolution since 2016. It has a software management plan (SWP), too, see [here](https://smw.dsw.elixir-europe.org/wizard/projects/c3dda921-b7b0-4f4d-b5dc-778b9780552d). Any contribution is dearly welcome! If you want to contribute, please take the time to read our [Contribution Guidelines](./CONTRIBUTING.md) (and don't forget to take a peek at our nice [Code of Conduct](./CODE_OF_CONDUCT.md) :wink:). ## Acknowledgements This project has received funding from the [European High-Performance Computing Joint Undertaking](https://eurohpc-ju.europa.eu/) (JU) under grant agreement No 955701 ([TIME-X](https://www.time-x-eurohpc.eu/)) and grant agreement No 101118139. The JU receives support from the European Union's Horizon 2020 research and innovation programme and Belgium, France, Germany, and Switzerland. This project also received funding from the [German Federal Ministry of Education and Research](https://www.bmbf.de/bmbf/en/home/home_node.html) (BMBF) grants 16HPC047 and 16ME0679K. Supported by the European Union - NextGenerationEU. The project also received help from the [Helmholtz Platform for Research Software Engineering - Preparatory Study (HiRSE_PS)](https://www.helmholtz-hirse.de/). <p align=\"center\"> <img src=\"./docs/img/EuroHPC.jpg\" height=\"105\"/> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src=\"./docs/img/LogoTime-X.png\" height=\"105\" /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src=\"./docs/img/BMBF_gefoerdert_2017_en.jpg\" height=\"105\" /> </p>\n",
                "dependencies": "[build-system] requires = [\"flit_core >=3.2,<4\"] build-backend = \"flit_core.buildapi\" [project] name = 'pySDC' version = '5.6' description = 'A Python implementation of spectral deferred correction methods and the likes' license = {text = \"BSD-2-Clause\"} readme = 'README.md' authors=[ {name='Robert Speck', email='r.speck@fz-juelich.de'}, {name='Thibaut Lunet', email='thibaut.lunet@tuhh.de'}, {name='Thomas Baumann', email='t.baumann@fz-juelich.de'}, {name='Lisa Wimmer', email='wimmer@uni-wuppertal.de'}, {name='Ikrom Akramov', email='ikrom.akramov@tuhh.de'}, {name='Giacomo Rosilho De Souza', email='giacomo.rosilhodesouza@usi.ch'}, {name='Jakob Fritz', email='j.fritz@fz-juelich.de'}, {name='Jemma Shipton', email='j.shipton@exeter.ac.uk'}, ] classifiers = [ \"Topic :: Scientific/Engineering :: Mathematics\", ] dependencies = [ 'numpy>=1.15.4', 'scipy>=0.17.1', 'matplotlib>=3.0', 'sympy>=1.0', 'numba>=0.35', 'dill>=0.2.6', 'qmat>=0.1.8', ] [project.urls] Homepage = \"http://www.parallel-in-time.org/pySDC/\" Repository = \"https://github.com/Parallel-in-Time/pySDC/\" Documentation = \"http://www.parallel-in-time.org/pySDC/\" Tracker = \"https://github.com/Parallel-in-Time/pySDC/issues\" [project.optional-dependencies] apps = [ 'petsc4py>=3.10.0', 'mpi4py>=3.0.0', 'fenics>=2019.1.0', 'mpi4py-fft>=2.0.2' ] dev = [ 'ruff', 'pytest', 'pytest-cov', 'sphinx' ] [tool.pytest.ini_options] markers = [ 'base: all basic tests', 'fenics: tests relying on FEniCS', 'slow: tests taking much longer than bearable', 'mpi4py: tests using MPI parallelism (but no other external library such as petsc)', 'petsc: tests relying on PETSc/petsc4py', 'benchmark: tests for benchmarking', 'cupy: tests for cupy on GPUs', 'libpressio: tests using the libpressio library', 'monodomain: tests the monodomain project, which requires previous compilation of c++ code', 'pytorch: tests for PyTorch related things in pySDC', 'firedrake: tests for firedrake', ] timeout = 300 [tool.ruff] line-length = 120 extend-exclude = [ 'playgrounds', 'tests', '*/data/*' ] [tool.ruff.lint] select = [\"C\", \"E\", \"F\", \"W\", \"B\"] ignore = [\"E203\", # Whitespace before punctuation \"E741\", # Ambiguous variable name \"E402\", # Module level import not at top of cell \"W605\", # Invalid escape sequence \"F401\", # unused import \"B023\", # function uses loop variable \"B028\", # No excplicit stackvariable \"C408\", # Unnecessary collection call \"C417\", # Unnecessary map usage # Newly added \"C901\", # Complex name \"E501\", # Line length, as enforced by black, but black ignores comments \"E721\" # Type comparison ] # W504 is not supported by ruff and does not need to be excluded [tool.ruff.lint.per-file-ignores] \"pySDC/tutorial/step_6/C_MPI_parallelization.py\" = [\"F401\"] \"pySDC/projects/Hamiltonian/solar_system.py\" = [\"F401\"] [tool.black] line-length = 120 skip-string-normalization = true exclude = '''playgrounds/''' [tool.coverage.run] omit = ['*/pySDC/tests/*', '*/data/*', '*/pySDC/playgrounds/*', '*/pySDC/projects/deprecated/*', '*/pySDC/projects/*/tests/*'] relative_files = true concurrency = ['multiprocessing'] source = ['pySDC'] [tool.coverage.report] skip_empty = true # Regexes for lines to exclude from consideration exclude_lines = [ # Have to re-enable the standard pragma 'pragma: no cover', # Don't complain about missing debug-only code: 'def __repr__', 'if self\\.debug', # Don't complain if tests don't hit defensive assertion code: 'raise', # Don't complain if non-runnable code isn't run: 'if 0:', 'if __name__ == .__main__.:', 'pass', '@abc.abstractmethod', '__author__*', ]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/python-icat",
            "repo_link": "https://github.com/icatproject/python-icat",
            "content": {
                "codemeta": "",
                "readme": "|doi| |rtd| |pypi| .. |doi| image:: https://zenodo.org/badge/37250056.svg :target: https://zenodo.org/badge/latestdoi/37250056 .. |rtd| image:: https://img.shields.io/readthedocs/python-icat/latest :target: https://python-icat.readthedocs.io/en/latest/ :alt: Documentation build status .. |pypi| image:: https://img.shields.io/pypi/v/python-icat :target: https://pypi.org/project/python-icat/ :alt: PyPI version python-icat - Python interface to ICAT and IDS ============================================== This package provides a collection of modules for writing Python programs that access an `ICAT`_ service using the SOAP interface. It is based on Suds and extends it with ICAT specific features. Download -------- The latest release version can be found at the `release page on GitHub`__. .. __: `GitHub release`_ Documentation ------------- See the `online documentation`__. Example scripts can be found in doc/examples. This is mostly an unsorted collection of test scripts that I initially wrote for myself to try things out. Almost all scripts use example_data.yaml as input for test data. Of course for real production, the input will come from different sources, out of some workflow from the site. But this would be dynamic and site specific and thus not suitable, neither for testing nor for the inclusion into example scripts. So its easier to have just one blob of dummy input data in one single file. That is also the reason why the example scripts require PyYAML. .. __: `Read the Docs site`_ Copyright and License --------------------- Copyright 2013-2024 Helmholtz-Zentrum Berlin für Materialien und Energie GmbH Licensed under the `Apache License`_, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. .. _ICAT: https://icatproject.org/ .. _GitHub release: https://github.com/icatproject/python-icat/releases/latest .. _Read the Docs site: https://python-icat.readthedocs.io/ .. _Apache License: https://www.apache.org/licenses/LICENSE-2.0\n",
                "dependencies": "\"\"\"Python interface to ICAT and IDS This package provides a collection of modules for writing Python programs that access an `ICAT`_ service using the SOAP interface. It is based on Suds and extends it with ICAT specific features. .. _ICAT: https://icatproject.org/ \"\"\" import setuptools from setuptools import setup import setuptools.command.build_py import distutils.command.sdist import distutils.dist from distutils import log from pathlib import Path import string try: import distutils_pytest cmdclass = distutils_pytest.cmdclass except (ImportError, AttributeError): cmdclass = dict() try: import gitprops release = str(gitprops.get_last_release()) version = str(gitprops.get_version()) except (ImportError, LookupError): try: from _meta import release, version except ImportError: log.warn(\"warning: cannot determine version number\") release = version = \"UNKNOWN\" docstring = __doc__ # Enforcing of PEP 625 has been added in setuptools 69.3.0. We don't # want this, we want to keep control on the name of the sdist # ourselves. Disable it. def _fixed_get_fullname(self): return \"%s-%s\" % (self.get_name(), self.get_version()) distutils.dist.DistributionMetadata.get_fullname = _fixed_get_fullname class meta(setuptools.Command): description = \"generate meta files\" user_options = [] meta_template = ''' release = \"%(release)s\" version = \"%(version)s\" ''' def initialize_options(self): pass def finalize_options(self): pass def run(self): version = self.distribution.get_version() log.info(\"version: %s\", version) values = { 'release': release, 'version': version, } with Path(\"_meta.py\").open(\"wt\") as f: print(self.meta_template % values, file=f) class build_test(setuptools.Command): \"\"\"Copy all stuff needed for the tests (example scripts, test data) into the test directory. \"\"\" description = \"set up test environment\" user_options = [] def initialize_options(self): pass def finalize_options(self): pass def run(self): self.copy_test_scripts() self.copy_test_data() def copy_test_scripts(self): destdir = Path(\"tests\", \"scripts\") self.mkpath(str(destdir)) scripts = [] scripts += Path(\"doc\", \"examples\").glob(\"*.py\") scripts += (Path(s) for s in self.distribution.scripts) for script in scripts: dest = destdir / script.name self.copy_file(str(script), str(dest), preserve_mode=False) def copy_test_data(self): destdir = Path(\"tests\", \"data\") self.mkpath(str(destdir)) etc = Path(\"etc\") doc = Path(\"doc\") examples = doc / \"examples\" files = [] files += ( examples / f for f in (\"example_data.yaml\", \"ingest-datafiles.xml\", \"ingest-ds-params.xml\", \"ingest-sample-ds.xml\") ) files += ( examples / (\"icatdump-%s.%s\" % (ver, ext)) for ver in (\"4.4\", \"4.7\", \"4.10\", \"5.0\") for ext in (\"xml\", \"yaml\") ) files += doc.glob(\"icatdata-*.xsd\") files += examples.glob(\"metadata-*.xml\") files += ( etc / f for f in (\"ingest-10.xsd\", \"ingest-11.xsd\", \"ingest.xslt\") ) for f in files: dest = destdir / f.name self.copy_file(str(f), str(dest), preserve_mode=False) # Note: Do not use setuptools for making the source distribution, # rather use the good old distutils instead. # Rationale: https://rhodesmill.org/brandon/2009/eby-magic/ class sdist(distutils.command.sdist.sdist): def run(self): self.run_command('meta') super().run() subst = { \"version\": self.distribution.get_version(), \"url\": self.distribution.get_url(), \"description\": docstring.split(\"\\n\")[0], \"long_description\": docstring.split(\"\\n\", maxsplit=2)[2].strip(), } for spec in Path().glob(\"*.spec\"): with spec.open('rt') as inf: with Path(self.dist_dir, spec).open('wt') as outf: outf.write(string.Template(inf.read()).substitute(subst)) class build_py(setuptools.command.build_py.build_py): def run(self): self.run_command('meta') super().run() package = self.distribution.packages[0].split('.') outfile = self.get_module_outfile(self.build_lib, package, \"_meta\") self.copy_file(\"_meta.py\", outfile, preserve_mode=0) # There are several forks of the original suds package around, most of # them short-lived. Two of them have been evaluated with python-icat # and found to work: suds-jurko and the more recent suds-community. # The latter has been renamed to suds. We don't want to force to use # one particular suds clone. Therefore, we first try if (any clone # of) suds is already installed and only add suds to install_requires # if not. requires = [\"setuptools\", \"lxml\", \"packaging\"] try: import suds except ImportError: requires.append(\"suds\") with Path(\"README.rst\").open(\"rt\", encoding=\"utf8\") as f: readme = f.read() setup( name = \"python-icat\", version = version, description = docstring.split(\"\\n\")[0], long_description = readme, long_description_content_type = \"text/x-rst\", url = \"https://github.com/icatproject/python-icat\", author = \"Rolf Krahl\", author_email = \"rolf.krahl@helmholtz-berlin.de\", license = \"Apache-2.0\", classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Intended Audience :: Developers\", \"License :: OSI Approved :: Apache Software License\", \"Operating System :: OS Independent\", \"Programming Language :: Python\", \"Programming Language :: Python :: 3.4\", \"Programming Language :: Python :: 3.5\", \"Programming Language :: Python :: 3.6\", \"Programming Language :: Python :: 3.7\", \"Programming Language :: Python :: 3.8\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Programming Language :: Python :: 3.13\", \"Topic :: Software Development :: Libraries :: Python Modules\", ], project_urls = dict( Documentation=\"https://python-icat.readthedocs.io/\", Source=\"https://github.com/icatproject/python-icat/\", Download=(\"https://github.com/icatproject/python-icat/releases/%s/\" % release), Changes=(\"https://python-icat.readthedocs.io/en/stable\" \"/changelog.html#changes-%s\" % release.replace('.', '-')), ), packages = [\"icat\"], package_dir = {\"\": \"src\"}, python_requires = \">=3.4\", install_requires = requires, scripts = [ \"src/scripts/icatdump.py\", \"src/scripts/icatingest.py\", \"src/scripts/wipeicat.py\" ], cmdclass = dict(cmdclass, meta=meta, build_py=build_py, build_test=build_test, sdist=sdist), )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/pyxmake",
            "repo_link": "https://gitlab.com/dlr-sy/pyxmake",
            "content": {
                "codemeta": "",
                "readme": "[![doi](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.13352143-red.svg)](https://zenodo.org/records/13352143) [![doc](https://img.shields.io/static/v1?label=Pages&message=User%20Guide&color=blue&style=flat&logo=gitlab)](https://dlr-sy.gitlab.io/pyxmake) [![PyPi](https://img.shields.io/pypi/v/pyx-core?label=PyPi)](https://pypi.org/project/pyx-core) # PyXMake > This subpackage belongs to [PyXMake](https://gitlab.com/dlr-sy/pyxmake) and contains all core functionalities. It is installed automatically with the parent project. However, it is also separately available as a build system dependency. Please refer to the linked [repository](https://gitlab.com/dlr-sy/pyxmake) for documentation and application examples. If you came across the project via the [GitLab Catalog Explorer](https://gitlab.com/explore/catalog), you can jump directly to the [CI/CD Catalog section](#cicd-catalog) ## Downloading Use GIT to get the latest code base. From the command line, use ``` git clone https://gitlab.dlr.de/dlr-sy/pyxmake pyxmake ``` If you check out the repository for the first time, you have to initialize all submodule dependencies first. Execute the following from within the repository. ``` git submodule update --init --recursive ``` To fetch all required metadata for each submodule, use ``` git submodule foreach --recursive 'git checkout $(git config -f $toplevel/.gitmodules submodule.$name.branch || echo master) || git checkout main' ``` To update all refererenced submodules to the latest production level, use ``` git submodule foreach --recursive 'git pull origin $(git config -f $toplevel/.gitmodules submodule.$name.branch || echo master) || git pull origin main' ``` ## Installation PyXMake can be installed from source using [poetry](https://python-poetry.org). If you don't have [poetry](https://python-poetry.org) installed, run ``` pip install poetry --pre --upgrade ``` to install the latest version of [poetry](https://python-poetry.org) within your python environment. Use ``` poetry update ``` to update all dependencies in the lock file or directly execute ``` poetry install ``` to install all dependencies from the lock file. Last, you should be able to import PyXMake as a python package. ```python import PyXMake ``` ## CI/CD Catalog [PyXMake](https://gitlab.com/dlr-sy/pyxmake) provides its software development and deployment capabilities as pre-built [CI/CD components](https://docs.gitlab.com/ee/ci/components/). The individual components can be integrated either directly via the CI/CD Catalog or classically via the \"include\" syntax. The components are divided into three groups: * General * Python * Docker To include a given component, use ```yaml include: - component: $CI_SERVER_FQDN/<groupname>/PyXMake/<component>@<tag> ``` where *\\<groupname\\>* depends on the local Gitlab repository, *\\<component\\>* refers to the name of the component under *templates\\/*\\* and *\\<tag\\>* to a specific version. *\\<tag\\>* can also be set to *master* to always use the latest yet potentially unstable version. ## Contact * [Marc Garbade](mailto:marc.garbade@dlr.de)\n",
                "dependencies": "# TOML file for PyXMake as a build system. # # @note: TOML file # Created on 20.09.2021 # # @version: 1.0 # ---------------------------------------------------------------------------------------------- # @requires: # - # # @change: # - # # @author: garb_ma [DLR-SY,STM Braunschweig] # ---------------------------------------------------------------------------------------------- [build-system] requires = [\"poetry-core>=1.0\"] build-backend = \"poetry.core.masonry.api\" [tool.poetry] name = \"pyx-core\" version = \"0.0.0dev\" description = \"All core functionalities for using PyXMake as a build system\" authors = [\"Garbade, Marc <marc.garbade@dlr.de>\"] license = \"MIT\" readme = \"README.md\" packages = [{include=\"PyXMake\", from=\"src\"}] include = [\"src/PyXMake/*\"] exclude = [\"src/PyXMake/API/*.py\", \"src/PyXMake/Plugin/*.py\", \"src/PyXMake/**/*.git*\"] repository = \"https://gitlab.com/dlr-sy/pyxmake\" documentation = \"https://dlr-sy.gitlab.io/pyxmake\" keywords = [\"compilation\", \"documentation\", \"packaging\"] classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Topic :: Software Development :: Build Tools\", \"Topic :: Software Development :: Libraries :: Python Modules\", \"Programming Language :: Python :: 2\", \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: MIT License\", \"Operating System :: OS Independent\" ] [tool.poetry.urls] Changelog = \"https://gitlab.com/dlr-sy/pyxmake/-/blob/master/CHANGELOG.md\" [[tool.poetry.source]] name = \"PyPI\" priority = \"primary\" [[tool.poetry.source]] name = \"dlr-sy\" url = \"https://gitlab.dlr.de/api/v4/groups/541/-/packages/pypi/simple\" priority = \"supplemental\" [tool.poetry.dependencies] python = \"~2.7 || ^3.5\" autodocsumm = [{version = \"^0.2.6\", python = \"^3.7\"}] beautifulsoup4 = [{version = \"^4.8\", python = \"~2.7 || ^3.5,<3.7\"}, {version = \">=4.11\", python = \"^3.7\"}] cmake = [{version = \"^3.26,<3.27.8\", python = \"~2.7 || ^3.5,<3.7\"}, {version = \">=3.26\", python = \"^3.7\"}] contextlib2 = [{version = \"^0.6\", python = \"~2.7 || ^3.5,<3.7\"}, {version = \">=21.6\", python = \"^3.7\"}] cryptography = [{version = \"^3.1\", python = \"~2.7 || ^3.5,<3.7\"}, {version = \">=38.0\", python = \"^3.7\"}] distlib = [{version = \">=0.3.5\"}] dnspython = [{version = \">=2.2\", python = \"^3.7\"}] fastapi = [{version = \"^0.79\", python = \"~3.7\"}, {version = \">=0.115\", python = \"^3.8\"}] future = [{version = \">=0.18\", python = \"~2.7 || ^3.5\"}] git-filter-repo = [{version = \">=2.34\", python = \"^3.7\"}] GitPython = [{version = \"~2.1\", python = \"~2.7\"}, {version = \"^3,<3.1.15\", python = \"~3.5\"}, {version = \"^3,<3.1.19\", python = \"~3.6\"}, {version = \">=3.1.24\", python = \"^3.7\"}] html2text = [{version = \"^2020.1\", python=\"~3.7\"}, {version = \">=2024.2\", python=\"^3.8\"}] httpx = [{version = \"^0.24\", python = \"~3.7\"}, {version = \">=0.26\", python = \">=3.8\"}] Jinja2 = [{version = \"^3.0\", python = \"~3.6\"}, {version = \">=3.1\", python = \"^3.7\"}] keyring = [{version = \"~18.0\", python = \"~2.7\"}, {version = \">=19,<24.1\", python = \"^3.5,<3.7\"}, {version = \">=23.0\", python = \"^3.7\"}] markupsafe = [{version = \"^2.0\", python = \"~3.6\"}, {version = \">=2.1\", python = \"^3.7\"}] numpy = [{version = \"^1.16\", python = \"~2.7 || ~3.5\"}, {version = \"^1.18\", python = \"~3.6\"}, {version = \"^1.21\", python = \"~3.7\"}, {version = \">=1.22\", python = \"^3.8\"}] packaging = [{version = \"^20.9\", python = \"~2.7 || ^3.5,<3.7\"}, {version = \">=21.0\", python = \"^3.7\"}] paramiko = [{version = \">=2.9\", python = \"~2.7 || ^3.5\"}] Pillow = [{version = \"~6.2\", python = \"~2.7 || ^3.5,<3.7\"}, {version = \">=9.2\", python = \"^3.7\"}] pipreqs = [{version = \">=0.4\", python = \"~2.7 || ^3.5\"}] psutil = [{version = \">=5.8\", python = \"~2.7 || ^3.6\"}, {version = \">=5.8\", python = \"~3.5\", markers = \"sys_platform != 'windows'\"}, {version = \">=5.7\", python = \"~3.5\", markers = \"sys_platform == 'windows'\"}] pydantic = [{version = \">=1.8\", python = \"^3.7\"}] pyinstaller = [{version = \">=5.3\", python = \"^3.7,<3.12\"}] pytest = [{version = \">=7.1\", python = \"^3.7\"}] pytest-cov = [{version = \">=3,<5.0\", python = \"^3.7\"}] python-dateutil = [{version = \">=2.8\", python = \"^3.7\"}] python-multipart = [{version = \">=0.0.5\", python = \"^3.7\"}] pythonsed = [{version = \"*\", platform = \"win32\"}] pywin32 = [{version = \"228\", python = \"~2.7 || ^3.5,<3.7\", platform = \"win32\"}, {version = \"^306\", python = \"^3.7\", platform = \"win32\"}] rapidfuzz = [{version = \">=2.15,<3.3\", python = \"~3.7\"}, {version = \">=3,<4.0\", python = \"^3.8\"}] recommonmark = [{version = \">=0.7\", python = \"^3.7\"}] requests = [{version = \"^2.25\", python = \"^3.5,<3.7\"}, {version = \">=2.28\", python = \"^3.7\"}] requests-toolbelt = [{version = \">=0.8,<10\", python = \"~2.7 || ^3.5,<3.7\"}, {version = \">=0.10\", python = \"^3.7\"}] scipy = [{version = \"^1.2\", python = \"~2.7 || ~3.5\"}, {version = \"^1.5\", python = \"~3.6\"}, {version = \"^1.6\", python = \"~3.7\"}, {version = \"~1.8\", python = \"~3.8\"}, {version = \">=1.10\", python = \">=3.9,<3.13\"}] six = [{version = \">=1.15\", python = \"~2.7 || ^3.5\"}] sphinx = [{version = \"^3.0\", python = \"~3.6\"}, {version = \"^5.1\", python = \"~3.7\"}, {version = \"^6.2\", python = \"~3.8\"}, {version = \"^7.4\", python = \"~3.9\"}, {version = \">=8.1\", python = \"^3.10,<4\"}] sphinx-rtd-theme = [{version = \"^1.3\", python = \"~3.7\"}, {version = \">=3\", python = \"^3.8\"}] sphinxcontrib-bibtex = [{version = \"^1.0\", python = \"~3.6\"}, {version = \">=2.5\", python = \"^3.7\"}] sphinxemoji = [{version = \"^0.2\", python = \"^3.7,<3.9\"}, {version = \">=0.3\", python = \"^3.9\"}] starlette = [{version = \"^0.19\", python = \"~3.7\"}, {version = \"^0.43\", python = \"~3.8\"}, {version = \">=0.45\", python = \"^3.9\"}] stdlib-list = [{version = \"^0.8\", python = \"~2.7 || ^3.5,<3.7\"}, {version = \"^0.10\", python = \"^3.7,<3.9\"}, {version = \">=0.11\", python = \"^3.9\"}] setuptools = [{version = \"^39.0\", python = \"~2.7\"}, {version = \"^49.0\", python = \"~3.5\"}, {version = \"^58.0\", python = \"~3.6\"}, {version = \"^64.0\", python = \"~3.7\"}, {version = \"^70,<74\", python = \"^3.8\"}] svn = [{version = \">=1.0\", python = \"~2.7 || ^3.5\"}] tomlkit = [{version = \">=0.5,<1\", python = \"~2.7 || ^3.5,<3.7\"}, {version = \">=0.12\", python = \"^3.7\"}] uvicorn = [{version = \">=0.17\", python = \"^3.7\"}] whichcraft = [{version = \">=0.6\", python = \"~2.7 || ^3.5\"}] # All optional dependencies pyx-poetry = [{version = \">=1.19\", python = \"~2.7 || ^3.5,<3.7\", optional = true}, {version = \"*\", python = \"^3.7\", optional = true}] pyc-core = [{version = \">=1.10\", python = \"~2.7 || ^3.5,<3.7\", optional = true}, {version = \"*\", python = \"^3.7\", optional = true}] # All mandatory development dependencies [tool.poetry.group.dev.dependencies] pyx-poetry = [{git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake.git\", branch=\"pyx_poetry\", python=\"^3.7\"}] pyc-core = [{git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyCODAC.git\", python=\"^3.7\"}] [tool.poetry.group.doc.dependencies] atlassian-python-api = [{version = \">=3.14\", python = \">=3.7\"}] markdown-it-py = [{version = \"~2.2\", python=\"~3.7\"}, {version = \"^3.0\", python=\"^3.8\"}] myst-parser = [{version = \"^1,<2.0\", python=\"~3.7\"}, {version = \"^3.0\", python=\"^3.8,<3.10\"}, {version = \">=4.0\", python=\"^3.10\"}] sphinx-design = [{version = \"^0.4,<0.5\", python=\"~3.7\"}, {version = \"^0.5\", python=\"~3.8\"}, {version = \">=0.6\", python=\"^3.9\"}] sphinx-favicon = [{version = \">=1.0\", python=\"^3.7\"}] pydata-sphinx-theme = [{version = \"^0.13,<0.14\", python=\"~3.7\"}, {version = \">=0.14,<0.16\", python=\"~3.8\"}, {version = \">=0.16\", python=\"3.9\"}] [tool.poetry.extras] all = [\"pyx-poetry\",\"pyc-core\"] core = [\"pyx-poetry\"] devel = [\"pyc-core\"] [tool.poetry.scripts] poetry-auto-cli = \"PyXMake.VTL:run\" pyxmake = \"PyXMake.VTL:main\" [tool.pyxmake.archive] name = \"pyx_core-master.tar.gz\" source = \"src/PyXMake\" [tool.pyxmake.doxygen] name = \"PyXMake\" title = [\"Harmonized interfaces and workflows to selected software development tools\", \"PyXMake Reference Guide\"] format = \"Python\" source = \"src\" output = \"doc/pyx_core\" icon = \"doc/pyxmake/assets/pyxmake_logo.png\" filter = [{\"contains\" = [\"doc\",\"bin\",\"config\"]}, {\"endswith\" = [\"make\",\"scratch\",\"examples\"]}] [tool.pyxmake.sphinx] name = \"PyXMake\" source = \"doc\" output = \"doc\" file = \"index\" icon = \"doc/pyxmake/assets/favicon.ico\" theme = \"pydata_sphinx_theme\" [tool.pyxmake.coverage] name = \"PyXMake\" source = \"src\" exclude = [\"src/PyXMake/Plugin\", \"src/PyXMake/VTL\"] include = [\"example/pyx_api.py\", \"example/pyx_app.py\", \"example/pyx_cmake.py\", \"example/pyx_archive.py\", \"example/pyx_gfortran.py\", \"example/pyx_py2x.py.py\", \"example/pyx_pyreq.py\", \"example/pyx_cxx.py\", \"example/pyx_doxygen.py\", \"example/pyx_latex.py\", \"example/pyx_sphinx.py\", \"example/pyx_openapi.py\", \"example/pyx_bundle.py\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/quast",
            "repo_link": "https://github.com/ablab/quast/",
            "content": {
                "codemeta": "",
                "readme": "[![GitHub release (latest by date)](https://img.shields.io/github/v/release/ablab/quast)](https://github.com/ablab/quast/releases/) [![BioConda Install](https://img.shields.io/conda/dn/bioconda/quast.svg?style=flag&label=BioConda%20install)](https://anaconda.org/bioconda/quast) [![SourceForge Download QUAST](https://img.shields.io/sourceforge/dt/quast.svg?style=flag&label=SourceForge%20download)](https://sourceforge.net/projects/quast/files/latest/download) [![PyPI version](https://badge.fury.io/py/quast.svg)](https://badge.fury.io/py/quast) [![GitHub Downloads](https://img.shields.io/github/downloads/ablab/quast/total.svg?style=social&logo=github&label=Download)](https://github.com/ablab/quast/releases) [![License](https://img.shields.io/badge/licence-GPLv2-blue)](https://www.gnu.org/licenses/old-licenses/gpl-2.0) <img src=\"quast_libs/html_saver/static/img/quast_logo_black.png\" width=\"300\" title=\"QUAST\"> ### Genome assembly evaluation tool QUAST stands for QUality ASsessment Tool. It evaluates genome/metagenome assemblies by computing various metrics. The current QUAST toolkit includes the general QUAST tool for genome assemblies, MetaQUAST, the extension for metagenomic datasets, QUAST-LG, the extension for large genomes (e.g., mammalians), and Icarus, the interactive visualizer for these tools. The QUAST package works both with and without reference genomes. However, it is much more informative if at least a close reference genome is provided along with the assemblies. The tool accepts multiple assemblies, thus is suitable for comparison. This README file gives a brief introduction into installation, basic usage and parsing of output of QUAST. A much more detailed description of these and many other topics is available in the [online manual](http://quast.sf.net/manual.html). There are also many more installation methods for the latest stable release of the QUAST toolkit, all of them are discussed [here](http://quast.sf.net/install.html). For the cutting-edge version, please clone our [GitHub repo](https://github.com/ablab/quast). The [Gurevich Lab](https://helmholtz-hips.de/en/hmsb) at the [Helmholtz Institute for Pharmaceutical Research Saarland (HIPS)](https://helmholtz-hips.de/en/) currently maintains and develops the tool. For copyright information and citation instructions, please refer to [LICENSE.txt](LICENSE.txt). We warmly welcome external contributions to the QUAST project. If you would like to contribute, please review our [Contributor Covenant](CODE_OF_CONDUCT.md). #### System requirements Linux 64-bit and macOS are supported. For the main pipeline: - Python3 (3.3 or higher) - Perl 5.6.0 or higher - GCC 4.7 or higher - GNU make and ar - zlib development files For the optional submodules: - Time::HiRes perl module for GeneMark-ES (needed when using `--gene-finding --eukaryote`) - Java 1.8 or later for GRIDSS (needed for the structural variation detection) - R for GRIDSS (needed for the structural variation detection) Most of those tools are usually preinstalled on Linux. MacOS, however, requires to install the Command Line Tools for Xcode to make them available. QUAST draws plots in two formats: HTML and PDF. If you need the PDF versions, make sure that you have installed [Matplotlib](https://matplotlib.org/). We recommend to use Matplotlib version 1.1 or higher. QUAST is fully tested with Matplotlib v.1.3.1. Installation on Ubuntu (tested on Ubuntu 20.04): sudo apt-get update && sudo apt-get install -y pkg-config libfreetype6-dev libpng-dev python3-matplotlib #### Installation QUAST automatically compiles all its sub-parts when needed (on the first use). Thus, installation is not required. However, if you want to precompile everything and add quast.py to your `PATH`, you may choose either: Basic installation (about 120 MB): ./setup.py install Full installation (about 540 MB, includes (1) tools for SV detection based on read pairs, which is used for more precise misassembly detection, (2) and tools/data for reference genome detection in metagenomic datasets): ./setup.py install_full The default installation location is `/usr/local/bin/` for the executable scripts, and `/usr/local/lib/` for the python modules and auxiliary files. If you are getting a permission error during the installation, consider running setup.py with `sudo`, or create a virtual python environment and [install into it](http://docs.python-guide.org/en/latest/dev/virtualenvs/). Alternatively, you may use old-style installation scripts (`./install.sh` or `./install_full.sh`), which build QUAST package inplace. #### Usage ./quast.py test_data/contigs_1.fasta \\ test_data/contigs_2.fasta \\ -r test_data/reference.fasta.gz \\ -g test_data/genes.txt \\ -1 test_data/reads1.fastq.gz -2 test_data/reads2.fastq.gz \\ -o quast_test_output #### Output report.txt summary table report.tsv tab-separated version, for parsing, or for spreadsheets (Google Docs, Excel, etc) report.tex Latex version report.pdf PDF version, includes all tables and plots for some statistics report.html everything in an interactive HTML file icarus.html Icarus main menu with links to interactive viewers contigs_reports/ [only if a reference genome is provided] misassemblies_report detailed report on misassemblies unaligned_report detailed report on unaligned and partially unaligned contigs k_mer_stats/ [only if --k-mer-stats is specified] kmers_report detailed report on k-mer-based metrics reads_stats/ [only if reads are provided] reads_report detailed report on mapped reads statistics Metrics based only on contigs: * Number of large contigs (i.e., longer than 500 bp) and total length of them. * Length of the largest contig. * N50 (length of a contig, such that all the contigs of at least the same length together cover at least 50% of the assembly). * Number of predicted genes, discovered either by GeneMark.hmm (for prokaryotes), GeneMark-ES or GlimmerHMM (for eukaryotes), or MetaGeneMark (for metagenomes). When a reference is given: * Numbers of misassemblies of different kinds (inversions, relocations, translocations, interspecies translocations (metaQUAST only) or local). * Number and total length of unaligned contigs. * Numbers of mismatches and indels, over the assembly and per 100 kb. * Genome fraction %, assembled part of the reference. * Duplication ratio, the total number of aligned bases in the assembly divided by the total number of those in the reference. If the assembly contains many contigs that cover the same regions, its duplication ratio will significantly exceed 1. This occurs due to multiple reasons, including overestimating repeat multiplicities and overlaps between contigs. * Number of genes in the assembly, completely or partially covered, based on a user-provided list of gene positions in the reference. * NGA50, a reference-aware version of N50 metric. It is calculated using aligned blocks instead of contigs. Such blocks are obtained after removing unaligned regions, and then splitting contigs at misassembly breakpoints. Thus, NGA50 is the length of a block, such that all the blocks of at least the same length together cover at least 50% of the reference. #### Contact & Info * Support email: [alexey.gurevich@helmholtz-hips.de](alexey.gurevich@helmholtz-hips.de) * Issue tracker: [https://github.com/ablab/quast/issues](https://github.com/ablab/quast/issues) * Website: [http://quast.sf.net](http://quast.sf.net) * Latest news: [https://x.com/quast_bioinf](https://x.com/quast_bioinf)\n",
                "dependencies": "#!/bin/bash ############################################################################ # Copyright (c) 2022-2024 Helmholtz Institute for Pharmaceutical Research Saarland (HIPS), HZI # Copyright (c) 2015-2022 Saint Petersburg State University # Copyright (c) 2011-2015 Saint Petersburg Academic University # All Rights Reserved # See file LICENSE for details. ############################################################################ # installs general QUAST pipeline and general MetaQUAST pipeline. # MetaQUAST for de novo datasets (without references) will NOT be installed. quast_home=$(dirname \"$0\") stdout_log_fname=$quast_home/install_log.stdout echo \"Starting QUAST test... (stdout redirected to $stdout_log_fname)\" echo \"\" > $stdout_log_fname echo \"Starting QUAST test\" >> $stdout_log_fname $quast_home/quast.py --test >> $stdout_log_fname return_code=$? if [ $return_code -ne 0 ]; then echo 'ERROR! QUAST TEST FAILED!' exit 1 fi echo \"Starting MetaQUAST test... (stdout redirected to $stdout_log_fname)\" echo \"\" >> $stdout_log_fname echo \"Starting MetaQUAST test\" >> $stdout_log_fname $quast_home/metaquast.py --test >> $stdout_log_fname return_code=$? if [ $return_code -ne 0 ]; then echo 'ERROR! METAQUAST TEST FAILED!' exit 1 fi echo 'QUAST INSTALLED SUCCESSFULLY!' echo 'You can install full version of QUAST with ./install_full.sh (see manual.html)' exit 0\n#!/usr/bin/env python ############################################################################ # Copyright (c) 2022-2024 Helmholtz Institute for Pharmaceutical Research Saarland (HIPS), HZI # Copyright (c) 2015-2022 Saint Petersburg State University # Copyright (c) 2011-2015 Saint Petersburg Academic University # All Rights Reserved # See file LICENSE for details. ############################################################################ from __future__ import with_statement import os import sys from glob import glob from os.path import join, isfile, abspath, dirname, isdir import shutil from quast_libs import qconfig qconfig.check_python_version() from quast_libs import qutils from quast_libs.log import get_logger logger = get_logger(qconfig.LOGGER_DEFAULT_NAME) logger.set_up_console_handler(debug=True) try: from setuptools import setup, find_packages except: logger.error('setuptools is not installed or outdated!\\n\\n' 'You can install or update setuptools using\\n' 'pip install --upgrade setuptools (if you have pip)\\n' 'or\\n' 'sudo apt-get install python-setuptools (on Ubuntu)\\n' '\\n' 'You may also use old-style installation scripts: ./install.sh or ./install_full.sh', exit_with_code=1) from quast_libs.glimmer import compile_glimmer from quast_libs.run_busco import download_augustus, download_all_db from quast_libs.search_references_meta import download_blast_binaries, download_blastdb from quast_libs.ca_utils.misc import compile_aligner from quast_libs.ra_utils.misc import compile_reads_analyzer_tools, compile_bwa, compile_bedtools, download_gridss name = 'quast' quast_package = qconfig.PACKAGE_NAME args = sys.argv[1:] def cmd_in(cmds): return any(c in args for c in cmds) if abspath(dirname(__file__)) != abspath(os.getcwd()): logger.error('Please change to ' + dirname(__file__) + ' before running setup.py') sys.exit() if cmd_in(['clean', 'sdist']): logger.info('Cleaning up binary files...') compile_aligner(logger, only_clean=True) compile_glimmer(logger, only_clean=True) compile_bwa(logger, only_clean=True) compile_bedtools(logger, only_clean=True) for fpath in [fn for fn in glob(join(quast_package, '*.pyc'))]: os.remove(fpath) for fpath in [fn for fn in glob(join(quast_package, 'html_saver', '*.pyc'))]: os.remove(fpath) for fpath in [fn for fn in glob(join(quast_package, 'site_packages', '*', '*.pyc'))]: os.remove(fpath) if cmd_in(['clean']): if isdir('build'): shutil.rmtree('build') if isdir('dist'): shutil.rmtree('dist') if isdir(name + '.egg-info'): shutil.rmtree(name + '.egg-info') download_gridss(logger, only_clean=True) download_blast_binaries(logger, only_clean=True) download_blastdb(logger, only_clean=True) if qconfig.platform_name != 'macosx': download_augustus(logger, only_clean=True) download_all_db(logger, only_clean=True) logger.info('Done.') sys.exit() if cmd_in(['test']): ret_code = os.system('quast.py --test') sys.exit(ret_code) def write_version_py(): version_py = os.path.join(os.path.dirname(__file__), quast_package, 'version.py') with open('VERSION.txt') as f: v = f.read().strip().split('\\n')[0] try: import subprocess git_revision = subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], stderr=open(os.devnull, 'w')).rstrip() except: git_revision = '' pass if not isinstance(git_revision, str): git_revision = git_revision.decode('utf-8') with open(version_py, 'w') as f: f.write(( '# Do not edit this file, pipeline versioning is governed by git tags\\n'+ '__version__ = \\'' + v + '\\'\\n' + '__git_revision__ = \\'%s\\'' % git_revision)) return v version = write_version_py() if cmd_in(['tag']): cmdl = 'git tag -a %s -m \"Version %s\" && git push --tags' % (version, version) os.system(cmdl) sys.exit() if cmd_in(['publish']): # make sure you updated pip and installed twine `pip install -U pip setuptools twine` cmdl = 'git clean -dfx && python setup.py sdist && twine upload dist/*' os.system(cmdl) sys.exit() def find_package_files(dirpath, package=quast_package): paths = [] for (path, dirs, fnames) in os.walk(join(package, dirpath)): for fname in fnames: paths.append(qutils.relpath(join(path, fname), package)) return paths install_full = False if cmd_in(['install_full']): install_full = True args2 = [] for a_ in args: if a_ == 'install_full': args2.append('install') else: args2.append(a_) args = args2 modules_failed_to_install = [] if cmd_in(['install', 'develop', 'build', 'build_ext']): try: import matplotlib except ImportError: try: pip.main(['install', 'matplotlib']) except: logger.warning('Cannot install matplotlib. Static plots will not be drawn (however, HTML will be)') logger.info('* Compiling aligner *') if not compile_aligner(logger): modules_failed_to_install.append('Contigs aligners for reference-based evaluation (affects -R and many other options)') logger.info('* Compiling Glimmer *') if not compile_glimmer(logger): modules_failed_to_install.append('Glimmer gene-finding tool (affects --glimmer option)') logger.info('* Compiling read analysis tools *') if not compile_reads_analyzer_tools(logger): modules_failed_to_install.append('Read analysis tools (affects -1/--reads1 and -2/--reads2 options)') if install_full: logger.info('* Downloading GRIDSS *') if not download_gridss(logger): modules_failed_to_install.append('GRIDSS (affects -1/--reads1 and -2/--reads2 options)') logger.info('* Downloading BLAST *') if not download_blast_binaries(logger): modules_failed_to_install.append('BLAST (affects metaquast.py in without references mode and --find-conserved-genes option)') logger.info('* Downloading SILVA 16S rRNA gene database *') if not download_blastdb(logger): modules_failed_to_install.append('SILVA 16S rRNA gene database (affects metaquast.py in without references mode)') if qconfig.platform_name != 'macosx': logger.info('* Downloading and compiling Augustus *') if not download_augustus(logger): modules_failed_to_install.append('Augustus (affects --find-conserved-genes option)') logger.info('* Downloading BUSCO databases *') if not download_all_db(logger): modules_failed_to_install.append('BUSCO databases (affects --find-conserved-genes option)') else: logger.notice('* BUSCO dependecies will not be installed (unavailable in OS X) *') logger.info('') if qconfig.platform_name == 'macosx': sambamba_files = [join('sambamba', 'sambamba_osx')] else: sambamba_files = [join('sambamba', 'sambamba_linux')] minimap_files = find_package_files('minimap2') bwa_files = [ join('bwa', fp) for fp in os.listdir(join(quast_package, 'bwa')) if isfile(join(quast_package, 'bwa', fp)) and fp.startswith('bwa')] bedtools_files = [join('bedtools', 'bin', '*')] full_install_tools = ( find_package_files('gridss') + find_package_files('blast') + [join(quast_package, 'busco', 'hmmsearch')] ) setup( name=name, version=version, author='Alexey Gurevich, Vladislav Saveliev, Alla Mikheenko, and others', author_email='alexey.gurevich@helmholtz-hips.de', description='Genome assembly evaluation tool', long_description='''QUAST evaluates genome assemblies. It works both with and without reference genomes. The tool accepts multiple assemblies, thus is suitable for comparison.''', keywords=['bioinformatics', 'genome assembly', 'metagenome assembly', 'visualization'], url='http://quast.sf.net', download_url='https://sourceforge.net/projects/quast/files', platforms=['Linux', 'OS X'], license='GPLv2', packages=find_packages(), package_data={ quast_package: find_package_files('test_data', package='') + [ 'README.md', 'CHANGES.txt', 'VERSION.txt', 'LICENSE.txt', 'manual.html', ] + find_package_files('html_saver') + minimap_files + find_package_files('genemark/' + qconfig.platform_name) + find_package_files('genemark-es/' + qconfig.platform_name) + find_package_files('genemark-es/lib') + find_package_files('glimmer') + bwa_files + bedtools_files + sambamba_files + (full_install_tools if install_full else []) }, include_package_data=True, zip_safe=False, scripts=['quast.py', 'metaquast.py', 'icarus.py', 'quast-lg.py'], install_requires=[ 'joblib', 'simplejson', ], classifiers=[ 'Environment :: Console', 'Environment :: Web Environment', 'Intended Audience :: Science/Research', 'License :: OSI Approved :: GNU General Public License v2 (GPLv2)', 'Natural Language :: English', 'Operating System :: MacOS :: MacOS X', 'Operating System :: POSIX', 'Operating System :: Unix', 'Programming Language :: Python', 'Programming Language :: JavaScript', 'Topic :: Scientific/Engineering', 'Topic :: Scientific/Engineering :: Bio-Informatics', 'Topic :: Scientific/Engineering :: Visualization', ], script_args=args, ) if cmd_in(['install']): not_installed_message = '' if modules_failed_to_install: not_installed_message = 'WARNING: some modules were not installed properly and\\n' \\ 'QUAST functionality will be restricted!\\n' \\ 'The full list of malformed modules and affected options:\\n' not_installed_message += \"\\n\".join(map(lambda x: \" * \" + x, modules_failed_to_install)) not_installed_message += \"\\n\\n\" if not install_full: logger.info(''' ---------------------------------------------- QUAST version %s installation complete. %sPlease run ./setup.py test to verify installation. For help in running QUAST, please see the documentation available at quast.sf.net/manual.html, or run quast.py --help Usage: $ quast.py test_data/contigs_1.fasta \\\\ test_data/contigs_2.fasta \\\\ -r test_data/reference.fasta.gz \\\\ -g test_data/genes.txt \\\\ -o quast_test_output ----------------------------------------------''' % (str(version), not_installed_message)) else: logger.info(''' ---------------------------------------------- QUAST version %s installation complete. The full package is installed, with the features for reference sequence detection in MetaQUAST, and structural variant detection for misassembly events refinement. %sPlease run ./setup.py test to verify installation. For help in running QUAST, please see the documentation available at quast.sf.net/manual.html, or run quast.py --help Usage: $ quast.py test_data/contigs_1.fasta \\\\ test_data/contigs_2.fasta \\\\ -r test_data/reference.fasta.gz \\\\ -g test_data/genes.txt \\\\ -1 test_data/reads1.fastq.gz -2 test_data/reads2.fastq.gz \\\\ -o quast_test_output ----------------------------------------------''' % (str(version), not_installed_message))\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/radiative-forcing-of-hypersonic-aircraft-trajectories",
            "repo_link": "https://github.com/johannespletzer/rf-of-hypersonic-trajectories/",
            "content": {
                "codemeta": "",
                "readme": "[![DOI](https://zenodo.org/badge/518852238.svg)](https://zenodo.org/badge/latestdoi/518852238) # Radiative forcing of hypersonic aircraft emission inventories The software quantifies climate impact of hypersonic aircraft emission inventories as a number and within seconds instead of very long numerical simulations that produce Petabytes of data. The input requires water vapor, hydrogen and nitrogen oxide emission data along flight trajectories. The repository provides a Python package, examples and an executable to calculate the climate impact (stratosphere adjusted radiative forcing) of hypersonic aircraft emission inventories. The radiative forcing of water vapour changes and ozone changes are calculated on the basis of water vapour, hydrogen and nitrogen oxide emissions. The current version is able to read in mat- and nc-files. NetCDF read in is currently optimised for data published online, e.g. for the aircraft design [STRATOFLY-MR3](https://zenodo.org/records/10818082). # Limitations Interpolation (30-38 km) and extrapolation surface-30 km are used. It is recommended to note the following: - The atmospheric and radiative sensitivites are based on results from [Pletzer et al (2024)](https://acp.copernicus.org/articles/24/1743/2024/). The atmospheric composition of the numerical climate model is based on surface emission inventories for 2050. - The class includes a function (`drop_vertical_levels()`) that drops emission in the troposphere or below specified altitude levels and excludes it from the climate calculation. Its use is strongly recommended as long as sensitivities are not yet extended to altitudes below 30 km. - The climate impact of emission inventories where the average flight altitude does not correspond to the typical hypersonic flight altitudes (about 24-40 km) should not be estimated. - Meaningful results can be expected for the radiative effect of water vapour changes due to water vapour emissions. This explicitly excludes the radiative effect of water vapour changes due to hydrogen and nitrogen oxide emissions. - Meaningful results can be expected for the radiative effect of ozone changes due to water vapour, hydrogen and nitrogen oxide emissions. Please keep these limitations in mind when using the software. # Getting started The software can be installed via ```bash pip install -e . ``` The repo contains two example notebooks for processing of emission inventories in mat- and nc-format. Otherwise, the user can various routines, e.g. one that reads all emission inventory files within the folder and returns the calculated radiative forcing in an xlsx file. # Acknowledgements Daniel Bodmer contributed with validation of model results by offering current state of the art hypersonic aircraft emission inventories on trajectory and route network level: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10818082.svg )](https://doi.org/10.5281/zenodo.10818082 )\n",
                "dependencies": "[build-system] requires = [\"setuptools>=61.0\"] build-backend = \"setuptools.build_meta\" [project] name = \"hypersonic-effects\" version = \"2.0\" description = \"Calculate the radiative forcing of hypersonic aircraft trajectories.\" readme = \"README.md\" requires-python = \">=3.7\" dependencies = [ \"numpy\", \"pandas\", \"xarray\", \"scipy\", \"xlsxwriter\", \"netcdf4\", \"aerocalc3\", \"pytest\", \"build\", \"twine\", \"ruff\" ] [project.urls] \"Homepage\" = \"https://github.com/johannespletzer/rf-of-hypersonic-aircraft-emissions\" \"Repository\" = \"https://github.com/johannespletzer/rf-of-hypersonic-aircraft-emissions.git\" [tool.setuptools] package-dir = {\"\" = \"package\"} [tool.setuptools.packages.find] where = [\"package\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/radplanbio",
            "repo_link": "https://github.com/ddRPB/rpb",
            "content": {
                "codemeta": "",
                "readme": "# ddRPB ## Radiotherapy clinical research IT infrastructure RadiationDosePlan- Image/Biomarker-Outcome-platform (RPB) is a collection of open source software systems integrated via portal to deliver a core software infrastructure necessary to support the operation of non-commercial trials unit. The RPB platform is a web-based solution which supports collection and exchange of radiotherapy specific research data in large scale multi-centre clinical and pre-clinical studies. It delivers a study management and electronic data capture system with special extensions dedicated to secure upload of medical imaging and treatment plans in DICOM format. These tools allow the trial personnel to handle multi-modal data conduction activities that yield to finalised patient cohort datasets. ## Platform features * Managing multi-centre clinical trials * Patient identity management and pseudonymisation * Study subject randomisation (permuted block + strata) * Electronic case report forms (eCRFs) for clinical data collection * Patient reported outcomes (ePRO) * Linked medical imaging (DICOM) and treatment plans (DICOM-RT) * DICOM data de-identification and RTSTRUCT ROI naming harmonisation * DICOM viewer with DICOM-RT plugin * WebDAV access to DICOM data collected in particular project * Storage for laboratory assays and other tabulated data ## Acknowledgment For people who do use the RPB software platform as managed or self-operated solution supporting the conduction of their trials, when it is possible, we would appreciate your acknowledgement in publications and citation of the relevant papers: * \"This work was conducted in part using the RadPlanBio software platform.\" ``` @article{skripcak_toward_2016, title = {Toward {Distributed} {Conduction} of {Large}-{Scale} {Studies} in {Radiation} {Therapy} and {Oncology}: {Open}-{Source} {System} {Integration} {Approach}}, volume = {20}, issn = {2168-2194, 2168-2208}, shorttitle = {Toward {Distributed} {Conduction} of {Large}-{Scale} {Studies} in {Radiation} {Therapy} and {Oncology}}, url = {http://ieeexplore.ieee.org/document/7138574/}, doi = {10.1109/JBHI.2015.2450833}, number = {5}, urldate = {2017-12-08}, journal = {IEEE Journal of Biomedical and Health Informatics}, author = {Skripcak, Tomas and Just, Uwe and Simon, Monique and Buttner, Daniel and Luhr, Armin and Baumann, Michael and Krause, Mechthild}, month = sep, year = {2016}, pages = {1397--1403} } ``` Note: In case RPB managed service utilisation (such as dedicated Institute/Department operating RPB platform instance used by others) additional acknowledgements/citations usually defined by service provider may apply.\n",
                "dependencies": "<?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\" > <modelVersion>4.0.0</modelVersion> <groupId>de.dktk.dd.rpb</groupId> <artifactId>rpb-parent</artifactId> <packaging>pom</packaging> <version>1.0.0.14</version> <name>radplanbio</name> <description>RadPlanBio Parent Project</description> <properties> <!-- System --> <jdk.version>1.8</jdk.version> <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding> <!-- Spring --> <spring.version>4.3.18.RELEASE</spring.version><!--old 4.1.6.RELEASE new 4.3.18.RELEASE--> <spring-security.version>4.1.5.RELEASE</spring-security.version><!--old 3.2.7.RELEASE new 4.1.1.RELEASE--> <!-- Testing/Mocking --> <powermock.version>2.0.2</powermock.version> </properties> <modules> <module>radplanbio-core</module> <module>radplanbio-portal</module> <module>radplanbio-participate</module> </modules> <build> <pluginManagement> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>2.5.1</version> <configuration> <source>${jdk.version}</source> <target>${jdk.version}</target> </configuration> </plugin> </plugins> </pluginManagement> </build> <dependencies> <!-- Testing --> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.12</version> <scope>test</scope> </dependency> <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-core</artifactId> <version>2.23.4</version> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-module-junit4</artifactId> <version>${powermock.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-module-junit4-rule</artifactId> <version>${powermock.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-api-mockito2</artifactId> <version>${powermock.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-classloading-objenesis</artifactId> <version>${powermock.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.easytesting</groupId> <artifactId>fest-assert</artifactId> <version>1.4</version> <scope>test</scope> </dependency> <dependency> <groupId>org.springframework</groupId> <artifactId>spring-test</artifactId> <version>${spring.version}</version> <scope>test</scope> </dependency> <dependency> <groupId>org.seleniumhq.selenium</groupId> <artifactId>selenium-java</artifactId> <version>2.29.1</version> <scope>test</scope> </dependency> <!-- read JSON data for tests from file--> <dependency> <groupId>com.googlecode.json-simple</groupId> <artifactId>json-simple</artifactId> <version>1.1.1</version> <scope>test</scope> </dependency> <dependency> <groupId>org.springframework.ws</groupId> <artifactId>spring-ws-core</artifactId> <version>2.0.0.RELEASE</version> </dependency> <dependency> <groupId>commons-cli</groupId> <artifactId>commons-cli</artifactId> <version>1.0</version> </dependency> </dependencies> </project>\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rafcon",
            "repo_link": "https://github.com/DLR-RM/RAFCON",
            "content": {
                "codemeta": "",
                "readme": "RAFCON ====== .. figure:: documents/assets/Screenshot_Drill_Skill_Scaled.png :figwidth: 100% :width: 800px :align: left :alt: Screenshot showing RAFCON with a big state machine :target: documents/assets/Screenshot_Drill_Skill_Scaled.png?raw=true * Documentation: Hosted on `Read the Docs <http://rafcon.readthedocs.io/en/latest/>`_ * Homepage: `DLR-RM.github.io/RAFCON/ <https://dlr-rm.github.io/RAFCON/>`_ * License: `EPL <https://github.com/DLR-RM/RAFCON/blob/master/LICENSE>`_ * Cheatsheet: `Download Cheatsheet <doc/_static/rafcon_cheatsheet.pdf>`_ Develop your robotic tasks using an intuitive graphical user interface ---------------------------------------------------------------------- RAFCON uses hierarchical state machines, featuring concurrent state execution, to represent robot programs. It ships with a graphical user interface supporting the creation of state machines and contains IDE like debugging mechanisms. Alternatively, state machines can programmatically be generated using RAFCON's API. Universal application RAFCON is written in Python, can be extended with plugins and is hard- and middleware independent. Visual programming The sophisticated graphical editor can be used for the creation, execution and debugging of state machines. Collaborative working Share and reuse your state machines in form of libraries, stored as JSON strings in text files. .. figure:: https://raw.githubusercontent.com/DLR-RM/RAFCON/master/documents/assets/RAFCON-sm-creation-preview.gif :figwidth: 100% :width: 570px :align: left :alt: Example on how to create a simple state machine Installation preparations ------------------------- Before installing RAFCON, Python >=3.7, pip and setuptools are required on your system. Most of the other dependencies are automatically resolved by pip/setuptools, but not all of them. Those need be be installed manually, too: Installation requirements ^^^^^^^^^^^^^^^^^^^^^^^^^ .. code-block:: bash sudo apt-get install python-dev python-pip build-essential glade python-gi-cairo sudo -H pip install --upgrade pip sudo -H pip install --upgrade setuptools General requirements ^^^^^^^^^^^^^^^^^^^^ * Python >=3.7 * pip (recent version required: v23 known to be working) * pdm (recent version required: v2.9.3 known to be working) Installing RAFCON ----------------- .. code-block:: bash pip install rafcon --user The ``--user`` flag is optional. If not set, RAFCON is installed globally (in this case you normaly need to have root privileges). If during the installation the error ``ImportError: No module named cairo`` occurs, please install pycairo directly via: .. code-block:: bash pip install --user \"pycairo==1.19.1\" Of course you can also directly use the RAFCON sources from GitHub. .. code-block:: bash cd /install/directory git clone https://github.com/DLR-RM/RAFCON rafcon Start RAFCON ------------ No matter which installation option you choose, RAFCON can be started from any location using (make sure ``/usr/local/bin`` or ``~/.local/bin`` is in your ``PATH`` environment variable): .. code-block:: bash rafcon On a multi-python setup start rafcon using: .. code-block:: bash python<your-version> -m rafcon Uninstallation -------------- If you want to uninstall RAFCON, all you need to do is call .. code-block:: bash pip uninstall rafcon\n",
                "dependencies": "[project] name = \"rafcon\" version = \"2.2.1\" # Handled by bump2version description = \"Develop your robotic tasks with hierarchical state machines using an intuitive graphical user interface\" keywords = [\"state machine\", \"robotic\", \"FSM\", \"development\", \"GUI\", \"visual programming\"] readme = \"README.rst\" authors = [ {name = \"Sebastian Brunner\", email = \"sebastian.brunner@dlr.de\"}, {name = \"Rico Belder\", email = \"rico.belder@dlr.de\"}, {name = \"Franz Steinmetz\", email = \"franz.steinmetz@dlr.de\"} ] classifiers = [ \"Development Status :: 4 - Beta\", \"Environment :: Console\", \"Environment :: X11 Applications :: GTK\", \"Framework :: Robot Framework\", \"Intended Audience :: Developers\", \"Intended Audience :: Education\", \"Intended Audience :: Manufacturing\", \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: Eclipse Public License 1.0 (EPL-1.0)\", \"Natural Language :: English\", \"Operating System :: Unix\", \"Programming Language :: Python :: 3.6\", \"Topic :: Scientific/Engineering\", \"Topic :: Software Development\", \"Topic :: Utilities\", ] dependencies = [ \"PyGObject( >=3.42.0,<4.0.0)\", \"wrapt (>=1.14.0,<2.0.0)\", \"astroid (>=3.0.0,<4.0.0) ; python_version >= '3.11'\", \"astroid (>=2.15.8,<3.0.0) ; python_version < '3.11'\", \"gaphas (>=2.1.0,<2.2.0)\", \"jsonconversion (>=1.1.0,<2.0.0)\", \"psutil (>=5.0.0,<6.0.0)\", \"pycairo (>=1.11.0,<2.0.0)\", \"pylint (>=3.0.0,<4.0.0) ; python_version >= '3.11'\", \"pylint (>=2.11.0,<3.0.0) ; python_version < '3.11'\", \"simplegeneric (>=0.5.0,<1.0.0)\", \"numpy (>=2.0,<3.0) ; python_version >= '3.11'\", \"numpy (>=1.26,<2.0) ; python_version < '3.11'\", \"pandas (>=1.2.0,<2.0.0)\", \"yaml_configuration (>=0.2.5,<0.3.0)\", ] license = {text = \"EPL\"} requires-python = \">=3.9\" [project.urls] Homepage = \"https://github.com/DLR-RM/RAFCON\" [tool.pdm.dev-dependencies] dev = [ \"attrs >=22.2.0\", \"graphviz >=0.18.2\", \"matplotlib (>=3.10,<4.0) ; python_version >= '3.11'\", \"matplotlib (>=3.9,<3.10) ; python_version < '3.11'\", \"monitoring >=0.9.12\", \"objgraph >=3.5.0\", \"profiling (>=0.1.3) ; python_version < '3.11'\", \"pyuserinput >=0.1.11\", \"pytest-faulthandler >=1.6.0\", \"pytest-mock (>=1.9.0,<3)\", \"pytest-timeout\", \"pytest (>=6.0.0,<7.0.0)\", \"libsass\", \"coverage\", \"tox >=3.28.0\", \"bump2version >=1.0.1\", ] [tool.pdm.build] package-dir = \"source\" includes = [\"source/rafcon\"] excludes = [\"source/rafcon/share/ln\", \"source/rafcon/share/themes/RAFCON/templates\", \"tests\"] [tool.pdm.scripts] pre_build.shell = \"\"\" python3 pre_build.py \"\"\" [project.scripts] rafcon_core = \"rafcon.core.start:main\" [project.gui-scripts] rafcon = \"rafcon.gui.start:main\" rafcon_execution_log_viewer = \"rafcon.gui.execution_log_viewer:main\" [build-system] requires = [\"pdm-backend\"] build-backend = \"pdm.backend\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/random-simplex",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Random-Simplex",
            "content": {
                "codemeta": "",
                "readme": "![MATLAB](https://img.shields.io/badge/MATLAB-%23D00000.svg?style=plastic&logo=mathworks&logoColor=white) ![Fortran](https://img.shields.io/badge/Fortran-%23734F96.svg?style=plastic&logo=fortran&logoColor=white) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/random-simplex?style=plastic) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808899.svg)](https://doi.org/10.5281/zenodo.12808899) # Random Simplex Matrix ## Overview This repository contains a function that utilises the Dirichlet distribution method to generate points on the `(n−1)`-dimensional simplex. The `randomSimplexMatrix.m` generates `m x n` matrices where each row is a random sample from the `(n−1)`-dimensional simplex, i.e., it produces vectors where each element is a non-negative number and the sum of all elements in each vector is 1. ### About [Mathematica Link](https://reference.wolfram.com/language/ref/Simplex.html) ## Methodology 1. **Generation of K Unit-Exponential Distributed Random Draws**: - In each row (sample), the function generates K uniform random numbers `y_i` from the open interval `(0,1]`. - These are transformed to unit-exponential distributed random numbers `x_i = -log(y_i)`. 2. **Normalization**: - Compute the sum `S` of all `x_i` values. 3. **Calculation of Simplex Coordinates**: - The coordinates `t_1, ..., t_K` of the final point on the unit simplex are computed as `t_i = x_i / S`. 4. **Output**: - Returns a matrix, where each row is a vector on the `(n-1)`-dimensional simplex. ## Some Applications #### Stability Analysis - Polynomial Stability/Stability of discrete-time control systems: > Use simplex sampling to generate coefficients for polynomials and analyse their stability by checking if all roots lie within the unit circle. - Lyapunov Functions: > Construct Lyapunov functions with randomly sampled coefficients to study the stability of equilibrium points in dynamical systems. #### Bifurcation Analysis - Parameter Space Exploration: > Investigate the behavior of dynamical systems under different parameter regimes by sampling parameters from a simplex. Identify bifurcation points where system behavior changes qualitatively. - Nonlinear Dynamics: > Model/simulate nonlinear systems to study chaos, where initial conditions or parameters are sampled from a simplex. ## Additional Scripts 1. `simplexSpace.m` > Demonstrates various plots and visualisations of simplex sampling. 2. `MultivariateND.m` > Showcases an application of simplex sampling for sampling from a multivariate normal distribution. 3. `VoronoiDiagram.m` > Visualise the Voronoi diagram of random points on a 2-dimensional simplex, divides regions based on proximity. ## Example Usage ```matlab n = 100; % Number of columns (dimensionality of simplex) m = 1500; % Number of rows (number of samples) y = randomSimplexMatrix(n, m); disp('Generated simplex matrix:'); disp(y); ```\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rankings-reloaded",
            "repo_link": "https://github.com/wiesenfa/challengeR",
            "content": {
                "codemeta": "",
                "readme": "Methods and open-source toolkit for analyzing and visualizing challenge results ================ - [Introduction](#introduction) - [Installation](#installation) - [Terms of use](#terms-of-use) - [Usage](#usage) - [Troubleshooting](#troubleshooting) - [Changes](#changes) - [Team](#team) - [Reference](#reference) # Introduction The current framework is a tool for analyzing and visualizing challenge results in the field of biomedical image analysis and beyond. Biomedical challenges have become the de facto standard for benchmarking biomedical image analysis algorithms. While the number of challenges is steadily increasing, surprisingly little effort has been invested in ensuring high quality design, execution and reporting for these international competitions. Specifically, results analysis and visualization in the event of uncertainties have been given almost no attention in the literature. Given these shortcomings, the current framework aims to enable fast and wide adoption of comprehensively analyzing and visualizing the results of single-task and multi-task challenges. This approach offers an intuitive way to gain important insights into the relative and absolute performance of algorithms, which cannot be revealed by commonly applied visualization techniques. # Installation Requires R version &gt;= 3.5.2 (<https://www.r-project.org>). Further, a recent version of Pandoc (&gt;= 1.12.3) is required. RStudio (<https://rstudio.com>) automatically includes this so you do not need to download Pandoc if you plan to use rmarkdown from the RStudio IDE, otherwise you'll need to install Pandoc for your platform (<https://pandoc.org/installing.html>). Finally, if you want to generate a PDF report you will need to have LaTeX installed (e.g. MiKTeX, MacTeX or TinyTeX). To get the latest released version (master branch) of the R package from GitHub: ``` r if (!requireNamespace(\"devtools\", quietly = TRUE)) install.packages(\"devtools\") if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"Rgraphviz\", dependencies = TRUE) devtools::install_github(\"wiesenfa/challengeR\", dependencies = TRUE) ``` If you are asked whether you want to update installed packages and you type \"a\" for all, you might need administrator permissions to update R core packages. You can also try to type \"n\" for updating no packages. If you are asked \"Do you want to install from sources the packages which need compilation? (Yes/no/cancel)\", you can safely type \"no\". If you get *warning* messages (in contrast to *error* messages), these might not be problematic and you can try to proceed. If you encounter errors during the setup, looking into the \"Troubleshooting\" section might be worth it. For Linux users: Some system libraries might be missing. Check the output in the R console for further hints carefully during the installation of packages. # Terms of use Copyright (c) German Cancer Research Center (DKFZ). All rights reserved. challengeR is available under license GPLv2 or any later version. If you use this software for a publication, please cite: Wiesenfarth, M., Reinke, A., Landman, B.A., Eisenmann, M., Aguilera Saiz, L., Cardoso, M.J., Maier-Hein, L. and Kopp-Schneider, A. Methods and open-source toolkit for analyzing and visualizing challenge results. *Sci Rep* **11**, 2369 (2021). <https://doi.org/10.1038/s41598-021-82017-6> # Usage Each of the following steps has to be run to generate the report: (1) Load package, (2) load data, (3) perform ranking, (4) perform bootstrapping and (5) generation of the report You can find R scripts for quickstart in the directory \"vignettes\". An overview of all available plots is provided in the \"Visualizations\" vignette demonstrating the use of their corresponding plot functions as well. Here, we provide a step-by-step guide that leads you to your final report. ## 1. Load package Load package ``` r library(challengeR) ``` ## 2. Load data ### Data requirements Data requires the following *columns*: - *task identifier* in case of multi-task challenges (string or numeric) - *test case identifier* (string or numeric) - *algorithm identifier* (string or numeric) - *metric value* (numeric) In case of missing metric values, a missing observation has to be provided (either as blank field or \"NA\"). For example, in a challenge with 2 tasks, 2 test cases and 2 algorithms, where in task \"T2\", test case \"case2\", algorithm \"A2\" didn't give a prediction (and thus NA or a blank field for missing value is inserted), the data set might look like this: | Task | TestCase | Algorithm | MetricValue | |:-----|:---------|:----------|------------:| | T1 | case1 | A1 | 0.266 | | T1 | case1 | A2 | 0.202 | | T1 | case2 | A1 | 0.573 | | T1 | case2 | A2 | 0.945 | | T2 | case1 | A1 | 0.372 | | T2 | case1 | A2 | 0.898 | | T2 | case2 | A1 | 0.908 | | T2 | case2 | A2 | NA | ### 2.1 Load data from file If you have assessment data at hand stored in a csv file (if you want to use simulated data, skip the following code line) use ``` r data_matrix <- read.csv(file.choose()) # type ?read.csv for help ``` This allows to choose a file interactively, otherwise replace *file.choose()* by the file path (in style \"/path/to/dataset.csv\") in quotation marks. ### 2.2 Simulate data In the following, simulated data is generated *instead* for illustration purposes (skip the following code chunk if you have already loaded data). The data is also stored as \"inst/extdata/data\\_matrix.csv\" in the repository. ``` r if (!requireNamespace(\"permute\", quietly = TRUE)) install.packages(\"permute\") n <- 50 set.seed(4) strip <- runif(n,.9,1) c_ideal <- cbind(task=\"c_ideal\", rbind( data.frame(alg_name=\"A1\",value=runif(n,.9,1),case=1:n), data.frame(alg_name=\"A2\",value=runif(n,.8,.89),case=1:n), data.frame(alg_name=\"A3\",value=runif(n,.7,.79),case=1:n), data.frame(alg_name=\"A4\",value=runif(n,.6,.69),case=1:n), data.frame(alg_name=\"A5\",value=runif(n,.5,.59),case=1:n) )) set.seed(1) c_random <- data.frame(task=\"c_random\", alg_name=factor(paste0(\"A\",rep(1:5,each=n))), value=plogis(rnorm(5*n,1.5,1)),case=rep(1:n,times=5) ) strip2 <- seq(.8,1,length.out=5) a <- permute::allPerms(1:5) c_worstcase <- data.frame(task=\"c_worstcase\", alg_name=c(t(a)), value=rep(strip2,nrow(a)), case=rep(1:nrow(a),each=5) ) c_worstcase <- rbind(c_worstcase, data.frame(task=\"c_worstcase\",alg_name=1:5,value=strip2,case=max(c_worstcase$case)+1) ) c_worstcase$alg_name <- factor(c_worstcase$alg_name,labels=paste0(\"A\",1:5)) data_matrix <- rbind(c_ideal, c_random, c_worstcase) ``` ## 3. Perform ranking ### 3.1 Define challenge object Code differs slightly for single- and multi-task challenges. In case of a single-task challenge use ``` r # Use only task \"c_random\" in object data_matrix dataSubset <- subset(data_matrix, task==\"c_random\") challenge <- as.challenge(dataSubset, # Specify which column contains the algorithms, # which column contains a test case identifier # and which contains the metric value: algorithm = \"alg_name\", case = \"case\", value = \"value\", # Specify if small metric values are better smallBetter = FALSE) ``` *Instead*, for a multi-task challenge use ``` r # Same as above but with 'by=\"task\"' where variable \"task\" contains the task identifier challenge <- as.challenge(data_matrix, by = \"task\", algorithm = \"alg_name\", case = \"case\", value = \"value\", smallBetter = FALSE) ``` ### 3.2 Configure ranking Different ranking methods are available, choose one of them: - for \"aggregate-then-rank\" use (here: take mean for aggregation) ``` r ranking <- challenge%>%aggregateThenRank(FUN = mean, # aggregation function, # e.g. mean, median, min, max, # or e.g. function(x) quantile(x, probs=0.05) na.treat = 0, # either \"na.rm\" to remove missing data, # set missings to numeric value (e.g. 0) # or specify a function, # e.g. function(x) min(x) ties.method = \"min\" # a character string specifying # how ties are treated, see ?base::rank ) ``` - *alternatively*, for \"rank-then-aggregate\" with arguments as above (here: take mean for aggregation) ``` r ranking <- challenge%>%rankThenAggregate(FUN = mean, ties.method = \"min\" ) ``` - *alternatively*, for test-then-rank based on Wilcoxon signed rank test ``` r ranking <- challenge%>%testThenRank(alpha = 0.05, # significance level p.adjust.method = \"none\", # method for adjustment for # multiple testing, see ?p.adjust na.treat = 0, # either \"na.rm\" to remove missing data, # set missings to numeric value (e.g. 0) # or specify a function, e.g. function(x) min(x) ties.method = \"min\" # a character string specifying # how ties are treated, see ?base::rank ) ``` ## 4. Perform bootstrapping Perform bootstrapping with 1000 bootstrap samples using one CPU ``` r set.seed(123, kind = \"L'Ecuyer-CMRG\") ranking_bootstrapped <- ranking%>%bootstrap(nboot = 1000) ``` If you want to use multiple CPUs (here: 8 CPUs), use ``` r library(doParallel) library(doRNG) registerDoParallel(cores = 8) registerDoRNG(123) ranking_bootstrapped <- ranking%>%bootstrap(nboot = 1000, parallel = TRUE, progress = \"none\") stopImplicitCluster() ``` ## 5. Generate the report Generate report in PDF, HTML or DOCX format. Code differs slightly for single- and multi-task challenges. ### 5.1 For single-task challenges ``` r ranking_bootstrapped %>% report(title = \"singleTaskChallengeExample\", # used for the title of the report file = \"filename\", format = \"PDF\", # format can be \"PDF\", \"HTML\" or \"Word\" latex_engine = \"pdflatex\", #LaTeX engine for producing PDF output. Options are \"pdflatex\", \"lualatex\", and \"xelatex\" clean = TRUE #optional. Using TRUE will clean intermediate files that are created during rendering. ) ``` Argument *file* allows for specifying the output file path as well, otherwise the working directory is used. If file is specified but does not have a file extension, an extension will be automatically added according to the output format given in *format*. Using argument *clean=FALSE* allows to retain intermediate files, such as separate files for each figure. If argument \"file\" is omitted, the report is created in a temporary folder with file name \"report\". ### 5.2 For multi-task challenges Same as for single-task challenges, but additionally consensus ranking (rank aggregation across tasks) has to be given. Compute ranking consensus across tasks (here: consensus ranking according to mean ranks across tasks) ``` r # See ?relation_consensus for different methods to derive consensus ranking meanRanks <- ranking%>%consensus(method = \"euclidean\") meanRanks # note that there may be ties (i.e. some algorithms have identical mean rank) ``` Generate report as above, but with additional specification of consensus ranking ``` r ranking_bootstrapped %>% report(consensus = meanRanks, title = \"multiTaskChallengeExample\", file = \"filename\", format = \"PDF\", # format can be \"PDF\", \"HTML\" or \"Word\" latex_engine = \"pdflatex\"#LaTeX engine for producing PDF output. Options are \"pdflatex\", \"lualatex\", and \"xelatex\" ) ``` The consensus ranking is given according to mean ranks across tasks if method=\"euclidean\" where in case of ties (equal ranks for multiple algorithms) the average rank is used, i.e. ties.method=\"average\". # Troubleshooting In this section we provide an overview of issues that the users reported and how they were solved. ## Issues related to RStudio ### Issue: Rtools is missing While trying to install the current version of the repository: ``` r devtools::install_github(\"wiesenfa/challengeR\", dependencies = TRUE) ``` The following warning showed up in the output: ``` r WARNING: Rtools is required to build R packages, but is not currently installed. ``` Therefore, Rtools was installed via a separate executable: <https://cran.r-project.org/bin/windows/Rtools/> and the warning disappeared. #### Solution: Actually there is no need of installing Rtools, it is not really used in the toolkit. Insted, choose not to install it when it is asked. See comment in the installation section: \"If you are asked whether you want to update installed packages and you type \"a\" for all, you might need administrator rights to update R core packages. You can also try to type \"n\" for updating no packages. If you are asked \"Do you want to install from sources the packages which need compilation? (Yes/no/cancel)\", you can safely type \"no\".\" ### Issue: Package versions are mismatching Installing the current version of the tool from GitHub failed. The error message was: ``` r byte-compile and prepare package for lazy loading Error: (converted from warning) package 'ggplot2' was built under R version 3.6.3 Execution halted ERROR: lazy loading failed for package 'challengeR' * removing 'C:/Users/.../Documents/R/win-library/3.6/challengeR' * restoring previous 'C:/Users/.../Documents/R/win-library/3.6/challengeR' Error: Failed to install 'challengeR' from GitHub: (converted from warning) installation of package 'C:/Users/.../AppData/Local/Temp/Rtmp615qmV/file4fd419555eb4/challengeR_0.3.1.tar.gz' had non-zero exit status ``` The problem was that some of the packages that were built under R3.6.1 had been updated, but the current installed version was still R3.6.1. #### Solution: The solution was to update R3.6.1 to R3.6.3. Another way would have been to reset the single packages to the versions built under R3.6.1. ### Issue: Package is missing Installing the current version of the tool from GitHub failed. ``` r devtools::install_github(\"wiesenfa/challengeR\", dependencies = TRUE) ``` The error message was: ``` r Error: .onLoad failed in loadNamespace() for 'pkgload', details: call: loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) error: there is no package called 'backports' ``` The problem was that the packages 'backports' had not been installed. #### Solution: The solution was to install 'backports' manually. ``` r install.packages(\"backports\") ``` ### Issue: Packages are not detected correctly While trying to install the package after running the following commands: ``` r if (!requireNamespace(\"devtools\", quietly = TRUE)) install.packages(\"devtools\") if (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"Rgraphviz\", dependencies = TRUE) devtools::install_github(\"wiesenfa/challengeR\", dependencies = TRUE) ``` The error message was: ``` r ERROR: 1: In file(con, \"r\") : URL 'https://bioconductor.org/config.yaml': status was 'SSL connect error' 2: packages 'BiocVersion', 'Rgraphviz' are not available (for R version 3.6.1) ``` #### Solution: The solution was to restart RStudio. ## Issues related to MiKTeX ### Issue: Missing packages While generating the PDF with MiKTeX (2.9), the following error showed up: ``` r fatal pdflatex - gui framework cannot be initialized ``` There is an issue with installing missing packages in LaTeX. ##### Solution: Open your MiKTeX Console -&gt; Settings, select \"Always install missing packages on-the-fly\". Then generate the report. Once the report is generated, you can reset the settings to your preferred ones. ### Issue: Unable to generate report While generating the PDF with MiKTeX (2.9): ``` r ranking_bootstrapped %>% report(title = \"singleTaskChallengeExample\", # used for the title of the report file = \"filename\", format = \"PDF\", # format can be \"PDF\", \"HTML\" or \"Word\" latex_engine = \"pdflatex\", #LaTeX engine for producing PDF output. Options are \"pdflatex\", \"lualatex\", and \"xelatex\" clean = TRUE #optional. Using TRUE will clean intermediate files that are created during rendering. ) ``` The following error showed up: ``` r output file: filename.knit.md \"C:/Program Files/RStudio/bin/pandoc/pandoc\" +RTS -K512m -RTS filename.utf8.md --to latex --from markdown+autolink_bare_uris+tex_math_single_backslash --output filename.tex --self-contained --number-sections --highlight-style tango --pdf-engine pdflatex --variable graphics --lua-filter \"C:/Users/adm/Documents/R/win-library/3.6/rmarkdown/rmd/lua/pagebreak.lua\" --lua-filter \"C:/Users/adm/Documents/R/win-library/3.6/rmarkdown/rmd/lua/latex-div.lua\" --variable \"geometry:margin=1in\" Error: LaTeX failed to compile filename.tex. See https://yihui.org/tinytex/r/#debugging for debugging tips. Warning message: In system2(..., stdout = if (use_file_stdout()) f1 else FALSE, stderr = f2) : '\"pdflatex\"' not found ``` #### Solution: The solution was to restart RStudio. # Changes #### Version 1.0.5 - Ensure reproducibility with parallel bootstrapping ([T29361](https://phabricator.mitk.org/T29361)) #### Version 1.0.4 - Fix NaN values cause error ([T28746](https://phabricator.mitk.org/T28746)) - Fix Bars and dots don't match in podium plot ([T29167](https://phabricator.mitk.org/T29167)) - Fix y-axis of blob plots always scaled to 5 ([T28966](https://phabricator.mitk.org/T28966)) #### Version 1.0.3 - Fix ggplot warning in various places of the report ([T28710](https://phabricator.mitk.org/T28710)) #### Version 1.0.2 - Fix error when all metric values are the same ([T28453](https://phabricator.mitk.org/T28453)) - Fix wrong number of algorithms shown in report summary ([T28465](https://phabricator.mitk.org/T28465)) #### Version 1.0.1 - Fix error raised in case there are more tasks than algorithms contained in the dataset ([T28193](https://phabricator.mitk.org/T28193)) - Drop restriction that at least three algorithms are required for bootstrapping ([T28194](https://phabricator.mitk.org/T28194)) - Avoid blank pages in PDF report when bootstrapping is disabled ([T28201](https://phabricator.mitk.org/T28201)) - Handle tasks having only one case for bootstrapping ([T28202](https://phabricator.mitk.org/T28202)) - Update citation ([T28210](https://phabricator.mitk.org/T28210)) #### Version 1.0.0 - Revision of the underlying data structure - Roxygen documentation for main functionality - Vignettes for quickstart and overview of available plots demonstrating the use of their corresponding plot functions - Introduction of unit tests (package coverage &gt;70%) - Troubleshooting section covering potential issues during setup - Finally: Extensive bug fixes and improvements (for a complete overview please check the [Phabricator tasks](https://phabricator.mitk.org/search/query/vtj0qOqH5qL6/)) #### Version 0.3.3 - Force line break to avoid that authors exceed the page in generated PDF reports #### Version 0.3.2 - Correct names of authors #### Version 0.3.1 - Refactoring #### Version 0.3.0 - Major bug fix release #### Version 0.2.5 - Bug fixes #### Version 0.2.4 - Automatic insertion of missings #### Version 0.2.3 - Bug fixes - Reports for subsets (top list) of algorithms: Use e.g. `subset(ranking_bootstrapped, top=3) %>% report(...)` (or `subset(ranking, top=3) %>% report(...)` for report without bootstrap results) to only show the top 3 algorithms according to the chosen ranking methods, where `ranking_bootstrapped` and `ranking` objects as defined in the example. Line plot for ranking robustness can be used to check whether algorithms performing well in other ranking methods are excluded. Bootstrapping still takes entire uncertainty into account. Podium plot and ranking heatmap neglect excluded algorithms. Only available for single-task challenges (for multi-task challenges not sensible because each task would contain a different set of algorithms). - Reports for subsets of tasks: Use e.g. `subset(ranking_bootstrapped, tasks=c(\"task1\", \"task2\",\"task3\")) %>% report(...)` to restrict report to tasks \"task1\", \"task2\",\"task3. You may want to recompute the consensus ranking before using `meanRanks=subset(ranking, tasks=c(\"task1\", \"task2\", \"task3\"))%>%consensus(method = \"euclidean\")` #### Version 0.2.1 - Introduction in reports now mentions e.g. ranking method, number of test cases,... - Function `subset()` allows selection of tasks after bootstrapping, e.g. `subset(ranking_bootstrapped,1:3)` - `report()` functions gain argument `colors` (default: `default_colors`). Change e.g. to `colors=viridisLite::inferno` which \"is designed in such a way that it will analytically be perfectly perceptually-uniform, both in regular form and also when converted to black-and-white. It is also designed to be perceived by readers with the most common form of color blindness.\" See package `viridis` for further similar functions. #### Version 0.2.0 - Improved layout in case of many algorithms and tasks (while probably still not perfect) - Consistent coloring of algorithms across figures - `report()` function can be applied to ranked object before bootstrapping (and thus excluding figures based on bootstrapping), i.e. in the example `ranking %>% report(...)` - bug fixes # Team The developer team includes members from both division of Intelligent Medical Systems (IMSY) and Biostatistics at the German Cancer Research Center (DKFZ): - Manuel Wiesenfarth - Annette Kopp-Schneider - Annika Reinke - Matthias Eisenmann - Laura Aguilera Saiz - Elise Récéjac - Lena Maier-Hein - Ali Emre Kavur # Reference Wiesenfarth, M., Reinke, A., Landman, B.A., Eisenmann, M., Aguilera Saiz, L., Cardoso, M.J., Maier-Hein, L. and Kopp-Schneider, A. Methods and open-source toolkit for analyzing and visualizing challenge results. *Sci Rep* **11**, 2369 (2021). <https://doi.org/10.1038/s41598-021-82017-6> </br> <img src=\"Helmholtz_Imaging_Logo.svg\" height=\"70px\" /> </br></br> <img src=\"DKFZ_Logo.png\" height=\"100px\" />\n",
                "dependencies": "Package: challengeR Type: Package Title: Analyzing assessment data of biomedical image analysis competitions and visualization of results Version: 1.0.5 Date: 2023-08-10 Author: Manuel Wiesenfarth, Matthias Eisenmann, Laura Aguilera Saiz, Annette Kopp-Schneider, Ali Emre Kavur Maintainer: Manuel Wiesenfarth <m.wiesenfarth@dkfz.de> Description: Analyzing assessment data of biomedical image analysis competitions and visualization of results. License: GPL (>= 2) Depends: R (>= 3.5.2), ggplot2 (>= 3.3.0), purrr (>= 0.3.3) Imports: dplyr (>= 0.8.5), graph (>= 1.64.0), knitr (>= 1.28), methods (>= 3.6.0), plyr (>= 1.8.6), relations (>= 0.6-9), reshape2 (>= 1.4.3), rlang (>= 0.4.5), rmarkdown (>= 2.1), tidyr (>= 1.0.2), viridisLite (>= 0.3.0) Suggests: doParallel (>= 1.0.15), doRNG (>= 1.8.6), foreach (>= 1.4.8), ggpubr (>= 0.2.5), Rgraphviz (>= 2.30.0), testthat (>= 2.1.0) VignetteBuilder: knitr Roxygen: list(markdown = TRUE) RoxygenNote: 7.1.0\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rayx",
            "repo_link": "https://github.com/hz-b/rayx",
            "content": {
                "codemeta": "",
                "readme": "# RAYX <table> <tr> <td> <img src=\"https://github.com/user-attachments/assets/d12229b0-7820-475f-8f02-6b2f253c5081\" alt=\"RAYX Logo\" width=\"600\"> </td> <td> <strong>RAYX</strong> is a powerful, multi-component simulation platform designed to streamline the design and optimization of beamlines in synchrotron light source facilities. At the core of the platform is <i>rayx-core</i>, a high-performance library that delivers precise light tracing capabilities on both CPUs and GPUs. This core library ensures that users can achieve detailed and accurate simulations at high speeds, making it an ideal solution for complex beamline designs. </td> </tr> </table> To simplify the usage of _rayx-core_, the platform includes rayx, a command-line interface (CLI) tool designed for fast, one-shot tracing of beamlines. It provides comprehensive data on every ray-element intersection, making it especially valuable for generating large datasets efficiently. With its focus on ease of use, _rayx_ empowers users to quickly run simulations and retrieve detailed ray-tracing results. For users who prefer a more visual approach, _rayx-ui_ offers a graphical user interface (GUI) that includes a 3D viewport of the beamline, enabling interactive design and exploration. This GUI provides an intuitive interface to construct and modify beamlines, allowing users to visualize their designs in real-time. _rayx-ui_ not only enhances the design process but also allows users to iteratively optimize configurations based on immediate visual feedback. ## RAYX vs RAY-UI RAYX offers several advanced features, including: - Global (not sequential) tracing of beamlines - GPU utilization for accelerated tracing performance - A dedicated mode for tracing multiple beamlines with ease - Objects in RAYX can be grouped for simplified group transformations - A GUI for intuitive beamline design ## Installing or Building RAYX [![testUbuntu](https://github.com/hz-b/rayx/actions/workflows/testUbuntu.yml/badge.svg?branch=master)](https://github.com/hz-b/rayx/actions/workflows/testUbuntu.yml) [![testWindows](https://github.com/hz-b/rayx/actions/workflows/testWindows.yml/badge.svg?branch=master)](https://github.com/hz-b/rayx/actions/workflows/testWindows.yml) [![testUbuntuClang](https://github.com/hz-b/rayx/actions/workflows/testUbuntuClang.yml/badge.svg?branch=master)](https://github.com/hz-b/rayx/actions/workflows/testUbuntuClang.yml) [![MDBookDeploy](https://github.com/hz-b/rayx/actions/workflows/mdBookDeploy.yml/badge.svg)](https://github.com/hz-b/rayx/actions/workflows/mdBookDeploy.yml) For additional information, please visit our [Wiki](https://hz-b.github.io/rayx/). We are committed to delivering stable releases, which can be found [here](https://github.com/hz-b/rayx/releases). Please note that the `master` branch and other branches might be unstable, and building RAYX from the source could lead to unstable software. We recommend this only for developers and experienced users. If you experience issues with our distributed binaries or API, do not hesitate to [open an issue](https://github.com/hz-b/rayx/issues/new/choose). We are keen to provide assistance and develop features as the need arises.\n",
                "dependencies": "cmake_minimum_required(VERSION 3.15 FATAL_ERROR) # ---- Project ---- project(RAYX VERSION 0.21.7) if(MSVC) set(CMAKE_MSVC_RUNTIME_LIBRARY \"MultiThreaded$<$<CONFIG:Debug>:Debug>DLL\") endif() set(CMAKE_CXX_STANDARD 20) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(CMAKE_CUDA_STANDARD 20) set(CMAKE_CUDA_STANDARD_REQUIRED ON) # ------------------ # ---- Options ---- option(WERROR \"add -Werror option\" \"NO\") # inactive per default option(RAYX_ENABLE_CUDA \"This option enables the search for CUDA. Project will be compiled without cuda if not found.\" ON) option(RAYX_REQUIRE_CUDA \"If option 'RAYX_ENABLE_CUDA' is ON, this option will add the requirement that cuda must be found.\" OFF) option(RAYX_ENABLE_OPENMP \"This option enables the search for OPENMP. Project will be compiled without openmp if not found.\" ON) option(RAYX_REQUIRE_OPENMP \"If option 'RAYX_ENABLE_OPENMP' is ON, this option will add the requirement that openmp must be found.\" OFF) option(RAYX_STATIC_LIB \"This option builds 'rayx-core' as a static library.\" OFF) # ------------------ # ---- Build options ---- set(CMAKE_RUNTIME_OUTPUT_DIRECTORY_RELEASE ${CMAKE_BINARY_DIR}/bin/release) set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY_RELEASE ${CMAKE_BINARY_DIR}/lib/release) set(CMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE ${CMAKE_BINARY_DIR}/lib/release) set(CMAKE_RUNTIME_OUTPUT_DIRECTORY_DEBUG ${CMAKE_BINARY_DIR}/bin/debug) set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY_DEBUG ${CMAKE_BINARY_DIR}/lib/debug) set(CMAKE_LIBRARY_OUTPUT_DIRECTORY_DEBUG ${CMAKE_BINARY_DIR}/lib/debug) # ------------------ # ---- Defaults ---- if(NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE \"Debug\") endif() # ------------------ # ---- RAYX directory ---- set(RAYX_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}) # ------------------------ # ---- Code Coverage ---- set(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/Extern/cmake) # ------------------ # ---- CPack ---- set(CPACK_PROJECT_NAME ${PROJECT_NAME}) set(CPACK_RESOURCE_FILE_LICENSE \"${CMAKE_SOURCE_DIR}/LICENSE\") set(CPACK_PACKAGE_DESCRIPTION \"${PROJECT_NAME} - For simulating and designing beamlines at synchrotron light sources\") set(CPACK_PACKAGE_DESCRIPTION_SUMMARY \"${PROJECT_NAME} - For simulating and designing beamlines at synchrotron light sources\") set(CPACK_PACKAGE_VENDOR \"Helmhotz-Zentrum Berlin\") set(CPACK_PACKAGE_CONTACT \"jannis.maier@helmholtz-berlin.de\") # Disable GoogleTest installation set(INSTALL_GTEST OFF CACHE BOOL \"Disable installation of GoogleTest\" FORCE) set(INSTALL_GMOCK OFF CACHE BOOL \"Disable installation of GoogleMock\" FORCE) # Install directories if(APPLE) set(INSTALL_DATA_DIR \"Library/Application Support/${PROJECT_NAME}\") set(INSTALL_FONTS_DIR \"Library/Fonts/${PROJECT_NAME}\") elseif(UNIX AND NOT APPLE) set(INSTALL_DATA_DIR \"share/${PROJECT_NAME}\") set(INSTALL_FONTS_DIR \"share/fonts/${PROJECT_NAME}\") elseif(WIN32) set(INSTALL_DATA_DIR \".\") set(INSTALL_FONTS_DIR \".\") endif() # ------------------ # ---- Subdirectories ---- add_subdirectory(Extern) add_subdirectory(Intern) # ------------------\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rce",
            "repo_link": "https://github.com/rcenvironment/rce",
            "content": {
                "codemeta": "",
                "readme": "RCE is a distributed, workflow-driven integration environment. It is used by engineers and scientists to analyze, optimize, and design complex systems (e.g., aircraft, ships, or satellites). Using RCE, they can combine their specialized design and simulation tools into distributed workflows. Software website: [https://rcenvironment.de](https://rcenvironment.de) ##### Issue Tracker See the [main issue tracker](https://mantis.sc.dlr.de/roadmap_page.php) for the complete set of issues and an up-to-date roadmap. It is currently read-only. Please use the [GitHub issue tracker](https://github.com/rcenvironment/rce/issues) to report new issues. ##### Changelog The most relevant changes and new features in each release are listed on [this page](https://github.com/rcenvironment/rce/releases). Similar information for older releases is available [here](https://github.com/rcenvironment/rce/wiki/Changelog-Overview). For a more detailed, but also more technical list of changes, see the [main issue tracker](https://mantis.sc.dlr.de/changelog_page.php). ##### Get RCE (as a ready-to-run software) For Windows, a simple .zip file is provided to set up both client and server installations. On Linux, .deb/.rpm packages as well as a simple .zip file are provided to set up both client and server installations. [Download latest release](https://software.dlr.de/updates/rce/10.x/products/standard/releases/latest/) | [Update site of latest release](https://software.dlr.de/updates/rce/10.x/repositories/standard/releases/latest/) ##### License RCE is Open Source Software provided under the terms of the [Eclipse Public License (EPL)](http://opensource.org/licenses/EPL-1.0). This source repository as well as related releases also contain software covered by other open source licenses. More information is available in embedded licensing files.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/reflectorch",
            "repo_link": "https://github.com/schreiber-lab/reflectorch",
            "content": {
                "codemeta": "",
                "readme": "# Reflectorch [![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)](https://pytorch.org/) [![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white)](https://numpy.org/) [![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=for-the-badge&logo=scipy&logoColor=%white)](https://scipy.org/) [![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=black)](https://matplotlib.org/) [![YAML](https://img.shields.io/badge/yaml-%23ffffff.svg?style=for-the-badge&logo=yaml&logoColor=151515)](https://yaml.org/) [![Hugging Face](https://img.shields.io/badge/Hugging%20Face-%23FFD700.svg?style=for-the-badge&logo=huggingface&logoColor=black)](https://huggingface.co/valentinsingularity/reflectivity) [![Python version](https://img.shields.io/badge/python-3.7%7C3.8%7C3.9%7C3.10%7C3.11%7C3.12-blue.svg)](https://www.python.org/) ![CI workflow status](https://github.com/schreiber-lab/reflectorch/actions/workflows/ci.yml/badge.svg) ![Repos size](https://img.shields.io/github/repo-size/schreiber-lab/reflectorch) [![CodeFactor](https://www.codefactor.io/repository/github/schreiber-lab/reflectorch/badge)](https://www.codefactor.io/repository/github/schreiber-lab/reflectorch) [![Jupyter Book Documentation](https://jupyterbook.org/badge.svg)](https://jupyterbook.org/) [![Documentation Page](https://img.shields.io/badge/Documentation%20Page-%23FFDD33.svg?style=flat&logo=read-the-docs&logoColor=black)](https://schreiber-lab.github.io/reflectorch/) <!-- [![Code style: Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff) --> **Reflectorch** is a machine learning Python package for the analysis of X-ray and neutron reflectometry data, written by [Vladimir Starostin](https://github.com/StarostinV/) & [Valentin Munteanu](https://github.com/valentinsingularity) at the University of Tübingen. It provides functionality for the fast simulation of reflectometry curves on the GPU, customizable setup of the physical parameterization model and neural network architecture via YAML configuration files, and prior-aware training of neural networks as described in our paper [Neural network analysis of neutron and X-ray reflectivity data incorporating prior knowledge](https://doi.org/10.1107/S1600576724002115). ## Installation **Reflectorch** can be installed from [![PyPi](https://img.shields.io/badge/PyPi-3776AB.svg?style=flat&logo=pypi&logoColor=white)](https://pypi.org/project/reflectorch/) via ``pip``: <!-- or from [![conda-forge](https://img.shields.io/badge/conda--forge-44A833.svg?style=flat&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/reflectorch/) via ``conda``: --> ```bash pip install reflectorch ``` <!-- or ```bash conda install -c conda-forge reflectorch ``` --> Alternatively, one can clone the entire Github repository and install the package in editable mode: ```bash git clone https://github.com/schreiber-lab/reflectorch.git pip install -e . ``` For development purposes, the package can be installed together with the optional dependencies for building the distribution, testing and documentation: ```bash git clone https://github.com/schreiber-lab/reflectorch.git pip install -e .[tests,docs,build] ``` Users with Nvidia **GPU**s need to additionally install **Pytorch with CUDA support** corresponding to their hardware and operating system according to the instructions from the [Pytorch website](https://pytorch.org/get-started/locally/) ## Get started [![Documentation Page](https://img.shields.io/badge/Documentation%20Page-%23FFDD33.svg?style=flat&logo=read-the-docs&logoColor=black)](https://schreiber-lab.github.io/reflectorch/) The full documentation of the package, containing tutorials and the API reference, was built with [Jupyter Book](https://jupyterbook.org/) and [Sphinx](https://www.sphinx-doc.org) and it is hosted at the address: [https://schreiber-lab.github.io/reflectorch/](https://schreiber-lab.github.io/reflectorch/). [![Interactive Notebook](https://img.shields.io/badge/Interactive%20Notebook-%23F9AB00.svg?style=flat&logo=google-colab&logoColor=black)](https://colab.research.google.com/drive/1rf_M8S_5kYvUoK0-9-AYal_fO3oFl7ck?usp=sharing) We provide an interactive Google Colab notebook for exploring the basic functionality of the package: [![Explore reflectorch in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rf_M8S_5kYvUoK0-9-AYal_fO3oFl7ck?usp=sharing)<br> [![Hugging Face](https://img.shields.io/badge/Hugging%20Face-%23FFD700.svg?style=flat&logo=huggingface&logoColor=black)](https://huggingface.co/valentinsingularity/reflectivity) Configuration files and the corresponding pretrained model weights are hosted on Huggingface: [https://huggingface.co/valentinsingularity/reflectivity](https://huggingface.co/valentinsingularity/reflectivity). [![Docker](https://img.shields.io/badge/Docker-2496ED.svg?style=flat&logo=docker&logoColor=white)](https://hub.docker.com/) Docker images for reflectorch *will* be hosted on Dockerhub. ## Citation If you find our work useful in your research, please cite as follows: ``` @Article{Munteanu2024, author = {Munteanu, Valentin and Starostin, Vladimir and Greco, Alessandro and Pithan, Linus and Gerlach, Alexander and Hinderhofer, Alexander and Kowarik, Stefan and Schreiber, Frank}, journal = {Journal of Applied Crystallography}, title = {Neural network analysis of neutron and X-ray reflectivity data incorporating prior knowledge}, year = {2024}, issn = {1600-5767}, month = mar, number = {2}, volume = {57}, doi = {10.1107/s1600576724002115}, publisher = {International Union of Crystallography (IUCr)}, } ```\n",
                "dependencies": "[build-system] requires = [\"setuptools>=61\", \"setuptools-scm\"] build-backend = \"setuptools.build_meta\" [project] name = \"reflectorch\" version = \"1.2.1\" authors = [ {name = \"Vladimir Starostin\", email=\"vladimir.starostin@uni-tuebingen.de\"}, {name = \"Valentin Munteanu\", email=\"valentin.munteanu@uni-tuebingen.de\"} ] maintainers = [ {name = \"Valentin Munteanu\", email=\"valentin.munteanu@uni-tuebingen.de\"}, {name = \"Vladimir Starostin\", email=\"vladimir.starostin@uni-tuebingen.de\"}, {name = \"Alexander Hinderhofer\", email=\"alexander.hinderhofer@uni-tuebingen.de\"} ] description = \"A Pytorch-based package for the analysis of reflectometry data\" keywords = [\"reflectometry\", \"machine learning\"] readme = \"README.md\" license = {file = \"LICENSE\"} classifiers = [ \"Programming Language :: Python :: 3\", \"Operating System :: OS Independent\", \"Environment :: GPU :: NVIDIA CUDA\", \"License :: OSI Approved :: GNU General Public License v3 (GPLv3)\", \"Development Status :: 4 - Beta\", \"Topic :: Scientific/Engineering :: Physics\", \"Intended Audience :: Science/Research\", ] requires-python = \">=3.7\" dependencies = [ \"numpy>=1.18.1,<2.0\", \"torch>=1.8.1\", \"scipy\", \"tqdm\", \"PyYAML\", \"click\", \"matplotlib\", \"ipywidgets\", \"huggingface_hub\", \"safetensors\", ] [project.optional-dependencies] tests = [\"pytest\", \"pytest-cov\"] docs = [\"jupyter-book\", \"sphinx\"] build = [\"build\", \"twine\"] [project.urls] Source = \"https://github.com/schreiber-lab/reflectorch/\" Issues = \"https://github.com/schreiber-lab/reflectorch/issues\" Documentation = \"https://schreiber-lab.github.io/reflectorch/\" [tool.setuptools] packages = [\"reflectorch\"] [tool.pytest.ini_options] testpaths = [\"tests\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/remix",
            "repo_link": "https://gitlab.com/dlr-ve/esy/remix/framework",
            "content": {
                "codemeta": "",
                "readme": "# REMix REMix is addressing research questions in the field of energy system analysis. The main focus is on the broad techno-economical assessment of possible future energy system designs and analysis of interactions between technologies. This will allow system analysts to inform policy makers and technology researchers to gain a better understanding of both the system and individual components. The documentation of REMix is hosted online: [REMix docu](https://dlr-ve.gitlab.io/esy/remix/framework/). Do not hesitate to ask questions about REMix in the [openmod forum](https://forum.openmod.org/tag/remix). ## Key Features To know if REMix is apt for your project take into account these key features: **Large Models**: REMix is developed with large models in mind. This means high spatial and technological resolutions. **Path Optimization**: Multi-year analyses are built into the framework. **Custom accounting approaches**: The indicator module allows for a very flexible definition of what contributes to the objective functions. **Flexible modeling**: There is not a single way of modeling technologies in REMix. With the flexible [modelling concept](https://dlr-ve.gitlab.io/esy/remix/framework/dev/documentation/modeling-concept/index.html) you can find the best way of integrating your modeling needs. **Multi-criteria optimization**: Apart from running a cost minimization, also other criteria like ecological or resilience indicators can be taken into account in the objective function. ## How to use To run a REMix model, users need a recent [GAMS](https://www.gams.com/) installation (version 37 or above). To use REMix, clone this repository, create a new Python environment and install remix.framework through pip. Either clone with ssh: ```bash git clone git@gitlab.com:dlr-ve/esy/remix/framework.git ``` Or clone with https: ```bash git clone https://gitlab.com/dlr-ve/esy/remix/framework.git ``` For the installation of the `remix.framework` package, you have two options: 1. install from PyPI: ```bash mamba create -n remix-env python # make sure the Python version matches your GAMS version mamba activate remix-env pip install remix.framework ``` 2. install from the cloned repository: ```bash cd framework conda create -n remix-env python conda activate remix-env pip install -e .[dev] ``` Please find the extensive [installation instructions in the online documentation](https://dlr-ve.gitlab.io/esy/remix/framework/dev/getting-started/install-remix.html). Additionally, a data project is required which contains the parametrization of the model scope and technologies. We provide [example projects](https://gitlab.com/dlr-ve/esy/remix/projects), which can be used to gain first experience with running the REMix optimization model. To run your model, you can use the command line interface: ```bash conda activate remix-env remix run --datadir=/path/to/data/folder/of/your/model/data ``` All configuration options available with the command line tool are documented in the [technical documentation](https://dlr-ve.gitlab.io/esy/remix/framework/dev/documentation/tech-docs/index.html). ## Citing REMix * [Wetzel et al. (2024): \"REMix: A GAMS-based framework for optimizing energy system models\"](https://doi.org/10.21105/joss.06330) ## Latest publications using REMix * [Nitsch et al. (2024): \"The future role of Carnot batteries in Central Europe: Combining energy system and market perspective\"](https://doi.org/10.1016/j.est.2024.110959) * [Wetzel et al. (2023): \"Green energy carriers and energy sovereignty in a climate neutral European energy system\"](https://doi.org/10.1016/j.renene.2023.04.015) * [Gils et al. (2022): \"Model-related outcome differences in power system models with sector coupling - quantification and drivers\"](https://doi.org/10.1016/j.rser.2022.112177)[^1] * [Gils et al. (2021): \"Interaction of hydrogen infrastructures with other sector coupling options towards a zero-emission energy system in Germany\"](https://doi.org/10.1016/j.renene.2021.08.016)[^1] * [Sasanpour et al. (2021): \"Strategic policy targets and the contribution of hydrogen in a 100% renewable European power system\"](https://doi.org/10.1016/j.egyr.2021.07.005)[^1] ## Contribute to REMix Contributions are welcome, and they are greatly appreciated! Every bit helps, and credit will always be given. To learn how to contribute to REMix we have included a respective section in our [online documentation](https://dlr-ve.gitlab.io/esy/remix/framework/dev/contributing/index.html). ## Acknowledgments The preparation of the open source version of REMix was financed by the Helmholtz Association's Energy System Design research programme, which also enables the continuous maintenance of the framework. The methodological and content-related development of REMix was made possible by funding from the German Federal Ministries for Economic Affairs and Climate Protection (BMWK) and for Education and Research (BMBF) as part of the projects UNSEEN (BMWK, FKZ 03EI1004A), Sesame Seed (BMWK, FKZ 03EI1021B), Fahrplan Gaswende (BMWK, FKZ 03EI1030B), ReMoDigital (BMWK, FKZ 03EI1020B), and HINT (BMBF, FKZ 03SF0690) as well as the DLR-internal projects NaGsys and CarnotBat, which were also funded by the Helmholtz Association's Energy System Design research programme. The development of earlier REMix versions, which provided the basis for the published version, was made possible by funding from the projects MuSeKo (BMWK, FKZ 03ET4038B), INTEEVER-II (BMWK, FKZ 03ET4069A), START (BMBF, FKZ 03EK3046D), BEAM-ME (BMWK, FKZ 03ET4023A), INTEEVER (BMWK, FKZ 03ET4020A), Plan-DelyKaD (BMWK, FKZ 0325501), \"Lastausgleich\" (BMWK, FKZ 0328009), and \"Elektromobilitaet\" (BMWK, FKZ 0328005A) as well as the Helmholtz Association's Energy System Design research programme and its predecessors. ## Footnotes [^1]: These papers were still using a non-open legacy version of the REMix framework\n",
                "dependencies": "[project] name = \"remix.framework\" keywords = [] readme = \"README.md\" authors = [ {name = \"REMix Developers\", email = \"remix@dlr.de\"}, ] license = {file = \"LICENSE\"} classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: BSD License\", \"Operating System :: Microsoft :: Windows\", \"Operating System :: POSIX\", \"Operating System :: Unix\", \"Programming Language :: Python\", \"Programming Language :: Python :: 3 :: Only\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Topic :: Scientific/Engineering\", ] requires-python = \">=3.9\" dependencies = [ \"jsonschema\", \"networkx\", \"numpy>=1.0.0,<2.0.0\", \"pandas\", \"pyyaml\", \"requests\", \"typer\", ] dynamic = [\"version\", \"description\"] [project.urls] Homepage = \"https://gitlab.com/dlr-ve/esy/remix/framework\" Repository = \"https://gitlab.com/dlr-ve/esy/remix/framework.git\" Documentation = \"https://dlr-ve.gitlab.io/esy/remix/framework/\" Changelog = \"https://gitlab.com/dlr-ve/esy/remix/framework/-/releases\" [project.optional-dependencies] dev = [ \"remix.framework[plotting]\", \"black\", \"build\", \"coverage\", \"flake8\", \"isort\", \"py\", \"pytest\", \"pytest-cov\", \"twine\", ] polars = [ \"polars\", ] excel = [ \"openpyxl\", ] plotting = [ \"cartopy\", \"fiona\", \"shapely\", \"ipykernel\", \"matplotlib\", ] tutorials = [ \"remix.framework[plotting]\", ] all = [ \"remix.framework[dev]\", \"remix.framework[polars]\", \"remix.framework[excel]\", \"remix.framework[plotting]\", \"remix.framework[docs]\", ] docs = [ \"remix.framework[plotting]\", \"jupytext\", \"myst_parser\", \"nbconvert\", \"pandoc\", \"pydata-sphinx-theme<=0.10.1\", \"sphinx<7\", \"sphinx-autobuild\", \"sphinx-copybutton\", \"sphinxcontrib-jquery\", \"sphinx-design\", \"sphinxcontrib.bibtex\", ] [project.scripts] remix = \"remix.framework.cli.main:program\" [project.entry-points.\"remix.plugins\"] run = \"remix.framework.cli.run:program_run\" test = \"remix.framework.cli.test:program_test\" transform = \"remix.framework.cli.transform:program_transform\" build_schemas = \"remix.framework.cli.build_schemas:program_build_schemas\" [build-system] requires = [\"flit_core >=3.2,<4\"] build-backend = \"flit_core.buildapi\" [tool.isort] force_single_line = true line_length = 120 known_first_party = \"remix\" default_section = \"THIRDPARTY\" forced_separate = \"test_remix\" skip = [ \".eggs\", \"build\", \"dist\" ] [tool.pytest.ini_options] pythonpath = [ \"remix\" ] [tool.flit.sdist] exclude = [ \"docs/_build\", \"*/**/*.gdx\", \"*/**/diff.json\", \"*/**/check.json\", \"*/**/*.lst\", \"*/**/cplex.o*\", \"*/**/*.log\", \"*/**/*.gsp\", \"tutorials/**/data/\" ] include = [ \"LICENSE*\", \"docs/\", \"testing/\", \"tutorials/\", \"*/**/valid.json\" ]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/restore",
            "repo_link": "https://github.com/ReStoreCpp/ReStore",
            "content": {
                "codemeta": "",
                "readme": "# ReStore: In-Memory REplicated STORagE for Rapid Recovery in Fault-Tolerant Algorithms Fault-tolerant distributed applications require mechanisms to recover data lost via a process failure. On modern cluster systems it is typically impractical to request replacement resources after such a failure. Therefore, applications have to continue working with the remaining resources. This requires redistributing the workload and that the non-failed processes reload the lost data. ReStore is a C++ header-only library for MPI programs that enables recovery of lost data after (a) process failure(s). By storing all required data in memory via an appropriate data distribution and replication, recovery is substantially faster than with standard checkpointing schemes that rely on a parallel file system. As you as the application programmer can specify which data to load ReStore also supports shrinking recovery instead of recovery using spare compute nodes. ## Including ReStore into your application To use ReStore, first add the repository as a submodule into your project: ```Bash git submodule add --recursive https://github.com/ReStoreCpp/ReStore.git extern/ReStore ``` Then, include the following into your CMakeLists.txt: ```CMake # Configure and link ReStore set(ReStore_BUILD_TESTS Off) set(ReStore_BUILD_BENCHMARKS Off) set(ReStore_ID_RANDOMIZATION On) add_subdirectory(extern/ReStore) target_link_libraries(${YOUR_TARGETS_NAME} ReStore) ``` You can use ID-randomization to break up access patterns in your `load` requests. If enabled, the block IDs you provide will be permuted using a pseudorandom-projection. If you then for example access a range of consecutive blocks IDs, e.g. after the PE which worked on these IDs failed; more PEs will be able to serve the request, resulting in a speedup. If you request most or all of the data `submitted` in each `load`, turning ID-randomization of will be faster. See Hespe and Hübner et al. (2022) [1] for details. ## Code examples ### The general use case This example shows the general usage of ReStore. ```cpp #include <core.hpp> // First, create the restore object. ReStore::ReStore<YourAwesomeDatatype> store( MPI_COMM_WORLD, // MPI communicator to use. ULFM currently supports only MPI_COMM_WORLD. 4, // Replication level, 3 or 4 are sane defaults. ReStore::OffsetMode::constant, // Currently, the only supported mode. sizeof(YourAwesomeDatatype) // Your block size, use at least 64 bytes. ); // Next, submit you data to the ReStore, if a failure happened between creation of the ReStore // and the submission of the data, please re-create the ReStore. ReStore::block_id_t localBlockId = 0; store.submitBlocks( // The serialization function; your can stream your data to the provided stream using // the << operator. [](const YourAwesomeDatatype& value, ReStore::SerializedBlockStoreStream& stream) { // Either use: stream << value; // or, for big, already consecutively stored data: stream.writeBytes(constBytePtr, sizeof(YourAwesomeDatatype)); }, // The enumerator function; should return nullopt if there are no more blocks to submit // on this PE. [localBlockId, ...]() { auto ret = numberOfBlocksOnThisPE == localBlockId ? std::nullopt : std::make_optional(ReStore::NextBlock<YourAwesomeDatatype>( {globalBlockId(localBlockId), constRefToYourDataForThisBlock})); localBlockId++; return ret; }, globalNumberOfBlocks ); // A failure occurred; set ReStore's communicator to the fixed communicator obtained by // MPIX_Comm_shrink() store.updateComm(newComm); // Next, request the data you need on each PE. // requestedBlocks is of type // std::vector<std::pair<ReStore::block_id_t, size_t>> // [ (firstBlockIdOfRange1, numberOfBlocks1), (firstBlockIdOfRange2, numberOfBlocks2), ...] store.pullBlocks( requestedBlocks, // De-serialization function. [...] (const std::byte* dataPtr, size_t size, ReStore::block_id_t blockId) { // ... }); ``` ### Data stored in a std::vector If your data resides in a `std::vector`, you can use the ReStore-provided wrapper. ```cpp #include <restore/core.hpp> #include <restore/restore_vector.hpp> // Create the ReStoreVector wrapper. ReStore::ReStoreVector<YourAwesomeDatatype>> reStoreVectorWrapper( blockSizeInBytes, // Can for example be used to group all dimensions of a single data point. MPI_COMM_WORLD, replicationLevel, blocksPerPermutationRange, // defaults to 4096 paddingValue, // The value used to pad the data; defaults to 0 ); // Submit your data to the ReStore. const auto numBlocksLocal = reStoreVectorWrapper->submitData(referenceToYourDataVector); // After a failure reStoreVectorWrapper.updateComm(newComm); // see above reStoreVectorWrapper.restoreDataAppendPullBlocks( referenceToVectorContainingYourData, // ReStore will append the new data points at the end. requestedBlocks, // see above ); ``` ### A simple load-balancer You can use the ReStore-provided LoadBalancer. If a PE fails, it will help you with calculating the new distribution of blocks to PEs. Each surviving PE will get an equal share of the blocks residing on each PE that failed. This of course works for multiple rounds of failing PEs, too. ```cpp #include <restore/core.hpp> #include <restore/equal_load_balancer.hpp> // Describes, which block range (firstBlockId, numberOfBlocks) resides on which PE. using BlockRange = std::pair<std::pair<ReStore::block_id_t, size_t>, ReStoreMPI::original_rank_t>; using BlockRangeList = std::vector<BlockRange>; // Create the LoadBalancer object. ReStore::EqualLoadBalancer loadBalancer(blockRangeList, numberOfPEs) // After a failure, let the LoadBalancer decide which PE gets which data points: const auto newBlocks = _loadBalancer.getNewBlocksAfterFailureForPullBlocks( ranksDiedSinceLastCall, myRankWhenCreatingTheLoadBalancer ); // You can hand newBlocks to restore.pullBlocks() or the ReStoreVector wrapper. // If everyone completed the restoration successfully, we can commit to the new data distribution. If // there was another PE failure in the meantime, you can re-call getNewBlocksAfterFailureForPullBlocks. _loadBalancer.commitToPreviousCall(); // Further failures, repeat the above steps. ``` ## Publication If you use ReStore in your research, please cite the following paper: ```bibtex @inproceedings{restore, author={Hübner, Lukas and Hespe, Demian and Sanders, Peter and Stamatakis, Alexandros}, booktitle={2022 IEEE/ACM 12th Workshop on Fault Tolerance for HPC at eXtreme Scale (FTXS)}, title={ReStore: In-Memory REplicated STORagE for Rapid Recovery in Fault-Tolerant Algorithms}, year={2022}, volume={}, number={}, pages={24-35}, doi={10.1109/FTXS56515.2022.00008} } ```\n",
                "dependencies": "cmake_minimum_required(VERSION 3.16) if(\"${CMAKE_SOURCE_DIR}\" STREQUAL \"${CMAKE_CURRENT_SOURCE_DIR}\") set(IS_SUBPROJECT OFF) message(STATUS \"Building ReStore as a stand-alone project.\") else() set(IS_SUBPROJECT ON) message(STATUS \"Building ReStore as a subbroject.\") endif() # Use ccache if available find_program(CCACHE_PROGRAM ccache) if((NOT ${IS_SUBPROBJECT}) AND ${CCACHE_PROGRAM}) message(STATUS \"Found ccache; using it to speed up compilation.\") set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE \"${CCACHE_PROGRAM}\") endif() # Project settings project(ReStore VERSION 0.1.0 LANGUAGES CXX) set(CMAKE_CXX_STANDARD 17) # Find and link MPI find_package(MPI REQUIRED) if(MPI_CXX_FOUND) include_directories(BEFORE SYSTEM ${MPI_CXX_INCLUDE_PATH}) link_libraries(${MPI_LIBRARIES}) endif() # Find and link threading library (e.g. pthreads) set(THREADS_PREFER_PTHREAD_FLAG ON) find_package(Threads REQUIRED) link_libraries(Threads::Threads) # Make additinal cmake modules findable list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\") list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/extern/sanitizers-cmake/cmake\") # Require out-of-source builds include(require_out_of_source_builds) require_out_of_source_builds() include(CMakeDependentOption) #include(CMakePackageConfigHelpers) include(CTest) include(GNUInstallDirs) # Load (memory, adress, ...) sanitizer support include(sanitizers) # Enable sorting targets (of external libraries) into folders set_property(GLOBAL PROPERTY USE_FOLDERS ON) # Are we building in DEBUG mode? if(CMAKE_BUILD_TYPE MATCHES Debug) message(\"!!!!! Building in DEBUG mode !!!!!\") # Enable extra checks in the STL add_definitions(-D_GLIBCXX_DEBUG) add_definitions(-D_GLIBCXX_DEBUG_PEDANTIC) endif(CMAKE_BUILD_TYPE MATCHES Debug) option(${PROJECT_NAME}_INSTALL \"Add ${PROJECT_NAME} to the install list\" ON) # Options to disable building the tests, examples, benchmarks, ... option(${PROJECT_NAME}_BUILD_TESTS \"Build unit tests\" ON) option(${PROJECT_NAME}_BUILD_BENCHMARKS \"Build benchmarks\" ON) # Other build options option(${PROJECT_NAME}_SIMULATE_FAILURES \"Simulate node failues (for example when running unit test).\" OFF) option(${PROJECT_NAME}_USE_FTMPI \"Use a fault-tolerant MPI implementation\" ON) option(${PROJECT_NAME}_ID_RANDOMIZATION \"Use randomization of block IDs.\" ON) option(${PROJECT_NAME}_WARNINGS_ARE_ERRORS \"Treat warnings as errors.\" OFF) # Output config if(NOT ${PROJECT_NAME}_SIMULATE_FAILURES AND NOT ${PROJECT_NAME}_USE_FTMPI) message(FATAL_ERROR \"You have to either use a fault-tolerant MPI implementation or simulated failures (or both).\") endif() message(STATUS \"Using simulated failures: ${${PROJECT_NAME}_SIMULATE_FAILURES}\") message(STATUS \"Using a fault-tolerant MPI implementation: ${${PROJECT_NAME}_USE_FTMPI}\") message(STATUS \"Using block id randomization: ${${PROJECT_NAME}_ID_RANDOMIZATION}\") # Extra options when building tests cmake_dependent_option(\"${PROJECT_NAME}_SYSTEM_GTEST\" \"Use googletest version installed on the system\" OFF \"${PROJECT_NAME}_BUILD_TESTS\" OFF) # Library specific settings set(\"NEEDS_GOOGLETEST\" ${PROJECT_NAME}_BUILD_TESTS OR ${PROJECT_NAME}_BUILD_BENCHMARKS) set(\"NEEDS_CPPITERTOOLS\" ${PROJECT_NAME}_BUILD_TESTS OR ${PROJECT_NAME}_BUILD_BENCHMARKS) set(\"NEEDS_CXXOPTS\" ${PROJECT_NAME}_BUILD_BENCHMARKS) set(\"NEEDS_GOOGLEBENCHMARK\" ${PROJECT_NAME}_BUILD_BENCHMARKS) set(\"NEEDS_BACKWARDCPP\" ${PROJECT_NAME}_BUILD_TESTS OR ${PROJECT_NAME}_BUILD_BENCHMARKS) set(\"NEEDS_XXHASH\" ON) if(NEEDS_GOOGLETEST) message(STATUS \"Using googletest library.\") set(\"${PROJECT_NAME}_GTEST_DIR\" \"${CMAKE_CURRENT_LIST_DIR}/extern/googletest\" CACHE PATH \"Path to the googletest source directory\") endif() if(NEEDS_GOOGLEBENCHMARK) message(STATUS \"Using googlebenchmark library.\") set(\"${PROJECT_NAME}_GBENCHMARK_DIR\" \"${CMAKE_CURRENT_LIST_DIR}/extern/googlebenchmark\" CACHE PATH \"Path to the googlebenchmark source directory\") endif() if(NEEDS_CPPITERTOOLS) message(STATUS \"Using cppitertools library.\") set(\"${PROJECT_NAME}_CPPITERTOOLS_DIR\" \"${CMAKE_CURRENT_LIST_DIR}/extern/cppitertools\" CACHE PATH \"Path to the cppitertools source directory\") endif() if(NEEDS_CXXOPTS) message(STATUS \"Using cxxopts library.\") set(\"${PROJECT_NAME}_CXXOPTS_DIR\" \"${CMAKE_CURRENT_LIST_DIR}/extern/cxxopts\" CACHE PATH \"Path to the cxxopts source directory\") endif() if(NEEDS_BACKWARDCPP) message(STATUS \"Using backwardcpp library.\") set(\"${PROJECT_NAME}_BACKWARDCPP_DIR\" \"${CMAKE_CURRENT_LIST_DIR}/extern/backward-cpp\" CACHE PATH \"Path to the backward-cpp source directory\") option(${PROJECT_NAME}_BACKWARD_ENABLED \"Enable pretty printing of stack traces when a test case fails.\" ON) else() option(${PROJECT_NAME}_BACKWARD_ENABLED \"Enable pretty printing of stack traces when a test case fails.\" OFF) endif() # Organize targets into folders set_property(GLOBAL PROPERTY USE_FOLDERS ON) # The target to be linked against by other targets. This library is an # interface target and as such does not generate any artefacts. It rather sets # include directories and required compiler flags. add_library(\"${PROJECT_NAME}\" INTERFACE) target_sources(\"${PROJECT_NAME}\" INTERFACE ${CMAKE_CURRENT_SOURCE_DIR}/include) target_include_directories(\"${PROJECT_NAME}\" INTERFACE ${CMAKE_CURRENT_SOURCE_DIR}/include) ### Compiler and linker settings ### # This interface reflects the requirements to the compiler for building targets # linking against. add_library(\"${PROJECT_NAME}_compile_requirements\" INTERFACE) # We just want c++17 or above target_compile_features( \"${PROJECT_NAME}_compile_requirements\" INTERFACE cxx_std_17 ) target_link_libraries(\"${PROJECT_NAME}\" INTERFACE \"${PROJECT_NAME}_compile_requirements\") # The namespace alias can be used as link target if this project is a # subproject. add_library(\"${PROJECT_NAME}::${PROJECT_NAME}\" ALIAS \"${PROJECT_NAME}\") # Not added as link target to avoid propagation of warning # flags. Only to be used by internal targets that compile the library. add_library(\"${PROJECT_NAME}_warnings\" INTERFACE) # TODO: Use CheckCXXCompilerFlag for this? list( APPEND WARNING_FLAGS \"-Wall\" \"-Wextra\" \"-Wconversion\" \"-Wnon-virtual-dtor\" \"-Woverloaded-virtual\" \"-Wshadow\" \"-Wsign-conversion\" \"-Wundef\" \"-Wunreachable-code\" \"-Wunused\" ) if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\") list( APPEND WARNING_FLAGS \"-Wcast-align\" \"-Wpedantic\" ) endif() if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\") list( APPEND WARNING_FLAGS \"-Wcast-align\" \"-Wpedantic\" \"-Wnoexcept\" \"-Wsuggest-attribute=const\" \"-Wsuggest-attribute=noreturn\" \"-Wsuggest-override\" ) endif() if(${PROJECT_NAME}_WARNINGS_ARE_ERRORS) list( APPEND WARNING_FLAGS \"-Werror\" ) endif() target_compile_options( \"${PROJECT_NAME}_warnings\" INTERFACE $<BUILD_INTERFACE:${WARNING_FLAGS}> ) # Populate some of the options to the source code if (${PROJECT_NAME}_SIMULATE_FAILURES) target_compile_definitions(${PROJECT_NAME}_compile_requirements INTERFACE SIMULATE_FAILURES) endif() if (${PROJECT_NAME}_USE_FTMPI) target_compile_definitions(${PROJECT_NAME}_compile_requirements INTERFACE USE_FTMPI) endif() if (${PROJECT_NAME}_ID_RANDOMIZATION) target_compile_definitions(${PROJECT_NAME}_compile_requirements INTERFACE ID_RANDOMIZATION) target_compile_definitions(${PROJECT_NAME}_compile_requirements INTERFACE DENSE_ALL_TO_ALL_IN_SUBMIT_BLOCKS) endif() ### Libraries ### # Load and include cppitertools if(NEEDS_CPPITERTOOLS) if(NOT EXISTS \"${${PROJECT_NAME}_CPPITERTOOLS_DIR}/CMakeLists.txt\") message(FATAL_ERROR \"Could not find cppitertools in ${${PROJECT_NAME}_CPPITERTOOLS_DIR}\") endif() message(STATUS \"Configuring cppitertools...\") set(\"ENV{cppitertools_INSTALL_CMAKE_DIR}\" \"share\") # default value; supress the warning message add_subdirectory(\"${${PROJECT_NAME}_CPPITERTOOLS_DIR}\") message(STATUS \"cppitertools configured.\") endif() # Load and include cxxopts if(NEEDS_CXXOPTS) if(NOT EXISTS \"${${PROJECT_NAME}_CXXOPTS_DIR}/CMakeLists.txt\") message(FATAL_ERROR \"Could not find cxxopts in ${${PROJECT_NAME}_CXXOPTS_DIR}\") endif() message(STATUS \"Configuring cxxopts...\") set(\"ENV{cxxopts_INSTALL_CMAKE_DIR}\" \"share\") # default value; supress the warning message add_subdirectory(\"${${PROJECT_NAME}_CXXOPTS_DIR}\") message(STATUS \"cxxopts configured.\") endif() # Load and include googletest if(NEEDS_GOOGLETEST) if(${PROJECT_NAME}_SYSTEM_GTEST) find_package(GTest REQUIRED) else() if(NOT EXISTS \"${${PROJECT_NAME}_GTEST_DIR}/CMakeLists.txt\") message(FATAL_ERROR \"Could not find googletest in ${${PROJECT_NAME}_GTEST_DIR}\") endif() message(STATUS \"Configuring googletest...\") add_subdirectory(\"${${PROJECT_NAME}_GTEST_DIR}\" EXCLUDE_FROM_ALL) list(APPEND CMAKE_MODULE_PATH \"${${PROJECT_NAME}_GTEST_DIR}/contrib\") message(STATUS \"googletest configured.\") endif() include(GoogleTest) set_target_properties(gtest gmock PROPERTIES FOLDER googletest) endif() # Load and include googlebenchmark if(NEEDS_GOOGLEBENCHMARK) message(STATUS \"Configuring googlebenchmark...\") set(BENCHMARK_ENABLE_TESTING OFF CACHE BOOL \"Suppressing benchmark's tests\" FORCE) add_subdirectory(\"${${PROJECT_NAME}_GBENCHMARK_DIR}\" EXCLUDE_FROM_ALL) set_target_properties(benchmark PROPERTIES FOLDER googlebenchmark) message(STATUS \"googlebenchmark configured.\") endif() # Load and include backward-cpp if (NEEDS_BACKWARDCPP) message(STATUS \"Configuring backward-cpp...\") set(Backward_DIR \"${PROJECT_SOURCE_DIR}/extern/backward-cpp\") find_package(Backward) message(STATUS \"backward-cpp configured.\") endif() # Load and include xxHash message(STATUS \"Configuring xxHash...\") set(${PROJECT_NAME}_XXHASH_DIR \"${CMAKE_CURRENT_LIST_DIR}/extern/xxHash\" CACHE PATH \"Path to the xxHash source directory\") set(BUILD_SHARED_LIBS OFF) set(XXHASH_BUILD_ENABLE_INLINE_API ON) set(XXHASH_BUILD_XXHSUM OFF) add_subdirectory(\"${${PROJECT_NAME}_XXHASH_DIR}/cmake_unofficial\" \"${${PROJECT_NAME}_XXHASH_DIR/build}\" EXCLUDE_FROM_ALL) # Link xxHash with ReStore target_link_libraries(\"${PROJECT_NAME}\" INTERFACE xxhash) ### Subdirectories ### # Build the unit tests if(${PROJECT_NAME}_BUILD_TESTS) message(STATUS \"Building unit tests: YES\") set(TEST_RUNNER_PARAMS \"\" CACHE STRING \"Options added to the test runner\") enable_testing() add_subdirectory(\"${CMAKE_CURRENT_LIST_DIR}/tests\") else() message(STATUS \"Building unit tests: NO\") endif() # Build the benchmarks if(${PROJECT_NAME}_BUILD_BENCHMARKS) message(STATUS \"Building benchmarks: YES\") add_subdirectory(\"${CMAKE_CURRENT_LIST_DIR}/benchmark\") else() message(STATUS \"Building benchmarks: NO\") endif()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rgreat",
            "repo_link": "https://github.com/jokergoo/rGREAT",
            "content": {
                "codemeta": "",
                "readme": "# GREAT Analysis - Functional Enrichment on Genomic Regions [![R-CMD-check](https://github.com/jokergoo/rGREAT/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/rGREAT/actions) [![codecov](https://img.shields.io/codecov/c/github/jokergoo/rGREAT.svg)](https://codecov.io/github/jokergoo/rGREAT) [![bioc](https://bioconductor.org/shields/downloads/devel/rGREAT.svg)](https://bioconductor.org/packages/stats/bioc/rGREAT/) [![bioc](http://www.bioconductor.org/shields/years-in-bioc/rGREAT.svg)](http://bioconductor.org/packages/devel/bioc/html/rGREAT.html) **GREAT** ([Genomic Regions Enrichment of Annotations Tool](http://great.stanford.edu)) is a type of functional enrichment analysis directly performed on genomic regions. This package implements the GREAT algorithm (the local GREAT analysis), also it supports directly interacting with the GREAT web service (the online GREAT analysis). Both analysis can be viewed by a Shiny application. ## Install **rGREAT** is available on Bioconductor (http://bioconductor.org/packages/devel/bioc/html/rGREAT.html) ```r if(!requireNamespace(\"BiocManager\", quietly = TRUE)) { install.packages(\"BiocManager\") } BiocManager::install(\"rGREAT\") ``` If you want the latest version, install it directly from GitHub: ```r library(devtools) install_github(\"jokergoo/rGREAT\") ``` ## Citation Zuguang Gu, et al., rGREAT: an R/Bioconductor package for functional enrichment on genomic regions. Bioinformatics, https://doi.org/10.1093/bioinformatics/btac745 ## Online GREAT analysis With online GREAT analysis, the input regions will be directly submitted to GREAT server, and the results are automatically retrieved from GREAT server. ```r set.seed(123) gr = randomRegions(nr = 1000, genome = \"hg19\") job = submitGreatJob(gr) tbl = getEnrichmentTables(job) ``` ## Local GREAT analysis **rGREAT** also implements the GREAT algorithms locally and it can be seamlessly integrated to the Bioconductor annotation ecosystem. This means, theoretically, with **rGREAT**, it is possible to perform GREAT analysis with any organism and with any type of gene set collection / ontology ```r res = great(gr, \"MSigDB:H\", \"TxDb.Hsapiens.UCSC.hg19.knownGene\") tb = getEnrichmentTable(res) ``` To apply `great()` on other organisms, set the `biomart_dataset` argument: ```r # giant panda great(gr, \"GO:BP\", biomart_dataset = \"amelanoleuca_gene_ensembl\") ``` ## License MIT @ Zuguang Gu\n",
                "dependencies": "Package: rGREAT Type: Package Title: GREAT Analysis - Functional Enrichment on Genomic Regions Version: 2.9.2 Date: 2025-03-13 Authors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"), comment = c('ORCID'=\"0000-0002-7395-8709\")) Depends: R (>= 4.0.0), GenomicRanges, IRanges, methods Imports: graphics, rjson, GetoptLong (>= 0.0.9), RCurl, utils, stats, GlobalOptions, shiny, DT, GenomicFeatures, digest, GO.db, progress, circlize, AnnotationDbi, TxDb.Hsapiens.UCSC.hg19.knownGene, TxDb.Hsapiens.UCSC.hg38.knownGene, org.Hs.eg.db, RColorBrewer, S4Vectors, GenomeInfoDb, foreach, doParallel, Rcpp Suggests: testthat (>= 0.3), knitr, rmarkdown, BiocManager, org.Mm.eg.db, msigdbr, KEGGREST, reactome.db Enhances: BioMartGOGeneSets, UniProtKeywords VignetteBuilder: knitr biocViews: GeneSetEnrichment, GO, Pathways, Software, Sequencing, WholeGenome, GenomeAnnotation, Coverage Description: GREAT (Genomic Regions Enrichment of Annotations Tool) is a type of functional enrichment analysis directly performed on genomic regions. This package implements the GREAT algorithm (the local GREAT analysis), also it supports directly interacting with the GREAT web service (the online GREAT analysis). Both analysis can be viewed by a Shiny application. rGREAT by default supports more than 600 organisms and a large number of gene set collections, as well as self-provided gene sets and organisms from users. Additionally, it implements a general method for dealing with background regions. URL: https://github.com/jokergoo/rGREAT, http://great.stanford.edu/public/html/ License: MIT + file LICENSE LinkingTo: Rcpp\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ribodetector",
            "repo_link": "https://github.com/hzi-bifo/RiboDetector",
            "content": {
                "codemeta": "",
                "readme": "## RiboDetector - Accurate and rapid RiboRNA sequences Detector based on deep learning ### About Ribodetector <img src=\"RiboDetector_logo.png\" width=\"600\" /> `RiboDetector` is a software developed to accurately yet rapidly detect and remove rRNA sequences from metagenomeic, metatranscriptomic, and ncRNA sequencing data. It was developed based on LSTMs and optimized for both GPU and CPU usage to achieve a **10** times on CPU and **50** times on a consumer GPU faster runtime compared to the current state-of-the-art software. Moreover, it is very accurate, with ~**10** times fewer false classifications. Finally, it has a low level of bias towards any GO functional groups. ### Prerequirements #### 1. Create `conda` env and install `Python v3.8` To be able to use `RiboDetector`, you need to install `Python v3.8` or `v3.9` (make sure you have version `3.8` because `3.7` cannot serialize a string larger than 4GiB) with `conda`: ```shell conda create -n ribodetector python=3.8 conda activate ribodetector ``` #### 2. Install `pytorch` in the ribodetector env if GPU is available To install `pytorch` compatible with your CUDA version, please fellow this instruction: https://pytorch.org/get-started/locally/. Our code was tested with `pytorch v1.7`, `v1.7.1`, `v1.10.2`. Note: you can skip this step if you don't use GPU ### Installation #### Using pip ```shell pip install ribodetector ``` #### Using conda ```shell conda install -c bioconda ribodetector ``` ### Usage #### GPU mode #### Example ```shell ribodetector -t 20 \\ -l 100 \\ -i inputs/reads.1.fq.gz inputs/reads.2.fq.gz \\ -m 10 \\ -e rrna \\ --chunk_size 256 \\ -o outputs/reads.nonrrna.1.fq outputs/reads.nonrrna.2.fq ``` The command lind above excutes ribodetector for paired-end reads with mean length 100 using GPU and 20 CPU cores. The input reads do not need to be same length. RiboDetector supports reads with variable length. Setting `-l` to the mean read length is recommended. #### Full help ```shell usage: ribodetector [-h] [-c CONFIG] [-d DEVICEID] -l LEN -i [INPUT [INPUT ...]] -o [OUTPUT [OUTPUT ...]] [-r [RRNA [RRNA ...]]] [-e {rrna,norrna,both,none}] [-t THREADS] [-m MEMORY] [--chunk_size CHUNK_SIZE] [-v] rRNA sequence detector optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG Path of config file -d DEVICEID, --deviceid DEVICEID Indices of GPUs to enable. Quotated comma-separated device ID numbers. (default: all) -l LEN, --len LEN Sequencing read length (mean length). Note: the accuracy reduces for reads shorter than 40. -i [INPUT [INPUT ...]], --input [INPUT [INPUT ...]] Path of input sequence files (fasta and fastq), the second file will be considered as second end if two files given. -o [OUTPUT [OUTPUT ...]], --output [OUTPUT [OUTPUT ...]] Path of the output sequence files after rRNAs removal (same number of files as input). (Note: 2 times slower to write gz files) -r [RRNA [RRNA ...]], --rrna [RRNA [RRNA ...]] Path of the output sequence file of detected rRNAs (same number of files as input) -e {rrna,norrna,both,none}, --ensure {rrna,norrna,both,none} Ensure which classificaion has high confidence for paired end reads. norrna: output only high confident non-rRNAs, the rest are clasified as rRNAs; rrna: vice versa, only high confident rRNAs are classified as rRNA and the rest output as non-rRNAs; both: both non-rRNA and rRNA prediction with high confidence; none: give label based on the mean probability of read pair. (Only applicable for paired end reads, discard the read pair when their predicitons are discordant) -t THREADS, --threads THREADS number of threads to use. (default: 10) -m MEMORY, --memory MEMORY Amount (GB) of GPU RAM. (default: 12) --chunk_size CHUNK_SIZE Use this parameter when having low memory. Parsing the file in chunks. Not needed when free RAM >=5 * your_file_size (uncompressed, sum of paired ends). When chunk_size=256, memory=16 it will load 256 * 16 * 1024 reads each chunk (use ~20 GB for 100bp paired end). --log LOG Log file name -v, --version Show program's version number and exit ``` #### CPU mode #### Example ```shell ribodetector_cpu -t 20 \\ -l 100 \\ -i inputs/reads.1.fq.gz inputs/reads.2.fq.gz \\ -e rrna \\ --chunk_size 256 \\ -o outputs/reads.nonrrna.1.fq outputs/reads.nonrrna.2.fq ``` The above command line excutes ribodetector for paired-end reads with mean length 100 using 20 CPU cores. The input reads do not need to be same length. RiboDetector supports reads with variable length. Setting `-l` to the mean read length is recommended. If you need to save the log into a file, you can specify it with `--log <logfile>` Note: when using **SLURM** job submission system, you need to specify `--cpus-per-task` to the number you CPU cores you need and set `--threads-per-core` to 1. #### Full help ```shell usage: ribodetector_cpu [-h] [-c CONFIG] -l LEN -i [INPUT [INPUT ...]] -o [OUTPUT [OUTPUT ...]] [-r [RRNA [RRNA ...]]] [-e {rrna,norrna,both,none}] [-t THREADS] [--chunk_size CHUNK_SIZE] [-v] rRNA sequence detector optional arguments: -h, --help show this help message and exit -c CONFIG, --config CONFIG Path of config file -l LEN, --len LEN Sequencing read length (mean length). Note: the accuracy reduces for reads shorter than 40. -i [INPUT [INPUT ...]], --input [INPUT [INPUT ...]] Path of input sequence files (fasta and fastq), the second file will be considered as second end if two files given. -o [OUTPUT [OUTPUT ...]], --output [OUTPUT [OUTPUT ...]] Path of the output sequence files after rRNAs removal (same number of files as input). (Note: 2 times slower to write gz files) -r [RRNA [RRNA ...]], --rrna [RRNA [RRNA ...]] Path of the output sequence file of detected rRNAs (same number of files as input) -e {rrna,norrna,both,none}, --ensure {rrna,norrna,both,none} Ensure which classificaion has high confidence for paired end reads. norrna: output only high confident non-rRNAs, the rest are clasified as rRNAs; rrna: vice versa, only high confident rRNAs are classified as rRNA and the rest output as non-rRNAs; both: both non-rRNA and rRNA prediction with high confidence; none: give label based on the mean probability of read pair. (Only applicable for paired end reads, discard the read pair when their predicitons are discordant) -t THREADS, --threads THREADS number of threads to use. (default: 20) --chunk_size CHUNK_SIZE chunk_size * 1024 reads to load each time. When chunk_size=1000 and threads=20, consumming ~20G memory, better to be multiples of the number of threads.. --log LOG Log file name -v, --version Show program's version number and exit ``` **Note**: RiboDetector uses multiprocessing with shared memory, thus the memory use of a single process indicated in `htop` or `top` is actually the total memory used by RiboDector. Some job submission system like SGE mis-calculated the total memory use by adding up the memory use of all process. If you see this do not worry it will cause out of memory issue. <!-- ### Benchmarks We benchmarked five different rRNA detection methods including RiboDetector on 8 benchmarking datasets as following: - 20M paired end reads simulated based on rRNA sequences from Silva database, those sequences are distinct from sequences used for training and validation. - 20M paired end reads simulated based on 500K CDS sequences from OMA databases. - 27,206,792 paired end reads simulated based on 13,848 viral gene sequences downloaded from ENA database. - 7,917,920 real paired end amplicon sequencing reads targeting V1-V2 region of 16s rRNA genes from oral microbiome study. - 6,330,381 paired end reads simulated from 106,880 human noncoding RNA sequences. - OMA_Silva dataset in figure C contains 1,027,675 paired end reads simulated on CDS sequences which share similarity to rRNA genes, the sequences with identity >=98% and query coverage >=90% to rRNAs were excluded. - HOMD dataset in figure C has 100,558 paired end reads simulated on CDS sequences from HOMD database which share similarity to the FP sequences of three tools, again sequences with identity >=98% and query coverage >=90% to rRNAs were excluded. - GO_FP_N_02 in figure C consisting of 678,250 paired end reads was simulated from OMA sequences which have the GO with FP reads ratio >=0.2 on 20M mRNA reads dataset for BWA, RiboDetector or SortMeRNA. ![Benchmarking the performance and runtime of different rRNA sequences detection methods](./benchmarks/benchmarks.jpg) In the above figures, the definitions of *FPNR* and *FNR* are: <img src=\"https://render.githubusercontent.com/render/math?math=\\large FPNR=100\\frac{false \\:predictions}{total \\: sequences}\"> <img src=\"https://render.githubusercontent.com/render/math?math=\\large FNR=100\\frac{false \\:negatives}{total \\:positives}\"> RiboDetector has a very high generalization ability and is capable of detecting novel rRNA sequences (Fig. C). --> ### FAQ 1. What should I set for `-l` when I have reads with variable length? > You can set the `-l` parameter to the mean read length if you have reads with variable length. The mean read length can be computed with `seqkit stats`. This parameter tells how many bases will be used to capture the sequences patterns for classification. 2. How does `-e` parameter work? What should I set (`rrna`, `norrna`, `none`, `both`)? > This parameter is only necessary for paired end reads. When setting to `rrna`, the paired read ends will be predicted as rRNA only if both ends were classified as rRNA. If you want to identify or remove rRNAs with high confidence, you should set it to `rrna`. Conversely, `norrna` will predict the read pair as nonrRNA only if both ends were classified as nonrRNA. This setting will only output nonrRNAs with high confidence. `both` will discard the read pairs with two ends classified inconsistently, only pairs with concordant prediction will be reported in the corresponding output. `none` will take the mean of the probabilities of both ends and decide the final prediction. This is also the default setting. 3. I have very large input file but limited memory, what should I do? > You can set the `--chunk_size` parameter which specifies how many reads the software load into memory once. 4. What should I do if RiboDetector hangs with SLURM? > The most likely cause is that the requested computational resource is not sufficient for the input file. You need to make sure you specified `--cpus-per-task` to the number you CPU cores you want to use and set `--threads-per-core` to 1 in the SLURM submission script or command. If the issue remains, you can try to reduce the memory use by setting `--chunk_size` parameter in `ribodetector` or `ribodetector_cpu` command. ### Citation Deng ZL, Münch PC, Mreches R, McHardy AC. Rapid and accurate detection of ribosomal RNA sequences using deep learning. <i>Nucleic Acids Research</i>. 2022. (https://doi.org/10.1093/nar/gkac112) ### Acknowledgements The scripts from the `base` dir were from the template [pytorch-template ](https://github.com/victoresque/pytorch-template) by [Victor Huang](https://github.com/victoresque) and other [contributors](https://github.com/victoresque/pytorch-template/graphs/contributors).\n",
                "dependencies": "#!/usr/bin/env python # -*- coding: UTF-8 -*- from setuptools import find_packages, setup with open(\"README.md\", \"r\", encoding='utf-8') as fh: long_description = fh.read() required = [ \"pandas\", \"tqdm\", \"numpy\", \"biopython\", \"onnxruntime >= 1.10.0, <= 1.15.1\", \"torch >= 1.7.1, <= 1.12.1\", ] setup( name=\"ribodetector\", version=\"0.3.1\", python_requires=\">=3.8, <=3.12\", author=\"Z-L Deng\", author_email=\"dawnmsg@gmail.com\", description=\"Accurate and rapid RiboRNA sequences Detector based on deep learning.\", license=\"GPL-3 License\", long_description=long_description, long_description_content_type=\"text/markdown\", url=\"https://github.com/hzi-bifo/RiboDetector\", packages=find_packages(include=[\"ribodetector\", \"ribodetector.*\"]), package_data={'': ['*.json', '*.yaml', '*.pth', '*.onnx']}, include_package_data=True, classifiers=[ \"Topic :: Scientific/Engineering :: Artificial Intelligence\", \"Topic :: Scientific/Engineering :: Bio-Informatics\", \"Intended Audience :: Science/Research\", \"Programming Language :: Python :: 3\", \"Operating System :: OS Independent\", ], entry_points={ 'console_scripts': [ 'ribodetector = ribodetector.detect:main', 'ribodetector_cpu = ribodetector.detect_cpu:main', ] }, zip_safe=True, install_requires=required )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rtlola-frontend",
            "repo_link": "https://github.com/reactive-systems/RTLola-Frontend",
            "content": {
                "codemeta": "",
                "readme": "# RTLola Frontend [![Crate](https://img.shields.io/crates/v/rtlola-frontend.svg)](https://crates.io/crates/rtlola-frontend) [![API](https://docs.rs/rtlola-frontend/badge.svg)](https://docs.rs/rtlola-frontend) [![License](https://img.shields.io/crates/l/rtlola-frontend)](https://crates.io/crates/rtlola-frontend) RTLola is a stream-based runtime verification framework. It parses an RTLola specification, analyses it, and generates executable monitors for it. The framework is separated into a front-end and several back-ends. This crate summarizes the entire RTLola front-end, which includes several sub-modules: * A parser for RTLola specifications: [rtlola-parser](https://crates.io/crates/rtlola-parser) * The RTLola high-level intermediate representation including a strong static analysis: [rtlola-hir](https://crates.io/crates/rtlola-hir) * The RTLola error reporting: [rtlola-reporting](https://crates.io/crates/rtlola-reporting) * Procedural macros: [rtlola-macros](https://crates.io/crates/rtlola-macros) # Copyright Copyright (C) CISPA - Helmholtz Center for Information Security 2021-2023. Authors: Jan Baumeister, Florian Kohn, Stefan Oswald, Frederik Scheerer, Malte Schledjewski, Maximilian Schwenger. Based on original work at Universität des Saarlandes (C) 2020. Authors: Jan Baumeister, Florian Kohn, Malte Schledjewski, Maximilian Schwenger, Marvin Stenger, and Leander Tentrup.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/rtlola-interpreter",
            "repo_link": "https://github.com/reactive-systems/RTLola-Interpreter",
            "content": {
                "codemeta": "",
                "readme": "![RTLola logo](https://pages.cispa.de/rtlola/assets/img/logos/rtlola-logo-ultrawide-blue.png) # RTLola Interpreter Repository RTLola is a runtime monitoring framework. It consists of a parser, analyzer, and interpreter for the RTLola specification language. The project is split into two crates. The interpreter crate provides a library for interpreting RTLola specifications. The CLI crate provides a command line interface to the interpreter. ## RTLola Interpreter [![Crate](https://img.shields.io/crates/v/rtlola-interpreter.svg)](https://crates.io/crates/rtlola-interpreter) [![API](https://docs.rs/rtlola-interpreter/badge.svg)](https://docs.rs/rtlola-interpreter) [![License](https://img.shields.io/crates/l/rtlola-interpreter)](https://crates.io/crates/rtlola-interpreter) This library crate provides two APIs to evaluate RTLola specifications through interpretation. ## RTLola Interpreter Input Plugins [![Crate](https://img.shields.io/crates/v/rtlola-input-plugins.svg)](https://crates.io/crates/rtlola-input-plugins) [![API](https://docs.rs/rtlola-input-plugins/badge.svg)](https://docs.rs/rtlola-input-plugins) [![License](https://img.shields.io/crates/l/rtlola-input-plugins)](https://crates.io/crates/rtlola-input-plugins) This crate contains the different input methods like the csv and pcap parser for the interpreter. ## RTLola Interpreter CLI [![Crate](https://img.shields.io/crates/v/rtlola-cli.svg)](https://crates.io/crates/rtlola-cli) [![API](https://docs.rs/rtlola-cli/badge.svg)](https://docs.rs/rtlola-cli) [![License](https://img.shields.io/crates/l/rtlola-cli)](https://crates.io/crates/rtlola-cli) This crate contains a CLI interface to the interpreter capable of reading csv and pcap files. # Copyright Copyright (C) CISPA - Helmholtz Center for Information Security 2024. Authors: Jan Baumeister, Florian Kohn, Stefan Oswald, Frederik Scheerer, Maximilian Schwenger. Based on original work at Universität des Saarlandes (C) 2020. Authors: Jan Baumeister, Florian Kohn, Malte Schledjewski, Maximilian Schwenger, Marvin Stenger, and Leander Tentrup.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/s2downloader",
            "repo_link": "https://git.gfz-potsdam.de/fernlab/products/data-portal/s2downloader",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/sampledb",
            "repo_link": "https://github.com/sciapp/sampledb",
            "content": {
                "codemeta": "",
                "readme": "<img src=\"https://raw.githubusercontent.com/sciapp/sampledb/develop/docs/static/img/logo.svg\" align=\"right\" width=\"60\" height=\"60\" /> # SampleDB [![MIT license](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE) [![DOI](https://zenodo.org/badge/221237572.svg)](https://zenodo.org/badge/latestdoi/221237572) [![DOI](https://joss.theoj.org/papers/10.21105/joss.02107/status.svg)](https://doi.org/10.21105/joss.02107) SampleDB is a web-based sample and measurement metadata database. ## Documentation You can find the documentation for the current release at https://scientific-it-systems.iffgit.fz-juelich.de/SampleDB/. ## Getting Started We recommend using our pre-built Docker images for setting up `SampleDB`. You will need two containers, one for a PostgreSQL database and another for SampleDB itself, and a directory to store all files in. If you would like to set up a development version of SampleDB instead, please see the [contribution guide](https://github.com/sciapp/sampledb/blob/develop/CONTRIBUTING.md). If you do not have Docker installed yet, please [install Docker](https://docs.docker.com/engine/install/). ### Using docker-compose First, get the [docker-compose.yml](https://raw.githubusercontent.com/sciapp/sampledb/develop/docker-compose.yml.dist) configuration file. You can git clone this repo or just get the file: ```bash curl https://raw.githubusercontent.com/sciapp/sampledb/develop/docker-compose.yml.dist --output docker-compose.yml ``` Then simply bring everything up with: ```bash docker compose up -d ``` ### Using docker commands First, start your database container: ```bash docker run \\ -d \\ -e POSTGRES_PASSWORD=password \\ -e PGDATA=/var/lib/postgresql/data/pgdata \\ -v `pwd`/pgdata:/var/lib/postgresql/data/pgdata:rw \\ --restart=always \\ --name sampledb-postgres \\ postgres:15 ``` Next, start the SampleDB container: ```bash docker run \\ -d \\ --link sampledb-postgres \\ -e SAMPLEDB_CONTACT_EMAIL=sampledb@example.com \\ -e SAMPLEDB_MAIL_SERVER=mail.example.com \\ -e SAMPLEDB_MAIL_SENDER=sampledb@example.com \\ -e SAMPLEDB_ADMIN_PASSWORD=password \\ -e SAMPLEDB_SQLALCHEMY_DATABASE_URI=postgresql+psycopg2://postgres:password@sampledb-postgres:5432/postgres \\ --restart=always \\ --name sampledb \\ -p 8000:8000 \\ sciapp/sampledb:0.30.0 ``` ### Once it's started This will start a minimal SampleDB installation at `http://localhost:8000` and allow you to sign in with the username `admin` and the password `password` (which you should change immediately after signing in). To learn how to further set up SampleDB, please follow the rest of the [Getting Started guide](https://scientific-it-systems.iffgit.fz-juelich.de/SampleDB/administrator_guide/getting_started.html). ## Contributing If you want to improve SampleDB, please read the [contribution guide](https://github.com/sciapp/sampledb/blob/develop/CONTRIBUTING.md) for a few notes on how to report issues or submit changes. ## Support If you run into any issues setting up or running SampleDB, please [open an issue on GitHub](https://github.com/sciapp/sampledb/issues/new). You can also subscribe to the [SampleDB mailing list](https://lists.fz-juelich.de/postorius/lists/sampledb.lists.fz-juelich.de/) to learn about new features and to discuss any questions regarding SampleDB.\n",
                "dependencies": "annotated-types==0.7.0 APScheduler==3.10.4 autocommand==2.2.2 babel==2.17.0 bcrypt==4.2.1 beautifulsoup4==4.13.3 bleach==6.2.0 blinker==1.9.0 Brotli==1.1.0 certifi==2025.1.31 cffi==1.17.1 chardet==5.2.0 charset-normalizer==3.4.1 cheroot==10.0.1 CherryPy==18.10.0 click==8.1.8 colorhash==2.0.0 configparser==7.1.0 cryptography==43.0.3 cryptojwt==1.9.3 cssselect2==0.7.0 defusedxml==0.7.1 dnspython==2.7.0 elementpath==4.7.0 email_validator==2.2.0 fido2==1.2.0 Flask==3.1.0 flask-babel==4.0.0 Flask-HTTPAuth==4.8.0 Flask-Login==0.6.3 Flask-Mail==0.10.0 Flask-MonitoringDashboard @ git+https://github.com/FlorianRhiem/Flask-MonitoringDashboard.git@29130f76a4dda6940bb067e449797c3600a561ae flask-orjson==2.0.0 Flask-SQLAlchemy==3.1.1 Flask-WTF==1.2.2 flexcache==0.3 flexparser==0.4 fonttools==4.56.0 furl==2.1.3 greenlet==3.1.1 idna==3.10 itsdangerous==2.2.0 jaraco.collections==5.1.0 jaraco.context==6.0.1 jaraco.functools==4.1.0 jaraco.text==4.0.0 Jinja2==3.1.5 kaleido==0.2.1 ldap3==2.9.1 Markdown==3.7 MarkupSafe==3.0.2 py-minisign @ git+https://github.com/x13a/py-minisign.git more-itertools==10.6.0 narwhals==1.26.0 numpy==2.2.2 orderedmultidict==1.0.1 orjson==3.10.15 packaging==24.2 pillow==11.1.0 Pint==0.24.4 platformdirs==4.3.6 plotly==6.0.1 portend==3.2.0 psutil==6.1.1 psycopg2-binary==2.9.10 pyasn1==0.6.1 pycparser==2.22 pydantic==2.10.6 pydantic_core==2.27.2 pydyf==0.11.0 pyOpenSSL==24.2.1 pyotp==2.9.0 pyphen==0.17.2 pysaml2==7.5.2 python-dateutil==2.9.0.post0 pytz==2025.1 qrcode==8.0 requests==2.32.3 scipy==1.15.1 setuptools==75.8.0 simple_openid_connect @ git+https://github.com/timhallmann/py_simple_openid_connect.git@9125e1dc9da9a0233a5f5b504a7416006afc5e70 six==1.17.0 soupsieve==2.6 SQLAlchemy==2.0.38 tempora==5.8.0 tinycss2==1.4.0 tinyhtml5==2.0.0 typing_extensions==4.12.2 tzlocal==2.0.0 urllib3==2.3.0 weasyprint==64.0 webencodings==0.5.1 Werkzeug==3.1.3 WTForms==3.2.1 xmlschema==2.5.1 zc.lockfile==3.0.post1 zopfli==0.2.3.post1\nimport importlib.util from setuptools import setup, find_packages import os.path setup_directory = os.path.abspath(os.path.dirname(__file__)) with open(os.path.join(setup_directory, 'README.md')) as readme_file: long_description = readme_file.read() with open(os.path.join(setup_directory, 'requirements.txt')) as requirements_file: requirements = requirements_file.readlines() version_file_path = os.path.join(setup_directory, 'sampledb', 'version.py') spec = importlib.util.spec_from_file_location(\"version\", version_file_path) version = importlib.util.module_from_spec(spec) spec.loader.exec_module(version) setup( name='sampledb', version=version.__version__, description='A sample and measurement metadata database', long_description=long_description, long_description_content_type='text/markdown', url='https://github.com/sciapp/sampledb', author='Florian Rhiem', author_email='f.rhiem@fz-juelich.de', license='MIT', classifiers=[ 'Development Status :: 5 - Production/Stable', 'Intended Audience :: Developers', 'License :: OSI Approved :: MIT License', 'Programming Language :: Python :: 3', 'Framework :: Flask', 'Topic :: Scientific/Engineering', ], packages=find_packages(exclude=['tests', 'tests.*']), install_requires=requirements, package_data={ 'sampledb': [ 'static/*/*.*', 'static/*/*/*.*', 'static/*/*/*/*.*' ], 'sampledb.logic': [ 'unit_definitions.txt' ], 'sampledb.frontend': [ 'templates/*/*.*', 'templates/*/*/*.*', 'templates/*/*/*/*.*' ] } )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/saqc",
            "repo_link": "https://git.ufz.de/rdm-software/saqc",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/scanpy",
            "repo_link": "https://github.com/scverse/scanpy",
            "content": {
                "codemeta": "",
                "readme": "[![Stars](https://img.shields.io/github/stars/scverse/scanpy?style=flat&logo=GitHub&color=yellow)](https://github.com/scverse/scanpy/stargazers) [![PyPI](https://img.shields.io/pypi/v/scanpy)](https://pypi.org/project/scanpy) [![PyPI Downloads](https://img.shields.io/pepy/dt/scanpy?logo=pypi)](https://pepy.tech/project/scanpy) [![Conda Forge](https://img.shields.io/conda/vn/conda-forge/scanpy) ![Conda Forge Downloads](https://img.shields.io/conda/dn/conda-forge/scanpy?logo=condaforge)](https://anaconda.org/conda-forge/scanpy) [![Docs](https://readthedocs.com/projects/icb-scanpy/badge/?version=latest)](https://scanpy.readthedocs.io) [![CI](https://github.com/scverse/scanpy/actions/workflows/ci.yml/badge.svg)](https://github.com/scverse/scanpy/actions/workflows/ci.yml) [![Discourse topics](https://img.shields.io/discourse/posts?color=yellow&logo=discourse&server=https%3A%2F%2Fdiscourse.scverse.org)](https://discourse.scverse.org/) [![Chat](https://img.shields.io/badge/zulip-join_chat-%2367b08f.svg)](https://scverse.zulipchat.com) [![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org/) # Scanpy - Single-Cell Analysis in Python Scanpy is a scalable toolkit for analyzing single-cell gene expression data built jointly with [anndata][]. It includes preprocessing, visualization, clustering, trajectory inference and differential expression testing. The Python-based implementation efficiently deals with datasets of more than one million cells. Discuss usage on the scverse [Discourse][]. Read the [documentation][]. If you'd like to contribute by opening an issue or creating a pull request, please take a look at our [contribution guide][]. [anndata]: https://anndata.readthedocs.io [discourse]: https://discourse.scverse.org/ [documentation]: https://scanpy.readthedocs.io [//]: # (numfocus-fiscal-sponsor-attribution) scanpy is part of the scverse® project ([website](https://scverse.org), [governance](https://scverse.org/about/roles)) and is fiscally sponsored by [NumFOCUS](https://numfocus.org/). If you like scverse® and want to support our mission, please consider making a tax-deductible [donation](https://numfocus.org/donate-to-scverse) to help the project pay for developer time, professional services, travel, workshops, and a variety of other needs. <div align=\"center\"> <a href=\"https://numfocus.org/project/scverse\"> <img src=\"https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png\" width=\"200\" > </a> </div> ## Citation If you use `scanpy` in your work, please cite the `scanpy` publication as follows: > **SCANPY: large-scale single-cell gene expression data analysis** > > F. Alexander Wolf, Philipp Angerer, Fabian J. Theis > > _Genome Biology_ 2018 Feb 06. doi: [10.1186/s13059-017-1382-0](https://doi.org/10.1186/s13059-017-1382-0). You can cite the scverse publication as follows: > **The scverse project provides a computational ecosystem for single-cell omics data analysis** > > Isaac Virshup, Danila Bredikhin, Lukas Heumos, Giovanni Palla, Gregor Sturm, Adam Gayoso, Ilia Kats, Mikaela Koutrouli, Scverse Community, Bonnie Berger, Dana Pe'er, Aviv Regev, Sarah A. Teichmann, Francesca Finotello, F. Alexander Wolf, Nir Yosef, Oliver Stegle & Fabian J. Theis > > _Nat Biotechnol._ 2023 Apr 10. doi: [10.1038/s41587-023-01733-8](https://doi.org/10.1038/s41587-023-01733-8). [contribution guide]: CONTRIBUTING.md\n",
                "dependencies": "[build-system] build-backend = \"hatchling.build\" requires = [ \"hatchling\", \"hatch-vcs\" ] [project] name = \"scanpy\" description = \"Single-Cell Analysis in Python.\" requires-python = \">=3.11\" license = \"BSD-3-clause\" authors = [ { name = \"Alex Wolf\" }, { name = \"Philipp Angerer\" }, { name = \"Fidel Ramirez\" }, { name = \"Isaac Virshup\" }, { name = \"Sergei Rybakov\" }, { name = \"Gokcen Eraslan\" }, { name = \"Tom White\" }, { name = \"Malte Luecken\" }, { name = \"Davide Cittaro\" }, { name = \"Tobias Callies\" }, { name = \"Marius Lange\" }, { name = \"Andrés R. Muñoz-Rojas\" }, ] maintainers = [ { name = \"Philipp Angerer\", email = \"phil.angerer@gmail.com\" }, { name = \"Ilan Gold\", email = \"ilan.gold@helmholtz-munich.de\" }, { name = \"Severin Dicks\" }, ] readme = \"README.md\" classifiers = [ \"License :: OSI Approved :: BSD License\", \"Development Status :: 5 - Production/Stable\", \"Environment :: Console\", \"Framework :: Jupyter\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", \"Natural Language :: English\", \"Operating System :: MacOS :: MacOS X\", \"Operating System :: Microsoft :: Windows\", \"Operating System :: POSIX :: Linux\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Programming Language :: Python :: 3.13\", \"Topic :: Scientific/Engineering :: Bio-Informatics\", \"Topic :: Scientific/Engineering :: Visualization\", ] dependencies = [ \"anndata>=0.9\", \"numpy>=1.25\", \"matplotlib>=3.7\", \"pandas >=2.0\", \"scipy>=1.11\", \"seaborn>=0.13\", \"h5py>=3.8\", \"tqdm\", \"scikit-learn>=1.1\", \"statsmodels>=0.14\", \"patsy!=1.0.0\", # https://github.com/pydata/patsy/issues/215 \"networkx>=2.8\", \"natsort\", \"joblib\", \"numba>=0.58\", \"umap-learn>=0.5,!=0.5\", \"pynndescent>=0.5\", \"packaging>=21.3\", \"session-info2\", \"legacy-api-wrap>=1.4\", # for positional API deprecations \"typing-extensions; python_version < '3.13'\", ] dynamic = [ \"version\" ] # https://docs.pypi.org/project_metadata/#project-urls [project.urls] Documentation = \"https://scanpy.readthedocs.io/\" Source = \"https://github.com/scverse/scanpy\" Homepage = \"https://scanpy.org\" Discourse = \"https://discourse.scverse.org/c/help/scanpy/37\" Bluesky = \"https://bsky.app/profile/scverse.bsky.social\" Twitter = \"https://x.com/scverse_team\" [project.scripts] scanpy = \"scanpy.cli:console_main\" [project.optional-dependencies] test-min = [ \"pytest>=8.2\", \"pytest-mock\", \"pytest-cov\", \"pytest-xdist[psutil]\", \"pytest-randomly\", \"pytest-rerunfailures\", \"profimp\", ] test = [ \"scanpy[test-min]\", # Optional but important dependencies \"scanpy[leiden]\", \"zarr<3\", \"scanpy[dask]\", \"scanpy[scrublet]\", ] test-full = [ \"scanpy[test]\", # optional storage modes \"zappy\", # additional tested algorithms \"scanpy[louvain]\", \"scanpy[magic]\", \"scanpy[skmisc]\", \"scanpy[harmony]\", \"scanpy[scanorama]\", \"scanpy[dask-ml]\", ] doc = [ \"sphinx >=7, !=8.2.0\", \"sphinx-book-theme>=1.1.0\", \"scanpydoc>=0.15.3\", \"sphinx-autodoc-typehints>=1.25.2\", \"myst-parser>=2\", \"myst-nb>=1\", \"sphinx-design\", \"sphinx-tabs\", \"sphinxext-opengraph\", # for nice cards when sharing on social \"sphinx-copybutton\", \"nbsphinx>=0.9\", \"ipython>=7.20\", # for nbsphinx code highlighting \"matplotlib!=3.6.1\", \"sphinxcontrib-bibtex\", \"setuptools\", # undeclared dependency of sphinxcontrib-bibtex→pybtex # TODO: remove necessity for being able to import doc-linked classes \"scanpy[paga,dask-ml]\", \"sam-algorithm\", ] dev = [ \"hatch-vcs\", # runtime dev version generation \"pre-commit\", # static checking \"towncrier\", # release note management ] # Algorithms paga = [ \"igraph\" ] louvain = [ \"igraph\", \"louvain>=0.6.0,!=0.6.2\" ] # Louvain community detection leiden = [ \"igraph>=0.10\", \"leidenalg>=0.9.0\" ] # Leiden community detection bbknn = [ \"bbknn\" ] # Batch balanced KNN (batch correction) magic = [ \"magic-impute>=2.0\" ] # MAGIC imputation method skmisc = [ \"scikit-misc>=0.1.3\" ] # highly_variable_genes method 'seurat_v3' harmony = [ \"harmonypy\" ] # Harmony dataset integration scanorama = [ \"scanorama\" ] # Scanorama dataset integration scrublet = [ \"scikit-image>=0.20\" ] # Doublet detection with automatic thresholds # Acceleration rapids = [ \"cudf>=0.9\", \"cuml>=0.9\", \"cugraph>=0.9\" ] # GPU accelerated calculation of neighbors dask = [ \"dask[array]>=2023.5.1\" ] # Use the Dask parallelization engine dask-ml = [ \"dask-ml\", \"scanpy[dask]\" ] # Dask-ML for sklearn-like API [tool.hatch.build.targets.wheel] packages = [ \"src/testing\", \"src/scanpy\" ] [tool.hatch.version] source = \"vcs\" raw-options.version_scheme = \"release-branch-semver\" [tool.pytest.ini_options] addopts = [ \"--import-mode=importlib\", \"--strict-markers\", \"--doctest-modules\", \"-ptesting.scanpy._pytest\", \"--pyargs\", ] testpaths = [ \"./tests\", \"./ci\", \"scanpy\" ] norecursedirs = [ \"tests/_images\" ] xfail_strict = true junit_family = \"xunit1\" markers = [ \"internet: tests which rely on internet resources (enable with `--internet-tests`)\", \"gpu: tests that use a GPU (currently unused, but needs to be specified here as we import anndata.tests.helpers, which uses it)\", \"anndata_dask_support: tests that require dask support in anndata\", ] filterwarnings = [ # legacy-api-wrap: internal use of positional API \"error:The specified parameters:FutureWarning\", # When calling `.show()` in tests, this is raised \"ignore:FigureCanvasAgg is non-interactive:UserWarning\", # We explicitly handle the below errors in tests \"error:`anndata.read` is deprecated:FutureWarning\", \"error:Observation names are not unique:UserWarning\", \"error:The dtype argument is deprecated and will be removed:FutureWarning\", \"error:The behavior of DataFrame\\\\.sum with axis=None is deprecated:FutureWarning\", \"error:The default of observed=False is deprecated:FutureWarning\", \"error:Series\\\\.__getitem__ treating keys as positions is deprecated:FutureWarning\", \"error:The default value of 'ignore' for the `na_action` parameter in pandas\\\\.Categorical\\\\.map:FutureWarning\", \"error:The provided callable.* is currently using:FutureWarning\", \"error:The behavior of DataFrame concatenation with empty or all-NA entries is deprecated:FutureWarning\", \"error:A value is trying to be set on a copy of a slice from a DataFrame\", \"error:No data for colormapping provided via 'c'\", \"error:\\\\n*The `scale` parameter has been renamed and will be removed\", \"error:\\\\n*Passing `palette` without assigning `hue` is deprecated\", \"error:\\\\n*Setting a gradient palette using color= is deprecated\", ] [tool.coverage.run] data_file = \"test-data/coverage\" source_pkgs = [ \"scanpy\" ] omit = [ \"tests/*\", \"src/testing/*\" ] concurrency = [ \"multiprocessing\" ] parallel = \"true\" [tool.coverage.xml] output = \"test-data/coverage.xml\" [tool.coverage.paths] source = [ \".\", \"**/site-packages\" ] [tool.coverage.report] exclude_also = [ \"if __name__ == .__main__.:\", \"if TYPE_CHECKING:\", # https://github.com/numba/numba/issues/4268 '@(numba\\.|nb\\.)njit.*', ] [tool.ruff] src = [ \"src\" ] [tool.ruff.format] docstring-code-format = true [tool.ruff.lint] select = [ \"B\", # Likely bugs and design issues \"BLE\", # Blind exception raised \"C4\", # Comprehensions \"D\", # Documentation style \"E\", # Error detected by Pycodestyle \"EM\", # Traceback-friendly error messages \"F\", # Errors detected by Pyflakes \"FBT\", # No positional boolean parameters \"I\", # Import sorting \"ICN\", # Follow import conventions \"ISC\", # Implicit string concatenation \"PERF\", # Performance \"PIE\", # Syntax simplifications \"PL\", # Pylint \"PT\", # Pytest style \"PTH\", # Pathlib instead of os.path \"PYI\", # Typing \"RUF\", # Miscellaneous Ruff-only lints \"SIM\", # Simplify control flow \"TC\", # Manage type checking blocks \"UP\", # Update legacy syntax \"TID251\", # Banned imports \"W\", # Warning detected by Pycodestyle ] external = [ \"PLR0917\" ] # preview lint that we use ignore = [ \"C408\", # dict() syntax is preferrable when creating dicts for kwargs \"E262\", # E266 too many leading '#' for block comment -> Scanpy allows them for comments into sections \"E402\", # module level import not at top of file -> required to circumvent circular imports for Scanpys API \"E501\", # line too long -> we accept long comment lines; black gets rid of long code lines \"E741\", # allow I, O, l as variable names -> I is the identity matrix, i, j, k, l is reasonable indexing notation \"D203\", # We ban blank lines before docstrings instead of the opposite \"D213\", # We want multiline summaries to start on the first line, not the second \"D417\", # TODO: replace our current param docs reuse with this and remove it here: \"PLR2004\", # Numbers like “2” aren’t that “magic”. \"PYI051\", # `Literal[\"...\"] | str` is useful for autocompletion ] allowed-confusables = [ \"×\", \"’\", \"–\", \"α\" ] [tool.ruff.lint.per-file-ignores] # Do not assign a lambda expression, use a def \"src/scanpy/tools/_rank_genes_groups.py\" = [ \"E731\" ] # No need for docstrings for all benchmarks \"benchmarks/**/*.py\" = [ \"D102\", \"D103\" ] # D*: No need for docstrings for all test modules and test functions # PLR0913: Test may use many fixtures \"tests/**/*.py\" = [ \"D100\", \"D101\", \"D103\", \"PLR0913\" ] [tool.ruff.lint.isort] known-first-party = [ \"scanpy\", \"testing.scanpy\" ] required-imports = [ \"from __future__ import annotations\" ] [tool.ruff.lint.flake8-tidy-imports.banned-api] \"pytest.importorskip\".msg = \"Use the “@needs” decorator/mark instead\" \"pandas.api.types.is_categorical_dtype\".msg = \"Use isinstance(s.dtype, CategoricalDtype) instead\" \"pandas.value_counts\".msg = \"Use pd.Series(a).value_counts() instead\" \"scipy.sparse.spmatrix\".msg = \"Use _compat.SpBase instead\" \"scipy.sparse.sparray\".msg = \"Use _compat.SpBase instead\" \"scipy.sparse.csr_matrix\".msg = \"Use _compat.CSRBase or _compat.CSBase for typing/type checks and add `# noqa: TID251` when constructing\" \"scipy.sparse.csc_matrix\".msg = \"Use _compat.CSCBase or _compat.CSBase for typing/type checks and add `# noqa: TID251` when constructing\" \"scipy.sparse.csr_array\".msg = \"Use _compat.CSRBase or _compat.CSBase for typing/type checks and add `# noqa: TID251` when constructing\" \"scipy.sparse.csc_array\".msg = \"Use _compat.CSCBase or _compat.CSBase for typing/type checks and add `# noqa: TID251` when constructing\" \"scipy.sparse.issparse\".msg = \"Use isinstance(_, _compat.CSBase) or isinstance(_, _compat.SpBase) instead\" \"legacy_api_wrap.legacy_api\".msg = \"Use scanpy._compat.old_positionals instead\" \"numpy.bool\".msg = \"Use `np.bool_` instead for numpy>=1.24<2 compatibility\" \"numba.jit\".msg = \"Use `scanpy._compat.njit` instead\" \"numba.njit\".msg = \"Use `scanpy._compat.njit` instead\" [tool.ruff.lint.flake8-type-checking] exempt-modules = [ ] strict = true [tool.ruff.lint.pydocstyle] convention = \"numpy\" [tool.ruff.lint.pylint] max-args = 10 max-positional-args = 5 [tool.towncrier] name = \"scanpy\" package = \"scanpy\" directory = \"docs/release-notes\" filename = \"docs/release-notes/{version}.md\" single_file = false package_dir = \"src\" issue_format = \"{{pr}}`{issue}`\" title_format = \"(v{version})=\\n### {version} {{small}}`{project_date}`\" fragment.bugfix.name = \"Bug fixes\" fragment.doc.name = \"Documentation\" fragment.feature.name = \"Features\" fragment.misc.name = \"Miscellaneous improvements\" fragment.performance.name = \"Performance\" fragment.breaking.name = \"Breaking changes\" fragment.dev.name = \"Development Process\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/scits",
            "repo_link": "https://github.com/jalalmostafa/SciTS",
            "content": {
                "codemeta": "",
                "readme": "# SciTS v2, 2023 update A tool to benchmark Time-series on different databases - reworked architecture - adds mixed, online workloads - adds regular and irregular ingestion modes. - adds multiple values per time series (\"Dimensions\") - adds limited queries - adds CLI arguments - adds ClientLatency metric to measure differences in local processing. for questions on these features, please contact info@saninfo.de Requires .NET 7.x cross-platform framework. ## Citation [![DOI](https://zenodo.org/badge/429005385.svg)](https://zenodo.org/badge/latestdoi/429005385) Please cite our work: > Jalal Mostafa, Sara Wehbi, Suren Chilingaryan, and Andreas Kopmann. 2022. SciTS: A Benchmark for Time-Series Databases in Scientific Experiments and Industrial Internet of Things. In 34th International Conference on Scientific and Statistical Database Management (SSDBM 2022). Association for Computing Machinery, New York, NY, USA, Article 12, 1-11. https://doi.org/10.1145/3538712.3538723 ### Bibtex ```bibtex @inproceedings{10.1145/3538712.3538723, author = {Mostafa, Jalal and Wehbi, Sara and Chilingaryan, Suren and Kopmann, Andreas}, title = {SciTS: A Benchmark for Time-Series Databases in Scientific Experiments and Industrial Internet of Things}, year = {2022},☺ isbn = {9781450396677}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3538712.3538723}, doi = {10.1145/3538712.3538723}, abstract = {Time-series data has an increasingly growing usage in Industrial Internet of Things (IIoT) and large-scale scientific experiments. Managing time-series data needs a storage engine that can keep up with their constantly growing volumes while providing an acceptable query latency. While traditional ACID databases favor consistency over performance, many time-series databases with novel storage engines have been developed to provide better ingestion performance and lower query latency. To understand how the unique design of a time-series database affects its performance, we design SciTS, a highly extensible and parameterizable benchmark for time-series data. The benchmark studies the data ingestion capabilities of time-series databases especially as they grow larger in size. It also studies the latencies of 5 practical queries from the scientific experiments use case. We use SciTS to evaluate the performance of 4 databases of 4 distinct storage engines: ClickHouse, InfluxDB, TimescaleDB, and PostgreSQL.}, booktitle = {Proceedings of the 34th International Conference on Scientific and Statistical Database Management}, articleno = {12}, numpages = {11}, keywords = {time-series databases, database management systems, industrial internet of things, scientific experiments, sensor data, time-series}, location = {Copenhagen, Denmark}, series = {SSDBM '22} } ``` # How to run 1. Create your workload as `App.config` (case-sensitive) in `BenchmarkTool`. 2. Edit the connection strings to your database servers in the workload file. 3. Choose the target database in the workload file using `TargetDatabase` element. 4. run `dotnet run --project BenchmarkTool write` if it's an ingestion workload, and `dotnet run --project BenchmarkTool read` if it's a query workload. x. Use `Scripts/ccache.sh <database-service-name>` to clear the cache between query tests. ## Additional Command Line options: `dotnet run --project BenchmarkTool [action] [regular/irregular] [DatabaseNameDB]` Available Actions: * read: start the specified retrieval and aggregation workloads. * write: start the ingestion across specified batchsize, number of clients, dimensions. * mixed-AggQueries: start the online, mixed workload benchmark as a mixture of aggregated quieries and Ingestion-Parameters * mixed-LimitedQueries: start the online, mixed workload benchmark as a mixture of queried and ingested datapoints according the specified percentage parameter and the requested Ingestion-Parameters. E.g. 100% means that as much datapoints are retrieved as ingested. ## System Metrics using Glances This tool uses [glances](https://github.com/nicolargo/glances/). 1. Install glances with all plugins on the database server using `pip install glances[all]` 2. Run glances REST API on the database server using `glances -w --disable-webui` ## Workload Definition Files you can open Default-App.config edit it and save it as App.config. It has following content: ```xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <configuration> <appSettings> <!-- Attention: This file \"AppDefault.config\" is to be renamed in \"App.config\", after updating the \"###\" and other fields. --> <!-- Datalayerts connection settings --> <add key=\"DatalayertsConnection\" value=\"https://datalayerts.com\" /> <add key=\"DatalayertsUser\" value=\"###\" /> <add key=\"DatalayertsPassword\" value=\"###\" /> <!-- Postgres connection settings --> <add key=\"PostgresConnection\" value=\"Server=localhost;Port=5432;Database=postgres;User Id=postgres;Password=###;\" /> <!-- Timescale connection settings --> <add key=\"TimescaleConnection\" value=\"Server=localhost;Port=6432;Database=postgres;User Id=postgres;Password=###;CommandTimeout=300\" /> <!-- InfluxDB connection settings --> <add key=\"InfluxDBHost\" value=\"http://localhost:8086\" /> <add key=\"InfluxDBToken\" value=\"u7Ek4P5s0Nle61QQF1nNA3ywL1JYZky6rHRXxkPBX5bY4H3YFJ6T4KApWSRhaKNj_kHgx70ZLBowB6Di4t2YXg==\" /> <add key=\"InfluxDBBucket\" value=\"scitsdb\" /> <add key=\"InfluxDBOrganization\" value=\"scits\" /> <!-- Clickhouse connection settings --> <add key=\"ClickhouseHost\" value=\"localhost\" /> <add key=\"ClickhousePort\" value=\"9000\" /> <add key=\"ClickhouseUser\" value=\"default\" /> <add key=\"ClickhouseDatabase\" value=\"default\" /> <!-- General Settings--> <add key=\"PrintModeEnabled\" value=\"false\" /> <add key=\"TestRetries\" value=\"2\" /> <add key=\"DaySpan\" value=\"1\" /> <!-- Could be: DummyDB, PostgresDB , DatalayertsDB , ClickhouseDB , TimescaleDB , InfluxDB --> <add key=\"TargetDatabase\" value=\"DummyDB\" /> <add key=\"StartTime\" value=\"2022-01-01T00:00:00.00\" /> <add key=\"RegularTsScaleMilliseconds\" value=\"1000\" /> <!-- Where to store metrics files: The Programm will split the files in \"[...]Read.csv\" and \"[...]Write.csv\" --> <add key=\"MetricsCSVPath\" value=\"Metrics_Source_Month-Day\" /> <!-- System Metrics Options --> <add key=\"GlancesOutput\" value=\"Glances_Source_Month-Day.csv\"/> <add key=\"GlancesUrl\" value=\"http://localhost:61208\" /> <add key=\"GlancesDatabasePid\" value=\"1\" /> <add key=\"GlancesPeriod\" value=\"1\" /> <add key=\"GlancesNIC\" value=\"lo\" /> <add key=\"GlancesDisk\" value=\"sda1\" /> <add key=\"GlancesStorageFileSystem\" value=\"/\" /> <!-- Insert multiple dimensionnrs, e.g. 1,6 ,12 ,50, 100, --> <add key=\"DataDimensionsNrOptions\" value=\"1,6\" /> <!-- Read Query Options --> <!-- Could be: Agg, All, RangeQueryRawData, RangeQueryRawAllDimsData, RangeQueryRawLimitedData, RangeQueryRawAllDimsLimitedData RangeQueryAggData, OutOfRangeQuery, DifferenceAggQuery, STDDevQuery --> <add key=\"QueryType\" value=\"All\" /> <add key=\"AggregationIntervalHour\" value=\"1\" /> <add key=\"DurationMinutes\" value=\"60\" /> <add key=\"SensorsFilter\" value=\"1,2,3,4\" /> <!-- or \"All\" --> <add key=\"SensorID\" value=\"1\" /> <add key=\"MaxValue\" value=\"0.9\" /> <add key=\"MinValue\" value=\"0.1\" /> <add key=\"FirstSensorID\" value=\"1\" /> <add key=\"SecondSensorID\" value=\"2\" /> <!-- Ingestion --> <!-- Could be: regular, irregular --> <add key=\"IngestionType\" value=\"regular\" /> <!-- Coulde be: 33, 100 , 300 --> <add key=\"MixedWLPercentageOptions\" value=\"33, 100,300\" /> <!-- Could be: array, column. Array is not fully implemented in all DBMS. --> <add key=\"MultiDimensionStorageType\" value=\"column\" /> <!-- 10, 1000, 5000, 10000 , 50000 --> <add key=\"BatchSizeOptions\" value=\" 100 , 1000, 6000 \" /> <!-- Number of concurrent clients e.g.(1,8,16) must be less than sensors. BatchSizes will be shared out between the clients --> <add key=\"ClientNumberOptions\" value=\"1 , 8\" /> <add key=\"SensorNumber\" value=\"100\" /> </appSettings> </configuration> ``` ### Workload Files You can choose from the available workloads by choosing a `*.config` file from `Workloads` folder. The file to workload mapping is as follow: | Workload | Workload file | | ----------- | ---------------------------------- | | 2022 WLs | | | ----------- | | | Q1 | query-q1.config | | Q2 | query-q2.config | | Q3 | query-q3.config | | Q4 | query-q4.config | | Q5 | query-q5.config | | Batching | ingestion-batching-1client.config | | Concurrency | ingestion-batching-nclients.config | | Scaling | ingestion-scaling.config | |.............|....................................| |Collection | | |of 2023 WLs | test2023.sh | #### Timescale We discovered abnormal high latencies and other failures with NPGSQL, so we embedded a python script which does the queries. Therefore you need to configure the python location. In case of python 3.10, e.g. use \"whereis libpython3.10.so\", and copy this path. Then you go into TimescaleDB.cd, and edit the string in line 134 [ Runtime.PythonDLL = \"/usr/lib/_Architecture_-linux-gnu/libpython3.10.so\"; ] So it points to your python location\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/score-p",
            "repo_link": "https://gitlab.com/score-p/scorep",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/sdaas",
            "repo_link": "https://github.com/rizac/sdaas",
            "content": {
                "codemeta": "",
                "readme": "# <img align=\"left\" height=\"30\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/medien_kommunikation/Infothek/Mediathek/Bilder/GFZ/GFZ_Logo/GFZ-Logo_eng_RGB.svg\"> Sdaas <img align=\"right\" height=\"50\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/GFZ_Wortmarke_SVG_klein_en_edit.svg\"> |Jump to: | [Installation](#installation) | [Usage](#usage) | [Maintenance](#maintenance) | [Citation](#citation) | | - | - | - | - | - | <!-- **S**eismic **D**ata (and metadata) **A**mplitude **A**nomaly **S**core --> Simple, general and flexible tool for the identification of anomalies in seismic waveform amplitude, e.g.: - recording artifacts (e.g., anomalous noise, peaks, gaps, spikes) - sensor problems (e.g. digitizer noise) - metadata field errors (e.g. wrong stage gain in StationXML) **For any waveform analyzed, the program computes an amplitude anomaly score in [0, 1] representing the degree of belief of a waveform to be an outlier**. The score can be used: - in any processing pipeline to - pre-filter malformed data via a user-defined threshold - assign robustness weights - as station installation / metadata checker, exploiting the scores temporal trends. See e.g. Channel (a) in the figure: the abrupt onset/offset of persistently high anomaly scores roughly between March and May 2018 clearly indicates an installation problem that has been fixed ![anomaly_scores_image](https://github.com/rizac/sdaas/blob/6b1dca95f4a5f931874c4aaedd278f4692dc0f96/outlierspaper-img008.png?raw=true) Anomaly scores for four different channels (a) to (d). Each dot represents a recorded waveform segment of variable length Notes: This program uses a machine learning algorithm specifically designed for outlier detection (Isolation forest) where - scores <= 0.5 can be safely interpreted in all applications as \"no significant anomaly\" (see Isolation Forest original paper - Liu et al. 2008 - for theoretical details) - extreme score values are virtually impossible [by design](https://scikit-learn.org/stable/modules/calibration.html). This has to be considered when setting a user defined threshold T to discard malformed waveforms. In many application, setting T between 0.7 and 0.75 has proven to be a good compromise between [precision and recall (F1 score)](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics) - (Disclaimer) \"False positives\", i.e. relatively high anomaly scores even for well formed recordings have been sometimes observed in two specific cases: - recordings from stations with extremely and abnormaly low noise level (e.g. borhole installations) - recordings containing strong and close earthquakes. This is not a problem to check metadata errors, as the scores trend of several recordings from a given sensor / station will not be affected (except maybe for few sparse slightly higer scores), but has to be considered when filtering out segments in specific studies (e.g. with strong motion data): in this cases, setting a higher threshold is advisable. A model trained for accelerometers only (usually employed with these kind of recordings) is under study <!-- <img align=\"right\" width=\"27%\" src=\"outlierspaper-img004.png\"><img align=\"right\" width=\"29%\" src=\"outlierspaper-img005.png\"> Simple program to compute amplitude anomaly scores (in [0, 1]) of seismic data and metadata. Given a set of waveforms and their metadata, it removes the waveform response and returns the anomaly score of the waveforms amplitudes. This program can be used to filter out a set of malformed waveforms, assign robustness weights or to check the correctness of the metadata fields (e.g. Station inventory xml) by checking the anomaly score on a set of station recordings. --> ## Installation Always work within a virtual environment. From a terminal, in the directory where you cloned this repository (last argument of `git clone`), 1. create a virtual environment (once). **Be sure you use Python>=3.7 (Type `python3 --version` to check)**: ```bash python3 -m venv .env ``` 2. activate it (**to be done also every time you use this program**): ```bash source .env/bin/activate ``` (then to deactivate, simply type ... `deactivate` on the terminal). Update `pip` and `setuptools` (not mandatory, but in rare cases it could prevent errors during the installation): ``` pip install --upgrade pip setuptools ``` 3. Install the program (one line command): with [requirements file](https://pip.pypa.io/en/stable/user_guide/#requirements-files): ``` pip install -r ./requirements.txt && pip install -e . ``` or standard (use this mainly if you install sdaas on an already existing virtualenv and you are concerned about breaking existing code): ``` pip install \"numpy>=1.15.4\" && pip install -e . ``` Notes: - The \"standard\" install actually checks the `setup.py` file and avoids overwriting libraries already matching the required version. The downside is that you might use library version that were not tested - `-e` is optional. With -e, you can update the installed program to the latest release by simply issuing a `git pull` - although used to train, test and generate the underlying model, `scikit learn` is not required for Security & maintainability limitations. If you want to install it, type `pip install scikit-learn>=0.21.3` or, in the standard installation you can include scikit learn with `pip install .[dev]` instead of `pip install .` <details> <summary>Reported scikit learn installation problems (click for details)</summary> Due to the specific version to be installed, scikit might have problems installing. Few hints here: - you might need to preinstall `cython` (`pip install cython`) - you might need to `brew install libomp`, set the follwing env variables: ``` export CC=/usr/bin/clang;export CXX=/usr/bin/clang++;export CPPFLAGS=\"$CPPFLAGS -Xpreprocessor -fopenmp\";export CFLAGS=\"$CFLAGS -I/usr/local/opt/libomp/include\";export CXXFLAGS=\"$CXXFLAGS -I/usr/local/opt/libomp/include\";export LDFLAGS=\"$LDFLAGS -Wl,-rpath,/usr/local/opt/libomp/lib -L/usr/local/opt/libomp/lib -lomp\" ``` - and then install with the following flags: ``` pip install --verbose --no-build-isolation \"scikit-learn==0.21.3\" ``` **(For any further detail, see [scikit-learn installation page](https://scikit-learn.org/dev/developers/advanced_installation.html))** </details> #### Run tests (optional) ```bash python -m unittest -fv ``` (`-f` is optional and means: stop at first failure, `-v`: verbose) ## Usage `sdaas` can be used as command line application or as library in your Python code ### As command line application After activating your virtual environment (see above) you can access the program as command line application in your terminal by typing `sdaas`. The application can compute the score(s) of a single miniSEED file, a directory of miniSEED files, or a FDSN url ([dataselect or station](https://www.fdsn.org/webservices/) url). **Please type `sdaas --help` for details and usages not covered in the examples below, such as computing scores from locally stored files** **Examples** Compute scores of waveforms fetched from a FDSN URL: ```bash >>> sdaas \"http://geofon.gfz-potsdam.de/fdsnws/dataselect/1/query?net=GE&sta=EIL&cha=BH?&start=2019-01-01T00:00:00&end=2019-01-01T00:02:00\" -v [████████████████████████████████████████████████████████████████████████████████████████████████████████]100% 0d 00:00:00 id start end anomaly_score GE.EIL..BHN 2019-01-01T00:00:09.100 2019-01-01T00:02:12.050 0.45 GE.EIL..BHE 2019-01-01T00:00:22.350 2019-01-01T00:02:00.300 0.45 GE.EIL..BHZ 2019-01-01T00:00:02.250 2019-01-01T00:02:03.550 0.45 ``` *(note: The paremeter `-v` / verbose prints additional info before the scores table)* Compute scores from randomly selected segments of a given station and channel, and provide also a user-defined threshold (parameter `-th`) which will also append a column \"class_label\" (1 = outlier - assigned to the scores greater than the threshold and 0 = inlier) ```bash >>> sdaas \"http://geofon.gfz-potsdam.de/fdsnws/station/1/query?net=GE&sta=EIL&cha=BH?&start=2019-01-01\" -v -th 0.7 [██████████████████████████████████████████████████████████]100% 0d 00:00:00 id start end anomaly_score class_label GE.EIL..BHN 2019-01-01T00:00:09.100 2019-01-01T00:02:12.050 0.45 0 GE.EIL..BHN 2019-10-16T18:48:11.700 2019-10-16T18:51:02.300 0.83 1 GE.EIL..BHN 2020-07-31T13:37:19.299 2020-07-31T13:39:41.299 0.45 0 GE.EIL..BHN 2021-05-16T08:25:38.100 2021-05-16T08:28:03.150 0.53 0 GE.EIL..BHN 2022-03-01T03:14:23.300 2022-03-01T03:17:01.750 0.45 0 GE.EIL..BHE 2019-01-01T00:00:22.350 2019-01-01T00:02:00.300 0.45 0 GE.EIL..BHE 2019-10-16T18:48:18.050 2019-10-16T18:51:09.650 0.83 1 GE.EIL..BHE 2020-07-31T13:37:08.599 2020-07-31T13:39:28.499 0.45 0 GE.EIL..BHE 2021-05-16T08:25:49.150 2021-05-16T08:28:14.800 0.49 0 GE.EIL..BHE 2022-03-01T03:14:26.050 2022-03-01T03:16:41.900 0.45 0 GE.EIL..BHZ 2019-01-01T00:00:02.250 2019-01-01T00:02:03.550 0.45 0 GE.EIL..BHZ 2019-10-16T18:48:24.800 2019-10-16T18:50:47.300 0.45 0 GE.EIL..BHZ 2020-07-31T13:37:08.249 2020-07-31T13:39:30.199 0.45 0 GE.EIL..BHZ 2021-05-16T08:25:47.250 2021-05-16T08:28:10.850 0.47 0 GE.EIL..BHZ 2022-03-01T03:14:40.800 2022-03-01T03:16:53.900 0.45 0 ``` Compute scores from randomly selected segments of a given station and channel, but aggregating scores per channel and returning their median: ```bash >>> sdaas \"http://geofon.gfz-potsdam.de/fdsnws/station/1/query?net=GE&sta=EIL&cha=BH?&start=2019-01-01\" -v -agg median [██████████████████████████████████████████████████████████]100% 0d 00:00:00 id start end median_anomaly_score GE.EIL..BHN 2019-01-01T00:00:09.100 2022-03-01T03:17:45.950 0.46 GE.EIL..BHE 2019-01-01T00:00:22.350 2022-03-01T03:17:48.700 0.45 GE.EIL..BHZ 2019-01-01T00:00:02.250 2022-03-01T03:17:39.300 0.45 ``` Same as above, but save the scores table to CSV via the parameter `-sep` and `>` (normal redirection of the standard output `stdout` to file) ```bash >>> sdaas \"http://geofon.gfz-potsdam.de/fdsnws/station/1/query?net=GE&sta=EIL&cha=BH?&start=2019-01-01\" -v -sep \",\" > /path/to/myfile.csv [████████████████████████████████████████████████████████████]100% 0d 00:00:00 ``` *in this case, providing `-v` / verbose will also redirect the header row to CSV. Note that only the scores table is output to `stdout`, everything else is printed to `stderr` and thus should still be visible on the terminal, as in the example above* ### As library in your Python code This software can also be used as library in Python code (e.g. Jupyter Notebook) to work with [ObsPy](https://docs.obspy.org/) objects (ObsPy is already included in the installation): assuming you have one or more [Stream](https://docs.obspy.org/packages/autogen/obspy.core.stream.Stream.html) or [Trace](https://docs.obspy.org/packages/autogen/obspy.core.trace.Trace.html), with relative [Inventory](https://docs.obspy.org/packages/obspy.core.inventory.html), then <!-- IMPORTANT: --> <!-- EACH \"```python...```\" code snippet is executed also in `test_and_create_code_snippet` to check that it works. If you implement new snippets here, add them in the test file as well --> Compute the scores in a stream or iterable of traces (e.g. list. tuple, generator), returning the score of each Trace: ```python from obspy.core.inventory.inventory import read_inventory from obspy.core.stream import read from sdaas.core import traces_scores # Load a Stream object and its inventory # (use as example the test directory of the package): stream = read('./tests/data/GE.FLT1..HH?.mseed') inventory = read_inventory('./tests/data/GE.FLT1.xml') # Compute the Stream anomaly score (3 scores, one for each Trace): output = traces_scores(stream, inventory) ``` Then `output` is: ``` [0.45729656, 0.45199387, 0.45113142] ``` Compute the scores in a stream or iterable of traces, getting also the traces id (by default the tuple `(seed_id, start, end)`, where seed_id is the [Trace SEED identifier](https://docs.obspy.org/packages/autogen/obspy.core.trace.Trace.get_id.html)): ```python from obspy.core.inventory.inventory import read_inventory from obspy.core.stream import read from sdaas.core import traces_idscores # Load a Stream object and its inventory # (use as example the test directory of the package): stream = read('./tests/data/GE.FLT1..HH?.mseed') inventory = read_inventory('./tests/data/GE.FLT1.xml') # Compute the Stream anomaly score: output = traces_idscores(stream, inventory) ``` Then `output` is: ``` ([('GE.FLT1..HHE', datetime.datetime(2011, 9, 3, 16, 38, 5, 550001), datetime.datetime(2011, 9, 3, 16, 42, 12, 50001)), ('GE.FLT1..HHN', datetime.datetime(2011, 9, 3, 16, 38, 5, 760000), datetime.datetime(2011, 9, 3, 16, 42, 9, 670000)), ('GE.FLT1..HHZ', datetime.datetime(2011, 9, 3, 16, 38, 8, 40000), datetime.datetime(2011, 9, 3, 16, 42, 9, 670000))], array([ 0.45729656, 0.45199387, 0.45113142])) ``` Same as above, with custom traces id (their SEED identifier only): ```python from obspy.core.inventory.inventory import read_inventory from obspy.core.stream import read from sdaas.core import traces_idscores # Load a Stream object and its inventory # (use as example the test directory of the package): stream = read('./tests/data/GE.FLT1..HH?.mseed') inventory = read_inventory('./tests/data/GE.FLT1.xml') # Compute the Stream anomaly score: output = traces_idscores(stream, inventory, idfunc=lambda t: t.get_id()) ``` Then `output` is: ``` (['GE.FLT1..HHE', 'GE.FLT1..HHN', 'GE.FLT1..HHZ'], array([ 0.45729656, 0.45199387, 0.45113142])) ``` You can also compute scores and ids from iterables of streams (e.g., when reading from files)... ```python from sdaas.core import streams_scores from sdaas.core import streams_idscores ``` ... or from a single trace: ```python from sdaas.core import trace_score ``` For instance, to compute the anomaly score of several streams (for each stream and for each trace therein, return the trace anomaly score): ```python from obspy.core.inventory.inventory import read_inventory from obspy.core.stream import read from sdaas.core import streams_scores # Load a Stream objects and its inventory # (use as example the test directory of the package # and mock a list of streams by loading twice the same Stream): streams = [read('./tests/data/GE.FLT1..HH?.mseed'), read('./tests/data/GE.FLT1..HH?.mseed')] inventory = read_inventory('./tests/data/GE.FLT1.xml') # Compute Streams scores: output = streams_scores(streams, inventory) ``` Then `output` is: ``` [ 0.45729656 0.45199387 0.45113142 0.45729656 0.45199387 0.45113142] ``` Same as above, computing the features and the scores separately for more control: ```python from obspy.core.inventory.inventory import read_inventory from obspy.core.stream import read from sdaas.core import trace_features, aa_scores # Load a Stream object and its inventory # (use as example the test directory of the package # and mock a list of streams by loading twice the same Stream): streams = [read('./tests/data/GE.FLT1..HH?.mseed'), read('./tests/data/GE.FLT1..HH?.mseed')] inventory = read_inventory('./tests/data/GE.FLT1.xml') # Compute Streams scores: feats = [] for stream in streams: for trace in stream: feats.append(trace_features(trace, inventory)) output = aa_scores(feats) ``` Then `output` is: ``` [0.45729656, 0.45199387, 0.45113142, 0.45729656, 0.45199387, 0.45113142] ``` ## Maintenance Although scikit learn is not used anymore for [maintainability limitations](https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations), you can always consult the [README](./sdaas/core/models) explaining how to manage create new scikit-learn models. ## Citation **Software:** > Zaccarelli, Riccardo (2022). sdas - a Python tool computing an amplitude anomaly score of seismic data and metadata using simple machine‐Learning models. GFZ Data Services. https://doi.org/10.5880/GFZ.2.6.2023.009 **Research article:** > Riccardo Zaccarelli, Dino Bindi, Angelo Strollo; Anomaly Detection in Seismic Data-Metadata Using Simple Machine‐Learning Models. Seismological Research Letters 2021; 92 (4): 2627-2639. doi: https://doi.org/10.1785/0220200339\n",
                "dependencies": "certifi==2022.12.7 charset-normalizer==2.1.1 contourpy==1.0.6 cycler==0.11.0 decorator==5.1.1 fonttools==4.38.0 greenlet==2.0.1 idna==3.4 kiwisolver==1.4.4 lxml==4.9.1 matplotlib==3.6.2 numpy==1.23.5 obspy==1.4.0 packaging==22.0 Pillow==9.3.0 pyparsing==3.0.9 python-dateutil==2.8.2 requests==2.28.1 scipy==1.9.3 # -e git+https://github.com/rizac/sdaas.git@89aaf28d7f6f77393d1ce32d62fba7f8f4895e8f#egg=sdaas six==1.16.0 SQLAlchemy==1.4.44 urllib3==1.26.13\nfrom setuptools import setup, find_packages _README = \"\"\" Python program to compute amplitude anomaly score on one or more seismic waveforms (data and metadata) \"\"\" setup( name='sdaas', version='1.2.0', description=_README, url='https://github.com/rizac/sdaas', packages=find_packages(exclude=['tests', 'tests.*']), python_requires='>=3.6.9', # Minimal requirements, for a complete list see requirements-*.txt install_requires=[ 'numpy>=1.15.4', 'obspy>=1.1.1' ], # List additional groups of dependencies here (e.g. development # dependencies). You can install these using the following syntax, # for example: # $ pip install -e .[jupyter,test] extras_require={ # 'jupyter': [ # 'jupyter>=1.1.0' # ], 'dev': [ 'scikit-learn>=0.21.3' ] }, author='riccardo zaccarelli', author_email='', # FIXME: what to provide? maintainer='Section 2.6 (Seismic Hazard and Risk Dynamics), GFZ Potsdam', # FIXME maintainer_email='', classifiers=( 'Development Status :: 1 - Beta', 'Intended Audience :: Education', 'Intended Audience :: Science/Research', 'License :: OSI Approved :: GNU General Public License v3', 'Operating System :: OS Independent', 'Programming Language :: Python :: 3', 'Topic :: Scientific/Engineering', ), keywords=[ \"seismic waveform\", \"isolation forest\", \"outlier score\", \"anomaly detection\", \"machine learning\" ], license=\"GPL3\", platforms=[\"any\"], # FIXME: shouldn't be unix/macos? (shallow google search didn't help) # package_data={\"smtk\": [ # \"README.md\", \"LICENSE\"]}, # make the installation process copy also the iforest models (see MANIFEST.in) # for info see https://python-packaging.readthedocs.io/en/latest/non-code-files.html include_package_data=True, zip_safe=False, # To provide executable scripts, use entry points in preference to the # \"scripts\" keyword. Entry points provide cross-platform support and allow # pip to create the appropriate form of executable for the target platform. entry_points={ 'console_scripts': [ 'sdaas=sdaas.run:cli_entry_point', ], }, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/seisbench",
            "repo_link": "https://github.com/seisbench/seisbench",
            "content": {
                "codemeta": "",
                "readme": "<p align=\"center\"> <img src=\"https://raw.githubusercontent.com/seisbench/seisbench/main/docs/_static/seisbench_logo_subtitle_outlined.svg\" /> </p> --- [![PyPI - License](https://img.shields.io/pypi/l/seisbench)](https://github.com/seisbench/seisbench/blob/main/LICENSE) [![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/seisbench/seisbench/main_push.yml?branch=main)](https://github.com/seisbench/seisbench) [![Read the Docs](https://img.shields.io/readthedocs/seisbench)](https://seisbench.readthedocs.io/en/latest/) [![PyPI](https://img.shields.io/pypi/v/seisbench)](https://pypi.org/project/seisbench/) [![Python 3.9](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5568813.svg)](https://doi.org/10.5281/zenodo.5568813) The Seismology Benchmark collection (*SeisBench*) is an open-source python toolbox for machine learning in seismology. It provides a unified API for accessing seismic datasets and both training and applying machine learning algorithms to seismic data. SeisBench has been built to reduce the overhead when applying or developing machine learning techniques for seismological tasks. ## Getting started SeisBench offers three core modules, `data`, `models`, and `generate`. `data` provides access to benchmark datasets and offers functionality for loading datasets. `models` offers a collection of machine learning models for seismology. You can easily create models, load pretrained models or train models on any dataset. `generate` contains tools for building data generation pipelines. They bridge the gap between `data` and `models`. The easiest way of getting started is through our colab notebooks. | Examples | | |--------------------------------------------------|---| | Dataset basics | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/01a_dataset_basics.ipynb) | | Model API | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/01b_model_api.ipynb) | | Generator Pipelines | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/01c_generator_pipelines.ipynb) | | Applied picking | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/02a_deploy_model_on_streams_example.ipynb) | | Using DeepDenoiser | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/02b_deep_denoiser.ipynb) | | Depth phases and earthquake depth | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/02c_depth_phases.ipynb) | | Training PhaseNet (advanced) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03a_training_phasenet.ipynb) | | Creating a dataset (advanced) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03b_creating_a_dataset.ipynb) | | Building an event catalog with GaMMA (advanced) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03c_catalog_seisbench_gamma.ipynb) | | Building an event catalog with PyOcto (advanced) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03d_catalog_seisbench_pyocto.ipynb) | Alternatively, you can clone the repository and run the same [examples](https://github.com/seisbench/seisbench/tree/main/examples) locally. For more detailed information on Seisbench check out the [SeisBench documentation](https://seisbench.readthedocs.io/). ## Installation SeisBench can be installed in two ways. In both cases, you might consider installing SeisBench in a virtual environment, for example using [conda](https://docs.conda.io/en/latest/). The recommended way is installation through pip. Simply run: ``` pip install seisbench ``` Alternatively, you can install the latest version from source. For this approach, clone the repository, switch to the repository root and run: ``` pip install . ``` which will install SeisBench in your current python environment. ### CPU only installation SeisBench is built on pytorch, which in turn runs on CUDA for GPU acceleration. Sometimes, it might be preferable to install pytorch without CUDA, for example, because CUDA will not be used and the CUDA binaries are rather large. To install such a pure CPU version, the easiest way is to follow a two-step installation. First, install pytorch in a pure CPU version [as explained here](https://pytorch.org/). Second, install SeisBench the regular way through pip. Example instructions would be: ``` pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu pip install seisbench ``` ## Contributing There are many ways to contribute to SeisBench and we are always looking forward to your contributions. Check out the [contribution guidelines](https://github.com/seisbench/seisbench/blob/main/CONTRIBUTING.md) for details on how to contribute. ## Known issues - Some institutions and internet providers are blocking access to our data and model repository, as it is running on a non-standard port (2880). This usually manifests in timeouts when trying to download data or model weights. To verify the issue, try accessing [https://hifis-storage.desy.de:2880/](https://hifis-storage.desy.de:2880/) directly from the same machine. As a mitigation, you can use our backup repository. Just run `seisbench.use_backup_repository()`. Please note that the backup repository will usually show lower download speeds. We recommend contacting your network administrator to allow outgoing access to TCP port 2880 on our server as a higher performance solution. - We've recently changed the URL of the SeisBench repository. To use the new URL update to SeisBench 0.4.1. It this is not possible, you can use the following commands within your runtime to update the URL manually: ```python import seisbench from urllib.parse import urljoin seisbench.remote_root = \"https://hifis-storage.desy.de:2880/Helmholtz/HelmholtzAI/SeisBench/\" seisbench.remote_data_root = urljoin(seisbench.remote_root, \"datasets/\") seisbench.remote_model_root = urljoin(seisbench.remote_root, \"models/v3/\") ``` - On the Apple M1 and M2 chips, pytorch seems to not always work when installed directly within `pip install seisbench`. As a workaround, follow the instructions at (https://pytorch.org/) to install pytorch and then install SeisBench as usual through pip. - EQTransformer model weights \"original\" in version 1 and 2 are incompatible with SeisBench >=0.2.3. Simply use `from_pretrained(\"original\", version=\"3\")` or `from_pretrained(\"original\", update=True)`. The weights will not differ in their predictions. ## References Reference publications for SeisBench: --- * [SeisBench - A Toolbox for Machine Learning in Seismology](https://doi.org/10.1785/0220210324) _Reference publication for software._ --- * [Which picker fits my data? A quantitative evaluation of deep learning based seismic pickers](https://doi.org/10.1029/2021JB023499) _Example of in-depth bencharking study of deep learning-based picking routines using the SeisBench framework._ --- ## Acknowledgement The initial version of SeisBench has been developed at [GFZ Potsdam](https://www.gfz-potsdam.de/) and [KIT](https://www.gpi.kit.edu/) with funding from [Helmholtz AI](https://www.helmholtz.ai/). The SeisBench repository is hosted by [HIFIS - Helmholtz Federated IT Services](https://www.hifis.net/).\n",
                "dependencies": "[build-system] requires = [\"wheel\", \"setuptools >= 61.0.0\", \"setuptools_scm[toml]>=6.2\"] build-backend = \"setuptools.build_meta\" [project] name = \"seisbench\" dynamic = [\"version\"] description = \"The seismological machine learning benchmark collection\" readme = \"README.md\" license = { text = \"GPLv3\" } requires-python = \">=3.9\" authors = [ { name = \"Jack Woolam\", email = \"jack.woollam@kit.edu\" }, { name = \"Jannes Münchmeyer\", email = \"munchmej@gfz-potsdam.de\" }, ] maintainers = [ { name = \"Jack Woolam\", email = \"jack.woollam@kit.edu\" }, { name = \"Jannes Münchmeyer\", email = \"munchmej@gfz-potsdam.de\" }, ] keywords = [\"seismology\", \"machine learning\", \"signal processing\", \"earthquake\"] classifiers = [ \"Intended Audience :: Science/Research\", \"Topic :: Scientific/Engineering\", \"Programming Language :: Python :: 3.9\", \"Operating System :: OS Independent\", \"License :: OSI Approved :: GNU General Public License v3 (GPLv3)\", ] dependencies = [ # < 2 pinning is required for obspy (https://github.com/obspy/obspy/issues/3484) # remove once fixed with obspy 1.5 \"numpy>=1.21.6, <2.0\", \"pandas>=1.1\", \"h5py>=3.1\", \"obspy>=1.3.1\", \"tqdm>=4.52\", \"torch>=1.10.0\", \"scipy>=1.9\", \"nest_asyncio>=1.5.3\", \"bottleneck>=1.3\", ] [tool.setuptools.packages.find] include = [\"seisbench*\"] [project.optional-dependencies] development = [\"flake8\", \"black\", \"isort\", \"pre-commit\"] tests = [\"pytest\", \"pytest-asyncio\"] [tool.setuptools_scm] [project.urls] GitHub = \"https://github.com/seisbench/seisbench\" Documentation = \"https://seisbench.readthedocs.io/en/latest/\" Issues = \"https://github.com/seisbench/seisbench/issues\" [tool.pytest.ini_options] markers = [ \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\", ]\nfrom setuptools import setup setup()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/sensor-management-system",
            "repo_link": "https://codebase.helmholtz.cloud/hub-terra/sms/orchestration",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/serghei",
            "repo_link": "https://gitlab.com/serghei-model/serghei",
            "content": {
                "codemeta": "",
                "readme": "# SERGHEI Simulation Environment for Geomorphology, Hydrodynamics and Ecohydrology in Integrated form [![doi](https://zenodo.org/badge/DOI/10.5281/zenodo.8159947.svg)](https://doi.org/10.5281/zenodo.8159947) [![BSD3 License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause) [![doi](https://img.shields.io/badge/rsd-serghei-00a3e3.svg)](https://helmholtz.software/software/serghei) [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8B-yellow)](https://fair-software.eu) # Dependencies SERGHEI has the following dependencies: + [Kokkos](https://github.com/kokkos/kokkos) handles the parallelization + [Parallel NetCDF](https://github.com/Parallel-NetCDF/PnetCDF) writes output files + We use [R](https://www.r-project.org/) scripts for postprocessing (optional) Both Kokkos and PNetCDF are linked as git submodules in this project. If you are unfamiliar with submodules, you can simply clone this repo together with the submodules with the `git clone --recurse-submodules` command. PnetCDF is often available in Linux distributions via package managers, and also as software modules in HPC systems. It is recommended to use these system wide installations, and only fall back on building the PNetCDF from source if there's no other option. # Building SERGHEI with CMake The CMake workflow is the recommended approach to build SERGHEI. This assumes you have cloned the Kokkos submodule. 1. Build Kokkos ``` bash buildKokkos [GPU_ARCHITECTURE] ``` The `GPU_ARCHITECTURE` argument is optional (it defaults to a shared memory CPU architecture). If provided it must be a valid [GPU architecture string as defined by Kokkos](https://kokkos.org/kokkos-core-wiki/keywords.html?highlight=ampere80#architectures), and matching your GPU architecture. This configures and builds Kokkos with the target backend and architecture. 2. Configure and build SERGHEI ``` cmake . [-DSERGHEI_OPTION -DSERGHEI_OPTION ...] make ``` Note that when builing SERGHEI, there is no backend/architecture selection (this was done in the Kokkos build step). Build options for SERGHEI are passed to CMake as usual, e.g., `-DSERGHEI_WRITE_HZ=ON`. Read the [documentation on the available build options](https://gitlab.com/serghei-model/serghei/-/wikis/CMake-build-options). # Non-Cmake installation This procedure is deprecated, and only partially maintained for legacy reasons. It will be progressively phased-out. ## Environment configuration The environment needs to be configured properly for this procedure to be consistent. ### Automatic configuration SERGHEI provides the `scripts/configure.sh` script to help you configure your environment for SERGHEI, including setting up environment variables. You can simply run ```bash bash ./scripts/configure.sh -a ``` which will fetch the dependencies and install them in your local SERGEHI directory. In case you have already the dependencies and wish to use existing paths, take a look at ``` bash ./scripts/configure.sh --help ``` ### Understanding the (manual) environment configuration You need to set the SERGHEIPATH environment variable to your local SERGHEI root path. ``` export SERGHEIPATH=/path/to/serghei ``` Keep in mind doing so is not a persisting configuration. For persisting configuration, you can set this in your `.bashrc` file. Paths in the build scripts, but also in other workflows in SERGHEI use the `SERGHEIPATH` environment variable. ## Building SERGHEI with Make SERGHEI can be built through `make`. Before attempting to compile, the correct paths pointing to `Kokkos` and `PNetCDF` must be included in the `src/Makefile`. The default paths for these are `$SERGHEIPATH/Kokkos` and `$SERGHEIPATH/PnetCDF` or system paths. If you have these dependencies elsewhere, point the variable `PNETCDF_PATH` to the `PNetCDF` base folder and the variable `KOKKOS_PATH` to the `Kokkos` base folder. The automatic environment configuration should also do this for you. To build SERGHEI, use either one of these: + `make arch=cpu`: compiles the code for CPU (OpenMP + MPI) + `make arch=gpu device=DEVICESM`: compiles the code for GPU (CudaUVM + MPI). DEVICESM is the keyword for the device architecture for your GPUs. The list of architecture keywords can be found [here](https://github.com/kokkos/kokkos/wiki/Compiling#table-43-architecture-variables). Commonly used may be, for example Ampere80, Volta70, Pascal61, etc. This places an executable `serghei` into the directory `$SERGHEITPATH/bin` directory. Further commands: + `make clean`: cleans the program objects and the exe file + `make kclean`: cleans the prgram objects, the exe file and the Kokkos objects ### Model component compilation flags SERHGEI's Makefile includes flags to control which model components are compiled. The default configuration of SERGHEI components is hardcoded. These flags allow you to control which model components are actually compiled. The possible flags are commented inside the Makefile, therefore, unless you either uncomment some of them, or pass them to Make through the command line, you will get the default compilation configuration. You can control through the command line the setup, for example ``` $ make arch=cpu SERGHEI_SUBSURFACE_MODEL=1 ``` will compile SERGHEI for CPUs, including the subsurface solver. Another example, to compile for Volta GPUs, including subsurface and particle tracking modules is ``` $ make arch=gpu device=Volta70 SERGHEI_SUBSURFACE_MODEL=1 SERGHEI_PARTICLE_TRACKING=1 ``` Take a look at the [full list of compilation flags](https://gitlab.com/serghei-model/serghei/-/wikis/User-Guide/Build-flags). # Running SERGHEI Once installed, SERGHEI can be invoked by: ``` $ mpirun -n N ./serghei inputDir/ outputDir/ M ``` where + `N`: number of MPI tasks (subdomains). This must be in accordance with the partition chosen in `parameters.input` + `inputDir`: directory where the input files are located + `outputDir`: directory where the output files will be located + `M`: number of threads (OpenMP) or number of GPUs per resource set (GPUs) ## Examples and test cases Take a look at the [collection of test cases](https://gitlab.com/serghei-model/serghei-tests). To run the test case located at `cases/paraboloid2`, execute SERGHEI with ``` $ mpirun -n 2 /path/to/serghei/bin/serghei ../cases/paraboloid2/ output/ 4 ``` Depending on the architecture, this command causes different things to happen: 1. If the code has been compiled for CPU, this means that it would be 2 subdomains (MPI tasks) parallelized with 4 threads per subdomain (OpenMP). 2. If the code has been compiled for GPU, this means that it would be 8 subdomains (MPI tasks). The code is run on 2 nodes, each of them containing 4 GPUs. Similarly, the use of `mpirun` is conditioned to the execution with MPI and the corresponding architecture. For example, the code can be run just using: ``` /path/to/serghei/bin/serghei ./cases/paraboloid2/ output/ 1 ``` # Known issues + The `clang` compiler may fail to correctly load the OpenMP library. Thus, if SERGHEI is compiled with `clang`, OpenMP may not be available. + `gcc-10` has trouble compiling Parallel NetCDF and throws a type mismatch errors. The errors can be turned into warnings by passing ``` FCFLAGS=\"-fallow-argument-mismatch\" FFLAGS=\"-fallow-argument-mismatch\" ``` to `configure` and `make`. See [this github issue](https://github.com/Unidata/netcdf-fortran/issues/212). # How to cite Please cite the software using the corresponding [SERGHEI Zenodo DOI](https://doi.org/10.5281/zenodo.8159947), and if necessary with the specific release DOI. You can refer to the [SERGHEI-SWE paper](https://gmd.copernicus.org/articles/16/977/2023/) for the shallow water module SERGHEI-SWE. ``` @Article{Caviedes2023, AUTHOR = {Caviedes-Voulli\\`eme, D. and Morales-Hern\\'andez, M. and Norman, M. R. and \\\"Ozgen-Xian, I.}, TITLE = {SERGHEI (SERGHEI-SWE) v1.0: a performance portable high-performance parallel-computing shallow-water solver for hydrology and environmental hydraulics}, JOURNAL = {Geoscientific Model Development Discussions}, VOLUME = {16} YEAR = {2023}, PAGES = {977--1008}, URL = {https://gmd.copernicus.org/articles/16/977/2023/}, DOI = {10.5194/gmd-16-977-2023} } ```\n",
                "dependencies": "# This is the CMakeLists file for building and compiling SERGHEI # To use, open a terminal, cd to the SERGHEI directory and type \"cmake .\" # Then, type \"make\" to compile SERGHEI # Note: If you changed any cmake options herein, before rebuilding, delete \"CMakeCache.txt\" to erase old cmake settings cmake_minimum_required (VERSION 3.20) #IF(${CMAKE_VERSION} VERSION_GREATER_EQUAL \"3.12.0\") # MESSAGE(STATUS \"Setting policy CMP0074 to use <Package>_ROOT variables\") # CMAKE_POLICY(SET CMP0074 NEW) #ENDIF() # If needed, specify the CXX compiler here, otherwise comment out these lines set(CMAKE_C_COMPILER gcc) set(CMAKE_CXX_COMPILER g++) # Name the project project (serghei LANGUAGES CXX) # Enable color output for messages string(ASCII 27 Esc) set(ColourReset \"${Esc}[m\") set(ColourBold \"${Esc}[1m\") set(Red \"${Esc}[31m\") set(Green \"${Esc}[32m\") set(Yellow \"${Esc}[33m\") set(Blue \"${Esc}[34m\") set(Magenta \"${Esc}[35m\") set(Cyan \"${Esc}[36m\") set(White \"${Esc}[37m\") set(BoldRed \"${Esc}[1;31m\") set(BoldGreen \"${Esc}[1;32m\") set(BoldYellow \"${Esc}[1;33m\") set(BoldBlue \"${Esc}[1;34m\") set(BoldMagenta \"${Esc}[1;35m\") set(BoldCyan \"${Esc}[1;36m\") set(BoldWhite \"${Esc}[1;37m\") set(Gray \"${Esc}[90m\") set(DebugColour ${Magenta}) set(EnableColour ${Green}) set(DisableColour ${Yellow}) set(ncColour ${Cyan}) set(MiscColour ${Blue}) set(DefaultColour ${Gray}) # colour used for default options list(APPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/cmake) if(NOT DEFINED CMAKE_CXX_STANDARD) set(CMAKE_CXX_STANDARD 17) endif() set(CMAKE_CXX_STANDARD_REQUIRED On) set(CMAKE_CXX_EXTENSIONS Off) # find SERGHEI dependencies find_package(PkgConfig REQUIRED) if (PkgConfig_FOUND) pkg_check_modules(PnetCDF REQUIRED pnetcdf) else() set(PNETCDF_ROOT \"./PnetCDF\") set(PnetCDF_INCLUDE_DIRS ${PNETCDF_ROOT}/include) set(PnetCDF_LINK_LIBRARIES ${PNETCDF_ROOT}/lib/libpnetcdf.a) endif() set(CMAKE_EXPORT_COMPILE_COMMANDS ON) # Specify the path where Kokkos is built set(Kokkos_ROOT \"./kokkos/install\") # Specify the path where KokkosKernels is built set(KokkosKernels_ROOT \"./kokkos-kernels/install\") # Specify if using GPU or not set(Enable_GPU \"OFF\") # If using GPU, specify the architecture if (Enable_GPU) if (NOT DEFINED CMAKE_CUDA_ARCHITECTURES) set(CMAKE_CUDA_ARCHITECTURES 80) endif() endif() # Specify if the CI tests will be built set(Build_Tests \"ON\") # Specify SERGHEI options (You can add more below) # The defaults should be consistent with what is defined in SERGHEI source code execute_process(COMMAND git describe COMMAND tr -d '\\n' OUTPUT_VARIABLE serghei_git_version) add_compile_definitions(SERGHEI_GIT_VERSION=\"${serghei_git_version}\") message(STATUS \"SERGHEI version: \" ${serghei_git_version}) message(STATUS ${ColourBold} \"SERGHEI OPTIONS\" ${ColourReset} \" (\" ${DefaultColour} \"Default \" ${ColourReset} \"| \" ${EnableColour} \"Enabled \" ${ColourReset} \"| \" ${DisableColour} \"Disabled \" ${ColourReset} \"| \" ${DebugColour} \"Debug\" ${ColourReset} \")\" ) option(SERGHEI_SWE_GW \"Enable coupled shallow water and subsurface models\" OFF) if(SERGHEI_SWE_GW) add_compile_definitions(SERGHEI_SWE_GW=1) message(STATUS ${EnableColour} \"Coupled shallow water and subsurface models enabled\" ${ColourReset}) set(SERGHEI_SWE_MODEL ON) set(SERGHEI_SUBSURFACE_MODEL ON) else() add_compile_definitions(SERGHEI_SWE_GW=0) message(STATUS ${DefaultColour} \"Coupled shallow water and subsurface models disabled\" ${ColourReset}) endif() option(SERGHEI_SUBSURFACE_MODEL \"Enable subsurface model\" OFF) if(SERGHEI_SUBSURFACE_MODEL) add_compile_definitions(SERGHEI_SUBSURFACE_MODEL=1) if(NOT SERGHEI_SWE_GW) set(SERGHEI_SWE_MODEL OFF) endif() message(STATUS ${EnableColour} \"Subsurface model enabled\" ${ColourReset}) # Specify the path where KokkosKernels is built #set(KokkosKernels_ROOT \"/Users/zli/Codes/kokkoskernels4-omp\") endif() option(SERGHEI_SWE_MODEL \"Enable shallow water model\" ON) if(SERGHEI_SWE_MODEL) add_compile_definitions(SERGHEI_SWE_MODEL=1) message(STATUS ${DefaultColour} \"Shallow water model enabled\" ${ColourReset}) else() add_compile_definitions(SERGHEI_SWE_MODEL=0) message(STATUS ${DisableColour} \"Shallow water model disabled\" ${ColourReset}) endif() option(SERGHEI_LPT \"Enable particle tracking\" OFF) if(SERGHEI_LPT) add_compile_definitions(SERGHEI_LPT=1) message(STATUS ${EnableColour} \"Particle tracking enabled\" ${ColourReset}) endif() option(SERGHEI_HYDRODYNAMIC_NOT_EVOLUTION \"Enable not hydrodynamic evolution\" OFF) if(SERGHEI_HYDRODYNAMIC_NOT_EVOLUTION) add_compile_definitions(SERGHEI_HYDRODYNAMIC_NOT_EVOLUTION=1) message(STATUS ${EnableColour} \"Not hydrodynamic evolution enabled\" ${ColourReset}) endif() option(SERGHEI_VEGETATION_MODEL \"Enable vegetation model\" OFF) if(SERGHEI_VEGETATION_MODEL) add_compile_definitions(SERGHEI_VEGETATION_MODEL=1) message(STATUS ${EnableColour} \"Vegetation model enabled\" ${ColourReset}) endif() option(SERGHEI_TOOLS \"Enable additional tools\" ON) if(SERGHEI_TOOLS) message(STATUS ${DefaultColour} \"Tools enabled\" ${ColourReset}) else() message(STATUS ${DisableColour} \"Tools disabled\" ${ColourReset}) add_compile_definitions(SERGHEI_TOOLS=0) endif() option(SERGHEI_FRICTION_MODEL \"Enable friction model\" ON) if(SERGHEI_FRICTION_MODEL) message(STATUS ${DefaultColour} \"Friction model enabled (Manning default)\" ${ColourReset}) else() message(STATUS ${DisableColour} \"Friction model disabled\" ${ColourReset}) add_compile_definitions(SERGHEI_FRICTION_MODEL=0) endif() option(SERGHEI_FRICTION_CENTERED \"Enabled centered friction discretisation\" OFF) if(SERGHEI_FRICTION_CENTERED) message(STATUS ${EnableColour} \"Centered friction discretisation enabled\" ${ColourReset}) add_compile_definitions(SERGHEI_POINTWISE_FRICTION=1) else() message(STATUS ${DefaultColour} \"Centered friction discretisation disabled\" ${ColourReset}) endif() option(SERGHEI_FRICTION_DARCY_WEISBACH \"Enable Darcy-Weisbach friction model\" OFF) if(SERGHEI_FRICTION_DARCY_WEISBACH) add_compile_definitions(SERGHEI_FRICTION_MODEL=2) message(STATUS ${EnableColour} \"Enabling Darcy-Weisbach friction model\" ${ColourReset}) endif() option(SERGHEI_FRICTION_CHEZY \"Enable Chezy friction model\" OFF) if(SERGHEI_FRICTION_CHEZY) add_compile_definitions(SERGHEI_FRICTION_MODEL=3) message(STATUS ${EnableColour} \"Enabling Chezy friction model\" ${ColourReset}) endif() option(SERGHEI_MESH_UNIFORM \"Set uniform cartesian grid mesh\" ON) if(SERGHEI_MESH_UNIFORM) message(STATUS ${DefaultColour} \"Mesh set to uniform cartesian grid\" ${ColourReset}) else() message(SEND_ERROR \"Non-uniform mesh not available\") endif() # Arithmetic precision options option(SERGHEI_FP32 \"Set single precision computations\" OFF) if(SERGHEI_FP32) message(STATUS ${MiscColour} \"Single precision set\" ${ColourReset}) add_compile_definitions(SERGHEI_REAL=1) set(SERGHEI_NC_FLOAT ON) set(SERGHEI_NC_DOUBLE OFF) else() message(STATUS ${DefaultColour} \"Double precision set\" ${ColourReset}) add_compile_definitions(SERGHEI_REAL=2) endif() # I/O options option(SERGHEI_MAXFLOOD \"Enable maximum flood statistics\" OFF) if(SERGHEI_MAXFLOOD) message(STATUS ${EnableColour} \"Enabling maximum flood statistics\" ${ColourReset}) add_compile_definitions(SERGHEI_MAXFLOOD=1) endif() option(SERGHEI_WRITE_HZ \"Write water surface elevation output\" OFF) if(SERGHEI_WRITE_HZ) message(STATUS ${EnableColour} \"Writing water surface elevation output\" ${ColourReset}) add_compile_definitions(SERGHEI_WRITE_HZ=1) endif() option(SERGHEI_INPUT_NETCDF \"Enable NetCDF input files\" OFF) if(SERGHEI_INPUT_NETCDF) message(STATUS ${EnableColour} \"NetCDF input enabled\" ${ColourReset}) add_compile_definitions(SERGHEI_INPUT_NETCDF=1) endif() option(SERGHEI_WRITE_SUBDOMS \"Enable writing subdomain indices in NetCDF output\" OFF) if(SERGHEI_WRITE_SUBDOMS) message(STATUS ${EnableColour} \"Subdomain indices enabled in NetCDF output\" ${ColourReset}) add_compile_definitions(SERGHEI_WRITE_SUBDOMS=1) endif() # NetCDF options # A useful option for large problems is NC_64BIT_DATA. option(SERGHEI_NC_CLOBBER \"Set NetCDF mode to NC_CLOBBER\" ON) option(SERGHEI_NC_64BIT_DATA \"Set NetCDF mode to NC_64BIT_DATA\" OFF) if(SERGHEI_NC_64BIT_DATA) add_compile_definitions(SERGHEI_NC_MODE=NC_64BIT_DATA) message(STATUS ${ncColour} \"NetCDF mode set to NC_64BIT_DATA\" ${ColourReset}) set(SERGHEI_NC_CLOBBER OFF) endif() # You can set up SERGHEI_NC_MODE and SERGHEI_NC_REAL to control NetCDF file behavior. # SERGHEI_NC_REAL determines if data is written as double or float (default follows SERGHEI_REAL). # Override at compile time with SERGHEI_NC_REAL=NC_DOUBLE or SERGHEI_NC_REAL=NC_FLOAT. option(SERGHEI_NC_FLOAT \"Set single precision for NetCDF\" OFF) if(SERGHEI_NC_FLOAT) message(STATUS ${MiscColour} \"NetCDF output set to single precision\" ${ColourReset}) add_compile_definitions(SERGHEI_NC_REAL=NC_FLOAT) set(SERGHEI_NC_DOUBLE OFF) endif() option(SERGHEI_NC_DOUBLE \"Set double precision for NetCDF\" ON) if(SERGHEI_NC_DOUBLE) message(STATUS ${DefaultColour} \"NetCDF output set to double precision\" ${ColourReset}) endif() option(SERGHEI_NC_ENABLE_NAN \"Enable writing NaN for cells outside the domain\" ON) if(SERGHEI_NC_ENABLE_NAN) message(STATUS ${DefaultColour} \"NetCDF NaN enabled\" ${ColourReset}) else() message(STATUS ${MiscColour} \"NetCDF NaN disabled\" ${ColourReset}) add_compile_definitions(SERGHEI_NC_ENABLE_NAN=0) endif() # debugging options option(SERGHEI_DEBUG_PARALLEL_DECOMPOSITION \"Enable parallel decomposition debugging\" OFF) if(SERGHEI_DEBUG_PARALLEL_DECOMPOSITION) add_compile_definitions(SERGHEI_DEBUG_PARALLEL_DECOMPOSITION=1) message(STATUS ${DebugColour} \"Parallel decomposition debugging enabled\" ${ColourReset}) set(CMAKE_BUILD_TYPE \"Debug\") endif() option(SERGHEI_DEBUG_WORKFLOW \"Enable workflow debugging\" OFF) if(SERGHEI_DEBUG_WORKFLOW) add_compile_definitions(SERGHEI_DEBUG_WORKFLOW=1) message(STATUS ${DebugColour} \"Workflow debugging enabled\" ${ColourReset}) set(CMAKE_BUILD_TYPE \"Debug\") endif() option(SERGHEI_DEBUG_KOKKOS_SETUP \"Enable Kokkos setup debugging\" OFF) if(SERGHEI_DEBUG_KOKKOS_SETUP) add_compile_definitions(SERGHEI_DEBUG_KOKKOS_SETUP=1) message(STATUS ${DebugColour} \"Kokkos setup debugging enabled\" ${ColourReset}) set(CMAKE_BUILD_TYPE \"Debug\") endif() option(SERGHEI_DEBUG_BOUNDARY \"Enable boundary debugging\" OFF) if(SERGHEI_DEBUG_BOUNDARY) add_compile_definitions(SERGHEI_DEBUG_BOUNDARY=1) message(STATUS ${DebugColour} \"Boundary debugging enabled\" ${ColourReset}) set(CMAKE_BUILD_TYPE \"Debug\") endif() option(SERGHEI_DEBUG_DT \"Enable timestep debugging\" OFF) if(SERGHEI_DEBUG_DT) add_compile_definitions(SERGHEI_DEBUG_DT=1) message(STATUS ${DebugColour} \"Time stepping debugging enabled\" ${ColourReset}) set(CMAKE_BUILD_TYPE \"Debug\") endif() option(SERGHEI_DEBUG_TOOLS \"Enable tools debugging\" OFF) if(SERGHEI_DEBUG_TOOLS) add_compile_definitions(SERGHEI_DEUBG_TOOLS=1) message(STATUS ${DebugColour} \"Tools debugging enabled\" ${ColourReset}) set(CMAKE_BUILD_TYPE \"Debug\") endif() option(SERGHEI_DEBUG_MASS_CONS \"Enable mass conservation debugging\" OFF) if(SERGHEI_DEBUG_MASS_CONS) add_compile_definitions(SERGHEI_DEBUG_MASS_CONS=1) message(STATUS ${DebugColour} \"Mass conservation debugging enabled\" ${ColourReset}) set(CMAKE_BUILD_TYPE \"Debug\") endif() option(SERGHEI_DEBUG_INFILTRATION \"Enable infiltration debugging\" OFF) if(SERGHEI_DEBUG_INFILTRATION) add_compile_definitions(SERGHEI_DEBUG_INFILTRATION=1) message(STATUS ${DebugColour} \"Infiltration debugging enabled\" ${ColourReset}) set(CMAKE_BUILD_TYPE \"Debug\") endif() option(SERGHEI_DEBUG_MPI \"Enable MPI debugging\" OFF) if(SERGHEI_DEBUG_MPI) add_compile_definitions(SERGHEI_DEBUG_MPI=1) message(STATUS ${DebugColour} \"MPI debugging enabled\" ${ColourReset}) set(CMAKE_BUILD_TYPE \"Debug\") endif() option(SERGHEI_DEBUG_INPUT_NETCDF \"Enable NetCDF input debugging\" OFF) if(SERGHEI_DEBUG_INPUT_NETCDF) add_compile_definitions(SERGHEI_DEBUG_INPUT_NETCDF=1) message(STATUS ${DebugColour} \"NetCDF input debugging enabled\" ${ColourReset}) set(CMAKE_BUILD_TYPE \"Debug\") endif() option(SERGHEI_DEBUG_OUTPUT \"Enable output debugging\" OFF) if(SERGHEI_DEBUG_OUTPUT) add_compile_definitions(SERGHEI_DEBUG_OUTPUT=1) message(STATUS ${DebugColour} \"Output debugging enabled\" ${ColourReset}) set(CMAKE_BUILD_TYPE \"Debug\") endif() if(CMAKE_BUILD_TYPE STREQUAL \"Debug\") message(STATUS ${DebugColour} \"Enabling debug mode due to enabled debugging flags\" ${ColourReset}) endif() # this is to print variables #include(CMakePrintHelpers) #cmake_print_variables(serghei_git_version) # print out the compile defintions, useful to debug CMakeLists # get_directory_property( DirDefs DIRECTORY ${CMAKE_SOURCE_DIR} COMPILE_DEFINITIONS ) # message(STATUS \"SERGHEI Compilation definitions\") #foreach( d ${DirDefs} ) # message( STATUS \"\\t\" ${d}) # endforeach() # Add the subdirectory where the source codes are stored add_subdirectory(src)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/sfctools",
            "repo_link": "https://gitlab.com/dlr-ve/esy/sfctools/framework",
            "content": {
                "codemeta": "",
                "readme": "# sfctools - A toolbox for stock-flow consistent, agent-based models Sfctools is a lightweight and easy-to-use Python framework for agent-based macroeconomic, stock-flow consistent (ABM-SFC) modeling. It concentrates on agents in economics and helps you to construct agents, helps you to manage and document your model parameters, assures stock-flow consistency, and facilitates basic economic data structures (such as the balance sheet). For more documentation, see https://sfctools-framework.readthedocs.io/en/latest/. ## Installation We recommend to install sfctools in a fresh Python 3.8 environment. For example, with conda, do conda create --name sfcenv python=3.8 conda activate sfcenv conda install pip Then, in a terminal of your choice, type: pip install sfctools see https://pypi.org/project/sfctools/ ## Usage with Graphical User Interface 'Attune' Type python -m sfctools attune to start the GUI. ## Usage inside Python Try out this simple example: ```python from sfctools import Agent, World class SomeAgent(Agent): def __init__(self, a): super().__init__() self.some_attribute = a my_agent = SomeAgent(a='Hello') your_agent = SomeAgent(a='World') my_agents = World().get_agents_of_type(\"SomeAgent\") my_message = my_agents[0].some_attribute your_message = my_agents[1].some_attribute print(\"%s says %s, %s says %s\" % (my_agent, my_message, your_agent, your_message)) ``` The resulting output will be ```console SomeAgent__00001 says Hello, SomeAgent__00002 says World ``` ## More Examples Have a look at the [documentation page](https://sfctools-framework.readthedocs.io/en/latest/doc_api_examples/examples_framework.html) for more examples. ## Cite this Software You can cite the software as follows: Baldauf, T., (2023). sfctools - A toolbox for stock-flow consistent, agent-based models. Journal of Open Source Software, 8(87), 4980, https://doi.org/10.21105/joss.04980 You can cite the software repository as follows: Thomas Baldauf. (2023). sfctools - A toolbox for stock-flow consistent, agent-based models (1.1.0.2b). Zenodo. https://doi.org/10.5281/zenodo.8118870 ----------------------------------- | Corresponding author: Thomas Baldauf, German Aerospace Center (DLR), Curiestr. 4 70563 Stuttgart | thomas.baldauf@dlr.de |\n",
                "dependencies": "[tool.poetry] name = \"sfctools\" version = \"1.1.9.2\" description = \"Framework for stock-flow consistent agent-based modeling, being developed at the German Aerospace Center (DLR) for and in the scientific context of energy systems analysis, however, it is widely applicable in other scientific fields.\" authors = [\"Thomas Baldauf <thomas.baldauf@dlr.de>\"] license = \"MIT\" maintainers = [\"Thomas Baldauf, Benjamin Fuchs <thomas.baldauf@dlr.de, benjamin.fuchs@dlr.de>\"] homepage = 'https://gitlab.com/dlr-ve/esy/sfctools/framework' documentation = 'https://sfctools-framework.readthedocs.io/en/latest/' readme = 'README.md' keywords = [\"stock-flow-consistent\",\"agent-based\",\"agent\",\"macroeconomics\",\"computational economics\"] packages = [ { include = \"sfctools\" }, ] [tool.poetry.dependencies] python = \">=3.6,<=3.12\" pyyaml = \">=3.0.3\" # \">=3.10\" # >=6.0 setuptools = \">=50.0.0\" pandas = \"*\"# \">=1.3\" numpy = \"*\"# \">=1.6.0\" attrs = \"*\" # \">=19.3.0\" matplotlib = \">=2.0.0\" # \">=3.6\" seaborn = \"*\" # \">=0.9.0\" # \">=0.11.2\" networkx = \">=2.2\" graphviz = \"*\" # \">=0.19\" PyQt5 =\">=5.9\" # pyqt5-tools =\"^5.14.4.3\" # pyqt5-plugins =\"^5.14.4.2\" # poetry = \"*\" # attrs = \"^19\" # \"19.3.0\" sympy= \">=1.10.0\" # \">=1.10\" scipy=\">1.9.1\" # >1.9.1 # pydot = \"*\" # \">=1.4\" pyperclip = \">=1.5.0\" openpyxl = \"*\" # \">=3.0.10\" might be needed in later versions # pdflatex = \"^0.1\" <- compatibility problem with cattrs and pdflatex pytest-qt = \"*\" pytest-cov = \"*\" [tool.poetry.dev-dependencies] [build-system] requires = [\"poetry-core>=1.0.0\"] build-backend = \"poetry.core.masonry.api\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/shockhash",
            "repo_link": "https://github.com/ByteHamster/ShockHash",
            "content": {
                "codemeta": "",
                "readme": "# ShockHash [![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0) ![Build status](https://github.com/ByteHamster/ShockHash/actions/workflows/build.yml/badge.svg) A minimal perfect hash function (MPHF) maps a set S of n keys to the first n integers without collisions. Perfect hash functions have applications in databases, bioinformatics, and as a building block of various space-efficient data structures. ShockHash (**s**mall, **h**eavily **o**verloaded **c**uc**k**oo **hash** tables) is an MPHF that achieves space very close to the lower bound, while still being fast to construct. In contrast to the simple brute-force approach that needs to try e^n = 2.72^n different hash function seeds, ShockHash significantly reduces the search space. Instead of sampling hash functions hoping for them to be minimal perfect, it samples random graphs, hoping for them to be a pseudoforest. In its most space-efficient variant, it can reduce the running time to just 1.16^n, while still being asymptotically space optimal. Still being an exponential time algorithm, we integrate ShockHash into several partitioning frameworks. Our implementation inside the [RecSplit](https://github.com/vigna/sux/blob/master/sux/function/RecSplit.hpp) framework achieves the best space efficiency. Using ShockHash inside our novel k-perfect hash function achieves fast queries while still being faster to construct and more space efficient than any previous approaches. ### Library Usage Clone this repo and add the following to your `CMakeLists.txt`. Note that the repo has submodules, so either use `git clone --recursive` or `git submodule update --init --recursive`. ```cmake add_subdirectory(path/to/ShockHash) target_link_libraries(YourTarget PRIVATE ShockHash) ``` Then use one of the following classes: - [ShockHash](https://github.com/ByteHamster/ShockHash/blob/main/include/ShockHash.h) is the original ShockHash algorithm integrated into the RecSplit framework. - [SIMDShockHash](https://github.com/ByteHamster/ShockHash/blob/main/include/SIMDShockHash.hpp) is the SIMD-parallel version of the original ShockHash algorithm. Both ShockHash and the RecSplit framework are SIMD-parallelized. If this implementation is used on a machine without SIMD support, it is slower than the non-SIMD version because SIMD operations are emulated. - [ShockHash2](https://github.com/ByteHamster/ShockHash/blob/main/include/ShockHash2.h) is the bipartite ShockHash algorithm. Only the inner ShockHash loop is SIMD-parallel, the RecSplit framework is not. If this implementation is used on a machine without SIMD support, the implementation uses sequential operations without explicitly emulating SIMD. To turn off SIMD, change to SIMD lanes of size 1 in [ShockHash2-internal.h](https://github.com/ByteHamster/ShockHash/blob/main/include/ShockHash2-internal.h). Constructing a ShockHash perfect hash function is then straightforward: ```cpp std::vector<std::string> keys = {\"abc\", \"def\", \"123\", \"456\"}; shockhash::ShockHash<30, false> shockHash(keys, 2000); // ShockHash base case size n=30, bucket size b=2000 std::cout << shockHash(\"abc\") << \" \" << shockHash(\"def\") << \" \" << shockHash(\"123\") << \" \" << shockHash(\"456\") << std::endl; // Output: 1 3 2 0 ``` We also give the base-case implementations without the RecSplit framework, which makes it easier to understand the main idea. - Original [ShockHash](https://github.com/ByteHamster/ShockHash/blob/main/benchmark/bijections/ShockHash1.h). - [Bipartite ShockHash](https://github.com/ByteHamster/ShockHash/blob/main/include/ShockHash2-internal.h). The outer loop that is also given in the pseudocode of the paper is given in `BijectionsShockHash2::findSeed`. ### Construction performance [![Plots preview](https://raw.githubusercontent.com/ByteHamster/ShockHash/main/plots.png)](https://arxiv.org/abs/2310.14959) ### Licensing ShockHash is licensed exactly like `libstdc++` (GPLv3 + GCC Runtime Library Exception), which essentially means you can use it everywhere, exactly like `libstdc++`. You can find details in the [COPYING](/COPYING) and [COPYING.RUNTIME](/COPYING.RUNTIME) files. If you use [ShockHash](https://arxiv.org/abs/2308.09561) or [bipartite ShockHash](https://arxiv.org/abs/2310.14959) in an academic context or publication, please cite our papers: ``` @inproceedings{lehmann2023shockhash, author = {Hans-Peter Lehmann and Peter Sanders and Stefan Walzer}, title = {{ShockHash}: Towards Optimal-Space Minimal Perfect Hashing Beyond Brute-Force}, booktitle = {{ALENEX}}, pages = {194--206}, publisher = {{SIAM}}, year = {2024}, doi = {10.1137/1.9781611977929.15} } @article{lehmann2023towardsArxiv, author = {Hans-Peter Lehmann and Peter Sanders and Stefan Walzer}, title = {{ShockHash}: Towards Optimal-Space Minimal Perfect Hashing Beyond Brute-Force}, journal = {CoRR}, volume = {abs/2308.09561}, year = {2023}, doi = {10.48550/ARXIV.2308.09561} } ```\n",
                "dependencies": "cmake_minimum_required (VERSION 3.25...4.0) project(ShockHash DESCRIPTION \"Various basic data structures\" HOMEPAGE_URL \"https://github.com/ByteHamster/ShockHash\" VERSION 1.0 LANGUAGES CXX) if(TARGET ShockHash) return() endif() if (NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE \"Release\") endif () if((CMAKE_BUILD_TYPE STREQUAL \"Release\" OR CMAKE_BUILD_TYPE STREQUAL \"RelWithDebInfo\") AND PROJECT_IS_TOP_LEVEL) add_compile_options(-march=native) endif() # ---------------------------- Dependencies ---------------------------- if(NOT TARGET vectorclass) add_library(vectorclass INTERFACE) target_include_directories(vectorclass INTERFACE extlib/vectorclass) endif() if(NOT TARGET Sux) add_library(Sux INTERFACE) target_include_directories(Sux SYSTEM INTERFACE extlib/sux) endif() if(NOT TARGET tlx) set(TLX_INSTALL_INCLUDE_DIR tlx CACHE PATH \"Workaround for TLX breaking the first cmake call\") add_subdirectory(extlib/tlx SYSTEM EXCLUDE_FROM_ALL) endif() if(NOT TARGET simpleRibbon) set(IPS2RA_DISABLE_PARALLEL ON CACHE PATH \"ips2ra's FindTBB greps a file that does not exist in recent TBB versions\") add_subdirectory(extlib/simpleRibbon SYSTEM EXCLUDE_FROM_ALL) find_package(TBB) target_compile_options(ips2ra INTERFACE -D_REENTRANT) target_link_libraries(ips2ra INTERFACE pthread atomic TBB::tbb) endif() if(NOT TARGET Ips2raShockHashSorter) add_library(Ips2raShockHashSorter SHARED src/Sorter.cpp) target_compile_features(Ips2raShockHashSorter PRIVATE cxx_std_20) target_include_directories(Ips2raShockHashSorter PRIVATE include) target_link_libraries(Ips2raShockHashSorter PUBLIC ips2ra tlx Sux) target_compile_options(Ips2raShockHashSorter PRIVATE $<$<COMPILE_LANGUAGE:CXX>:-march=native>) endif() if(NOT TARGET ByteHamsterUtil) add_subdirectory(extlib/util EXCLUDE_FROM_ALL) endif() # ---------------------------- Library Setup ---------------------------- add_library(ShockHash2Precompiled SHARED src/ShockHash2-precompiled.cpp) target_compile_features(ShockHash2Precompiled PRIVATE cxx_std_20) target_include_directories(ShockHash2Precompiled PRIVATE include) target_link_libraries(ShockHash2Precompiled PUBLIC ips2ra tlx Sux ByteHamsterUtil) target_compile_options(ShockHash2Precompiled PRIVATE $<$<COMPILE_LANGUAGE:CXX>:-march=native>) add_library(ShockHash INTERFACE) target_include_directories(ShockHash INTERFACE include) target_compile_features(ShockHash INTERFACE cxx_std_20) target_link_libraries(ShockHash INTERFACE Sux SimpleRibbon Ips2raShockHashSorter ByteHamsterUtil ShockHash2Precompiled) include(${CMAKE_CURRENT_SOURCE_DIR}/extlib/cmake-findsse/FindSSE.cmake) FindSSE() if(SSE4_2_FOUND) add_library(ShockHashSIMD INTERFACE) target_link_libraries(ShockHashSIMD INTERFACE ShockHash vectorclass) target_compile_options(ShockHashSIMD INTERFACE -DSIMD -march=native) endif() # ---------------------------- Benchmarks ---------------------------- if(PROJECT_IS_TOP_LEVEL) add_library(BenchmarkUtils INTERFACE) target_include_directories(BenchmarkUtils INTERFACE benchmark) target_include_directories(BenchmarkUtils INTERFACE test) target_link_libraries(BenchmarkUtils INTERFACE tlx ByteHamster::Util) # Warnings if this is the main project target_compile_options(ShockHash INTERFACE $<$<COMPILE_LANGUAGE:CXX>:-Wall -Wextra -Wpedantic -Werror -Wno-error=stringop-overflow -frecord-gcc-switches>) add_executable(Benchmark benchmark/benchmark_construction.cpp) target_link_libraries(Benchmark PUBLIC BenchmarkUtils ShockHash) if(SSE4_2_FOUND) add_executable(BenchmarkSIMD benchmark/benchmark_construction.cpp) target_link_libraries(BenchmarkSIMD PUBLIC BenchmarkUtils ShockHashSIMD) endif() add_executable(NumHashEvals benchmark/numHashEvals.cpp) target_link_libraries(NumHashEvals PUBLIC BenchmarkUtils ShockHash) add_executable(Bijections benchmark/bijections.cpp) target_link_libraries(Bijections PUBLIC ShockHash BenchmarkUtils) add_executable(GolombMemoTuner benchmark/golombMemoTuner.cpp) target_link_libraries(GolombMemoTuner PUBLIC ShockHash BenchmarkUtils) endif()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/sichash",
            "repo_link": "https://github.com/ByteHamster/SicHash",
            "content": {
                "codemeta": "",
                "readme": "# SicHash [![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0) ![Build status](https://github.com/ByteHamster/SicHash/actions/workflows/build.yml/badge.svg) A perfect hash function (PHF) maps a set S of n keys to the first m integers without collisions. It is called _minimal_ perfect (MPHF) if m=n. Perfect hash functions have applications in databases, bioinformatics, and as a building block of various space-efficient data structures. SicHash is a (minimal) perfect hash function based on irregular cuckoo hashing, retrieval, and overloading. Each input key has a small number of choices for output positions. Using cuckoo hashing, SicHash determines a mapping from each key to one of its choices, such that there are no collisions between keys. It then stores the mapping from keys to their candidate index space-efficiently using the [BuRR](https://github.com/lorenzhs/BuRR) retrieval data structure. SicHash offers a very good trade-off between construction performance, query performance, and space consumption. ### Library Usage Clone this repo and add the following to your `CMakeLists.txt`. Note that the repo has submodules, so either use `git clone --recursive` or `git submodule update --init --recursive`. ``` add_subdirectory(path/to/SicHash) target_link_libraries(YourTarget PRIVATE SicHash) ``` Constructing a SicHash perfect hash function is then straightforward: ```cpp std::vector<std::string> keys = {\"abc\", \"def\", \"123\", \"456\"}; sichash::SicHashConfig config; sichash::SicHash<true> hashFunc(keys, config); std::cout << hashFunc(\"abc\") << std::endl; ``` ### Construction Performance [![Plots preview](https://raw.githubusercontent.com/ByteHamster/SicHash/main/plots-construction.png)](https://arxiv.org/pdf/2210.01560) ### Query Performance [![Plots preview](https://raw.githubusercontent.com/ByteHamster/SicHash/main/plots-query.png)](https://arxiv.org/pdf/2210.01560) ### Reproducing Experiments This repository contains the source code and our reproducibility artifacts for the benchmarks specific to SicHash. Benchmarks that compare SicHash to competitors are available in a different repository: https://github.com/ByteHamster/MPHF-Experiments We provide an easy to use Docker image to quickly reproduce our results. Alternatively, you can look at the `Dockerfile` to see all libraries, tools, and commands necessary to compile SicHash. #### Building the Docker Image Run the following command to build the Docker image. Building the image takes about 5 minutes, as some packages (including LaTeX for the plots) have to be installed. ```bash docker build -t sichash --no-cache . ``` Some compiler warnings (red) are expected when building competitors and will not prevent building the image or running the experiments. Please ignore them! #### Running the Experiments Due to the long total running time of all experiments in our paper, we provide run scripts for a slightly simplified version of the experiments. They run fewer iterations and output fewer data points. You can modify the benchmarks scripts in `scripts/dockerVolume` if you want to change the number of runs or data points. This does not require the Docker image to recompile. Different experiments can be started by using the following command: ```bash docker run --interactive --tty -v \"$(pwd)/scripts/dockerVolume:/opt/dockerVolume\" sichash /opt/dockerVolume/figure-1.sh ``` The number also refers to the figure in the paper. | Figure in paper | Launch command | Estimated runtime | | :-------------- | :---------------------------- | :----------------- | | 1 | /opt/dockerVolume/figure-1.sh | 10 minutes | The resulting plots can be found in `scripts/dockerVolume` and are called `figure-<number>.pdf`. More experiments comparing SicHash with competitors can be found in a different repository: https://github.com/ByteHamster/MPHF-Experiments ### License This code is licensed under the [GPLv3](/LICENSE). If you use the project in an academic context or publication, please cite [our paper](https://doi.org/10.1137/1.9781611977561.ch15): ``` @inproceedings{lehmann2023sichash, author = {Hans{-}Peter Lehmann and Peter Sanders and Stefan Walzer}, title = {SicHash - Small Irregular Cuckoo Tables for Perfect Hashing}, booktitle = {{ALENEX}}, pages = {176--189}, publisher = {{SIAM}}, year = {2023}, doi = {10.1137/1.9781611977561.CH15} } ```\n",
                "dependencies": "cmake_minimum_required(VERSION 3.25...4.0) list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_SOURCE_DIR}/cmake\") project(SicHash) if(TARGET SicHash) return() endif() if (NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE \"Release\") endif () if(CMAKE_BUILD_TYPE STREQUAL \"Release\" AND PROJECT_IS_TOP_LEVEL) add_compile_options(-march=native) endif() set(TLX_INSTALL_INCLUDE_DIR tlx CACHE PATH \"Workaround for TLX breaking the first cmake call\") add_subdirectory(extlib/tlx) add_library(SicHash INTERFACE) target_include_directories(SicHash INTERFACE include) target_compile_features(SicHash INTERFACE cxx_std_20) add_subdirectory(extlib/util EXCLUDE_FROM_ALL) target_link_libraries(SicHash INTERFACE ByteHamster::Util) add_subdirectory(extlib/simpleRibbon EXCLUDE_FROM_ALL) target_link_libraries(SicHash INTERFACE SimpleRibbon ips2ra) add_library(SicHash::sichash ALIAS SicHash) if(PROJECT_IS_TOP_LEVEL) target_compile_options(SicHash INTERFACE $<$<COMPILE_LANGUAGE:CXX>:-Wall -Wextra -Wpedantic -Werror -frecord-gcc-switches>) add_executable(Example src/example.cpp) target_link_libraries(Example PRIVATE SicHash) target_compile_features(Example PRIVATE cxx_std_20) add_executable(Solvers src/solvers.cpp) target_link_libraries(Solvers PRIVATE SicHash tlx) target_compile_features(Solvers PRIVATE cxx_std_20) add_executable(ConstructionSuccess src/constructionSuccess.cpp) target_link_libraries(ConstructionSuccess PRIVATE SicHash tlx) target_compile_features(ConstructionSuccess PRIVATE cxx_std_20) add_executable(SicHashBenchmark src/sicHashBenchmark.cpp) target_link_libraries(SicHashBenchmark PRIVATE SicHash tlx) target_compile_features(SicHashBenchmark PRIVATE cxx_std_20) add_executable(MaxLoadFactor src/maxLoadFactor.cpp) target_link_libraries(MaxLoadFactor PRIVATE SicHash tlx) target_compile_features(MaxLoadFactor PRIVATE cxx_std_20) endif()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/signal-processor",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/Signal-Processor",
            "content": {
                "codemeta": "",
                "readme": "![Python](https://img.shields.io/badge/Python-3670A0?style=plastic&logo=python&logoColor=ffdd54) ![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=plastic&logo=scipy&logoColor=%white) ![NumPy](https://img.shields.io/badge/Numpy-777BB4.svg?style=plastic&logo=numpy&logoColor=white) ![Plotly](https://img.shields.io/badge/Plotly-239120.svg?style=plastic&logo=plotly&logoColor=white) ![Matplotlib](https://img.shields.io/badge/Matplotlib-%233F4F75.svg?style=plastic&logo=plotly&logoColor=white) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/SignalProcessor?style=plastic) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13286179.svg)](https://doi.org/10.5281/zenodo.13286179) # SignalProcessor This repository provides a Python package for generating, filtering, fitting, and analyzing signals. The package includes functionalities for creating noisy signals, applying filters, fitting damped sine waves, and performing statistical analysis. ### Overview - Generate noisy sine wave signals (or import custom signals) - Apply Butterworth low-pass filters - Fit damped sine waves to filtered signals - Perform t-tests between filtered signals and fitted models - Compute and visualize Fourier Transforms ### Installation 1) Create and source virtual environment: ```shell python -m venv env source env/bin/activate # On Windows use `env\\Scripts\\activate` ``` 2) Install the dependencies: ```shell pip install -r requirements.txt ``` ### Running Tests Using unittest ```shell python -m unittest discover -s tests ``` ### Example An example demonstrating generating a signal, applying filters, fitting models, and performing analysis, exists in the `main.py`. >[!Note] > An example plot has been uploaded to the `plots` directory. ### Example Usage Generate a Noisy Signal ```shell import numpy as np from src.signal_processor import SignalProcessor timeVector = np.linspace(0, 1, 1000, endpoint = False) # Or consider importing or modifying your time vector generator = SignalGenerator(timeVector) generator.generateNoisySignal(frequency = 20, noiseStdDev = 0.6) # or with defaults: processor.generateNoisySignal() # frequency = 10, noiseStdDev = 0.5 ``` Apply a Filter (`butter`, `bessel`, `highpass`). Default is `butter`. ```shell from src.signal_filter import SignalFilter filteredInstance = generator.generateNoisySignal() \\ .applyFilter(filterType = 'butter', filterOrder = 4, cutOffFrequency = 0.2, bType = 'lowpass') # Or with different filter parameters: filteredInstance.setFilterParameters('bessel', 5, 0.5, 'highpass').applyFilter() ``` Fit a damped sine wave to the filtered signal ```shell from src.signal_fitter import SignalFitter # default sine wave parameters: amplitudeParam = 1.0, frequencyParam = 10.0, phaseParam = 0.0, decayRateParam = 0.1 fittedInstance = filteredInstance.fitDampedSineWave() # Or with custom parameters: fittedInstance.setDampedSineWaveParameters(3.0, 12.0, np.pi / 6, 0.3) fittedInstance.setDampedSineWaveBounds([0, 0, -np.pi/2, 0], [10, 20, np.pi/2, 1]) fittedInstance.fitDampedSineWave() ``` Perform a t-test between the filtered signal and the fitted damped sine wave ```shell from src.statistical_analyzer import StatisticalAnalyzer analyzedInstance = fittedInstance.analyzeFit() tTestResults = analyzedInstance.getTTestResults() print(f\"T-test result: statistic={tTestResults[0]}, p-value={tTestResults[1]}\") ``` Plot and save the results (will be saved under `plots` directory) ```shell from src.signal_visualizer import SignalVisualizer visualizer = SignalVisualizer(timeVector, generator.getNoisySignal(), filteredInstance.getFilteredSignal(), fittedInstance.getFittedSignal() ) visualizer.plotResults() visualizer.plotInteractiveResults() ```\n",
                "dependencies": "plotly numpy matplotlib scipy\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/simcats",
            "repo_link": "https://github.com/f-hader/SimCATS/",
            "content": {
                "codemeta": "",
                "readme": "<h1 align=\"center\"> <img src=\"https://raw.githubusercontent.com/f-hader/SimCATS/main/SimCATS_symbol.svg\" alt=\"SimCATS logo\"> <br> </h1> <div align=\"center\"> <a href=\"https://github.com/f-hader/SimCATS/blob/main/LICENSE\"> <img src=\"https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg\" alt=\"License: CC BY-NC-SA 4.0\"/> </a> <a href=\"https://pypi.org/project/simcats/\"> <img src=\"https://img.shields.io/pypi/v/simcats.svg\" alt=\"PyPi Latest Release\"/> </a> <a href=\"https://simcats.readthedocs.io/en/latest/\"> <img src=\"https://img.shields.io/readthedocs/simcats\" alt=\"Read the Docs\"/> </a> <a href=\"https://doi.org/10.1109/TQE.2024.3445967\"> <img src=\"https://img.shields.io/badge/DOI (Paper)-10.1109/TQE.2024.3445967-007ec6.svg\" alt=\"DOI Paper\"/> </a> <a href=\"https://doi.org/10.5281/zenodo.13805205\"> <img src=\"https://img.shields.io/badge/DOI (Code)-10.5281/zenodo.13805205-007ec6.svg\" alt=\"DOI Code\"/> </a> </div> # SimCATS Simulation of CSDs for Automated Tuning Solutions (`SimCATS`) is a Python framework for simulating charge stability diagrams (CSDs) typically measured during the tuning process of qubits. ## Installation The framework supports Python versions 3.7 - 3.11 and installs via pip: ``` pip install simcats ``` Alternatively, the `SimCATS` package can be installed by cloning the GitHub repository, navigating to the folder containing the `setup.py` file and executing ``` pip install . ``` For the installation in development/editable mode, use the option `-e`. ## Examples / Tutorials After installing the package, a good starting point is a look into the Jupyter Notebook `example_SimCATS_simulation_class.ipynb`, which provides an overview of the usage of the simulation class offered by the framework. For more detailed examples and explanations of the geometric ideal CSD simulation using Total Charge Transitions (TCTs), look at the Jupyter Notebook `example_SimCATS_IdealCSDGeometric.ipynb`. This notebook also includes a hint regarding the generation of required labels for training algorithms that might need line labels defined as start and end points or require semantic information about particular transitions. ## Tests The tests are written for the `PyTest` framework but should also work with the `unittest` framework. To run the tests, install the packages `pytest`, `pytest-cov`, and `pytest-xdist` with ``` pip install pytest pytest-cov pytest-xdist ``` and run the following command: ``` pytest --cov=simcats -n auto --dist loadfile .\\tests\\ ``` The argument - `--cov=simcats` enables a coverage summary of the `SimCATS` package, - `-n auto` enables the test to run with multiple threads (auto will choose as many threads as possible, but can be replaced with a specific number of threads to use), and - `--dist loadfile` specifies that each file should be executed only by one thread. <!-- start sec:documentation --> ## Documentation The official documentation is hosted on [ReadtheDocs](https://simcats.readthedocs.io), but can also be built locally. To do this, first install the packages `sphinx`, `sphinx-rtd-theme`, `sphinx-autoapi`, `myst-nb `, and `jupytext` with ``` pip install sphinx sphinx-rtd-theme sphinx-autoapi myst-nb jupytext ``` and then, in the `docs` folder, execute the following command: ``` .\\make html ``` To view the generated HTML documentation, open the file `docs\\build\\html\\index.html`. <!-- end sec:documentation --> ## Structure of SimCATS The primary user interface for `SimCATS` is the class `Simulation`, which combines all the necessary functionalities to measure (simulate) a CSD and adjust the parameters for the simulated measurement. The class `Simulation` and default configurations for the simulation (`default_configs`) can be imported directly from `simcats`. Aside from that, `SimCATS` contains the subpackages `ideal_csd`, `sensor`, `distortions`, and `support_functions`, described in the following sections. ### Module `simulation` An instance of the simulation class requires - an implementation of the `IdealCSDInterface` for the simulation of ideal CSD data, - an implementation of the `SensorInterface` for the simulation of the sensor (dot) reaction based on the ideal CSD data, and - (optionally) implementations of the desired types of distortions, which can be implementations from `OccupationDistortionInterface`, `SensorPotentialDistortionInterface`, or `SensorResponseDistortionInterface`. With an initialized instance of the `Simulation` class, it is possible to run simulations using the `measure` function (see `example_SimCATS_simulation_class.ipynb`). ### Subpackage `ideal_csd` This subpackage contains the `IdealCSDInterface` used by the `Simulation` class and an implementation of the `IdealCSDInterface` (`IdealCSDGeometric`) based on our geometric simulation approach. Additionally, it contains in the subpackage `geometric` the functions used by `IdealCSDGeometric`, including the implementation of the total charge transition (TCT) definition and functions for calculating the occupations using TCTs. ### Subpackage `distortions` The distortions subpackage contains the `DistortionInterface` from which the `OccupationDistortionInterface`, the `SensorPotentialDistortionInterface`, and the `SensorResponseDistortionInterface` are derived. Distortion functions used in the `Simulation` class have to implement these specific interfaces. Implemented distortions included in the subpackage are: - white noise, generated by sampling from a normal distribution, - pink noise, generated using the package colorednoise ([https://github.com/felixpatzelt/colorednoise](https://github.com/felixpatzelt/colorednoise)), - random telegraph noise (RTN), generated using the algorithm described in [\"Toward Robust Autotuning of Noisy Quantum Dot Devices\" by Ziegler et al.](https://doi.org/10.1103/PhysRevApplied.17.024069) (RTN is called sensor jumps there), - dot jumps, simulated using the algorithm described in [\"Toward Robust Autotuning of Noisy Quantum Dot Devices\" by Ziegler et al.](https://doi.org/10.1103/PhysRevApplied.17.024069) (In the `Simulation` class, this is applied to a whole block of rows or columns, but there is also a function for applying it linewise.), and - lead transition blurring, simulated using Gaussian or Fermi-Dirac blurring. The implementations also offer the option to set ratios (parameter `ratio`) for the occurrence of the distortion (e.g. dot jumps may only happen sometimes and not in every measurement). Moreover, it is also possible to sample the noise parameters from a given sampling range using an object of type `ParameterSamplingInterface`. Classes for randomly sampling from a normal distribution or a uniform distribution within a given range are available in the subpackage `support_functions`. In this case, the strength is randomly chosen from the given range for every measurement. Additionally, it is possible to specify that this range should be a smaller subrange of the provided range. This allows restricting distortion fluctuations during a simulation while enabling a large variety of different strengths for the initialization of the objects. <br> RTN, dot jumps, and lead transition blurring are applied in the pixel domain. However, the jump length or the blurring strength should be consistent in the voltage domain even if the resolution changes. Therefore, the parameters are given in the voltage domain and adjusted according to the resolution in terms of pixel per voltage. <br> For a simulated measurement with a continuous voltage sweep involving an averaging for each pixel, the noise strength of the white and pink noise should be adjusted if the resolution (volt per pixel) changes, due to smoothing out the noise. This smoothing depends on the type of averaging used and is not incorporated in the default implementation. ### Subpackage `sensor` This subpackage contains the `SensorInterface` that defines how a sensor simulation must be implemented to be used by the `Simulation` class. The `SensorPeakInterface` provides the desired representation for the definition of the Coulomb peaks the sensor uses. `SensorGeneric` implements the `SensorInterface` and offers functions for simulating the sensor response and potential. It offers the possibility to simulate with a single peak or multiple sensor peaks. Current implementations of the `SensorPeakInterface` are `SensorPeakGaussian` and `SensorPeakLorentzian`. ### Subpackage `support_functions` This subpackage contains support functions, which are used by the end user and by different functions of the framework. - `fermi_filter1d` is an implementation of a one-dimensional Fermi-Dirac filter. - `plot_csd` plots one and two-dimensional CSDs. The function can also plot ground truth data (see `example_SimCATS_simulation_class.ipynb` for examples). - `rotate_points` simply rotates coordinates (stored in a (n, 2) shaped array) by a given angle. It is especially used during the generation of the ideal data. - `ParameterSamplingInterface` defines an interface for randomly sampled (fluctuated) strengths of distortions. - `NormalSamplingRange` and `UniformSamplingRange` are implementations of the `ParameterSamplingInterface`. ## Citations ```bibtex @article{hader2024simcats, author={Hader, Fabian and Fleitmann, Sarah and Vogelbruch, Jan and Geck, Lotte and Waasen, Stefan van}, journal={IEEE Transactions on Quantum Engineering}, title={Simulation of Charge Stability Diagrams for Automated Tuning Solutions (SimCATS)}, year={2024}, volume={5}, pages={1-14}, doi={10.1109/TQE.2024.3445967} } ``` ## License, CLA, and Copyright [![CC BY-NC-SA 4.0][cc-by-nc-sa-shield]][cc-by-nc-sa] This work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License][cc-by-nc-sa]. [![CC BY-NC-SA 4.0][cc-by-nc-sa-image]][cc-by-nc-sa] [cc-by-nc-sa]: http://creativecommons.org/licenses/by-nc-sa/4.0/ [cc-by-nc-sa-image]: https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png [cc-by-nc-sa-shield]: https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg Contributions must follow the Contributor License Agreement. For more information, see the CONTRIBUTING.md file at the top of the GitHub repository. Copyright © 2024 Forschungszentrum Jülich GmbH - Central Institute of Engineering, Electronics and Analytics (ZEA) - Electronic Systems (ZEA-2)\n",
                "dependencies": "[build-system] requires = [\"setuptools>=61.0\"] build-backend = \"setuptools.build_meta\" [project] name = \"simcats\" version = \"1.2.0\" # also update in docs (conf.py) and simcats (__init__.py) license = { text=\"CC BY-NC-SA 4.0\" } authors = [ { name=\"Fabian Hader\", email=\"f.hader@fz-juelich.de\" }, { name=\"Sarah Fleitmann\", email=\"s.fleitmann@fz-juelich.de\" }, { name=\"Fabian Fuchs\", email=\"f.fuchs@fz-juelich.de\" }, { name=\"Jan Vogelbruch\", email=\"j.vogelbruch@fz-juelich.de\" }, ] description = \"\"\"\\ SimCATS is a python framework for simulating charge stability diagrams (CSDs) typically measured during the tuning process of qubits.\\ \"\"\" requires-python = \">=3.7\" readme = \"README.md\" classifiers = [ 'Development Status :: 5 - Production/Stable', 'Intended Audience :: Science/Research', 'Programming Language :: Python', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.7', 'Programming Language :: Python :: 3.8', 'Programming Language :: Python :: 3.9', 'Programming Language :: Python :: 3.10', 'Programming Language :: Python :: 3.11', 'Topic :: Scientific/Engineering', 'Typing :: Typed', ] dependencies = [ 'bezier', 'colorednoise', 'diplib', 'matplotlib', 'numpy', 'opencv-python', 'scipy', 'sympy' ] [project.urls] homepage = \"https://github.com/f-hader/SimCATS\" documentation = \"https://simcats.readthedocs.io\" source = \"https://github.com/f-hader/SimCATS\" tracker = \"https://github.com/f-hader/SimCATS/issues\"\n#!/usr/bin/env python import setuptools if __name__ == \"__main__\": setuptools.setup()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/simona",
            "repo_link": "https://github.com/jokergoo/simona",
            "content": {
                "codemeta": "",
                "readme": "# simona: Semantic Similarity on Bio-Ontologies ## Introduction This package implements infrastructures for ontology analysis by offering efficient data structures, fast ontology traversal methods, and elegant visualizations. It provides a robust toolbox supporting over 70 methods for semantic similarity analysis. Most methods implemented in **simona** are from the [supplementary file](https://academic.oup.com/bib/article/18/5/886/2562801#supplementary-data) of the paper [\"Mazandu et al., Gene Ontology semantic similarity tools: survey on features and challenges for biological knowledge discovery. Briefings in Bioinformatics 2017\"](https://doi.org/10.1093/bib/bbw067). ## Citation Zuguang Gu. simona: a comprehensive R package for semantic similarity analysis on bio-ontologies. bioRxiv, 2023. https://doi.org/10.1101/2023.12.03.569758 ## Install **simona** is available on [Bioconductor](https://bioconductor.org/packages/release/bioc/html/simona.html). It can be installed by: ```r if (!require(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"simona\") ``` Or the devel version: ```r devtools::install_github(\"jokergoo/simona\") ``` ## Usage Creat an ontology object: ```r library(simona) parents = c(\"a\", \"a\", \"b\", \"b\", \"c\", \"d\") children = c(\"b\", \"c\", \"c\", \"d\", \"e\", \"f\") dag = create_ontology_DAG(parents, children) dag ``` ``` An ontology_DAG object: Source: Ontology 6 terms / 6 relations Root: a Terms: a, b, c, d, ... Max depth: 3 Aspect ratio: 0.67:1 (based on the longest distance from root) 0.68:1 (based on the shortest distance from root) ``` From GO: ```r dag = create_ontology_DAG_from_GO_db(\"BP\", org_db = \"org.Hs.eg.db\") dag ``` ``` An ontology_DAG object: Source: GO BP / GO.db package 28140 terms / 56449 relations Root: GO:0008150 Terms: GO:0000001, GO:0000002, GO:0000003, GO:0000011, ... Max depth: 18 Aspect ratio: 342.43:1 (based on the longest distance from root) 780.22:1 (based on the shortest distance from root) Relations: is_a, part_of Annotations are available. With the following columns in the metadata data frame: id, name, definition ``` Import from an `.obo` file: ```r dag = import_obo(\"https://purl.obolibrary.org/obo/po.obo\") dag ``` ``` An ontology_DAG object: Source: po, releases/2023-07-13 1656 terms / 2512 relations Root: _all_ Terms: PO:0000001, PO:0000002, PO:0000003, PO:0000004, ... Max depth: 13 Aspect ratio: 25.08:1 (based on the longest distance from root) 39.6:1 (based on the shortest distance from root) Relations: is_a, part_of With the following columns in the metadata data frame: id, short_id, name, namespace, definition ``` The following IC methods are provided: ``` > all_term_IC_methods() [1] \"IC_offspring\" \"IC_height\" \"IC_annotation\" \"IC_universal\" [5] \"IC_Zhang_2006\" \"IC_Seco_2004\" \"IC_Zhou_2008\" \"IC_Seddiqui_2010\" [9] \"IC_Sanchez_2011\" \"IC_Meng_2012\" \"IC_Wang_2007\" ``` The following semantic similarity methods are provided: ``` > all_term_sim_methods() [1] \"Sim_Lin_1998\" \"Sim_Resnik_1999\" \"Sim_FaITH_2010\" [4] \"Sim_Relevance_2006\" \"Sim_SimIC_2010\" \"Sim_XGraSM_2013\" [7] \"Sim_EISI_2015\" \"Sim_AIC_2014\" \"Sim_Zhang_2006\" [10] \"Sim_universal\" \"Sim_Wang_2007\" \"Sim_GOGO_2018\" [13] \"Sim_Rada_1989\" \"Sim_Resnik_edge_2005\" \"Sim_Leocock_1998\" [16] \"Sim_WP_1994\" \"Sim_Slimani_2006\" \"Sim_Shenoy_2012\" [19] \"Sim_Pekar_2002\" \"Sim_Stojanovic_2001\" \"Sim_Wang_edge_2012\" [22] \"Sim_Zhong_2002\" \"Sim_AlMubaid_2006\" \"Sim_Li_2003\" [25] \"Sim_RSS_2013\" \"Sim_HRSS_2013\" \"Sim_Shen_2010\" [28] \"Sim_SSDD_2013\" \"Sim_Jiang_1997\" \"Sim_Kappa\" [31] \"Sim_Jaccard\" \"Sim_Dice\" \"Sim_Overlap\" [34] \"Sim_Ancestor\" ``` The following group similarity methods are provided: ``` > all_group_sim_methods() [1] \"GroupSim_pairwise_avg\" \"GroupSim_pairwise_max\" [3] \"GroupSim_pairwise_BMA\" \"GroupSim_pairwise_BMM\" [5] \"GroupSim_pairwise_ABM\" \"GroupSim_pairwise_HDF\" [7] \"GroupSim_pairwise_MHDF\" \"GroupSim_pairwise_VHDF\" [9] \"GroupSim_pairwise_Froehlich_2007\" \"GroupSim_pairwise_Joeng_2014\" [11] \"GroupSim_SimALN\" \"GroupSim_SimGIC\" [13] \"GroupSim_SimDIC\" \"GroupSim_SimUIC\" [15] \"GroupSim_SimUI\" \"GroupSim_SimDB\" [17] \"GroupSim_SimUB\" \"GroupSim_SimNTO\" [19] \"GroupSim_SimCOU\" \"GroupSim_SimCOT\" [21] \"GroupSim_SimLP\" \"GroupSim_Ye_2005\" [23] \"GroupSim_SimCHO\" \"GroupSim_SimALD\" [25] \"GroupSim_Jaccard\" \"GroupSim_Dice\" [27] \"GroupSim_Overlap\" \"GroupSim_Kappa\" ``` There is also a visualization on the complete DAG: ```r sig_go_ids = readRDS(system.file(\"extdata\", \"sig_go_ids.rds\", package = \"simona\")) dag_circular_viz(dag, highlight = sig_go_ids, reorder_level = 3, legend_labels_from = \"name\") ``` ![](https://github.com/jokergoo/simona/assets/449218/ada30534-182e-4513-93bf-9819e84b8604) ## License MIT @ Zuguang Gu\n",
                "dependencies": "Package: simona Type: Package Title: Semantic Similarity on Bio-Ontologies Version: 1.3.13 Date: 2024-09-17 Authors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"), comment = c('ORCID'=\"0000-0002-7395-8709\")) Depends: R (>= 4.1.0) Imports: methods, Rcpp, matrixStats, GetoptLong, grid, GlobalOptions, igraph, Polychrome, S4Vectors, xml2 (>= 1.3.3), circlize, ComplexHeatmap, grDevices, stats, utils, shiny Suggests: knitr, testthat, BiocManager, GO.db, org.Hs.eg.db, proxyC, AnnotationDbi, Matrix, DiagrammeR, ragg, png, InteractiveComplexHeatmap, UniProtKeywords, simplifyEnrichment, AnnotationHub, jsonlite LinkingTo: Rcpp VignetteBuilder: knitr Description: This package implements infrastructures for ontology analysis by offering efficient data structures, fast ontology traversal methods, and elegant visualizations. It provides a robust toolbox supporting over 70 methods for semantic similarity analysis. biocViews: Software, Annotation, GO, BiomedicalInformatics URL: https://github.com/jokergoo/simona BugReports: https://github.com/jokergoo/simona/issues SystemRequirements: Perl, Java License: MIT + file LICENSE RoxygenNote: 7.3.2 Encoding: UTF-8 Roxygen: list(markdown = TRUE)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/simpa",
            "repo_link": "https://github.com/IMSY-DKFZ/simpa",
            "content": {
                "codemeta": "",
                "readme": "<div align=\"center\"> ![Logo](https://github.com/IMSY-DKFZ/simpa/raw/main/docs/source/images/simpa_logo.png?raw=true \"SIMPA Logo\") [![Documentation Status](https://readthedocs.org/projects/simpa/badge/?version=develop)](https://simpa.readthedocs.io/en/develop/?badge=develop) ![Build Status](https://github.com/IMSY-DKFZ/simpa/actions/workflows/automatic_testing.yml/badge.svg) [![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/IMSY-DKFZ/simpa/blob/main/LICENSE.md) [![Pypi Badge](https://img.shields.io/pypi/v/simpa)](https://pypi.org/project/simpa/) [![PyPI downloads](https://img.shields.io/pypi/dw/simpa?color=gr&label=pypi%20downloads)](https://pypi.org/project/simpa/) </div> # The toolkit for Simulation and Image Processing for Photonics and Acoustics (SIMPA) SIMPA aims to facilitate realistic image simulation for optical and acoustic imaging modalities by providing adapters to crucial modelling steps, such as volume generation; optical modelling; acoustic modelling; and image reconstruction. SIMPA provides a communication layer between various modules that implement optical and acoustic forward and inverse models. Non-experts can use the toolkit to create sensible simulations from default parameters in an end-to-end fashion. Domain experts are provided with the functionality to set up a highly customisable pipeline according to their specific use cases and tool requirements. The paper that introduces SIMPA including visualisations and explanations can be found here: [https://doi.org/10.1117/1.JBO.27.8.083010](https://doi.org/10.1117/1.JBO.27.8.083010) * [Getting started](#getting-started) * [Simulation examples](#simulation-examples) * [Documentation](#documentation) * [Reproducibility](#reproducibility) * [Contributing](#how-to-contribute) * [Performance profiling](#performance-profiling) * [Troubleshooting](#troubleshooting) * [Citation](#citation) * [Funding](#funding) The toolkit is still under development and is thus not fully tested and may contain bugs. Please report any issues that you find in our Issue Tracker: https://github.com/IMSY-DKFZ/simpa/issues. Also make sure to double check all value ranges of the optical and acoustic tissue properties and to assess all simulation results for plausibility. # Getting started In order to use SIMPA in your project, SIMPA has to be installed as well as the external tools that make the actual simulations possible. Finally, to connect everything, SIMPA has to find all the binaries of the simulation modules you would like to use. The SIMPA path management takes care of that. * [SIMPA installation instructions](#simpa-installation-instructions) * [External tools installation instructions](#external-tools-installation-instructions) * [Path Management](#path-management) * [Testing](#run-manual-tests) ## SIMPA installation instructions The recommended way to install SIMPA is a manual installation from the GitHub repository, please follow steps 1 - 3: 1. `git clone https://github.com/IMSY-DKFZ/simpa.git` 2. `cd simpa` 3. `git checkout main` 4. `git pull` Now open a python instance in the 'simpa' folder that you have just downloaded. Make sure that you have your preferred virtual environment activated (we also recommend python 3.10) 1. `pip install .` or `pip install -e .` for an editable mode. 2. Test if the installation worked by using `python` followed by `import simpa` then `exit()` If no error messages arise, you are now setup to use SIMPA in your project. You can also install SIMPA with pip. Simply run: `pip install simpa` You also need to manually install the pytorch library to use all features of SIMPA. To this end, use the pytorch website tool to figure out which version to install: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) ## External tools installation instructions In order to get the full SIMPA functionality, you should install all third party toolkits that make the optical and acoustic simulations possible. ### mcx (Optical Forward Model) Download the latest nightly build of [mcx](http://mcx.space/) on [this page](http://mcx.space/nightly/github/) for your operating system: - Linux: `mcx-linux-x64-github-latest.zip` - MacOS: `mcx-macos-x64-github-latest.zip` - Windows: `mcx-windows-x64-github-latest.zip` Then extract the files and set `MCX_BINARY_PATH=/.../mcx/bin/mcx` in your path_config.env. ### k-Wave (Acoustic Forward Model) Please follow the following steps and use the k-Wave install instructions for further (and much better) guidance under: [http://www.k-wave.org/](http://www.k-wave.org/) 1. Install MATLAB with the core, image processing and parallel computing toolboxes activated at the minimum. 2. Download the kWave toolbox (version >= 1.4) 3. Add the kWave toolbox base path to the toolbox paths in MATLAB 4. If wanted: Download the CPP and CUDA binary files and place them in the k-Wave/binaries folder 5. Note down the system path to the `matlab` executable file. ## Path management As a pipelining tool that serves as a communication layer between different numerical forward models and processing tools, SIMPA needs to be configured with the paths to these tools on your local hard drive. You have a couple of options to define the required path variables. ### Option 1: Ensure that the environment variables defined in `simpa_examples/path_config.env.example` are accessible to your script during runtime. This can be done through any method you prefer, as long as the environment variables are accessible through `os.environ`. ### Option 2: Import the `PathManager` class to your project using `from simpa.utils import PathManager`. If a path to a `.env` file is not provided, the `PathManager` looks for a `path_config.env` file (just like the one we provided in the `simpa_examples/path_config.env.example`) in the following places, in this order: 1. The optional path you give the PathManager 2. Your $HOME$ directory 3. The current working directory 4. The SIMPA home directory path For this option, please follow the instructions in the `simpa_examples/path_config.env.example` file. ## Run manual tests To check the success of your installation ot to assess how your contributions affect the Simpa simulation outcomes, you can run the manual tests automatically. Install the testing requirements by doing `pip install .[testing]` and run the `simpa_tests/manual_tests/generate_overview.py` file. This script runs all manual tests and generates both a markdown and an HTML file that compare your results with the reference results. # Simulation examples To get started with actual simulations, SIMPA provides an [example package](simpa_examples) of simple simulation scripts to build your custom simulations upon. The [minimal optical simulation](simpa_examples/minimal_optical_simulation.py) is a nice start if you have MCX installed. Generally, the following pseudo code demonstrates the construction and run of a simulation pipeline: ```python import simpa as sp # Create general settings settings = sp.Settings(general_settings) # Create specific settings for each pipeline element # in the simulation pipeline settings.set_volume_creation_settings(volume_creation_settings) settings.set_optical_settings(optical_settings) settings.set_acoustic_settings(acoustic_settings) settings.set_reconstruction_settings(reconstruction_settings) # Set the simulation pipeline simulation_pipeline = [sp.VolumeCreationModule(settings), sp.OpticalModule(settings), sp.AcousticModule(settings), sp.ReconstructionModule(settings)] # Choose a PA device with device position in the volume device = sp.CustomDevice() # Simulate the pipeline sp.simulate(simulation_pipeline, settings, device) ``` # Reproducibility For reproducibility, we provide the exact version number including the commit hash in the simpa output file. This can be accessed via `simpa.__version__` or by checking the tag `Tags.SIMPA_VERSION` in the output file. This way, you can always trace back the exact version of the code that was used to generate the simulation results. # Documentation The updated version of the SIMPA documentation can be found at [https://simpa.readthedocs.io/en/develop](https://simpa.readthedocs.io/en/develop). ## Building the documentation It is also easily possible to build the SIMPA documentation from scratch. When the installation succeeded, and you want to make sure that you have the latest documentation you should do the following steps in a command line: 1. Make sure that you've installed the optional dependencies needed for the documentation by running `pip install .[docs]` 2. Navigate to the `simpa/docs` directory 2. If you would like the documentation to have the https://readthedocs.org/ style, type `pip install sphinx-rtd-theme` 3. Type `make html` 4. Open the `index.html` file in the `simpa/docs/build/html` directory with your favourite browser. # How to contribute Please find a more detailed description of how to contribute as well as code style references in our [contribution guidelines](CONTRIBUTING.md). To contribute to SIMPA, please fork the SIMPA github repository and create a pull request with a branch containing your suggested changes. The core developers will then review the suggested changes and integrate these into the code base. Please make sure that you have included unit tests for your code and that all previous tests still run through. Please also run the pre-commit hooks and make sure they are passing. Details are found in our [contribution guidelines](CONTRIBUTING.md). There is a regular SIMPA status meeting every Friday on even calendar weeks at 10:00 CET/CEST, and you are very welcome to participate and raise any issues or suggest new features. If you want to join this meeting, write one of the core developers. Please see the github guidelines for creating pull requests: [https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests) # Performance profiling When changing the SIMPA core, e.g., by refactoring/optimizing, or if you are curious about how fast your machine runs SIMPA, you can run the SIMPA [benchmarking scripts](simpa_examples/benchmarking/run_benchmarking.sh). Make sure to install the necessary dependencies via `pip install .[profile]` and then run: ```bash bash ./run_benchmark.sh ``` once for checking if it works and then parse [--number 100] to run it at eg 100 times for actual benchmarking. Please see [benchmarking.md](docs/source/benchmarking.md) for a complete explanation. # Understanding SIMPA **Tags** are identifiers in SIMPA used to categorize settings and components within simulations, making configurations modular, readable, and manageable. Tags offer organizational, flexible, and reusable benefits by acting as keys in configuration dictionaries. **Settings** in SIMPA control simulation behavior. They include: - **Global Settings**: Apply to the entire simulation, affecting overall properties and parameters. - **Component Settings**: Specific to individual components, allowing for detailed customization and optimization of each part of the simulation. Settings are defined in a hierarchical structure, where global settings are established first, followed by component-specific settings. This approach ensures comprehensive and precise control over the simulation process. For detailed information, users can refer to the [understanding SIMPA documentation](./docs/source/understanding_simpa.md). # Troubleshooting In this section, known problems are listed with their solutions (if available): ## 1. Error reading hdf5-files when using k-Wave binaries: If you encounter an error similar to: Error using h5readc The filename specified was either not found on the MATLAB path or it contains unsupported characters. Look up the solution in [this thread of the k-Wave forum](http://www.k-wave.org/forum/topic/error-reading-h5-files-when-using-binaries). ## 2. KeyError: 'time_series_data' This is the error which will occur for ANY k-Wave problem. For the actual root of the problem, please either look above in the terminal for the source of the bug or run the scripts in Matlab to find it manually. # Citation If you use the SIMPA tool, we would appreciate if you cite our Journal publication in the Journal of Biomedical Optics: Gröhl, Janek, Kris K. Dreher, Melanie Schellenberg, Tom Rix, Niklas Holzwarth, Patricia Vieten, Leonardo Ayala, Sarah E. Bohndiek, Alexander Seitel, and Lena Maier-Hein. *\"SIMPA: an open-source toolkit for simulation and image processing for photonics and acoustics.\"* **Journal of Biomedical Optics** 27, no. 8 (2022). ```Bibtex @article{2022simpatoolkit, title={SIMPA: an open-source toolkit for simulation and image processing for photonics and acoustics}, author={Gr{\\\"o}hl, Janek and Dreher, Kris K and Schellenberg, Melanie and Rix, Tom and Holzwarth, Niklas and Vieten, Patricia and Ayala, Leonardo and Bohndiek, Sarah E and Seitel, Alexander and Maier-Hein, Lena}, journal={Journal of Biomedical Optics}, volume={27}, number={8}, year={2022}, publisher={SPIE} } ``` # Funding This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. [101002198]). ![ERC](https://github.com/IMSY-DKFZ/simpa/raw/main/docs/source/images/LOGO_ERC-FLAG_EU_.jpg \"ERC\")\n",
                "dependencies": "[build-system] requires = [\"setuptools\", \"setuptools-scm\"] build-backend = \"setuptools.build_meta\" [project] name = \"simpa\" dynamic = [\"version\"] authors = [ {name = \"Division of Intelligent Medical Systems (IMSY), DKFZ\", email = \"k.dreher@dkfz-heidelberg.de\"}, {name = \"Janek Groehl <janekgroehl@live.de>\"} ] description = \"Simulation and Image Processing for Photonics and Acoustics\" license = {text = \"MIT\"} readme = \"README.md\" keywords = [\"simulation\", \"photonics\", \"acoustics\"] requires-python = \">=3.9\" dependencies = [ \"matplotlib>=3.5.0\", # Uses PSF-License (MIT compatible) \"numpy>=1.21.4\", # Uses BSD-License (MIT compatible) \"scipy>=1.13.0\", # Uses BSD-like-License (MIT compatible) \"pynrrd>=0.4.2\", # Uses MIT-License (MIT compatible) \"scikit-image>=0.18.3\", # Uses BSD-License (MIT compatible) \"xmltodict>=0.12.0\", # Uses MIT-License (MIT compatible) \"h5py>=3.6.0\", # Uses BSD-License (MIT compatible) \"pandas>=1.3.4\", # Uses BSD-License (MIT compatible) \"coverage>=6.1.2\", # Uses Apache 2.0-License (MIT compatible) \"Deprecated>=1.2.13\", # Uses MIT-License (MIT compatible) \"torch>=1.10.0\", # Uses BSD-License (MIT compatible) \"python-dotenv>=0.19.2\", # Uses BSD-License (MIT compatible) \"pacfish>=0.4.4\", # Uses BSD-License (MIT compatible) \"requests>=2.26.0\", # Uses Apache 2.0-License (MIT compatible) \"wget>=3.2\", # Is Public Domain (MIT compatible) \"jdata>=0.5.2\", # Uses Apache 2.0-License (MIT compatible) \"pre-commit>=3.2.2\", # Uses MIT-License (MIT compatible) \"PyWavelets\", # Uses MIT-License (MIT compatible) \"scikit-learn>=1.1.0\", # Uses BSD-License (MIT compatible) ] [project.optional-dependencies] docs = [ \"sphinx-rtd-theme>=2.0.0,<3.0.0\", # Uses MIT-License (MIT compatible) \"Sphinx>=5.1.1,<6.0.0\", # Uses BSD-License (MIT compatible) \"myst-parser>=0.18.0,<1.1\" # Uses MIT-License (MIT compatible) ] profile = [ \"pytorch_memlab>=0.3.0\", # Uses MIT-License (MIT compatible) \"line_profiler>=4.0.0\", # Uses BSD-License (MIT compatible) \"memory_profiler>=0.61.0\", # Uses BSD-License (MIT compatible) \"tabulate>=0.9.0\" # Uses MIT-License (MIT compatible) ] testing = [ \"mdutils>=1.4.0\", # Uses MIT-License (MIT compatible) \"pypandoc>=1.13\", # Uses MIT-License (MIT compatible) \"pypandoc_binary>=1.13\" # Uses MIT-License (MIT compatible) ] [project.urls] Homepage = \"https://github.com/IMSY-DKFZ/simpa\" Documentation = \"https://simpa.readthedocs.io/en/main/\" Repository = \"https://github.com/IMSY-DKFZ/simpa\" [tool.setuptools.packages.find] include = [\"simpa\", \"simpa_tests\", \"simpa_examples\"] [tool.setuptools_scm] [tool.autopep8] max_line_length = 120\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/simplifyenrichment",
            "repo_link": "https://github.com/jokergoo/simplifyEnrichment",
            "content": {
                "codemeta": "",
                "readme": "# Simplify Functional Enrichment Results [![R-CMD-check](https://github.com/jokergoo/simplifyEnrichment/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/simplifyEnrichment/actions) [![bioc](http://www.bioconductor.org/shields/downloads/devel/simplifyEnrichment.svg)](https://bioconductor.org/packages/stats/bioc/simplifyEnrichment/) [![bioc](http://www.bioconductor.org/shields/years-in-bioc/simplifyEnrichment.svg)](http://bioconductor.org/packages/devel/bioc/html/simplifyEnrichment.html) ### Features - A new method (binary cut) is proposed to efficiently cluster functional terms (_e.g._ GO terms) into groups from the semantic similarity matrix. - Summaries of functional terms in each cluster are visualized by word clouds. ### Citation Zuguang Gu, et al., simplifyEnrichment: an R/Bioconductor package for Clustering and Visualizing Functional Enrichment Results, _Genomics, Proteomics & Bioinformatics 2022_. [https://doi.org/10.1016/j.gpb.2022.04.008](https://doi.org/10.1016/j.gpb.2022.04.008). ### Install `simplifyEnrichment` is available on [Bioconductor](http://www.bioconductor.org/packages/devel/bioc/html/simplifyEnrichment.html), you can install it by: ```r if (!requireNamespace(\"BiocManager\", quietly=TRUE)) install.packages(\"BiocManager\") BiocManager::install(\"simplifyEnrichment\") ``` If you want to try the latest version, install it directly from GitHub: ```r library(devtools) install_github(\"jokergoo/simplifyEnrichment\") ``` ### Usage As an example, I first generate a list of random GO IDs. ```r library(simplifyEnrichment) set.seed(888) go_id = random_GO(500) head(go_id) # [1] \"GO:0003283\" \"GO:0060032\" \"GO:0031334\" \"GO:0097476\" \"GO:1901222\" # [6] \"GO:0018216\" ``` Then generate the GO similarity matrix, split GO terms into clusters and visualize it. ```r mat = GO_similarity(go_id) simplifyGO(mat) ``` ![](https://user-images.githubusercontent.com/449218/89673686-133c8600-d8e7-11ea-89fe-5221cb64d819.png) ### License MIT @ Zuguang Gu\n",
                "dependencies": "Package: simplifyEnrichment Type: Package Title: Simplify Functional Enrichment Results Version: 1.99.0 Date: 2024-09-13 Authors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"), comment = c('ORCID'=\"0000-0002-7395-8709\")) Depends: R (>= 4.0.0) Imports: simona, ComplexHeatmap (>= 2.7.4), grid, circlize, GetoptLong, digest, tm, GO.db, AnnotationDbi, slam, methods, clue, grDevices, stats, utils, cluster (>= 1.14.2), colorspace, GlobalOptions (>= 0.1.0) Suggests: knitr, ggplot2, cowplot, mclust, apcluster, MCL, dbscan, igraph, gridExtra, dynamicTreeCut, testthat, gridGraphics, flexclust, BiocManager, InteractiveComplexHeatmap (>= 0.99.11), shiny, shinydashboard, cola, hu6800.db, rmarkdown, genefilter, gridtext, fpc Description: A new clustering algorithm, \"binary cut\", for clustering similarity matrices of functional terms is implemeted in this package. It also provides functions for visualizing, summarizing and comparing the clusterings. biocViews: Software, Visualization, GO, Clustering, GeneSetEnrichment URL: https://github.com/jokergoo/simplifyEnrichment, https://simplifyEnrichment.github.io VignetteBuilder: knitr License: MIT + file LICENSE Encoding: UTF-8 Roxygen: list(markdown = TRUE) RoxygenNote: 7.3.1\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/smash",
            "repo_link": "https://github.com/smash-transport/smash",
            "content": {
                "codemeta": "",
                "readme": "# SMASH [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3484711.svg)](https://doi.org/10.5281/zenodo.3484711) SMASH (Simulating Many Accelerated Strongly-interacting Hadrons) is a relativistic hadronic transport approach for the dynamical description of heavy-ion reactions. Please see [Phys. Rev. C 94, 054905 (2016)](https://arxiv.org/abs/1606.06642) for details and, if you are using SMASH, cite this reference together with the [software DOI](https://doi.org/10.5281/zenodo.3484711) for the specific code version employed. A BibTeX entry for the software DOI is found on the respective Zenodo pages. See [CONTRIBUTING](CONTRIBUTING.md) for development hints. A complete [User Guide](https://theory.gsi.de/~smash/userguide/current/) as well as a more detailed [development documentation](http://theory.gsi.de/~smash/doc/current/) are available for the latest version of the code. For documentation of older versions, refer to links in the [releases pages](https://github.com/smash-transport/smash/releases). If Pythia is used, please cite the following references (both article and the codebase release you used): * [_A comprehensive guide to the physics and usage of PYTHIA 8.3_](https://scipost.org/SciPostPhysCodeb.8), C. Bierlich et al; SciPost Phys. Codebases 8 (2022), DOI: `10.21468/SciPostPhysCodeb.8`, also available on [arXiv](https://arxiv.org/abs/2203.11601); * [SciPost Phys. Codebases 8-r8.3](https://scipost.org/SciPostPhysCodeb.8-r8.3) (2022), DOI: `10.21468/SciPostPhysCodeb.8-r8.3`. Report issues [on GitHub](https://github.com/smash-transport/smash/issues) or contact us by [✉️ email](mailto:elfner@itp.uni-frankfurt.de). ## How to build and install SMASH In the following you can find a minimal quick start guide. Refer to the [INSTALL](INSTALL.md) file for more detailed information. ### Prerequisites SMASH is known to compile and work on 64-bit little endian machines (most CPUs are such) with UNIX-like operating systems (e.g. GNU/Linux, MacOS) and one of the following compilers (which have the required C++17 features): | Compiler | Required version | | :---: | :---: | | GCC | 8.0 or higher | | Clang | 7.0 or higher | | Apple clang| 11.0 or higher | Any different operating system and/or compiler and/or endianness is not officially supported and SMASH will ask you to continue at your own risk before compilation. SMASH requires the following tools and libraries: | Software | Required version | | :---: | :---: | | [CMake](https://cmake.org) | 3.16 or higher | | [GNU Scientific Library (GSL)](https://www.gnu.org/software/gsl/) | 2.0 or higher | | [Eigen3 library](http://eigen.tuxfamily.org) | 3.0 or higher | | [Pythia](https://pythia.org) | 8.312 | Support for ROOT, HepMC3 and Rivet output is automatically enabled if a suitable version is found on the system: | Software | Required version | | :---: | :---: | | ROOT | 5.34 or higher | | HepMC3 | 3.2.3 or higher | | Rivet | 3.1.4 or higher | ### Compilation and installation Clone the SMASH repository with the help of `git clone`. From within the cloned repository, use the following commands to build the codebase in a separate directory: ```console mkdir build cd build cmake -DPythia_CONFIG_EXECUTABLE=/path/to/pythia8312/bin/pythia8-config .. make ``` Please note that the `make` command builds everything (executables, tests and libraries) and might take a while. You can use `make smash` if you are interest in the SMASH executable only or use `make smash_shared` to exclusively build the libraries (needed e.g. in another project using SMASH as library). You can run SMASH with specific settings (e.g. at a given collision energy or impact parameter) by modifying the config.yaml file, for example with ```console vi config.yaml ./smash ``` Refer to the [section below](README.md#running-smash-with-example-input-files) for more information. If you want to install SMASH system-wide (into `/usr/local`) use ```console make install ``` ⚠️ **NOTE:** All commands above are the bare minimum needed for an installation. It is not guaranteed that this minimum setup is appropriate for your needs or your specific computing environment. For example, several different options can be passed e.g. to the `cmake` command. We strongly advise you to further refer to the [INSTALL](INSTALL.md) file for more guidance, especially if you encounter any issues. ## Using the Docker containers As an alternative to building or installing SMASH, a Docker image of the latest or recently tagged version can be pulled from the Github container registry. Get the newest version with ```console docker pull ghcr.io/smash-transport/smash:newest ``` Start the container with ```console docker run -it ghcr.io/smash-transport/smash:newest ``` A ready-to-use executable of SMASH is found in the `smash_bin` directory. Run it as explained below. If needed, SMASH can also be built inside the container as explained in the previous section (the SMASH source files and Pythia are also found in the `/SMASH` directory). Two container versions of SMASH are offered: * a small version (`ghcr.io/smash-transport/smash`) with a minimal set of dependencies pre-installed and * a large version with all possible external dependencies, e.g. ROOT, HepMC and Rivet, already included (`ghcr.io/smash-transport/smash-max`). Note that running SMASH inside of a Docker container might negatively affect performance. More information about containers usage can be found [here](containers/README.md). #### Note for users with ARM CPUs (e.g. Apple M1/M2 chips) Our Docker images are prepared for the x86-64 CPU architecture. To make them compatible with computers with ARM CPUs (like in the case of Apple M1 and M2 chips), `docker` must be launched with the `--platform=linux/amd64` option. For example: ```console docker run --platform=linux/amd64 -it ghcr.io/smash-transport/smash:newest ``` However, this is not always guaranteed to work and it might be necessary to build an image for the ARM architecture, as described [here](containers/README.md). ## Running SMASH with Example Input Files SMASH ships example configuration files for running in the collider, box, sphere, and list mode (`Modus` in the configuration jargon). By default, i.e. by running `./smash`, the simulation is set up from the collider configuration file, called _config.yaml_, and using the default particles and decay modes files (_particles.txt_ and _decaymodes.txt_, respectively). They are located in the repository ***input*** folder. Additionally, example configuration files for the box, sphere and list modus can be found in the respective directories ***input/{box,sphere,list}***. If needed, e.g. in the case of a box simulation, different default particles and decay modes files can be used. Examples for these are also provided in ***input/box***. Finally, for the list modus, an input list file to be read in is required and an example is provided as _input/list/example_list0_. In general, to run SMASH with a non-default configuration file, use the `-i` command. For example, for the sphere or list example file, from the ***build*** folder, use: ```console ./smash -i ../input/sphere/config.yaml ./smash -i ../input/list/config.yaml ``` Furthermore, if using non-default particles and decay modes files is necessary, these can be specified through the `-p` and `-d` options. In the box and the dileptons example, always from the ***build*** folder, this means: ```console ./smash -i ../input/box/config.yaml -p ../input/box/particles.txt -d ../input/box/decaymodes.txt ./smash -i ../input/dileptons/config.yaml -d ../input/dileptons/decaymodes.txt ``` All available command line options for SMASH can be viewed with ```console ./smash -h ``` To run SMASH completely silently for production runs, we recommend to suppress the standard output via e.g. ```console ./smash > /dev/null ``` and it might be useful to redirect warnings and error messages, that will still be displayed, to a file: ```console ./smash > /dev/null 2> /path/to/error-and-warnings-file ``` ## License SMASH is licensed under the terms of the GNU General Public License, Version 3 or above. The build scripts are licensed under terms of the BSD 3-clause license. For more information, see [LICENSE](LICENSE). ## Projects Using SMASH SMASH source and documentation are provided to check and reproduce published results of the authors. Cooperation and joint projects with outside researchers are encouraged and comparison to results by experimental collaborations is supported. If you are interested in starting a project, please contact us to avoid interference with current thesis topics. If your project involves changes to the code, please refer to [CONTRIBUTING](CONTRIBUTING.md) for coding guidelines and helpful tools. SMASH can also be used as a 3rd party library, for examples see the ***examples*** folder in the repository.\n",
                "dependencies": "######################################################## # # Copyright (c) 2012-2025 # SMASH Team # # BSD 3-clause license # ######################################################### # Minimum cmake version this is tested on cmake_minimum_required(VERSION 3.16) # The name, version and language of our project project(SMASH VERSION 3.2 LANGUAGES CXX) # Fail if cmake is called in the source directory if(CMAKE_SOURCE_DIR STREQUAL CMAKE_BINARY_DIR) message(FATAL_ERROR \"You don't want to configure in the source directory!\") endif() # Restrict supported operating systems if(NOT UNIX AND NOT APPLE OR CMAKE_SIZEOF_VOID_P LESS 8) option(FORCE_USE_ANY_SYSTEM \"Force cmake to setup and compile on any OS/architecture.\" OFF) if(NOT FORCE_USE_ANY_SYSTEM) if(CMAKE_SIZEOF_VOID_P LESS 8) set(AUX_MSG \"32-bit architecture\") endif() if(NOT UNIX AND NOT APPLE) list(APPEND AUX_MSG \"operating system\") list(JOIN AUX_MSG \"/\" AUX_MSG) endif() message(FATAL_ERROR \" \\n\" # See e.g. https://stackoverflow.com/a/51035045 for formatting \" Using SMASH on your ${AUX_MSG} is not officially supported.\\n\" \" Please, contact the SMASH development team (see README) if you would\\n\" \" like to collaborate with us to get the code run on your system.\\n\" \" At your own risk you can pass -DFORCE_USE_ANY_SYSTEM =TRUE to\\n\" \" cmake in order to setup and compile SMASH on your system.\\n\" \" Alternatively you can use the provided Docker containers (see README).\\n\" ) endif() endif() # Restrict supported compilers if(NOT CMAKE_CXX_COMPILER_ID MATCHES \"^((Apple)?Clang|GNU)$\") option(FORCE_USE_ANY_COMPILER \"Force cmake to setup and compile with any compiler.\" OFF) if(NOT FORCE_USE_ANY_COMPILER) message(FATAL_ERROR \" \\n\" \" Compiling SMASH with the specified compiler is not officially supported.\\n\" \" Please, contact the SMASH development team (see README) if you would\\n\" \" like to collaborate with us to get the codebase support your compiler.\\n\" \" At your own risk you can pass -DFORCE_USE_ANY_COMPILER=TRUE to cmake in\\n\" \" order to setup and try to compile SMASH with the chosen compiler.\\n\" \" Alternatively you can use the provided Docker containers (see README).\\n\" ) endif() endif() # Before starting, tell cmake where to find our modules and include utilities set(CMAKE_MODULE_PATH \"${CMAKE_CURRENT_SOURCE_DIR}/cmake\") include(Utilities) # The variable SMASH_VERSION is already set via the project() command, since we specify there a # VERSION option. Here we check if .git information is available and increase verbosity include(GetGitRevisionDescription) git_describe(SMASH_VERSION_VERBOSE) if(SMASH_VERSION_VERBOSE) set(SMASH_VERSION \"${SMASH_VERSION_VERBOSE}\") else() set(SMASH_VERSION \"${CMAKE_PROJECT_NAME}-${SMASH_VERSION}\") endif() # Define installation top-level directory name inside which SMASH files are installed in sub-folders string(TOLOWER \"${SMASH_VERSION}\" SMASH_INSTALLATION_SUBFOLDER) # SMASH will be shipped as shared library and hence we want to build position independent code include(CheckPIESupported) check_pie_supported(LANGUAGES CXX) if(NOT CMAKE_CXX_LINK_PIE_SUPPORTED) message(ATTENTION \"PIE is not supported at link time for C++. \" \"PIE link options will not be passed to linker.\") endif() set(CMAKE_POSITION_INDEPENDENT_CODE ON) # Fail early if a too old officially supported compiler is used if(CMAKE_CXX_COMPILER_ID MATCHES \"^((Apple)?Clang|GNU)$\") unset(MINIMUM_CXX_COMPILER_VERSION) if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\") set(MINIMUM_CXX_COMPILER_VERSION \"8\") elseif(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\") set(MINIMUM_CXX_COMPILER_VERSION \"7\") elseif(CMAKE_CXX_COMPILER_ID STREQUAL \"AppleClang\") set(MINIMUM_CXX_COMPILER_VERSION \"11\") endif() if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS ${MINIMUM_CXX_COMPILER_VERSION}) message(FATAL_ERROR \" At least version ${MINIMUM_CXX_COMPILER_VERSION}\" \" of ${CMAKE_CXX_COMPILER_ID} compiler is required.\\n\" \" Version in use is ${CMAKE_CXX_COMPILER_VERSION}\") endif() endif() # Request a given C++ standard set(CXX_SMASH_STANDARD \"17\") if(NOT \"${CMAKE_CXX_STANDARD}\") set(CMAKE_CXX_STANDARD ${CXX_SMASH_STANDARD}) endif() set(CMAKE_CXX_STANDARD_REQUIRED ON) set(CMAKE_CXX_EXTENSIONS OFF) # needed for clang-tidy set(CMAKE_EXPORT_COMPILE_COMMANDS \"ON\") # Set a default value for CMAKE_BUILD_TYPE, otherwise we get something which is none of the options. if(NOT DEFINED CMAKE_BUILD_TYPE OR CMAKE_BUILD_TYPE STREQUAL \"\") set(CMAKE_BUILD_TYPE Release CACHE STRING \"Choose the type of build, options are: Debug Release RelWithDebInfo MinSizeRel Profiling.\" FORCE) elseif(NOT CMAKE_BUILD_TYPE MATCHES \"^(Debug|Release|RelWithDebInfo|MinSizeRel|Profiling)$\") message(FATAL_ERROR \" \\n\" \" Invalid build configuration specified, CMAKE_BUILD_TYPE=${CMAKE_BUILD_TYPE}.\\n\" \" Valid values are: Debug, Release, RelWithDebInfo, MinSizeRel or Profiling.\\n\" ) elseif(CMAKE_BUILD_TYPE STREQUAL \"Debug\") message(ATTENTION \"Consider adding '-O0' compiler flag for best debug information.\") endif() # Retrieve architecture information about endianness include(TestBigEndian) test_big_endian(IS_BIG_ENDIAN) if(IS_BIG_ENDIAN) message(STATUS \"Big endian architecture detected.\") option(FORCE_USE_ON_BIG_ENDIAN \"Force cmake to setup and compile on big endian machine.\" OFF) if(NOT FORCE_USE_ON_BIG_ENDIAN) message(FATAL_ERROR \" \\n\" \" Using SMASH on big endian machines is not officially supported.\\n\" \" Please, contact the SMASH development team (see README) if you would like\\n\" \" to collaborate with us to get the code run on big endian architectures.\\n\" \" At your own risk you can pass -DFORCE_USE_ON_BIG_ENDIAN=TRUE to cmake in\\n\" \" order to setup and compile SMASH on a big endian machine.\\n\" \" Alternatively you can use the provided Docker containers (see README).\\n\" ) endif() add_definitions(\"-DBIG_ENDIAN_ARCHITECTURE\") else() message(STATUS \"Little endian architecture detected.\") add_definitions(\"-DLITTLE_ENDIAN_ARCHITECTURE\") endif() # Let Clang users on Linux be able to use libc++ if(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\") option(CLANG_USE_LIBC++ \"If turned on clang will explicitly be asked to use libc++ (otherwise it uses the system default)\" OFF) if(CLANG_USE_LIBC++) message(STATUS \"Prepare compilation in order to use LLVM libc++ implementation\") set(MESSAGE_QUIET ON) add_compiler_flag(-stdlib=libc++ CXX_FLAGS CMAKE_CXX_FLAGS CXX_RESULT _use_libcxx) unset(MESSAGE_QUIET) if(_use_libcxx AND \"${CMAKE_SYSTEM_NAME}\" STREQUAL \"Linux\") link_libraries(c++abi supc++) endif() endif() endif() # add 3rd-party libraries (before setting compiler flags etc) include_directories(SYSTEM \"${CMAKE_CURRENT_SOURCE_DIR}/3rdparty/Cuba-4.2.2\") include_directories(SYSTEM \"${CMAKE_CURRENT_SOURCE_DIR}/3rdparty/einhard\") include_directories(SYSTEM \"${CMAKE_CURRENT_SOURCE_DIR}/3rdparty/yaml-cpp-0.8.0/include\") add_subdirectory(3rdparty) # Set up compile options passed to all targets. From CMake documentation: \"The flags in # CMAKE_<LANG>_FLAGS will be passed to the compiler before those in the per-configuration # CMAKE_<LANG>_FLAGS_<CONFIG> variant, and before flags added by the add_compile_options() or # target_compile_options() commands.\" add_compiler_flags_if_supported(\"-fno-math-errno\" # Tell the compiler to ignore errno setting of # math functions. This can help the compiler # considerably in optimizing mathematical # expressions. \"-march=native\" # The '-march=native' flag is not supported e.g. on # Apple M1 machines CXX_FLAGS CMAKE_CXX_FLAGS) # CMake provides 4 build configurations (Debug, Release, RelWithDebInfo, MinSizeRel) together with # sensible values for the corresponding CMAKE_(C|CXX)_FLAGS_<CONFIG> variables (cached and marked as # advanced). Here we need to set those for Profiling build, only. Since the relevant variables are # created and cached by CMake, we need to force their value in the cache if their value is false, # e.g. empty. # # See https://cristianadam.eu/20190223/modifying-the-default-cmake-build-types/ for more infos. if(NOT CMAKE_CXX_FLAGS_PROFILING) add_compiler_flags_if_supported(\"-O3\" \"-DNDEBUG\" \"-pg\" CXX_FLAGS SUPPORTED_CXX_FLAGS_PROFILING) endif() if(NOT CMAKE_C_FLAGS_PROFILING) add_compiler_flags_if_supported(\"-O3\" \"-DNDEBUG\" \"-pg\" C_FLAGS SUPPORTED_C_FLAGS_PROFILING) endif() if(NOT CMAKE_CXX_FLAGS_PROFILING) set(CMAKE_CXX_FLAGS_PROFILING \"${SUPPORTED_CXX_FLAGS_PROFILING}\" CACHE STRING \"Flags used by the C++ compiler during profile builds.\" FORCE) endif() if(NOT CMAKE_C_FLAGS_PROFILING) set(CMAKE_C_FLAGS_PROFILING \"${SUPPORTED_C_FLAGS_PROFILING}\" CACHE STRING \"Flags used by the C compiler during profile builds.\" FORCE) endif() if(NOT CMAKE_EXE_LINKER_FLAGS_PROFILING) set(CMAKE_EXE_LINKER_FLAGS_PROFILING \"${CMAKE_EXE_LINKER_FLAGS_RELEASE} -pg\" CACHE STRING \"Flags used by the linker during profile builds.\" FORCE) endif() mark_as_advanced(CMAKE_CXX_FLAGS_PROFILING CMAKE_C_FLAGS_PROFILING CMAKE_EXE_LINKER_FLAGS_PROFILING) # have binary in the build directory set(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}) # enable standard CTest include(CTest) # subdirectories where the code is add_subdirectory(src) add_subdirectory(doc) # Don't make the install target depend on the all target (i.e. also all tests, if on) set(CMAKE_SKIP_INSTALL_ALL_DEPENDENCY TRUE) # Copy the default input files to the installation directory install(DIRECTORY input/ DESTINATION \"share/${SMASH_INSTALLATION_SUBFOLDER}/input_files\")\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/smg2s",
            "repo_link": "https://github.com/SMG2S/SMG2S",
            "content": {
                "codemeta": "",
                "readme": "# Sparse Matrix Generator with Given Spectrum [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2692117.svg)](https://doi.org/10.5281/zenodo.2692117) ------------------------------------------------------------------------------- * [Overview](#overview) * [What is SMG2S?](#what-is-smg2s?) * [Cite SMG2S](#cite-smg2s) * [Gallery: Sparsity Patterns](#some-sparsity-patterns) * [Contact and Contributation](#contact-and-contributation) * [Documentation](#documentation) * [Getting SMG2S](#getting-smg2s) * [Dependencies](#dependencies) * [Quick start](#quick-start) * [Installation](#installation) * [Use SMG2S with own project](#use-smg2s-with-own-project) * [header-only](#header-only) * [CMake](#cmake) * [Usage](#usage) * [Parallel vector and sparse matrix](#parallel-vector-and-sparse-matrix) * [parVectorMap class](#parvectormap-class) * [parVector class](#parvector-class) * [parMatrixSparse class](#parmatrixsparse-class) * [Building blocks of SMG2S](#building-blocks-of-smg2s) * [Assembling the building blocks](#assembling-the-building-blocks) * [Mini-app](#mini-app) * [Format of Given Spectrum Files](#format-of-given-spectrum-files) * [Real eigenvalues for non-Symmetric matrices](#real-eigenvalues-for-non-symmetric-matrices) * [Complex eigenvalues for non-Hermtian matrices](#complex-eigenvalues-for-non-hermtian-matrices) * [Conjugate eigenvalues for non-Symmetric matrices](#conjugate-eigenvalues-for-non-symmetric-matrices) * [Parallel I/O](#parallel-i/o) * [I/O for parallel vector](#i/o-for-parallel-vector) * [I/O for parallel sparse matrix](#i/o-for-parallel-sparse-matrix) * [Interface](#interface) * [Interface to C](#interface-to-c) * [Plotting and Validation](#plotting-and-validation) ------------------------------------------------------------------------------- ## Overview Author [Xinzhe Wu](https://brunowu.github.io) @ [Maison de la Simulation](http://www.maisondelasimulation.fr), France (2016-2019). @ [SDL Quantum Materials](https://www.fz-juelich.de/en/ias/jsc/about-us/structure/simulation-and-data-labs/sdl-quantum-materials), Forschungszentrum Juelich GmbH, Germany (2019-present). **** ### What is SMG2S? **SMG2S** is able to generate large-scale non-Hermitian and non-Symmetric matrices in parallel with the spectral distribution functions or eigenvalues given by users, and the spectrum of generated matrix is the same as the one specified by the users. SMG2S can be used to benchmark the iterative solvers for both linear systems and eigenvalue problems on supercomputers using the generated very large test matrices with customized spectral properties. As a matrix generator, SMG2S provides: - generating of both Non-Hermitian and Non-Symmetric sparse matrix - generated matrices are naturally sparse with non-trivial sparsity pattern - Given Spectrum: the spectrum of generated matrix is the same as the one specified by the users - Sparsity patterns are diverse and controllable As a software, SMG2S provides: * a collection of C++ header only files * C++ templated implementation for different data type * parallel implementation based on [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) which is able to efficiently generate very large sparse matrices in parallel on supercomputers * an easy-to-use C interface * a verification module based on Python for the sparsity pattern plotting and spectrum verification of small size of generated matrix. * Efficient parallel IO to store the generated matrix into [MatrixMarket format](https://math.nist.gov/MatrixMarket/formats.html) ### Cite SMG2S If you find SMG2S useful in your project, we kindly request that you cite the following paper: *Wu, Xinzhe, Serge G. Petiton, and Yutong Lu. \"A Parallel Generator of Non-Hermitian Matrices computed from Given Spectra.\" Concurrency and Computation: Practice and Experience, 32(20), e5710, 2020. [[DOI]](https://doi.org/10.1002/cpe.5710) [[PDF]](https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cpe.5710?casa_token=UUntHdbHvo4AAAAA:CHJa3O1_B-15_eHKY09LuWdh5TNs_trh_IXa_qDuNZLeTKcxa4CQt9WzrNsU1XSWxunknU8GeXP9Ihv9)* ### Some Sparsity Patterns Here are some sparsity patterns of matrices generated by SMG2S. ![Matrix Generation Pattern](docs/figure/pattern.png) ### Contact and Contribution Feel free to contact by email address: **xin DOT wu AT fz BAR juelich DOT de**. ## Documentation ### Getting SMG2S SMG2S is able to available on the Github. The most updated version of SMG2S can be gotten either by the following `git` command: ```bash git clone https://github.com/SMG2S/SMG2S.git ``` Moreover a released version can be downloaded [here](https://github.com/SMG2S/SMG2S/releases) ### Dependencies SMG2S is developed in `C++14` and `MPI`, and it is compiled with `CMake`. So the following software and compiler should be available before the installation of SMG2S. 1. a `C++` compiler with `C++14` support 2. `MPI`: message passing interface 3. `CMake`: version >= `3.8` ### Quick start SMG2S provides an executable `smg2s.exe` that the users can compile and start to play with SMG2S without installation as follows. ```bash cd SMG2S mkdir build & cd build cmake .. make -j ``` Then the executable `smg2s.exe`is available, and it can be run as follows: ```bash mpirun -np ${PROCS} ./smg2s.exe -D ${dim} -L ${diag_l} -U ${diag_u} -O ${offset} -C ${nbOne} -S ${sparsity} -M {no-herm or non-symm} ``` in which the command line parsers provides the customization of following parameters: ```bash usage: ./smg2s.exe [options] ... options: -D, --dim Dimension of matrix to be generated (int [=1000]) -L, --diagL offset of lower diagonal of initial matrix (int [=-10]) -U, --diagU offset of upper diagonal of initial matrix (int [=-5]) -O, --nilpOffset offset of diagonal of a nilpotent (int [=5]) -C, --continous Continuous length in Nilpotent matrix (int [=2]) -S, --sparsity sparsity of initial matrix (NOT THE FINAL GENERATED ONES) (double [=0.95]) -M, --mattype Matrix type to be generated: non-symmetric or non-Hermitian (string [=non-herm]) -?, --help print this message ``` ### Installation SMG2S relies on CMake for compiling and installation. A CMake flag `CMAKE_INSTALL_PREFIX` should be provided for the path of installation. ```bash cd SMG2S mkdir build & cd build cmake .. -DCMAKE_INSTALL_PREFIX=${PATH_TO_INSTALL} make -j install ``` ### Use SMG2S with own project #### header-only SMG2S is a collection of C++ header files. If users want to use SMG2S with C++, they can just copy SMG2S headers into their project. #### CMake SMG2S is installed as a CMake package, and it can be detected by the CMake `find_package` command. If the installation path is not in the default searching path of CMake, a CMake flag `CMAKE_PREFIX_PATH` should be provided which links to the installation path of SMG2S. So in your own project which want to use SMG2S: ```bash mkdir build & cd build cmake .. -DCMAKE_PREFIX_PATH=${INSTALLED_PATH_OF_SMG2S} make -j ``` and in the `CMakeLists.txt` of own project, it should provide some content as follows: ```cmake cmake_minimum_required(VERSION 3.8) project(YOUR-OWN-PROJECT) #find installation of SMG2S find_package( smg2s REQUIRED CONFIG) # for C++ code add_executable(smg2s-app test_parMatrix.cpp) target_link_libraries(smg2s-app PUBLIC SMG2S::smg2s) # for C-interface code add_executable(test_c.exe test_c.c) target_link_libraries(test_c.exe PRIVATE SMG2S::smg2s2c) ``` In case that the support of `C++14` is disabled by some compilers, please insert also the following lines into your `CMakeLists.txt` before the usage of SMG2S. ```cmake include(CheckCXXCompilerFlag) CHECK_CXX_COMPILER_FLAG(\"-std=c++14\" COMPILER_SUPPORTS_CXX14) if(COMPILER_SUPPORTS_CXX14) set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14\") else() message([FATAL_ERROR] \"The compiler ${CMAKE_CXX_COMPILER} has no C++14 support. Please use a different C++ compiler.\") endif() ``` ### Usage #### Parallel vector and sparse matrix In SMG2S, the parallelisation is supported through that both vectors and sparse matrices are naturally distributed across a 1D MPI grid of processes. SMG2S grants the freedom to the user to decide the way to distribute the vector and sparse matrices across a group of processes, e.g, for a parallel vector, user can decide the range of global indices dedicated to each process. ##### parVectorMap class `parVectorMap` is a class which determines the way to distribute a vector or sparse matrix across multiple MPI processes: - This class is to create a mapping from a fixed-size vector to multiple MPI procs in 1D grid. - This class can also be used to create more distributed vectors and sparse matrices following the same way. - For each MPI proc, a piece of vector with indexing `[lower_bound, upper_bound)` is allocated. - This class is templated such that different `integer` can be used to describes the dimension of vector and matrices. - This class provides a series of member funtions for querying, please refer to [parVectorMap full API](https://smg2s.github.io/SMG2S/classpar_vector_map.html) for more details. Here is an example: ```cpp MPI_Init(&argc, &argv); int world_size; int world_rank; int probSize = 7; MPI_Comm_size(MPI_COMM_WORLD, &world_size); MPI_Comm_rank(MPI_COMM_WORLD, &world_rank); int span, lower_b, upper_b; span = int(ceil(double(probSize)/double(world_size))); if(world_rank == world_size - 1){ lower_b = world_rank * span; upper_b = probSize - 1 + 1; }else{ lower_b = world_rank * span; upper_b = (world_rank + 1) * span - 1 + 1; } auto parVecMap = parVectorMap<int>(MPI_COMM_WORLD, lower_b, upper_b); ``` ##### parVector class `parVector` class construts a dense vector across a group of MPI processes. - It is able to be initialized by a `parVectorMap` object, and following the distribution scheme determined by this `parVectorMap` object. - It is also able to be directly initialized by the range of global indices of a vector owned by each MPI process, the same as the construction of a `parVectorMap` object. - This class provides a series of member funtions for querying and manuplating of a distributed vector, please refer to [parVector full API](https://smg2s.github.io/SMG2S/classpar_vector.html) for more details. - This class is also templated which allows to build a vector with different scalar types, either real or complex, either double precision or single precision.... ##### parMatrixSparse class `parMatrixSparse`class constructs a sparse matrix across a group of MPI processes. - The sparse matrix is distributed on a 1D MPI process grid by row. - a `parMatrixSparse` object can be constructed with a `parVectorMap` obejct, such that the distribution of rows of a sparse matrix follows the way determined by this `paraVectorMap` object - a `parMatrixSparse`object can be also constributed with a `parVector`object, what makes this sparse matrix share the same distribution scheme with this vector. - For any operations, which takes both sparse matrix and vector, they should also share the same distribution scheme - This class provides a series of member funtions for querying, manuplating and mathematical operations of a distributed sparse matrices, please refer to [parMatrixSparse full API](https://smg2s.github.io/SMG2S/classpar_matrix_sparse.html) for more details. #### Building blocks of SMG2S In order to generate a sparse matrix with user-provided spectrum, SMG2S requires the customization of three building blocks by the users: 1. user-provided spectrum 2. a nilpotent matrix 3. initial matrix Roughly, the workflow of SMG2S to generate a sparse matrix is that: 1. a strict lower-triangular matrix is generated, which is of the same size as the matrices to be generated. This matrix can be any shape, - for generating non-Symmetric matrices of real eigenvalues or non-Hermtian matrices, the only constraint is to be strict lower-triangluar. - for generating non-Synmmetric matrices of conjugated eigenvalues, this matrix can be any strict lower-triangluar and the diagonal next to the main diagonal to be empty. - In SMG2S, a simple struct `initMat` is provided which stores 4 parameters determining a initial matrix: `diag_l`, `diag_u`, `scale` and `sparisty`. It means that the between diagonal with offset `diag_l` and `diag_u` of lower-triangular part is filled with non-zeros elements randomly generated between `0` and `1`. The values of these elements can also be scaled with the parameter `scale`. The parameter `sparisty` determines that sparsity of inital matrix, not the final generated matrices. - Here is the details of [`initMat`](https://smg2s.github.io/SMG2S/structinit_mat.html) 2. the user-provided spectrum is stored in a `parVector` object, which shares the same distribution scheme as initial matrix. The spectrum can be generated: - inplace through the manuplating member functions of the `parVector` class - by loading from local text files following a specific formats, [click](#format-of-given-spectrum-files) for more details. - parallel I/Os are provided which loads the spectrums from local, for more details, please visit [I/O for loading spectrum](https://smg2s.github.io/SMG2S/group__group1.html). The spectrum vector is to set on the initial matrix in a way that: - for non-symmetric matrices with real eigenvalues or non-Hermtian matrices, the spectrum will be set directly on the main diagonal. - for non-symmetric matrices with conjugated eigenvalues, the real parts of eigenvalues are set on the main diagonal, and their imaginary parts are set either on the lower or upper diagonal next to the diagonal following a built-in mechanism in SMG2S. (That's why the lower diagonal next to the main diagonal of initial matrix is expected to be empty for generating non-Symmetric matrices with conjugated eigenvalues). 3. the initial matrix is either right or left multiplied by a nilpotent which manages to more its entries around and keeps the spectrum at the same time. For more details, please see the algorithm shown in our paper: [[PDF]](https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cpe.5710?casa_token=UUntHdbHvo4AAAAA:CHJa3O1_B-15_eHKY09LuWdh5TNs_trh_IXa_qDuNZLeTKcxa4CQt9WzrNsU1XSWxunknU8GeXP9Ihv9). - a class named `Nilpotent` is implemented in SMG2S, which determines the nilpotent matrix used in SMG2S. - this class provides multiple constructors of a nilpotent matrix, either with some simple parameters, or a user-provided vector. please visit [Nilpotent class](https://smg2s.github.io/SMG2S/group__group1.html) for more details. #### Assembling the building blocks SMG2S provides the generation of matrices in three different categories: 1. non-Hermtian matrices with complex eigenvalues 2. non-Symmetric matrices with real eigenvalues 3. non-Symmetric matrices with conjugated eigenvalues For each categories, SMG2S provides three functions which allows the users having different levels of freedom to control and customize the properties of generated matrices. - 1st level: users need to provide multiple simple parameters (for inital matrix and nilpotent) and a local text file containing the eigenvalues. The distribution of matrix over MPI processes is established by using the built-in scheme in SMG2S. - 2nd level: users need to provide multiple simple parameters (for inital matrix and nilpotent). The spectrum is generated by the user on the fly and stored in a `parVector` object. The generated matrix shares the same distribution scheme as the spectrum vector. - 3rd level: users need to provide multiple simple parameters for the nilpotent. The initial matrix is provided by the users with the manuplating operations provided by SMG2S. The spectrum is also generated by the user and stored in a `parVector` object which shares the same distribution with the initial matrix. For more details about the APIs, please visit [here](https://smg2s.github.io/SMG2S/group__group2.html). #### Mini-app Here is an mini-app of SMG2S which generates non-Hermitian and non-Symmetric matrices with different types of input spectrum. ```cpp #include <mpi.h> #include <smg2s-interface.hpp> int main(int argc, char** argv) { MPI_Init(&argc, &argv); int world_size; int world_rank; int probSize = 7; int l_diag = -7; int u_diag = -3; int nbOne = 2; int offset = 1; double sparsity = 0.5; /* construct a nilpotent object for generation */ Nilpotent<int> nilp = Nilpotent<int>(nbOne, offset, probSize); MPI_Comm_size(MPI_COMM_WORLD, &world_size); MPI_Comm_rank(MPI_COMM_WORLD, &world_rank); int span, lower_b, upper_b; span = int(floor(double(probSize)/double(world_size))); if(world_rank == world_size - 1){ lower_b = world_rank * span; upper_b = probSize - 1 + 1; }else{ lower_b = world_rank * span; upper_b = (world_rank + 1) * span - 1 + 1; } /* construct a parVecMap object which determines the distribution scheme of vectors and matrices*/ auto parVecMap = parVectorMap<int>(MPI_COMM_WORLD, lower_b, upper_b); /* example 1, generation of a non-Hermtian matrix */ // 1. generate the spectrum on the fly parVector<std::complex<double>, int> spec1 = parVector<std::complex<double>, int>(parVecMap); for(int i = lower_b; i < upper_b; i++){ std::complex<double> v(i+1, i+2); spec1.SetValueGlobal(i, v); } // 2. generation auto mat = nonherm<std::complex<double>, int>(probSize, nilp, initMat<int>(l_diag, u_diag, sparsity), spec1); /* example 2, generation of a non-Symmetric matrix with real eigenvalues */ // 1. generate the spectrum on the fly parVector<double, int> spec2 = parVector<double, int>(parVecMap); for(int i = lower_b; i < upper_b; i++){ spec2.SetValueGlobal(i, i+1); } // 2. generation auto mat2 = nonsymm<double , int>(probSize, nilp, initMat<int>(l_diag, u_diag, sparsity), spec2); /* example 3, generation of a non-Symmetric matrix with conjugated eigenvalues */ // 1. generate the spectrum on the fly parVector<std::complex<double>, int> spec3 = parVector<std::complex<double>, int>(parVecMap); for(int i = lower_b; i < upper_b; i++){ if(i % 2 == 0){ std::complex<double> v(i/2 + 1, i/2 + 2); spec3.SetValueGlobal(i, v); }else{ std::complex<double> v(i/2 + 1, -i/2 - 2); spec3.SetValueGlobal(i, v); } if(i == probSize - 1){ std::complex<double> v(i + 1, 0); spec3.SetValueGlobal(i, v); } } // 2. generation auto mat3 = nonsymmconj<double , int>(probSize, nilp, initMat<int>(l_diag, u_diag, sparsity), spec3); MPI_Finalize(); } ``` ### Format of Given Spectrum Files SMG2S is able to load user-provided spectrum in parallel from local text files. However, the provided files should conform into a specific format. 1. The first line is the comment part which includes the scalar types of given spectrum. This line should be: `%%SMG2S vector in complex scalar` and `%%SMG2S vector in real scalar` for the eigenvalues in complex or real scalar type, respectively. **Attention**, for this line, the keyword `complex` or `real` should always be there and conform with the type of user-provided spectrum. The parallel IO of SMG2S queries at first this line to check if the provided eigenvalues are complex or real. 2. The second line indicates the number of given eigenvaues in the files. For the ones with `3` complex values, it is `3 3 3`, and for the ones with `3` real eigenvalues, it should be `3 3`. 3. Starting from the `3rd` line, it is the main content of this file. It can have either `2` or `3` columns, which depends on the scalar types of eigenvalues. For the case with complex values, the first column indicates the coordinates for each eigenvalue, the second column contains the real part of eigenvalues, and the third column is for the imaginary part of eigenvalues. For the case with real values, the two columns contain the indexing and values of eigenvalues, respectively. **Attention**, the indexing is `1`-based, rather than `0`-based. #### Real eigenvalues for non-Symmetric matrices For the case with real eigenvalues for non-Symmetric matrices, the given spectrum file format should be as follows: ``` %%SMG2S vector in real scalar 3 3 1 10 2 3.4790 3 5.0540 ``` #### Complex eigenvalues for non-Hermtian matrices For the complex values for non-Hermitian matrices which are not supposed to be conjugated, the given spectrum is stored in three columns, the first column is the coordinates, the second column is the real part of complex values, and the third column is the imaginary part of complex values. Here is an example with `3` eigenvalues: %%SMG2S vector in complex scalar 3 3 3 1 10 6.5154 2 10.6288 3.4790 3 10.7621 5.0540 #### Conjugate eigenvalues for non-Symmetric matrices For the non-Symmetric matrices whose entries are all in real scalar, they can have conjugate eigenvalues which are in complex scalar. So in order to generate non-Symmetric test matrices with given conjugated eigenvalues, the give spectrum are always stored in complex form, with three columns. **Attention** For the non-Symmetric matrices, if one eigenvalue is complex, there is another value that they two are symmetric to the real axis in the real-imaginary plain, this is their conjugated eigenvalue. So when setting up the spectral file, one eigenvalue `a+bi` with `b != 0` should be closely followed by another eigenvalue `a-bi`. For the eigenvalues with their imaginary part to be `0`, they are stored with their imaginary part being 0. Here is an example %%SMG2S vector in complex scalar 9 9 9 1 10.6288 -3.4790 2 10.6288 3.4790 3 2.332 0 4 10.7621 5.0540 5 10.7621 -5.0540 6 -2.332 0 7 -11.02 0 8 21.21 4.4 9 21.21 -4.4 ### Parallel I/O #### I/O for parallel vector - SMG2S provides the input funtionalities which is able to load spectrum from local in parameters. For more details, please visit [I/O for loading spectrum](https://smg2s.github.io/SMG2S/group__group1.html). - SMG2S provides also the output functions as member functions of `parVector` class which saves a `parVector` object into local text files (the same format as [Format of Given Spectrum Files](#format-of-given-spectrum-files)): - [writeToTxt](https://smg2s.github.io/SMG2S/classpar_vector.html#ae52e90a3105c377140f432251e2a3a8b) saves a vector with real scalar types. - [writeToTxtCmplx](https://smg2s.github.io/SMG2S/classpar_vector.html#aab9f38beb4a793bf20c43431eb665d36) saves a vector with complex scalar types. #### I/O for parallel sparse matrix - SMG2S provides the writing functions which saves a `parMatrixSparse` in parallel into a local files with MatrixMarket format. - [writeToMatrixMarket](https://smg2s.github.io/SMG2S/classpar_matrix_sparse.html#ae0bb25445d859997c267d01028de2457) saves a sparse matrix with real scalar types. - [writeToMatrixMarketCmplx](https://smg2s.github.io/SMG2S/classpar_matrix_sparse.html#ae2106220d853b165ff53a4a643fc47d9) saves a sparse with complex scalar types. ### Interface #### Interface to C SMG2S provides a interface to `C` for the cases with double precision scalar and both `int` and `long` for integer. In summary: - For the scalars, only `double precision` are supported. - For the integer, both `int` and `long` are supported. - Namings for C-intefaces: - for `Nilpotent`, `initMat` and `parVectorMap`, the interfaces for the case with `long` integer has a letter `L` in some part of related function names. - for `parVector` and `parMatrixSparse` and other independant functions which don't belong to any classes and structs, their function names starts with one of the four prefix `ds_`, `dl_`, `zs_` and `zl_`. Here, `d` refers to `double`, `s` refers to `int`, `z` refers to `dcomplex_t` and `l` refers to `long`. - for the meaning of each function, please refer to the coresponding `C++` functions for more details. - Here's a full [list](https://smg2s.github.io/SMG2S/group__group5.html) of APIs for the `C` interface. A basic example of usage, which is equivalent to the previous `C++` [Mini-app](#mini-app): ```c #include <C/c-smg2s.h> #include <mpi.h> #include <math.h> int main(int argc, char* argv[]) { MPI_Init(&argc, &argv); int world_size; int world_rank; MPI_Comm_size(MPI_COMM_WORLD, &world_size); MPI_Comm_rank(MPI_COMM_WORLD, &world_rank); int probSize = 7; int span, lower_b, upper_b; int diagl = -5; int diagu = -3; int offset = 1; int nbOne = 2; double sparsity = 0.5; /* construct a nilpotent object for generation */ nilp_t *nilp = newNilp_2(nbOne, offset, probSize); span = (int)ceil((double)probSize/(double)world_size); if(world_rank == world_size - 1){ lower_b = world_rank * span; upper_b = probSize - 1 + 1; }else{ lower_b = world_rank * span; upper_b = (world_rank + 1) * span - 1 + 1; } /* construct a initMat object for SMG2S*/ initMatrix_t *initMat = newInitMatrix_3(diagl, diagu, sparsity); /* construct a parVecMap object which determines the distribution scheme of vectors and matrices*/ parVecMap_t *p = newParVecMap(MPI_COMM_WORLD, lower_b, upper_b); /* example 1, generation of a non-Hermtian matrix */ // 1. generate the spectrum on the fly zs_parVec_t *spec1 = new_zs_ParVec_2(p); for(int i = lower_b; i < upper_b; i++){ dcomplex_t v = {i+1, i+2}; zs_parVecSetVal(spec1, i, v); } // 2. generation zs_parMatSparse_t *mat1 = zs_nonherm_2(probSize, nilp, initMat, spec1); zs_parMatSparse_destory(mat1); /* example 2, generation of a non-Symmetric matrix with real eigenvalues */ // 1. generate the spectrum on the fly ds_parVec_t *spec2 = new_ds_ParVec_2(p); for(int i = lower_b; i < upper_b; i++){ ds_parVecSetVal(spec2, i, i + 1); } // 2. generation ds_parMatSparse_t *mat2 = ds_nonsymm_2(probSize, nilp, initMat, spec2); ds_parMatSparse_destory(mat2); /* example 3, generation of a non-Symmetric matrix with conjugated eigenvalues */ // 1. generate the spectrum on the fly zs_parVec_t *spec3 = new_zs_ParVec_2(p); for(int i = lower_b; i < upper_b; i++){ if(i % 2 == 0){ dcomplex_t v = {i/2 + 1, i/2 + 2}; zs_parVecSetVal(spec3, i, v); }else{ dcomplex_t v = {i/2 + 1, -i/2 - 2}; zs_parVecSetVal(spec3, i, v); } if(i == probSize - 1){ dcomplex_t v = {i + 1, 0}; zs_parVecSetVal(spec3, i, v); } } // 2. generation ds_parMatSparse_t *mat3 = ds_nonsymmconj_2(probSize, nilp, initMat, spec3); ds_parMatSparse_destory(mat3); initMatrix_destory(initMat); zs_parVec_destory(spec1); ds_parVec_destory(spec2); zs_parVec_destory(spec3); nilp_destory (nilp); MPI_Finalize(); } ``` ### Plotting and Validation SMG2S provides also a simple `python` script in [scripts/verification.py](scripts/verification.py), which provides a `spy` plotting function for the structure of a sparse matrix, and a verification function which compares the difference between the input spectrum and the spectrum of generated matrices. **Attention**, the spectrum of generated matrices are computed by the direct solver `eig` of `Python` package `numpy.linalg`, so this script is supposed to use for small size of matrices. ```bash usage: verification.py [-h] [--matpath MATPATH] [--specpath SPECPATH] [--verify] verification of matrices generated matrices to keep given spectra optional arguments: -h, --help show this help message and exit --matpath MATPATH path of matrix to be plotted or verified. Matrix should be in MatrixMarket format --specpath SPECPATH path of spectrum to be verified which is used to generate the related matrix. Vector should be in SMG2S vector file format --verify if only plotting patterns or also verifying the spectrum: default false ``` The [example 1](examples/ex1.cpp) provided by SMG2S can be a good starting point for the verification, since it provies multiple types of spectrums, and the spectrums and related matrices are all saved to local files through the parallel I/O of SMG2S. Below are examples of the outputs of `verification.py` with two different sparse matrices generated with a same given spectrum. For more examples, please visit [docs/figure](docs/figure). ![](docs/figure/verification_4.png) ![](docs/figure/verification_5.png)\n",
                "dependencies": "cmake_minimum_required(VERSION 3.8) #project setting project(SMG2S LANGUAGES C CXX VERSION 1.2.0) include(GNUInstallDirs) # MPI compiler find_package(MPI REQUIRED) include(CheckCXXCompilerFlag) CHECK_CXX_COMPILER_FLAG(\"-std=c++14\" COMPILER_SUPPORTS_CXX14) if(COMPILER_SUPPORTS_CXX14) set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14\") else() message([FATAL_ERROR] \"The compiler ${CMAKE_CXX_COMPILER} has no C++14 support. Please use a different C++ compiler.\") endif() add_library(smg2s INTERFACE) target_include_directories(smg2s INTERFACE \"$<BUILD_INTERFACE:${CMAKE_CURRENT_LIST_DIR}/include/>\" $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}> ) target_link_libraries(smg2s INTERFACE MPI::MPI_CXX) # Generate SMG2S executable add_executable(smg2s.exe smg2s.cpp) target_link_libraries(smg2s.exe PRIVATE smg2s ) install( TARGETS smg2s EXPORT smg2s-headers LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} INCLUDES DESTINATION ${CMAKE_INSTALL_INCLUDEDIR} ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} ) install(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/include/ DESTINATION ${CMAKE_INSTALL_INCLUDEDIR} FILES_MATCHING PATTERN \"*.hpp\" ) install(EXPORT smg2s-headers NAMESPACE SMG2S:: FILE smg2s-header.cmake EXPORT_LINK_INTERFACE_LIBRARIES DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME} ) INSTALL(TARGETS smg2s.exe DESTINATION ${CMAKE_INSTALL_BINDIR}) file(GLOB C_WRAPPERS \"src/C/*.cc\") add_library(smg2s2c ${C_WRAPPERS}) target_include_directories(smg2s2c INTERFACE \"$<BUILD_INTERFACE:${CMAKE_CURRENT_LIST_DIR}/include/>\" $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}> ) target_link_libraries(smg2s2c PUBLIC smg2s) install( TARGETS smg2s2c EXPORT smg2s-c LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR} INCLUDES DESTINATION ${CMAKE_INSTALL_INCLUDEDIR} ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR} ) install(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/include/ DESTINATION ${CMAKE_INSTALL_INCLUDEDIR} FILES_MATCHING PATTERN \"*.h\" ) install(EXPORT smg2s-c NAMESPACE SMG2S:: FILE smg2s-c.cmake EXPORT_LINK_INTERFACE_LIBRARIES DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME} ) include(CMakePackageConfigHelpers) configure_package_config_file( \"cmake/Config.cmake.in\" \"${CMAKE_CURRENT_BINARY_DIR}/smg2s-config.cmake\" INSTALL_DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME} ) install( FILES \"${CMAKE_CURRENT_BINARY_DIR}/smg2s-config.cmake\" DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME} ) ###tests add_subdirectory(tests) ###examples add_subdirectory(examples) # SMG2S test enable_testing() add_test(Test_Size_10000_w_proc1 ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 1 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 10000) add_test(Test_Size_20000_w_proc2 ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 2 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 20000) add_test(Test_Size_10000_s_proc1 ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 1 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 10000) add_test(Test_Size_10000_s_proc2 ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 2 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 10000) add_test(Test_Size_10000_w_proc1_nonsymm ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 1 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 10000 -M non-symm) add_test(Test_Size_20000_w_proc2_nonsymm ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 2 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 20000 -M non-symm) add_test(Test_Size_10000_s_proc1_nonsymm ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 1 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 10000 -M non-symm) add_test(Test_Size_10000_s_proc2_nonsymm ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG} 2 ${CMAKE_CURRENT_BINARY_DIR}/smg2s.exe -D 10000 -M non-symm)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/somesy",
            "repo_link": "https://github.com/Materials-Data-Science-and-Informatics/somesy",
            "content": {
                "codemeta": "{\"@context\":[\"https://doi.org/10.5063/schema/codemeta-2.0\",\"https://w3id.org/software-iodata\",\"https://raw.githubusercontent.com/jantman/repostatus.org/master/badges/latest/ontology.jsonld\",\"https://schema.org\",\"https://w3id.org/software-types\"],\"@type\":\"SoftwareSourceCode\",\"author\":[{\"@type\":\"Person\",\"givenName\":\"Mustafa\",\"familyName\":\"Soylu\",\"email\":\"m.soylu@fz-juelich.de\",\"@id\":\"https://orcid.org/0000-0003-2637-0432\",\"identifier\":\"https://orcid.org/0000-0003-2637-0432\"},{\"@type\":\"Person\",\"givenName\":\"Anton\",\"familyName\":\"Pirogov\",\"email\":\"a.pirogov@fz-juelich.de\",\"@id\":\"https://orcid.org/0000-0002-5077-7497\",\"identifier\":\"https://orcid.org/0000-0002-5077-7497\"}],\"name\":\"somesy\",\"description\":\"A CLI tool for synchronizing software project metadata.\",\"version\":\"0.7.3\",\"keywords\":[\"metadata\",\"FAIR\"],\"maintainer\":[{\"@type\":\"Person\",\"givenName\":\"Mustafa\",\"familyName\":\"Soylu\",\"email\":\"m.soylu@fz-juelich.de\",\"@id\":\"https://orcid.org/0000-0003-2637-0432\",\"identifier\":\"https://orcid.org/0000-0003-2637-0432\"}],\"license\":[\"https://spdx.org/licenses/MIT\"],\"softwareHelp\":\"https://materials-data-science-and-informatics.github.io/somesy\",\"codeRepository\":\"https://github.com/Materials-Data-Science-and-Informatics/somesy\",\"buildInstructions\":\"https://materials-data-science-and-informatics.github.io/somesy\",\"contributor\":[{\"@type\":\"Person\",\"givenName\":\"Jens\",\"familyName\":\"Br\\u00f6der\",\"email\":\"j.broeder@fz-juelich.de\",\"@id\":\"https://orcid.org/0000-0001-7939-226X\",\"identifier\":\"https://orcid.org/0000-0001-7939-226X\"},{\"@type\":\"Person\",\"givenName\":\"Volker\",\"familyName\":\"Hofmann\",\"email\":\"v.hofmann@fz-juelich.de\",\"@id\":\"https://orcid.org/0000-0002-5149-603X\",\"identifier\":\"https://orcid.org/0000-0002-5149-603X\"},{\"@type\":\"Person\",\"givenName\":\"Stefan\",\"familyName\":\"Sandfeld\",\"email\":\"s.sandfeld@fz-juelich.de\",\"@id\":\"https://orcid.org/0000-0001-9560-4728\",\"identifier\":\"https://orcid.org/0000-0001-9560-4728\"}],\"url\":\"https://materials-data-science-and-informatics.github.io/somesy\"}\n",
                "readme": "[ ![Docs](https://img.shields.io/badge/read-docs-success) ](https://materials-data-science-and-informatics.github.io/somesy) [ ![CI](https://img.shields.io/github/actions/workflow/status/Materials-Data-Science-and-Informatics/somesy/ci.yml?branch=main&label=ci) ](https://github.com/Materials-Data-Science-and-Informatics/somesy/actions/workflows/ci.yml) [ ![Test Coverage](https://materials-data-science-and-informatics.github.io/somesy/main/coverage_badge.svg) ](https://materials-data-science-and-informatics.github.io/somesy/main/coverage) [ ![Docs Coverage](https://materials-data-science-and-informatics.github.io/somesy/main/interrogate_badge.svg) ](https://materials-data-science-and-informatics.github.io/somesy) [ ![PyPIPkgVersion](https://img.shields.io/pypi/v/somesy) ](https://pypi.org/project/somesy/) [ ![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7701/badge) ](https://bestpractices.coreinfrastructure.org/projects/7701) [ ![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green) ](https://fair-software.eu) <!-- --8<-- [start:abstract] --> <div style=\"text-align: center;\"> <img alt=\"HMC Logo\" src=\"https://github.com/Materials-Data-Science-and-Informatics/Logos/raw/main/Somesy/Somesy_Logo_Text.png\" style=\"width: 50%; height: 50%;\" /> </div> # somesy Somesy (**so**ftware **me**tadata **sy**nc) is a CLI tool to avoid messy software project metadata by keeping it in sync. ## Description Many development tools either declare or need information about the software project they are used in, such as: the project name, description, version, repository url, license or project authors. Most such tools come with configuration files and conventions that are specific to the programming language or chosen technology. Emerging best practices for [FAIR](https://www.go-fair.org/fair-principles/) software metadata require to add even _more_ files where such metadata must be stated. If good project metadata was a fire-and-forget issue, this would be acceptable, but software is never standing still - maintainers change, contributors come and go, the version number is regularly increased, the project might be moved to a different location. Properly maintaining this kind of information in various files scattered around the project is usually _tedious, error-prone and time consuming manual labor_. **Somesy automates the synchronization of software project metadata and frees your time to focus on your _actual_ work**. <!-- --8<-- [end:abstract] --> **You can find more information on configuring, using and contributing to `somesy` in the [documentation](https://materials-data-science-and-informatics.github.io/somesy/main).** <!-- --8<-- [start:quickstart] --> ## Getting Started ### Platform Support Starting with version **0.3.0**, `somesy` supports Linux, MacOS and Windows. Make sure that you use the latest version in order to avoid any problems. ! info Poetry changed location of its project metadata with its version 2. Starting with version **0.7.0**, `somesy` supports both major versions of `poetry`, version 1 and 2. ### Installing somesy Somesy requires Python `>=3.8`. To get a first impression, you can install the latest stable version of somesy from PyPI using `pip`: ```bash pip install somesy ``` > **Note** > > If you use somesy as a pre-commit hook, you don't have to install somesy on your PC nor add it as a dependency in your Python project. Pre-commit will handle the installation automatically. ### Configuring somesy Yes, somesy is _another_ tool with its own configuration. However, for your project metadata it is hopefully the last file you need, and the only one you have to think about, `somesy` will take care of the others for you! To get started, create a file named `somesy.toml`: <!-- --8<-- [start:somesytoml] --> ```toml [project] name = \"my-amazing-project\" version = \"0.1.0\" description = \"Brief description of my amazing software.\" keywords = [\"some\", \"descriptive\", \"keywords\"] license = \"MIT\" repository = \"https://github.com/username/my-amazing-project\" # This is you, the proud author of your project: [[project.people]] given-names = \"Jane\" family-names = \"Doe\" email = \"j.doe@example.com\" orcid = \"https://orcid.org/0000-0000-0000-0001\" author = true # is a full author of the project (i.e. appears in citations) maintainer = true # currently maintains the project (i.e. is a contact person) # this person is an acknowledged contributor, but not author or maintainer: [[project.people]] given-names = \"Another\" family-names = \"Contributor\" email = \"a.contributor@example.com\" orcid = \"https://orcid.org/0000-0000-0000-0002\" # ... but for scientific publications, this contributor should be listed as author: publication_author = true # add an organization as a maintainer [[project.entities]] name = \"My Super Organization\" email = \"info@my-super-org.com\" website = \"https://my-super-org.com\" rorid = \"https://ror.org/02nv7yv05\" # highly recommended set a ror id for your organization [config] verbose = true # show detailed information about what somesy is doing ``` <!-- --8<-- [end:somesytoml] --> As Helmholtz Metadata Collaboration (HMC), our goal is to increase usage of metadata and improve metadata quality. Therefore, some fields in `somesy.toml` are set as required fields. This is to increase rigour and completeness of metadata recorded with `somesy` . Alternatively, you can also add the somesy configuration to an existing `pyproject.toml`, `package.json`, `Project.toml`, or `fpm.toml` file. The somesy [manual](https://materials-data-science-and-informatics.github.io/somesy/main/manual/#somesy-input-file) contains examples showing how to do that. ### Using somesy Once somesy is installed and configured, somesy can take over and manage your project metadata. Now you can run `somesy` simply by using ```bash somesy sync ``` The information in your `somesy.toml` is used as the **primary and authoritative** source for project metadata, which is used to update all supported (and enabled) _target files_. You can find an overview of supported formats further below. By default, `somesy` will create (if they did not exist) or update `CITATION.cff` and `codemeta.json` files in your repository. If you happen to use - `pyproject.toml` (in Python projects), - `package.json` (in JavaScript projects), - `Project.toml` (in Julia projects), - `fpm.toml` (in Fortran projects), - `pom.xml` (in Java projects), - `mkdocs.yml` (in projects using MkDocs), - `Cargo.toml` (in Rust projects) then somesy would also update the respective information there. You can see call available options with `somesy --help`, all of these can also be conveniently set in your `somesy.toml` file. ### Somesy as a pre-commit hook <!-- --8<-- [start:precommit] --> We highly recommend to use `somesy` as a [pre-commit hook](https://pre-commit.com/). A pre-commit hook runs on every commit to automatically point out issues or fix them on the spot, so if you do not use pre-commit in your project yet, you should start today! When used this way, `somesy` can fix most typical issues with your project metadata even before your changes can leave your computer. To add `somesy` as a pre-commit hook, add it to your `.pre-commit-config.yaml` file in the root folder of your repository: ```yaml repos: # ... (your other hooks) ... - repo: https://github.com/Materials-Data-Science-and-Informatics/somesy rev: 'v0.7.2' hooks: - id: somesy ``` > **Note** > > Please add the latest version of Somesy to your project. You can update the version of Somesy in your config file now and later to use the newest versions as they become available. Note that `pre-commit` gives `somesy` the [staged](https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F) version of files, so when using `somesy` with pre-commit, keep in mind that - if `somesy` changed some files, you need to `git add` them again (and rerun pre-commit) - if you explicitly run `pre-commit`, make sure to `git add` all changed files (just like before a commit) <!-- --8<-- [end:precommit] --> ## Supported File Formats Here is an overview of all the currently supported files and formats. | Input Formats | Status | | Target Formats | Status | | -------------- | ------ | --- | ---------------------------------------- | ------ | | (.)somesy.toml | ✓ | | - | ✓ | | pyproject.toml | ✓ | | pyproject.toml _(setuptools and poetry)_ | ✓(1.) | | package.json | ✓ | | package.json _(JavaScript)_ | ✓(2.) | | Project.toml | ✓ | | Project.toml _(Julia)_ | ✓ | | fpm.toml | ✓ | | fpm.toml _(Fortran)_ | ✓(3.) | | | ✓ | | pom.toml _(Java)_ | ✓(4.) | | Cargo.toml | ✓ | | Cargo.toml _(Rust)_ | ✓ | | | | | mkdocs.yml | ✓(5.) | | | | | CITATION.cff | ✓ | | | | | codemeta.json | ✓(6.) | **Notes:** 1. note that `somesy` does not support setuptools or poetry _dynamic fields_ 2. `package.json` only supports one author, so `somesy` will pick the _first_ listed author 3. `fpm.toml` only supports one author and maintainer, so `somesy` will pick the _first_ listed author and maintainer 4. `pom.xml` has no concept of `maintainers`, but it can have multiple licenses (somesy only supports one main project license) 5. `mkdocs.yml` is a bit special, as it is not a project file, but a documentation file. `somesy` will only update it if it exists and is enabled in the configuration 6. For handling `codemeta.json` different options exists: Either (A) `somesy` removes any prior existing `codemata.json` files and re-creates it anew, or (B) `somesy` merges an existing `codemeta.json` with the information handled by `somesy`. See the [user manual](https://materials-data-science-and-informatics.github.io/somesy/main/manual/#codemeta) for additional details about CodeMeta handling. <!-- --8<-- [end:quickstart] --> <!-- --8<-- [start:citation] --> ## How to Cite If you want to cite this project in your scientific work, please use the [citation file](https://citation-file-format.github.io/) in the [repository](https://github.com/Materials-Data-Science-and-Informatics/somesy/blob/main/CITATION.cff). <!-- --8<-- [end:citation] --> <!-- --8<-- [start:acknowledgements] --> ## Acknowledgements We kindly thank all [authors and contributors](https://materials-data-science-and-informatics.github.io/somesy/latest/credits). <div> <img style=\"vertical-align: middle;\" alt=\"HMC Logo\" src=\"https://github.com/Materials-Data-Science-and-Informatics/Logos/raw/main/HMC/HMC_Logo_M.png\" width=50% height=50% /> &nbsp;&nbsp; <img style=\"vertical-align: middle;\" alt=\"FZJ Logo\" src=\"https://github.com/Materials-Data-Science-and-Informatics/Logos/raw/main/FZJ/FZJ.png\" width=30% height=30% /> </div> <br /> This project was developed at the Institute for Materials Data Science and Informatics (IAS-9) of the Jülich Research Center and funded by the Helmholtz Metadata Collaboration (HMC), an incubator-platform of the Helmholtz Association within the framework of the Information and Data Science strategic initiative. <!-- --8<-- [end:acknowledgements] -->\n",
                "dependencies": "[project] name = \"somesy\" version = \"0.7.3\" description = \"A CLI tool for synchronizing software project metadata.\" readme = \"README.md\" keywords = [ \"metadata\", \"FAIR\", ] classifiers = [ \"Operating System :: POSIX :: Linux\", \"Operating System :: MacOS :: MacOS X\", \"Operating System :: Microsoft :: Windows\", \"Natural Language :: English\", \"Intended Audience :: Developers\", \"Intended Audience :: Science/Research\", \"Topic :: Software Development :: Libraries :: Application Frameworks\", \"License :: OSI Approved :: MIT License\", ] requires-python = \">=3.8\" dependencies = [ \"pydantic[email]>=2.8.2\", \"ruamel-yaml>=0.18.6\", \"tomlkit>=0.13.0\", \"importlib-metadata>=8.0.0\", \"typer[all]>=0.12.3\", \"cffconvert>=2.0.0\", \"wrapt>=1.16.0\", \"packaging>=24.1\", \"jinja2>=3.1.4\", \"defusedxml>=0.7.1\", \"pyld>=2.0.4\" ] authors = [ {name = \"Mustafa Soylu\", email = \"m.soylu@fz-juelich.de\"}, {name = \"Anton Pirogov\", email = \"a.pirogov@fz-juelich.de\"}, ] maintainers = [ {name = \"Mustafa Soylu\", email = \"m.soylu@fz-juelich.de\"}, ] [project.license] text = \"MIT\" [project.urls] homepage = \"https://materials-data-science-and-informatics.github.io/somesy\" repository = \"https://github.com/Materials-Data-Science-and-Informatics/somesy\" documentation = \"https://materials-data-science-and-informatics.github.io/somesy\" [tool.poetry] # the Python packages that will be included in a built distribution: packages = [{include = \"somesy\", from = \"src\"}] # always include basic info for humans and core metadata in the distribution, # include files related to test and documentation only in sdist: include = [ \"*.md\", \"LICENSE\", \"LICENSES\", \"REUSE.toml\", \"CITATION.cff\", \"codemeta.json\", \"mkdocs.yml\", \"docs\", \"tests\", { path = \"mkdocs.yml\", format = \"sdist\" }, { path = \"docs\", format = \"sdist\" }, { path = \"tests\", format = \"sdist\" }, ] [tool.poetry.dependencies] python = \">=3.8,<4.0\" [tool.poetry.group.dev.dependencies] poethepoet = \"^0.27.0\" pre-commit = \"^3.5.0\" pytest = \"^8.3.1\" pytest-cov = \"^5.0.0\" hypothesis = \"^6.108.4\" licensecheck = \"^2024.2\" pytest-mock = \"^3.14.0\" [tool.poetry.group.docs] optional = true [tool.poetry.group.docs.dependencies] mkdocs = \"^1.6.0\" mkdocstrings = {extras = [\"python\"], version = \"^0.25.1\"} mkdocs-material = \"^9.5.30\" mkdocs-gen-files = \"^0.5.0\" mkdocs-literate-nav = \"^0.6.1\" mkdocs-section-index = \"^0.3.9\" mkdocs-macros-plugin = \"^1.0.5\" markdown-include = \"^0.8.1\" pymdown-extensions = \"^10.8.1\" markdown-exec = {extras = [\"ansi\"], version = \"^1.9.3\"} mkdocs-coverage = \"^1.1.0\" mike = \"^2.1.2\" anybadge = \"^1.14.0\" interrogate = \"^1.7.0\" black = \"^24.4.2\" mkdocs-exclude = \"^1.0.2\" [project.scripts] somesy = \"somesy.main:app\" [build-system] requires = [\"poetry-core\"] build-backend = \"poetry.core.masonry.api\" # NOTE: You can run the following with \"poetry poe TASK\" [tool.poe.tasks] init-dev = { shell = \"pre-commit install\" } lint = \"pre-commit run\" # pass --all-files to check everything test = \"pytest\" # pass --cov to also collect coverage info docs = \"mkdocs build\" # run this to generate local documentation licensecheck = \"licensecheck\" # run this when you add new deps # Tool Configurations # ------------------- [tool.pytest.ini_options] pythonpath = [\"src\"] addopts = \"--cov-report=term-missing:skip-covered\" filterwarnings = [ \"ignore::DeprecationWarning:pkg_resources.*\", \"ignore::DeprecationWarning:pyshacl.*\", # Example: # \"ignore::DeprecationWarning:importlib_metadata.*\", ] [tool.coverage.run] source = [\"somesy\"] [tool.coverage.report] exclude_lines = [ \"pragma: no cover\", \"def __repr__\", \"if self.debug:\", \"if settings.DEBUG\", \"raise AssertionError\", \"raise NotImplementedError\", \"if 0:\", \"if TYPE_CHECKING:\", \"if __name__ == .__main__.:\", \"class .*\\\\bProtocol\\\\):\", \"@(abc\\\\.)?abstractmethod\", ] [tool.semantic_release] version_variable = \"src/somesy/__init__.py:__version__\" [tool.ruff.lint] extend-select = [\"I\", \"D\", \"B\", \"S\", \"W\"] ignore = [\"D203\", \"D213\", \"D407\", \"B008\"] [tool.ruff.lint.per-file-ignores] \"**/{tests,docs}/*\" = [\"ALL\"] [tool.licensecheck] using = \"poetry\" [tool.mypy] disable_error_code = [\"attr-defined\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/spatialdata-framework",
            "repo_link": "https://github.com/scverse/spatialdata/",
            "content": {
                "codemeta": "",
                "readme": "![SpatialData banner](https://github.com/scverse/spatialdata/blob/main/docs/_static/img/spatialdata_horizontal.png?raw=true) # SpatialData: an open and universal framework for processing spatial omics data. [![Tests][badge-tests]][link-tests] [![pre-commit.ci status](https://results.pre-commit.ci/badge/github/scverse/spatialdata/main.svg)](https://results.pre-commit.ci/latest/github/scverse/spatialdata/main) [![codecov](https://codecov.io/gh/scverse/spatialdata/branch/main/graph/badge.svg?token=X19DRSIMCU)](https://codecov.io/gh/scverse/spatialdata) [![documentation badge](https://readthedocs.org/projects/scverse-spatialdata/badge/?version=latest)](https://spatialdata.scverse.org/en/latest/) [![DOI](https://zenodo.org/badge/487366481.svg)](https://zenodo.org/badge/latestdoi/487366481) [![Downloads](https://static.pepy.tech/badge/spatialdata)](https://pepy.tech/project/spatialdata) [![Release](https://github.com/scverse/spatialdata/actions/workflows/release.yaml/badge.svg?event=release)](https://github.com/scverse/spatialdata/actions/workflows/release.yaml) [![Documentation][badge-pypi]][link-pypi] [![Anaconda-Server Badge](https://anaconda.org/conda-forge/spatialdata/badges/version.svg)](https://anaconda.org/conda-forge/spatialdata) [badge-pypi]: https://badge.fury.io/py/spatialdata.svg [link-pypi]: https://pypi.org/project/spatialdata/ SpatialData is a data framework that comprises a FAIR storage format and a collection of python libraries for performant access, alignment, and processing of uni- and multi-modal spatial omics datasets. This repository contains the core spatialdata library. See the links below to learn more about other packages in the SpatialData ecosystem. - [spatialdata-io](https://github.com/scverse/spatialdata-io): load data from common spatial omics technologies into spatialdata. - [spatialdata-plot](https://github.com/scverse/spatialdata-plot): Static plotting library for spatialdata. - [napari-spatialdata](https://github.com/scverse/napari-spatialdata): napari plugin for interactive exploration and annotation of spatial data. [//]: # \"numfocus-fiscal-sponsor-attribution\" spatialdata is part of the scverse® project ([website](https://scverse.org), [governance](https://scverse.org/about/roles)) and is fiscally sponsored by [NumFOCUS](https://numfocus.org/). If you like scverse® and want to support our mission, please consider making a tax-deductible [donation](https://numfocus.org/donate-to-scverse) to help the project pay for developer time, professional services, travel, workshops, and a variety of other needs. The spatialdata project also received support by the Chan Zuckerberg Initiative. <div align=\"center\"> <a href=\"https://numfocus.org/project/scverse\"> <img height=\"60px\" src=\"https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png\" align=\"center\"> </a> </div> <br> ![SpatialDataOverview](https://github.com/scverse/spatialdata/assets/1120672/cb91071f-12a7-4b8e-9430-2b3a0f65e52f) - **The library is currently under review.** We expect there to be changes as the community provides feedback. We have an announcement channel for communicating these changes, please see the contact section below. - The SpatialData storage format is built on top of the [OME-NGFF](https://ngff.openmicroscopy.org/latest/) specification. ## Getting started Please refer to the [documentation][link-docs]. In particular: - [API documentation][link-api]. - [Design doc][link-design-doc] (includes the roadmap). - [Example notebooks][link-notebooks]. Another useful resource to get started is the source code of the [`spatialdata-io`](https://github.com/scverse/spatialdata-io) package, which shows example of how to read data from common technologies. ## Installation Check out the docs for more complete [installation instructions](https://spatialdata.scverse.org/en/stable/installation.html). To get started with the \"batteries included\" installation, you can install via pip: ```bash pip install \"spatialdata[extra]\" ``` ~~or via conda:~~ Update Feb 2025: `spatialdata` cannot be currently be installed via `conda` because some dependencies of our dependencies are not updated in `conda-forge` and we are still waiting for an update. Please install from `pip`; the latest versions of the `spatialdata` libraries are always available via `PyPI`. ```bash mamba install -c conda-forge spatialdata napari-spatialdata spatialdata-io spatialdata-plot ``` ## Limitations - Code only manually tested for Windows machines. Currently the framework is being developed using Linux, macOS and Windows machines, but it is automatically tested only for Linux and macOS machines. ## Contact To get involved in the discussion, or if you need help to get started, you are welcome to use the following options. - <ins>Chat</ins> via [`scverse` Zulip](https://scverse.zulipchat.com/#narrow/stream/315824-spatial) (public or 1 to 1). - <ins>Forum post</ins> in the [scverse discourse forum](https://discourse.scverse.org/). - <ins>Bug report/feature request</ins> via the [GitHub issue tracker][issue-tracker]. - <ins>Zoom call</ins> as part of the SpatialData Community Meetings, held every 2 weeks on Thursday, [schedule here](https://hackmd.io/enWU826vRai-JYaL7TZaSw). Finally, especially relevant for for developers that are building a library upon `spatialdata`, please follow this channel for: - <ins>Announcements</ins> on new features and important changes [Zulip](https://imagesc.zulipchat.com/#narrow/stream/329057-scverse/topic/spatialdata.20announcements). ## Citation Marconato, L., Palla, G., Yamauchi, K.A. et al. SpatialData: an open and universal data framework for spatial omics. Nat Methods (2024). https://doi.org/10.1038/s41592-024-02212-x <!-- Links --> [scverse-discourse]: https://discourse.scverse.org/ [issue-tracker]: https://github.com/scverse/spatialdata/issues [design doc]: https://scverse-spatialdata.readthedocs.io/en/stable/design_doc.html [link-docs]: https://spatialdata.scverse.org/en/stable/ [link-api]: https://spatialdata.scverse.org/en/stable/api.html [link-design-doc]: https://spatialdata.scverse.org/en/stable/design_doc.html [link-notebooks]: https://spatialdata.scverse.org/en/stable/tutorials/notebooks/notebooks.html [badge-tests]: https://github.com/scverse/spatialdata/actions/workflows/test.yaml/badge.svg [link-tests]: https://github.com/scverse/spatialdata/actions/workflows/test.yaml\n",
                "dependencies": "[build-system] build-backend = \"hatchling.build\" requires = [\"hatchling\", \"hatch-vcs\"] [project] name = \"spatialdata\" description = \"Spatial data format.\" authors = [ {name = \"scverse\"}, ] maintainers = [ {name = \"scverse\", email = \"giov.pll@gmail.com\"}, ] urls.Documentation = \"https://spatialdata.scverse.org/en/latest\" urls.Source = \"https://github.com/scverse/spatialdata.git\" urls.Home-page = \"https://github.com/scverse/spatialdata.git\" requires-python = \">=3.10, <3.13\" # include 3.13 once multiscale-spatial-image conflicts are resolved dynamic= [ \"version\" # allow version to be set by git tags ] license = {file = \"LICENSE\"} readme = \"README.md\" dependencies = [ \"anndata>=0.9.1\", \"click\", \"dask-image\", \"dask>=2024.4.1,<=2024.11.2\", \"datashader\", \"fsspec\", \"geopandas>=0.14\", \"multiscale_spatial_image>=2.0.2\", \"networkx\", \"numba>=0.55.0\", \"numpy\", \"ome_zarr>=0.8.4\", \"pandas\", \"pooch\", \"pyarrow\", \"rich\", \"setuptools\", \"shapely>=2.0.1\", \"spatial_image>=1.1.0\", \"scikit-image\", \"scipy\", \"typing_extensions>=4.8.0\", \"xarray>=2024.10.0\", \"xarray-schema\", \"xarray-spatial>=0.3.5\", \t\"xarray-dataclasses>=1.8.0\", \"zarr<3\", ] [project.optional-dependencies] dev = [ \"bump2version\", ] test = [ \"pytest\", \"pytest-cov\", \"pytest-mock\", \"torch\", ] docs = [ \"sphinx>=4.5\", \t\"sphinx-autobuild\", \"sphinx-book-theme>=1.0.0\", \"myst-nb\", \"sphinxcontrib-bibtex>=1.0.0\", \"sphinx-autodoc-typehints\", \"sphinx-design\", # For notebooks \"ipython>=8.6.0\", \"sphinx-copybutton\", \"sphinx-pytest\", ] benchmark = [ \"asv\", ] torch = [ \"torch\" ] extra = [ \"napari-spatialdata[all]\", \"spatialdata-plot\", \"spatialdata-io\", ] [tool.coverage.run] source = [\"spatialdata\"] omit = [ \"**/test_*.py\", ] [tool.pytest.ini_options] testpaths = [\"tests\"] xfail_strict = true addopts = [ # \"-Werror\", # if 3rd party libs raise DeprecationWarnings, just use filterwarnings below \"--import-mode=importlib\", # allow using test files with same name \"-s\" # print output from tests ] # info on how to use this https://stackoverflow.com/questions/57925071/how-do-i-avoid-getting-deprecationwarning-from-inside-dependencies-with-pytest filterwarnings = [ # \"ignore:.*U.*mode is deprecated:DeprecationWarning\", ] [tool.jupytext] formats = \"ipynb,md\" [tool.hatch.build.targets.wheel] packages = ['src/spatialdata'] [tool.hatch.version] source = \"vcs\" [tool.hatch.build.hooks.vcs] version-file = \"_version.py\" [tool.hatch.metadata] allow-direct-references = true [tool.ruff] exclude = [ \".git\", \".tox\", \"__pycache__\", \"build\", \"docs/_build\", \"dist\", \"setup.py\", ] line-length = 120 target-version = \"py310\" [tool.ruff.lint] ignore = [ # Do not assign a lambda expression, use a def -> lambda expression assignments are convenient \"E731\", # allow I, O, l as variable names -> I is the identity matrix, i, j, k, l is reasonable indexing notation \"E741\", # Missing docstring in public package \"D104\", # Missing docstring in public module \"D100\", # Missing docstring in __init__ \"D107\", # Missing docstring in magic method \"D105\", # Do not perform function calls in argument defaults. \"B008\", # Missing docstring in magic method \"D105\", ] select = [ \"D\", # flake8-docstrings \"I\", # isort \"E\", # pycodestyle \"F\", # pyflakes \"W\", # pycodestyle \"Q\", # flake8-quotes \"SIM\", # flake8-simplify \"TID\", # flake-8-tidy-imports \"NPY\", # NumPy-specific rules \"PT\", # flake8-pytest-style \"B\", # flake8-bugbear \"UP\", # pyupgrade \"C4\", # flake8-comprehensions \"BLE\", # flake8-blind-except \"T20\", # flake8-print \"RET\", # flake8-raise \"PGH\", # pygrep-hooks ] unfixable = [\"B\", \"C4\", \"UP\", \"BLE\", \"T20\", \"RET\"] [tool.ruff.lint.pydocstyle] convention = \"numpy\" [tool.ruff.lint.per-file-ignores] \"tests/*\" = [\"D\", \"PT\", \"B024\"] \"*/__init__.py\" = [\"F401\", \"D104\", \"D107\", \"E402\"] \"docs/*\" = [\"D\",\"B\",\"E\",\"A\"] \"src/spatialdata/transformations/transformations.py\" = [\"D101\",\"D102\", \"D106\", \"B024\", \"T201\", \"RET504\", \"UP006\", \"UP007\"] \"src/spatialdata/transformations/operations.py\" = [\"D101\",\"D102\", \"D106\", \"B024\",\"D401\", \"T201\", \"RET504\", \"RET506\", \"RET505\", \"RET504\", \"UP006\", \"UP007\"] \"src/spatialdata/transformations/ngff/*.py\" = [\"D101\",\"D102\", \"D106\", \"D401\", \"E501\",\"RET506\", \"RET505\", \"RET504\", \"UP006\", \"UP007\"] \"src/spatialdata/transformations/*\" = [\"RET\", \"D\", \"UP006\", \"UP007\"] \"src/spatialdata/models/models.py\" = [\"D101\", \"B026\"] \"src/spatialdata/dataloader/datasets.py\" = [\"D101\"] \"tests/test_models/test_models.py\" = [\"NPY002\"] \"tests/conftest.py\"= [\"E402\"] \"benchmarks/*\" = [\"ALL\"] # pyupgrade typing rewrite TODO: remove at some point from per-file ignore # \"UP006\", \"UP007\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/spatialio",
            "repo_link": "https://codebase.helmholtz.cloud/ufz-sdi/spatialio",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/spechomo",
            "repo_link": "https://git.gfz-potsdam.de/geomultisens/spechomo",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/spiralize",
            "repo_link": "https://github.com/jokergoo/spiralize",
            "content": {
                "codemeta": "",
                "readme": "# Visualize Data on Spirals <img width=\"150\" src=\"https://user-images.githubusercontent.com/449218/121876090-723e0900-cd09-11eb-8d0d-82fbeeb83997.png\" align=\"right\"> [![R-CMD-check](https://github.com/jokergoo/spiral/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/spiral/actions) [![CRAN](https://www.r-pkg.org/badges/version/spiralize)](https://cran.r-project.org/web/packages/spiralize/index.html) [![CRAN](https://cranlogs.r-pkg.org/badges/grand-total/spiralize)](https://cran.r-project.org/web/packages/spiralize/index.html) ## Features The package **spiralize** visualizes data along an [Archimedean spiral](https://en.wikipedia.org/wiki/Archimedean_spiral). It has two major advantages for visualization: 1. It is able to visualize data with very long axis with high resolution. 2. It is efficient for time series data to reveal periodic patterns. ## Documentation https://jokergoo.github.io/spiralize/ ## Citation Zuguang Gu, et al., spiralize: an R package for Visualizing Data on Spirals, Bioinformatics, 2021. https://doi.org/10.1093/bioinformatics/btab778 ## Install The package is available on CRAN and can be installed by: ```r install.packages(\"spiralize\") ``` If you want the latest version, install it directly from GitHub: ```r library(devtools) install_github(\"jokergoo/spiralize\") ``` ## Usage It includes three steps: 1. initialize the spiral, 2. add a track, 3. add graphics to the track. Step 2 and 3 can be applied multiple times to allow multiple-track visualization along the spiral. The code for making spiral plot looks likes follows: ```r library(spiralize) spiral_initialize(...) spiral_track(...) spiral_points(...) ... ``` ## Graphics Complex plots are baiscally constructed from simple graphics. Here there are following low-level graphics functions: - `spiral_points()` - `spiral_lines()` - `spiral_rect()` - `spiral_segments()` - `spiral_polygon()` - `spiral_bars()` - `spiral_text()` - `spiral_axis()` - `spiral_yaxis()` - `spiral_raster()` Particularlly, horizon chart is very suitable to put on the spiral, thus there is one function for this: - `spiral_horizon()` Spiral plot can also visualize dendrograms with large number of leaves, thus there are following two functions: - `spiral_dendrogram()` - `spiral_phylo()` ## Examples 1. Difference of **ggplot2** daily downloads to the mean of the current year (2015-2021). Each loop contains 52 weeks so that same weeks in different years locate at the same angle in the polar coordinates. ![](https://user-images.githubusercontent.com/449218/122206221-671de100-cea1-11eb-823e-6c48de851667.png) 2. A phylogenetic life tree with 50645 species. ![](https://user-images.githubusercontent.com/449218/123804978-fbe6fc80-d8ed-11eb-93d8-d3f83d552dde.png) 3. The spiral COVID-19 Shiny app ![](https://user-images.githubusercontent.com/449218/154753102-d66b3588-eca1-471b-bdfe-2c147ed257f5.gif) ## License MIT @ Zuguang Gu\n",
                "dependencies": "Package: spiralize Type: Package Title: Visualize Data on Spirals Version: 1.1.0 Date: 2024-06-14 Authors@R: person(\"Zuguang\", \"Gu\", email = \"z.gu@dkfz.de\", role = c(\"aut\", \"cre\"), comment = c('ORCID'=\"0000-0002-7395-8709\")) Depends: R (>= 4.0.0), grid Imports: GlobalOptions (>= 0.1.1), GetoptLong (>= 0.1.8), circlize, stats, methods, grDevices, lubridate, utils, ComplexHeatmap Suggests: knitr, rmarkdown, grImport, grImport2, jpeg, png, tiff, cranlogs, cowplot, dendextend, bezier, magick, ape Description: It visualizes data along an Archimedean spiral <https://en.wikipedia.org/wiki/Archimedean_spiral>, makes so-called spiral graph or spiral chart. It has two major advantages for visualization: 1. It is able to visualize data with very long axis with high resolution. 2. It is efficient for time series data to reveal periodic patterns. VignetteBuilder: knitr URL: https://github.com/jokergoo/spiralize, https://jokergoo.github.io/spiralize/ License: MIT + file LICENSE NeedsCompilation: no RoxygenNote: 7.3.1 Encoding: UTF-8 Roxygen: list(markdown = TRUE)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/spirit",
            "repo_link": "https://github.com/spirit-code/spirit",
            "content": {
                "codemeta": "",
                "readme": "SPIRIT ============================= **SPIN SIMULATION FRAMEWORK**<br /> ![Logo](https://imgur.com/iWc1kuE.png \"Spirit Logo\") &nbsp; **Core Library:** | Branch | Build Status | Python Package Coverage | Core Library Coverage | | :------- | :----------: | :---------------------: | :-------------------: | | master: | ![CI](https://github.com/spirit-code/spirit/workflows/CI/badge.svg?branch=master) | [![Coverage Status](https://coveralls.io/repos/github/spirit-code/spirit/badge.svg?branch=master)](https://coveralls.io/github/spirit-code/spirit?branch=master) | [![Coverage Status](https://codecov.io/gh/spirit-code/spirit/branch/master/graph/badge.svg)](https://codecov.io/gh/spirit-code/spirit/branch/master) | | develop: | ![CI](https://github.com/spirit-code/spirit/workflows/CI/badge.svg?branch=develop) | [![Coverage Status](https://coveralls.io/repos/github/spirit-code/spirit/badge.svg?branch=develop)](https://coveralls.io/github/spirit-code/spirit?branch=develop) | [![Coverage Status](https://codecov.io/gh/spirit-code/spirit/branch/develop/graph/badge.svg)](https://codecov.io/gh/spirit-code/spirit/branch/develop) | **[Python package](https://pypi.org/project/spirit/):** [![PyPI version](https://badge.fury.io/py/spirit.svg)](https://badge.fury.io/py/spirit) &nbsp; The code is released under [MIT License](LICENSE.txt).<br /> If you intend to *present and/or publish* scientific results or visualisations for which you used Spirit, please cite [`G. P. Müller et al., Phys. Rev. B 99, 224414 (2019)`](https://link.aps.org/doi/10.1103/PhysRevB.99.224414) and read the [docs/REFERENCE.md](docs/REFERENCE.md). **This is an open project and contributions and collaborations are always welcome!!** See [docs/CONTRIBUTING.md](docs/CONTRIBUTING.md) on how to contribute or write an email to m.sallermann@fz-juelich.de<br /> For contributions and affiliations, see [docs/CONTRIBUTORS.md](docs/CONTRIBUTORS.md). Please note that a version of the *Spirit Web interface* is hosted by the Research Centre Jülich at http://juspin.de &nbsp; <!-- ![nur ein Beispiel](https://commons.wikimedia.org/wiki/File:Example_de.jpg \"Beispielbild\") --> ![Skyrmions](http://imgur.com/JgPj8t5.jpg \"Skyrmions on a 2D grid\") &nbsp; Contents -------- 1. [Introduction](#Introduction) 2. [Getting started with the Desktop User Interface](#Desktop) 3. [Getting started with the Python Package](#Python) --------------------------------------------- &nbsp; Introduction <a name=\"Introduction\"></a> --------------------------------------------- #### A modern framework for magnetism science on clusters, desktops & laptops and even your Phone **Spirit** is a **platform-independent** framework for spin dynamics, written in C++14. It combines the traditional cluster work, using using the command-line, with modern visualisation capabilites in order to maximize scientists' productivity. > \"It is unworthy of excellent men to lose hours like slaves in > the labour of calculation which could safely be relegated to > anyone else if machines were used.\" > - Gottfried Wilhelm Leibniz *Our goal is to build such machines*. The core library of the *Spirit* framework provides an **easy to use API**, which can be used from almost any programming language, and includes ready-to-use python bindings. A **powerful desktop user interface** is available, providing real-time visualisation and control of parameters. ### *Physics Features* - Atomistic Spin Lattice Heisenberg Model including also DMI and dipole-dipole - **Spin Dynamics simulations** obeying the [Landau-Lifschitz-Gilbert equation](https://en.wikipedia.org/wiki/Landau%E2%80%93Lifshitz%E2%80%93Gilbert_equation) - Direct **Energy minimisation** with different solvers - **Minimum Energy Path calculations** for transitions between different spin configurations, using the GNEB method ### *Highlights of the Framework* - Cross-platform: everything can be built and run on Linux, OSX and Windows - Standalone core library with C API which can be used from almost any programming language - **Python package** making complex simulation workflows easy - Desktop UI with powerful, live **3D visualisations** and direct control of most system parameters - Modular backends including **parallelisation on GPU** (CUDA) and **CPU** (OpenMP) ### *Documentation* More details may be found at [spirit-docs.readthedocs.io](http://spirit-docs.readthedocs.io) or in the [Reference section](docs/README.md) including - [Unix/OSX build instructions](docs/Build_Unix_OSX.md) - [Windows build instructions](docs/Build_Windows.md) - [Input File Reference](core/docs/Input.md) There is also a [Wiki](https://iffwiki.fz-juelich.de/index.php/Spirit \"Click me...\"), hosted by the Research Centre Jülich. --------------------------------------------- &nbsp; Getting started with the Desktop Interface <a name=\"Desktop\"></a> --------------------------------------------- See the build instructions for [Unix/OSX](docs/Build_Unix_OSX.md) or [Windows](docs/Build_Windows.md) on how to get the desktop user interface. ![Desktop UI with Isosurfaces in a thin layer](http://imgur.com/QUcN4aG.jpg \"Isosurfaces in a thin layer\") The user interface provides a powerful OpenGL visualisation window using the [VFRendering](https://github.com/FlorianRhiem/VFRendering) library. It provides functionality to - Control Calculations - Locally insert Configurations (homogeneous, skyrmions, spin spiral, ... ) - Generate homogeneous Transition Paths - Change parameters of the Hamiltonian - Change parameters of the Method and Solver - Configure the Visualization (arrows, isosurfaces, lighting, ...) See the [UI-QT Reference](docs/UI-Qt.md) for the key bindings of the various features. *Unfortunately, distribution of binaries for the Desktop UI is not possible due to the restrictive license on QT-Charts.* --------------------------------------------- &nbsp; Getting started with the Python Package <a name=\"Python\"></a> --------------------------------------------- To install the *Spirit python package*, either build and install from source ([Unix/OSX](docs/Build_Unix_OSX.md), [Windows](docs/Build_Windows.md)) or simply use pip install spirit With this package you have access to powerful Python APIs to run and control dynamics simulations or optimizations. This is especially useful for work on clusters, where you can now script your workflow, never having to re-compile when testing, debugging or adding features. The most simple example of a **spin dynamics simulation** would be ``` python from spirit import state, simulation with state.State(\"input/input.cfg\") as p_state: simulation.start(p_state, simulation.METHOD_LLG, simulation.SOLVER_SIB) ``` Where `SOLVER_SIB` denotes the semi-implicit method B and the starting configuration will be random. To add some meaningful content, we can change the **initial configuration** by inserting a Skyrmion into a homogeneous background: ``` python def skyrmion_on_homogeneous(p_state): from spirit import configuration configuration.plus_z(p_state) configuration.skyrmion(p_state, 5.0, phase=-90.0) ``` If we want to calculate a **minimum energy path** for a transition, we need to generate a sensible initial guess for the path and use the **GNEB method**. Let us consider the collapse of a skyrmion to the homogeneous state: ``` python from spirit import state, chain, configuration, transition, simulation ### Copy the system and set chain length chain.image_to_clipboard(p_state) noi = 7 chain.set_length(p_state, noi) ### First image is homogeneous with a Skyrmion in the center configuration.plus_z(p_state, idx_image=0) configuration.skyrmion(p_state, 5.0, phase=-90.0, idx_image=0) simulation.start(p_state, simulation.METHOD_LLG, simulation.SOLVER_VP, idx_image=0) ### Last image is homogeneous configuration.plus_z(p_state, idx_image=noi-1) simulation.start(p_state, simulation.METHOD_LLG, simulation.SOLVER_VP, idx_image=noi-1) ### Create transition of images between first and last transition.homogeneous(p_state, 0, noi-1) ### GNEB calculation simulation.start(p_state, simulation.METHOD_GNEB, simulation.SOLVER_VP) ``` where `SOLVER_VP` denotes a direct minimization with the velocity projection algorithm. You may also use *Spirit* order to **extract quantitative data**, such as the energy. ``` python def evaluate(p_state): from spirit import system, quantities M = quantities.get_magnetization(p_state) E = system.get_energy(p_state) return M, E ``` Obviously you may easily create significantly more complex workflows and use Python to e.g. pre- or post-process data or to distribute your work on a cluster and much more!\n",
                "dependencies": "######### CMake Version ############################################ cmake_minimum_required( VERSION 3.12 ) ### Distinguish between Clang and AppleClang cmake_policy( SET CMP0025 NEW ) #################################################################### ######### Build options ############################################ ### CMake Verbosity set( SPIRIT_PRINT_SOURCES OFF CACHE BOOL \"Print Spirit Headers and Sources from CMake.\" ) ### These decide which projects are built set( SPIRIT_BUILD_FOR_JS OFF CACHE BOOL \"Build the JavaScript library.\" ) set( SPIRIT_BUILD_FOR_JULIA OFF CACHE BOOL \"Build the shared library for Julia.\" ) set( SPIRIT_BUILD_FOR_PYTHON ON CACHE BOOL \"Build the shared library for Python.\" ) set( SPIRIT_BUILD_FOR_CXX ON CACHE BOOL \"Build the static library for C++ applications\" ) ### Feature switches for Spirit set( SPIRIT_ENABLE_PINNING OFF CACHE BOOL \"Enable pinning individual or rows of spins.\" ) set( SPIRIT_ENABLE_DEFECTS OFF CACHE BOOL \"Enable defects and disorder in the lattice.\" ) ### Options for Spirit set( SPIRIT_BUILD_TEST ON CACHE BOOL \"Build unit tests for the Spirit library.\" ) set( SPIRIT_TEST_COVERAGE OFF CACHE BOOL \"Build in debug mode with special flags for coverage checks.\" ) set( SPIRIT_USE_CUDA OFF CACHE BOOL \"Use CUDA to speed up certain parts of the code.\" ) set( SPIRIT_USE_OPENMP OFF CACHE BOOL \"Use OpenMP to speed up certain parts of the code.\" ) set( SPIRIT_USE_THREADS OFF CACHE BOOL \"Use std threads to speed up certain parts of the code.\" ) set( SPIRIT_USE_FFTW ON CACHE BOOL \"If available, use the FFTW library instead of kissFFT.\" ) ### Set the scalar type used in the Spirit library set( SPIRIT_SCALAR_TYPE \"double\" CACHE STRING \"The scalar type to be used in the Spirit library.\" ) ### Set the compute capability for CUDA compilation set( SPIRIT_CUDA_ARCH \"sm_60\" CACHE STRING \"The CUDA compute architecture to use in case of a CUDA build.\" ) #################################################################### ### CMake Verbosity option( SPIRIT_PRINT_SOURCES \"Print Headers and Sources from Cmake.\" OFF ) ### Decide UI option( SPIRIT_UI_USE_IMGUI \"Build the ImGUI user interface instead of the console version.\" OFF ) option( SPIRIT_UI_CXX_USE_QT \"Build the QT user interface instead of the console version.\" ON ) ### Bundle option option( SPIRIT_BUNDLE_APP \"On installation, bundle the executable with its dependencies.\" OFF ) ### Option for building on the IFF cluster option( SPIRIT_USER_PATHS_IFF \"Use the compiler and library paths etc. for the IFF Cluster.\" OFF ) #################################################################### #################################################################### ### Set a default build type in case none is passed if( NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES ) message( STATUS \">> Setting build type to 'Release' as none was specified.\" ) set( CMAKE_BUILD_TYPE Release CACHE STRING \"Choose the type of build.\" FORCE ) # Set the possible values of build type for cmake-gui set_property( CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\" ) elseif( CMAKE_BUILD_TYPE ) message( STATUS \">> Did not need to set build type, using: ${CMAKE_BUILD_TYPE}\" ) else() message( STATUS \">> Did not need to set build type. Configuration types: ${CMAKE_CONFIGURATION_TYPES}\" ) endif() ### Set a default install directory in case none is passed if( CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT OR NOT CMAKE_INSTALL_PREFIX ) set( CMAKE_INSTALL_PREFIX \"${CMAKE_BINARY_DIR}/install\" CACHE PATH \"default install path\" FORCE ) message( STATUS \">> No installation directory given. Using: '${CMAKE_INSTALL_PREFIX}'\" ) else() message( STATUS \">> Installation directory given: '${CMAKE_INSTALL_PREFIX}'\" ) endif() ### Prevent in-source builds # set(CMAKE_DISABLE_SOURCE_CHANGES ON) # we need source changes for the generated VERSION.txt set( CMAKE_DISABLE_IN_SOURCE_BUILD ON ) ### Position independent code set( CMAKE_POSITION_INDEPENDENT_CODE ON ) ### We need at least C++14 set( CMAKE_CXX_STANDARD 14 ) ### Set the cmake subdirectory list( APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_SOURCE_DIR}/CMake\" ) #################################################################### #################################################################### ### Depending on compiler versions it may be necessary to specify ### the compiler. Either pass them in via command-line or use ### the CUDA_TOOLKIT_ROOT_DIR variable. if( SPIRIT_USE_CUDA ) ### Deactivate OpenMP set( SPIRIT_USE_OPENMP OFF ) ### Set cuda toolkit path if( NOT CUDA_TOOLKIT_ROOT_DIR ) if( APPLE OR UNIX ) set( CUDA_TOOLKIT_ROOT_DIR /usr/local/cuda ) # set( CUDA_TOOLKIT_ROOT_DIR /opt/cuda ) elseif( WIN32 ) set( CUDA_TOOLKIT_ROOT_DIR \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/\" ) message( WARNING \">> We are on Windows... CUDA_TOOLKIT_ROOT_DIR may need to be passed to cmake...\" ) endif() endif() ### Set compilers if( APPLE OR UNIX ) if( DEFINED CUDA_TOOLKIT_ROOT_DIR ) message( STATUS \">> CUDA toolkit root dir: ${CUDA_TOOLKIT_ROOT_DIR}\" ) if( NOT DEFINED CMAKE_C_COMPILER ) message( STATUS \">> Set C compiler accordingly: ${CMAKE_C_COMPILER}\" ) endif() if( NOT DEFINED CMAKE_CXX_COMPILER ) message( STATUS \">> Set CXX compiler accordingly: ${CMAKE_CXX_COMPILER}\" ) endif() else() message( STATUS \">> No CUDA toolkit root dir specified\" ) endif() elseif( WIN32 ) # MESSAGE( STATUS \">> We are on Windows... CUDA untested\" ) endif() endif() #################################################################### ######### Determine the compiler ################################### ### IFF cluster paths if( SPIRIT_USER_PATHS_IFF ) message( STATUS \">> Using IFF Paths\" ) ### GCC compiler set( USER_COMPILER_C \"gcc\" ) set( USER_COMPILER_CXX \"g++\" ) set( USER_PATH_COMPILER \"/usr/local/gcc6/bin\" ) ### Qt location set( USER_PATH_QT \"/usr/local/qt5\" ) endif() ### User Paths ### Set the following if you do not want cmake to choose your compiler # set( USER_COMPILER_C \"gcc\" ) # set( USER_COMPILER_CXX \"g++\" ) # set( USER_PATH_COMPILER \"/usr/bin\" ) ### Set the following if you need cmake to find your Qt installation # set( USER_PATH_QT \"~/QT/5.7\" ) ### Choose the right compiler include( ChooseCompiler ) #################################################################### ######### Project name ############################################# project( spirit ) ### Print compiler info message( STATUS \">> Please check the CMAKE_CXX_COMPILER to make sure it's the right one\" ) message( STATUS \">> CMAKE_C_COMPILER: ${CMAKE_C_COMPILER}\" ) message( STATUS \">> CMAKE_CXX_COMPILER: ${CMAKE_CXX_COMPILER}\" ) #################################################################### ######### Platform-specific Flags ################################## ### Platform-specific flags if( APPLE ) set( PLATFORM_NAME \"Apple\" ) if( SPIRIT_BUNDLE_APP ) message( STATUS \">> Going to create a .app bundle\" ) set( OS_BUNDLE MACOSX_BUNDLE ) endif() elseif( UNIX ) set( PLATFORM_NAME \"UNIX\" ) elseif( WIN32 ) set( PLATFORM_NAME \"Win32\" ) add_compile_definitions( NOMINMAX _CRT_SECURE_NO_WARNINGS ) endif() ### Compiler-specific flags if( \"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\" ) if( CMAKE_CXX_COMPILER_VERSION VERSION_LESS 5.1 ) message( FATAL_ERROR \"GCC version must be at least 5.1!\" ) endif() elseif( \"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"MSVC\" ) ### Disable unnecessary warnings on Windows, such as C4996 and C4267, C4244 set( CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DNOMINMAX /wd4018 /wd4244 /wd4267 /wd4661 /wd4996\" ) endif() ### message( STATUS \">> We are on the platform: ${PLATFORM_NAME}\" ) message( STATUS \">> CMAKE_CXX_COMPILER_ID: ${CMAKE_CXX_COMPILER_ID}\" ) message( STATUS \">> CMAKE_CXX_FLAGS: ${CMAKE_CXX_FLAGS}\" ) message( STATUS \">> CMAKE_EXE_LINKER_FLAGS: ${CMAKE_EXE_LINKER_FLAGS}\" ) #################################################################### #################################################################### if( SPIRIT_BUILD_TEST ) enable_testing() endif() ### if( SPIRIT_USE_CUDA ) enable_language( CUDA ) endif() ### if( SPIRIT_USE_OPENMP ) include( FindOpenMP ) if( OPENMP_FOUND ) set( CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${OpenMP_C_FLAGS}\" ) set( CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}\" ) set( CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${OpenMP_EXE_LINKER_FLAGS}\" ) endif() endif() ### if( SPIRIT_SKIP_HTST ) add_definitions( \"-DSPIRIT_SKIP_HTST\" ) message( STATUS \">> Skipping compilation of HTST!\" ) endif() ### if( SPIRIT_BUILD_FOR_JS ) set( SPIRIT_BUILD_FOR_CXX OFF ) set( SPIRIT_USE_CUDA OFF ) set( SPIRIT_USE_OPENMP OFF ) set( SPIRIT_UI_CXX_USE_QT OFF ) set( SPIRIT_UI_USE_IMGUI ON ) if( ${CMAKE_SYSTEM_NAME} MATCHES \"Emscripten\" ) message( WARNING \"You set SPIRIT_BUILD_FOR_JS to ON but your are not using emscripten. That might not work\" ) endif() endif() ### Need to use pthread if emscripten is used if( SPIRIT_BUILD_FOR_JS AND SPIRIT_UI_USE_IMGUI ) set( CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pthread -s USE_PTHREADS=1 -s USE_PTHREADS=1 -s WASM=1\" ) endif() #################################################################### ######### Add subdirectory projects ################################ add_subdirectory( thirdparty/qhull ) set( qhull_INCLUDE_DIRS ${CMAKE_CURRENT_LIST_DIR}/thirdparty/qhull/src ${CMAKE_CURRENT_LIST_DIR}/thirdparty/qhull/src/libqhullcpp ) if( CMAKE_BUILD_TYPE MATCHES \"[dD]ebug\" ) set( qhull_LIBS qhullcpp_d qhullstatic_rd) else() set( qhull_LIBS qhullcpp qhullstatic_r ) endif() ### Spirit library is built in any case add_subdirectory( core ) ### Web UI if( SPIRIT_BUILD_FOR_JS ) if( SPIRIT_UI_USE_IMGUI ) add_definitions( -DSPIRIT_UI_USE_IMGUI ) endif() add_subdirectory( VFRendering ) add_subdirectory( ui-cpp ) add_subdirectory( ui-web ) ### CXX UI elseif( SPIRIT_BUILD_FOR_CXX ) if( SPIRIT_UI_CXX_USE_QT ) add_definitions( -DSPIRIT_UI_CXX_USE_QT ) elseif( SPIRIT_UI_USE_IMGUI ) add_definitions( -DSPIRIT_UI_USE_IMGUI ) endif() add_subdirectory( VFRendering ) add_subdirectory( ui-cpp ) endif() #################################################################### ################ Install ########################################### install( DIRECTORY ${CMAKE_CURRENT_LIST_DIR}/docs/ DESTINATION docs/Spirit/ COMPONENT spirit_root_files ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/README.md ${CMAKE_CURRENT_LIST_DIR}/VERSION.txt DESTINATION ./ COMPONENT spirit_root_files ) if( SPIRIT_BUILD_FOR_CXX ) install( DIRECTORY input DESTINATION bin COMPONENT spirit_root_files ) endif() install( FILES ${CMAKE_CURRENT_LIST_DIR}/LICENSE.txt DESTINATION ./ COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/thirdparty/qhull/COPYING.txt DESTINATION licenses RENAME qhull.txt COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/cub/LICENSE.TXT DESTINATION licenses RENAME cub.txt COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/Eigen/COPYING.BSD DESTINATION licenses RENAME eigen.txt COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/fmt/LICENSE.rst DESTINATION licenses RENAME fmt.rst COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/kiss_fft/COPYING DESTINATION licenses RENAME kiss_fft.txt COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/ovf/README.md DESTINATION licenses RENAME ovf.md COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/spectra/LICENSE DESTINATION licenses RENAME spectra.txt COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/core/thirdparty/termcolor/LICENSE DESTINATION licenses RENAME termcolor.txt COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/thirdparty/Lyra/LICENSE.txt DESTINATION licenses RENAME lyra.txt COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/filesystem/LICENSE DESTINATION licenses RENAME filesystem.txt COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/glad/LICENSE DESTINATION licenses RENAME glad.md COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/glfw/LICENSE.md DESTINATION licenses RENAME glfw.md COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/imgui/LICENSE.txt DESTINATION licenses RENAME imgui.txt COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/implot/LICENSE DESTINATION licenses RENAME implot.txt COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/json/LICENSE.MIT DESTINATION licenses RENAME json.txt COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/nativefiledialog/LICENSE DESTINATION licenses RENAME nativefiledialog.txt COMPONENT spirit_licenses ) install( FILES ${CMAKE_CURRENT_LIST_DIR}/ui-cpp/ui-imgui/thirdparty/stb/LICENSE DESTINATION licenses RENAME stb.txt COMPONENT spirit_licenses ) #################################################################### ######### Write VERSION.txt ######################################## file( WRITE \"${CMAKE_SOURCE_DIR}/VERSION.txt\" \"${SPIRIT_META_NAME_VERSION}\" ) ####################################################################\ncd build make install $1\n[tool.black] line-length = 88 [tool.pylint.messages_control] disable = [ \"duplicate-code\", \"line-too-long\", \"too-few-public-methods\", \"too-many-ancestors\", \"too-many-arguments\", \"too-many-branches\", \"too-many-instance-attributes\", \"too-many-lines\", \"too-many-locals\", \"too-many-return-statements\", \"too-many-statements\", \"no-member\" ]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/stable-baselines3",
            "repo_link": "https://github.com/DLR-RM/stable-baselines3",
            "content": {
                "codemeta": "",
                "readme": "<!-- [![pipeline status](https://gitlab.com/araffin/stable-baselines3/badges/master/pipeline.svg)](https://gitlab.com/araffin/stable-baselines3/-/commits/master) --> [![CI](https://github.com/DLR-RM/stable-baselines3/workflows/CI/badge.svg)](https://github.com/DLR-RM/stable-baselines3/actions/workflows/ci.yml) [![Documentation Status](https://readthedocs.org/projects/stable-baselines/badge/?version=master)](https://stable-baselines3.readthedocs.io/en/master/?badge=master) [![coverage report](https://gitlab.com/araffin/stable-baselines3/badges/master/coverage.svg)](https://github.com/DLR-RM/stable-baselines3/actions/workflows/ci.yml) [![codestyle](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) # Stable Baselines3 <img src=\"docs/\\_static/img/logo.png\" align=\"right\" width=\"40%\"/> Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of [Stable Baselines](https://github.com/hill-a/stable-baselines). You can read a detailed presentation of Stable Baselines3 in the [v1.0 blog post](https://araffin.github.io/post/sb3/) or our [JMLR paper](https://jmlr.org/papers/volume22/20-1364/20-1364.pdf). These algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details. **Note: Despite its simplicity of use, Stable Baselines3 (SB3) assumes you have some knowledge about Reinforcement Learning (RL).** You should not utilize this library without some practice. To that extent, we provide good resources in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/rl.html) to get started with RL. ## Main Features **The performance of each algorithm was tested** (see *Results* section in their respective page), you can take a look at the issues [#48](https://github.com/DLR-RM/stable-baselines3/issues/48) and [#49](https://github.com/DLR-RM/stable-baselines3/issues/49) for more details. We also provide detailed logs and reports on the [OpenRL Benchmark](https://wandb.ai/openrlbenchmark/sb3) platform. | **Features** | **Stable-Baselines3** | | --------------------------- | ----------------------| | State of the art RL methods | :heavy_check_mark: | | Documentation | :heavy_check_mark: | | Custom environments | :heavy_check_mark: | | Custom policies | :heavy_check_mark: | | Common interface | :heavy_check_mark: | | `Dict` observation space support | :heavy_check_mark: | | Ipython / Notebook friendly | :heavy_check_mark: | | Tensorboard support | :heavy_check_mark: | | PEP8 code style | :heavy_check_mark: | | Custom callback | :heavy_check_mark: | | High code coverage | :heavy_check_mark: | | Type hints | :heavy_check_mark: | ### Planned features Since most of the features from the [original roadmap](https://github.com/DLR-RM/stable-baselines3/issues/1) have been implemented, there are no major changes planned for SB3, it is now *stable*. If you want to contribute, you can search in the issues for the ones where [help is welcomed](https://github.com/DLR-RM/stable-baselines3/labels/help%20wanted) and the other [proposed enhancements](https://github.com/DLR-RM/stable-baselines3/labels/enhancement). While SB3 development is now focused on bug fixes and maintenance (doc update, user experience, ...), there is more active development going on in the associated repositories: - newer algorithms are regularly added to the [SB3 Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib) repository - faster variants are developed in the [SBX (SB3 + Jax)](https://github.com/araffin/sbx) repository - the training framework for SB3, the RL Zoo, has an active [roadmap](https://github.com/DLR-RM/rl-baselines3-zoo/issues/299) ## Migration guide: from Stable-Baselines (SB2) to Stable-Baselines3 (SB3) A migration guide from SB2 to SB3 can be found in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html). ## Documentation Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/) ## Integrations Stable-Baselines3 has some integration with other libraries/services like Weights & Biases for experiment tracking or Hugging Face for storing/sharing trained models. You can find out more in the [dedicated section](https://stable-baselines3.readthedocs.io/en/master/guide/integrations.html) of the documentation. ## RL Baselines3 Zoo: A Training Framework for Stable Baselines3 Reinforcement Learning Agents [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL). It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos. In addition, it includes a collection of tuned hyperparameters for common environments and RL algorithms, and agents trained with those settings. Goals of this repository: 1. Provide a simple interface to train and enjoy RL agents 2. Benchmark the different Reinforcement Learning algorithms 3. Provide tuned hyperparameters for each environment and RL algorithm 4. Have fun with the trained agents! Github repo: https://github.com/DLR-RM/rl-baselines3-zoo Documentation: https://rl-baselines3-zoo.readthedocs.io/en/master/ ## SB3-Contrib: Experimental RL Features We implement experimental features in a separate contrib repository: [SB3-Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib) This allows SB3 to maintain a stable and compact core, while still providing the latest features, like Recurrent PPO (PPO LSTM), CrossQ, Truncated Quantile Critics (TQC), Quantile Regression DQN (QR-DQN) or PPO with invalid action masking (Maskable PPO). Documentation is available online: [https://sb3-contrib.readthedocs.io/](https://sb3-contrib.readthedocs.io/) ## Stable-Baselines Jax (SBX) [Stable Baselines Jax (SBX)](https://github.com/araffin/sbx) is a proof of concept version of Stable-Baselines3 in Jax, with recent algorithms like DroQ or CrossQ. It provides a minimal number of features compared to SB3 but can be much faster (up to 20x times!): https://twitter.com/araffin2/status/1590714558628253698 ## Installation **Note:** Stable-Baselines3 supports PyTorch >= 2.3 ### Prerequisites Stable Baselines3 requires Python 3.9+. #### Windows To install stable-baselines on Windows, please look at the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/install.html#prerequisites). ### Install using pip Install the Stable Baselines3 package: ```sh pip install 'stable-baselines3[extra]' ``` This includes an optional dependencies like Tensorboard, OpenCV or `ale-py` to train on atari games. If you do not need those, you can use: ```sh pip install stable-baselines3 ``` Please read the [documentation](https://stable-baselines3.readthedocs.io/) for more details and alternatives (from source, using docker). ## Example Most of the code in the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms. Here is a quick example of how to train and run PPO on a cartpole environment: ```python import gymnasium as gym from stable_baselines3 import PPO env = gym.make(\"CartPole-v1\", render_mode=\"human\") model = PPO(\"MlpPolicy\", env, verbose=1) model.learn(total_timesteps=10_000) vec_env = model.get_env() obs = vec_env.reset() for i in range(1000): action, _states = model.predict(obs, deterministic=True) obs, reward, done, info = vec_env.step(action) vec_env.render() # VecEnv resets automatically # if done: # obs = env.reset() env.close() ``` Or just train a model with a one liner if [the environment is registered in Gymnasium](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#registering-envs) and if [the policy is registered](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html): ```python from stable_baselines3 import PPO model = PPO(\"MlpPolicy\", \"CartPole-v1\").learn(10_000) ``` Please read the [documentation](https://stable-baselines3.readthedocs.io/) for more examples. ## Try it online with Colab Notebooks ! All the following examples can be executed online using Google Colab notebooks: - [Full Tutorial](https://github.com/araffin/rl-tutorial-jnrr19) - [All Notebooks](https://github.com/Stable-Baselines-Team/rl-colab-notebooks/tree/sb3) - [Getting Started](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/stable_baselines_getting_started.ipynb) - [Training, Saving, Loading](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/saving_loading_dqn.ipynb) - [Multiprocessing](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/multiprocessing_rl.ipynb) - [Monitor Training and Plotting](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/monitor_training.ipynb) - [Atari Games](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/atari_games.ipynb) - [RL Baselines Zoo](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/rl-baselines-zoo.ipynb) - [PyBullet](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/pybullet.ipynb) ## Implemented Algorithms | **Name** | **Recurrent** | `Box` | `Discrete` | `MultiDiscrete` | `MultiBinary` | **Multi Processing** | | ------------------- | ------------------ | ------------------ | ------------------ | ------------------- | ------------------ | --------------------------------- | | ARS<sup>[1](#f1)</sup> | :x: | :heavy_check_mark: | :heavy_check_mark: | :x: | :x: | :heavy_check_mark: | | A2C | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | CrossQ<sup>[1](#f1)</sup> | :x: | :heavy_check_mark: | :x: | :x: | :x: | :heavy_check_mark: | | DDPG | :x: | :heavy_check_mark: | :x: | :x: | :x: | :heavy_check_mark: | | DQN | :x: | :x: | :heavy_check_mark: | :x: | :x: | :heavy_check_mark: | | HER | :x: | :heavy_check_mark: | :heavy_check_mark: | :x: | :x: | :heavy_check_mark: | | PPO | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | QR-DQN<sup>[1](#f1)</sup> | :x: | :x: | :heavy_check_mark: | :x: | :x: | :heavy_check_mark: | | RecurrentPPO<sup>[1](#f1)</sup> | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | SAC | :x: | :heavy_check_mark: | :x: | :x: | :x: | :heavy_check_mark: | | TD3 | :x: | :heavy_check_mark: | :x: | :x: | :x: | :heavy_check_mark: | | TQC<sup>[1](#f1)</sup> | :x: | :heavy_check_mark: | :x: | :x: | :x: | :heavy_check_mark: | | TRPO<sup>[1](#f1)</sup> | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Maskable PPO<sup>[1](#f1)</sup> | :x: | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | <b id=\"f1\">1</b>: Implemented in [SB3 Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib) GitHub repository. Actions `gymnasium.spaces`: * `Box`: A N-dimensional box that contains every point in the action space. * `Discrete`: A list of possible actions, where each timestep only one of the actions can be used. * `MultiDiscrete`: A list of possible actions, where each timestep only one action of each discrete set can be used. * `MultiBinary`: A list of possible actions, where each timestep any of the actions can be used in any combination. ## Testing the installation ### Install dependencies ```sh pip install -e .[docs,tests,extra] ``` ### Run tests All unit tests in stable baselines3 can be run using `pytest` runner: ```sh make pytest ``` To run a single test file: ```sh python3 -m pytest -v tests/test_env_checker.py ``` To run a single test: ```sh python3 -m pytest -v -k 'test_check_env_dict_action' ``` You can also do a static type check using `mypy`: ```sh pip install mypy make type ``` Codestyle check with `ruff`: ```sh pip install ruff make lint ``` ## Projects Using Stable-Baselines3 We try to maintain a list of projects using stable-baselines3 in the [documentation](https://stable-baselines3.readthedocs.io/en/master/misc/projects.html), please tell us if you want your project to appear on this page ;) ## Citing the Project To cite this repository in publications: ```bibtex @article{stable-baselines3, author = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann}, title = {Stable-Baselines3: Reliable Reinforcement Learning Implementations}, journal = {Journal of Machine Learning Research}, year = {2021}, volume = {22}, number = {268}, pages = {1-8}, url = {http://jmlr.org/papers/v22/20-1364.html} } ``` Note: If you need to refer to a specific version of SB3, you can also use the [Zenodo DOI](https://doi.org/10.5281/zenodo.8123988). ## Maintainers Stable-Baselines3 is currently maintained by [Ashley Hill](https://github.com/hill-a) (aka @hill-a), [Antonin Raffin](https://araffin.github.io/) (aka [@araffin](https://github.com/araffin)), [Maximilian Ernestus](https://github.com/ernestum) (aka @ernestum), [Adam Gleave](https://github.com/adamgleave) (@AdamGleave), [Anssi Kanervisto](https://github.com/Miffyli) (@Miffyli) and [Quentin Gallouédec](https://gallouedec.com/) (@qgallouedec). **Important Note: We do not provide technical support, or consulting** and do not answer personal questions via email. Please post your question on the [RL Discord](https://discord.com/invite/xhfNqQv), [Reddit](https://www.reddit.com/r/reinforcementlearning/), or [Stack Overflow](https://stackoverflow.com/) in that case. ## How To Contribute To any interested in making the baselines better, there is still some documentation that needs to be done. If you want to contribute, please read [**CONTRIBUTING.md**](./CONTRIBUTING.md) guide first. ## Acknowledgments The initial work to develop Stable Baselines3 was partially funded by the project *Reduced Complexity Models* from the *Helmholtz-Gemeinschaft Deutscher Forschungszentren*, and by the EU's Horizon 2020 Research and Innovation Programme under grant number 951992 ([VeriDream](https://www.veridream.eu/)). The original version, Stable Baselines, was created in the [robotics lab U2IS](http://u2is.ensta-paristech.fr/index.php?lang=en) ([INRIA Flowers](https://flowers.inria.fr/) team) at [ENSTA ParisTech](http://www.ensta-paristech.fr/en). Logo credits: [L.M. Tenkes](https://www.instagram.com/lucillehue/)\n",
                "dependencies": "[tool.ruff] # Same as Black. line-length = 127 # Assume Python 3.9 target-version = \"py39\" [tool.ruff.lint] # See https://beta.ruff.rs/docs/rules/ select = [\"E\", \"F\", \"B\", \"UP\", \"C90\", \"RUF\"] # B028: Ignore explicit stacklevel` # RUF013: Too many false positives (implicit optional) ignore = [\"B028\", \"RUF013\"] [tool.ruff.lint.per-file-ignores] # Default implementation in abstract methods \"./stable_baselines3/common/callbacks.py\" = [\"B027\"] \"./stable_baselines3/common/noise.py\" = [\"B027\"] # ClassVar, implicit optional check not needed for tests \"./tests/*.py\" = [\"RUF012\", \"RUF013\"] [tool.ruff.lint.mccabe] # Unlike Flake8, default to a complexity level of 10. max-complexity = 15 [tool.black] line-length = 127 [tool.mypy] ignore_missing_imports = true follow_imports = \"silent\" show_error_codes = true exclude = \"\"\"(?x)( tests/test_logger.py$ | tests/test_train_eval_mode.py$ )\"\"\" [tool.pytest.ini_options] # Deterministic ordering for tests; useful for pytest-xdist. env = [\"PYTHONHASHSEED=0\"] filterwarnings = [ # A2C/PPO on GPU \"ignore:You are trying to run (PPO|A2C) on the GPU\", # Tensorboard warnings \"ignore::DeprecationWarning:tensorboard\", # Gymnasium warnings \"ignore::UserWarning:gymnasium\", # tqdm warning about rich being experimental \"ignore:rich is experimental\", ] markers = [ \"expensive: marks tests as expensive (deselect with '-m \\\"not expensive\\\"')\", ] [tool.coverage.run] disable_warnings = [\"couldnt-parse\"] branch = false omit = [ \"tests/*\", \"setup.py\", # Require graphical interface \"stable_baselines3/common/results_plotter.py\", # Require ffmpeg \"stable_baselines3/common/vec_env/vec_video_recorder.py\", ] [tool.coverage.report] exclude_lines = [ \"pragma: no cover\", \"raise NotImplementedError()\", \"if typing.TYPE_CHECKING:\", ]\nimport os from setuptools import find_packages, setup with open(os.path.join(\"stable_baselines3\", \"version.txt\")) as file_handler: __version__ = file_handler.read().strip() long_description = \"\"\" # Stable Baselines3 Stable Baselines3 is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of [Stable Baselines](https://github.com/hill-a/stable-baselines). These algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details. ## Links Repository: https://github.com/DLR-RM/stable-baselines3 Blog post: https://araffin.github.io/post/sb3/ Documentation: https://stable-baselines3.readthedocs.io/en/master/ RL Baselines3 Zoo: https://github.com/DLR-RM/rl-baselines3-zoo SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib ## Quick example Most of the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms using Gym. Here is a quick example of how to train and run PPO on a cartpole environment: ```python import gymnasium from stable_baselines3 import PPO env = gymnasium.make(\"CartPole-v1\", render_mode=\"human\") model = PPO(\"MlpPolicy\", env, verbose=1) model.learn(total_timesteps=10_000) vec_env = model.get_env() obs = vec_env.reset() for i in range(1000): action, _states = model.predict(obs, deterministic=True) obs, reward, done, info = vec_env.step(action) vec_env.render() # VecEnv resets automatically # if done: # obs = vec_env.reset() ``` Or just train a model with a one liner if [the environment is registered in Gymnasium](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/) and if [the policy is registered](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html): ```python from stable_baselines3 import PPO model = PPO(\"MlpPolicy\", \"CartPole-v1\").learn(10_000) ``` \"\"\" # noqa:E501 setup( name=\"stable_baselines3\", packages=[package for package in find_packages() if package.startswith(\"stable_baselines3\")], package_data={\"stable_baselines3\": [\"py.typed\", \"version.txt\"]}, install_requires=[ \"gymnasium>=0.29.1,<1.2.0\", \"numpy>=1.20,<3.0\", \"torch>=2.3,<3.0\", # For saving models \"cloudpickle\", # For reading logs \"pandas\", # Plotting learning curves \"matplotlib\", ], extras_require={ \"tests\": [ # Run tests and coverage \"pytest\", \"pytest-cov\", \"pytest-env\", \"pytest-xdist\", # Type check \"mypy\", # Lint code and sort imports (flake8 and isort replacement) \"ruff>=0.3.1\", # Reformat \"black>=25.1.0,<26\", ], \"docs\": [ \"sphinx>=5,<9\", \"sphinx-autobuild\", \"sphinx-rtd-theme>=1.3.0\", # For spelling \"sphinxcontrib.spelling\", # Copy button for code snippets \"sphinx_copybutton\", ], \"extra\": [ # For render \"opencv-python\", \"pygame\", # Tensorboard support \"tensorboard>=2.9.1\", # Checking memory taken by replay buffer \"psutil\", # For progress bar callback \"tqdm\", \"rich\", # For atari games, \"ale-py>=0.9.0\", \"pillow\", ], }, description=\"Pytorch version of Stable Baselines, implementations of reinforcement learning algorithms.\", author=\"Antonin Raffin\", url=\"https://github.com/DLR-RM/stable-baselines3\", author_email=\"antonin.raffin@dlr.de\", keywords=\"reinforcement-learning-algorithms reinforcement-learning machine-learning \" \"gymnasium gym openai stable baselines toolbox python data-science\", license=\"MIT\", long_description=long_description, long_description_content_type=\"text/markdown\", version=__version__, python_requires=\">=3.9\", # PyPI package information. project_urls={ \"Code\": \"https://github.com/DLR-RM/stable-baselines3\", \"Documentation\": \"https://stable-baselines3.readthedocs.io/\", \"Changelog\": \"https://stable-baselines3.readthedocs.io/en/master/misc/changelog.html\", \"SB3-Contrib\": \"https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\", \"RL-Zoo\": \"https://github.com/DLR-RM/rl-baselines3-zoo\", \"SBX\": \"https://github.com/araffin/sbx\", }, classifiers=[ \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", ], ) # python setup.py sdist # python setup.py bdist_wheel # twine upload --repository-url https://test.pypi.org/legacy/ dist/* # twine upload dist/*\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/stmlab",
            "repo_link": "https://gitlab.com/dlr-sy/stmlab",
            "content": {
                "codemeta": "",
                "readme": "[![doi](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.13844636-red.svg)](https://zenodo.org/records/13844636) [![PyPi](https://img.shields.io/pypi/v/stmlab?label=PyPi)](https://pypi.org/project/stmlab/) # STMLab > This Python package provides an independent standard runtime environment for software projects developed by the [Department of Structural Mechanics](https://www.dlr.de/en/sy/about-us/departments/structural-mechanics) at the [Institute of Lightweight Structures](https://www.dlr.de/en/sy) of the [German Aerospace Center](https://www.dlr.de/en) It uses the [Jupyter](https://jupyter.org/) project as its graphical user interface. Two types of installation procedures are available. A community version can be installed and executed using pip. An enterprise version with yet unpublished software projects is available as an offline installer on request. ## Downloading Use GIT to get the latest code base. From the command line, use ``` git clone https://gitlab.dlr.de/dlr-sy/stmlab stmlab ``` If you check out the repository for the first time, you have to initialize all submodule dependencies first. Execute the following from within the repository. ``` git submodule update --init --recursive ``` To update all refererenced submodules to the latest production level, use ``` git submodule foreach --recursive 'git pull origin $(git config -f $toplevel/.gitmodules submodule.$name.branch || echo master)' ``` ## Installation STMLab can be installed from source using [poetry](https://python-poetry.org). If you don't have [poetry](https://python-poetry.org) installed, run ``` pip install poetry --pre --upgrade ``` to install the latest version of [poetry](https://python-poetry.org) within your python environment. Use ``` poetry update ``` to update all dependencies in the lock file or directly execute ``` poetry install ``` to install all dependencies from the lock file. Last, you should be able to import STMLab as a python package. ```python import stmlab ``` ## Contact * [Marc Garbade](mailto:marc.garbade@dlr.de)\n",
                "dependencies": "# TOML file for STMLab. # # @note: TOML file # Created on 20.03.2024 # # @version: 1.0 # ---------------------------------------------------------------------------------------------- # @requires: # - # # @change: # - # # @author: garb_ma [DLR-SY,STM Braunschweig] # ---------------------------------------------------------------------------------------------- [build-system] requires = [\"poetry-core>=1.0.0\"] build-backend = \"poetry.core.masonry.api\" [tool.poetry] name = \"stmlab\" version = \"0.11.0\" description = \"JupyterLab-based launch environment for software projects developed by the Department of Structural Mechanics at the German Aerospace Center\" authors = [\"Garbade, Marc <marc.garbade@dlr.de>\"] license = \"MIT\" readme = \"README.md\" packages = [{include=\"stmlab\", from=\"src\"}] repository = \"https://gitlab.com/dlr-sy/stmlab\" documentation = \"https://gitlab.com/dlr-sy/stmlab/-/blob/main/README.md\" keywords = [\"stmlab\",\"jupyter\",\"manager\"] classifiers = [ \"Development Status :: 3 - Alpha\", \"Topic :: Scientific/Engineering\", \"Programming Language :: Python :: 2\", \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: MIT License\", \"Operating System :: OS Independent\" ] [tool.poetry.urls] Changelog = \"https://gitlab.com/dlr-sy/stmlab/-/blob/master/CHANGELOG.md\" [[tool.poetry.source]] name = \"PyPI\" priority = \"primary\" [[tool.poetry.source]] name = \"dlr-sy\" url = \"https://gitlab.dlr.de/api/v4/groups/541/-/packages/pypi/simple\" priority = \"supplemental\" [tool.poetry.dependencies] python = \"~2.7 || ^3.5\" typer = [{version = \">=0.12\", python = \"^3.7\"}] pyx-core = [{version = \">=1.20\", python = \"~2.7 || ^3.5,<3.7\"}, {version = \"*\", python = \"^3.7\"}] pyc-core = [{version = \">=1.11\", python = \"~2.7 || ^3.5,<3.7\", optional = true}, {version = \"*\", python = \"^3.7\", optional = true}] # All optional dependencies beos = [{version = \">=1.3\", python = \"~2.7 || ^3.5\", optional = true}] boxbeam = [{version = \"1.3\", python = \"~2.7 || ^3.5\", optional = true}] damapper = [{version = \">=1.7\", python = \"^3.6\", optional = true}] displam = [{version = \">=1.3\", python = \"~2.7 || ^3.5\", optional = true}] micofam = [{version = \">=1.1\", python = \"~2.7 || ^3.5\", optional = true}] mcodac = [{version = \">=1.2\", python = \"~2.7 || ^3.5\"}] vampire = [{version = \">=0.2.5\", python = \"^3.7\", source = \"dlr-sy\", optional = true}] pyx-client = [{version = \">=1.20\", python = \"^3.7\", optional = true}] pyc-client = [{version = \">=1.11\", python = \"^3.7\", optional = true}] pyx-webservice = [{version = \">=1.20\", python = \"^3.7\", optional = true}] pyc-webservice = [{version = \">=1.11\", python = \"^3.7\", optional = true}] # Additional dependencies for development [tool.poetry.group.dev.dependencies] pyx-poetry = [{version = \">=1.20\", python=\"^3.7\"}] pyx-core = [{git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyXMake.git\", python = \"^3.7\"}] pyc-core = [{git = \"https://gitlab.dlr.de/fa_sw/stmlab/PyCODAC.git\", python = \"^3.7\"}] [tool.poetry.group.lock.dependencies] setuptools = [{version = \"^39.0\", python = \"~2.7\"}, {version = \"^49.0\", python = \"~3.5\"}, {version = \"^58.0\", python = \"~3.6\"}, {version = \"^64.0\", python = \"~3.7\"}, {version = \"^70,<74\", python = \"^3.8\"}] sympy = [{version = \"^1.10\", python = \"~3.7\"}, {version = \"^1.11\", python = \"^3.8\"}] typing-extensions = [{version = \"^4.7\", python = \"~3.7\"}, {version = \">=4.8\", python = \"^3.8\"}] vtk = [{version = \"^9,<9.4\", python = \"^3.5,<3.8\"}, {version = \"*\", python = \"^3.8\"}] [tool.poetry.extras] all = [\"beos\",\"boxbeam\",\"displam\",\"pyc-core\",\"pyc-webservice\",\"pyx-webservice\"] server = [\"pyc-webservice\",\"pyx-webservice\"] client = [\"pyc-client\",\"pyx-client\"] displam = [\"displam\"] beos = [\"beos\"] boxbeam = [\"boxbeam\"] damapper = [\"damapper\"] micofam = [\"micofam\"] mcodac = [\"mcodac\"] vampire = [\"vampire\"] codac = [\"damapper\",\"pyc-core\"] [tool.poetry.scripts] stmlab = \"stmlab.command:main\" [tool.pyxmake.archive] name = \"stmlab-main.tar.gz\" source = \"src/stmlab\" [tool.pyxmake.pyinstaller] name = \"stmlab\" icon = \"src/stmlab/assets/favicon.ico\" source = \"src/stmlab\" file = \"__exe__.py\" output = \"bin\" mode = \"onedir\" verbosity = \"2\" [tool.pyxmake.sphinx] name = \"'Structural Mechanics Lab'\" icon = \"doc/stmlab/pics/stm_lab_logo_bubbles.png\" source = \"doc/stmlab/source\" output = \"doc/stmlab\" file = \"stmlab\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/stream2segment",
            "repo_link": "https://github.com/rizac/stream2segment",
            "content": {
                "codemeta": "",
                "readme": "# <img align=\"left\" height=\"30\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/medien_kommunikation/Infothek/Mediathek/Bilder/GFZ/GFZ_Logo/GFZ-Logo_eng_RGB.svg\"> Stream2segment <img align=\"right\" height=\"50\" src=\"https://www.gfz-potsdam.de/fileadmin/gfz/GFZ_Wortmarke_SVG_klein_en_edit.svg\"> |Jump to: | [Usage](#usage) | [Installation](#installation) | [Development and Maintenance](#development-and-maintenance) | [Citation](#citation) | | - | - | - | - | - | A Python library and command line application to download, process and visualize event-based seismic waveform segments, specifically designed to manage big volumes of data. The key aspects with respect to widely-used similar applications is the use of a Relational database management system (RDBMS) to store downloaded data and metadata. The main advantages of this approach are: * **Storage efficiency**: no huge amount of files, no complex, virtually unusable directory structures. Moreover, a database prevents data and metadata inconsistency by design, and allows more easily to track what has already been downloaded in order to customize and improve further downloads * **Simple Python objects representing stored data and relationships**, easy to work with in any kind of custom code accessing the database. For instance, a segment is represented by a `Segment` object with its data, metadata and related objects easily accessible through its attributes, e.g., `segment.stream()`, `segment.maxgap_numsamples`, `segment.event.magnitude`, `segment.station.network`, `segment.channel.orientation_code` and so on. * **A powerful segments selection** made even easier by means of a simplified syntax: map any attribute described above to a selection expression (e.g. `segment.event.magnitude: \"[4, 5)\"`) and with few lines you can compose complex database queries such as e.g., *\"get all downloaded segments within a given magnitude range, with well-formed data and no gaps, from broadband channels only and a given specific network\"* ## Usage For full details, please consult the [wiki page](https://github.com/rizac/stream2segment/wiki) Stream2segment is a Python library and command line application available after installation via the command `s2s` on the terminal. By typing `s2s --help` you will see all available subcommands for downloading and managing data, launching Python processing functions, creating class labels for segments annotation, or producing graphical output, as shown below: ![S2s GUI](https://raw.githubusercontent.com/wiki/rizac/stream2segment/images/screenshot_gui.png) <!-- <table> <tr> <td align=\"center\"><img width=\"90%\" src=\"https://geofon.gfz-potsdam.de/software/stream2segment/processgui.png\"/></td> <td align=\"center\"><img width=\"90%\" src=\"https://geofon.gfz-potsdam.de/software/stream2segment/s2s_dinfogui.png\"/></td> </tr> <tr> <td>The <code>s2s show ...</code> command opens a GUI in the browser where downloaded data and customizable plots are shown</td> <td> The <code>s2s dl dstats ...</code> command opens an HTML page in the browser where download statistics can be shown</td> </tr> </table> <sub>Both image linked from https://geofon.gfz-potsdam.de/software/stream2segment/</sub> --> You start the program via the command `init` ( `s2s init --help` for details) to create several fully documented examples files that you can immediately start to configure and modify (see the **[gitHub wiki page](https://github.com/rizac/stream2segment/wiki)** for details). In a nutshell: 1. **A download configuration file** in YAML syntax. Edit the file (all documentation is provided in the file as block comments) and start downloading by typing: ```console s2s download -c <config_file> ... ``` > **Note** the path of the database where to store the downloaded data must be input in the config file. The supported database types are SQLite and Postgres: for massive downloads (as a rule of thumb: &ge; 1 million segments) we suggest to use Postgres. In any case, we **strongly** suggest running the program on computers with at least **16GB** of RAM. > **Note** massive downloads are time-consuming operations where it is likely to miss some data due to any kind of temporary connection problems. Consequently, **it is advisable to perform the same massive download at least twice with the same configuration** (subsequent runs will be faster as data will not be re-downloaded unnecessarily) 2. **A Jupyter notebook tutorial with examples for processing downloaded data**, for user who prefer this approach instead of the processing module described below (online version **[here](https://github.com/rizac/stream2segment/wiki/Using-Stream2segment-in-your-Python-code)**) 3. **Two Python modules** (with relative configuration in YAML syntax): 1. `paramtable.py`: process downloaded data and produce a tabular output (CSV, HDF) by executing the module as script (see code block after `if __name__ == \"__main__\"` in the module for details): ```console python paramtable.py ... ``` 2. `gui.py`: visualize downloaded data in the user browser via the plots defined in the module (an example in the figure above): ```console s2s show -d download.yaml -p gui.py -c gui.yaml ... ``` (Type `s2s show --help` for details). > **Note**: the associated YAML files (`paramtable.yaml`, `gui.yaml`) are not mandatory but enforce the good practice of separating configuration settings (YAML) and the actual Python code. This way you can experiment the same code with several settings by only creating different YAML files ## Installation This program has been installed and tested on Ubuntu (14 and later) and macOS (El Capitan and later). In case of installation problems, we suggest you to proceed in this order: 1. Look at [Installation Notes](#installation-notes) to check if the problem has already been observed and a solution proposed 2. Google for the solution (as always) 3. [Ask for help](https://github.com/rizac/stream2segment/issues) ### 1 Requirements In this section we assume that you already have Python (**3.5 or later**) and the required database software. The latter should not be needed if you use [SQLite](https://docs.python.org/3/library/sqlite3.html) or if the database is already installed remotely, so basically you are concerned only if you need to download data locally (on your computer) on a Postgres database. #### 1.1 macOS On macOS (El Capitan and later) all required software is generally already preinstalled. We suggest you to go to the next step and look at the [Installation Notes](#installation-notes) in case of problems (to install software on macOS, we recommend to use [brew](https://brew.sh/)). <details> <summary>Details</summary> In few cases, on some computers we needed to run one or more of the following commands (it's up to you to run them now or later, only those really needed): ``` xcode-select --install brew install openssl brew install c-blosc brew install git ``` </details> #### 1.2 Ubuntu Ubuntu does not generally have all required packages pre-installed. The bare minimum of the necessary packages can be installed with the `apt-get` command: ``` sudo apt-get install git python3-pip python3-dev # python 3 ``` <details> <summary>Details</summary> In few cases, on some computers we needed to run one or more of the following commands (it's up to you to run them now or later, only those really needed): Upgrade `gcc` first: ``` sudo apt-get update sudo apt-get upgrade gcc ``` Then: ``` sudo apt-get update sudo apt-get install libpng-dev libfreetype6-dev \\ build-essential gfortran libatlas-base-dev libxml2-dev libxslt-dev python-tk ``` </details> ### 2 Clone repository Git-clone (basically: download) this repository to a specific folder of your choice: ``` git clone https://github.com/rizac/stream2segment.git ./stream2segment ``` and move into the repository root: ``` cd stream2segment ``` ### 3 Install and activate Python virtualenv We strongly recommend to use Python virtual environment to avoid conflicts with already installed packages on your operating system (if you already have a virtual environment, just activate it and go to the next section). Conda users (e.g. Anaconda, Miniconda) can skip this section and check the [Conda documentation](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) instead. Make virtual environment in a \"stream2segment/env\" directory (env is a convention, but it's ignored by `git commit` so better keeping it. You can also use \".env\" which makes it usually hidden in Ubuntu. Also on Ubuntu, you might need to install `venv` first via `sudo apt-get install python3-venv`) ``` python3 -m venv ./env ``` To activate your virtual environment, type: ``` source env/bin/activate ``` or `source env/bin/activate.csh` (depending on your shell) > <sub>Activation needs to be done __each time__ we will run the program.</sub> > <sub>To check you are in the right env, type: `which pip` and you should see it's pointing inside the env folder</sub> ### 4 Install Stream2segment **Important reminders before installing**: - From now on you are supposed to be in the stream2segment directory, (where you cloned the repository) with your Python virtualenv activated - In case of errors, check the [Installation notes below](#installation-Notes) Install the required packages with the tested versions listed in `requirements.txt` (if you are working on an existing environment with stuff already installed in it, **please read the [first installation note](#installation-notes) below** before proceeding): ```console pip install --upgrade pip setuptools wheel && pip install -r ./requirements.txt ``` > <sub>type `requirements.dev.txt` instead of `requirements.txt` if you want to install also test packages, e.g., you want to contribute to the code and/or run tests</sub> Install this package: ```console pip install -e . ``` (optional) install jupyter notebook or jupyterlab (see [Jupyter page for details](https://jupyter.org/install)), e.g.: ```console pip install jupyterlab ``` The program is now installed. To double-check the program functionalities, you can run tests (see below) and report the problem in case of failure. In any case, before reporting a problem remember to check first the [Installation Notes](#installation-notes) ### 5 Installation Notes - in case of a message like `ERROR: No matching distribution found for <package_name>`, try to skip the requirements file: ```console pip install --upgrade pip setuptools wheel && pip install -e . ``` This will install packages satisfying a *minimum* required version instead of the *exact* version passing tests: while less safe in general, this approach lets `pip` handle the best versions to use, with more chance of success in this case. **You can choose this strategy not only in case of mismatching distributions, but also while working on a virtual environment with already installed packages, because it has less chance of breaking existing code.** - In older ObsPy version, numpy needs to be installed first. If you see an error like \"you need to install numpy first\", open \"requirements.txt\" and copy the line which starts with numpy. Supposing it's `numpy==0.1.12`, then run `pip install numpy==0.1.12` before re-running the `pip install ...` command above - When installing the program (`pip install -e .`), `-e` is optional and makes the package editable, meaning that you can edit the repository and make all changes immediately available, without re-installing the package. This is useful when, e.g., `git pull`-ing new versions frequently. - (update January 2021) On macOS (version 11.1, with Python 3.8 and 3.9): - if the installation fails with a lot of printout, and you spot a \"Failed building wheel for psycopg2\", see <!--, try to execute: ``` export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/opt/openssl/lib/ && pip ./installme-dev ``` (you might need to change the path of openssl below). Credits --> [here](https://stackoverflow.com/a/61159643/3526777) and [here](https://stackoverflow.com/a/39800677/3526777) - If the error message is \"Failed building wheel for tables\", then try to install c-blosc (on macOS, `brew install c-blosc`) <!-- and re-run `installme-dev` installation command (with the `export` command above, if needed) --> - If you see (we experienced this while running tests, thus we can guess you should see it whenever accessing the program for the first time): ``` This system supports the C.UTF-8 locale which is recommended. You might be able to resolve your issue by exporting the following environment variables: export LC_ALL=C.UTF-8 export LANG=C.UTF-8 ``` Then edit your `~/.profile` (or `~/.bash_profile` on Mac) and put the two lines starting with 'export', and execute `source ~/.profile` (`source ~/.bash_profile` on Mac) and re-execute the program. - On Ubuntu 12.10, there might be problems with libxml (`version libxml2_2.9.0' not found`). Move the file or create a link in the proper folder. The problem has been solved looking at http://phersung.blogspot.de/2013/06/how-to-compile-libxml2-for-lxml-python.html All following issues should be solved by installing all dependencies as described in the section [Prerequisites](#prerequisites). If you did not install them, here the solutions to common problems you might have and that we collected from several Ubuntu installations: - For numpy installation problems (such as `Cannot compile 'Python.h'`) , the fix has been to update gcc and install python3-dev (python2.7-dev if you are using Python2.7, discouraged): ``` sudo apt-get update sudo apt-get upgrade gcc sudo apt-get install python3-dev ``` For details see [here](http://stackoverflow.com/questions/18785063/install-numpy-in-python-virtualenv) - For scipy problems, `build-essential gfortran libatlas-base-dev` are required for scipy. For details see [here](http://stackoverflow.com/questions/2213551/installing-scipy-with-pip/3865521#3865521) - For lxml problems, `libxml2-dev libxslt-dev` are required. For details see [here](http://lxml.de/installation.html) - For matplotlib problems (matplotlib is not used by the program but from imported libraries), `libpng-dev libfreetype6-dev` are required. For details see [here](http://stackoverflow.com/questions/25593512/cant-install-matplotlib-using-pip) and [here]( http://stackoverflow.com/questions/28914202/pip-install-matplotlib-fails-cannot-build-package-freetype-python-setup-py-e) ## Development and Maintenance ### 1 Run tests Stream2segment has been highly tested (current test coverage is above 90%) on Python version >= 3.5+. Although automatic continuous integration (CI) systems are not in place, we do our best to regularly tests under new Python and package versions. Remember that tests are time-consuming (some minutes currently). Here some examples depending on your needs: ``` pytest -xvvv -W ignore ./tests/ ``` ``` pytest -xvvv -W ignore --dburl postgresql://<user>:<password>@localhost/<dbname> ./tests/ ``` <!-- ``` pytest -xvvv -W ignore --cov=./stream2segment --cov-report=html ./tests/ ``` --> ``` pytest -xvvv -W ignore --dburl postgresql://<user>:<password>@localhost/<dbname> --cov=./stream2segment --cov-report=html ./tests/ ``` Where the options denote: - `-x`: stop at first error - `-vvv`: increase verbosity, - `-W ignore`: do not print Python warnings issued during tests. You can omit the `-W` option to turn warnings on and inspect them, but consider that a lot of redundant messages will be printed: in case of test failure, it is hard to spot the relevant error message. Alternatively, try `-W once` - warn once per process - and `-W module` -warn once per calling module. - `--cov`: track code coverage, to know how much code has been executed during tests, and `--cov-report`: type of report (if html, you will have to opened 'index.html' in the project directory 'htmlcov') - `--dburl`: Additional database to use. The default database is an in-memory sqlite database (e.g., no file will be created), thus this option is basically for testing the program also on postgres. In the example, the postgres is installed locally (`localhost`) but it does not need to. *Remember that a database with name `<dbname>` must be created first in postgres, and that the data in any given postgres database will be overwritten if not empty* > <sub>Note on coding: although PEP8 recommends 79 character length, the program used initially a 100 characters max line width, which is being reverted to 79 (you might see mixed lengths in the modules). It seems that [among new features planned for Python 4 there is an increment to 89.5 characters](https://charlesleifer.com/blog/new-features-planned-for-python-4-0/). If true, we might stick to that in the future</sub> ### 2 Updating dependencies In the absence of Continuous Integration in place, from times to times, it is necessary to update the dependencies, to make `pip install` more likely to work (at least for some time). The procedure is: ``` pip install -e . pip freeze > ./requirements.tmp pip install -e \".[dev]\" pip freeze > ./requirements.dev.tmp ``` **Remember to comment the line of stream2segment from all requirements** (as it should be installed as argument of pip: `pip install <options> .`, and not inside the requirements file). Run tests (see above) with warnings on: fix what might go wrong, and eventually you can replace the old `requirements.txt` and `requirements.dev.txt` with the `.tmp` file created. ### 3 Updating wiki Requirements (to be done once): - `jupyter` installed. - The git repository `stream2segment.wiki` which you can clone from the stream2segment/wiki URL on the GitHub page. The repository must be cloned next to (on the same parent directory of) the stream2segment repository The wiki is simply a git project composed of Markdown (.md) files, where `Home.md` implements the landing page of the wiki on the browser, and thus usually hosts the table of contents with links to other markdown files `.md` in the directory. Currently, two of those `.md` files are generated from the notebooks `.ipynb` inside stream2segment: - ./resources/templates/ - Using-Stream2segment-in-your-Python-code.ipynb - The-Segment-object.ipynb #### 3.1 Update existing notebook 1. Edit the notebook in stream2segment/resources/templates: `jupyter notebook stream2segment/resources/templates` Execute the whole notebook to update it, then `git push` as usual 2. Create `.md` versions of the notebook for the wiki. From the stream2segment repository as `cwd`: ```bash F='Using-Stream2segment-in-your-Python-code';jupyter nbconvert --to markdown ./stream2segment/resources/templates/$F.ipynb --output-dir ../stream2segment.wiki ``` (repeat for every notebook file, e.g. `The-Segment-object`. Note only the file name, no file extension needed) 3. Commit and push to the `stream2segment.wiki` repo: `cd ../stream2segment.wiki`, then as usual `git commit` and `git push`. One line command: `(cd ../stream2segment.wiki && git commit -am 'updating wiki' && git push)` #### 3.2 Add a new notebook Create the notebook (`jupyter notebook stream2segment/resources/templates`). **Choose a meaningful file name: use upper case when needed, type hyphens '-' instead of spaces**: the file name will be used as title to show the page online (replacing hyphens with spaces). Once the notebook is created and executed: - (optional) If you want to include the notebook also as example in the `s2s init` command, look at `stream2segment/cli.py` - Make the notebook being executed during tests (see examples in `tests/misc/test_notebook.py`) and run tests to check everything works. - Make the notebook visible in the wiki by adding a reference to it (the notebook URL is the file name with no extension, I guess case- insensitive). A reference can be added in several places: - In the file `_Sidebar.md` (in the wiki repository) which will show it in the sidebar on GitHub - In `Home.md` - In some other notebook (see example in `Using-stream2segment-in-you-Python-code.ipynb`). In this case, note that you might need to update also the referencing notebook (see points 2-3 [above](#to-update-one-of-those-existing-notebooks)) - Create the markdown file and commit to the wiki (see points 2-3 above under `To update one of those existing notebooks`) ## Citation **Software:** > Zaccarelli, Riccardo (2018): Stream2segment: a tool to download, process and visualize event-based seismic waveform data. GFZ Data Services. [http://doi.org/10.5880/GFZ.2.4.2019.002](http://doi.org/10.5880/GFZ.2.4.2019.002) **Research article:** > Riccardo Zaccarelli, Dino Bindi, Angelo Strollo, Javier Quinteros and Fabrice Cotton. Stream2segment: An Open‐Source Tool for Downloading, Processing, and Visualizing Massive Event‐Based Seismic Waveform Datasets. *Seismological Research Letters* (2019). [https://doi.org/10.1785/0220180314](https://doi.org/10.1785/0220180314)\n",
                "dependencies": "blinker==1.6.2 blosc2==2.0.0 certifi==2023.5.7 charset-normalizer==3.1.0 click==8.1.3 contourpy==1.0.7 cycler==0.11.0 Cython==0.29.34 decorator==5.1.1 Flask==2.3.2 fonttools==4.39.4 greenlet==2.0.2 idna==3.4 itsdangerous==2.1.2 Jinja2==3.1.2 kiwisolver==1.4.4 lxml==4.9.2 MarkupSafe==2.1.2 matplotlib==3.7.1 msgpack==1.0.5 numexpr==2.8.4 numpy==1.24.3 obspy==1.4.0 packaging==23.1 pandas==2.0.1 Pillow==9.5.0 psutil==5.9.5 psycopg2==2.9.6 py-cpuinfo==9.0.0 pyparsing==3.0.9 python-dateutil==2.8.2 pytz==2023.3 PyYAML==6.0 requests==2.30.0 scipy==1.10.1 six==1.16.0 SQLAlchemy==2.0.13 -e git+https://github.com/rizac/stream2segment.git@ea3d7f4b7e4b89f9f58e93f375569272f50e8af2#egg=stream2segment tables==3.8.0 typing_extensions==4.5.0 tzdata==2023.3 urllib3==2.0.2 Werkzeug==2.3.4\n\"\"\"A setuptools based setup module. Taken from: https://github.com/pypa/sampleproject/blob/master/setup.py See also: http://python-packaging-user-guide.readthedocs.org/en/latest/distributing/ Additional links: https://packaging.python.org/en/latest/distributing.html https://github.com/pypa/sampleproject \"\"\" from __future__ import print_function # Always prefer setuptools over distutils from setuptools import setup, find_packages # To use a consistent encoding from codecs import open from os import path here = path.abspath(path.dirname(__file__)) # Get the long description from the README file with open(path.join(here, 'README.md'), encoding='utf-8') as f: long_description = f.read() # http://stackoverflow.com/questions/2058802/how-can-i-get-the-version-defined-in-setup-py-setuptools-in-my-package version = \"\" with open(path.join(here, 'stream2segment', 'resources', 'program_version')) as version_file: version = version_file.read().strip() setup( name='stream2segment', # Versions should comply with PEP440. For a discussion on single-sourcing # the version across setup.py and the project code, see # https://packaging.python.org/en/latest/single_source_version.html version=version, description='A python project to download, process and visualize ' 'event-based seismic waveforms', long_description=long_description, # The project's main homepage. url='https://github.com/rizac/stream2segment', # Author details author='riccardo zaccarelli', author_email='rizac@gfz-potsdam.de', # FIXME: what to provide? # Choose your license license='GNU', # See https://pypi.python.org/pypi?%3Aaction=list_classifiers classifiers=[ # How mature is this project? Common values are # 3 - Alpha # 4 - Beta # 5 - Production/Stable 'Development Status :: 4 - Beta', # Indicate who your project is intended for 'Intended Audience :: Science/Research', 'Topic :: Scientific/Engineering', # Pick your license as you wish (should match \"license\" above) 'License :: OSI Approved :: GNU License', # Specify the Python versions you support here. # 'Programming Language :: Python :: 3.5', # 'Programming Language :: Python :: 3.6', # 'Programming Language :: Python :: 3.7', 'Programming Language :: Python :: 3.8', 'Programming Language :: Python :: 3.9', 'Programming Language :: Python :: 3.10', 'Programming Language :: Python :: 3.11', ], # What does your project relate to? keywords='download seismic waveforms related to events', # You can just specify the packages manually here if your project is # simple. Or you can use find_packages(). packages=find_packages(exclude=['contrib', 'docs', 'tests', 'htmlcov']), # Alternatively, if you want to distribute just a my_module.py, uncomment # this: # py_modules=[\"my_module\"], # List run-time dependencies here. These will be installed by pip when # your project is installed. For info see: # https://packaging.python.org/en/latest/requirements.html install_requires=['PyYAML>=3.12', 'numpy>=1.13.1', 'tables>=3.5.2', 'pandas>=0.20.3', 'obspy>=1.0.3', 'Flask>=0.12.3', 'psycopg2>=2.7.3.1', 'psutil>=5.3.1', 'SQLAlchemy>=1.1.14', 'click>=6.7' ], # List additional groups of dependencies here (e.g. development # dependencies). You can install these using the following syntax, # for example: # $ pip install -e .[dev,test] (pip install -e \".[dev,test]\" in zsh) extras_require={ # use latest versions. Without boundaries 'dev': ['pep8>=1.7.0', 'pylint>=1.7.2', 'pytest>=3.2.2', 'pytest-cov>=2.5.1', 'pytest-mock>=1.6.2'], 'jupyter': ['jupyter>=1.0.0'] }, # If there are data files included in your packages that need to be # installed, specify them here. If using Python 2.6 or less, then these # have to be included in MANIFEST.in as well. # # package_data={ # 'sample': ['package_data.dat'], # }, # make the installation process copy also the package data (see MANIFEST.in) # for info see https://python-packaging.readthedocs.io/en/latest/non-code-files.html include_package_data=True, # Although 'package_data' is the preferred approach, in some case you may # need to place data files outside of your packages. See: # http://docs.python.org/3.4/distutils/setupscript.html#installing-additional-files # noqa # In this case, 'data_file' will be installed into '<sys.prefix>/my_data' # # data_files=[('my_data', ['data/data_file'])], # To provide executable scripts, use entry points in preference to the # \"scripts\" keyword. Entry points provide cross-platform support and allow # pip to create the appropriate form of executable for the target platform. entry_points={ 'console_scripts': [ 'stream2segment=stream2segment.cli:cli', 's2s=stream2segment.cli:cli', ], }, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/sumo",
            "repo_link": "https://github.com/eclipse-sumo/sumo",
            "content": {
                "codemeta": "",
                "readme": "<a href=\"https://sumo.dlr.de/docs\"><p align=\"center\"><img width=50% src=\"https://raw.githubusercontent.com/eclipse/sumo/main/docs/web/docs/images/sumo-logo.svg\"></p></a> Eclipse SUMO - Simulation of Urban MObility =========================================== [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.14796685.svg )](https://doi.org/10.5281/zenodo.14796685) [![Windows](https://github.com/eclipse-sumo/sumo/actions/workflows/windows.yml/badge.svg)](https://github.com/eclipse-sumo/sumo/actions/workflows/windows.yml) [![Linux](https://github.com/eclipse-sumo/sumo/actions/workflows/linux.yml/badge.svg)](https://github.com/eclipse-sumo/sumo/actions/workflows/linux.yml) [![macOS](https://github.com/eclipse-sumo/sumo/actions/workflows/macos.yml/badge.svg)](https://github.com/eclipse-sumo/sumo/actions/workflows/macos.yml) [![sonarcloud security](https://sonarcloud.io/api/project_badges/measure?project=org.eclipse.sumo&metric=security_rating)](https://sonarcloud.io/summary/overall?id=org.eclipse.sumo) [![Translation status](https://hosted.weblate.org/widgets/eclipse-sumo/-/svg-badge.svg)](https://hosted.weblate.org/engage/eclipse-sumo/) ![Repo Size](https://img.shields.io/github/repo-size/eclipse/sumo.svg) What is SUMO ------------ [\"Simulation of Urban MObility\" (SUMO)](https://sumo.dlr.de/) is an open source, highly portable, microscopic traffic simulation package designed to handle large road networks and different modes of transport. <p align=\"center\"><img width=70% src=\"https://raw.githubusercontent.com/eclipse/sumo/main/docs/web/docs/images/multiple-screenshots.png\"></p> It is mainly developed by employees of the [Institute of Transportation Systems at the German Aerospace Center](https://www.dlr.de/ts/en/). Where to get it --------------- You can download SUMO via our [downloads site](https://sumo.dlr.de/docs/Downloads.html). As the program is still under development (and is being extended continuously), we advice you to use the latest sources from our GitHub repository. Using a command line client, execute the following command: ``` git clone --recursive https://github.com/eclipse-sumo/sumo ``` Contact ------- To stay informed, we have a mailing list for SUMO, which [you can subscribe](https://dev.eclipse.org/mailman/listinfo/sumo-user) to. Messages to the list can be sent to sumo-user@eclipse.org. SUMO announcements will be made through the sumo-announce@eclipse.org list; [you can subscribe](https://dev.eclipse.org/mailman/listinfo/sumo-announce) to it as well. For further contact information, have a look at [this page](https://sumo.dlr.de/docs/Contact.html). Build and Installation ---------------------- For Windows we provide pre-compiled binaries and CMake files to generate Visual Studio projects. If you want to develop under Windows, please also clone the dependent libraries using: ``` git clone --recursive https://github.com/DLR-TS/SUMOLibraries ``` If you're using Linux, you should have a look whether your distribution already contains sumo. There is also a [ppa for ubuntu users](https://launchpad.net/~sumo) and an [open build service instance](https://build.opensuse.org/project/show/science:dlr). If you want to build SUMO yourself, the steps for ubuntu are: ``` cd <SUMO_DIR> # please insert the correct directory name here export SUMO_HOME=\"$PWD\" sudo apt-get install $(cat build_config/build_req_deb.txt build_config/tools_req_deb.txt) cmake -B build . cmake --build build -j$(nproc) ``` For [detailed build instructions, have a look at our Documentation](https://sumo.dlr.de/docs/Developer/Main.html#build_instructions). Getting started --------------- To get started with SUMO, take a look at the docs/tutorial and examples directories, which contain some example networks with routing data and configuration files. There is also user documentation provided in the docs/ directory and on the homepage. Documentation --------------- - The main documentation is at [sumo.dlr.de/docs](https://sumo.dlr.de/docs). Note that this tracks the [development version](https://sumo.dlr.de/docs/FAQ.html#why_does_sumo_not_behave_as_documented_in_this_wiki). - A mirror of the main documentation is at [sumo.sourceforge.net/docs](https://sumo.sourceforge.net/docs). - An offline version of the documentation is part of every release and can be accessed via `docs/userdoc/index.html`. Improving SUMO -------------- Please use the [GitHub issue tracking tool](https://github.com/eclipse-sumo/sumo/issues) for bugs and requests, or file them to the sumo-user@eclipse.org list. Before filing a bug, please consider to check with a current repository checkout whether the problem has already been fixed. We welcome patches, pull requests and other contributions! For details see [our contribution guidelines](CONTRIBUTING.md). We use [Weblate for translating SUMO](https://hosted.weblate.org/projects/eclipse-sumo/). If you want to add translation strings or a language, see [our contribution guidelines](CONTRIBUTING.md#translating) and [this page](https://sumo.dlr.de/docs/Developer/Translating.html) for more information. License ------- SUMO is licensed under the [Eclipse Public License Version 2](https://eclipse.org/legal/epl-v20.html). The licenses of the different libraries and supplementary code information are in the subdirectories and in the [Documentation](https://sumo.dlr.de/docs/Libraries_Licenses.html).\n",
                "dependencies": "# set VERSION to empty string cmake_policy(SET CMP0048 NEW) # do not expand quoted variables in if statements cmake_policy(SET CMP0054 NEW) # find python by path set(Python_FIND_STRATEGY LOCATION) # Options option(CHECK_OPTIONAL_LIBS \"Try to download / enable all optional libraries (use only EPL clean libraries, if set to false)\" ON) option(MULTITHREADED_BUILD \"Use all available cores for building (applies to Visual Studio only)\" ON) option(PROFILING \"Enable output of profiling data (applies to gcc/clang builds only)\") option(PPROF \"Link the pprof profiler library (applies to gcc/clang builds only)\") option(COVERAGE \"Enable output of coverage data (applies to gcc/clang builds only)\") option(SUMO_UTILS \"Enable generation of a shared library for the utility functions for option handling, XML parsing etc.\") option(FMI \"Enable generation of an FMI library for SUMO\" ON) option(NETEDIT \"Enable build of netedit\" ON) option(ENABLE_PYTHON_BINDINGS \"Build Python Bindings\" ON) option(ENABLE_JAVA_BINDINGS \"Build Java Bindings\" ON) option(ENABLE_CS_BINDINGS \"Build C# Bindings\" ON) option(CCACHE_SUPPORT \"Enable ccache support if installed\" ON) option(TCMALLOC \"Use tcmalloc if installed\" ON) option(VERBOSE_SUB \"Let sub-commands be more verbose\") set(BINARY_SUFFIX \"\" CACHE STRING \"Append the suffix to every generated binary\") set(COMPILE_DEFINITIONS \"\" CACHE STRING \"Macros or defines to add when compiling\") set(JUPEDSIM_CUSTOMDIR \"\" CACHE PATH \"Custom install location of jupedsim\") set(PROJECT_NAME \"SUMO\" CACHE STRING \"Project/Solution name\") # Set a default build type if none was specified # you may use -DCMAKE_BUILD_TYPE:STRING=Debug from the command line set(default_build_type \"Release\") set(CMAKE_ORIGINAL_BUILD_TYPE \"${CMAKE_BUILD_TYPE}\") if (NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES) message(STATUS \"Setting build type to '${default_build_type}' as none was specified.\") set(CMAKE_BUILD_TYPE \"${default_build_type}\" CACHE STRING \"Choose the type of build.\" FORCE) # Set the possible values of build type for cmake-gui set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"RelWithDebInfo\") endif () cmake_minimum_required(VERSION 3.5) project(\"${PROJECT_NAME}\" CXX C) set(PACKAGE_VERSION \"git\") # Check if libraries have to be found, depending on SUMO_LIBRARIES set(SUMO_LIBRARIES \"$ENV{SUMO_LIBRARIES}\" CACHE PATH \"Location of SUMOLibraries dependencies\") # some user place SUMOLibraries in the same SUMO folder if (NOT SUMO_LIBRARIES AND EXISTS \"${CMAKE_SOURCE_DIR}/../SUMOLibraries\") set(SUMO_LIBRARIES \"${CMAKE_SOURCE_DIR}/../SUMOLibraries\") endif () if (CCACHE_SUPPORT) if (MSVC) file(GLOB SCCACHE_ROOT \"${SUMO_LIBRARIES}/sccache-*\") find_program(SCCACHE_PROGRAM sccache ${SCCACHE_ROOT}) if (SCCACHE_PROGRAM) message(STATUS \"Found sccache: ${SCCACHE_PROGRAM}\") set(CMAKE_C_COMPILER_LAUNCHER \"${SCCACHE_PROGRAM}\") set(CMAKE_CXX_COMPILER_LAUNCHER \"${SCCACHE_PROGRAM}\") if (CMAKE_BUILD_TYPE STREQUAL \"Debug\") string(REPLACE \"/Zi\" \"/Z7\" CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG}\") string(REPLACE \"/Zi\" \"/Z7\" CMAKE_C_FLAGS_DEBUG \"${CMAKE_C_FLAGS_DEBUG}\") elseif (CMAKE_BUILD_TYPE STREQUAL \"RelWithDebInfo\") string(REPLACE \"/Zi\" \"/Z7\" CMAKE_CXX_FLAGS_RELWITHDEBINFO \"${CMAKE_CXX_FLAGS_RELWITHDEBINFO}\") string(REPLACE \"/Zi\" \"/Z7\" CMAKE_C_FLAGS_RELWITHDEBINFO \"${CMAKE_C_FLAGS_RELWITHDEBINFO}\") endif() endif() else() find_program(CCACHE_PROGRAM ccache) if (CCACHE_PROGRAM) message(STATUS \"Found ccache: ${CCACHE_PROGRAM}\") set(CMAKE_C_COMPILER_LAUNCHER \"${CCACHE_PROGRAM}\") set(CMAKE_CXX_COMPILER_LAUNCHER \"${CCACHE_PROGRAM}\") endif() endif() endif() set(CMAKE_COLOR_MAKEFILE ON) set(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} \"${CMAKE_SOURCE_DIR}/build_config/cmake_modules/\") set(ENABLED_FEATURES \"${CMAKE_SYSTEM} ${CMAKE_SYSTEM_PROCESSOR} ${CMAKE_CXX_COMPILER_ID} ${CMAKE_CXX_COMPILER_VERSION} ${CMAKE_BUILD_TYPE}\") if (COMPILE_DEFINITIONS) add_compile_definitions(${COMPILE_DEFINITIONS}) endif () # declare flags for compilers if (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\") set (GNU_COMPILER True) endif () if (\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\") set (CLANG_COMPILER True) endif () # C++14 is needed by Google Test >= 1.13, for all the other parts C++11 should be enough. # This will silently fall back to C++11 if 14 is not supported by the compiler. set(CMAKE_CXX_STANDARD 14) # compiler specific flags if (GNU_COMPILER OR CLANG_COMPILER) # set flags for clang in windows if (CLANG_COMPILER AND WIN32) # flags for clang in windows set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall -Wextra\") # disable debug build (due a problem with Runtime Libraries) set(CMAKE_CONFIGURATION_TYPES \"Release\") set(CMAKE_BUILD_TYPE \"Release\") else () # flags for clang and gcc in linux set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pthread -Wall -pedantic -Wextra\") endif () if (PROFILING) set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pg\") set(ENABLED_FEATURES \"${ENABLED_FEATURES} Profiling\") set(BINARY_SUFFIX \"${BINARY_SUFFIX}P\") endif () if (COVERAGE) set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} --coverage -O0\") set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} --coverage\") set(ENABLED_FEATURES \"${ENABLED_FEATURES} Coverage\") endif () elseif (MSVC) # enabling /WX is not possible due to warnings in external headers if (CMAKE_BUILD_TYPE STREQUAL \"Debug\") # this is a workaround to disable lots of warnings for replacing /W3 with /W4 in VS 2019 # there is a policy for that starting with CMake 3.15 if (MSVC_VERSION GREATER 1900) string(REPLACE \"/W3\" \"/Wall\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\") else () string(REPLACE \"/W3\" \"/W4\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\") endif () else () # /Wall brings MSVC 2013 to complete halt if (MSVC_VERSION GREATER 1900) set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} /Wall\") else () set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} /W4\") endif () endif () if (MULTITHREADED_BUILD) set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /MP\") endif () # SWIG generates large obj files set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /bigobj\") # backward compatible mutex API, see https://github.com/actions/runner-images/issues/10004 set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /D_DISABLE_CONSTEXPR_MUTEX_CONSTRUCTOR\") # exporting symbols for shared libraries needs to enabled explicitly set(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS ON) # add option for enabling console in release mode option(CONSOLE_RELEASE \"Enable console in SUMO-GUI and NETEDIT in release (use only for debug purposes)\" false) endif () # special debug flags set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -D_DEBUG\") if (CLANG_COMPILER) if (WIN32) set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -fsanitize=undefined,integer -fsanitize-blacklist=${CMAKE_SOURCE_DIR}/build_config/clang_sanitize_blacklist.txt\") add_compile_definitions(_ITERATOR_DEBUG_LEVEL=0) else () set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -fsanitize=undefined,address,integer -fno-omit-frame-pointer -fsanitize-blacklist=${CMAKE_SOURCE_DIR}/build_config/clang_sanitize_blacklist.txt\") endif () endif () # we need to build position independent code when generating a shared library set(CMAKE_POSITION_INDEPENDENT_CODE ON) if (SUMO_UTILS) set(ENABLED_FEATURES \"${ENABLED_FEATURES} SumoUtilsLibrary\") endif () if (FMI) set(ENABLED_FEATURES \"${ENABLED_FEATURES} FMI\") endif () set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_SOURCE_DIR}/bin) set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_SOURCE_DIR}/bin) # force Visual Studio to leave out the Release / Debug dirs set(CMAKE_RUNTIME_OUTPUT_DIRECTORY_DEBUG ${CMAKE_SOURCE_DIR}/bin) set(CMAKE_RUNTIME_OUTPUT_DIRECTORY_RELEASE ${CMAKE_SOURCE_DIR}/bin) set(CMAKE_LIBRARY_OUTPUT_DIRECTORY_DEBUG ${CMAKE_SOURCE_DIR}/bin) set(CMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE ${CMAKE_SOURCE_DIR}/bin) # Debug messages message(STATUS \"CMAKE_BINARY_DIR: \" ${CMAKE_BINARY_DIR}) message(STATUS \"CMAKE_SOURCE_DIR: \" ${CMAKE_SOURCE_DIR}) message(STATUS \"\") message(STATUS \"Platform: \") message(STATUS \" Host: \" ${CMAKE_HOST_SYSTEM} \" \" ${CMAKE_HOST_SYSTEM_PROCESSOR}) message(STATUS \" Target: \" ${CMAKE_SYSTEM} \" \" ${CMAKE_SYSTEM_PROCESSOR}) message(STATUS \" CMake: \" ${CMAKE_VERSION}) message(STATUS \" CMake generator: \" ${CMAKE_GENERATOR}) message(STATUS \" CMake build tool: \" ${CMAKE_BUILD_TOOL}) message(STATUS \" Compiler: \" ${CMAKE_CXX_COMPILER_ID} \" \" ${CMAKE_CXX_COMPILER_VERSION}) if (CMAKE_GENERATOR MATCHES Xcode) message(STATUS \" Xcode: \" ${XCODE_VERSION}) endif () message(STATUS \"\") if (SKBUILD OR ${CMAKE_VERSION} VERSION_LESS 3.12.0) find_package(PythonInterp REQUIRED) find_package(PythonLibs) else() find_package(Python REQUIRED COMPONENTS Interpreter Development) # define variables for compatibility. refactor once older cmake unsupported if (Python_FOUND) set(PYTHONLIBS_FOUND ${Python_FOUND}) set(PYTHON_EXECUTABLE ${Python_EXECUTABLE} CACHE FILEPATH \"Python executable\") set(PYTHON_LIBRARIES ${Python_LIBRARIES} CACHE FILEPATH \"Python libraries\") set(PYTHON_INCLUDE_DIRS ${Python_INCLUDE_DIRS} CACHE PATH \"Python include folder\") set(PYTHON_LIBRARY_DIRS ${Python_LIBRARY_DIRS}) endif() endif() message(STATUS \"Found Python: \" ${PYTHON_EXECUTABLE}) if (MSVC AND ENABLE_PYTHON_BINDINGS) # recheck that the platform of the generator and python matches execute_process(COMMAND ${PYTHON_EXECUTABLE} -c \"import sys; print(sys.maxsize > 2**32)\" OUTPUT_VARIABLE IS_PYTHON64 OUTPUT_STRIP_TRAILING_WHITESPACE) if (${CMAKE_MODULE_LINKER_FLAGS} STREQUAL \"/machine:x64\") if (${IS_PYTHON64} STREQUAL \"False\") message(WARNING \"Did not find Python 64 bit. Please set PYTHON_EXECUTABLE, PYTHON_INCLUDE_DIR and PYTHON_LIBRARY manually.\") set(PYTHON_INCLUDE_DIRS \"\") endif() else() if (${IS_PYTHON64} STREQUAL \"True\") message(WARNING \"Did not find Python 32 bit. Please set PYTHON_EXECUTABLE, PYTHON_INCLUDE_DIR and PYTHON_LIBRARY manually.\") set(PYTHON_INCLUDE_DIRS \"\") endif() endif() if (NOT PYTHON_DEBUG_LIBRARY AND \"${CMAKE_BUILD_TYPE}\" STREQUAL \"Debug\") message(WARNING \"Did not find Python debug library. Please reinstall your Python and enable the Python debug libraries in the installer.\") set(PYTHON_INCLUDE_DIRS \"\") endif() endif() execute_process(COMMAND ${PYTHON_EXECUTABLE} -c \"import setuptools\" RESULT_VARIABLE PYTHON_SETUPTOOLS_MISSING ERROR_QUIET) execute_process(COMMAND ${PYTHON_EXECUTABLE} -c \"import build; import pip; build.__version__\" RESULT_VARIABLE PYTHON_BUILD_MISSING ERROR_QUIET) # exclusive from Apple if (APPLE) # we know that openGL is deprecated for newer MacOS set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DGL_SILENCE_DEPRECATION\") # Xcode 15 on macOS sonoma complains about duplicate libraries for the linker, so we disable the warning for xcode >= 15 if (CLANG_COMPILER) string(REGEX MATCH \"^[0-9]+\" ClangMajorVersion \"${CMAKE_CXX_COMPILER_VERSION}\") if (ClangMajorVersion GREATER_EQUAL 15) set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -Wl,-no_warn_duplicate_libraries\") endif() endif() # Recommendation, also add a \"brew --prefix\" custom command to detect a homebrew build environment execute_process(COMMAND brew --prefix RESULT_VARIABLE DETECT_BREW OUTPUT_VARIABLE BREW_PREFIX ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE) if (${DETECT_BREW} EQUAL 0) link_directories(${BREW_PREFIX}/lib) include_directories(${BREW_PREFIX}/include) message(STATUS \"Brew detected: ${BREW_PREFIX}\") endif () # Detect if the \"port\" command is valid on this system; if so, return full path execute_process(COMMAND which port RESULT_VARIABLE DETECT_MACPORTS OUTPUT_VARIABLE MACPORTS_PREFIX ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE) if (${DETECT_MACPORTS} EQUAL 0) # MACPORTS_PREFIX contains now something like \"/opt/local/bin/port\", so we get the parent directory get_filename_component(MACPORTS_PREFIX ${MACPORTS_PREFIX} DIRECTORY) # \"/opt/local\" is where MacPorts lives, add `/lib` suffix and link # http://stackoverflow.com/questions/1487752/how-do-i-instruct-cmake-to-look-for-libraries-installed-by-macports link_directories(/opt/X11/lib ${MACPORTS_PREFIX}/../lib) include_directories(SYSTEM /opt/X11/include ${MACPORTS_PREFIX}/../include) message(STATUS \"Macports detected: ${MACPORTS_PREFIX}\") endif () endif (APPLE) # Specifically define variable WIN32 for compilations under windows (due an error in Shawn) if (GNU_COMPILER AND WIN32) option(USE_MINGW_64BITS \"Use 64 bits libraries for the compilation with MinGW\" true) set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DWIN32 -D_WIN32_WINNT=0x0501\") set(MINGW32 1) endif () if (NOT JUPEDSIM_CUSTOMDIR) # check if set JuPedSim directory from SUMOLibraries if (SUMO_LIBRARIES AND WIN32) file(GLOB JUPEDSIM_CUSTOMDIR_SUMOLIBRARIES \"${SUMO_LIBRARIES}/JuPedSim-*\") set(JUPEDSIM_CUSTOMDIR \"${JUPEDSIM_CUSTOMDIR_SUMOLIBRARIES}\") else () if (EXISTS \"${CMAKE_SOURCE_DIR}/../jupedsim-install\") set(JUPEDSIM_CUSTOMDIR \"${CMAKE_SOURCE_DIR}/../jupedsim-install\") endif () endif () if (JUPEDSIM_CUSTOMDIR) message(STATUS \"Using JuPedSim from ${JUPEDSIM_CUSTOMDIR}\") endif () endif () # check if SUMOLibraries was found (Only in Windows) if (SUMO_LIBRARIES AND WIN32) # set option for install debug and release runtime dlls option(INSTALL_DLL_RUNTIMES \"Copy debug and release runtimes for MSVC\" true) # Special option for MinGW32 message(STATUS \"Using 64 bit libraries from SUMO_LIBRARIES placed in \" ${SUMO_LIBRARIES}) file(GLOB XERCES_PATH \"${SUMO_LIBRARIES}/xerces-c-3.?.?\") file(GLOB ZLIB_PATH \"${SUMO_LIBRARIES}/3rdPartyLibs/zlib-?.?.*\") file(GLOB PROJ_PATH \"${SUMO_LIBRARIES}/proj-?.?.?\") file(GLOB FOX_PATH \"${SUMO_LIBRARIES}/fox-?.?.??\") file(GLOB FREETYPE_PATH \"${SUMO_LIBRARIES}/3rdPartyLibs/freetype-2.??.?\") file(GLOB EIGEN3_PATH \"${SUMO_LIBRARIES}/eigen-3.?.?\") file(GLOB GETTEXT_PATH \"${SUMO_LIBRARIES}/gettext-?.??\") # set all paths in prefix set(CMAKE_PREFIX_PATH \"${CMAKE_PREFIX_PATH};${EIGEN3_PATH};${XERCES_PATH};${ZLIB_PATH};${PROJ_PATH};${FOX_PATH};${FREETYPE_PATH};${GETTEXT_PATH};${GETTEXT_PATH}/tools/gettext\") # save in variable SUMO_LIBRARIES_DLL all paths to dll files file(GLOB SUMO_LIBRARIES_DLL \"${XERCES_PATH}/bin/*.dll\" \"${PROJ_PATH}/bin/*.dll\" \"${FOX_PATH}/bin/*.dll\" \"${ZLIB_PATH}/bin/*.dll\" \"${FREETYPE_PATH}/bin/*.dll\" \"${GETTEXT_PATH}/bin/*.dll\" \"${GDAL_PATH}/bin/*.dll\" \"${SUMO_LIBRARIES}/libspatialite-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/bzip2-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/geos-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/curl-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/libpng-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/libssh2-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/openssl-*/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/sqlite-*/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/tiff-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/libxml2-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/libiconv-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/openssl-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/sqlite-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/freexl-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/libexpat-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/librttopo-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/minizip-*/bin/*.dll\" \"${SUMO_LIBRARIES}/3rdPartyLibs/bzip2-*/bin/*.dll\" ) # check if MSVC dll runtimes have to be copied if (INSTALL_DLL_RUNTIMES) file(GLOB MSVC_DLLS \"${SUMO_LIBRARIES}/runtimes/*.dll\") list(APPEND SUMO_LIBRARIES_DLL ${MSVC_DLLS}) endif(INSTALL_DLL_RUNTIMES) if (MSVC_VERSION GREATER 1919) # fmt in SUMOLibraries only works with MSVC 2019 and later file(GLOB fmt_PATH \"${SUMO_LIBRARIES}/3rdPartyLibs/fmt-??.?.?\") endif () # declare flag for use google test set(USE_GOOGLETEST true) # check if use google test if (WIN32) if (GNU_COMPILER AND NOT USE_MINGW_64BITS) message(STATUS \"Disabled Google Test in Mingw32\") set(USE_GOOGLETEST false) elseif(CLANG_COMPILER) message(STATUS \"Disabled Google Test in Clang (Windows)\") set(USE_GOOGLETEST false) endif () endif() if (USE_GOOGLETEST) file(GLOB GTEST_ROOT \"${SUMO_LIBRARIES}/googletest-?.??.?\") message(STATUS \"Found Google test: \" ${GTEST_ROOT}) include_directories(\"${GTEST_ROOT}/include\") # set both google test library set(GTEST_BOTH_LIBRARIES \"${GTEST_ROOT}/lib/gtest.lib\" \"${GTEST_ROOT}/lib/gtest_main.lib\") # set google test DLLs file(GLOB GTEST_DLL \"${GTEST_ROOT}/bin/gtest*.dll\") set(GTEST_FOUND true) endif () file(GLOB SWIG_EXECUTABLE \"${SUMO_LIBRARIES}/swigwin-*/swig.exe\") file(GLOB MVN_EXECUTABLE \"${SUMO_LIBRARIES}/apache-maven-*/bin/mvn.cmd\") file(GLOB TEXTTEST_EXECUTABLE \"${SUMO_LIBRARIES}/TextTest-*/texttest.exe\") else () # for Linux and Mac only find_package(GTest) endif () find_package(XercesC REQUIRED) if (XercesC_FOUND) include_directories(SYSTEM ${XercesC_INCLUDE_DIRS}) endif (XercesC_FOUND) find_package(Proj) if (PROJ_FOUND) include_directories(SYSTEM ${PROJ_INCLUDE_DIR}) set(ENABLED_FEATURES \"${ENABLED_FEATURES} Proj\") endif (PROJ_FOUND) find_package(FOX) if (FOX_FOUND) set(OpenGL_GL_PREFERENCE LEGACY) find_package(OpenGL) if (OPENGL_FOUND AND OPENGL_GLU_FOUND) include_directories(SYSTEM ${FOX_INCLUDE_DIR}) add_definitions(${FOX_CXX_FLAGS}) add_definitions(-DFLOAT_MATH_FUNCTIONS) set(HAVE_FOX 1) set(ENABLED_FEATURES \"${ENABLED_FEATURES} GUI\") else () message(WARNING \"Fox is present but OpenGL or GLU cannot be found, GUI will be disabled.\") endif () endif (FOX_FOUND) find_package(Freetype) if (FREETYPE_FOUND) include_directories(SYSTEM ${FREETYPE_INCLUDE_DIRS}) else (FREETYPE_FOUND) message(WARNING \"FreeType cannot be found. If you are using SUMO libraries, update SUMO_LIBRARIES folder with git pull. Otherwise, specify manually FreeType include folder and libs.\") endif (FREETYPE_FOUND) if (SUMO_LIBRARIES AND WIN32) find_package(fmt PATHS ${fmt_PATH} NO_DEFAULT_PATH) if (fmt_FOUND) message(STATUS \"Found FMT (SUMOLibraries): \" ${fmt_DIR}) include_directories(SYSTEM \"${fmt_PATH}/include\") set(FMT_LIBRARY fmt::fmt) set(HAVE_FMT 1) set(ENABLED_FEATURES \"${ENABLED_FEATURES} FMT\") endif (fmt_FOUND) elseif (NOT MSVC) find_package(fmt QUIET) if (fmt_FOUND) set(HAVE_FMT 1) set(FMT_LIBRARY fmt::fmt) # next 2 lines could fix the include path for mambaforge fmt on DLR machines (maybe outdated cmake?) # get_target_property(fmt_INCLUDE_DIR fmt::fmt INTERFACE_INCLUDE_DIRECTORIES) # include_directories(SYSTEM ${fmt_INCLUDE_DIR}) set(ENABLED_FEATURES \"${ENABLED_FEATURES} FMT\") endif (fmt_FOUND) endif () find_package(X11) if (X11_FOUND) link_directories(${X11_LIBRARY_DIR}) include_directories(SYSTEM ${X11_INCLUDE_DIR}) endif (X11_FOUND) find_package(ZLIB) if (ZLIB_FOUND) set(HAVE_ZLIB 1) link_directories(${ZLIB_LIBRARY_DIR}) include_directories(SYSTEM ${ZLIB_INCLUDE_DIR}) endif () find_package(Intl) if (Intl_FOUND) set(HAVE_INTL 1) link_directories(${Intl_LIBRARY_DIR}) include_directories(SYSTEM ${Intl_INCLUDE_DIRS}) set(ENABLED_FEATURES \"${ENABLED_FEATURES} Intl\") endif () find_package(SWIG 3.0) if (SWIG_FOUND) set(ENABLED_FEATURES \"${ENABLED_FEATURES} SWIG\") endif () if (TCMALLOC) find_library(TCMALLOC_LIBRARY NAMES tcmalloc_minimal) if (TCMALLOC_LIBRARY) set(ENABLED_FEATURES \"${ENABLED_FEATURES} tcmalloc\") else () set(TCMALLOC_LIBRARY \"\") endif () endif () # Eigen (for overhead wire / electric circuit simulation) find_package(Eigen3 3.2) if (EIGEN3_FOUND) include_directories(SYSTEM ${EIGEN3_INCLUDE_DIR}) set(HAVE_EIGEN 1) # see config.h.cmake for #cmakedefine set(ENABLED_FEATURES \"${ENABLED_FEATURES} Eigen\") endif (EIGEN3_FOUND) if (CHECK_OPTIONAL_LIBS) file(GLOB GDAL_PATH \"${SUMO_LIBRARIES}/gdal-?.?.?\") file(GLOB FFMPEG_PATH \"${SUMO_LIBRARIES}/FFMPEG-?.?.?\") file(GLOB OSG_PATH \"${SUMO_LIBRARIES}/OSG-?.?.?\") file(GLOB GL2PS_PATH \"${SUMO_LIBRARIES}/gl2ps-?.?.?\") file(GLOB GEOS_PATH \"${SUMO_LIBRARIES}/3rdPartyLibs/geos-*\") set(CMAKE_PREFIX_PATH \"${CMAKE_PREFIX_PATH};${GDAL_PATH};${FFMPEG_PATH};${OSG_PATH};${GL2PS_PATH};${GEOS_PATH}\") file(GLOB SUMO_OPTIONAL_LIBRARIES_DLL \"${GDAL_PATH}/bin/*.dll\" \"${FFMPEG_PATH}/bin/*.dll\" \"${OSG_PATH}/bin/*.dll\" \"${GL2PS_PATH}/bin/*.dll\" \"${JUPEDSIM_CUSTOMDIR}/bin/*.dll\") # GDAL (for geopositioning) find_package(GDAL) if (GDAL_FOUND) include_directories(SYSTEM ${GDAL_INCLUDE_DIR}) set(HAVE_GDAL 1) set(ENABLED_FEATURES \"${ENABLED_FEATURES} GDAL\") else (GDAL_FOUND) set(GDAL_LIBRARY \"\") endif (GDAL_FOUND) if (FOX_FOUND) # FFMPEG (for recording videos) find_package(FFMPEG) if (FFMPEG_FOUND) include_directories(SYSTEM ${FFMPEG_INCLUDE_DIR}) set(HAVE_FFMPEG 1) set(ENABLED_FEATURES \"${ENABLED_FEATURES} FFmpeg\") endif () # OSG (For 3D view) find_package(OpenSceneGraph 3.4.0 COMPONENTS osgGA osgViewer osgUtil osgDB osgText) if (OPENSCENEGRAPH_FOUND) include_directories(SYSTEM ${OPENSCENEGRAPH_INCLUDE_DIRS}) set(HAVE_OSG 1) set(ENABLED_FEATURES \"${ENABLED_FEATURES} OSG\") endif () # GL2PS (deprecated, will be changed by FreeType) find_package(GL2PS) if (GL2PS_FOUND) include_directories(SYSTEM ${GL2PS_INCLUDE_DIR}) set(HAVE_GL2PS 1) set(ENABLED_FEATURES \"${ENABLED_FEATURES} GL2PS\") endif (GL2PS_FOUND) endif () find_package(jupedsim CONFIG HINTS ${JUPEDSIM_CUSTOMDIR} QUIET) if (jupedsim_FOUND) find_package(GEOS) if (GEOS_FOUND) include_directories(SYSTEM ${GEOS_INCLUDE_DIR}) set(JPS_VERSION \"${jupedsim_VERSION_MAJOR}${jupedsim_VERSION_MINOR}${jupedsim_VERSION_PATCH}\") set(ENABLED_FEATURES \"${ENABLED_FEATURES} JuPedSim\") message(STATUS \"Found JuPedSim: ${jupedsim_DIR} ${jupedsim_VERSION}\") else (GEOS_FOUND) message(STATUS \"Could NOT find GEOS! JuPedSim requires GEOS.\") endif (GEOS_FOUND) else (jupedsim_FOUND) message(STATUS \"Could NOT find JuPedSim! Skipping JuPedSim integration.\") endif (jupedsim_FOUND) # add optional libraries list(APPEND SUMO_LIBRARIES_DLL ${SUMO_OPTIONAL_LIBRARIES_DLL}) endif (CHECK_OPTIONAL_LIBS) include_directories(${CMAKE_CURRENT_BINARY_DIR}/src) include_directories(${CMAKE_CURRENT_SOURCE_DIR}/src) set(commonlibs utils_distribution utils_handlers utils_shapes utils_options utils_xml utils_geom utils_common utils_importio utils_iodevices utils_traction_wire foreign_tcpip ${XercesC_LIBRARIES} ${ZLIB_LIBRARIES} ${PROJ_LIBRARY} ${Intl_LIBRARIES} ${FMT_LIBRARY}) if (WIN32) set(commonlibs ${commonlibs} ws2_32) endif () if (PPROF) set(commonlibs ${commonlibs} profiler) endif () set(commonvehiclelibs utils_emissions foreign_phemlight foreign_phemlight_V5 utils_vehicle ${commonlibs} ${FOX_LIBRARY}) # set custom name and folder for ALL_BUILD and ZERO_CHECK in visual studio solutions set_property(GLOBAL PROPERTY USE_FOLDERS ON) set_property(GLOBAL PROPERTY PREDEFINED_TARGETS_FOLDER \"CMake\") # installation if (MSVC) if (CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT) set(CMAKE_INSTALL_PREFIX \"sumo-${PACKAGE_VERSION}\") endif() install(DIRECTORY bin/ DESTINATION bin FILES_MATCHING PATTERN \"*.bat\" PATTERN \"*.dll\" PATTERN \"*d.dll\" EXCLUDE PATTERN \"gtest*.dll\" EXCLUDE) else () include(GNUInstallDirs) endif () if (SKBUILD OR MSVC) set(DATA_PATH \"\") else () set(DATA_PATH \"share/sumo/\") endif () if (SKBUILD) set(EXCLUDE_LIBSUMO \"libsumo\") set(EXCLUDE_LIBTRACI \"libtraci\") endif () install(DIRECTORY data/ DESTINATION ${DATA_PATH}data) install(DIRECTORY tools/ DESTINATION ${DATA_PATH}tools USE_SOURCE_PERMISSIONS PATTERN \"build_config/sumo\" EXCLUDE # needed for the scikit build PATTERN \"calibration\" EXCLUDE PATTERN \"lisum-core\" EXCLUDE PATTERN \"lisum-gui\" EXCLUDE PATTERN \"sumolib4matlab/src\" EXCLUDE PATTERN \"${EXCLUDE_LIBSUMO}\" EXCLUDE PATTERN \"${EXCLUDE_LIBTRACI}\" EXCLUDE PATTERN \"traas\" EXCLUDE PATTERN \"traci4matlab/src\" EXCLUDE PATTERN \"__pycache__\" EXCLUDE PATTERN \"*.pyc\" EXCLUDE PATTERN \"*.egg-info\" EXCLUDE PATTERN \".git\" EXCLUDE) if (DATA_PATH) install(CODE \"execute_process(COMMAND ${CMAKE_COMMAND} -E create_symlink ../../bin \\$ENV{DESTDIR}${CMAKE_INSTALL_PREFIX}/${DATA_PATH}bin)\") endif () install(EXPORT SUMOConfig DESTINATION ${DATA_PATH}cmake NAMESPACE SUMO::) if (NOT ${PYTHON_SETUPTOOLS_MISSING}) set(TOOLS_DIR \"${CMAKE_SOURCE_DIR}/tools\") if (${PYTHON_BUILD_MISSING}) install(CODE \"execute_process(COMMAND ${PYTHON_EXECUTABLE} build_config/setup-sumolib.py clean --all install --root=\\$ENV{DESTDIR}/ --prefix=${CMAKE_INSTALL_PREFIX} --optimize=1 WORKING_DIRECTORY ${TOOLS_DIR})\" COMPONENT pysumolib) install(CODE \"execute_process(COMMAND ${PYTHON_EXECUTABLE} build_config/setup-traci.py clean --all install --root=\\$ENV{DESTDIR}/ --prefix=${CMAKE_INSTALL_PREFIX} --optimize=1 WORKING_DIRECTORY ${TOOLS_DIR})\" COMPONENT pytraci) else () if (NOT VERBOSE_SUB) set(PYTHON_BUILD_OPTS \"-C--quiet\") endif () install(CODE \"execute_process(COMMAND ${PYTHON_EXECUTABLE} ./build_config/version.py ./build_config/setup-sumolib.py ./setup.py WORKING_DIRECTORY ${TOOLS_DIR} COMMAND ${PYTHON_EXECUTABLE} -m build ${PYTHON_BUILD_OPTS} --wheel . -o ${CMAKE_BINARY_DIR})\" COMPONENT pysumolib) install(CODE \"execute_process(COMMAND ${PYTHON_EXECUTABLE} ./build_config/version.py ./build_config/setup-traci.py ./setup.py WORKING_DIRECTORY ${TOOLS_DIR} COMMAND ${PYTHON_EXECUTABLE} -m build ${PYTHON_BUILD_OPTS} --wheel . -o ${CMAKE_BINARY_DIR})\" COMPONENT pytraci) install(CODE \"execute_process(COMMAND ${PYTHON_EXECUTABLE} -m pip install --root=\\$ENV{DESTDIR}/ --prefix=${CMAKE_INSTALL_PREFIX} -f ${CMAKE_BINARY_DIR} --no-index traci)\" COMPONENT pytraci) endif () endif () if (SUMO_LIBRARIES AND WIN32) # filter release DLLs execute_process(COMMAND ${PYTHON_EXECUTABLE} ${CMAKE_SOURCE_DIR}/tools/build_config/filterDebugDLL.py ${SUMO_LIBRARIES_DLL} OUTPUT_VARIABLE SUMO_LIBRARIES_RELEASE_DLL) # copy debug dlls foreach(DLL_PATH ${SUMO_LIBRARIES_DLL} ${GTEST_DLL}) get_filename_component(DLL_BASENAME ${DLL_PATH} NAME) set(TARGET_DLL ${CMAKE_SOURCE_DIR}/bin/${DLL_BASENAME}) list(FIND TARGET_ALL_DLLS ${TARGET_DLL} DLL_KNOWN) list(FIND SUMO_LIBRARIES_RELEASE_DLL ${DLL_PATH} IS_RELEASE_DLL) list(FIND GTEST_DLL ${DLL_PATH} IS_TEST_DLL) if (DLL_KNOWN EQUAL -1) add_custom_command(OUTPUT ${TARGET_DLL} COMMAND ${CMAKE_COMMAND} -E copy ${DLL_PATH} ${TARGET_DLL} DEPENDS ${DLL_PATH}) if (IS_TEST_DLL GREATER -1) list(APPEND TARGET_TEST_DLLS ${TARGET_DLL}) elseif (IS_RELEASE_DLL GREATER -1) list(APPEND TARGET_ALL_DLLS ${TARGET_DLL}) else () list(APPEND TARGET_ALL_DLLS $<$<CONFIG:Debug>:${TARGET_DLL}>) endif () endif() endforeach(DLL_PATH) # copy OSG dlls if (HAVE_OSG) file(GLOB OSG_PLUGIN_DLL \"${OSG_PATH}/bin/osgPlugins*/osgdb_osg.dll\") get_filename_component(OSG_PLUGIN_DLL_BASENAME ${OSG_PLUGIN_DLL} NAME) get_filename_component(OSG_PLUGIN_DIR ${OSG_PLUGIN_DLL} DIRECTORY) get_filename_component(OSG_PLUGIN_DIR ${OSG_PLUGIN_DIR} NAME) set(OSG_TARGET ${CMAKE_SOURCE_DIR}/bin/${OSG_PLUGIN_DIR}/${OSG_PLUGIN_DLL_BASENAME}) add_custom_command(OUTPUT ${OSG_TARGET} COMMAND ${CMAKE_COMMAND} -E copy_directory ${OSG_PATH}/bin/${OSG_PLUGIN_DIR} ${CMAKE_SOURCE_DIR}/bin/${OSG_PLUGIN_DIR} DEPENDS ${OSG_PLUGIN_DLL}) endif (HAVE_OSG) # proj 7 needs to copy \"share/proj\" folder in SUMO_HOME/share set(PROJ_DATA ${CMAKE_SOURCE_DIR}/share/proj/proj.db) add_custom_command(OUTPUT ${PROJ_DATA} COMMAND ${CMAKE_COMMAND} -E copy_directory ${PROJ_PATH}/share/proj ${CMAKE_SOURCE_DIR}/share/proj DEPENDS ${PROJ_PATH}/share/proj/proj.db) install(DIRECTORY share/ DESTINATION ${DATA_PATH}share) endif() # set install dll targets, for non-Windows they depend on nothing add_custom_target(install_dll DEPENDS ${TARGET_ALL_DLLS} ${OSG_TARGET} ${PROJ_DATA}) add_custom_target(install_test_dll DEPENDS ${TARGET_TEST_DLLS}) set_property(TARGET install_dll PROPERTY FOLDER \"CMake\") set_property(TARGET install_test_dll PROPERTY FOLDER \"CMake\") # i18n target find_package(Gettext) if (Gettext_FOUND) set(I18N_DATA ${CMAKE_SOURCE_DIR}/data/locale/tr/LC_MESSAGES/sumo.mo) add_custom_command(OUTPUT ${I18N_DATA} COMMAND ${PYTHON_EXECUTABLE} ${CMAKE_SOURCE_DIR}/tools/build_config/i18n.py -m DEPENDS ${CMAKE_SOURCE_DIR}/data/po/tr_gui.po) else () message(WARNING \"Gettext tools not found, translation files will not be generated.\") endif() add_custom_target(install_mo DEPENDS ${I18N_DATA}) # java targets find_program(MVN_EXECUTABLE mvn) if (NOT VERBOSE_SUB) set(MVN_OPTS \"-q\") endif () find_package(Java COMPONENTS Development) if (MVN_EXECUTABLE AND Java_FOUND) if (NOT DEFINED ENV{JAVA_HOME}) get_filename_component(JAVA_BIN ${Java_JAVAC_EXECUTABLE} DIRECTORY) get_filename_component(JAVA_HOME ${JAVA_BIN} DIRECTORY) set(MVN_COMMAND_PREFIX ${CMAKE_COMMAND} -E env JAVA_HOME=${JAVA_HOME}) set(MVN_REPO \"-Dmaven.repo.local=${CMAKE_BINARY_DIR}/m2\") endif () add_custom_target(traas COMMAND ${MVN_COMMAND_PREFIX} ${MVN_EXECUTABLE} ${MVN_REPO} ${MVN_OPTS} --batch-mode -f tools/contributed/traas/pom.xml clean install COMMAND ${CMAKE_COMMAND} -E copy tools/contributed/traas/target/traas-1.1.jar bin/TraaS.jar WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}) add_custom_target(lisum COMMAND ${MVN_COMMAND_PREFIX} ${MVN_EXECUTABLE} ${MVN_REPO} ${MVN_OPTS} --batch-mode -f tools/contributed/lisum/pom.xml clean install COMMAND ${CMAKE_COMMAND} -E copy tools/contributed/lisum/lisum-core/target/lisum-core-1.0.2.jar bin/lisum-core.jar COMMAND ${CMAKE_COMMAND} -E copy tools/contributed/lisum/lisum-gui/target/lisum-gui-1.1.jar bin/lisum-gui.jar WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}) add_dependencies(lisum traas) set(JAVA_TARGETS traas lisum) foreach (JAVAT ${JAVA_TARGETS}) set_property(TARGET ${JAVAT} PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE) set_property(TARGET ${JAVAT} PROPERTY FOLDER \"java\") endforeach () endif () # doc targets add_custom_target(doxygen COMMAND rm -rf docs/doxygen COMMAND mkdir docs/doxygen COMMAND doxygen sumo.doxyconf > doxygen.log 2>&1 WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}) set_property(TARGET doxygen PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE) set_property(TARGET doxygen PROPERTY FOLDER \"doc\") add_custom_target(userdoc COMMAND ../../tools/build_config/buildPyDoc.py -p ../pydoc -c COMMAND mkdocs build -d ../userdoc WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}/docs/web) set_property(TARGET userdoc PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE) set_property(TARGET userdoc PROPERTY FOLDER \"doc\") if (MVN_EXECUTABLE) add_custom_target(javadoc COMMAND rm -rf docs/javadoc COMMAND mkdir docs/javadoc COMMAND ${MVN_EXECUTABLE} ${MVN_OPTS} --batch-mode -f tools/contributed/traas/pom.xml javadoc:javadoc COMMAND mv tools/contributed/traas/target/site/apidocs docs/javadoc/traas WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}) set_property(TARGET javadoc PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE) set_property(TARGET javadoc PROPERTY FOLDER \"doc\") set(JAVADOC_TARGET javadoc) endif () find_program(HELPMAN_EXECUTABLE help2man) if (HELPMAN_EXECUTABLE) add_custom_target(man COMMAND rm -rf docs/man COMMAND mkdir docs/man COMMAND ${HELPMAN_EXECUTABLE} -N -n \"A microscopic, multi-modal traffic simulation\" bin/sumo > docs/man/sumo.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"GUI version of the simulation SUMO\" bin/sumo-gui > docs/man/sumo-gui.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Builds vehicle routes for SUMO using detector values\" bin/dfrouter > docs/man/dfrouter.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Shortest path router and DUE computer for the microscopic traffic simulation SUMO\" bin/duarouter > docs/man/duarouter.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Router for the microscopic traffic simulation SUMO based on junction turning ratios\" bin/jtrrouter > docs/man/jtrrouter.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Import O/D-matrices and trips using macroscopic traffic assignment for SUMO\" bin/marouter > docs/man/marouter.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Generates routes of persons throughout a day for the microscopic traffic simulation SUMO\" bin/activitygen > docs/man/activitygen.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Importer of O/D-matrices for the traffic simulation SUMO\" bin/od2trips > docs/man/od2trips.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Road network importer / builder for the traffic simulation SUMO\" bin/netconvert > docs/man/netconvert.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Road network editor for the traffic simulation SUMO\" bin/netedit > docs/man/netedit.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Road network generator for the microscopic traffic simulation SUMO\" bin/netgenerate > docs/man/netgenerate.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Importer of polygons and POIs for the traffic simulation SUMO\" bin/polyconvert > docs/man/polyconvert.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"TraCITestClient for the traffic simulation SUMO\" bin/TraCITestClient > docs/man/TraCITestClient.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Builds and writes an emissions map for SUMO\\\\\\'s emission models\" bin/emissionsMap > docs/man/emissionsMap.1 COMMAND ${HELPMAN_EXECUTABLE} -N -n \"Computes emissions by driving a time line using SUMO\\\\\\'s emission models\" bin/emissionsDrivingCycle > docs/man/emissionsDrivingCycle.1 WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}) set_property(TARGET man PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE) set_property(TARGET man PROPERTY FOLDER \"doc\") set(MAN_TARGET man) endif () add_custom_target(doc) add_dependencies(doc doxygen userdoc ${JAVADOC_TARGET} ${MAN_TARGET}) set_property(TARGET doc PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE) set_property(TARGET doc PROPERTY FOLDER \"doc\") # coverage targets if (COVERAGE) find_program(LCOV_EXECUTABLE lcov) if (LCOV_EXECUTABLE) add_custom_target(lcov COMMAND rm -rf docs/lcov COMMAND mkdir docs/lcov COMMAND ${LCOV_EXECUTABLE} -d . --capture --output-file docs/lcov/lcov.info COMMAND ${LCOV_EXECUTABLE} --remove docs/lcov/lcov.info '/usr/*' --output-file docs/lcov/lcov.info COMMAND genhtml -o docs/lcov/html docs/lcov/lcov.info WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}) add_custom_target(lcov-reset COMMAND ${LCOV_EXECUTABLE} -d . -z WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}) else () message(WARNING \"COVERAGE is enabled but lcov was not found.\") endif () endif () # testing enable_testing() add_subdirectory(src) if (GTEST_FOUND) add_subdirectory(unittest) endif () if (TEXTTEST_EXECUTABLE AND EXISTS ${CMAKE_SOURCE_DIR}/tests/runCiTests.bat) add_test(NAME texttest COMMAND ${CMAKE_SOURCE_DIR}/tests/runCiTests.bat ${TEXTTEST_EXECUTABLE} $<CONFIG>) else () find_program(TEXTTEST_FOUND \"texttest\") if (TEXTTEST_FOUND AND EXISTS ${CMAKE_SOURCE_DIR}/tests/runTests.sh) add_test(texttest ${CMAKE_SOURCE_DIR}/tests/runTests.sh -b ci -v ci) endif() endif() # example and dist targets add_custom_target(examples COMMAND ${PYTHON_EXECUTABLE} tools/extractTest.py -x -f tests/examples.txt -p docs/examples/runAll.py WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}) add_dependencies(examples sumo netconvert dfrouter duarouter jtrrouter) set_property(TARGET examples PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE) set_property(TARGET examples PROPERTY FOLDER \"doc\") find_program(PYINSTALLER_FOUND \"pyinstaller\") if (PYINSTALLER_FOUND AND EXISTS ${CMAKE_SOURCE_DIR}/tools/game/runner.spec) add_custom_target(game COMMAND pyinstaller --noconfirm ${CMAKE_SOURCE_DIR}/tools/game/runner.spec ) endif() add_test(exampletest ${PYTHON_EXECUTABLE} ${CMAKE_SOURCE_DIR}/docs/examples/runAll.py) find_package(Git) if (GIT_FOUND) add_custom_target(dist COMMAND rm -rf sumo-${PACKAGE_VERSION} sumo-${PACKAGE_VERSION}.zip sumo-src-${PACKAGE_VERSION}.tar.gz sumo-src-${PACKAGE_VERSION}.zip sumo-gui-macos-${PACKAGE_VERSION}.zip COMMAND ${GIT_EXECUTABLE} archive --prefix sumo-${PACKAGE_VERSION}/ -o sumo-${PACKAGE_VERSION}.zip HEAD COMMAND unzip -q sumo-${PACKAGE_VERSION}.zip COMMAND rm -r sumo-${PACKAGE_VERSION}/tests COMMAND cp -a docs/tutorial docs/examples sumo-${PACKAGE_VERSION}/docs COMMAND find tools/contributed/saga/ tools/contributed/traci4matlab -type f | grep -v .git | xargs cp --parents --target-dir sumo-${PACKAGE_VERSION} COMMAND mkdir sumo-${PACKAGE_VERSION}/include COMMAND cp ${CMAKE_BINARY_DIR}/src/version.h sumo-${PACKAGE_VERSION}/include COMMAND zip -rq sumo-src-${PACKAGE_VERSION}.zip sumo-${PACKAGE_VERSION} COMMAND tar -czf sumo-src-${PACKAGE_VERSION}.tar.gz sumo-${PACKAGE_VERSION} COMMAND cp -a docs/userdoc docs/pydoc docs/javadoc docs/man sumo-${PACKAGE_VERSION}/docs COMMAND rm -r sumo-${PACKAGE_VERSION}/docs/web/docs/images COMMAND ln -s ../../userdoc/images sumo-${PACKAGE_VERSION}/docs/web/docs/images COMMAND tar -czf sumo_${PACKAGE_VERSION}.orig.tar.gz --exclude \"*.jar\" sumo-${PACKAGE_VERSION} COMMAND rm -rf sumo-${PACKAGE_VERSION} sumo-${PACKAGE_VERSION}.zip WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}) add_dependencies(dist examples doc ${JAVA_TARGETS}) set_property(TARGET dist PROPERTY FOLDER \"CMake\") set_property(TARGET dist PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE) add_custom_target(distcheck COMMAND rm -rf sumo-${PACKAGE_VERSION} COMMAND unzip -q sumo-src-${PACKAGE_VERSION}.zip COMMAND cd sumo-${PACKAGE_VERSION} && mkdir _cmake_build _cmake_install && cd _cmake_build && cmake -DCMAKE_INSTALL_PREFIX=../_cmake_install -DCHECK_OPTIONAL_LIBS=${CHECK_OPTIONAL_LIBS} .. || (echo \"ERROR: the cmake configuration failed.\" && false) && make -j8 || (echo \"ERROR: the compilation failed.\" && false) && make test || (echo \"ERROR: the test suite failed.\" && false) && make install || (echo \"ERROR: the install target failed.\" && false) COMMAND rm -rf sumo-${PACKAGE_VERSION} WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}) add_dependencies(distcheck dist) set_property(TARGET distcheck PROPERTY FOLDER \"CMake\") set_property(TARGET distcheck PROPERTY EXCLUDE_FROM_DEFAULT_BUILD TRUE) endif () message(STATUS \"Enabled features: ${ENABLED_FEATURES}\")\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/supervillain",
            "repo_link": "https://github.com/evanberkowitz/supervillain",
            "content": {
                "codemeta": "",
                "readme": "# supervillain [![DOI](https://zenodo.org/badge/679369801.svg)](https://zenodo.org/badge/latestdoi/679369801) Supervillain is a python package for studying the Villain model. # Installation + Development Navigate to the cloned repo and try ``` pip install . # for development use pip install -e . ./test/end-to-end.py ``` If pip installation succeeds so too should the example script, which by default samples the (φ, n) formulation of the model on a small lattice. supervillain has documentation built with [sphinx](https://www.sphinx-doc.org/en/master/). To build the documentation once you ``` sphinx-build . _build ``` and then can open `_build/index.html` in a browser. If you are developing you can replace `sphinx-build` with [`sphinx-autobuild`](https://pypi.org/project/sphinx-autobuild/) to get live updates to the documentation as you go.\n",
                "dependencies": "[project] name = \"supervillain\" description = \"Quantum monte carlo for the Villain model\" authors = [ ] dynamic = [\"dependencies\", \"version\"] [options] install_requires = [ ] [tool.setuptools] py-modules = [ \"supervillain\", ] [tool.setuptools.dynamic] dependencies = { file = [\"requirements.txt\"] } version = { attr = \"supervillain.meta.version\" }\n# MATH numpy>=1.21 scipy>=1.8 numba>=0.60 pandas>=2.1 # IO h5py>=3.7 tqdm>=4.64 # TESTING pytest >= 8.0.0 pytest-cov >= 4.0.0 # DOCS sphinx sphinx-git==11.0.0 sphinx-rtd-theme==1.1.0 sphinxcontrib-bibtex==2.5.0 sphinx-math-dollar sphinx_toolbox sphinx-favicon pybtex pybtex-docutils nbstripout # PLOTTING matplotlib>=3.5.2\n#!/usr/bin/env python import setuptools if __name__ == \"__main__\": setuptools.setup()\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/swh-client",
            "repo_link": "https://github.com/Ramy-Badr-Ahmed/php-swh-client",
            "content": {
                "codemeta": "{\"@context\":\"https://doi.org/10.5063/schema/codemeta-2.0\",\"@type\":\"SoftwareSourceCode\",\"datePublished\":\"2023-10-06\",\"dateCreated\":\"2023-01-31\",\"description\":\" Software-Heritage API-Client \",\"name\":\"swh-client\",\"license\":\"https://spdx.org/licenses/Apache-2.0.html\",\"dateModified\":\"2024-05-06\",\"softwareVersion\":\"1.0-Beta\",\"codeRepository\":\"https://github.com/Ramy-Badr-Ahmed/swh-client\",\"identifier\":\"https://archive.softwareheritage.org/swh:1:dir:40caa1be82f300bda332b03e69aa4591e3eb2235;origin=https://github.com/Ramy-Badr-Ahmed/swh-client;visit=swh:1:snp:0940a0d4cac24a623a199e8bfa115a54dabf0b70;anchor=swh:1:rev:749a9a916ada5ddf01465773be8862489bc9328a\",\"developmentStatus\":\"active\",\"readme\":\"https://github.com/Ramy-Badr-Ahmed/swh-client/blob/main/README.md\",\"programmingLanguage\":\"PHP\",\"operatingSystem\":\"cross-platform\",\"runtimePlatform\":\"PHP Interpreter\",\"relatedLink\":\"https://1959e979-c58a-4d3c-86bb-09ec2dfcec8a.ka.bw-cloud-instance.org\",\"author\":[{\"@type\":\"Person\",\"givenName\":\"Ramy-Badr\",\"familyName\":\"Ahmed\",\"email\":\"126559907+Ramy-Badr-Ahmed@users.noreply.github.com\"}],\"keywords\":[\"software-heritage\",\"swhid\",\"software-heritage-api-client\",\"swh-connector\",\"swh-client\",\"software-heritage-dag\",\"swh-metadata\",\"swh-webclient\",\"swh-graph\",\"swh-model\",\"swh-endpoints\",\"swh-api\",\"swh-archiver\"],\"softwareRequirements\":[\"PHP >=8.0\",\"guzzlehttp/guzzle: >=7.2\"],\"maintainer\":{\"@type\":\"Person\",\"givenName\":\"Ramy-Badr\",\"familyName\":\"Ahmed\",\"email\":\"126559907+Ramy-Badr-Ahmed@users.noreply.github.com\"},\"applicationCategory\":[\"API Connectors\",\"Research Software\",\"Metadata\"],\"funder\":{\"@type\":\"Organization\",\"funding\":\"EU-Horizon: 101057264\",\"@id\":\"https://doi.org/10.3030/101057264\",\"name\":\"European Comission\"}}\n",
                "readme": "![GitHub top language](https://img.shields.io/github/languages/top/Ramy-Badr-Ahmed/swh-client) ![GitHub](https://img.shields.io/github/license/Ramy-Badr-Ahmed/swh-client) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.12808864.svg)](https://doi.org/10.5281/zenodo.12808864) [![SWH](https://archive.softwareheritage.org/badge/swh:1:dir:ce683dcced024cb3af1db3b01bbe86f2a9b08028/)](https://archive.softwareheritage.org/swh:1:dir:ce683dcced024cb3af1db3b01bbe86f2a9b08028;origin=https://github.com/Ramy-Badr-Ahmed/swh-client;visit=swh:1:snp:63102a06d859d7d3bcccf1bfe5ade84d8e54e2d5;anchor=swh:1:rev:fb18ecd48c6d62947316845716fc578030ccf749) # SWH API Client This is a PHP API client/connector for [Software Heritage (SWH) web API](https://archive.softwareheritage.org/api/) - currently in Beta phase. The client is wrapped round the [`Illuminate Http package`](https://packagist.org/packages/illuminate/http) and the [`GuzzleHTTP`](https://docs.guzzlephp.org/en/stable/index.html) library. >[!Note] > _**Detailed documentation**_ can be found in the [wiki pages](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki) of this very repository. > > A demonstrable version (some features) can be accessed here: <a href=\"https://1959e979-c58a-4d3c-86bb-09ec2dfcec8a.ka.bw-cloud-instance.org/\" target=\"_blank\">**Demo Version**</a> >> Working on new features and fixes will be gladly considered. Please feel free to report. ## Installation Steps: 1) Clone this project. 2) Open a console session and navigate to the cloned directory: Run \"composer install\" This should involve installing the PHP REPL, PsySH 3) (Optional) Acquire SWH tokens for increased SWH-API Rate-Limits. 4) Prepare .env file and add tokens: 4.1) Rename/Copy the cloned \".env.example\" file to .env cp .env.example .env 4.2) (Optional) Edit these two token keys: SWH_TOKEN_PROD=Your_TOKEN_FROM_SWH_ACCOUNT # step 3) SWH_TOKEN_STAGING=Your_STAGING_TOKEN_FROM_SWH_ACCOUNT # step 3) 5) (optional) Add psysh to PATH. ## Quickstart: In a console session inside the cloned directory, start the php REPL: ```php $ psysh // if not added to PATH replace with: vendor/bin/psysh Psy Shell v0.12.0 (PHP 8.2.0 -- cli) by Justin Hileman ``` This will open a REPL console-based session where one can test the functionality of the api classes and their methods before building a suitable workflow/use-cases. ### Presets As a one-time configuration parameter, you can set the desired returned data type by SWH (default JSON): ```php > namespace Module\\HTTPConnector; > use Module\\HTTPConnector; > HTTPClient::setOptions(responseType:'object') // json/collect/object available ``` > * More details on the default configs: [Default Configurations](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#default-configurations) > * More details on further options set: [Preset Configurations](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki). ### Visits Retrieve Latest Full Visit in the SWH archive: ```php > namespace Module\\OriginVisits; > use Module\\OriginVisits; > $visitObject = new SwhVisits('https://github.com/torvalds/linux/'); > $visitObject->getVisit('latest', requireSnapshot: true) ``` > More details on further swh visits methods: [SwhVisits](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#ii-swhvisits). ### DAG Model: As graph Nodes, retrieve node Contents, Edges or find a Path to other nodes (top-bottom): ```php > namespace Module\\DAGModel; > use Module\\DAGModel; > $snpNode = new GraphNode('swh:1:snp:bcfd516ef0e188d20056c77b8577577ac3ca6e58') > $snpNode->nodeHopp() // node contents > $snpNode->nodeEdges() // node edges keyed by the respective name > $revNode = new GraphNode('swh:1:rev:9cf5bf02b583b93aa0d149cac1aa06ee4a4f655c') > $revNode->nodeTraversal('deps/nghttp2/lib/includes/nghttp2/nghttp2ver.h.in') // traverse to a deeply nested file ``` More details on: > * General [Node Methods](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#iii-graphnode). > * The Graph methods: > * [Graph contents](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#iv-graphhopping) > * [Graph edges](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#v-graphedges) > * [Graph paths](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#vi-graphtraversal) ### Archive You can specify repositories URL w/o paths and archive to SWH using one of the two variants (`static/non-static methods`): ```php > namespace Module\\Archival; > use Module\\Archival; > $saveRequest = new Archive('https://github.com/torvalds/linux/') // Example 1 > $saveRequest->save2Swh() > $newSaveRequest = Archive::repository('https://github.com/hylang/hy/tree/stable/hy/core') // Example 2 // in both cases: the returned POST response contains the save request id and date ``` Enquire about archival status using the id/date of the archival request (available in the initial POST response) ```php > $saveRequest->getArchivalStatus($saveRequestDateOrID) // current status is returned > $saveRequest->trackArchivalStatus($saveRequestDateOrID) // tracks until archival has succeeded ``` > More details on further archive methods: [Archive](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#vii-archive). ### EBNF Grammar Validate a given swhID. `TypeError` is thrown for non-valid swhIDs. ```php > namespace Module\\DataType; > use Module\\DataType; $snpID = new SwhcoreId('swh:1:snp:bcfd516ef0e188d20056c77b8577577ac3ca6e5Z') // throws TypeError Exception ``` > Full details of the SWHID persistent Identifiers: [Syntax](https://docs.softwareheritage.org/devel/swh-model/persistent-identifiers.html#syntax) >[!Note] > Todo: Core identifiers with qualifiers. ### MetaData Returns a list of metadata authorities that provided metadata on the given target ```php > namespace Module\\MetaData; > use Module\\MetaData; > SwhMetaData::getOriginMetaData('https://github.com/torvalds/linux/') ``` > More details on further metadata methods: [Metadata](https://github.com/Ramy-Badr-Ahmed/swh-client/wiki#viii-metadata).\n",
                "dependencies": "{\"name\":\"swh/client\",\"type\":\"project\",\"description\":\"SWH connector and client that interacts with SWH data model. Part of the FAIRCORE4EOSC project for LZI-Dagstuhl\",\"keywords\":[\"software-heritage\",\"archive\",\"metadata\"],\"license\":\"MIT\",\"require\":{\"php\":\"^8.2\",\"psy/psysh\":\"^0.12.0\",\"illuminate/support\":\"^10.47\",\"illuminate/validation\":\"^10.43\",\"guzzlehttp/guzzle\":\"^7.8\",\"php-ds/php-ds\":\"^1.5\",\"illuminate/http\":\"^10.47\",\"illuminate/log\":\"^10.47\"},\"autoload\":{\"psr-4\":{\"Module\\\\\":\"Module/\"}},\"minimum-stability\":\"dev\",\"prefer-stable\":true}\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/t8code",
            "repo_link": "https://github.com/DLR-AMR/t8code",
            "content": {
                "codemeta": "",
                "readme": ":rotating_light: **Become part of our Conference on Adaptive Mesh Refinement and Applications (AMR25) on September 02-04, 2025!** The abstract submission deadline is **April 15, 2025**. Visit the [AMR25 website](https://dlr.de/AMR25) for further information. :rotating_light: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7034838.svg)](https://doi.org/10.5281/zenodo.7034838) [![t8code CI](https://github.com/DLR-AMR/t8code/actions/workflows/tests_cmake_testsuite.yml/badge.svg)](https://github.com/DLR-AMR/t8code/actions/workflows/tests_cmake_testsuite.yml) <p align=\"center\"> <img width=\"300px\" src=t8code_logo.png> </p> ### Introduction t8code (spoken as \"tetcode\") is a C/C++ library to manage parallel adaptive meshes with various element types. t8code uses a collection (a forest) of multiple connected adaptive space-trees in parallel and scales to at least one million MPI ranks and over 1 Trillion mesh elements. It is licensed under the GNU General Public License 2.0 or later. Copyright (c) 2015 the developers. t8code is intended to be used as a thirdparty library for numerical simulation codes or any other applications that require meshes. <table> <tr> <td><img src=\"https://github.com/DLR-AMR/t8code/blob/main/doc/pictures/cmesh_tet_holes.png?raw=true\" height=\"200\" /></td> <td><img src=\"https://github.com/DLR-AMR/t8code/blob/main/doc/pictures/flow_around_circle_sim2.jpg?raw=true\" height=\"181\" /></td> </tr> <tr> <td><img src=\"https://github.com/DLR-AMR/t8code/blob/main/doc/pictures/mesh_3d_hybrid_cutout.jpg?raw=true\" height=\"200\" /></td> <td><img src=\"https://github.com/DLR-AMR/t8code/blob/main/doc/pictures/AirplaneWithTetMesh.png?raw=true\" height=\"200\" /></td> </tr> </table> t8code, or T8 for short, supports the following element types (also different types in the same mesh): - 0D: vertices - 1D: lines - 2D: quadrilaterals and triangles - 3D: hexahedra, tetrahedra, prisms and pyramids Among others, t8code offers the following functionalities: - Create distributed adaptive meshes over complex domain geometries - Adapt meshes according to user given refinement/coarsening criteria - Establish a 2:1 balance - (Re-)partition a mesh (and associated data) among MPI ranks - Manage ghost (halo) elements and data - Hierarchical search in the mesh - Curved mesh elements t8code uses space-filling curves (SFCs) to manage the adaptive refinement and efficiently store the mesh elements and associated data. A modular approach makes it possible to exchange the underlying SFC without changing the high-level algorithms. Thus, we can use and compare different refinement schemes and users can implement their own refinement rules if so desired. Currently t8code offers the following implementations by default: - lines use a 1D Morton curve with 1:2 refinement - quadrilateral/hexahedral elements are inherited from the p4est submodule, using the Morton curve 1:4, 1:8 refinement; - triangular/tetrahedral are implemented using the Tetrahedral Morton curve, 1:4, 1:8 refinement; - prisms are implemented using the triangular TM curve and a line curve, 1:8 refinement. - pyramids are implemented using the Pyramidal Morton curve and the TM curve for its tetrahedral children, 1:10 (for pyramids) / 1:8 (for tetrahedra) refinement. - The code supports hybrid meshes including any of the above element types (of the same dimension). You find more information on t8code in the [t8code Wiki](https://github.com/DLR-AMR/t8code/wiki). For a brief introduction in AMR and the algorithms used by t8code we recommend to read our [overview paper](https://elib.dlr.de/194377/1/t8code_overview_IMR2023.pdf). ### Setup We provide a short guide to install t8code in our Wiki [Installation guide](https://github.com/DLR-AMR/t8code/wiki/Installation). ### Getting started To get familiar with t8code and its algorithms and data structures we recommend executing the tutorial examples in `tutorials` and read the corresponding Wiki pages starting with [Step 0 - Helloworld](https://github.com/DLR-AMR/t8code/wiki/Step-0---Hello-World). A sophisticated example of a complete numerical simulation is our finite volume solver of the advection equation in `example/advection`. ### Documentation t8code uses [Doxygen](https://doxygen.nl/) to generate the code documentation. You can find the documentation of our releases on the [t8code website](https://dlr-amr.github.io/t8code/pages/documentation.html). Follow the steps described in our Wiki [Documentation](https://github.com/DLR-AMR/t8code/wiki/Documentation) to create the documentation locally. ### License and contributing t8code is licensed under GPLv2 (see [COPYING](COPYING)). We appreciate contributions from the community and refer to [CONTRIBUTING.md](CONTRIBUTING.md) for more details. Note that we strive to be a friendly, inclusive open-source community and ask all members of our community to adhere to our [`CODE_OF_CONDUCT.md`](CODE_OF_CONDUCT.md). To get in touch, [open an issue](https://github.com/DLR-AMR/t8code/issues/new) or write an email to one of the principal developers. ### Julia wrapper We offer [T8code.jl](https://github.com/DLR-AMR/T8code.jl) - an official [Julia](https://julialang.org/) package allowing to call t8code routines from the [Julia](https://julialang.org/) programming language. From within a Julia session do ```julia julia> import Pkg; Pkg.add([\"T8code\", \"MPI\"]) ``` to install the package on your system. ### Publications An (incomplete) list of publications related to t8code: [1] **Overview Paper**: Holke, Johannes and Burstedde, Carsten and Knapp, David and Dreyer, Lukas and Elsweijer, Sandro and Ünlü, Veli and Markert, Johannes and Lilikakis, Ioannis and Böing, Niklas and Ponnusamy, Prasanna and Basermann, Achim (2023) *t8code v. 1.0 - Modular Adaptive Mesh Refinement in the Exascale Era*. SIAM International Meshing Round Table 2023, 06.03.2023 - 09.03.2023, Amsterdam, Niederlande. [Full text available](https://elib.dlr.de/194377/1/t8code_overview_IMR2023.pdf) [2] **Original PhD thesis**: Holke, Johannes *Scalable algorithms for parallel tree-based adaptive mesh refinement with general element types*, PhD thesis at University of Bonn, 2018, [Full text available](https://bonndoc.ulb.uni-bonn.de/xmlui/handle/20.500.11811/7661) [3] **Tetrahedral and triangular Space-filling curve**: Burstedde, Carsten and Holke, Johannes *A Tetrahedral Space-Filling Curve for Nonconforming Adaptive Meshes*, SIAM Journal on Scientific Computing, 2016, [10.1137/15M1040049](https://epubs.siam.org/doi/10.1137/15M1040049) [4] **Coarse mesh partitioning**: Burstedde, Carsten and Holke, Johannes *Coarse mesh partitioning for tree-based AMR*, SIAM Journal on Scientific Computing, 2017, [10.1137/16M1103518](https://epubs.siam.org/doi/10.1137/16M1103518) [5] **Ghost computation**: Holke, Johannes and Knapp, David and Burstedde, Carsten *An Optimized, Parallel Computation of the Ghost Layer for Adaptive Hybrid Forest Meshes*, SIAM Journal on Scientific Computing, 2021, [10.1137/20M1383033](https://epubs.siam.org/doi/abs/10.1137/20M1383033) [6] **Geometry controlled refinement for hexahedra**: Elsweijer, Sandro and Holke, Johannes and Kleinert, Jan and Reith, Dirk (2022) *Constructing a Volume Geometry Map for Hexahedra with Curved Boundary Geometries*. In: SIAM International Meshing Roundtable Workshop 2022. SIAM International Meshing Roundtable Workshop 2022, 22. - 25. Feb. 2022, [Full text available](https://elib.dlr.de/186570/1/ConstructingAVolumeGeometryMapForHexahedraWithCurvedBoundaryGeometries.pdf) [7] **JOSS entry**: Holke, Johannes and Markert, Johannes, et. al. (2025) *t8code - modular adaptive mesh refinement in the exascale era*. In: Journal of Open Source Software, [Full text available](https://www.theoj.org/joss-papers/joss.06887/10.21105.joss.06887.pdf) ### Theses with t8code relations An (incomplete) list of theses written with or about t8code: [A] **Prism space-filling curve**: Knapp, David (2017) *Adaptive Verfeinerung von Prismen*. Bachelor's thesis, Rheinische Friedrich-Wilhems-Universität Bonn. [B] **Pyramidal space-filling curve**: Knapp, David (2020) *A space-filling curve for pyramidal adaptive mesh refinement*. Master's thesis, Rheinische Friedrich-Wilhems-Universität Bonn. [Full text available](https://www.researchgate.net/publication/346789160_A_space-filling_curve_for_pyramidal_adaptive_mesh_refinement) [C] **DG solver based on t8code**: Dreyer, Lukas (2021) *The local discontinuous galerkin method for the advection-diffusion equation on adaptive meshes*. Master's thesis, Rheinische Friedrich-Wilhems-Universität Bonn. [Full text available](https://elib.dlr.de/143969/1/masterthesis_dreyer.pdf) [D] **Geometry controlled refinement for hexahedra (Part 1)**: Elsweijer, Sandro (2021) *Curved Domain Adaptive Mesh Refinement with Hexahedra*. Tech report, Hochschule Bonn-Rhein-Sieg. [Full text available](https://elib.dlr.de/186571/1/masterprojekt-2_elsweijer_ABGABEVERSION_TITEL.pdf) [E] **Subelement and resolving hanging faces in 2D**: Becker, Florian (2021) *Removing hanging faces from tree-based adaptive meshes for numerical simulation*, Master's thesis, Universität zu Köln. [Full text available](https://elib.dlr.de/187499/1/RemovingHangingFacesFromTreeBasedAMR.pdf) [F] **Coarsening as post-processing to reduce simulation file size**: Spataro, Luca (2021) *Lossy data compression for atmospheric chemistry using adaptive mesh coarsening*. Master's thesis, Technische Universität München. [Full text available](https://elib.dlr.de/144997/1/master-thesis-final-spataro.pdf) [G] **Geometry controlled refinement for hexahedra (Part 2)**: Elsweijer, Sandro (2022) *Evaluation and generic application scenarios for curved hexahedral adaptive mesh refinement*. Master's thesis, Hochschule Bonn-Rhein-Sieg. [10.13140/RG.2.2.34714.11203](<https://doi.org/10.13140/RG.2.2.34714.11203>) [Full text available](https://elib.dlr.de/186561/1/sandro_elsweijer-evaluation_and_generic_application_scenarios_for_curved_hexahedral_adaptive_mesh_refinement.pdf) [H] **Multigrid and other preconditioners for DG**: Böing, Niklas (2022) *Evaluation of preconditioners for implicit solvers of local DG for the advection-diffusion equation* (*Untersuchung von Präkonditionierern für implizite Löser für das Local DG-Verfahren zur Lösung der Advektions-Diffusionsgleichung*). Master's thesis, Universität zu Köln. [Full text available](https://elib.dlr.de/186347/1/Untersuchung%20von%20Pr%C3%A4konditionierern%20f%C3%BCr%20implizite%20L%C3%B6ser%20f%C3%BCr%20das%20Local%20DG-Verfahren%20zur%20L%C3%B6sung%20der%20Advektions-Diffusionsgleichung.pdf) [I] **Removing elements from the mesh (cutting holes)**: Lilikakis, Ioannis (2022) *Algorithms for tree-based adaptive meshes with incomplete trees*. Master's thesis, Universität zu Köln. [Full text may be available in future](https://elib.dlr.de/191968/) [J] **Curved tetrahedra**: Fussbroich Jakob (2023) *Towards high-order, hybrid adaptive mesh refinement: Implementation and evaluation of curved unstructured mesh elements*. Master's thesis. Technische Hochschule Köln. [Full text available](https://elib.dlr.de/200442/) [K] **Hanging node resolution 3D**: Tabea Leistikow (2024) *Derivation and implementation of a hanging nodes resolution scheme for hexahedral non-conforming meshes in t8code*. Master's thesis, Universität zu Köln. Full text currently not available. ### Citing t8code If you use t8code in any of your publications, please cite the [github repository](https://doi.org/10.5281/zenodo.7034838), [1] and [2]. For publications specifically related to - **the tetrahedral index**, please cite [3]. - **coarse mesh partitioning**, please cite [4]. - **construction and handling of the ghost layer**, please cite [5]. - **geometry controlled refinement**, please cite [6] (general) and [J] (tetrahedral). - **hanging node resolution and/or subelements**, please cite [E] and [K]. If you use any functionality described in the theses, we encourage you to cite them as well.\n",
                "dependencies": "cmake_minimum_required( VERSION 3.16 ) include( cmake/GitProjectVersion.cmake ) project( T8CODE DESCRIPTION \"Parallel algorithms and data structures for tree-based AMR with arbitrary element shapes.\" LANGUAGES C CXX VERSION \"${T8CODE_VERSION_MAJOR}.${T8CODE_VERSION_MINOR}.${T8CODE_VERSION_PATCH}\" ) include( CTest ) include( CMakeDependentOption ) option( T8CODE_BUILD_AS_SHARED_LIBRARY \"Whether t8code should be built as a shared or a static library\" ON ) option( T8CODE_BUILD_PEDANTIC \"Compile t8code with `-pedantic` as done in the Github CI.\" OFF ) option( T8CODE_BUILD_WALL \"Compile t8code with `-Wall` as done in the Github CI.\" OFF ) option( T8CODE_BUILD_WERROR \"Compile t8code with `-Werror` as done in the Github CI.\" OFF ) option( T8CODE_BUILD_WEXTRA \"Compile t8code with extra warnings as done in the Github CI.\" OFF ) option( T8CODE_EXPORT_COMPILE_COMMANDS \"Export the compile commands as json. Can be used by IDEs for code completion (e.g. intellisense, clangd)\" OFF ) option( T8CODE_BUILD_TESTS \"Build t8code's automated tests\" ON ) cmake_dependent_option( T8CODE_BUILD_TPL_TESTS \"Build the tests from libsc and p4est\" OFF \"T8CODE_BUILD_TESTS\" OFF ) option( T8CODE_BUILD_EXAMPLES \"Build t8code's examples\" ON ) cmake_dependent_option( T8CODE_BUILD_TPL_EXAMPLES \"Build the examples from libsc and p4est\" OFF \"T8CODE_BUILD_EXAMPLES\" OFF ) option( T8CODE_BUILD_TUTORIALS \"Build t8code's tutorials\" ON ) option( T8CODE_BUILD_BENCHMARKS \"Build t8code's benchmarks\" ON ) option( T8CODE_BUILD_FORTRAN_INTERFACE \"Build t8code's Fortran interface\" OFF ) option( T8CODE_ENABLE_MPI \"Enable t8code's features which rely on MPI\" ON ) option( T8CODE_ENABLE_VTK \"Enable t8code's features which rely on VTK\" OFF ) option( T8CODE_ENABLE_OCC \"Enable t8code's features which rely on OpenCASCADE\" OFF ) option( T8CODE_ENABLE_NETCDF \"Enable t8code's features which rely on netCDF\" OFF ) option( T8CODE_USE_SYSTEM_SC \"Use system-installed sc library\" OFF ) option( T8CODE_USE_SYSTEM_P4EST \"Use system-installed p4est library\" OFF ) option( T8CODE_BUILD_DOCUMENTATION \"Build t8code's documentation\" OFF ) cmake_dependent_option( T8CODE_BUILD_DOCUMENTATION_SPHINX \"Build t8code's documentation using sphinx\" OFF \"T8CODE_BUILD_DOCUMENTATION\" OFF ) set(T8CODE_TEST_LEVEL 0 CACHE STRING \"Test level: 0 for full tests, 1 for less thorough tests, 2 for minimal tests. (WARNING: Use with care.)\") add_definitions(-DT8CODE_TEST_LEVEL=${T8CODE_TEST_LEVEL}) set(T8CODE_CUSTOM_PARALLEL_TEST_COMMAND \"\" CACHE STRING \"Define a custom command for parallel tests , e.g.: mpirun -np 8 (overwrites standard mpirun -np 4 if build with mpi)\") set(T8CODE_CUSTOM_SERIAL_TEST_COMMAND \"\" CACHE STRING \"Define a custom command for serial tests.\") # Set a default build type if none was specified set(default_build_type \"Release\") if(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES) message(STATUS \"Setting build type to '${default_build_type}' as none was specified.\") set(CMAKE_BUILD_TYPE \"${default_build_type}\" CACHE STRING \"Choose the type of build. Build types available: Release Debug RelWithDebInfo\" FORCE) # Set the possible values of build type for cmake-gui set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"RelWithDebInfo\") endif() set(CMAKE_MODULE_PATH \"${PROJECT_SOURCE_DIR}/cmake\" ${CMAKE_MODULE_PATH}) if( T8CODE_BUILD_FORTRAN_INTERFACE ) enable_language( Fortran ) endif() if( T8CODE_ENABLE_MPI ) if( T8CODE_BUILD_FORTRAN_INTERFACE ) find_package( MPI COMPONENTS C Fortran REQUIRED ) else() find_package( MPI COMPONENTS C REQUIRED ) endif() if( NOT MPIEXEC_EXECUTABLE ) message( FATAL_ERROR \"MPIEXEC was not found\" ) endif() set( SC_ENABLE_MPI ON ) endif() if( T8CODE_ENABLE_VTK ) find_package( VTK REQUIRED COMPONENTS IOXML CommonExecutionModel CommonDataModel IOGeometry IOXMLParser IOParallelXML IOPLY ParallelMPI FiltersCore vtksys CommonCore zlib IOLegacy) if(VTK_FOUND) message(\"Found VTK\") endif (VTK_FOUND) endif( T8CODE_ENABLE_VTK ) if( T8CODE_ENABLE_OCC ) find_package( OpenCASCADE REQUIRED COMPONENTS TKBO TKPrim TKTopAlgo TKGeomAlgo TKBRep TKG3d TKG2d TKMath TKernel ) if(OpenCASCADE_FOUND) message(\"Found OpenCASCADE\") endif (OpenCASCADE_FOUND) endif( T8CODE_ENABLE_OCC ) if( T8CODE_ENABLE_NETCDF ) find_package( netCDF REQUIRED ) if(netCDF_FOUND) message(\"Found netCDF\") include(cmake/CheckNetCDFPar.cmake) endif (netCDF_FOUND) endif( T8CODE_ENABLE_NETCDF ) # Override default for this libsc option set( BUILD_SHARED_LIBS ON CACHE BOOL \"Build libsc as a shared library\" ) # Prevent `libsc` and `p4est` from overwriting the default install prefix. set(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT FALSE) # Rpath options necessary for shared library install to work correctly in user projects. set(CMAKE_INSTALL_NAME_DIR ${CMAKE_INSTALL_PREFIX}/lib) set(CMAKE_INSTALL_RPATH ${CMAKE_INSTALL_PREFIX}/lib) set(CMAKE_INSTALL_RPATH_USE_LINK_PATH true) if ( T8CODE_USE_SYSTEM_SC ) find_package( SC REQUIRED PATHS /path/to/system/sc ) else() set( SC_BUILD_EXAMPLES ${T8CODE_BUILD_TPL_EXAMPLES} ) set( SC_BUILD_TESTING ${T8CODE_BUILD_TPL_TESTS} ) # Capture the list of variables before adding the subdirectory to mark the added ones as advanced. get_cmake_property(_vars_before_sc VARIABLES ) add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/sc ) # Capture the list of variables after adding the subdirectory get_cmake_property(_vars_after_sc VARIABLES ) # Compute the difference (new variables added by the subdirectory) set( _new_sc_vars) foreach( _var IN LISTS _vars_after_sc ) if( NOT _var IN_LIST _vars_before_sc ) list( APPEND _new_sc_vars ${_var} ) endif() endforeach() # Mark the new variables as advanced mark_as_advanced( FORCE ${_new_sc_vars} ) endif() if ( T8CODE_USE_SYSTEM_P4EST ) find_package( P4EST REQUIRED PATHS /path/to/system/p4est ) else() set( P4EST_BUILD_EXAMPLES ${T8CODE_BUILD_TPL_EXAMPLES} ) set( P4EST_BUILD_TESTING ${T8CODE_BUILD_TPL_TESTS} ) # Capture the list of variables before adding the subdirectory to mark the added ones as advanced. get_cmake_property( _vars_before_p4est VARIABLES ) add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/p4est ) # Capture the list of variables after adding the subdirectory get_cmake_property( _vars_after_p4est VARIABLES ) # Compute the difference (new variables added by the subdirectory) set( _new_p4est_vars ) foreach( _var IN LISTS _vars_after_p4est ) if( NOT _var IN_LIST _vars_before_p4est ) list( APPEND _new_p4est_vars ${_var} ) endif() endforeach() # Mark the new variables as advanced mark_as_advanced( FORCE ${_new_p4est_vars} ) endif() add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/src ) if ( T8CODE_BUILD_TESTS ) add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/test ) endif() if ( T8CODE_BUILD_TUTORIALS ) add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/tutorials ) endif() if ( T8CODE_BUILD_EXAMPLES ) add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/example ) endif() if ( T8CODE_BUILD_BENCHMARKS ) add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/benchmarks ) endif() if ( T8CODE_BUILD_DOCUMENTATION ) add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/doc ) endif() if( T8CODE_BUILD_FORTRAN_INTERFACE ) add_subdirectory( ${CMAKE_CURRENT_LIST_DIR}/api/t8_fortran_interface ) if( NOT T8CODE_ENABLE_MPI ) message( FATAL_ERROR \"Fortran API only available when MPI is enabled.\" ) endif() endif() include (cmake/CPackConfig.cmake)\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tamarin-prover",
            "repo_link": "https://github.com/tamarin-prover/tamarin-prover",
            "content": {
                "codemeta": "",
                "readme": "The Tamarin prover repository ============================= [![master branch build-status](https://travis-ci.org/tamarin-prover/tamarin-prover.svg?branch=develop)](https://travis-ci.org/tamarin-prover/tamarin-prover) This README describes the organization of the repository of the Tamarin prover for security protocol verification. Its intended audience are interested users and future developers of the Tamarin prover. For installation and usage instructions of the Tamarin prover see chapter 2 of the manual: https://tamarin-prover.github.io/manual/master/book/002_installation.html Developing and contributing --------------------------- See [contributing instructions](CONTRIBUTING.md) for instructions on how to develop, test and release changes to the Tamarin prover source code. Version Numbering Policy ----------------------- We use version numbers with four components. - The first component is the major version number. It indicates complete rewrites of the codebase. - The second component is the minor version number. We use odd minor version numbers to denote development releases intended for early adopters. We use even minor version numbers to denote public releases, which are also published. - The third component indicates bugfix releases. - The fourth component indicates documentation and meta-data changes. We ensure that the external interface of a version of the Tamarin prover is backwards compatible with the external interface of all versions that agree on the major and minor version number. We announce all releases of the Tamarin prover on: http://tamarin-prover.github.io Manual ------ The manual is available as PDF or HTML at https://tamarin-prover.github.io/manual/index.html Experimental improved graph output ---------------------------------- You can use our experimental improved graph output which may be helpful for very large graphs that can be created for complicated protocols. To enable this feature read the instructions about [improved graphs](/misc/cleandot/README.md). Spthy code editors ------------------ The project contains support for spthy syntax highlighting and support in the [etc](/etc/) directory. This includes support for [Sublime Text](/etc/SUBLIME_TEXT.md), [VIM](/etc/spthy.vim) and [Notepad++](/etc/notepad_plus_plus_spthy.xml). External tools ------------------ External tools may use the [Tree-sitter](https://tree-sitter.github.io/tree-sitter/) grammar in the [tree-sitter/](/tree-sitter/) directory. Example Protocol Models ----------------------- All example protocol models are found in the directory ./examples/ All models that we consider stable are part of every installation of the Tamarin prover. See `tamarin-prover.cabal` for the list of installed protocols. We use the following sub-directories to organize the models. ~~~~ accountability/ case studies using the accountability implementation presented in the \"Verifying Accountability for Unbounded Sets of Participants\" paper csf12/ the AKE case studies from our CSF'12 paper. classic/ classic security protocols like the ones from [SPORE](http://www.lsv.ens-cachan.fr/Software/spore/table.html) loops/ experiments for testing loop-invariants and protocols with non-monotonic state related_work/ examples from related work on protocols with loops or non-monotonic state experiments/ all other experiments ake/ more AKE examples including ID-based and tripartite group KE protocols based on bilinear pairing features/ (small) models that demonstrate a given feature ccs15/ the observational equivalence case studies from our CCS'15 paper csf-18/ the XOR case studies from the CSF'18 paper ~~~~ Feel free to add more sub-directories and describe them here. In general, we try use descriptive names for files containing the models. We also document all our findings as comments in the protocol model. Moreover, we use the following header in all files to make their context more explicit. ~~~~ /* Protocol: Example Modeler: Simon Meier, Benedikt Schmidt Date: January 2012 Status: working Description of protocol. */ ~~~~\n",
                "dependencies": "flags: {} packages: - '.' - lib/theory/ - lib/term/ - lib/utils/ - lib/sapic/ - lib/export/ - lib/accountability/ resolver: lts-22.39 ghc-options: \"$everything\": -Wall nix: packages: [ zlib ]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tbt-segmentation",
            "repo_link": "https://github.com/DLR-FT/TBT-Segmentation",
            "content": {
                "codemeta": "",
                "readme": "<!-- SPDX-FileCopyrightText: 2023 German Aerospace Center (DLR) SPDX-License-Identifier: CC-BY-NC-ND-4.0 --> [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13807484.svg)](https://doi.org/10.5281/zenodo.13807484) > This tool is based on the paper [Temporal Behavior Trees: Robustness and Segmentation](https://doi.org/10.1145/3641513.3650180) and its companion poster [Temporal Behavior Trees - Segmentation](https://doi.org/10.1145/3641513.3652534), which were published at [HSCC'24](https://hscc.acm.org/2024/). > If you encounter any issues, have questions, or need assistance, feel free to reach out: sebastian dot schirmer at dlr dot de ## Table of Content - [Temporal Behavior Trees: Robustness and Segmentation](#temporal-behavior-trees-robustness-and-segmentation) - [Getting Started](#getting-started) - [Folder Structure](#folder-structure) - [Brief Summary of the Supported Operators](#brief-summary-of-the-supported-operators) - [Docker Environment](#docker-environment) - [How to Interpret the Output Format](#how-to-interpret-the-output-format) - [Contributors](#contributors) - [Contributing](#contributing) - [Changes](#changes) - [License](#license) # Temporal Behavior Trees: Robustness and Segmentation Temporal Behavior Trees (TBT) are a specification formalism for monitoring behaviors. They are inspired by behavior trees that are commonly used to program robotic applications, but allow to specify temporal properties in their leaf nodes. Therefore, they can be easily retrofitted to existing behavior trees. For instance, consider the following behavior tree that specifies the landing sequence of an unmanned aircraft (1) *move to position*, (2) *stay in position*, (3) *move to touchdown*, and (4) *descend*: <p align=\"center\"> <img src=\"https://github.com/DLR-FT/TBT-Segmentation/blob/main/figs/BehaviorTree.JPG\" width=\"400\"> </p> Given such a TBT specification and a trace, i.e., a sequence of events of a system, we can compute the corresponding robustness. Robustness provides an quantitative interpretation *how* much the TBT specification was satisfied or violated. Further, we can use a TBT specification to segment a trace. That means that we assign portions of the provided specification to segments of the given trace. Such a segmentation then helps to better explain which portions of the specification were satisfied or violated. It is also useful to visualize the resulting segmentation, as shown below for the landing maneuver: <p align=\"center\"> <img src=\"https://github.com/DLR-FT/TBT-Segmentation/blob/main/figs/Segmentation.png\" width=\"400\"> </p> ## Getting Started Requires Rust to compile source code and Python for visualization. 1. [Install Rust](https://www.rust-lang.org/) 1. Specify a TBT, e.g., as done [here](specification/shiplanding_formula.tbt). The grammar can be found [here](src/tbt.pest) 1. [Provide a Trace by implementing ``get_trace``](src/user_defined/get_trace.rs) 2. [Replace the ``user_defined``-function by your own (Line 5)](src/lib.rs) 3. Call ``cargo build`` or ``cargo build --release`` 4. Call ``cargo run -- --help`` to get help on the command-line-usage 5. Call ``cargo test`` to see if the tests are successful For instance: ``cargo run --release -- -u -s specification/shiplanding_formula_combined.tbt -f ./res/logs_wind_front_Lateral/`` runs segmentation using subsampling on a provided logfile. For this example, ``get_trace`` is already provided. Using the [visualization script](scripts/visualize_ship_landing.py), we can easily plot a segmentation by, e.g., ``python visualize_ship_landing.py plot -b Lateral -s 5000 10000 20000 -e 0 -l ../res/logs_wind_front_Lateral/`` where ``5000, 10000, 20000`` represent beginning of segments (omitting 0), ``-b`` states the expected behavior and is used to plot the dotted lines, and ``-e`` represents the number of skipped entries due to subsampling. There is also the option to save a plot to inspect it in a docker environment using ``-p``. We can also replay the flight by, e.g., ``python visualize_ship_landing.py live -l ../res/logs_wind_front_Lateral/ -b Lateral -f 0.005 0.1 2.0``. For more information call ``python visualize_ship_landing.py --help``. > Using ``--toml <FILE>`` as command-line-argument generates a .toml-file for the computed segmentations. ## Folder Structure - [figs](figs) are resources used for this readme document - [res](res) contains the logfiles used in the HSCC paper - The logfolder name specifies the wind direction (*front* or *side*) and the anticipated maneuver (*45Deg*, *Lateral*, *Oblique*, or *Straight*) - Each flight consists of two csv-files: one for the ship and one for the aircraft - The files contain the position, the velocity, and the angles for the ship and the aircraft - [scripts](scripts) provides auxiliary resources - [Makefile](scripts/Makefile) is used for Rust profiling - [clean.sh](scripts/clean.sh) is used for cleaning up the repository - [infer_parameters_visualization.py](scripts/infer_parameters_visualization.py) is used for the [run.sh](scripts/run.sh) script to extract the segments from the produced output files - [run.sh](scripts/run.sh) is a script that executes our segmentation tool on all the available [logfiles](res) and produces png files to further analyze ([infer_parameters_visualization.py](scripts/infer_parameters_visualization.py)) - [visualize_ship_landing.py](scripts/visualize_ship_landing.py) is a script that is used to produced the png files that show the flight and the computed segments - [specification](specification) contains some example specifications - [src](src) contains the source code - [lib.rs](src/lib.rs) provides a package of TBTs functions that can be used by others - It requires a user to provide two functions [get_trace()](src/lib.rs) and [get_events_per_second()](src/lib.rs) - This repository provides these functions for the ship landing: [user_defined/](src/user_defined/) - [main.rs](src/main.rs) is an example that uses [lib.rs](src/lib.rs) and the user-defined functions [tree/](src/tree/) - [stl.rs](src/stl.rs) provides the syntax and semantics for STL formulas - [behaviortree.rs](src/behaviortree.rs) provides the syntax and semantics for TBTs - [command_line_parser.rs](src/command_line_parser.rs) is used to interface with the command line - [functions.rs](src/functions.rs) is used to represent atomic functions that are used by the TBT parser - [parser.rs](src/parser.rs) is a TBT parser that reads a .tbt-file and produces a ```Tree``` - [tbt.pest](src/tbt.pest) represents the grammar used by the TBT parser - [toml_out.rs](src/toml_out.rs) is used to produce a .toml-file - [csv_reader.rs](src/csv_reader.rs) represent auxiliary functions such as reading a csv-file - [table.rs](src/table.rs) represents the main data structure for the dynamic programming - [user_defined/](src/user_defined/) is an example implementation for the *UserProvidedFunctions* required by [lib.rs](src/lib.rs) - [get_trace.rs](src/user_defined/get_trace.rs) reads the trace data used in the experiment (ie it reads a csv-file and provides a ``Trace``) - [tests](tests/) contains multiple test cases that can be executed to test whether the compilation works - [Dockerfile](Dockerfile) just c/p the whole repository and builds it to produce a docker container that then can run [run.sh](scripts/run.sh) to procude the HSCC artifacts > To use the TBT tool for a different use-case, a user needs to provide the *UserProvidedFunction* ([get_trace()](src/lib.rs) and [get_tree()](src/lib.rs)) similar to what has been done here for the ship landing ([tree/](src/tree/)). I.e., he/she needs to extract logdata into a *Trace* struct and needs to build the TBT. ## Brief Summary of the Supported Operators TBT ``T:=`` - ``Fallback([T_1,...,T_n])``: At least one of the subtrees must eventually be satisfied. - ``Sequence([T_1, T_2])``: Each subtree must be satisfied in order from left to right. - ``Parallel(m, [T_1,...,T_n])``: At least ``m`` of the subtrees must be simultaneously satisfied. - ``Timeout(t, T)``: The subtree must be satisfied by a finite prefix of length ``t``. - ``Kleene(n, T)``: There must be ``n`` repetitions of the subtree to be satisfied. - ``Leaf(S)``: STL formula ``S`` must be satisfied. STL ``S:=`` - ``Atomic(function)``: The function must return a positive number to be satisfied, otherwise it is violated. - ``Conjuntion(S_1, S_2)``: Both subformulas must be satisfied. - ``Disjunction(S_1, S_2)``: One of the subformulas or both must be satisfied. - ``Neg(S)``: The subformulas must be violated. - ``Next(S)``: The subformula must be satisfied in the next step. - ``Eventually(S)``: Eventually the subformula must be satisfied. - ``Globally(S)``: The subformula must always be satisfied. - ``Until(S_1, S_2)``: The subformula ``S_1`` must be satisfied *until* ``S_2`` is satisfied. - ``EventuallyInterval(l, u, S)``: Eventually within ``l`` and ``u`` steps, the subformula must be satisfied. - ``GloballyInterval(l, u, S)``: Always within ``l`` and ``u`` steps, the subformula must be satisfied. - ``UntilInterval(l, u, S_1, S_2)``: Within ``l`` and ``u`` steps, the subformula ``S_1`` must be satisfied *until* ``S_2`` is satisfied. The TBT operators are defined [here](src/behaviortree.rs) and the STL operators are defined [here](src/stl.rs). A TBT parser is implemented that is based on this [grammar](src/tbt.pest). For more details, we refer to the [paper](https://doi.org/10.1145/3641513.3650180). ## Docker Environment 1. Install [Docker](https://docs.docker.com/engine/install/) 2. Builder Docker Image: ``docker build -t tbt .`` 3. Run Docker Container: ``docker run -it --rm --name tbt-container tbt bash`` 4. To test if the container is working reproduce paper results by (being in the docker bash): - Run ``. scripts/run.sh`` that takes all logfiles and computes the segmentation. - The script calls the tool as defined [here](#getting-started) for each logfolder that exists in [res](res). - The results for each run are stored in the respective logfolder. - Check results of each logfiles that are located in the following subfolder: ``cd ./res/`` - The files in the folders [res/<folder>](res) are called ``subsampling_result.txt`` and ``subsamplingAndLazy_result.txt``. - Besides the result-files, for each segmentation, the script produces a `.png`-plot. Every `.png`-plot that has a name that ends with `aX`, where `X` is a number, represents an alternative segmentations where the number corresponds to the alternative in the result-file. For instance, Figure 9 of the HSCC paper can then be found [here](res/logs_wind_front_Oblique/subsampling_result_a3.png). - (Optional) To display plots copy them from the docker container to your host machine; dont use the docker bash. - ``docker cp <container_id>:<location_png/results> <location_to_be_stored>`` (copy whole folder or individual files), e.g., ``docker cp e7ba94d69e94:/app/res ./docker`` - to get container_id call ``docker ps`` > The Dockerfile uses multiple stages. The first stage builds the executable using rust/cargo and the second stage uses a debian environment to execute it. Therefore, there are no cargo-commands available in the container while running. > Also, there is a line ending issue, when building the docker image in a Windows environment. We recommend building the image on a Linux machine to avoid this issue (WSL is an option for Windows systems). ## How to Interpret the Output Format Running e.g. `cargo run --release -- -l -c -u -s <your-folder>/TBT-Segmentation/specification/shiplanding_formula_combined.tbt -f <your-foulder>/TBT-Segmentation/res/logs_wind_front_Lateral/` produces an output that contains the following lines: >Constants: { > \"Lateral_AngleToShip\": 90.0, ... } that represent all read constants in the provided TBT specification. > AP(0) in_pos ([\"uas_x\", \"uas_y\", \"uas_z\", \"ship_x\", \"ship_y\", \"ship_z\", \"ship_heading\"]): (2.5 - sqrt((((((ship_x + (30 * cos((deg_to_rad(135) + ship_heading)))) - uas_x) ^ 2) + (((ship_y + (30 * sin((deg_to_rad(135) + ship_heading)))) - uas_y) ^ 2)) + (((ship_z + 20) - uas_z) ^ 2)))) a sequence of the parsed atomic propositions. > SETTING: <br> > Logfile: /root/canbedeleted/TBT-Segmentation/res/logs_wind_front_Lateral/ <br> represents the logfile name. > Approximations: (lazy evaluation= true, subsampling = true(delta: 100)) show which approximations are enabled. In this case, lazy evauation and subsampling with a delta of 100 are enabled. > Trace length: 252 provides that the length of the trace is 252 after subsampling. I.e. the original file has >25.000 entries. > Temporal behavior tree: <br> Sequence(22)[ Fallback(20)[ <...> ]] shows a pretty print of the used TBT with its node identifiers. Here, the root node has ID 22. > Created tree table with 733,194 entries. <br> Created formula table with 828,828 entries. are information on the table used for dynamic programming. > Statistics: Robustness value is 0.05925286 with 3,277,611 total tree lookups and 1,503,504 formula lookups provides information how effective dynamic programming was. > Get segmentation after 0 seconds. states that it took 0 seconds to compute a segmentation. > Approximate segmentation with robustness 0.05925286 and subsampling delta of 0.5564443 is: is the beginning of the segmentation. The following lines provide information on the segments. > lower: 0 upper: 78 value: 0.31250286 segment: Leaf(0 move_to_position_lateral) represent one segment. It states that the leaf node `move_to_position_lateral` was assigned to the segment that begins at index 0 and ends at index 78. Further its robustness value is 0.31, i.e., the trace segment did satisfy this node. ## Contributors - Sebastian Schirmer ## Contributing Please see [the contribution guidelines](CONTRIBUTING.md) for further information about how to contribute. ## Changes Please see the [Changelog](CHANGELOG.md) for notable changes of the material. ## License Please see the file [LICENSE.md](LICENSE.md) for further information about how the content is licensed.\n",
                "dependencies": "# SPDX-FileCopyrightText: 2023 German Aerospace Center (DLR) # SPDX-License-Identifier: Apache-2.0 [package] name = \"tbt-segmentation\" version = \"1.0.0\" edition = \"2021\" authors = [\"Sebastian Schirmer <sebastian.schirmer@dlr.de>\"] description = \"A segmentation algorithm for TBT specifications\" readme = \"README.md\" homepage = \"www.dlr.de/ft/ulf\" repository = \"https://github.com/DLR-FT/TBT-Segmentation\" license = \"Apache-2.0\" keywords = [\"segmentation\", \"cyber-physical systems\", \"offline analysis\"] [dependencies] pest = \"2.7.9\" pest_derive = \"2.7.9\" csv = \"1.2.2\" clap = \"2.33.3\" num-format = \"0.4.4\" toml = \"0.8.13\" serde = { version = \"1.0\", features = [\"derive\"] }\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tereno-doi",
            "repo_link": "https://jugit.fz-juelich.de/ibg-3/ibg3_data_management/software/projects/tereno-doi/tdoi",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tetrax",
            "repo_link": "https://codebase.helmholtz.cloud/micromagnetic-modeling/tetrax",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tigl",
            "repo_link": "https://github.com/DLR-SC/tigl",
            "content": {
                "codemeta": "",
                "readme": "<p><img src=\"doc/images/logo.png\" alt=\"TiGL Logo\" title=\"TiGL Logo\" style=\"background-color:white;padding:5px;\"/></p> [![CI workflow for main branch](https://github.com/DLR-SC/tigl/actions/workflows/main.yml/badge.svg)](https://github.com/DLR-SC/tigl/actions/workflows/main.yml) [![codecov](https://codecov.io/gh/dlr-sc/tigl/branch/master/graph/badge.svg)](https://codecov.io/gh/dlr-sc/tigl) [![Apache 2.0](https://img.shields.io/crates/l/k)](https://github.com/DLR-SC/tigl/blob/cpacs_3/LICENSE.txt) [![Install with conda](https://anaconda.org/dlr-sc/tigl3/badges/version.svg)](https://anaconda.org/dlr-sc/tigl3/badges/version.svg) [![Cite-us](https://img.shields.io/badge/doi-10.1007%2Fs11786--019--00401--y-blue)](https://doi.org/10.1007/s11786-019-00401-y) [![Documentation](https://img.shields.io/badge/docs-online-green)](https://dlr-sc.github.io/tigl/doc/latest/) The **Ti**GL **G**eometry **L**ibrary can be used for the computation and processing of aircraft geometries stored inside [CPACS](https://github.com/DLR-LY/CPACS) files. TiGL offers many geometry related functions such as - Point retrieval functions to compute points on the aircraft surface - Intersection functions to compute the intersection of the aircraft with planes - Export functions for standard CAD file formats (STEP + IGES) or mesh formats, including VTK, Collada, and STL. The TiGL library uses the OpenCASCADE CAD kernel to represent the airplane geometry by NURBS surfaces. The library provides external interfaces for C, C++, Python, Java, MATLAB, and FORTRAN. TiGL is shipped with the Qt based _TiGL Viewer_ for visualizing aircraft geometries or viewing CAD files. ![Screenshot of the TiGL Viewer](doc/images/tiglviewer-web.jpg) # Downloads - Pre-Compiled Releases: https://github.com/DLR-SC/tigl/wiki/Downloads - Nightly Builds: https://github.com/DLR-SC/tigl/actions?query=workflow%3A%22Continuous+Integration%22+event%3Aschedule # News Please head over to our TiGL website: https://dlr-sc.github.io/tigl/#news # Cite us TiGL is available as Open Source and we encourage anyone to make use of it. If you are applying TiGL in a scientific environment and publish any related work, please cite the following article: Siggel, M., Kleinert, J., Stollenwerk, T. et al.: *TiGL: An Open Source Computational Geometry Library for Parametric Aircraft Design*, Math.Comput.Sci. (2019). https://doi.org/10.1007/s11786-019-00401-y A free copy of the paper is offered here: https://rdcu.be/bIGUH\n",
                "dependencies": "# Set a default build type if none was specified if(NOT DEFINED CMAKE_BUILD_TYPE) message(STATUS \"Setting build type to 'Release' as none was specified.\") set(CMAKE_BUILD_TYPE Release CACHE STRING \"Choose the type of build.\" FORCE) # Set the possible values of build type for cmake-gui set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\") endif() cmake_minimum_required (VERSION 3.11.0) project (TIGL VERSION 3.4.0) set(TIGL_VERSION 3.4.0) # enable C++17 support set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake) if(NOT DEFINED CMAKE_INSTALL_LIBDIR) set(CMAKE_INSTALL_LIBDIR \"lib\") endif(NOT DEFINED CMAKE_INSTALL_LIBDIR) if(NOT DEFINED CMAKE_INSTALL_BINDIR) set(CMAKE_INSTALL_BINDIR \"bin\") endif(NOT DEFINED CMAKE_INSTALL_BINDIR) # these settings are required in order to create fully relocatable # libraries on osx set(CMAKE_MACOSX_RPATH ON) set(CMAKE_SKIP_BUILD_RPATH FALSE) set(CMAKE_BUILD_WITH_INSTALL_RPATH FALSE) set(CMAKE_INSTALL_RPATH \"${CMAKE_INSTALL_PREFIX}/lib\") set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE) # convert path to absolute (required for some scripts) if (NOT IS_ABSOLUTE ${CMAKE_INSTALL_PREFIX}) set (CMAKE_INSTALL_PREFIX ${PROJECT_BINARY_DIR}/${CMAKE_INSTALL_PREFIX}) endif() option(TIGL_BINDINGS_INSTALL_CPP \"Install TiGL's CPP bindings\" OFF) OPTION(TIGL_NIGHTLY \"Creates a nightly build of tigl (includes git sha into tigl version)\" OFF) mark_as_advanced(TIGL_NIGHTLY) if(TIGL_NIGHTLY) message(STATUS \"Nightly build enabled\") # get git revision for daily builds include(GetGitRevisionDescription) get_git_head_revision(REFSPEC HASHVAR) if(NOT ${HASHVAR} STREQUAL \"GITDIR-NOTFOUND\") set(TIGL_REVISION ${HASHVAR}) endif() endif(TIGL_NIGHTLY) set(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/${CMAKE_INSTALL_LIBDIR}) include(UseOpenCASCADE) # search TiXI set(TIXI_PATH \"\" CACHE PATH \"TiXI installation prefix\") set(CMAKE_PREFIX_PATH \"${TIXI_PATH};${CMAKE_PREFIX_PATH}\") find_package( tixi3 3.0.3 REQUIRED CONFIG) find_package( PythonInterp ) OPTION(TIGL_USE_GLOG \"Enables advanced logging (requires google glog)\" OFF) if(TIGL_USE_GLOG) find_package( GLOG REQUIRED ) if(NOT GLOG_FOUND) message(STATUS \"Google GLOG not found. Advanced logging disabled.\") endif() endif(TIGL_USE_GLOG) # enable parallel builds in Visual Studio if (MSVC) set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /MP\") endif() # check features include(CheckCXXSourceCompiles) set(src_make_unqiue \"#include <memory>\\nint main(){\\n std::make_unique<int>(5)\\;\\n return 0\\;\\n}\\n\") CHECK_CXX_SOURCE_COMPILES(${src_make_unqiue} HAVE_STDMAKE_UNIQUE) # code coverage analysis IF (CMAKE_BUILD_TYPE STREQUAL \"Debug\") # enable extensive debug output OPTION(DEBUG_EXTENSIVE \"Switch on extensive debug output\" OFF) OPTION(TIGL_ENABLE_COVERAGE \"Enable GCov coverage analysis (defines a 'coverage' target and enforces static build of tigl)\" OFF) IF(TIGL_ENABLE_COVERAGE) MESSAGE(STATUS \"Coverage enabled\") INCLUDE(CodeCoverage) SETUP_TARGET_FOR_COVERAGE(coverage-unittests TiGL-unittests tests/unittests coverageReport-unit $ENV{COVERAGE_ARGS}) SETUP_TARGET_FOR_COVERAGE(coverage-integrationtests TiGL-integrationtests tests/integrationtests coverageReport-integration $ENV{COVERAGE_ARGS}) SETUP_TARGET_FOR_COVERAGE_COBERTURA(coverage-cobertura TiGL-unittests coverage $ENV{COVERAGE_ARGS}) ELSE() MESSAGE(STATUS \"Coverage disabled\") ENDIF() ENDIF() # visual leak detector, useful for debugging under windows if(WIN32) if(CMAKE_BUILD_TYPE STREQUAL \"Debug\") OPTION(TIGL_USE_VLD \"Enable Visual Leak Detector.\" OFF) if(TIGL_USE_VLD) find_package( VLD REQUIRED ) add_definitions(-DHAVE_VLD=1) include_directories(${VLD_INCLUDE_DIRS}) endif(TIGL_USE_VLD) endif(CMAKE_BUILD_TYPE STREQUAL \"Debug\") endif(WIN32) if(WIN32) # avoid export of oce/occt classes into tigl library endif(WIN32) add_subdirectory(thirdparty) # style checks add_custom_target(checkstyle) add_custom_target(checkstylexml) add_subdirectory(src) add_subdirectory(TIGLViewer) add_subdirectory(bindings) add_subdirectory(examples) #create gtests, override gtest standard setting option(TIGL_BUILD_TESTS \"Build TIGL Testsuite\" OFF) if(TIGL_BUILD_TESTS) enable_testing() option(gtest_force_shared_crt \"\" ON) mark_as_advanced(gtest_force_shared_crt gtest_build_tests gtest_build_samples gtest_disable_pthreads) add_subdirectory (\"thirdparty/googletest\" EXCLUDE_FROM_ALL) add_subdirectory(tests) endif(TIGL_BUILD_TESTS) include(createDoc) set(CPACK_DEBIAN_PACKAGE_MAINTAINER \"Martin Siggel\") #required for debian/ubuntu set(CPACK_PACKAGE_VENDOR \"www.dlr.de/sc\") if(TIGL_NIGHTLY) string(SUBSTRING ${TIGL_REVISION} 0 8 TIGL_REV_SHORT) set(CPACK_PACKAGE_VERSION ${TIGL_VERSION}-r${TIGL_REV_SHORT}) set(CPACK_PACKAGE_VERSION_PATCH ${TIGL_VERSION_PATCH}-r${TIGL_REV_SHORT}) else() set(CPACK_PACKAGE_VERSION ${TIGL_VERSION}) set(CPACK_PACKAGE_VERSION_PATCH ${TIGL_VERSION_PATCH}) endif() set(CPACK_PACKAGE_VERSION_MAJOR ${TIGL_VERSION_MAJOR}) set(CPACK_PACKAGE_VERSION_MINOR ${TIGL_VERSION_MINOR}) set(CPACK_RESOURCE_FILE_LICENSE ${PROJECT_SOURCE_DIR}/LICENSE.txt) set(CPACK_PACKAGE_INSTALL_REGISTRY_KEY \"TIGL\") set(CPACK_NSIS_MUI_ICON ${PROJECT_SOURCE_DIR}/TIGLViewer/TIGLViewer.ico) set(CPACK_NSIS_MUI_UNIICON ${PROJECT_SOURCE_DIR}/TIGLViewer/TIGLViewer.ico) # set installer icon if(WIN32) set(CPACK_PACKAGE_ICON ${PROJECT_SOURCE_DIR}/TIGLViewer/gfx\\\\\\\\TIGLViewerNSIS.bmp) elseif(APPLE) set(CPACK_PACKAGE_ICON ${PROJECT_SOURCE_DIR}/TIGLViewer/gfx/TiGL-Viewer3.icns) endif() # set generators if(CPACK_GENERATOR) #already set elseif(APPLE) set(CPACK_GENERATOR DragNDrop) elseif(WIN32) set(CPACK_GENERATOR \"NSIS;ZIP\") else() set(CPACK_GENERATOR TGZ) endif() # set path variable for installer set(CPACK_NSIS_MODIFY_PATH ON) if(CMAKE_SIZEOF_VOID_P EQUAL 8) set(CPACK_NSIS_INSTALL_ROOT \"$PROGRAMFILES64\") set(CPACK_CUSTOM_INITIAL_DEFINITIONS \"!define CPACK_REQUIRIRE_64BIT\") else() set(CPACK_NSIS_INSTALL_ROOT \"$PROGRAMFILES\") endif() if (APPLE) set(CPACK_PACKAGE_EXECUTABLES \"TiGL-Viewer3\" \"TiGL Viewer 3\" ) else(APPLE) set(CPACK_PACKAGE_EXECUTABLES \"tiglviewer-3\" \"TiGL Viewer 3\" ) endif(APPLE) include(CPack) cpack_add_component(viewer DISPLAY_NAME \"TiGL Viewer + 3rd Party DLLs\") cpack_add_component(headers DISPLAY_NAME \"Headers\") cpack_add_component(cpp_bindings DISPLAY_NAME \"Internal C++ Bindings\") cpack_add_component(interfaces DISPLAY_NAME \"Interfaces/Bindings\") cpack_add_component(docu DISPLAY_NAME \"Documentation\")\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tigramite",
            "repo_link": "https://github.com/jakobrunge/tigramite",
            "content": {
                "codemeta": "",
                "readme": "# Tigramite - Causal inference for time series datasets ![logo](docs/_images/tigramite_logo_header.png) Version 5.2 (Python Package) [Github](https://github.com/jakobrunge/tigramite.git) [Documentation](https://jakobrunge.github.io/tigramite/) [Tutorials](https://github.com/jakobrunge/tigramite/tree/master/tutorials/) ## Overview It's best to start with our [Overview/review paper: Causal inference for time series](https://github.com/jakobrunge/tigramite/blob/master/tutorials/Runge_Causal_Inference_for_Time_Series_NREE.pdf) __Update:__ Tigramite now has a new CausalEffects class that allows to estimate (conditional) causal effects and mediation based on assuming a causal graph. Have a look at the tutorial. Further, Tigramite provides several causal discovery methods that can be used under different sets of assumptions. An application always consists of a method and a chosen conditional independence test, e.g. PCMCIplus together with ParCorr. The following two tables give an overview of the assumptions involved: | Method | Assumptions | Output | | :-- | :-- | :-- | | | (in addition to Causal Markov Condition and Faithfulness) | | | PCMCI | Causal stationarity, no contemporaneous causal links, no hidden variables | Directed lagged links, undirected contemporaneous links (for tau_min=0) | | PCMCIplus | Causal stationarity, no hidden variables | Directed lagged links, directed and undirected contemp. links (Time series CPDAG) | | LPCMCI | Causal stationarity | Time series PAG | | RPCMCI | No contemporaneous causal links, no hidden variables | Regime-variable and causal graphs for each regime with directed lagged links, undirected contemporaneous links (for tau_min=0) | | J-PCMCI+ | Multiple datasets, causal stationarity, no hidden system confounding, except if context-related | Directed lagged links, directed and undirected contemp. links (Joint time series CPDAG) | | Conditional independence test | Assumptions | | :-- | :-- | | ParCorr | univariate, continuous variables with linear dependencies and Gaussian noise | | RobustParCorr | univariate, continuous variables with linear dependencies, robust for different marginal distributions | | ParCorrWLS | univariate, continuous variables with linear dependencies, can account for heteroskedastic data | | GPDC / GPDCtorch | univariate, continuous variables with additive dependencies | | CMIknn | multivariate, continuous variables with more general dependencies (permutation-based test) | | Gsquared | univariate discrete/categorical variables | | CMIsymb | multivariate discrete/categorical variables (permutation-based test) | | RegressionCI | mixed datasets with univariate discrete/categorical and (linear) continuous variables | Remark: With the conditional independence test wrapper class PairwiseMultCI you can turn every univariate test into a multivariate test. ## General Notes Tigramite is a causal inference for time series python package. It allows to efficiently estimate causal graphs from high-dimensional time series datasets (causal discovery) and to use graphs for robust forecasting and the estimation and prediction of direct, total, and mediated effects. Causal discovery is based on linear as well as non-parametric conditional independence tests applicable to discrete or continuously-valued time series. Also includes functions for high-quality plots of the results. Please cite the following papers depending on which method you use: - Overview: Runge, J., Gerhardus, A., Varando, G. et al. Causal inference for time series. Nat Rev Earth Environ (2023). https://doi.org/10.1038/s43017-023-00431-y - PCMCI: J. Runge, P. Nowack, M. Kretschmer, S. Flaxman, D. Sejdinovic, Detecting and quantifying causal associations in large nonlinear time series datasets. Sci. Adv. 5, eaau4996 (2019). https://advances.sciencemag.org/content/5/11/eaau4996 - PCMCI+: J. Runge (2020): Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets. Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence, UAI 2020,Toronto, Canada, 2019, AUAI Press, 2020. http://auai.org/uai2020/proceedings/579_main_paper.pdf - LPCMCI: Gerhardus, A. & Runge, J. High-recall causal discovery for autocorrelated time series with latent confounders Advances in Neural Information Processing Systems, 2020, 33. https://proceedings.neurips.cc/paper/2020/hash/94e70705efae423efda1088614128d0b-Abstract.html - RPCMCI: Elena Saggioro, Jana de Wiljes, Marlene Kretschmer, Jakob Runge; Reconstructing regime-dependent causal relationships from observational time series. Chaos 1 November 2020; 30 (11): 113115. https://doi.org/10.1063/5.0020538 - Generally: J. Runge (2018): Causal Network Reconstruction from Time Series: From Theoretical Assumptions to Practical Estimation. Chaos: An Interdisciplinary Journal of Nonlinear Science 28 (7): 075310. https://aip.scitation.org/doi/10.1063/1.5025050 - Nature Communications Perspective paper: https://www.nature.com/articles/s41467-019-10105-3 - Mediation class: J. Runge et al. (2015): Identifying causal gateways and mediators in complex spatio-temporal systems. Nature Communications, 6, 8502. http://doi.org/10.1038/ncomms9502 - Mediation class: J. Runge (2015): Quantifying information transfer and mediation along causal pathways in complex systems. Phys. Rev. E, 92(6), 62829. http://doi.org/10.1103/PhysRevE.92.062829 - CMIknn: J. Runge (2018): Conditional Independence Testing Based on a Nearest-Neighbor Estimator of Conditional Mutual Information. In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics. http://proceedings.mlr.press/v84/runge18a.html - CausalEffects: J. Runge, Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables, Advances in Neural Information Processing Systems, 2021, 34. https://proceedings.neurips.cc/paper/2021/hash/8485ae387a981d783f8764e508151cd9-Abstract.html ## Features - flexible conditional independence test statistics adapted to continuously-valued, discrete and mixed data, and different assumptions about linear or nonlinear dependencies - handling of missing values and masks - p-value correction and (bootstrap) confidence interval estimation - causal effect class to non-parametrically estimate (conditional) causal effects and also linear mediated causal effects - prediction class based on sklearn models including causal feature selection ## Required python packages - python=3.7/3.8/3.9/3.10 - numpy <1.24,>=1.18 - scipy>=1.10.0 - numba==0.56.4 ## Optional packages depending on used functions - scikit-learn>=1.2 # Gaussian Process (GP) Regression - matplotlib>=3.7.0 # Plotting - seaborn>=0.12.2 # Plotting - networkx>=3.0 # Plotting - torch>=1.13.1 # GPDC pytorch version (in conda install pytorch) - gpytorch>=1.9.1 # GPDC gpytorch version - dcor>=0.6 # GPDC distance correlation version - joblib>=1.2.0 # CMIsymb shuffle parallelization - ortools>=9.2 # RPCMCI ## Installation python setup.py install This will install tigramite in your path. To use just the ParCorr, CMIknn, and CMIsymb independence tests, only numpy/numba and scipy are required. For other independence tests more packages are required: - GPDC: scikit-learn is required for Gaussian Process regression and dcor for distance correlation - GPDCtorch: gpytorch is required for Gaussian Process regression Note: Due to incompatibility issues between numba and numpy, we currently enforce soft dependencies on the versions. ## User Agreement By downloading TIGRAMITE you agree with the following points: TIGRAMITE is provided without any warranty or conditions of any kind. We assume no responsibility for errors or omissions in the results and interpretations following from application of TIGRAMITE. You commit to cite above papers in your reports or publications. ## License Copyright (C) 2014-2025 Jakob Runge See license.txt for full text. GNU General Public License v3.0 TIGRAMITE is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version. TIGRAMITE is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
                "dependencies": "\"\"\" Install tigramite \"\"\" from __future__ import print_function import pathlib import os from setuptools import setup, Extension from setuptools.command.build_ext import build_ext import json # Handle building against numpy headers before installing numpy class UseNumpyHeadersBuildExt(build_ext): \"\"\" Subclassed build_ext command. Allows for numpy to be imported after it is automatically installed. This lets us use numpy.get_include() while listing numpy as a needed dependency. \"\"\" def run(self): self.distribution.fetch_build_eggs([\"numpy\"]) # Import numpy here, only when headers are needed import numpy # Add numpy headers to include_dirs self.include_dirs.append(numpy.get_include()) # Call original build_ext command build_ext.run(self) with open(\"README.md\", \"r\", encoding=\"utf-8\") as fh: long_description = fh.read() # Define the minimal classes needed to install and run tigramite INSTALL_REQUIRES = [\"numpy>=1.18\", \"scipy>=1.10.0\", \"six\"] # Define all the possible extras needed EXTRAS_REQUIRE = { \"all\": [ \"scikit-learn>=1.2\", # Gaussian Process (GP) Regression \"matplotlib>=3.7.0\", # plotting \"seaborn>=0.12.2\", # plotting \"networkx>=3.0\", # plotting \"pytorch>=1.13.1\", # GPDC torch version \"gpytorch>=1.9.1\", # GPDC gpytorch version \"dcor>=0.6\", # GPDC distance correlation version \"joblib>=1.2.0\", # CMIsymb shuffle parallelization and others \"ortools>=9.2\", # RPCMCI \"numba>=0.58\", # CMIknn and CMIsymb and derived classes ] } # Define the packages needed for testing TESTS_REQUIRE = [\"nose\", \"pytest\", \"networkx>=3.0\", \"scikit-learn>=1.2\", \"pytorch>=1.13.1\", \"gpytorch>=1.9.1\", \"dcor>=0.6\"] EXTRAS_REQUIRE[\"test\"] = TESTS_REQUIRE # Define the extras needed for development EXTRAS_REQUIRE[\"dev\"] = EXTRAS_REQUIRE[\"all\"] # Use a custom build to handle numpy.include_dirs() when building CMDCLASS = {\"build_ext\": UseNumpyHeadersBuildExt} # Run the setup setup( name=\"tigramite\", version=\"5.2.7.0\", packages=[\"tigramite\", \"tigramite.independence_tests\", \"tigramite.toymodels\"], license=\"GNU General Public License v3.0\", description=\"Tigramite causal inference for time series\", author=\"Jakob Runge\", author_email=\"jakob@jakob-runge.com\", url=\"https://github.com/jakobrunge/tigramite/\", long_description=long_description, long_description_content_type=\"text/markdown\", keywords=\"causal inference, causal discovery, prediction, time series\", cmdclass=CMDCLASS, install_requires=INSTALL_REQUIRES, extras_require=EXTRAS_REQUIRE, test_suite=\"tests\", tests_require=TESTS_REQUIRE, classifiers=[ \"Development Status :: 4 - Beta\", \"Intended Audience :: Science/Research\", \"Topic :: Scientific/Engineering :: Artificial Intelligence\", \"Topic :: Scientific/Engineering :: Mathematics\", \"License \" \":: OSI Approved \" \":: GNU General Public License v3 or later (GPLv3+)\", \"Programming Language :: Python\", ], )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/timeio",
            "repo_link": "https://codebase.helmholtz.cloud/ufz-tsm/tsm-orchestration",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tixi",
            "repo_link": "https://github.com/DLR-SC/tixi",
            "content": {
                "codemeta": "",
                "readme": "# TIXI # [![CI](https://github.com/DLR-SC/tixi/actions/workflows/main.yml/badge.svg)](https://github.com/DLR-SC/tixi/actions/workflows/main.yml) - Binary Downloads: https://github.com/DLR-SC/tixi/wiki/Downloads - API Documentation: http://dlr-sc.github.io/tixi/ - Issue Tracker: https://github.com/DLR-SC/tixi/issues - Wiki: https://github.com/DLR-SC/tixi/wiki ## Installation ## - with Conda: [![Anaconda-Server Badge](https://anaconda.org/dlr-sc/tixi3/badges/installer/conda.svg)](https://anaconda.org/DLR-SC/tixi3) - with Linux package manager: [OpenBuildService](https://software.opensuse.org/download.html?project=science:dlr&package=tixi3) ## Description ## TiXI is a fast and simple XML interface library and could be used from applications written in C, C++, Fortran, JAVA and Python. The library can be directly integrated into a code by third party software or can be used by users who don't want to deal with the complexity of XML when creating a new application. Although simplified and somewhat restricted compared to a full-fledged XML processing library the user can, for example, create documents, create and delete nodes, and add and remove element attributes. Routines to read and write simple text nodes and additionally specialized nodes holding integer and floating point numbers are part of this API. Furthermore, routines to process aggregates of these simple types have been implemented. For the processing of geometric data, reading and writing of multidimensional arrays or arrays of vectors, i.e. coordinates of points are supported. The library has been designed to hide the implementation details so that the underlying XML library, currently libxml2, can be replaced by another one without changing the XML processing API in the applications. Reading a text attribute could be as easy as: ``` tixiGetTextAttribute( handle, elementPath, attributeName, &attributeValue ); ``` Getting a double value would look like this: ``` tixiGetDoubleElement( handle, elementPath, &x ); ``` ## Multi Language Support ## The TIXI library is written in C, but there are interfaces and wrappers for C++, Fortran, Python, JAVA and Matlab. Take a look at our examples for [C](https://github.com/DLR-SC/tixi/wiki/CExamples) and [Fortran](https://github.com/DLR-SC/tixi/wiki/Fortran%20Examples).\n",
                "dependencies": "cmake_minimum_required (VERSION 3.1) if (EXISTS ${CMAKE_BINARY_DIR}/conan_toolchain.cmake) cmake_policy(SET CMP0091 NEW) include(${CMAKE_BINARY_DIR}/conan_toolchain.cmake) endif() # Set a default build type if none was specified if(NOT DEFINED CMAKE_BUILD_TYPE) message(STATUS \"Setting build type to 'Release' as none was specified.\") set(CMAKE_BUILD_TYPE Release CACHE STRING \"Choose the type of build.\" FORCE) # Set the possible values of build type for cmake-gui set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\") endif() project (TIXI) # convert path to absolute (required for some scripts) if (NOT IS_ABSOLUTE ${CMAKE_INSTALL_PREFIX}) set (CMAKE_INSTALL_PREFIX ${PROJECT_BINARY_DIR}/${CMAKE_INSTALL_PREFIX}) endif() set(TIXI_VERSION_MAJOR 3) set(TIXI_VERSION_MINOR 3) set(TIXI_VERSION_PATCH 0) set(TIXI_VERSION \"${TIXI_VERSION_MAJOR}.${TIXI_VERSION_MINOR}.${TIXI_VERSION_PATCH}\") # set name of the tixi library set(TIXI_LIB_NAME tixi${TIXI_VERSION_MAJOR}) set(ADD_INCLUDE_PATH \"\" CACHE PATH \"Additional include path for package search\") set(ADD_LIB_PATH \"\" CACHE PATH \"Additional library path for package search\") set(CMAKE_INCLUDE_PATH \"${CMAKE_INCLUDE_PATH}\" ${ADD_INCLUDE_PATH}) set(CMAKE_LIBRARY_PATH \"${CMAKE_LIBRARY_PATH}\" ${ADD_LIB_PATH}) set(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake) if(NOT DEFINED CMAKE_INSTALL_LIBDIR) set(CMAKE_INSTALL_LIBDIR \"lib\") endif(NOT DEFINED CMAKE_INSTALL_LIBDIR) if(NOT DEFINED CMAKE_INSTALL_BINDIR) set(CMAKE_INSTALL_BINDIR \"bin\") endif(NOT DEFINED CMAKE_INSTALL_BINDIR) if(NOT DEFINED CMAKE_INSTALL_INCLUDE_DIR) set(CMAKE_INSTALL_INCLUDE_DIR \"include/${TIXI_LIB_NAME}\") endif(NOT DEFINED CMAKE_INSTALL_INCLUDE_DIR) if(NOT DEFINED CONFIG_INSTALL_DIR) set(CONFIG_INSTALL_DIR \"${CMAKE_INSTALL_LIBDIR}/${TIXI_LIB_NAME}\") endif(NOT DEFINED CONFIG_INSTALL_DIR) if (NOT DEFINED CMAKE_DEBUG_POSTFIX) set(CMAKE_DEBUG_POSTFIX \"-d\") endif (NOT DEFINED CMAKE_DEBUG_POSTFIX) include(CheckCCompilerFlag) check_c_compiler_flag(\"-Wno-deprecated-declarations\" C_COMPILER_HAS_NO_DEPRECATED_DECL_OPTION) option (TIXI_ENABLE_FORTRAN \"Enable Fortran examples and binding\" OFF) find_package(PythonInterp) set(LIBRARY_OUTPUT_PATH ${PROJECT_BINARY_DIR}/${CMAKE_INSTALL_LIBDIR}) # create library output path file(MAKE_DIRECTORY ${LIBRARY_OUTPUT_PATH}) set(CMAKE_MACOSX_RPATH 1) # code coverage analysis IF (CMAKE_BUILD_TYPE STREQUAL \"Debug\") OPTION(TIXI_ENABLE_COVERAGE \"Enable GCov coverage analysis (defines a 'coverage' target and enforces static build of tixi)\" OFF) IF(TIXI_ENABLE_COVERAGE) MESSAGE(STATUS \"Coverage enabled\") INCLUDE(CodeCoverage) SETUP_TARGET_FOR_COVERAGE(coverage ${PROJECT_NAME}-unittests coverageReport \"--gtest_output=xml\") MESSAGE(STATUS \"Enabeling memcheck\") INCLUDE(Valgrind) SETUP_TARGET_FOR_VALGRIND(memcheck ${PROJECT_NAME}-unittests valgrind) ELSE() MESSAGE(STATUS \"Coverage disabled\") ENDIF() ENDIF() # visual leak detector, useful for debugging under windows if(WIN32) if(CMAKE_BUILD_TYPE STREQUAL \"Debug\") OPTION(TIXI_USE_VLD \"Enable Visual Leak Detector.\" OFF) if(TIXI_USE_VLD) find_package( VLD REQUIRED ) endif(TIXI_USE_VLD) endif(CMAKE_BUILD_TYPE STREQUAL \"Debug\") endif(WIN32) #create tixi library add_subdirectory(src) #create bindings to different languages (fortran, java ...) add_subdirectory(bindings) #create gtests, override gtest standard setting option(TIXI_BUILD_TESTS \"Build TIXI Testsuite\" OFF) if(TIXI_BUILD_TESTS) enable_testing() option(gtest_force_shared_crt \"\" ON) mark_as_advanced(gtest_force_shared_crt gtest_build_tests gtest_build_samples gtest_disable_pthreads) add_subdirectory (\"thirdparty/googletest\" EXCLUDE_FROM_ALL) add_subdirectory(tests) endif(TIXI_BUILD_TESTS) #demos add_subdirectory(examples/Demo) if (TIXI_ENABLE_FORTRAN) add_subdirectory(examples/fortran77) endif(TIXI_ENABLE_FORTRAN) # create the doc include(createDoc) set(CPACK_DEBIAN_PACKAGE_MAINTAINER \"Martin Siggel\") #required for debian/ubuntu set(CPACK_PACKAGE_VENDOR \"www.dlr.de/sc\") set(CPACK_PACKAGE_VERSION ${TIXI_VERSION}) set(CPACK_PACKAGE_VERSION_MAJOR ${TIXI_VERSION_MAJOR}) set(CPACK_PACKAGE_VERSION_MINOR ${TIXI_VERSION_MINOR}) set(CPACK_PACKAGE_VERSION_PATCH ${TIXI_VERSION_PATCH}) set(CPACK_RESOURCE_FILE_LICENSE ${PROJECT_SOURCE_DIR}/LICENSE) set(CPACK_PACKAGE_INSTALL_REGISTRY_KEY \"TIXI\") # set path variable for installer set(CPACK_NSIS_MODIFY_PATH ON) if(CMAKE_SIZEOF_VOID_P EQUAL 8) set(CPACK_NSIS_INSTALL_ROOT \"$PROGRAMFILES64\") set(CPACK_CUSTOM_INITIAL_DEFINITIONS \"!define CPACK_REQUIRIRE_64BIT\") else() set(CPACK_NSIS_INSTALL_ROOT \"$PROGRAMFILES\") endif() include(CPack) cpack_add_component(Runtime DISPLAY_NAME \"3rd Party Libraries\") cpack_add_component(headers DISPLAY_NAME \"Headers\") cpack_add_component(interfaces DISPLAY_NAME \"Interfaces/Bindings\") cpack_add_component(docu DISPLAY_NAME \"Documentation\")\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tomato-tools",
            "repo_link": "https://git.geomar.de/open-source/tomato-toolboxes/tomato",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tomobear",
            "repo_link": "https://github.com/KudryashevLab/TomoBEAR",
            "content": {
                "codemeta": "",
                "readme": "# TomoBEAR [![DOI](https://zenodo.org/badge/675692608.svg)](https://zenodo.org/badge/latestdoi/675692608) **TomoBEAR** is a configurable and customizable modular pipeline for streamlined processing of cryo-electron tomographic data and preliminary subtomogram averaging (StA) based on best practices in the scientific research group of Dr. Misha Kudryashev[^1][^2]. ![TomoBEAR Social Media Logo Image](images/TomoBEAR_gitlogo.png) Implementation details and benchmarks you can find in our publication: </br> Balyschew N, Yushkevich A, Mikirtumov V, Sanchez RM, Sprink T, Kudryashev M. Streamlined Structure Determination by Cryo-Electron Tomography and Subtomogram Averaging using TomoBEAR. *Nat Commun* **14**, 6543 (2023). doi: [10.1038/s41467-023-42085-w](https://www.nature.com/articles/s41467-023-42085-w) > **Warning** > <br/> This software is currently in pre-release state. New features may still appear and refactoring may still take place between all the current and future 0.x.y versions until 1.0.0 will be ready to be released. Binaries are not currently shipped. ## Contents - [Quick start](#quick-start) - [Documentation and licensing](#documentation-and-licensing) - [Changes and releases](#changes-and-releases) - [Feedback and contribution](#feedback-and-contribution) - [Citation](#citation) - [Acknowledgements](#acknowledgements) - [Contacts](#contacts) ## Quick start ### Video-tutorials We have prepared a range of short (8-12 min) video-tutorials explaining setup, usage and example output of the ```TomoBEAR``` to help you get started with ```TomoBEAR``` based on the [ribosome tutorial](https://github.com/KudryashevLab/TomoBEAR/wiki/Tutorials): * [Video 1](https://youtu.be/2uizkE616tE): how to get the latest ```TomoBEAR``` version and configure ```TomoBEAR``` and its dependencies; * [Video 2](https://youtu.be/N93tfAXp990): description of the project configuration file and the pipeline execution; * [Video 3](https://youtu.be/qbkRtMJp0eI): additional configuration file parameters description, ```TomoBEAR```-```IMOD```-```TomoBEAR``` loop for checking tilt series alignment results and fiducials refinement (if needed); * [Video 4](https://youtu.be/BP2T_Y7BiDo): checking on further intermediate results (alignment, CTF-correction, reconstruction, template matching). ### Pipeline structure In the following picture you can see a flow chart of the main `TomoBEAR` processing steps. As the basic input data you can use raw frame movies or already assembled tilt stacks. More on input formats you [can read here](https://github.com/KudryashevLab/TomoBEAR/wiki/Usage.md#input-data-file-formats). ![Schematic Pipeline Image](images/pipeline_upd.png) Blue boxes outline the steps that are performed fully automatically, green boxes may require human intervention. The steps encapsulated in the red frame represent the functionality of live data processing. More detailed diagram [is located on wiki](https://github.com/KudryashevLab/TomoBEAR/wiki). > **Note** > <br/> Full MATLAB (source code) version of `TomoBEAR` supports workstations and single interactive nodes with GPUs on the computing clusters at the moment. We are also working towards enabling the support of binaries on the mentioned systems as well as support of both source code and binary versions of the `TomoBEAR` on HPC clusters. ## Documentation and licensing Detailed information on the installation, setup and usage as well as tutorials and example results can be found in the corresponding [wiki](https://github.com/KudryashevLab/TomoBEAR/wiki). Please, see the [LICENSE file](LICENSE.md) for the information about how the content of this repository is licensed. ## Changes and releases The [CHANGELOG file](CHANGELOG.md) contains all notable changes corresponding to the different `TomoBEAR` releases, which are available at the [Releases page](https://github.com/KudryashevLab/TomoBEAR/releases). If you want to clone a specific ```TomoBEAR``` version, please refer to the **Setup > Get source code and binary > Clone specific version** section on the wiki page [Installation and Setup](https://github.com/KudryashevLab/TomoBEAR/wiki/Installation-and-Setup.md). ## Feedback and contribution In case of any questions, issues or suggestions you may interact with us by one of the following ways: * open an issue/bug report, feature request or post a question using [Issue Tracker](https://github.com/KudryashevLab/TomoBEAR/issues); * write an e-mail to [Misha Kudryashev](mailto:misha.kudryashev@gmail.com) or [Artsemi Yushkevich](mailto:Artsemi.Yushkevich@mdc-berlin.de); * start a discussion in [Discussions](https://github.com/KudryashevLab/TomoBEAR/discussions); If you wish to contribute, please, fork this repository and make a pull request back with your changes and a short description. For further details on contribution plase read our [contribution guidelines](CONTRIBUTING.md). ## Citation If you use `TomoBEAR` or its parts in your research, please **cite both** `TomoBEAR` and **all external software packages** which you have used under `TomoBEAR`. The `TomoBEAR` modules dependencies on third-party software are listed on the page [Modules](https://github.com/KudryashevLab/TomoBEAR/wiki/Modules.md) and the list of the corresponding references to cite is located on the page [External Software](https://github.com/KudryashevLab/TomoBEAR/wiki/External-Software.md). ## Acknowledgements We are grateful to the following organizations: - Buchmann family and [BMLS (Buchmann Institute for Molecular Life Sciences)](https://www.bmls.de) for supporting this project with their starters stipendia for PhD students; - [DFG (Deutsche Forschungsgesellschaft)](https://www.dfg.de) for funding the project. As well we are grateful to the [structural biology scientific research group of Werner Kühlbrandt](https://www.biophys.mpg.de/2207989/werner_kuehlbrandt) at the [MPIBP (Max Planck Institute of Biophysics)](https://www.biophys.mpg.de) and the [MPIBP](https://www.biophys.mpg.de) in Frankfurt (Hesse), Germany for support. The authors thank as well the following people: * Dr. Daniel Castano-Diez, Dr. Kendra Leigh and Dr. Christoph Diebolder and Dr. Wolfgang Lugmayr for useful discussions; * Uljana Kravchenko, Xiaofeng Chu, Giulia Glorani for testing the developmental versions and providing feedback, * Ricardo Sanchez for producing MATLAB version of the [SUSAN framework](https://github.com/rkms86/SUSAN) in order to be compatible with TomoBEAR; * Juan Castillo from the Max Planck Institute for Biophysics for the IT support at the Max Planck for Biophysics, * the high-performance computing team at the MDC for supporting our operation at the Max-Cluster. We would like to acknowledge as well that TomoBEAR contains modified pieces of MATLAB source code of the Dynamo package developed by Dr. Daniel Castaño-Díez et al.: https://www.dynamo-em.org. ## Contacts * Prof. Dr. Misha Kudryashev[^1][^2] ([e-mail](mailto:misha.kudryashev@gmail.com?subject=[GitHub]%20TomoBEAR)) - `TomoBEAR` project leader, Principal Investigator; * Nikita Balyschew[^2] - `TomoBEAR` core version developer, alumni Ph.D. student. * Artsemi Yushkevich[^1] ([e-mail](mailto:Artsemi.Yushkevich@mdc-berlin.de?subject=[GitHub]%20TomoBEAR)) - `TomoBEAR` contributing developer, Ph.D. student. * Vasilii Mikirtumov[^1] ([e-mail](mailto:mikivasia@gmail.com?subject=[GitHub]%20TomoBEAR)) - `TomoBEAR` application engineer, Ph.D. student. [^1]: [In situ Structural Biology Group](https://www.mdc-berlin.de/kudryashev) at the [MDCMM (Max Delbrück Center of Molecular Medicine)](https://www.mdc-berlin.de) in Berlin, Germany. [^2]: [Independent Research Group (Sofja Kovaleskaja)](https://www.biophys.mpg.de/2149775/members) at the Department of Structural Biology at [MPIBP (Max Planck Institute of Biophysics)](https://www.biophys.mpg.de/en) in Frankfurt (Hesse), Germany;\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/treams",
            "repo_link": "https://github.com/tfp-photonics/treams",
            "content": {
                "codemeta": "",
                "readme": "![Version](https://img.shields.io/github/v/tag/tfp-photonics/treams) [![PyPI](https://img.shields.io/pypi/v/treams)](https://pypi.org/project/treams) ![License](https://img.shields.io/github/license/tfp-photonics/treams) ![build](https://github.com/tfp-photonics/treams/actions/workflows/build.yml/badge.svg) [![docs](https://github.com/tfp-photonics/treams/actions/workflows/docs.yml/badge.svg)](https://tfp-photonics.github.io/treams) ![doctests](https://github.com/tfp-photonics/treams/actions/workflows/doctests.yml/badge.svg) ![tests](https://github.com/tfp-photonics/treams/actions/workflows/tests.yml/badge.svg) [![coverage](https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Ftfp-photonics%2Ftreams%2Fhtmlcov%2Fendpoint.json)](https://htmlpreview.github.io/?https://github.com/tfp-photonics/treams/blob/htmlcov/index.html) # treams The package `treams` provides a framework to simplify computations of the electromagnetic scattering of waves at finite and at periodic arrangements of particles based on the T-matrix method. ## Installation ### Installation using pip To install the package with pip, use ```sh pip install treams ``` If you're using the system wide installed version of python, you might consider the ``--user`` option. ## Documentation The documentation can be found at https://tfp-photonics.github.io/treams. ## Publications When using this code please cite: [D. Beutel, I. Fernandez-Corbaton, and C. Rockstuhl, treams - a T-matrix-based scattering code for nanophotonics, Comp. Phys. Commun. 297, 109076 (2024).](https://doi.org/10.1016/j.cpc.2023.109076) Other relevant publications are * [D. Beutel, I. Fernandez-Corbaton, and C. Rockstuhl, Unified Lattice Sums Accommodating Multiple Sublattices for Solutions of the Helmholtz Equation in Two and Three Dimensions, Phys. Rev. A 107, 013508 (2023).](https://doi.org/10.1103/PhysRevA.107.013508) * [D. Beutel, P. Scott, M. Wegener, C. Rockstuhl, and I. Fernandez-Corbaton, Enhancing the Optical Rotation of Chiral Molecules Using Helicity Preserving All-Dielectric Metasurfaces, Appl. Phys. Lett. 118, 221108 (2021).](https://doi.org/10.1063/5.0050411) * [D. Beutel, A. Groner, C. Rockstuhl, C. Rockstuhl, and I. Fernandez-Corbaton, Efficient Simulation of Biperiodic, Layered Structures Based on the T-Matrix Method, J. Opt. Soc. Am. B, JOSAB 38, 1782 (2021).](https://doi.org/10.1364/JOSAB.419645) ## Features * [x] T-matrix calculations using a spherical or cylindrical wave basis set * [x] Calculations in helicity and parity (TE/TM) basis * [x] Scattering from clusters of particles * [x] Scattering from particles and clusters arranged in 3d-, 2d-, and 1d-lattices * [x] Calculation of light propagation in stratified media * [x] Band calculation in crystal structures\n",
                "dependencies": "[build-system] requires = [ \"setuptools<=70.1.1\", \"wheel\", \"Cython\", \"numpy\", \"scipy\", \"setuptools_scm>=6.2\" ] build-backend = \"setuptools.build_meta\" [tool.isort] profile = \"black\" [tool.setuptools_scm] [tool.pylint.messages_control] extension-pkg-whitelist = \"treams\" [tool.cibuildwheel] archs = [\"auto64\"] skip = [\"pp*\", \"*musllinux*\"] test-command = \"python -m pytest {project}/tests/unit || cd .\" test-extras = [\"test\", \"io\"] [tool.pytest.ini_options] datadir = \"tests/datadir\"\n\"\"\"Packaging of treams.\"\"\" import os import numpy as np from setuptools import Extension, setup from setuptools.command.build_ext import build_ext as _build_ext try: from Cython.Build import cythonize except ImportError: cythonize = None if os.name == \"nt\": link_args = [ \"-static-libgcc\", \"-static-libstdc++\", \"-Wl,-Bstatic,--whole-archive\", \"-lwinpthread\", \"-Wl,--no-whole-archive\", ] compile_args = [\"-DMS_WIN64\"] class build_ext(_build_ext): \"\"\"build_ext for Windows.\"\"\" def finalize_options(self): \"\"\"Set compiler to gcc.\"\"\" super().finalize_options() self.compiler = \"mingw32\" # https://cython.readthedocs.io/en/latest/src/tutorial/appendix.html def build_extensions(self): \"\"\"Add Windows specific compiler and linker arguments.\"\"\" if self.compiler.compiler_type == \"mingw32\": for e in self.extensions: e.extra_compile_args = compile_args e.extra_link_args = link_args super().build_extensions() else: build_ext = _build_ext # https://cython.readthedocs.io/en/latest/src/userguide/source_files_and_compilation.html#distributing-cython-modules def no_cythonize(extensions, **_ignore): \"\"\"Add c and c++ code to source archive.\"\"\" for extension in extensions: sources = [] for sfile in extension.sources: path, ext = os.path.splitext(sfile) if ext in (\".pyx\", \".py\"): if extension.language == \"c++\": ext = \".cpp\" else: ext = \".c\" sfile = path + ext sources.append(sfile) extension.sources[:] = sources return extensions keys = {\"include_dirs\": [np.get_include()]} compiler_directives = {\"language_level\": \"3\"} if os.environ.get(\"CYTHON_COVERAGE\", False): keys[\"define_macros\"] = [(\"CYTHON_TRACE_NOGIL\", \"1\")] compiler_directives[\"linetrace\"] = True extension_names = [ \"treams.coeffs\", \"treams.config\", \"treams.cw\", \"treams.pw\", \"treams.sw\", \"treams.lattice._dsum\", \"treams.lattice._esum\", \"treams.lattice._gufuncs\", \"treams.lattice._misc\", \"treams.lattice.cython_lattice\", \"treams.special._bessel\", \"treams.special._coord\", \"treams.special._gufuncs\", \"treams.special._integrals\", \"treams.special._misc\", \"treams.special._ufuncs\", \"treams.special._waves\", \"treams.special._wigner3j\", \"treams.special._wignerd\", \"treams.special.cython_special\", ] extensions = [ Extension(name, [f\"src/{name.replace('.', '/')}.pyx\"], **keys) for name in extension_names ] if cythonize is not None: try: extensions = cythonize(extensions, compiler_directives=compiler_directives) except ValueError: extensions = no_cythonize(extensions) else: extensions = no_cythonize(extensions) setup(ext_modules=extensions, cmdclass={\"build_ext\": build_ext})\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tridec-cloud",
            "repo_link": "https://github.com/locationtech-archived/geoperil",
            "content": {
                "codemeta": "",
                "readme": "<!-- GeoPeril - A platform for the computation and web-mapping of hazard specific geospatial data, as well as for serving functionality to handle, share, and communicate threat specific information in a collaborative environment. Copyright (C) 2021 GFZ German Research Centre for Geosciences SPDX-License-Identifier: Apache-2.0 Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the Licence is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the Licence for the specific language governing permissions and limitations under the Licence. Contributors: Johannes Spazier (GFZ) Sven Reissland (GFZ) Martin Hammitzsch (GFZ) Matthias Rüster (GFZ) Hannes Fuchs (GFZ) --> # GeoPeril This project is a prototype implementation of an early warning system for tsunamis, including: * harvesting of earthquake events from catalogs or APIs * automatic execution of simulations for events with given thresholds * remote execution of simulations with EasyWave as the simulation processing backend * queuing of simulation processes with support for multiple remote processing servers * individual accounts with different permission levels * modifying earthquake parameters or creating fictional earthquakes to simulate a scenario ## Development environment To start up a development environment use the `docker-compose-dev.yml` file: ```shell cd docker docker-compose -f docker-compose-dev.yml up --build ``` You can then visit `http://localhost:8080` to see the frontend. Default login credentials for an administrative account are `admin`/`admin` and for a less privileged user `test`/`test`. Changes for the source code of the frontend component are then hot reloaded and will be rebuild on the fly. Data for world seas were downloaded from: https://catalog.data.gov/dataset/world-water-body-limits-detailed-2017mar30 ## License Copyright © 2021 Helmholtz Centre Potsdam - GFZ German Research Centre for Geosciences, Germany (https://www.gfz-potsdam.de) This work is licensed under the following license(s): * Software files are licensed under [Apache-2.0](LICENSES/Apache-2.0.txt) * Everything else is licensed under [Apache-2.0](LICENSES/Apache-2.0.txt) Please see the individual files for more accurate information. > **Hint:** We provided the copyright and license information in accordance to the [REUSE Specification 3.0](https://reuse.software/spec/). ## FAQ ### apt error: not signed on build The following error may appear: ```bash W: GPG error: http://security.debian.org/debian-security buster/updates InRelease: At least one invalid signature was encountered. E: The repository 'http://security.debian.org/debian-security buster/updates InRelease' is not signed. W: GPG error: http://deb.debian.org/debian buster InRelease: At least one invalid signature was encountered. E: The repository 'http://deb.debian.org/debian buster InRelease' is not signed. W: GPG error: http://deb.debian.org/debian buster-updates InRelease: At least one invalid signature was encountered. E: The repository 'http://deb.debian.org/debian buster-updates InRelease' is not signed. ERROR: Service '...' failed to build : ... ``` This could happen if you have an older base image cached. To solve this remove the local images with: `docker image prune -a` **NOTE:** This deletes all images on your machine. Save any image you can not download from a registry!\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/trimmomatic",
            "repo_link": "https://github.com/usadellab/Trimmomatic",
            "content": {
                "codemeta": "",
                "readme": "# Trimmomatic # Note while the software is licensed under the GPL, the adapter sequences are *not* included in the GPL part, but owned by and used with permission of Illumina. Oligonucleotide sequences © 2023 Illumina, Inc. All rights reserved. # Quick start ## Installation The easiest option is to download a binary release zip, and unpack it somewhere convenient. You'll need to modify the example command lines below to reference the trimmomatic JAR file and the location of the adapter fasta files. ## Build from Source The current version can be built by cloning the repository, change into the top level directory and build using 'ant'. To build from a source release, download the source zip or tar.gz, unpack it, change into top level directory (Trimmomatic-x.xx), and build using 'ant'. ## Paired End: With most new data sets you can use gentle quality trimming and adapter clipping. You often don't need leading and traling clipping. Also in general setting the keepBothReads to True can be useful when working with paired end data, you will keep even redunfant information but this likely makes your pipelines more manageable. Note the additional :2 in front of the True (for keepBothReads) - this is the minimum adapter length in palindrome mode, you can even set this to 1. (Default is a very conservative 8) If you have questions please don't hesitate to contact us, this is not necessarily one size fits all. (e.g. RNAseq expression analysis vs DNA assembly). java -jar trimmomatic-0.39.jar PE input_forward.fq.gz input_reverse.fq.gz output_forward_paired.fq.gz output_forward_unpaired.fq.gz output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:True LEADING:3 TRAILING:3 MINLEN:36 for reference only (less sensitive for adapters) java -jar trimmomatic-0.35.jar PE -phred33 input_forward.fq.gz input_reverse.fq.gz output_forward_paired.fq.gz output_forward_unpaired.fq.gz output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 This will perform the following: * Remove adapters (ILLUMINACLIP:TruSeq3-PE.fa:2:30:10) * Remove leading low quality or N bases (below quality 3) (LEADING:3) * Remove trailing low quality or N bases (below quality 3) (TRAILING:3) * Scan the read with a 4-base wide sliding window, cutting when the average quality per base drops below 15 (SLIDINGWINDOW:4:15) * Drop reads below the 36 bases long (MINLEN:36) ## Single End: java -jar trimmomatic-0.35.jar SE -phred33 input.fq.gz output.fq.gz ILLUMINACLIP:TruSeq3-SE:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 This will perform the same steps, using the single-ended adapter file # Description Trimmomatic performs a variety of useful trimming tasks for illumina paired-end and single ended data.The selection of trimming steps and their associated parameters are supplied on the command line. The current trimming steps are: * ILLUMINACLIP: Cut adapter and other illumina-specific sequences from the read. * SLIDINGWINDOW: Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold. * LEADING: Cut bases off the start of a read, if below a threshold quality * TRAILING: Cut bases off the end of a read, if below a threshold quality * CROP: Cut the read to a specified length * HEADCROP: Cut the specified number of bases from the start of the read * MINLEN: Drop the read if it is below a specified length * TOPHRED33: Convert quality scores to Phred-33 * TOPHRED64: Convert quality scores to Phred-64 It works with FASTQ (using phred + 33 or phred + 64 quality scores, depending on the Illumina pipeline used), either uncompressed or gzipp'ed FASTQ. Use of gzip format is determined based on the .gz extension. For single-ended data, one input and one output file are specified, plus the processing steps. For paired-end data, two input files are specified, and 4 output files, 2 for the 'paired' output where both reads survived the processing, and 2 for corresponding 'unpaired' output where a read survived, but the partner read did not. # Running Trimmomatic Since version 0.27, trimmomatic can be executed using -jar. The 'old' method, using the explicit class, continues to work. Paired End Mode: java -jar <path to trimmomatic.jar> PE [-threads <threads] [-phred33 | -phred64] [-trimlog <logFile>] <input 1> <input 2> <paired output 1> <unpaired output 1> <paired output 2> <unpaired output 2> <step 1> ... or java -classpath <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticPE [-threads <threads>] [-phred33 | -phred64] [-trimlog <logFile>] <input 1> <input 2> <paired output 1> <unpaired output 1> <paired output 2> <unpaired output 2> <step 1> ... Single End Mode: java -jar <path to trimmomatic jar> SE [-threads <threads>] [-phred33 | -phred64] [-trimlog <logFile>] <input> <output> <step 1> ... or java -classpath <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticSE [-threads <threads>] [-phred33 | -phred64] [-trimlog <logFile>] <input> <output> <step 1> ... If no quality score is specified, phred-64 is the default. This will be changed to an 'autodetected' quality score in a future version. Specifying a trimlog file creates a log of all read trimmings, indicating the following details: * the read name * the surviving sequence length * the location of the first surviving base, aka. the amount trimmed from the start * the location of the last surviving base in the original read * the amount trimmed from the end Multiple steps can be specified as required, by using additional arguments at the end. Most steps take one or more settings, delimited by ':' (a colon) Step options: * ILLUMINACLIP:&lt;fastaWithAdaptersEtc>:&lt;seed mismatches>:&lt;palindrome clip threshold>:&lt;simple clip threshold> * fastaWithAdaptersEtc: specifies the path to a fasta file containing all the adapters, PCR sequences etc. The naming of the various sequences within this file determines how they are used. See below. * seedMismatches: specifies the maximum mismatch count which will still allow a full match to be performed * palindromeClipThreshold: specifies how accurate the match between the two 'adapter ligated' reads must be for PE palindrome read alignment. * simpleClipThreshold: specifies how accurate the match between any adapter etc. sequence must be against a read. * SLIDINGWINDOW:&lt;windowSize>:&lt;requiredQuality> * windowSize: specifies the number of bases to average across * requiredQuality: specifies the average quality required. * LEADING:&lt;quality> * quality: Specifies the minimum quality required to keep a base. * TRAILING:&lt;quality> * quality: Specifies the minimum quality required to keep a base. * CROP:&lt;length> * length: The number of bases to keep, from the start of the read. * HEADCROP:&lt;length> * length: The number of bases to remove from the start of the read. * MINLEN:&lt;length> * length: Specifies the minimum length of reads to be kept. # Trimming Order Trimming occurs in the order which the steps are specified on the command line. It is recommended in most cases that adapter clipping, if required, is done as early as possible. # The Adapter Fasta Illumina adapter and other technical sequences are copyrighted by Illumina,but we have been granted permission to distribute them with Trimmomatic. Suggested adapter sequences are provided for TruSeq2 (as used in GAII machines) and TruSeq3 (as used by HiSeq and MiSeq machines), for both single-end and paired-end mode. These sequences have not been extensively tested, and depending on specific issues which may occur in library preparation, other sequences may work better for a given dataset. To make a custom version of fasta, you must first understand how it will be used. Trimmomatic uses two strategies for adapter trimming: Palindrome and Simple With 'simple' trimming, each adapter sequence is tested against the reads, and if a sufficiently accurate match is detected, the read is clipped appropriately. 'Palindrome' trimming is specifically designed for the case of 'reading through' a short fragment into the adapter sequence on the other end. In this approach, the appropriate adapter sequences are 'in silico ligated' onto the start of the reads, and the combined adapter+read sequences, forward and reverse are aligned. If they align in a manner which indicates 'read-through', the forward read is clipped and the reverse read dropped (since it contains no new data). Naming of the sequences indicates how they should be used. For 'Palindrome' clipping, the sequence names should both start with 'Prefix', and end in '/1' for the forward adapter and '/2' for the reverse adapter. All other sequences are checked using 'simple' mode. Sequences with names ending in '/1' or '/2' will be checked only against the forward or reverse read. Sequences not ending in '/1' or '/2' will be checked against both the forward and reverse read. If you want to check for the reverse-complement of a specific sequence, you need to specifically include the reverse-complemented form of the sequence as well, with another name. The thresholds used are a simplified log-likelihood approach. Each matching base adds just over 0.6, while each mismatch reduces the alignment score by Q/10. Therefore, a perfect match of a 12 base sequence will score just over 7, while 25 bases are needed to score 15. As such we recommend values between 7 - 15 for this parameter. For palindromic matches, a longer alignment is possible - therefore this threshold can be higher, in the range of 30. The 'seed mismatch' parameter is used to make alignments more efficient, specifying the maximum base mismatch count in the 'seed' (16 bases). Typical values here are 1 or 2.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/trixiparticlesjl",
            "repo_link": "https://github.com/trixi-framework/TrixiParticles.jl",
            "content": {
                "codemeta": "",
                "readme": "# TrixiParticles.jl [![Docs-stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://trixi-framework.github.io/TrixiParticles.jl/stable) [![Docs-dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://trixi-framework.github.io/TrixiParticles.jl/dev) [![Slack](https://img.shields.io/badge/chat-slack-e01e5a)](https://join.slack.com/t/trixi-framework/shared_invite/zt-sgkc6ppw-6OXJqZAD5SPjBYqLd8MU~g) [![Youtube](https://img.shields.io/youtube/channel/views/UCpd92vU2HjjTPup-AIN0pkg?style=social)](https://www.youtube.com/@trixi-framework) [![CI](https://github.com/trixi-framework/TrixiParticles.jl/actions/workflows/ci.yml/badge.svg)](https://github.com/trixi-framework/TrixiParticles.jl/actions/workflows/ci.yml) [![codecov](https://codecov.io/github/trixi-framework/TrixiParticles.jl/branch/main/graph/badge.svg?token=RDZXYbij0b)](https://codecov.io/github/trixi-framework/TrixiParticles.jl) [![SciML Code Style](https://img.shields.io/static/v1?label=code%20style&message=SciML&color=9558b2&labelColor=389826)](https://github.com/SciML/SciMLStyle) [![License: MIT](https://img.shields.io/badge/License-MIT-success.svg)](https://opensource.org/licenses/MIT) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10797541.svg)](https://zenodo.org/doi/10.5281/zenodo.10797541) [![DOI](https://joss.theoj.org/papers/10.21105/joss.07044/status.svg)](https://doi.org/10.21105/joss.07044) <p align=\"center\"> <img src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/10238714/479ff0c6-3c65-44fe-b3e0-2ed653e7e3a5\" alt=\"TrixiP_logo\" width=\"40%\"/> </p> **TrixiParticles.jl** is a high-performance numerical simulation framework for particle-based methods, focused on the simulation of complex multiphysics problems, and written in [Julia](https://julialang.org). TrixiParticles.jl focuses on the following use cases: - Accurate and efficient physics-based modelling of complex multiphysics problems. - Development of new particle-based methods and models. - Easy setup of accessible simulations for educational purposes, including student projects, coursework, and thesis work. It offers intuitive configuration, robust pre- and post-processing, and vendor-agnostic GPU-support based on the Julia package [KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl). [![YouTube](https://github.com/user-attachments/assets/dc2be627-a799-4bfd-9226-2077f737c4b0)](https://www.youtube.com/watch?v=V7FWl4YumcA&t=4667s) ## Features - Incompressible Navier-Stokes - Methods: Weakly Compressible Smoothed Particle Hydrodynamics (WCSPH), Entropically Damped Artificial Compressibility (EDAC) - Models: Surface Tension, Open Boundaries - Solid-body mechanics - Methods: Total Lagrangian SPH (TLSPH), Discrete Element Method (DEM) - Fluid-Structure Interaction - Particle sampling of complex geometries from `.stl` and `.asc` files. - Output formats: - VTK - Support for GPUs by Nvidia, AMD and Apple (experimental) ## Examples We provide several example simulation setups in the `examples` folder (which can be accessed from Julia via `examples_dir()`). <table align=\"center\" border=\"0\"> <tr> </tr> <tr> <td align=\"center\"> <img src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/10238714/683e9363-5705-49cc-9a5c-3b47d73ea4b8\" style=\"width: 80% !important;\"/><br><figcaption>2D Dam Break</figcaption> </td> <td align=\"center\"> <img src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/10238714/c10faddf-0400-47c9-b225-f5d286a8ecb8\" style=\"width: 80% !important;\"/><br><figcaption>Moving Wall</figcaption> </td> </tr> <tr> </tr> <tr> <td align=\"center\"> <img src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/10238714/e05ace63-e330-441a-a391-eda3d2764074\" style=\"width: 80% !important;\"/><br><figcaption>Oscillating Beam</figcaption> </td> <td align=\"center\"> <img src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/10238714/ada0d554-e0ba-44ed-923d-2b77ef252258\" style=\"width: 80% !important;\"/><br><figcaption>Dam Break with Elastic Plate</figcaption> </td> </tr> </table> ## Installation If you have not yet installed Julia, please [follow the instructions for your operating system](https://julialang.org/downloads/platform/). TrixiParticles.jl works with Julia v1.10 and newer. We recommend using the latest stable release of Julia. ### For users TrixiParticles.jl is a registered Julia package. You can install TrixiParticles.jl, [OrdinaryDiffEq.jl](https://github.com/SciML/OrdinaryDiffEq.jl) (used for time integration) and [Plots.jl](https://github.com/JuliaPlots/Plots.jl) by executing the following commands in the Julia REPL: ```julia julia> using Pkg julia> Pkg.add([\"TrixiParticles\", \"OrdinaryDiffEq\", \"Plots\"]) ``` ### For developers If you plan on editing TrixiParticles.jl itself, you can download TrixiParticles.jl to a local folder and use the code from the cloned directory: ```bash git clone git@github.com:trixi-framework/TrixiParticles.jl.git cd TrixiParticles.jl mkdir run julia --project=run -e 'using Pkg; Pkg.develop(PackageSpec(path=\".\"))' # Add TrixiParticles.jl to `run` project julia --project=run -e 'using Pkg; Pkg.add([\"OrdinaryDiffEq\", \"Plots\"])' # Add additional packages ``` If you installed TrixiParticles.jl this way, you always have to start Julia with the `--project` flag set to your `run` directory, e.g., ```bash julia --project=run ``` from the TrixiParticles.jl root directory. Further details can be found in the [documentation](https://trixi-framework.github.io/TrixiParticles.jl/stable). ## Usage In the Julia REPL, first load the package TrixiParticles.jl. ```jldoctest getting_started julia> using TrixiParticles ``` Then start the simulation by executing ```jldoctest getting_started; filter = r\".*\"s julia> trixi_include(joinpath(examples_dir(), \"fluid\", \"hydrostatic_water_column_2d.jl\")) ``` This will open a new window with a 2D visualization of the final solution: <img src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/44124897/95821154-577d-4323-ba57-16ef02ea24e0\" width=\"400\"> Further details can be found in the [documentation](https://trixi-framework.github.io/TrixiParticles.jl/stable). ## Documentation You can find the documentation for the latest release [here](https://trixi-framework.github.io/TrixiParticles.jl/stable). ## Publications ## Cite Us If you use TrixiParticles.jl in your own research or write a paper using results obtained with the help of TrixiParticles.jl, please cite it as ```bibtex @misc{trixiparticles, title={{T}rixi{P}articles.jl: {P}article-based multiphysics simulations in {J}ulia}, author={Erik Faulhaber and Niklas Neher and Sven Berger and Michael Schlottke-Lakemper and Gregor Gassner}, year={2024}, howpublished={\\url{https://github.com/trixi-framework/TrixiParticles.jl}}, doi={10.5281/zenodo.10797541} } ``` and ```bibtex @article{neher2025trixiparticles, author = {Niklas S. Neher and Erik Faulhaber and Sven Berger and Gregor J. Gassner and Michael Schlottke-Lakemper}, title = {TrixiParticles.jl: Particle-based multiphysics simulation in Julia}, journal = {Journal of Open Source Software}, volume = {10}, number = {105}, pages = {7044}, year = {2025}, publisher = {Open Journals}, doi = {10.21105/joss.07044}, url = {https://joss.theoj.org/papers/10.21105/joss.07044}, issn = {2475-9066} } ``` ## Authors Erik Faulhaber (University of Cologne) and Niklas Neher (HLRS) implemented the foundations for TrixiParticles.jl and are principal developers along with Sven Berger (hereon). The project was started by Michael Schlottke-Lakemper (University of Augsburg) and Gregor Gassner (University of Cologne), who provide scientific direction and technical advice. The full list of contributors can be found in [AUTHORS.md](AUTHORS.md). ## License and contributing TrixiParticles.jl is licensed under the MIT license (see [LICENSE.md](LICENSE.md)). Since TrixiParticles.jl is an open-source project, we are very happy to accept contributions from the community. Please refer to [CONTRIBUTING.md](CONTRIBUTING.md) for more details. Note that we strive to be a friendly, inclusive open-source community and ask all members of our community to adhere to our [`CODE_OF_CONDUCT.md`](CODE_OF_CONDUCT.md). To get in touch with the developers, [join us on Slack](https://join.slack.com/t/trixi-framework/shared_invite/zt-sgkc6ppw-6OXJqZAD5SPjBYqLd8MU~g) or [create an issue](https://github.com/trixi-framework/TrixiParticles.jl/issues/new). ## Acknowledgments <p align=\"center\"> <img align=\"middle\" src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/44124897/05132bf1-180f-4228-b30a-37dfb6e36ed5\" width=20%/>&nbsp;&nbsp;&nbsp; <img align=\"middle\" src=\"https://github.com/trixi-framework/TrixiParticles.jl/assets/44124897/ae2a91d1-7c10-4e0f-8b92-6ed1c43ddc28\" width=20%/>&nbsp;&nbsp;&nbsp; </p> The project has benefited from funding from [hereon](https://www.hereon.de/) and [HiRSE](https://www.helmholtz-hirse.de/).\n",
                "dependencies": "name = \"TrixiParticles\" uuid = \"66699cd8-9c01-4e9d-a059-b96c86d16b3a\" authors = [\"erik.faulhaber <44124897+efaulhaber@users.noreply.github.com>\"] version = \"0.2.8-dev\" [deps] Adapt = \"79e6a3ab-5dfb-504d-930d-738a2a938a0e\" CSV = \"336ed68f-0bac-5ca0-87d4-7b16caf5d00b\" DataFrames = \"a93c6f00-e57d-5684-b7b6-d8193f3e46c0\" Dates = \"ade2ca70-3891-5945-98fb-dc099432e06a\" DelimitedFiles = \"8bb1440f-4735-579b-a4ab-409b98df4dab\" DiffEqCallbacks = \"459566f4-90b8-5000-8ac3-15dfb0a30def\" FastPow = \"c0e83750-1142-43a8-81cf-6c956b72b4d1\" FileIO = \"5789e2e9-d7fb-5bc7-8068-2c6fae9b9549\" ForwardDiff = \"f6369f11-7733-5829-9624-2563aa707210\" GPUArraysCore = \"46192b85-c4d5-4398-a991-12ede77f4527\" JSON = \"682c06a0-de6a-54ab-a142-c8b1cf79cde6\" KernelAbstractions = \"63c18a36-062a-441e-b654-da1e3ab1ce7c\" LinearAlgebra = \"37e2e46d-f89d-539d-b4ee-838fcccc9c8e\" MuladdMacro = \"46d2c3a1-f734-5fdb-9937-b9b9aeba4221\" PointNeighbors = \"1c4d5385-0a27-49de-8e2c-43b175c8985c\" Polyester = \"f517fe37-dbe3-4b94-8317-1923a5111588\" Printf = \"de0858da-6303-5e67-8744-51eddeeeb8d7\" Random = \"9a3f8284-a2c9-5f02-9a11-845980a1fd5c\" RecipesBase = \"3cdcf5f2-1ef4-517c-9805-6587b60abb01\" Reexport = \"189a3867-3050-52da-a836-e630ba90ab69\" SciMLBase = \"0bca4576-84f4-4d90-8ffe-ffa030f20462\" StaticArrays = \"90137ffa-7385-5640-81b9-e52037218182\" StrideArrays = \"d1fa6d79-ef01-42a6-86c9-f7c551f8593b\" TimerOutputs = \"a759f4b9-e2f1-59dc-863e-4aeb61b1ea8f\" TrixiBase = \"9a0f1c46-06d5-4909-a5a3-ce25d3fa3284\" WriteVTK = \"64499a7a-5c06-52f2-abe2-ccb03c286192\" [weakdeps] OrdinaryDiffEq = \"1dea7af3-3e70-54e6-95c3-0bf5283fa5ed\" OrdinaryDiffEqCore = \"bbf590c4-e513-4bbe-9b18-05decba2e5d8\" [extensions] TrixiParticlesOrdinaryDiffEqExt = [\"OrdinaryDiffEq\", \"OrdinaryDiffEqCore\"] [compat] Adapt = \"4\" CSV = \"0.10\" DataFrames = \"1.6\" DelimitedFiles = \"1\" DiffEqCallbacks = \"4\" FastPow = \"0.1\" FileIO = \"1\" ForwardDiff = \"0.10\" GPUArraysCore = \"0.2\" JSON = \"0.21\" KernelAbstractions = \"0.9\" MuladdMacro = \"0.2\" OrdinaryDiffEq = \"6.91\" OrdinaryDiffEqCore = \"1\" PointNeighbors = \"0.4.8\" Polyester = \"0.7.10\" RecipesBase = \"1\" Reexport = \"1\" SciMLBase = \"2\" StaticArrays = \"1\" StrideArrays = \"0.1\" TimerOutputs = \"0.5.25\" TrixiBase = \"0.1.5\" WriteVTK = \"1.21.2\" julia = \"1.10\"\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/tsmp",
            "repo_link": "https://github.com/HPSCTerrSys/TSMP",
            "content": {
                "codemeta": "",
                "readme": "# Terrestrial System Modeling Platform - TSMP [![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/HPSCTerrSys/TSMP/RenderMasterSphinxDocumentation.yml?label=documentation)](https://hpscterrsys.github.io/TSMP/index.html) [![Latest release](https://img.shields.io/github/v/tag/HPSCTerrSys/TSMP.svg?color=brightgreen&label=latest%20release&sort=semver)](https://github.com/HPSCTerrSys/TSMP/tags) [![GitHub last commit](https://img.shields.io/github/last-commit/HPSCTerrSys/TSMP)](https://github.com/HPSCTerrSys/TSMP/commits/master) [![Twitter Follow](https://img.shields.io/twitter/follow/HPSCTerrSys?style=social)](https://twitter.com/HPSCTerrSys) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.8283715.svg)](https://doi.org/10.5281/zenodo.8283715) ## Introduction The Terrestrial System Modeling Platform (TSMP or TerrSysMP, https://www.terrsysmp.org) is an open source scale-consistent, highly modular, massively parallel regional Earth system model. TSMP essentially consists of an interface which couples dedicated versions of the Consortium for Small-scale Modeling ([COSMO](http://www.cosmo-model.org)) or ICOsahedral Nonhydrostatic ([ICON](https://code.mpimet.mpg.de/projects/iconpublic)) atmospheric model in NWP or climate mode, the Community Land Model ([CLM](http://www.cesm.ucar.edu/models/clm/)), and the hydrologic model [ParFlow](https://www.parflow.org) through the [OASIS3](https://oasis.cerfacs.fr/en/)-[MCT](https://www.mcs.anl.gov/research/projects/mct/) coupler. TSMP allows for a physically-based representation of transport processes of mass, energy and momentum and interactions between the different compartments of the geo-ecosystem across scales, explicitly reproducing feedbacks in the hydrological cycle from the groundwater into the atmosphere. TSMP is extensively used for idealized and real data process and sensitivity studies in water cycle research, for climate change simulations, data assimilation studies including reanalyses, as well as experimental real time forecasting and monitoring simulations, ranging from individual catchments to continental model domains. TSMP runs on notebooks as well on latest supercomputers using a range of compilers. TSMP development has been driven by groups within the [Center for High-Performance Scientific Computing in Terrestrial Systems](http://www.hpsc-terrsys.de) (HPSC-TerrSys), as part of the [Geoverbund ABC/J](http://www.geoverbund-abcj.de/geoverbund/EN/Home/home_node.html), the geoscientific network of the University of Cologne, Bonn University, RWTH Aachen University, and the Research Centre Jülich. The current team is anchored in Jülich and Bonn in Germany. **Visit** **https://www.terrsysmp.org** **for information on the features of TSMP, ongoing developments, citation, usage examples, links to documentation, the team, contact information and publications.** ## Quick Start on Linux Please see [getting started section](https://hpscterrsys.github.io/TSMP/content/gettingstarted.html) for guided steps on how the model can be setup and configured for *one* specific experiment, which we use as one of the default test cases. To get an overview on possible TSMP applications refer to the [TSMP website](https://www.terrsysmp.org) and the [TSMP documention](https://hpscterrsys.github.io/TSMP/index.html). ## TSMP version history The model components used in TSMP are OASIS3-MCT v2, COSMO v5.01, CLM v3.5, ParFlow 3.2 for TSMP versions v1.2.1, v1.2.2 and v1.2.3, ParFlow 3.9 for version v1.3.3 and ParFlow 3.12 for version v1.4.0. TSMP supports ParFlow 3.7 onwards from version v1.3.3 onward. ## Citing TSMP If you use TSMP in a publication, please cite the these papers that describe the model's basic functionalities: * Shrestha, P., Sulis, M., Masbou, M., Kollet, S., and Simmer, C. (2014). A Scale-Consistent Terrestrial Systems Modeling Platform Based on COSMO, CLM, and ParFlow. Monthly Weather Review, 142(9), 3466-3483. doi:[10.1175/MWR-D-14-00029.1](https://dx.doi.org/10.1175/MWR-D-14-00029.1). * Gasper, F., Goergen, K., Kollet, S., Shrestha, P., Sulis, M., Rihani, J., and Geimer, M. (2014). Implementation and scaling of the fully coupled Terrestrial Systems Modeling Platform (TerrSysMP) in a massively parallel supercomputing environment &ndash; a case study on JUQUEEN (IBM Blue Gene/Q). Geoscientific Model Development, 7(5), 2531-2543. doi:[10.5194/gmd-7-2531-2014](https://dx.doi.org/10.5194/gmd-7-2531-2014). ## License TSMP is open source software and is licensed under the [MIT-License](https://github.com/HPSCTerrSys/TSMP/blob/master/LICENSE).\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ukis-csmask",
            "repo_link": "https://github.com/dlr-eoc/ukis-csmask",
            "content": {
                "codemeta": "",
                "readme": "# [![UKIS](img/ukis-logo.png)](https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-5413/10560_read-21914/) ukis-csmask ![ukis-csmask](https://github.com/dlr-eoc/ukis-csmask/workflows/ukis-csmask/badge.svg) [![codecov](https://codecov.io/gh/dlr-eoc/ukis-csmask/branch/main/graph/badge.svg)](https://codecov.io/gh/dlr-eoc/ukis-csmask) ![Upload Python Package](https://github.com/dlr-eoc/ukis-csmask/workflows/Upload%20Python%20Package/badge.svg) [![PyPI version](https://img.shields.io/pypi/v/ukis-csmask)](https://pypi.python.org/pypi/ukis-csmask/) [![GitHub license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE) [![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://black.readthedocs.io/en/stable/) [![DOI](https://zenodo.org/badge/328616234.svg)](https://zenodo.org/badge/latestdoi/328616234) UKIS Cloud Shadow MASK (ukis-csmask) package masks clouds and cloud shadows in Sentinel-2, Landsat-9, Landsat-8, Landsat-7 and Landsat-5 images. Masking is performed with a pre-trained convolution neural network. It is fast and works with both Level-1C (no atmospheric correction) and Level-2A (atmospherically corrected) data. Images just need to be in reflectance and include at least the \"blue\", \"green\", \"red\" and \"nir\" spectral bands. Best performance (in terms of accuracy and speed) is achieved when images also include \"swir16\" and \"swir22\" spectral bands and are resampled to approximately 30 m spatial resolution. This [publication](https://doi.org/10.1016/j.rse.2019.05.022) provides further insight into the underlying algorithm and compares it to the widely used [Fmask](http://www.pythonfmask.org/en/latest/) algorithm across a heterogeneous test dataset. > Wieland, M.; Li, Y.; Martinis, S. Multi-sensor cloud and cloud shadow segmentation with a convolutional neural network. *Remote Sensing of Environment*, 2019, 230, 1-12. [https://doi.org/10.1016/j.rse.2019.05.022](https://doi.org/10.1016/j.rse.2019.05.022) This [publication](https://doi.org/10.5194/isprs-archives-XLIII-B3-2022-217-2022) introduces the Python package, performs additional evaluation on recent cloud and cloud shadow benchmark datasets and tests the applicability of ukis-csmask on Landsat-9 imagery. > Wieland, M.; Fichtner, F.; Martinis, S. UKIS-CSMASK: A Python package for multi-sensor cloud and cloud shadow segmentation. *Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.*, 2022, XLIII-B3-2022, 217-222. [https://doi.org/10.5194/isprs-archives-XLIII-B3-2022-217-2022](https://doi.org/10.5194/isprs-archives-XLIII-B3-2022-217-2022) If you use ukis-csmask in your work, please consider citing one of the above publications. ![Examples](img/examples.png) ## Example Here's an example on how to compute a cloud and cloud shadow mask from an image. Please note that here we use [ukis-pysat](https://github.com/dlr-eoc/ukis-pysat) for convencience image handling, but you can also work directly with [numpy](https://numpy.org/) arrays. Further examples can be found [here](examples). ````python from ukis_csmask.mask import CSmask from ukis_pysat.raster import Image, Platform # read Level-1C image from file, convert digital numbers to TOA reflectance # and make sure resolution is 30 m to get best performance # NOTE: band_order must match the order of bands in the input image. it does not have to be in this explicit order. band_order = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"] img = Image(data=\"sentinel2.tif\", dimorder=\"last\") img.dn2toa(platform=Platform.Sentinel2, wavelength=band_order) img.warp(resampling_method=0,resolution=30,dst_crs=img.dataset.crs) # compute cloud and cloud shadow mask csmask = CSmask(img=img.arr, product_level=\"l1c\", band_order=band_order, nodata_value=0) # access cloud and cloud shadow mask csmask_csm = csmask.csm # access valid mask csmask_valid = csmask.valid # convert results to UKIS-pysat Image csmask_csm = Image(csmask.csm, transform=img.dataset.transform, crs=img.dataset.crs, dimorder=\"last\") csmask_valid = Image(csmask.valid, transform=img.dataset.transform, crs=img.dataset.crs, dimorder=\"last\") # write results back to file csmask_csm.write_to_file(\"sentinel2_csm.tif\", dtype=\"uint8\", compress=\"PACKBITS\") csmask_valid.write_to_file(\"sentinel2_valid.tif\", dtype=\"uint8\", compress=\"PACKBITS\", kwargs={\"nbits\":2}) ```` ## Accuracy assessment The original ukis-csmask models, which are available in [ukis-csmask<=v0.2.2](https://github.com/dlr-eoc/ukis-csmask/releases/tag/v0.2.2) and are described in this [publication](https://doi.org/10.1016/j.rse.2019.05.022), have been trained and tested on a custom reference dataset specifically for Level-1C data. From [ukis-csmask>=v1.0.0](https://github.com/dlr-eoc/ukis-csmask/releases/tag/v1.0.0) on, we provide new models for Level-1C (L1C) and Level-2A (L2A) data, which have been trained on a much larger reference dataset (consisting of [SPARCS](https://www.usgs.gov/landsat-missions/spatial-procedures-automated-removal-cloud-and-shadow-sparcs-validation-data), [CloudSEN12+](https://cloudsen12.github.io/) and some additional custom samples). Both datasets natively only provide L1C images. Therefore, we have compiled corresponding L2A images for each sample. ![Accuracy](img/accuracy.png) Above barplot compares the new [ukis-csmask>=v1.0.0](https://github.com/dlr-eoc/ukis-csmask/releases/tag/v1.0.0) models against the previous [ukis-csmask<=v0.2.2](https://github.com/dlr-eoc/ukis-csmask/releases/tag/v0.2.2) models on [CloudSEN12+](https://cloudsen12.github.io/) and [SPARCS](https://www.usgs.gov/landsat-missions/spatial-procedures-automated-removal-cloud-and-shadow-sparcs-validation-data) test splits for both L1C and L2A images. The results indicate the superior performance of the new [ukis-csmask>=v1.0.0](https://github.com/dlr-eoc/ukis-csmask/releases/tag/v1.0.0) models against the previous [ukis-csmask<=v0.2.2](https://github.com/dlr-eoc/ukis-csmask/releases/tag/v0.2.2) models across all tested datasets and product levels. Providing separate models for each product level provides further improvements and enables greater flexibiliy. ## Installation The easiest way to install ukis-csmask is through pip. To install ukis-csmask with [default CPU provider](https://onnxruntime.ai/docs/execution-providers/) run the following. ```shell pip install ukis-csmask[cpu] ``` To install ukis-csmask with [OpenVino support](https://onnxruntime.ai/docs/execution-providers/OpenVINO-ExecutionProvider.html) for enhanced CPU inference run the following instead. ```shell pip install ukis-csmask[openvino] ``` To install ukis-csmask with [GPU support](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html) run the following instead. This requires that you have a GPU with CUDA runtime libraries installed on the system. ```shell pip install ukis-csmask[gpu] ``` ukis-csmask depends on [onnxruntime](https://onnxruntime.ai/). For a list of additional dependencies check the [requirements](https://github.com/dlr-eoc/ukis-csmask/blob/main/requirements.txt). ## Contributors The UKIS team creates and adapts libraries which simplify the usage of satellite data. Our team includes (in alphabetical order): * Boehnke, Christian * Fichtner, Florian * Mandery, Nico * Martinis, Sandro * Riedlinger, Torsten * Wieland, Marc German Aerospace Center (DLR) ## Licenses This software is licensed under the [Apache 2.0 License](https://github.com/dlr-eoc/ukis-csmask/blob/main/LICENSE). Copyright (c) 2020 German Aerospace Center (DLR) * German Remote Sensing Data Center * Department: Geo-Risks and Civil Security ## Changelog See [changelog](https://github.com/dlr-eoc/ukis-csmask/blob/main/CHANGELOG.rst). ## Contributing The UKIS team welcomes contributions from the community. For more detailed information, see our guide on [contributing](https://github.com/dlr-eoc/ukis-csmask/blob/main/CONTRIBUTING.md) if you're interested in getting involved. ## What is UKIS? The DLR project Environmental and Crisis Information System (the German abbreviation is UKIS, standing for [Umwelt- und Kriseninformationssysteme](https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-5413/10560_read-21914/) aims at harmonizing the development of information systems at the German Remote Sensing Data Center (DFD) and setting up a framework of modularized and generalized software components. UKIS is intended to ease and standardize the process of setting up specific information systems and thus bridging the gap from EO product generation and information fusion to the delivery of products and information to end users. Furthermore the intention is to save and broaden know-how that was and is invested and earned in the development of information systems and components in several ongoing and future DFD projects.\n",
                "dependencies": "numpy scipy\n#!/usr/bin/env python3 import codecs import os from setuptools import setup, find_packages with open(r\"README.md\", encoding=\"utf8\") as f: long_description = f.read() def read(rel_path): here = os.path.abspath(os.path.dirname(__file__)) with codecs.open(os.path.join(here, rel_path), \"r\") as fp: return fp.read() def get_version(rel_path): for line in read(rel_path).splitlines(): if line.startswith(\"__version__\"): delim = '\"' if '\"' in line else \"'\" return line.split(delim)[1] else: raise RuntimeError(\"Unable to find version string.\") setup( name=\"ukis-csmask\", version=get_version(os.path.join(\"ukis_csmask\", \"__init__.py\")), url=\"https://github.com/dlr-eoc/ukis-csmask\", author=\"German Aerospace Center (DLR)\", author_email=\"ukis-helpdesk@dlr.de\", license=\"Apache 2.0\", description=\"masks clouds and cloud shadows in Sentinel-2, Landsat-8, Landsat-7 and Landsat-5 images\", zip_safe=False, packages=find_packages(), install_requires=open(\"requirements.txt\").read().splitlines(), extras_require={ \"cpu\": [ \"onnxruntime\", ], \"gpu\": [ \"onnxruntime-gpu\", ], \"openvino\": [ \"onnxruntime-openvino\", ], \"dev\": [ \"pytest\", ], }, classifiers=[ \"Programming Language :: Python :: 3\", \"Operating System :: OS Independent\", \"Development Status :: 4 - Beta\", \"License :: OSI Approved :: Apache Software License\", \"Topic :: Scientific/Engineering :: GIS\", ], python_requires=\">=3.8\", long_description=long_description, long_description_content_type=\"text/markdown\", include_package_data=True, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ultimodel",
            "repo_link": "https://github.com/DLR-VF/ULTImodel",
            "content": {
                "codemeta": "",
                "readme": "# ULTImodel [![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/DLR-VF/ULTImodel/blob/master/LICENSE) [![PyPI version](https://badge.fury.io/py/ultimodel.svg)](https://pypi.python.org/pypi/ultimodel) [![Documentation Status](https://readthedocs.org/projects/ultimodel/badge/?version=latest)](https://ultimodel.readthedocs.io/en/latest/?badge=latest) [![Cite-us](https://img.shields.io/badge/doi-10.5281%2Fzenodo.7826486-blue)](https://doi.org/10.5281/zenodo.7826486) **ULTImodel** &mdash; A universal transport distribution model written in Python. ## Description **ULTImodel** is a distribution model that helps to spatially distribute road-based transport for countries, including border-crossing travel. It is set up using open data like [OSM](https://openstreetmap.org). The software includes modules for network generation, trip generation and trip distribution based on two main inputs: * Georeferenced traffic analysis zones (TAZ) for the respective region * Target value for national transport volume (i.e. person-kilometres or tonne-kilometres) ![Prim_Sec](ultimodel-mkdocs/docs/images/readme_visual_fr.png \"Results of distribution and secondary model\") ## Installation The __current version__ is [ultimodel-1.0.0](https://github.com/DLR-VF/ULTImodel/releases/tag/1.0.0). You may __install ULTImodel__ by executing the following __pip__ ```console python -m pip install ultimodel ``` You may __download a copy or fork the code__ at [ULTImodel&apos;s github page]([link-to-github](https://github.com/DLR-VF/ULTImodel)). Besides, you may __download the current release__ here: * [ultimodel-1.0.1.zip](https://github.com/DLR-VF/ULTImodel/archive/refs/tags/1.0.1.zip) * [ultimodel-1.0.1.tar.gz](https://github.com/DLR-VF/ULTImodel/archive/refs/tags/1.0.1.tar.gz) ## Usage Examples of using **ULTImodel** can be found in the [tutorials repository](https://github.com/DLR-VF/ULTImodel-tutorials). Additional documentation can be found at <https://ultimodel.readthedocs.io/>. ## Authors and acknowledgment **ULTImodel** was developed by Nina Thomsen. We want to thank the following persons for the help during **ULTImodel's** development: Lars Hedemann, Simon Metzler, Gernot Liedtke, Christian Winkler, Tudor Mocanu, and Daniel Krajzewicz. ## License **ULTImodel** is licensed under the [MIT license](https://github.com/DLR-VF/ULTImodel/blob/master/LICENSE). ## Links Please find further information on the web: * The complete documentation is located at <https://ultimodel.readthedocs.io/> * The github repository is located at <https://github.com/DLR-VF/ULTImodel> * The issue tracker is located at <https://github.com/DLR-VF/ULTImodel/issues> ## Legal Please find the legal information here: <https://github.com/DLR-VF/ULTImodel/blob/master/ultimodel-mkdocs/docs/legal.md>\n",
                "dependencies": "pandas numpy geopandas shapely networkx osmnx tqdm requests scikit-learn scipy jinja2==3.0.3 mkdocs mkdocs-material mkdocstrings-python mkdocstrings\n# ========================================================= # setup.py # @author Nina Thomsen # @date 20.03.2023 # @copyright Institut fuer Verkehrsforschung, # Deutsches Zentrum fuer Luft- und Raumfahrt # @brief setup module for ULTIMO # ========================================================= import setuptools with open(\"README.md\", \"r\", encoding=\"utf-8\") as fh: long_description = fh.read() ''' import required packages from requirements file with open(\"requirements.txt\") as f: INSTALL_REQUIRES = [line.strip() for line in f.readlines()]''' setuptools.setup( name='ultimodel', version='1.0.1', author='German Aerospace Center - DLR (Nina Thomsen)', author_email='nina.thomsen@dlr.de', description='Universal transport distribution model', long_description=long_description, long_description_content_type=\"text/markdown\", url='https://github.com/DLR-VF/ULTImodel', project_urls = { \"Documentation\": 'https://ultimodel.readthedocs.io/', \"Source\": 'https://github.com/DLR-VF/ULTImodel', \"Bug Tracker\": \"https://github.com/DLR-VF/ULTImodel/issues \" }, license='MIT', packages=['ultimodel'], install_requires=['pandas', 'numpy', 'geopandas', 'shapely', 'osmnx', 'networkx', 'tqdm', 'requests', 'scikit-learn', 'scipy'] )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/ultramassexplorer-ume",
            "repo_link": "https://gitlab.awi.de/bkoch/ume",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/unicore",
            "repo_link": "https://github.com/UNICORE-EU/",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/uqtestfuns",
            "repo_link": "https://github.com/casus/uqtestfuns",
            "content": {
                "codemeta": "",
                "readme": "# UQTestFuns [![JOSS](https://img.shields.io/badge/JOSS-10.21105/joss.05671-brightgreen?style=flat-square)](https://doi.org/10.21105/joss.05671) [![DOI](http://img.shields.io/badge/DOI-10.5281/zenodo.14710452-blue.svg?style=flat-square)](https://doi.org/10.5281/zenodo.14710452) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black) [![Python 3.7](https://img.shields.io/badge/python-3.7-blue.svg?style=flat-square)](https://www.python.org/downloads/release/python-370/) [![License](https://img.shields.io/github/license/damar-wicaksono/uqtestfuns?style=flat-square)](https://choosealicense.com/licenses/mit/) [![PyPI](https://img.shields.io/pypi/v/uqtestfuns?style=flat-square)](https://pypi.org/project/uqtestfuns/) | Branches | Status | |:--------------------------------------------------------------------------:|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | [`main`](https://github.com/damar-wicaksono/uqtestfuns/tree/main) (stable) | ![build](https://img.shields.io/github/actions/workflow/status/damar-wicaksono/uqtestfuns/main.yml?branch=main&style=flat-square) [![codecov](https://img.shields.io/codecov/c/github/damar-wicaksono/uqtestfuns/main?logo=CodeCov&style=flat-square&token=Y6YQEPJ1TT)](https://app.codecov.io/gh/damar-wicaksono/uqtestfuns/tree/main) [![Docs](https://readthedocs.org/projects/uqtestfuns/badge/?version=stable&style=flat-square)](https://uqtestfuns.readthedocs.io/en/stable/?badge=stable) | | [`dev`](https://github.com/damar-wicaksono/uqtestfuns/tree/dev) (latest) | ![build](https://img.shields.io/github/actions/workflow/status/damar-wicaksono/uqtestfuns/main.yml?branch=dev&style=flat-square) [![codecov](https://img.shields.io/codecov/c/github/damar-wicaksono/uqtestfuns/dev?logo=CodeCov&style=flat-square&token=Y6YQEPJ1TT)](https://app.codecov.io/gh/damar-wicaksono/uqtestfuns/tree/dev) [![Docs](https://readthedocs.org/projects/uqtestfuns/badge/?version=latest&style=flat-square)](https://uqtestfuns.readthedocs.io/en/latest/?badge=latest) | <!--One paragraph description--> UQTestFuns is an open-source Python3 library of test functions commonly used within the applied uncertainty quantification (UQ) community. Specifically, the package provides: - an implementation _with minimal dependencies_ (i.e., NumPy and SciPy) and _a common interface_ of many test functions available in the UQ literature - a _single entry point_ collecting test functions _and_ their probabilistic input specifications in a single Python package - an _opportunity for an open-source contribution_, supporting the implementation of new test functions or posting reference results. In short, UQTestFuns is an homage to the [Virtual Library of Simulation Experiments (VLSE)](https://www.sfu.ca/~ssurjano/). ## Usage UQTestFuns includes several commonly used test functions in the UQ community. To list the available functions: ```python-repl >>> import uqtestfuns as uqtf >>> uqtf.list_functions() +-------+-------------------------------+-----------+------------+----------+---------------+--------------------------------+ | No. | Constructor | # Input | # Output | Param. | Application | Description | +=======+===============================+===========+============+==========+===============+================================+ | 1 | Ackley() | M | 1 | True | optimization, | Optimization test function | | | | | | | metamodeling | from Ackley (1987) | +-------+-------------------------------+-----------+------------+----------+---------------+--------------------------------+ | 2 | Alemazkoor20D() | 20 | 1 | False | metamodeling | High-dimensional low-degree | | | | | | | | polynomial from Alemazkoor & | | | | | | | | Meidani (2018) | +-------+-------------------------------+-----------+------------+----------+---------------+--------------------------------+ | 3 | Alemazkoor2D() | 2 | 1 | False | metamodeling | Low-dimensional high-degree | | | | | | | | polynomial from Alemazkoor & | | | | | | | | Meidani (2018) | +-------+-------------------------------+-----------+------------+----------+---------------+--------------------------------+ | 4 | Borehole() | 8 | 1 | False | metamodeling, | Borehole function from Harper | | | | | | | sensitivity | and Gupta (1983) | +-------+-------------------------------+-----------+------------+----------+---------------+--------------------------------+ ... ``` Consider the Borehole function, a test function commonly used for metamodeling and sensitivity analysis purposes; to create an instance of this test function: ```python-repl >>> my_testfun = uqtf.Borehole() >>> print(my_testfun) Function ID : Borehole Input Dimension : 8 (fixed) Output Dimension : 1 Parameterized : False Description : Borehole function from Harper and Gupta (1983) Applications : metamodeling, sensitivity ``` The probabilistic input specification of this test function is built-in: ```python-repl >>> print(my_testfun.prob_input) Function ID : Borehole Input ID : Harper1983 Input Dimension : 8 Description : Probabilistic input model of the Borehole model from Harper and Gupta (1983) Marginals : No. Name Distribution Parameters Description ----- ------ -------------- --------------------- ----------------------------------------------- 1 rw normal [0.1 0.0161812] radius of the borehole [m] 2 r lognormal [7.71 1.0056] radius of influence [m] 3 Tu uniform [ 63070. 115600.] transmissivity of upper aquifer [m^2/year] 4 Hu uniform [ 990. 1100.] potentiometric head of upper aquifer [m] 5 Tl uniform [ 63.1 116. ] transmissivity of lower aquifer [m^2/year] 6 Hl uniform [700. 820.] potentiometric head of lower aquifer [m] 7 L uniform [1120. 1680.] length of the borehole [m] 8 Kw uniform [ 9985. 12045.] hydraulic conductivity of the borehole [m/year] Copulas : Independence ``` A sample of input values can be generated from the input model: ```python-repl >>> xx = my_testfun.prob_input.get_sample(10) array([[8.40623544e-02, 2.43926544e+03, 8.12290909e+04, 1.06612711e+03, 7.24216436e+01, 7.78916695e+02, 1.13125867e+03, 1.02170796e+04], [1.27235295e-01, 3.28026293e+03, 6.36463631e+04, 1.05132831e+03, 6.81653728e+01, 8.17868370e+02, 1.16603931e+03, 1.09370944e+04], [8.72711602e-02, 7.22496512e+02, 9.18506063e+04, 1.06436843e+03, 6.44306474e+01, 7.74700231e+02, 1.46266808e+03, 1.12531788e+04], [1.22301709e-01, 2.29922122e+02, 8.00390345e+04, 1.05290108e+03, 1.10852262e+02, 7.94709283e+02, 1.28026313e+03, 1.01879077e+04], ... ``` ...and used to evaluate the test function: ```python-repl >>> yy = my_testfun(xx) array([ 57.32635774, 110.12229548, 53.10585812, 96.15822154, 58.51714875, 89.40068404, 52.61710076, 61.47419171, 64.18005235, 79.00454634]) ``` ## Installation You can obtain UQTestFuns directly from PyPI using `pip`: ```bash $ pip install uqtestfuns ``` Alternatively, you can also install the latest version from the source: ```bash pip install git+https://github.com/damar-wicaksono/uqtestfuns.git ``` > **NOTE**: UQTestFuns is currently work in progress, > therefore interfaces are subject to change. It's a good idea to install the package in an isolated virtual environment. ## Getting help <!--Getting help--> For a getting-started guide on UQTestFuns, please refer to the [Documentation](https://uqtestfuns.readthedocs.io/en/latest/). The documentation also includes details on each of the available test functions. For any other questions related to the package, post your questions on the GitHub Issue page. ## Package development and contribution <!--Package Development--> UQTestFuns is under ongoing development; any contribution to the code (for example, a new test function) and the documentation (including new reference results) are welcomed! Please consider the [Contribution Guidelines](CONTRIBUTING.MD) first, before making a pull request. ## Citing UQTestFuns If you use this package in your research or projects, please consider citing both the associated paper and the Zenodo archive (for the specific version used). ### Citing the paper (JOSS) The citation of the paper associated with this package is: ```bibtex @article{Wicaksono2023, author = {Wicaksono, Damar and Hecht, Michael}, title = {{UQTestFuns}: A {Python3} library of uncertainty quantification ({UQ}) test functions}, journal = {Journal of Open Source Software}, year = {2023}, volume = {8}, number = {90}, doi = {10.21105/joss.05671}, } ``` ### Citing a specific version (Zenodo) To ensure reproducibility, cite the exact version of the package you used. Each release is archived on Zenodo with a unique DOI; find and use the DOI for the version you used at [Zenodo]. The citation for the current public version is: ```bibtex @software{UQTestFuns_0_6_0, author = {Wicaksono, Damar and Hecht, Michael}, title = {{UQTestFuns: A Python3 Library of Uncertainty Quantification (UQ) Test Functions}}, month = jan, year = 2025, publisher = {Zenodo}, version = {v0.6.0}, doi = {10.5281/zenodo.14710452}, url = {https://doi.org/10.5281/zenodo.14710452} } ``` ## Credits and contributors <!--Credits and contributors--> This work was partly funded by the [Center for Advanced Systems Understanding (CASUS)](https://www.casus.science/) which is financed by Germany's Federal Ministry of Education and Research (BMBF) and by the Saxony Ministry for Science, Culture and Tourism (SMWK) with tax funds on the basis of the budget approved by the Saxony State Parliament. UQTestFuns is currently maintained by: - Damar Wicaksono ([HZDR/CASUS](https://www.casus.science/)) under the Mathematical Foundations of Complex System Science Group led by Michael Hecht ([HZDR/CASUS](https://www.casus.science/)) at CASUS. ## License <!--License--> UQTestFuns is released under the [MIT License](LICENSE). [Zenodo]: https://zenodo.org/search?q=parent.id%3A7701903&f=allversions%3Atrue&l=list&p=1&s=10&sort=version\n",
                "dependencies": "[build-system] requires = [\"setuptools\", \"wheel\"] build-backend = \"setuptools.build_meta\" [tool.black] line-length = 79 target-version = [\"py37\", \"py38\", \"py39\", \"py310\"]\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/urbem-urban-emission-downscaling-for-air-quality-modeling",
            "repo_link": "https://github.com/martinottopaul/UrbEm",
            "content": {
                "codemeta": "",
                "readme": "# UrbEm v1.0.0 The Urban Emission downscaling model (UrbEm)for air quality modeling 0. Introduction The use of regional emission inventories can be challenging for urban-scale AQ applications and air quality management in cities. Nevertheless, their exploitation through disaggregation by utilizing spatial proxies is a credible solution for European cities that lack bottom-up emission inventories. To this end, we developed the UrbEm approach, which enables in a modular manner downscaling of gridded regional emissions with specific spatial proxies based on a variety of open access, robust, sustainable and frequently updated sources. UrbEm can be applied to any urban area in Europe and provides methodological homogeneity between different cities. To demonstrate the general applicability and performance of the developed method and tool, we introduced the method, and compared the spatial distribution of uniformly disaggregated regional emissions with emissions downscaled with the UrbEm approach for the differing cities of Athens and Hamburg (manuscript submitted for publication, pre-print accessible on request). The UrbEm downscaling approach is completely free of cost and open source. Its application is realized in (1) a series of R scripts and (2) as Pyhton script. Both applications rely on e.g. CAMS-REG emission inventories as well as a set (maps) of spatial proxies, which need to be downloaded before using UrbEm. 1. Access necessary input data 1.1. Emission datasets UrbEm v1.0 is configured to read CAMS-REG-AP v3.1 regional emissions. After registration, these can be downloaded free of cost at: https://eccad.aeris-data.fr After registration in the \"Access Data\" section, the CAMS-REG-AP dataset should be selected and downloaded for all pollutants and sectors. The second emission database applied is the European Pollutant Release and Transfer Register E-PRTR, which can be downloaded without registration at: https://www.eea.europa.eu/data-and-maps/data/member-states-reporting-art-7-under-the-european-pollutant-release-and-transfer-register-e-prtr-regulation-23/european-pollutant-release-and-transfer-register-e-prtr-data-base Both datasets should be placed in separate folders. 1.2. Spatial proxies Besides a collection of spatial proxies (download here: https://doi.org/10.5281/zenodo.5508739), which have been specifically prepared for application in UrbEm, the Global Human Settlement Layer (download here: https://ghsl.jrc.ec.europa.eu/ghs_pop2019.php) Population density product \"GHS_POP_E2015_GLOBE_R2019A_4326_30SS_V1_0\" needs to be downloaded. Make sure all proxies are placed in one folder. 2. Apply UrbEm v1.0 Both solutions (R and Python) are configured to (1) read CAMS-REG-AP v3.1 and E-PRTR emission input files, (2) downscale these gridded and point emissions with the downloaded set of spatial proxies, (3) to arrive at annual total emissions for a selected year and a selected urban domain, (4) and write these as area, line and/or point source emission files, (5) in a *.csv file format for the EPISODE-CityChem preprocessor UECT (Karl et al. 2019). Although UrbEm v1.0 delivers only UECT/EPISODE-CityChem file format as output, it is generally possible to change to the desired output format by code modification. Nevertheless, we promote to use the EPISODE-CityChem air quality model for urban-scales due to its efficiency, performance and ongoing development. EPISODE-CityChem can be downloaded free of cost at https://doi.org/10.5281/zenodo.1116173. 2.1. UrbEm Rscripts The UrbEm v1.0 Rscripts are separated in three main scripts: 1_UrbEm_pointsources_v1.R 2_UrbEm_areasources_v2.R 3_UrbEm_linesources_v3. These scripts need to be run sequentially to create point, area and line emission files. Before running the scripts, make sure the R libraries raster, sf, rgdal, osmdata and lwgeom are installed in your R environment. Addtionally the following auxiliary functions (scripts distributed with UrbEm v1.0) are necessary and will be sourced at the beginning of some main scripts: areasources_e-prtr_pointsource_correction.R areasources_to_osm_linesources.R proxy_distribution.R proxy_preparation.R While there are no changes in the auxiliary scripts necessary to run UrbEm, there need to be changes made in the main scripts. Each of the main scripts has an input section at the beginning, which needs to be adjusted, for e.g.: - setting input folders of emission files and proxies - setting output folders and output text strings - setting a domain definition - setting downscaling options The input section of each main script, as well as the code itself delivers a documentation of each step made in the script. Feel free to adjust the code for your purposes. 2.2. UrbEm Python scripts The UrbEm v1.0 Python scripts are separated in two main scripts: - 1_UrbEM_proxies_v1.py - 2_UrbEM_emissions_v1.py These scripts need to be run sequentially to 1. create proxy, point, area and line emission files. Before running the scripts, make sure the python libraries pandas, numpy, gdal, geopandas, os, sys, fnmatch, inspect, rasterio, rasterio.mask, earthpy.spatial, shapely.geometry, earthpy, fiona, osgeo, gc, geotable, pyproj, shapely, time, shutil, OSMPythonTools.nominatim, OSMPythonTools.overpass, OSMPythonTools.data, collections and shapefile are installed in your Python v3 environment. Spatial datasets: In order to be able to run the python scripts the user should also download: - Population density data (Global dataset/ 2015 / WGS84 / 30 arcsec): https://ghsl.jrc.ec.europa.eu/download.php?ds=pop - CORINE raster and GDB files: https://land.copernicus.eu/pan-european/corine-land-cover/clc2018 - E-PRTR kmz data: https://www.eea.europa.eu/data-and-maps/data/member-states-reporting-art-7-under-the-european-pollutant-release-and-transfer-register-e-prtr-regulation-23/e-prtr-facilities-kmz-format/eprtr_facilities_v9.kmz - E-PRTR csv data: https://www.eea.europa.eu/data-and-maps/data/member-states-reporting-art-7-under-the-european-pollutant-release-and-transfer-register-e-prtr-regulation-23/european-pollutant-release-and-transfer-register-e-prtr-data-base/eprtr_v9_csv.zip - Urban Center data: https://ghsl.jrc.ec.europa.eu/ghs_stat_ucdb2015mt_r2019a.php - Eurostat countries: https://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/countries - Shipping Routes Each of the main scripts has an input section at the beginning, which needs to be adjusted, for e.g.: - setting input folders of emission files and proxies - setting a domain definition - setting downscaling options The input section of each script, as well as the code itself delivers a documentation of each step made in the script. Feel free to adjust the code for your purposes.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/urmoac",
            "repo_link": "https://github.com/DLR-VF/UrMoAC",
            "content": {
                "codemeta": "",
                "readme": "# UrMoAC # ![logo.png](https://raw.githubusercontent.com/DLR-VF/UrMoAC/master/logo.png) UrMoAC [![License: EPL2](https://img.shields.io/badge/license-EPL2-green)](https://github.com/DLR-VF/UrMoAC/blob/master/LICENSE.md) [![DOI](https://img.shields.io/badge/doi-10.5281%2Fzenodo.13234444-blue)](https://doi.org/10.5281/zenodo.13234444) [![Documentation Status](https://readthedocs.org/projects/urmoac/badge/?version=latest)](https://urmoac.readthedocs.io/en/latest/?badge=latest) ![Build Status](https://github.com/DLR-VF/UrMoAC/actions/workflows/maven_build.yml/badge.svg) &ldquo;Urban Mobility Accessibility Computer&rdquo; or &ldquo;UrMoAC&rdquo; is a tool for computing accessibility measures, supporting aggregation, variable limits, and intermodal paths. It is a scientific tool. This version of the documentation describes the current development version. You should use one of the available [releases](https://github.com/DLR-VF/UrMoAC/releases). The according documentation can be found at [readthedocs](http://urmoac.readthedocs.io) or within the release itself. What the tool basically does is to load a set of origin locations and a set of destination locations as well as a road network and optionally a description of the public transport offer. Then, it iterates over all loaded origins and computes the respective accessibility measure for each of them by routing to all destinations within the defined limit. Optionally, areas by which the origins and destinations shall be aggregated may be loaded. Some features: * input is read from databases or files; * variable origins / destinations; * variable aggregation options; * weights for origins and destinations; * flexible limits for search: max. time, max. distance, max. number, max. seen value, nearest only; * support for different transport modes, as well as intermodal accessibilities; * GTFS-based public transport accessibility computation; * possibility to read time-dependent travel times (for motorised individual traffic); * support for data preparation and visualisation. ## Documentation The complete documentation is located at <http://urmoac.readthedocs.io>. It should cover different versions. When using one of the releases, you should consult the included documentation as the information below describes the current state of the development. Please consult the section *Links* below for further information sources. ## Installation **UrMoAC** is written in the [Java](https://www.java.com/) programming language. You need [Java](https://www.java.com/) to run it. The easiest way to install it is to download the .jar-file from the latest [release](https://github.com/DLR-VF/UrMoAC/releases). Further possibilities to run it are given at [Installation](https://github.com/DLR-VF/UrMoAC/blob/master/docs/mkdocs/Installation.md). ## Usage examples A most basic call may look as following: ```console java -jar UrMoAC.jar --from origins.csv --to destinations.csv --net network.csv --od-output nm_output.csv --mode bike --time 0 --epsg 0 ``` Which would compute the accessibility of the destinations stored in ```destinations.csv``` starting at the origins stored in ```origins.csv``` along the road network stored in ```network.csv``` for the transport mode bike. Information about the used file formats are given at [Input Data Formats](https://github.com/DLR-VF/UrMoAC/blob/master/docs/mkdocs/InputDataFormats.md). ## License **UrMoAC** is licensed under the [Eclipse Public License 2.0](LICENSE.md). **When using it, please cite it as:** Daniel Krajzewicz, Dirk Heinrichs and Rita Cyganski (2017) [_Intermodal Contour Accessibility Measures Computation Using the 'UrMo Accessibility Computer'_](https://elib.dlr.de/118235/). International Journal On Advances in Systems and Measurements, 10 (3&4), Seiten 111-123. IARIA. And / or use the DOI: [![DOI](https://img.shields.io/badge/doi-10.5281%2Fzenodo.13234444-blue)](https://doi.org/10.5281/zenodo.13234444) (v0.8.2) ## Support and Contribution **UrMoAC** is under active development and we are happy about any interaction with users or dvelopers. ## Authors **UrMoAC** has been developed at the [Institute of Transport Research](http://www.dlr.de/vf) of the [German Aerospace Center](http://www.dlr.de). ## Links You may find further information about **UrMoAC** at the following pages: * a complete documentation is located at <http://urmoac.readthedocs.io>; * the recent as well as the previous releases can be found at <https://github.com/DLR-VF/UrMoAC/releases>; * the source code repository is located at <https://github.com/DLR-VF/UrMoAC>; * the issue tracker is located at <https://github.com/DLR-VF/UrMoAC/issues>; * you may start a discussion or join an existing one at <https://github.com/DLR-VF/UrMoAC/discussions>. ## Legal Please find additional legal information at [Legal](https://github.com/DLR-VF/UrMoAC/blob/master/docs/mkdocs/Legal.md).\n",
                "dependencies": "<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <groupId>de.dlr.ivf.urmo</groupId> <artifactId>UrMoAC</artifactId> <version>0.8.2</version> <packaging>jar</packaging> \t<properties> \t\t<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> \t\t<maven.compiler.release>17</maven.compiler.release> \t</properties> <build> <defaultGoal>install</defaultGoal> <sourceDirectory>src</sourceDirectory> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.6.0</version> \t\t\t\t<configuration> \t\t\t\t<source>1.17</source> \t\t\t\t<target>1.17</target> \t\t\t\t</configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-shade-plugin</artifactId> <version>3.2.1</version> <executions> <execution> <goals> <goal>shade</goal> </goals> <configuration> <shadedArtifactAttached>true</shadedArtifactAttached> <transformers> <transformer implementation= \"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\"> <mainClass>de.dlr.ivf.urmo.UrMoAccessibilityComputer</mainClass> </transformer> <!-- transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/ --> </transformers> <filters> <filter> <artifact>*:*</artifact> <excludes> <exclude>META-INF/*.SF</exclude> <exclude>META-INF/*.DSA</exclude> <exclude>META-INF/*.RSA</exclude> </excludes> </filter> </filters> </configuration> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-jar-plugin</artifactId> <version>3.1.0</version> <configuration> <archive> <manifest> <addClasspath>true</addClasspath> <classpathPrefix>lib/</classpathPrefix> <mainClass>de.dlr.ivf.urmo.UrMoAccessibilityComputer</mainClass> </manifest> </archive> </configuration> </plugin> </plugins> </build> <organization> <name>German Aerospace Center (DLR) - Institute of Transport Research</name> <url>https://www.dlr.de/vf/en</url> </organization> <name>UrMoAC</name> <url>https://github.com/DLR-VF/UrMoAC</url> <description>A tool for computing accessibility measures, supporting aggregation, variable limits, and intermodality. </description> <repositories> <repository> <id>osgeo-alt</id> <url>https://repo.osgeo.org/repository/release/</url> </repository> <repository> <id>geotoolkit</id> <url>https://maven.geotoolkit.org</url> </repository> <repository> <id>Atlassian 3rdParty Repository</id> <name>Atlassian 3rdParty Repository</name> <url>https://packages.atlassian.com/maven-3rdparty/</url> </repository> <repository> <id>Clojars</id> <name>Clojars Repository</name> <url>http://clojars.org/repo/</url> </repository> </repositories> <dependencies> <dependency> <groupId>com.opencsv</groupId> <artifactId>opencsv</artifactId> </dependency> <dependency> <groupId>javax.measure</groupId> <artifactId>jsr-275</artifactId> </dependency> <dependency> <groupId>javax.media</groupId> <artifactId>jai_core</artifactId> </dependency> <dependency> <groupId>javax.vecmath</groupId> <artifactId>vecmath</artifactId> </dependency> <dependency> <groupId>org.jdom</groupId> <artifactId>jdom2</artifactId> </dependency> <dependency> <groupId>jgridshift</groupId> <artifactId>jgridshift</artifactId> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-core</artifactId> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-api</artifactId> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-data</artifactId> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-epsg-extension</artifactId> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-epsg-hsql</artifactId> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-geopkg</artifactId> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-main</artifactId> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-metadata</artifactId> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-opengis</artifactId> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-referencing</artifactId> </dependency> <dependency> <groupId>org.postgresql</groupId> <artifactId>postgresql</artifactId> </dependency> <dependency> <groupId>org.postgis</groupId> <artifactId>postgis-stubs</artifactId> </dependency> <dependency> <groupId>org.postgis</groupId> <artifactId>postgis-jdbc</artifactId> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-simple</artifactId> </dependency> <dependency> <groupId>org.xerial</groupId> <artifactId>sqlite-jdbc</artifactId> </dependency> </dependencies> <dependencyManagement> <dependencies> <dependency> <groupId>com.opencsv</groupId> <artifactId>opencsv</artifactId> <version>5.8</version> </dependency> <dependency> <groupId>commons-pool</groupId> <artifactId>commons-pool</artifactId> <version>1.5.4</version> </dependency> <dependency> <groupId>javax.media</groupId> <artifactId>jai_core</artifactId> <version>1.1.3</version> </dependency> <dependency> <groupId>javax.media</groupId> <artifactId>javax.media</artifactId> <version>1.1.3</version> </dependency> <dependency> <groupId>javax.vecmath</groupId> <artifactId>vecmath</artifactId> <version>1.5.2</version> </dependency> <dependency> <groupId>org.jdom</groupId> <artifactId>jdom2</artifactId> <version>2.0.6.1</version> </dependency> <dependency> <groupId>jgridshift</groupId> <artifactId>jgridshift</artifactId> <version>1.0</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-core</artifactId> <version>2.17.1</version> </dependency> <dependency> <groupId>javax.measure</groupId> <artifactId>jsr-275</artifactId> <version>1.0.0</version> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-api</artifactId> <version>20.5</version> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-data</artifactId> <version>20.5</version> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-epsg-hsql</artifactId> <version>25.2</version> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-epsg-extension</artifactId> <version>25.2</version> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-geopkg</artifactId> <version>25.2</version> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-main</artifactId> <version>25.2</version> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-metadata</artifactId> <version>25.2</version> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-opengis</artifactId> <version>25.2</version> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-referencing</artifactId> <version>25.2</version> </dependency> <dependency> <groupId>org.locationtech.jts</groupId> <artifactId>jts</artifactId> <version>1.18.2</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version>1.6.3</version> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-simple</artifactId> <version>1.6.3</version> </dependency> <dependency> <groupId>org.postgresql</groupId> <artifactId>postgresql</artifactId> <version>42.4.4</version> </dependency> <dependency> <groupId>org.postgis</groupId> <artifactId>postgis-stubs</artifactId> <version>1.3.3</version> </dependency> <dependency> <groupId>org.postgis</groupId> <artifactId>postgis-jdbc</artifactId> <version>1.3.3</version> </dependency> <dependency> <groupId>org.postgis</groupId> <artifactId>postgis</artifactId> <version>1.3.3</version> </dependency> <dependency> <groupId>org.postgis</groupId> <artifactId>postgis-main</artifactId> <version>1.3.3</version> </dependency> <dependency> <groupId>org.xerial</groupId> <artifactId>sqlite-jdbc</artifactId> <version>3.41.2.2</version> </dependency> </dependencies> </dependencyManagement> </project>\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/utile-oxy",
            "repo_link": "https://github.com/andyco98/UTILE-Oxy/",
            "content": {
                "codemeta": "",
                "readme": "# *UTILE-Oxy* - Deep Learning to Automate Video Analysis of Bubble Dynamics in Proton Exchange Membrane Electrolyzers ![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/workflow.png) We present an automated workflow using deep learning for the analysis of videos containing oxygen bubbles in PEM electrolyzers by 1. preparing an annotated dataset and training models in order to conduct semantic segmentation of bubbles and 2. automating the extraction of bubble properties for further distribution analysis. The publication [UTILE-Oxy - Deep Learning to Automate Video Analysis of Bubble Dynamics in Proton Exchange Membrane Electrolyzers](https://pubs.rsc.org/en/content/articlelanding/2024/cp/d3cp05869g) is available in an open access fashion on the journal PCCP for further information! ## Description This project focuses on the deep learning-based automatic analysis of polymer electrolyte membrane water electrolyzers (PEMWE) oxygen evolution videos. This repository contains the Python implementation of the UTILE-Oxy software for automatic video analysis, feature extraction, and plotting. The models we present in this work are trained on a specific use-case scenario of interest in oxygen bubble evolution videos of transparent cells. It is possible to fine-tune, re-train or employ another model suitable for your individual case if your data has a strong visual deviation from the presented data here, which was recorded and shown as follows: ![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/figexperiment.png) ## Model's benchmark In our study, we trained several models to compare their prediction performance on unseen data. We trained specifically three different models on the same dataset composed by : - Standard U-Net 2D - U-Net 2D with a ResNeXt 101 backbone - Attention U-Net We obtained the following performance results: | Model | Precision [%] | Recall [%] | F1-Score [%] | |---------------------------------|----------------|------------|--------------| | U-Net 2D | 81 | 89 | 85 | | U-Net with ResNeXt101 backbone | 95 | 78 | 86 | | Attention U-Net | 95 | 75 | 84 | Since the F1-Scores are similar a visual inspection was carried out to find the best-performing model : ![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/benchmark.png) But even clearer is the visual comparison of the running videos: ![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/video_results.gif) ## Extracted features ### Time-resolved bubble ratio computation and bubble coverage distribution ![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/timeresolved.png) ### Bubble position probability density map ![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/heatmaps.png) ### Individual bubble shape analysis ![](https://github.com/andyco98/UTILE-Oxy/blob/main/images/individualcorrect.png) # *NEW!* UTILE-Oxy Tunnel Vision 3D Spatiotemporal Tracking is Available With the use of image registration techniques, the extracted segmentation masks are spatiotemporally connected, augmented into a 3D volume and individual bubbles are tracked. This opens up new analysis possibilities that weren't available before. Here are some examples: ### Accumulated count of newly emerged bubbles over time ![](https://github.com/alpcanaras/UTILE-Oxy-ALP-FORKED/blob/main/images/New%20Bubbles%20vs.%20Time%201500mV.png) ### Accumulated count of bubble coalescence events ![](https://github.com/alpcanaras/UTILE-Oxy-ALP-FORKED/blob/main/images/Merge%20Events%20vs.%20Time%201500mV.png) ### Denisty map for newly emerging bubble locations ![](https://github.com/alpcanaras/UTILE-Oxy-ALP-FORKED/blob/main/images/Bubble%20Emerging%20Points%20Heatmap%20for%201500mV.png) ### Density map for bubble coalescence locations ![](https://github.com/alpcanaras/UTILE-Oxy-ALP-FORKED/blob/main/images/Bubble%20Merging%20Points%20Density%20Map%20for%201500mV.png) ### Bubble trajectory analysis and mapping ![](https://github.com/alpcanaras/UTILE-Oxy-ALP-FORKED/blob/main/images/trajectory_1500mV_start_end_withavg_and_std_velocity.png) ### Bubble movement metrics extraction This metrics are calculated and extracted separately for singular (never merging) and merge (that are a results of a coalescence) bubbles and written into a .csv file. ![](https://github.com/alpcanaras/UTILE-Oxy-ALP-FORKED/blob/main/images/Metrics%20CSV.png) ### Bubble coverage lifecycle analysis This denotes the distribution of coverage duration for related bubbles throughout the video, indicating the time spent in the flow field. ![](https://github.com/alpcanaras/UTILE-Oxy-ALP-FORKED/blob/main/images/lifecycle_histogram_1500mV.png) ## Installation In order to run the actual version of the code, the following steps need to be done: - Clone the repository - Create a new environment using Anaconda using Python 3.9 - Pip install the jupyter notebook library ``` pip install notebook ``` - From your Anaconda console open jupyter notebook (just tip \"jupyter notebook\" and a window will pop up) - Open the /UTILE-Oxy/UTILE-Oxy_prediction.ipynb file from the jupyter notebook directory - Further instructions on how to use the tool are attached to the code with examples in the juypter notebook ## Dependencies The following libraries are needed to run the program: ``` pip install opencv-python numpy patchify pillow segmentation_models keras tensorflow==2.13.1 matplotlib scikit-learn pandas seaborn tifffile scipy scikit-image pathlib ``` ### Notes The datasets used for training and the trained model are available at Zenodo: https://doi.org/10.5281/zenodo.10184579.\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/varfish",
            "repo_link": "https://github.com/bihealth/varfish-server",
            "content": {
                "codemeta": "",
                "readme": "[![Documentation Status](https://readthedocs.org/projects/varfish-server/badge/?version=latest)](https://varfish-server.readthedocs.io/en/latest/?badge=latest) [![Code Coverage](https://codecov.io/gh/varfish-org/varfish-server/branch/main/graph/badge.svg?token=5ZACSH5MZZ)](https://codecov.io/gh/varfish-org/varfish-server) [![image](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT) # VarFish **Comprehensive DNA variant analysis for diagnostics and research.** This is the repository for the web server component. Holtgrewe, M.; Stolpe, O.; Nieminen, M.; Mundlos, S.; Knaus, A.; Kornak, U.; Seelow, D.; Segebrecht, L.; Spielmann, M.; Fischer-Zirnsak, B.; Boschann, F.; Scholl, U.; Ehmke, N.; Beule, D. *VarFish: Comprehensive DNA Variant Analysis for Diagnostics and Research*. Nucleic Acids Research 2020, gkaa241. <https://doi.org/10.1093/nar/gkaa241>. ## Getting Started - [VarFish Homepage](https://www.cubi.bihealth.org/software/varfish/) - [Manual](https://varfish-server.readthedocs.io/en/latest/) - [Installation Instructions](https://varfish-server.readthedocs.io/en/latest/admin_install.html). - [Docker Compose Installer](https://github.com/varfish-org/varfish-docker-compose#run-varfish-server-using-docker-compose). ## VarFish Repositories - [varfish-server](https://github.com/varfish-org/varfish-server) The VarFish Server is the web frontend used by the end users / data analysts. - [varfish-annotator](https://github.com/varfish-org/varfish-annotator) The VarFish Annotator is a command line utility used for annotating VCF files and converting them to files that can be imported into VarFish Server. - [varfish-cli](https://github.com/varfish-org/varfish-cli) The VarFish Command Line Interface allows to import data through the VarFish REST API. - [varfish-db-downloader](https://github.com/varfish-org/varfish-db-downloader) The VarFish DB Downloader is a command line tool for downloading the background database. - [varfish-docker-compose](https://github.com/varfish-org/varfish-docker-compose) Quickly get started running a VarFish server by using Docker Compose. We provide a prebuilt data set with some already imported data. ## At a Glance - License: MIT - Dependencies / Tech Stack - Python \\>=3.8 - Django 3 - PostgreSQL \\>=12 GitHub is used for public issue tracking. Currently, development happens on internal infrastructure. ## VarFish Component Compatibility Table The following combinations have been validated / are supported to work. | VarFish Server | VarFish CLI | VarFish Annotator | | -------------- | ----------- | ----------------- | | v1.2.2 | v0.3.0 | v0.21 | | v1.2.1 | v0.3.0 | v0.21 | | v1.2.0 | v0.3.0 | v0.21 | ## VarFish Data Release Compatibility Table The following combinations have been validated / are supported to work. | VarFish Server | Data Release | VarFish DB Downloader | | -------------- | ------------ | --------------------- | | v1.2.2 | 20210728c | v0.3.\\* | | v1.2.1 | 20210728 | v0.3.\\* | | v1.2.1 | 20210728b | v0.3.\\* | | v1.2.0 | 20210728 | v0.3.\\* | | v1.2.0 | 20210728b | v0.3.\\* |\n",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/vasca",
            "repo_link": "https://github.com/rbuehler/vasca",
            "content": {
                "codemeta": "",
                "readme": "![VASCA icon](docs/images/VASCA_icon.png) [![🧪 pytest](https://github.com/rbuehler/vasca/actions/workflows/ci.yml/badge.svg)](https://github.com/rbuehler/vasca/actions/workflows/ci.yml) [![📚 docs](https://github.com/rbuehler/vasca/actions/workflows/docs.yml/badge.svg)](https://rbuehler.github.io/vasca/) [![🚀 pypi](https://github.com/rbuehler/vasca/actions/workflows/pypi.yml/badge.svg)](https://pypi.org/project/vasca/) # Variable Source Cluster Analysis (VASCA) 1. [Motivation](#motivation) 2. [Pipeline Overview](#pipeline-overview) 3. [Key Features](#key-features) 4. [Proof-of-Principle Study](#proof-of-principle-study) 5. [Documentation and Installation](#documentation-and-installation) 6. [Getting Started](docs/getting_started.md#getting-started) ## Motivation VASCA (Italian for \"bathtub\" 🛁) is a high-performance software package developed to address the challenges of time-domain astronomy, especially given the increasing volume of data from large-scale surveys such as [ZTF](https://en.wikipedia.org/wiki/Zwicky_Transient_Facility), [LSST](https://en.wikipedia.org/wiki/Vera_C._Rubin_Observatory), and [ULTRASAT](https://www.weizmann.ac.il/ultrasat/). Designed to analyze time-variable cosmic sources like active galactic nuclei, stars, and transient events, VASCA provides a modular, scalable solution for integrating data from multiple instruments and conducting a cohesive analysis. ## Pipeline Overview The VASCA analysis pipeline consists of three primary steps: 1. **Spatial Clustering**: Associate detections from repeated observations to unique cosmic sources using [mean-shift](https://en.wikipedia.org/wiki/Mean_shift) clustering. 2. **Statistical Variability Detection**: Identify time-variable sources by testing flux variations against a constant hypothesis at a 5-σ significance level. 3. **Source Classification**: Classify detected sources, including cross-matching with external catalogs (e.g., SIMBAD, Gaia). The main output of the pipeline is a catalog of time-variable cosmic sources, including detailed classifications and cross-matches with existing astronomical databases. ## Key Features - **Simplicity and Modularity**: The software uses a hierarchical data model and modular processing to ensure scalability and ease of use. It supports data from multiple instruments seamlessly. - **Proven Algorithms**: VASCA relies on established algorithms and statistical methods, ensuring robustness and reducing the maintenance burden. - **Focus on Specific Use Case**: Optimized for analyzing time-domain astronomical data, VASCA keeps complexity low, simplifying auditing and debugging. - **Standards Compliance**: Outputs are designed for publication readiness by adhering to IAU and CDS standards, using widely-accepted, non-proprietary data formats. - **Customization and Extensibility**: VASCA allows flexible configuration, making it adaptable to different datasets and instrument-specific requirements. ## Proof-of-Principle Study VASCA was applied to a proof-of-principle study using the Galaxy Evolution Explorer (GALEX) archive (2003-2013). This study produced a catalog of over 4,000 UV-variable sources, revealing UV variability across all classes of stars. Notably, a massive, pulsating white dwarf exhibited unique long-term variability in the UV. The full article including a description of VASCA's pipeline can be found here: [The time-variable ultraviolet sky: Active galactic nuclei, stars, and white dwarfs](https://ui.adsabs.harvard.edu/abs/2024A%26A...687A.313B/abstract). ## Documentation and Installation VASCA is distributed as an open-source package. Comprehensive documentation is available [here](https://rbuehler.github.io/vasca/), including example notebooks and an API reference to help users get started. For quick installation, VASCA can be installed via [PyPI](https://pypi.org/project/vasca/) using: ```shell pip install vasca ``` For more info see the [installation guide](docs/getting_started.md#installation).\n",
                "dependencies": "[build-system] requires = [\"setuptools\", \"setuptools-scm\", \"wheel\", \"build\"] build-backend = \"setuptools.build_meta\" [project] name = \"vasca\" dynamic = [\"version\"] description = \"Ultraviolet Variability Analysis is an astronomy pipeline for time-variable sources.\" readme = \"README.md\" requires-python = \">=3.10\" license = {file = \"LICENSE\"} keywords = [\"astronomy\", \"ultraviolet\", \"pipeline\"] authors = [ {name = \"Julian Schliwinski\", email = \"julian.schliwinski@desy.de\"}, {name = \"Rolf Bühler\", email = \"rolf.buehler@desy.de\"}, ] dependencies = [ \"astropy ~= 5.3\", \"astroquery ~= 0.4.6\", \"coverage ~= 7.2.7\", \"healpy ~= 1.17\", \"loguru ~= 0.7.0\", \"matplotlib ~= 3.7.1\", \"numpy ~= 1.24.3\", \"pandas ~= 2.0.2\", \"pytest ~= 7.3.1\", \"pytest-cov ~= 4.1.0\", \"python-dotenv ~= 1.0.0\", \"PyYAML ~= 6.0\", \"scikit-learn ~= 1.3.0\", \"setuptools ~= 67.8.0\", \"setuptools_scm\", \"regions ~= 0.7\", \"pyyaml-include ~= 1.3\", \"ipywidgets ~= 8.0.6\", \"ipympl ~= 0.9.3\", \"jupyterlab ~= 4.0.2\", \"sphinx ~= 7.2.6\", \"myst-nb\", \"furo\", \"sphinx_autodoc2\", \"sphinx_copybutton\", \"sphinx-tippy\", \"jupytext\", \"itables\", ] [project.urls] homepage = \"https://schliwiju.github.io/vasca-mirror/\" documentation = \"https://schliwiju.github.io/vasca-mirror/\" repository = \"https://github.com/rbuehler/vasca\" changelog = \"https://tbd.desy.de\" [project.scripts] vasca_pipe = \"vasca.vasca_pipe:run_from_file\" [tool.setuptools] [tool.setuptools_scm] write_to = \"vasca/_version.py\" [tool.pytest.ini_options] # Pytest settings (https://docs.pytest.org/en/6.2.x/reference.html#configuration-options) minversion = \"6.0\" addopts = \"-v\" testpaths = [\"vasca/test\"] log_cli = true [tool.isort] # Black-Isort compatibility (https://black.readthedocs.io/en/stable/guides/using_black_with_other_tools.html#isort) # could be replaced by 'profile = \"black\"' for isort versions >=5.0.0 multi_line_output = 3 include_trailing_comma = true force_grid_wrap = 0 use_parentheses = true ensure_newline_before_comments = true line_length = 88 [tool.ruff] # Exclude a variety of commonly ignored directories. exclude = [ \".bzr\", \".direnv\", \".eggs\", \".git\", \".git-rewrite\", \".hg\", \".mypy_cache\", \".nox\", \".pants.d\", \".pytype\", \".ruff_cache\", \".svn\", \".tox\", \".venv\", \"__pypackages__\", \"_build\", \"buck-out\", \"build\", \"dist\", \"node_modules\", \"venv\", ] # Same as Black. line-length = 88 indent-width = 4 # Assume Python 3.11 target-version = \"py311\" [tool.ruff.lint] # Ruff rules docs: https://docs.astral.sh/ruff/rules/ # Selection below from: https://codebase.helmholtz.cloud/hifis/cloud/access-layer/portal/-/blob/4f9600fb942da5eefc2c0d88f70b120ef2b72206/pyproject.toml#L67 select = [ \"F\", \"E\", \"W\", \"I\", \"N\", \"UP\", \"ANN\", \"BLE\", \"FBT\", \"B\", \"A\", \"C4\", \"DTZ\", \"T10\", \"T20\", \"DJ\", \"EM\", \"EXE\", \"ISC\", \"ICN\", \"G\", \"INP\", \"PIE\", \"PYI\", \"PT\", \"Q\", \"RSE\", \"RET\", \"SLF\", \"SIM\", \"TID\", \"TCH\", \"INT\", \"ARG\", \"PTH\", \"ERA\", \"PD\", \"PGH\", \"PL\", \"TRY\", \"NPY\", \"NPY201\", \"RUF\" ] ignore = [\"ANN101\"] # Allow fix for all enabled rules (when `--fix`) is provided. fixable = [\"ALL\"] unfixable = [] # Allow unused variables when underscore-prefixed. dummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\" [tool.ruff.lint.pydocstyle] convention = \"numpy\" [tool.ruff.format] # Like Black, use double quotes for strings. quote-style = \"double\" # Like Black, indent with spaces, rather than tabs. indent-style = \"space\" # Like Black, respect magic trailing commas. skip-magic-trailing-comma = false # Like Black, automatically detect the appropriate line ending. line-ending = \"auto\" [tool.jupytext] formats = \"ipynb,py:percent\" hide_notebook_metadata=true\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/velocityconversion",
            "repo_link": "https://github.com/cmeessen/VelocityConversion",
            "content": {
                "codemeta": "",
                "readme": "# VelocityConversion [![DOI](https://zenodo.org/badge/87794116.svg)](https://zenodo.org/badge/latestdoi/87794116) [![PyPI version](https://badge.fury.io/py/velocityconversion.svg)](https://badge.fury.io/py/velocityconversion) - [VelocityConversion](#velocityconversion) - [Introduction](#introduction) - [Getting started](#getting-started) - [Use the latest version not on PyPI](#use-the-latest-version-not-on-pypi) - [Usage as command line tool](#usage-as-command-line-tool) - [Usage as a Python module](#usage-as-a-python-module) - [Modifying physical properties of the minerals](#modifying-physical-properties-of-the-minerals) - [Contributing](#contributing) - [Citing](#citing) - [References](#references) - [Licence](#licence) ## Introduction This code is a python implementation of the p- and s-wave velocity to density conversion approach after Goes et al. (2000). The implementation was optimised for regular 3D grids using lookup tables instead of Newton iterations. Goes et al. (2000) regard the expansion coefficient as temperature dependent using the relation by Saxena and Shen (1992). In `VelocityConversion`, the user can additionally choose between a constant expansion coefficient or a pressure- and temperature dependent coefficient that was derived from Hacker and Abers (2004). For detailed information on the physics behind the approach have a look at the original paper by Goes et al. (2000). ## Getting started `VelocityConversion` requires Python 3 and numpy. Install `numpy` and `VelocityConversion` by running ```bash pip install numpy velocityconversion ``` To uninstall `VelocityConversion`, run ```bash pip uninstall velocityconversion ``` ### Use the latest version not on PyPI If you want to use the very latest version, or want to [contribute](#contributing), clone the repository to you local hard drive: ```bash git clone https://github.com/cmeessen/VelocityConversion.git ``` or, if you haven an [SSH key](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent) associated to your account: ```bash git clone git@github.com:cmeessen/VelocityConversion.git ``` To check whether everything is working run the tests ```bash python test.py ``` If the output looks like this, everything is working fine: ``` test_vp_AlphaConst (__main__.TestVelocityConversion) ... ok test_vs_AlphaConst (__main__.TestVelocityConversion) ... ok test_vs_AlphaPT (__main__.TestVelocityConversion) ... ok test_vs_AlphaT (__main__.TestVelocityConversion) ... ok ---------------------------------------------------------------------- Ran 4 tests in 1.633s OK ``` ## Usage as command line tool In order to use the code as command line tool, add the `./Examples` directory to your `PATH`, e.g. in your bash profile: ```bash export PATH=/path/to/VelocityConversion/Examples:$PATH ``` Alternatively you can move the bash script [VelocityConversion](./Examples/VelocityConversion) to a place that is within your `PATH`. Now the bash script `VelocityConversion` can be executed: ``` VelocityConversion Usage: VelocityConversion FileIn -type <P|S> [optional args] Optional arguments: -AlphaT -AlphaPT -dT <val> -comp <Filename> -h | --help -NN -out <FileOut> -scaleV <value> -setQ <1|2> -v | -verbose -XFe <val> --version ``` The steps to prepare a conversion are - definition of mantle rock composition in a `*.csv` file using the mineral terminology of [MinDB.csv](./VelocityConversion/MinDB.csv) - provide a velocity distribution on a regular 3D grid where columns are `x y z v` - run `VelocityConversion` specifying the velocity type with `-type P` or `-type S` Working examples for the usage as command line tool are provided in the script [RunExamples.sh](./Examples/RunExamples.sh). ## Usage as a Python module VelocityConversion can also be imported as a Python module. Therefore, navigate to the folder that contains your clone of the repository (and [setup.py](./setup.py)) and execute ```bash pip install -e . ``` Now, the module can be imported to Python: ```python from VelocityConversion import MantleConversion MC = MantleConversion() ``` A short working example for a conversion is: ```python from VelocityConversion import MantleConversion MC = MantleConversion() MC.LoadFile(\"./Examples/VsSL2013.dat\") MC.SetVelType(\"S\") MC.DefaultMineralogy() MC.FillTables() MC.CalcPT() MC.SaveFile(\"./Examples/VsSL2013_out.dat\") ``` For a more complete documentation on how to use `VelocityConversion` as a Python module please visit the [documentation](https://cmeessen.github.io/VelocityConversion/). ## Modifying physical properties of the minerals The database that contains the physical properties of the individual mineral phases is stored in [MinDB.csv](./VelocityConversion/MinDB.csv). Mineral parameters can be edited, or new minerals added. A new mineral phase should then be referred to in the code or the assemblage file using the name that was assigned in the `phase` column of `MinDB.csv`. ## Contributing Please see [CONTRIBUTING.md](./CONTRIBUTING.md) if you want to contribute to `VelocityConversion`. ## Citing If you use this code, please consider citing it as > Meeßen, Christian (2019): \"VelocityConversion (v1.1.2)\". Zenodo, > http://doi.org/10.5281/zenodo.5897455. or refer to [CITATION.cff](./CITATION.cff). ## References Berckhemer, H., W. Kampfmann, E. Aulbach, and H. Schmeling. \"Shear Modulus and Q of Forsterite and Dunite near Partial Melting from Forced-Oscillation Experiments.\" Physics of the Earth and Planetary Interiors, Special Issue Properties of Materials at High Pressures and High Temperatures, 29, no. 1 (July 1, 1982): 30-41. doi:10.1016/0031-9201(82)90135-2. Goes, S., R. Govers, and P. Vacher. \"Shallow Mantle Temperatures under Europe from P and S Wave Tomography.\" Journal of Geophysical Research 105, no. 11 (2000): 153-11. doi:10.1029/1999jb900300. Hacker, Bradley R., and Geoffrey A. Abers. \"Subduction Factory 3: An Excel Worksheet and Macro for Calculating the Densities, Seismic Wave Speeds, and H2O Contents of Minerals and Rocks at Pressure and Temperature.\" Geochemistry, Geophysics, Geosystems 5, no. 1 (January 1, 2004): Q01005. doi:10.1029/2003GC000614. Kennett, B. L. N., E. R. Engdahl, and R. Buland. \"Constraints on Seismic Velocities in the Earth from Traveltimes.\" Geophysical Journal International 122, no. 1 (July 1, 1995): 108-24. doi:10.1111/j.1365-246X.1995.tb03540.x. Saxena, Surendra K., and Guoyin Shen. \"Assessed Data on Heat Capacity, Thermal Expansion, and Compressibility for Some Oxides and Silicates.\" Journal of Geophysical Research: Solid Earth 97, no. B13 (Dezember 1992): 19813-25. doi:10.1029/92JB01555. Schaeffer, A. J., and S. Lebedev. \"Global Shear Speed Structure of the Upper Mantle and Transition Zone.\" Geophysical Journal International 194, no. 1 (July 1, 2013): 417-49. doi:10.1093/gji/ggt095. Sobolev, Stephan V., Hermann Zeyen, Gerald Stoll, Friederike Werling, Rainer Altherr, and Karl Fuchs. \"Upper Mantle Temperatures from Teleseismic Tomography of French Massif Central Including Effects of Composition, Mineral Reactions, Anharmonicity, Anelasticity and Partial Melt.\" Earth and Planetary Science Letters 139, no. 1-2 (März 1996): 147-63. doi:10.1016/0012-821X(95)00238-8. ## Licence Licence: GNU General Public Licence, Version 3, 29 June 2007 Copyright (2017): Christian Meeßen, Potsdam, Germany VelocityConversion is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. VelocityConversion is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a cop y of the GNU General Public License along with this program. If not, see http://www.gnu.org/licenses/.\n",
                "dependencies": "[[source]] url = \"https://pypi.org/simple\" verify_ssl = true name = \"pypi\" [packages] numpy = \"*\" [dev-packages] flake8 = \"*\" pydocstyle = \"*\" ipython = \"*\" pep8 = \"*\" sphinx = \"*\" sphinx-rtd-theme = \"*\" doc8 = \"*\" coverage = \"*\" matplotlib = \"*\" m2r2 = \"*\" pycodestyle = \"*\" [requires] python_version = \"3.9\"\nfrom setuptools import setup from setuptools import find_packages from pkg_resources import resource_filename from VelocityConversion import __version__ as VERSION import versioneer VERSION = versioneer.get_version() # METADATA NAME = 'velocityconversion' MODULE = 'VelocityConversion' AUTHOR = 'Christian Meeßen' AUTHOR_EMAIL = 'christian.meessen@gfz-potsdam.de' URL = 'https://github.com/cmeessen/VelocityConversion' DESCRIPTION = 'Conversion of seismic velocities to temperature and density' try: with open(resource_filename(MODULE, '../README.md'), 'r') as fh: LONG_DESCRIPTION = fh.read() except ImportError: with open('README.md') as fh: LONG_DESCRIPTION = fh.read() LONG_DESCRIPTION_TYPE = 'text/markdown' PACKAGES = [MODULE] PACKAGE_DIR = {MODULE: MODULE} PACKAGE_DATA = {MODULE: ['*.csv']} CLASSIFIERS = [ 'Natural Language :: English', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.7', 'Programming Language :: Python :: 3.8', 'Programming Language :: Python :: 3.9', 'License :: OSI Approved :: GNU General Public License v3 (GPLv3)', 'Operating System :: OS Independent', 'Topic :: Scientific/Engineering :: Physics', ] # DEPENDENCIES INSTALL_REQUIRES = [ 'numpy', ] if __name__ == '__main__': setup( name=NAME, version=VERSION, author=AUTHOR, author_email=AUTHOR_EMAIL, description=DESCRIPTION, long_description=LONG_DESCRIPTION, long_description_content_type=LONG_DESCRIPTION_TYPE, url=URL, packages=PACKAGES, package_dir=PACKAGE_DIR, package_data=PACKAGE_DATA, use_package_data=True, classifiers=CLASSIFIERS, install_requires=INSTALL_REQUIRES, )\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/vencopy",
            "repo_link": "https://gitlab.com/dlr-ve/esy/vencopy/vencopy",
            "content": {
                "codemeta": "",
                "readme": "# Welcome to venco.py! - Authors: Niklas Wulff, Fabia Miorelli - Contact: vencopy@dlr.de # Contents - [Description](#description) - [Installation](#installation) - [Codestyle](#codestyle) - [Documentation](#documentation) - [Useful Links](#useful-links) - [Want to contribute?](#want-to-contribute) ## Description A data processing tool estimating hourly electric demand and flexibility profiles for future electric vehicle fleets. Profiles are targeted to be scalable for the use in large-scale energy system models. ## Installation Depending on if you want to use venco.py or if you want to contribute, there are two different installation procedures described in venco.py's documentation: [I want to apply the tool](https://dlr-ve.gitlab.io/esy/vencopy/vencopy/gettingstarted/installation.html#installation-for-users) [I want to contribute to the codebase, the documentation or the tutorials](https://dlr-ve.gitlab.io/esy/vencopy/vencopy/gettingstarted/installation.html#installation-for-developers) In order to start using venco.py, check out our [tutorials](https://dlr-ve.gitlab.io/esy/vencopy/vencopy/gettingstarted/start.html). For this you won't need any additional data. To run venco.py in full mode, you will need the data set Mobilität in Deutschland (German for \"mobility in Germany\"). You can request it here from the clearingboard transport: https://daten.clearingstelle-verkehr.de/order-form.html Alternatively you can use venco.py with any National Travel Survey or mobility pattern dataset. ## Codestyle We use PEP-8, with the exception of UpperCamelCase for class names. ## Documentation The documentation can be found here: https://dlr-ve.gitlab.io/esy/vencopy/vencopy/ To be able to build the documentation locally on your machine you should additionally install the following three packages in your vencopy environment : sphinx, sphinx_rtd_theme and rst2pdf. After that you can build the documentation locally from a conda bash with the following command: ```python sphinx-build -b html ./docs/ ./build/ ``` ## Useful Links - Documentation: https://dlr-ve.gitlab.io/esy/vencopy/vencopy/ - Source code: https://gitlab.com/dlr-ve/esy/vencopy/vencopy - PyPI release: https://pypi.org/project/vencopy/ - Licence: https://opensource.org/licenses/BSD-3-Clause ## Want to contribute? Please read our contribute section in the documentation and reach out to Fabia (fabia.miorelli@dlr.de). If you experience difficulties on set up or have other technical questions, join our [gitter community](https://gitter.im/vencopy/community)\n",
                "dependencies": "[project] name = \"vencopy\" version = \"1.0.4\" description = \"Vehicle Energy Consumption in Python: A tool to simulate load flexibility of electric vehicle fleets.\" authors = [ { name = \"Fabia Miorelli\", email = \"fabia.miorelli@dlr.de\" }, { name = \"Niklas Wulff\", email = \"niklas.wulff@dlr.de\" } ] readme = \"README.md\" license-expression = \"BSD-3-Clause\" classifiers = [ \"Development Status :: 5 - Production/Stable\", \"Intended Audience :: Science/Research\", \"License :: OSI Approved :: BSD License\", \"Operating System :: Microsoft :: Windows\", \"Operating System :: Unix\", \"Programming Language :: Python\", \"Programming Language :: Python :: 3\", \"Programming Language :: Python :: 3.9\", \"Programming Language :: Python :: 3.10\", \"Programming Language :: Python :: 3.11\", \"Programming Language :: Python :: 3.12\", \"Topic :: Scientific/Engineering\", ] requires-python = \">=3.9\" dependencies = [ \"pandas\", \"click\", \"pyyaml\", \"scipy\", \"matplotlib\", ] [project.urls] Homepage = 'https://gitlab.com/dlr-ve/esy/vencopy/vencopy' Documentation = 'https://dlr-ve.gitlab.io/esy/vencopy/vencopy/index.html' Repository = 'https://gitlab.com/dlr-ve/esy/vencopy/vencopy.git' Gitter = 'https://gitter.im/vencopy/community' [project.optional-dependencies] docs = [ \"sphinx\", \"sphinx-rtd-theme\", \"rst2pdf\" ] test = [\"pytest\", \"coverage\"] dev = [ \"black\", ] [build-system] requires = [\"hatchling>=1.26.1\"] build-backend = \"hatchling.build\" [tool.pytest.ini_options] python_files = [\"test_*.py\", \"*_test.py\", \"tests.py\"] [tool.coverage.run] branch = true source = [\"vencopy\"] [tool.coverage.report] show_missing = true precision = 2\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/vinos",
            "repo_link": "https://codebase.helmholtz.cloud/mussel/netlogo-northsea-species.git",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/vitruvius",
            "repo_link": "https://github.com/vitruv-tools/Vitruv",
            "content": {
                "codemeta": "",
                "readme": "# Vitruv [![GitHub Action CI](https://github.com/vitruv-tools/Vitruv/actions/workflows/ci.yml/badge.svg)](https://github.com/vitruv-tools/Vitruv/actions/workflows/ci.yml) [![Latest Release](https://img.shields.io/github/release/vitruv-tools/Vitruv.svg)](https://github.com/vitruv-tools/Vitruv/releases/latest) [![Issues](https://img.shields.io/github/issues/vitruv-tools/Vitruv.svg)](https://github.com/vitruv-tools/Vitruv/issues) [![License](https://img.shields.io/github/license/vitruv-tools/Vitruv.svg)](https://raw.githubusercontent.com/vitruv-tools/Vitruv/main/LICENSE) [Vitruvius](https://vitruv.tools) is a framework for view-based (software) development. It assumes different models to be used for describing a system, which are automatically kept consistent by the framework executing (semi-)automated rules that preserve consistency. These models are modified only via views, which are projections from the underlying models. For general information on Vitruvius, see our [GitHub Organisation](https://github.com/vitruv-tools) and our [Wiki](https://github.com/vitruv-tools/.github/wiki). This project contains the central Vitruvius framework, providing the definition of a V-SUM (Virtual Single Underlying Model) containing development artifacts to be kept consistent and to be accessed and modified via views. In the implementation, a V-SUM is called `VirtualModel`, which is instantiated with a set of `ChangePropagationSpecifications` (no matter whether they are developed with the [Vitruv-DSLs](https://github.com/vitruv-tools/Vitruv-DSLs) or just as an implementation of the interface defined in the [Vitruv-Change](https://github.com/vitruv-tools/Vitruv-Change) repository). The `VirtualModel` then provides functionality to derive and modify views and to propagate the changes in these views back to the `VirtualModel`, which then executes the `ChangePropagationSpecifications` to preserve consistency. ## Framework-internal Dependencies This project depends on the following other projects from the Vitruvius framework: - [Vitruv-Change](https://github.com/vitruv-tools/Vitruv-Change) ## Module Overview | Name | Description | |--------------|----------------------------------------------------------------------------------------------| | views | Definition of view types on the underlying models. | | vsum | Definition of V-SUMs with consistency preservation rules between meta-models and view types. | | remote | Client-server infrastructure for working with V-SUMs. | | applications | Definition of and registry for V-SUMs. | | *testutils* | *Utilities for testing in Vitruvius or V-SUM projects.* |\n",
                "dependencies": "<?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <!-- Build Parent --> <parent> <groupId>tools.vitruv</groupId> <artifactId>parent</artifactId> <version>3.0.7</version> </parent> <!-- Project Information --> <artifactId>tools.vitruv.framework</artifactId> <version>3.2.0-SNAPSHOT</version> <packaging>pom</packaging> <name>Vitruv Framework</name> <description>The Vitruv framework providing the definition of views and v-sums.</description> <url>https://github.com/vitruv-tools/Vitruv</url> <!-- Organizational Information --> <licenses> <license> <name>Eclipse Public License - v 1.0</name> <url>https://www.eclipse.org/org/documents/epl-v10.php</url> </license> </licenses> <scm> <connection>scm:git:git://github.com/vitruv-tools/Vitruv.git</connection> <developerConnection>scm:git:https://github.com/vitruv-tools/Vitruv.git</developerConnection> <url>https://github.com/vitruv-tools/Vitruv/tree/main</url> </scm> <!-- Modules --> <modules> <module>views</module> <module>vsum</module> <module>testutils</module> <module>applications</module> <module>p2wrappers</module> </modules> <properties> <vitruv-change.version>3.2.0-SNAPSHOT</vitruv-change.version> <!-- SonarQube configuration --> <sonar.host.url>https://sonarcloud.io</sonar.host.url> <sonar.organization>vitruv-tools</sonar.organization> <sonar.projectKey>vitruv-tools_Vitruv</sonar.projectKey> </properties> <!-- Dependency Management --> <dependencyManagement> <dependencies> <!-- Vitruvius dependencies --> <dependency> <groupId>tools.vitruv</groupId> <artifactId>tools.vitruv.change.atomic</artifactId> <version>${vitruv-change.version}</version> </dependency> <dependency> <groupId>tools.vitruv</groupId> <artifactId>tools.vitruv.change.correspondence</artifactId> <version>${vitruv-change.version}</version> </dependency> <dependency> <groupId>tools.vitruv</groupId> <artifactId>tools.vitruv.change.composite</artifactId> <version>${vitruv-change.version}</version> </dependency> <dependency> <groupId>tools.vitruv</groupId> <artifactId>tools.vitruv.change.interaction</artifactId> <version>${vitruv-change.version}</version> </dependency> <dependency> <groupId>tools.vitruv</groupId> <artifactId>tools.vitruv.change.interaction.model</artifactId> <version>${vitruv-change.version}</version> </dependency> <dependency> <groupId>tools.vitruv</groupId> <artifactId>tools.vitruv.change.propagation</artifactId> <version>${vitruv-change.version}</version> </dependency> <dependency> <groupId>tools.vitruv</groupId> <artifactId>tools.vitruv.change.testutils.core</artifactId> <version>${vitruv-change.version}</version> </dependency> <dependency> <groupId>tools.vitruv</groupId> <artifactId>tools.vitruv.change.testutils.integration</artifactId> <version>${vitruv-change.version}</version> </dependency> <dependency> <groupId>tools.vitruv</groupId> <artifactId>tools.vitruv.change.testutils.metamodels</artifactId> <version>${vitruv-change.version}</version> </dependency> <dependency> <groupId>tools.vitruv</groupId> <artifactId>tools.vitruv.change.utils</artifactId> <version>${vitruv-change.version}</version> </dependency> <!-- External dependencies --> <dependency> <groupId>com.google.guava</groupId> <artifactId>guava</artifactId> <version>33.4.8-jre</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-core</artifactId> <version>2.24.3</version> </dependency> <dependency> <groupId>org.eclipse.emf</groupId> <artifactId>org.eclipse.emf.common</artifactId> <version>2.41.0</version> </dependency> <dependency> <groupId>org.eclipse.emf</groupId> <artifactId>org.eclipse.emf.ecore</artifactId> <version>2.38.0</version> </dependency> <dependency> <groupId>org.eclipse.emf</groupId> <artifactId>org.eclipse.emf.ecore.xmi</artifactId> <version>2.38.0</version> </dependency> <dependency> <groupId>org.eclipse.platform</groupId> <artifactId>org.eclipse.core.runtime</artifactId> <version>3.33.0</version> </dependency> <!-- required to mitigate Eclipse dependency signing problems --> <dependency> <groupId>org.eclipse.platform</groupId> <artifactId>org.eclipse.equinox.common</artifactId> <version>3.20.0</version> </dependency> <dependency> <groupId>org.eclipse.platform</groupId> <artifactId>org.eclipse.equinox.registry</artifactId> <version>3.12.300</version> </dependency> <dependency> <groupId>org.eclipse.xtend</groupId> <artifactId>org.eclipse.xtend.lib</artifactId> <version>2.38.0</version> </dependency> <dependency> <groupId>org.eclipse.xtext</groupId> <artifactId>org.eclipse.xtext.xbase.lib</artifactId> <version>2.38.0</version> </dependency> <dependency> <groupId>org.hamcrest</groupId> <artifactId>hamcrest</artifactId> <version>3.0</version> </dependency> <dependency> <groupId>org.junit.jupiter</groupId> <artifactId>junit-jupiter-api</artifactId> <version>5.12.2</version> </dependency> <dependency> <groupId>org.junit.jupiter</groupId> <artifactId>junit-jupiter-params</artifactId> <version>5.12.2</version> </dependency> <dependency> <groupId>org.junit.platform</groupId> <artifactId>junit-platform-commons</artifactId> <version>1.12.2</version> </dependency> <dependency> <groupId>org.junit.platform</groupId> <artifactId>junit-platform-launcher</artifactId> <version>1.12.2</version> </dependency> <dependency> <groupId>org.mockito</groupId> <artifactId>mockito-core</artifactId> <version>5.17.0</version> </dependency> </dependencies> </dependencyManagement> <repositories> <!-- allow snapshots --> <repository> <id>ossrh-snapshots</id> <name>OSSRH Snapshots</name> <url>https://oss.sonatype.org/content/repositories/snapshots</url> <snapshots> <enabled>true</enabled> </snapshots> <releases> <enabled>false</enabled> </releases> </repository> </repositories> </project>\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/voltron",
            "repo_link": "https://github.com/BIMSBbioinfo/VoltRon",
            "content": {
                "codemeta": "",
                "readme": "[![R-CMD-CHECK](https://github.com/BIMSBbioinfo/VoltRon/actions/workflows/check.yml/badge.svg)](https://github.com/BIMSBbioinfo/VoltRon/actions/workflows/check.yml) [![DOI:10.1101/2023.12.15.571667](https://zenodo.org/badge/DOI/10.1101/2023.12.15.571667-x.svg)](https://doi.org/10.1101/2023.12.15.571667) [![Publish Docker image](https://github.com/BIMSBbioinfo/VoltRon/actions/workflows/docker.yml/badge.svg)](https://github.com/BIMSBbioinfo/VoltRon/actions/workflows/docker.yml) # VoltRon ### Spatial omic analysis toolbox for multi-resolution and multi-omic integration using image registration ### [Website](https://bioinformatics.mdc-berlin.de/VoltRon/) | [Tutorials](https://bioinformatics.mdc-berlin.de/VoltRon/tutorials.html) | [Preprint](https://www.biorxiv.org/content/10.1101/2023.12.15.571667v1) ----- ![](https://bimsbstatic.mdc-berlin.de/landthaler/VoltRon/Package/images/voltron_framework_box_io.png) <br> **VoltRon** is a spatial omic analysis toolbox for multi-omics integration using spatial image registration. VoltRon is also capable of analyzing multiple types of spatially-aware data modalities. <ul class=\"maintext2\"> <li style=\"padding-bottom: 10px\"> <strong> Unique data structure </strong> of VoltRon allows users to seamlessly define tissue blocks, layers and multiple assay types in one R object. </li> <li style=\"padding-bottom: 10px\"> <strong> End-to-end downstream data analysis </strong> for distinct spatial biology technologies are supported. VoltRon visualizes and analyzes regions of interests (ROIs), spots, cells, molecules and tiles **(under development)**. </li> <li style=\"padding-bottom: 10px\"> <strong> Automated Image Registration </strong> incorporates <a href=\"https://opencv.org/\">OpenCV</a> (fully embedded into the package using <a href=\"https://www.rcpp.org/\">Rcpp</a>) to detect common features across images and achieves registration. Users may interact with built-in mini shiny apps to change alignment parameters and validate alignment accuracy. </li> <li style=\"padding-bottom: 10px\"> <strong> Manual Image Registration </strong> helps users to select common features across spatial datasets using reference images stored in VoltRon objects. In case automated image registration doesn't work, you can still align images by manually picking landmark points. </li> <li style=\"padding-bottom: 10px\"> <p style=\"padding-bottom: 3px\"> <strong> Spatially Aware Analysis </strong> allows detecting spatial patterns across cells, spots, molecules and other entities. </p> <ul class=\"maintext3\"> <li style=\"padding-bottom: 10px padding-top: 10px\"> <strong>(Niche Clustering: Spots)</strong> VoltRon allows integration to single cell RNA datasets using <a href=\"https://satijalab.org/seurat/\">Seurat</a>, <a href=\"https://www.bioconductor.org/packages/release/bioc/vignettes/SingleCellExperiment/inst/doc/intro.html\">SingleCellExperiment</a> and <a href=\"https://github.com/dmcable/spacexr\">spacexr</a> for spot deconvolution. Estimated cell type abundances are then used to cluster spots into groups of cell type niches which are defined as spots with distinct composition of cell types. </li> <li style=\"padding-bottom: 2px\"> <strong>(Niche Clustering: Cells)</strong> VoltRon creates spatial neighborhoods around cells to cluster local cellular compositions around all cells which in turn informs users on cell types that are likely within proximity to each other. </li> <li style=\"padding-bottom: 1px\"> <strong>(Hot Spot Detection)</strong> VoltRon detects region of locally spatial patterns of cells/molecules/spots that are abundant in biological events and/or features. </li> </ul> </li> <li> <p> <strong> Support for Big Data </strong> for VoltRon objects enables storing large feature data matrices and large microscopic images of tissues on disk without overloading memory, thus allowing analysis on large datasets with ease. VoltRon stores large images as pyramid structures to speed up visualization and data retrieval. </p> </li> <li style=\"padding-bottom: 10px\"> <p> <strong> Interoperability across R/Python frameworks </strong> allows users to convert VoltRon objects to a large number of objects used by other spatial omic platforms such as Seurat, Squidpy (AnnData), SpatialExperiment (BioConductor) and Giotto. </p> </li> </ul> ## Staying up-to-date To ask questions please use VoltRon discussion forum on google groups. - https://groups.google.com/forum/#!forum/voltron_discussion ## Installation Install from the GitHub repository using devtools (with R version 4.3.0 or higher): ``` r if (!require(\"devtools\", quietly = TRUE)) install.packages(\"devtools\") devtools::install_github(\"BIMSBbioinfo/VoltRon\") ``` Depending on the number of required dependencies, installation may be completed under a minute or may take a few minutes. On **Windows** and **MacOS**, OpenCV will be downloaded automatically upon installation. However, [Rtools](https://cran.r-project.org/bin/windows/Rtools/rtools43/rtools.html) may be required to be downloaded too, hence this may take some time! On **Ubuntu** we provide a set of instructions that may help users to build OpenCV with necessary headers [here](https://github.com/BIMSBbioinfo/VoltRon/blob/main/inst/extdata/install_ubuntu.md). On **Fedora** you may need [`opencv-devel`](https://src.fedoraproject.org/rpms/opencv): ```sh yum install opencv-devel ``` ## Dependencies VoltRon incorporates `RBioformats` package to import images from `ome.tiff` files, which requires [Java JDK](https://www.oracle.com/java/technologies/downloads/?er=221886) to be available in your system: See [https://cran.r-project.org/web/packages/rJava](https://cran.r-project.org/web/packages/rJava) below for more information. ## Docker Hub You can also run VoltRon from a container already available in [Docker Hub](https://hub.docker.com/repository/docker/amanukyan1385/rstudio-voltron/general). The docker image is based on the [Rocker Project](https://rocker-project.org/) and can be run from the terminal like below: ``` docker run --rm -ti -e PASSWORD=<yourpassword> -p 8787:8787 amanukyan1385/rstudio-voltron:main ``` Then, start the RStudio session from the browser at `http://localhost:8787/` and enter `rstudio` as username and `<yourpassword>` as password. See [here](https://github.com/BIMSBbioinfo/VoltRon/blob/main/inst/extdata/docker_desktop_instructions.md) for more instructions on how to run the container using [Docker Desktop](https://www.docker.com/products/docker-desktop/). ## References Manukyan, A., Bahry, E., Wyler, E., Becher, E., Pascual-Reguant, A., Plumbom, I., ... & Akalin, A. (2023). [VoltRon: A Spatial Omics Analysis Platform for Multi-Resolution and Multi-omics Integration using Image Registration](https://www.biorxiv.org/content/10.1101/2023.12.15.571667v1). bioRxiv, 2023-12.\n",
                "dependencies": "Package: VoltRon Type: Package Title: VoltRon for Spatial Omics Data Integration and Analysis Version: 0.2.0 Depends: R (>= 4.3.0) Author@R: \tperson(\"Artür\", \"Manukyan\", \t\t role=c(\"aut\", \"cre\"), \t email=\"artur-man@hotmail.com\", \t comment=c(ORCID=\"0000-0002-0441-9517\")), \tperson(\"Ella\", \"Bahry\", \t\t role=c(\"aut\")), \tperson(\"Raj Prateek\", \"Rai\", \t\t role=c(\"aut\")), \tperson(\"Wei-che\", \"Ko\", \t\t role=c(\"aut\")), \tperson(\"Markus\", \"Landthaler\", \t\t role=c(\"aut\")), \tperson(\"Altuna\", \"Akalin\", \t\t role=c(\"aut\")) Author: Artür Manukyan, Ella Bahry, Raj Prateek Rai, Wei-Che Ko, Markus Landthaler, Altuna Akalin Maintainer: Artür Manukyan <artur-man@hotmail.com> Description: VoltRon is a spatial omic analysis toolbox for multi-omics integration using spatial image registration. VoltRon is capable of analyzing multiple types and modalities of spatially-aware datasets. VoltRon visualizes and analyzes regions of interests (ROIs), spots, cells, molecules and event tiles. License: MIT + file LICENSE SystemRequirements: OpenCV 4.8 (or higher): libopencv-dev (Debian, Ubuntu) or opencv-devel (Fedora) Encoding: UTF-8 RoxygenNote: 7.3.2 biocViews: Imports: methods, grDevices, data.table, Matrix, S4Arrays, ids, RcppAnnoy, RANN, igraph, dplyr, ggplot2, ggrepel, ggpubr, rjson, magick, EBImage, sp, rlang, shiny, shinyjs, stringr, irlba, uwot, RCDT LinkingTo: Rcpp, RcppArmadillo (>= 0.4) Collate: 'RcppExports.R' 'zzz.R' 'allgenerics.R' 'allclasses.R' 'annotation.R' 'assay.R' 'auxiliary.R' 'clustering.R' 'conversion.R' 'data.R' 'deconvolution.R' 'differentialexpression.R' 'image.R' 'import.R' 'integration.R' 'interactive.R' 'io.R' 'metadata.R' 'objects.R' 'processing.R' 'registration.R' 'sample.R' 'spatial.R' 'visualization.R' Suggests: testthat (>= 3.0.0), S4Vectors, DelayedArray, rhdf5, basilisk, reticulate, RBioFormats, BPCells, HDF5Array, pizzarr, ZarrArray, HDF5DataFrame, ZarrDataFrame, ImageArray, viridisLite, SpatialExperiment, SingleCellExperiment, SummarizedExperiment, Seurat, SeuratObject, Giotto, DESeq2, MuSiC, spacexr, XML, ComplexHeatmap, xlsx, reshape2, tiledb, tiledbsc, arrow, vitessceR, geojsonR, circlize, rstudioapi, ggforce, ggnewscale, anndataR, anndata, Config/testthat/edition: 3 LazyData: true LazyDataCompression: gzip\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/wombat",
            "repo_link": "https://git.gsi.de/phelix/lv/wombat_ce",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/wps-command-line-tool-repository",
            "repo_link": "https://github.com/riesgos/gfz-command-line-tool-repository",
            "content": {
                "codemeta": "",
                "readme": "# gfz-riesgos-wps-repository [![pipeline status](https://gitext.gfz-potsdam.de/riesgos/gfz-riesgos-wps-repository/badges/master/pipeline.svg)](https://gitext.gfz-potsdam.de/riesgos/gfz-riesgos-wps-repository/commits/master) ## Description This is the java source code for the wps-repository for the riesgos project. It aims to be an framework for easy integration of command line programs as web processing services and provide a bunch of services within the scope of the [RIESGOS project](http://www.riesgos.de/en/). This focus here is mainly on those processes provided by the [GFZ](https://www.gfz-potsdam.de/en/home/). ## How it works The processes that are integrated here are command line programs. Most processes integrated so far use python3 but any executable command line program can be integrated. Each process must be wrapped in a docker image to provide fully independent execution of the processes (also in case of some hard coded temporary files) and to manage the dependencies (programs, libraries, python packages, internal configuration files, ...). For each processes a json configuration file must be provided, so that the basic process skeleton - which is the same for all processes - knows how to provide the input data, how to start the process and how to read the output of the programs. It is also used to specify the way of error handling in the process skeleton. For more information about dockerfiles you can take a look at the [official docker documentation](https://docs.docker.com/engine/reference/builder/). The role of docker for the overall framework here is explained on [its own documentation page](doc/RoleOfDocker.md). The json configuration is explained in more detail [here](doc/JsonConfigurationExplaned.md). ## Requirements All of the code here runs on top of the WPS Server provided by [52° North](https://github.com/52North/WPS). For other details please refer to the [installation guide](doc/Installationguide.md). ## Currently implemented processes Please refer to the following [sub page](doc/IncludedProcesses.md) for an overview of the processes that are already on board. Additionally to the main processes there are also some [format conversion processes](doc/FormatConversionProcesses.md) in the repository. ## How to add a service If you want to know how to add your own service, we provide a step-by-step guide to add a service [here](doc/HowToAddOwnProcess.md).\n",
                "dependencies": "<?xml version=\"1.0\" encoding=\"UTF-8\"?> <project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> <modelVersion>4.0.0</modelVersion> <!-- This pom.xml is mostly the same as https://github.com/riesgos/52north-wps-osmtovector-process/blob/master/pom.xml to ensure most possible compatibility --> <groupId>org.n52.gfz.riesgos.repository</groupId> <artifactId>gfz-riesgos-wps</artifactId> <version>1.0-SNAPSHOT</version> <properties> <wps.version>4.0.0-beta.10</wps.version> <java-version>1.8</java-version> <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> </properties> <repositories> <repository> <id>n52-releases</id> <name>52n Releases</name> <url>http://52north.org/maven/repo/releases</url> <releases> <enabled>true</enabled> </releases> <snapshots> <enabled>false</enabled> </snapshots> </repository> <repository> <id>geotools</id> <name>Geotools Repo</name> <url>http://download.osgeo.org/webdav/geotools</url> <releases> <enabled>true</enabled> </releases> <snapshots> <enabled>true</enabled> </snapshots> </repository> <!-- as the originally boundless geo repo is not available anymore, we need to try out some change like the one suggested here: https://github.com/geotools/geotools/wiki/Change-from-webdav-to-nexus-repository --> <repository> <snapshots> <enabled>false</enabled> </snapshots> <id>osgeo-release</id> <name>Open Source Geospatial Foundation Repository</name> <url>https://repo.osgeo.org/repository/release/</url> </repository> <repository> <snapshots> <enabled>true</enabled> </snapshots> <id>osgeo-snapshot</id> <name>Open Source Geospatial Foundation Build Repository</name> <url>https://repo.osgeo.org/repository/snapshot/</url> </repository> </repositories> <dependencies> <dependency> <groupId>org.n52.wps</groupId> <artifactId>52n-wps-commons</artifactId> <version>${wps.version}</version> <exclusions> <exclusion> <groupId>org.apache.xmlbeans</groupId> <artifactId>xmlbeans</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.n52.wps</groupId> <artifactId>52n-wps-commons</artifactId> <version>${wps.version}</version> <classifier>tests</classifier> <scope>test</scope> <exclusions> <exclusion> <groupId>org.apache.xmlbeans</groupId> <artifactId>xmlbeans</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.n52.wps</groupId> <artifactId>52n-wps-algorithm</artifactId> <version>${wps.version}</version> <exclusions> <exclusion> <groupId>org.apache.xmlbeans</groupId> <artifactId>xmlbeans</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.n52.wps</groupId> <artifactId>52n-wps-io-geotools</artifactId> <version>${wps.version}</version> <exclusions> <exclusion> <groupId>org.apache.xmlbeans</groupId> <artifactId>xmlbeans</artifactId> </exclusion> <exclusion> <groupId>commons-httpclient</groupId> <artifactId>commons-httpclient</artifactId> </exclusion> <exclusion> <groupId>commons-io</groupId> <artifactId>commons-io</artifactId> </exclusion> <exclusion> <groupId>com.vividsolutions</groupId> <artifactId>jts</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.n52.wps</groupId> <artifactId>52n-wps-io</artifactId> <version>${wps.version}</version> <scope>provided</scope> <exclusions> <exclusion> <groupId>org.apache.xmlbeans</groupId> <artifactId>xmlbeans</artifactId> </exclusion> <exclusion> <groupId>commons-io</groupId> <artifactId>commons-io</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.n52.wps</groupId> <artifactId>52n-wps-io-impl</artifactId> <version>${wps.version}</version> <exclusions> <exclusion> <groupId>org.apache.xmlbeans</groupId> <artifactId>xmlbeans</artifactId> </exclusion> <exclusion> <groupId>commons-io</groupId> <artifactId>commons-io</artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-compress</artifactId> <version>1.19</version> </dependency> <dependency> <groupId>commons-lang</groupId> <artifactId>commons-lang</artifactId> <version>2.5</version> </dependency> <dependency> <groupId>commons-io</groupId> <artifactId>commons-io</artifactId> <version>2.0</version> </dependency> <dependency> <groupId>commons-httpclient</groupId> <artifactId>commons-httpclient</artifactId> <version>3.1</version> </dependency> <dependency> <groupId>org.apache.xmlbeans</groupId> <artifactId>xmlbeans</artifactId> <version>2.6.0</version> </dependency> <dependency> <groupId>org.apache.ant</groupId> <artifactId>ant</artifactId> <version>1.10.8</version> </dependency> <dependency> <groupId>org.geotools</groupId> <artifactId>gt-process-raster</artifactId> <version>13.5</version> </dependency> <dependency> <groupId>com.google.guava</groupId> <artifactId>guava</artifactId> <version>28.0-jre</version> </dependency> <!-- if you need the interpolation function (kriging) add this dependency and insert it to <dependency> <groupId>com.github.haifengl</groupId> <artifactId>smile-interpolation</artifactId> <version>1.5.3</version> </dependency> --> <!-- just for stand alone testing; should later be removed --> <!--<dependency> <groupId>org.n52.wps</groupId> <artifactId>52n-wps-io</artifactId> <version>4.0.0-beta.7-SNAPSHOT</version> </dependency>--> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.13.1</version> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-module-junit4</artifactId> <version>2.0.2</version> <scope>test</scope> </dependency> <dependency> <groupId>org.powermock</groupId> <artifactId>powermock-api-mockito2</artifactId> <version>2.0.2</version> <scope>test</scope> </dependency> </dependencies> <build> <pluginManagement> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.1</version> <configuration> <source>${java-version}</source> <target>${java-version}</target> <compilerArgument>-Xlint:all</compilerArgument> <showWarnings>true</showWarnings> <showDeprecation>true</showDeprecation> </configuration> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-assembly-plugin</artifactId> <version>2.2.2</version> </plugin> </plugins> </pluginManagement> <resources> <resource> <directory>src/main/resources</directory> <filtering>true</filtering> <includes> <include>**/*</include> </includes> </resource> </resources> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-assembly-plugin</artifactId> <executions> <execution> <configuration> <descriptors> <descriptor>src/main/config/assemble.xml</descriptor> </descriptors> </configuration> <goals> <goal>single</goal> </goals> <phase>install</phase> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-checkstyle-plugin</artifactId> <version>3.1.0</version> <configuration> <configLocation>sun_checks.xml</configLocation> <encoding>UTF-8</encoding> <consoleOutput>true</consoleOutput> <failsOnError>true</failsOnError> <linkXRef>false</linkXRef> <suppressionsLocation>checkstyle-suppressions.xml</suppressionsLocation> <suppressionsFileExpression>checkstyle.suppressions.file</suppressionsFileExpression> </configuration> </plugin> <plugin> <groupId>pl.project13.maven</groupId> <artifactId>git-commit-id-plugin</artifactId> <version>3.0.0</version> <executions> <execution> <goals> <goal>revision</goal> </goals> <phase>validate</phase> </execution> </executions> <configuration> <commitIdGenerationMode>flat</commitIdGenerationMode> <gitDescribe> <skip>true</skip> </gitDescribe> </configuration> </plugin> <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>templating-maven-plugin</artifactId> <version>1.0.0</version> <executions> <execution> <id>filter-src</id> <goals> <goal>filter-sources</goal> </goals> </execution> <execution> <id>filter-test-src</id> <goals> <goal>filter-test-sources</goal> </goals> </execution> </executions> </plugin> </plugins> </build> </project>\n"
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/wrainfo",
            "repo_link": "https://git.gfz-potsdam.de/fernlab/products/furuno/wrainfo",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        },
        {
            "software_organization": "https://helmholtz.software/software/xraypac",
            "repo_link": "https://gitlab.desy.de/cdt/xraypac",
            "content": {
                "codemeta": "",
                "readme": "",
                "dependencies": ""
            }
        }
    ]
}