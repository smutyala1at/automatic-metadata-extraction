results = [
    {
  "repo_name": "NNPDF/nnpdf",
  "llm": {
    "precision": "1.0000",
    "recall": "1.0000",
    "f1": "1.0000",
    "accuracy": "1.0000",
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": "0.9000",
    "recall": "0.7500",
    "f1": "0.8182",
    "accuracy": "0.8125",
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "PE"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  }
},
{
  "repo_name": "FormingWorlds/PROTEUS",
  "llm": {
    "precision": "0.8571",
    "recall": "1.0000",
    "f1": "0.9231",
    "accuracy": "0.8750",
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TP"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "FP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": "0.7500",
    "recall": "0.5000",
    "f1": "0.6000",
    "accuracy": "0.5000",
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "FP"
      },
      {
        "contributor": "FN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "FN"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  }
},
{
  "repo_name": "KernelTuner/kernel_tuner",
  "llm": {
    "precision": 0.9167,
    "recall": 0.9167,
    "f1": 0.9167,
    "accuracy": 0.9375,
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "PE"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": 0.7,
    "recall": 0.7,
    "f1": 0.7,
    "accuracy": 0.6875,
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "PE"
      },
      {
        "author": "FP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  }
},
{
  "repo_name": "dtscalibration/python-dts-calibration",
  "llm": {
    "precision": 1,
    "recall": 1,
    "f1": 1,
    "accuracy": 1,
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": 0.8,
    "recall": 0.8,
    "f1": 0.8,
    "accuracy": 0.75,
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "FP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  }
},
{
  "repo_name": "NLeSC/python-template",
  "llm": {
    "precision": "1.0000",
    "recall": "1.0000",
    "f1": "1.0000",
    "accuracy": "1.0000",
    "elements": [
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      },
      {
        "softwareRequirements": "TN"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      }
    ]
  },
  "somef": {
    "precision": "0.7500",
    "recall": "0.7500",
    "f1": "0.7500",
    "accuracy": "0.7500",
    "elements": [
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      },
      {
        "author": "FP"
      },
      {
        "softwareRequirements": "TN"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "buildInstructions": "FN"
      }
    ]
  }
},
{
  "repo_name": "iomega/paired-data-form",
  "llm": {
    "precision": 1,
    "recall": 1,
    "f1": 1,
    "accuracy": 1,
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": 0.625,
    "recall": 0.5,
    "f1": 0.5556,
    "accuracy": 0.5625,
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "PE"
      },
      {
        "author": "FP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "FN"
      }
    ]
  }
},
{
  "repo_name": "pism/pism",
  "llm": {
    "precision": 1,
    "recall": 1,
    "f1": 1,
    "accuracy": 1,
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": 0.625,
    "recall": 0.5,
    "f1": 0.5556,
    "accuracy": 0.5625,
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "PE"
      },
      {
        "author": "FP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "FN"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  }
},
{
  "repo_name": "NLeSC/MAGMa",
  "llm": {
    "precision": 1,
    "recall": 0.7143,
    "f1": 0.8333,
    "accuracy": 0.75,
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "FN"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TP"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": 0.3333,
    "recall": 0.2,
    "f1": 0.25,
    "accuracy": 0.25,
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "FP"
      },
      {
        "author": "FP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "FN"
      },
      {
        "identifier": "FN"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "FN"
      }
    ]
  }
},
{
  "repo_name": "NLeSC/mcfly",
  "llm": {
    "precision": "1.0000",
    "recall": "1.0000",
    "f1": "1.0000",
    "accuracy": "1.0000",
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": "0.6000",
    "recall": "0.7500",
    "f1": "0.6667",
    "accuracy": "0.6250",
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "FP"
      },
      {
        "author": "FP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  }
},
{
  "repo_name": "cdk/cdk",
  "llm": {
    "precision": 1,
    "recall": 1,
    "f1": 1,
    "accuracy": 1,
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TP"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": 0.5,
    "recall": 0.4,
    "f1": 0.4444444444444445,
    "accuracy": 0.375,
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "FP"
      },
      {
        "author": "FP"
      },
      {
        "contributor": "FN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "FN"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  }
},
{
  "repo_name": "wadpac/GGIR",
  "llm": {
    "precision": 1,
    "recall": 0.8333333333333334,
    "f1": 0.9090909090909091,
    "accuracy": 0.875,
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "FN"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TP"
      },
      {
        "identifier": "TN"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": 0.75,
    "recall": 0.6,
    "f1": 0.6666666666666665,
    "accuracy": 0.625,
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "FP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "FN"
      },
      {
        "identifier": "TN"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  }
},
{
  "repo_name": "transientskp/tkp",
  "llm": {
    "precision": 1,
    "recall": 0.8333333333333334,
    "f1": 0.9090909090909091,
    "accuracy": 0.875,
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "FN"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": 0.6666666666666666,
    "recall": 0.4,
    "f1": 0.5,
    "accuracy": 0.5,
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "FP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "FN"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "FN"
      }
    ]
  }
},
{
  "repo_name": "GrainLearning/grainLearning",
  "llm": {
    "precision": "1.0000",
    "recall": "1.0000",
    "f1": "1.0000",
    "accuracy": "1.0000",
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TP"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": "0.8000",
    "recall": "0.6667",
    "f1": "0.7273",
    "accuracy": "0.6250",
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "FP"
      },
      {
        "contributor": "FN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  }
},
{
  "repo_name": "tpronk/splithalfr",
  "llm": {
    "precision": "1.0000",
    "recall": "1.0000",
    "f1": "1.0000",
    "accuracy": "1.0000",
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TN"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": "0.7500",
    "recall": "0.3000",
    "f1": "0.4286",
    "accuracy": "0.5625",
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "TN"
      },
      {
        "author": "PE"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "FN"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "FN"
      }
    ]
  }
},
{
  "repo_name": "NLeSC/pattyvis",
  "llm": {
    "precision": 1,
    "recall": 1,
    "f1": 1,
    "accuracy": 1,
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": 0.7,
    "recall": 0.7,
    "f1": 0.7,
    "accuracy": 0.6875,
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "PE"
      },
      {
        "author": "FP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "FN"
      }
    ]
  }
},
{
  "repo_name": "matchms/ms2deepscore",
  "llm": {
    "precision": 1,
    "recall": 1,
    "f1": 1,
    "accuracy": 1,
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": 0.6,
    "recall": 0.6,
    "f1": 0.6,
    "accuracy": 0.625,
    "elements": [
      {
        "softwareRequirements": "PE"
      },
      {
        "buildInstructions": "PE"
      },
      {
        "author": "FP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "FN"
      }
    ]
  }
},
{
  "repo_name": "WDscholia/scholia",
  "llm": {
    "precision": 0.9167,
    "recall": 0.9167,
    "f1": 0.9167,
    "accuracy": 0.9375,
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "PE"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": 1,
    "recall": 0.6667,
    "f1": 0.8,
    "accuracy": 0.75,
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "FN"
      },
      {
        "keywords": "TP"
      }
    ]
  }
},
{
  "repo_name": "AI4S2S/lilio",
  "llm": {
    "precision": 1,
    "recall": 1,
    "f1": 1,
    "accuracy": 1,
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TP"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": 1,
    "recall": 0.5714285714285714,
    "f1": 0.7272727272727273,
    "accuracy": 0.625,
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "FN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "FN"
      }
    ]
  }
},
{
  "repo_name": "eucp-project/storyboards",
  "llm": {
    "precision": "1.0000",
    "recall": "1.0000",
    "f1": "1.0000",
    "accuracy": "1.0000",
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": "1.0000",
    "recall": "0.5000",
    "f1": "0.6667",
    "accuracy": "0.6250",
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "FN"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TP"
      },
      {
        "license": "TP"
      },
      {
        "keywords": "FN"
      }
    ]
  }
},
{
  "repo_name": "UtrechtUniversity/streetview-segmentation",
  "llm": {
    "precision": 1,
    "recall": 0.75,
    "f1": 0.8571428571428571,
    "accuracy": 0.875,
    "elements": [
      {
        "softwareRequirements": "TP"
      },
      {
        "buildInstructions": "TP"
      },
      {
        "author": "FN"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TN"
      },
      {
        "license": "TN"
      },
      {
        "keywords": "TP"
      }
    ]
  },
  "somef": {
    "precision": 1,
    "recall": 0.25,
    "f1": 0.4,
    "accuracy": 0.625,
    "elements": [
      {
        "softwareRequirements": "FN"
      },
      {
        "buildInstructions": "FN"
      },
      {
        "author": "TP"
      },
      {
        "contributor": "TN"
      },
      {
        "funder": "TN"
      },
      {
        "identifier": "TN"
      },
      {
        "license": "TN"
      },
      {
        "keywords": "FN"
      }
    ]
  }
}
]





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

if not os.path.exists('element_result'):
    os.makedirs('element_result')
    print("Created 'element_result' folder")

BAR_FIGURE_SIZE = (16, 8)

def element_wise_comparison(results):
    """
    Performs element-wise comparison between LLM and SOMEF results.
    
    Args:
        results: List of dictionaries containing comparison results for each repository
        
    Returns:
        comparison_df: DataFrame with element-wise comparison metrics
        counts_df: DataFrame with raw counts for each element
        llm_avg: Dictionary with LLM average metrics
        somef_avg: Dictionary with SOMEF average metrics
    """
    # Define all possible elements
    elements = ['softwareRequirements', 'buildInstructions', 'author', 'contributor', 
                'funder', 'identifier', 'license', 'keywords']
    
    # Initialize counters for each element and system
    llm_results = {element: {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0, 'PE': 0} for element in elements}
    somef_results = {element: {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0, 'PE': 0} for element in elements}
    
    # Iterate through repositories
    for repo in results:
        # Process LLM results
        if 'llm' in repo and 'elements' in repo['llm']:
            for element_dict in repo['llm']['elements']:
                for element_name, result in element_dict.items():
                    # Normalize element name
                    element_name = element_name = element_name.lower() if element_name not in ['softwareRequirements', 'buildInstructions'] else element_name
                    if element_name in elements:
                        # Increment the appropriate counter
                        llm_results[element_name][result] += 1
        
        # Process SOMEF results
        if 'somef' in repo and 'elements' in repo['somef']:
            for element_dict in repo['somef']['elements']:
                for element_name, result in element_dict.items():
                    element_name = element_name = element_name.lower() if element_name not in ['softwareRequirements', 'buildInstructions'] else element_name
                    if element_name in elements:
                        somef_results[element_name][result] += 1
    
    # Calculate metrics for each element
    def calculate_element_metrics(counts):
        metrics = {}
        
        for element_name, count in counts.items():
            # Calculate metrics
            tp = count['TP']
            fp = count['FP']
            fn = count['FN']
            tn = count['TN']
            pe = count['PE']  # Partial extraction (equivalent to PC - Partially Correct)
            
            # Weight for partial matches
            w = 0.5
            
            # Calculate metrics with formal definitions including partial matches
            # Precision with Partial Matches: (TP + w × PC) / (TP + PC + FP)
            precision = (tp + w * pe) / (tp + pe + fp) if (tp + pe + fp) > 0 else 0
            
            # Recall with Partial Matches: (TP + w × PC) / (TP + PC + FN)
            recall = (tp + w * pe) / (tp + pe + fn) if (tp + pe + fn) > 0 else 0
            
            # F1 Score with Partial Matches: 2 × (Precision × Recall) / (Precision + Recall)
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            # Accuracy with Partial Matches: (TP + w × PC + TN) / (TP + PC + FP + FN + TN)
            accuracy = (tp + w * pe + tn) / (tp + pe + fp + fn + tn) if (tp + pe + fp + fn + tn) > 0 else 0
            
            metrics[element_name] = {
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'accuracy': accuracy,
                'tp': tp,
                'fp': fp,
                'fn': fn,
                'tn': tn,
                'pe': pe
            }
        
        return metrics
    
    # Calculate metrics
    llm_metrics = calculate_element_metrics(llm_results)
    somef_metrics = calculate_element_metrics(somef_results)

    # Create a comparison dataframe
    comparison_data = []
    
    for element in elements:
        llm_data = llm_metrics.get(element, {})
        somef_data = somef_metrics.get(element, {})
        
        comparison_data.append({
            'Element': element,
            'LLM_Precision': llm_data.get('precision', 0),
            'LLM_Recall': llm_data.get('recall', 0),
            'LLM_F1': llm_data.get('f1', 0),
            'LLM_Accuracy': llm_data.get('accuracy', 0),
            'SOMEF_Precision': somef_data.get('precision', 0),
            'SOMEF_Recall': somef_data.get('recall', 0),
            'SOMEF_F1': somef_data.get('f1', 0), 
            'SOMEF_Accuracy': somef_data.get('accuracy', 0),
            'Precision_Diff': llm_data.get('precision', 0) - somef_data.get('precision', 0),
            'Recall_Diff': llm_data.get('recall', 0) - somef_data.get('recall', 0),
            'F1_Diff': llm_data.get('f1', 0) - somef_data.get('f1', 0),
            'Accuracy_Diff': llm_data.get('accuracy', 0) - somef_data.get('accuracy', 0),
            'LLM_TP': llm_results[element]['TP'],
            'LLM_FP': llm_results[element]['FP'], 
            'LLM_FN': llm_results[element]['FN'],
            'LLM_TN': llm_results[element]['TN'],
            'LLM_PE': llm_results[element]['PE'],
            'SOMEF_TP': somef_results[element]['TP'],
            'SOMEF_FP': somef_results[element]['FP'],
            'SOMEF_FN': somef_results[element]['FN'],
            'SOMEF_TN': somef_results[element]['TN'],
            'SOMEF_PE': somef_results[element]['PE']
        })
    
    # Create DataFrames
    comparison_df = pd.DataFrame(comparison_data)
    
    # Calculate averages
    llm_avg = {
        'Precision': comparison_df['LLM_Precision'].mean(),
        'Recall': comparison_df['LLM_Recall'].mean(),
        'F1': comparison_df['LLM_F1'].mean(),
        'Accuracy': comparison_df['LLM_Accuracy'].mean()
    }
    
    somef_avg = {
        'Precision': comparison_df['SOMEF_Precision'].mean(),
        'Recall': comparison_df['SOMEF_Recall'].mean(),
        'F1': comparison_df['SOMEF_F1'].mean(),
        'Accuracy': comparison_df['SOMEF_Accuracy'].mean()
    }
    
    # Create counts dataframe
    counts_df = comparison_df[['Element', 
                              'LLM_TP', 'LLM_FP', 'LLM_FN', 'LLM_TN', 'LLM_PE',
                              'SOMEF_TP', 'SOMEF_FP', 'SOMEF_FN', 'SOMEF_TN', 'SOMEF_PE']]
    
    return comparison_df, counts_df, llm_avg, somef_avg

def add_value_labels(ax, bars, offset=0, precision=3, horizontal=False):
    """
    Add text labels displaying exact values on top/right of each bar
    
    Args:
        ax: The matplotlib axis
        bars: The bar container returned from ax.bar() or ax.barh()
        offset: Vertical/horizontal offset for label positioning
        precision: Decimal precision to display
        horizontal: Whether the bars are horizontal
    """
    for bar in bars:
        if horizontal:
            width = bar.get_width()
            x_pos = width + offset
            y_pos = bar.get_y() + bar.get_height()/2
            value = f'{width:.{precision}f}'
            va = 'center'
            ha = 'left'
        else:
            height = bar.get_height()
            x_pos = bar.get_x() + bar.get_width()/2
            y_pos = height + offset
            value = f'{height:.{precision}f}'
            va = 'bottom'
            ha = 'center'
        
        # Only add labels for non-zero values
        if float(value) > 0:
            ax.text(x_pos, y_pos, value, ha=ha, va=va, fontsize=9, fontweight='bold')

def create_visualizations(comparison_df, counts_df, llm_avg, somef_avg):
    """
    Creates visualizations for element-wise comparison.
    
    Args:
        comparison_df: DataFrame with element-wise comparison metrics
        counts_df: DataFrame with raw counts for each element
        llm_avg: Dictionary with LLM average metrics
        somef_avg: Dictionary with SOMEF average metrics
        
    Returns:
        Dictionary of matplotlib figure objects
    """
    figures = {}
    
    # Sort the data by F1 difference
    # Keep the original order
    metrics_df = comparison_df.copy()
    
    # 1. F1 Score Comparison
    fig1, ax1 = plt.subplots(figsize=BAR_FIGURE_SIZE)
    x = np.arange(len(metrics_df))
    width = 0.35
    
    # Store bar containers to use with label function
    llm_bars = ax1.bar(x - width/2, metrics_df['LLM_F1'], width, label='LLM')
    somef_bars = ax1.bar(x + width/2, metrics_df['SOMEF_F1'], width, label='SOMEF')
    
    # Add value labels to bars
    add_value_labels(ax1, llm_bars)
    add_value_labels(ax1, somef_bars)
    
    ax1.set_ylabel('F1 Score')
    ax1.set_title('F1 Score Comparison by Element')
    ax1.set_xticks(x)
    ax1.set_xticklabels(metrics_df['Element'], rotation=0)
    ax1.legend()
    plt.tight_layout()
    
    figures['f1_comparison'] = fig1
    
    # 2a. Precision Comparison (Separate plot)
    fig2a, ax2a = plt.subplots(figsize=BAR_FIGURE_SIZE)
    llm_pre_bars = ax2a.bar(x - width/2, metrics_df['LLM_Precision'], width, label='LLM')
    somef_pre_bars = ax2a.bar(x + width/2, metrics_df['SOMEF_Precision'], width, label='SOMEF')
    
    # Add value labels
    add_value_labels(ax2a, llm_pre_bars)
    add_value_labels(ax2a, somef_pre_bars)
    
    ax2a.set_ylabel('Precision')
    ax2a.set_title('Precision by Element')
    ax2a.set_xticks(x)
    ax2a.set_xticklabels(metrics_df['Element'], rotation=0)
    ax2a.legend()
    plt.tight_layout()
    figures['precision_comparison'] = fig2a
    
    # 2b. Recall Comparison (Separate plot)
    fig2b, ax2b = plt.subplots(figsize=BAR_FIGURE_SIZE)
    llm_rec_bars = ax2b.bar(x - width/2, metrics_df['LLM_Recall'], width, label='LLM')
    somef_rec_bars = ax2b.bar(x + width/2, metrics_df['SOMEF_Recall'], width, label='SOMEF')
    
    # Add value labels
    add_value_labels(ax2b, llm_rec_bars)
    add_value_labels(ax2b, somef_rec_bars)
    
    ax2b.set_ylabel('Recall')
    ax2b.set_title('Recall by Element')
    ax2b.set_xticks(x)
    ax2b.set_xticklabels(metrics_df['Element'], rotation=0)
    ax2b.legend()
    plt.tight_layout()
    figures['recall_comparison'] = fig2b
    
    # 2c. Accuracy Comparison (Separate plot)
    fig2c, ax2c = plt.subplots(figsize=BAR_FIGURE_SIZE)
    llm_acc_bars = ax2c.bar(x - width/2, metrics_df['LLM_Accuracy'], width, label='LLM')
    somef_acc_bars = ax2c.bar(x + width/2, metrics_df['SOMEF_Accuracy'], width, label='SOMEF')
    
    # Add value labels
    add_value_labels(ax2c, llm_acc_bars)
    add_value_labels(ax2c, somef_acc_bars)
    
    ax2c.set_ylabel('Accuracy')
    ax2c.set_title('Accuracy by Element')
    ax2c.set_xticks(x)
    ax2c.set_xticklabels(metrics_df['Element'], rotation=0)
    ax2c.legend()
    plt.tight_layout()
    figures['accuracy_comparison'] = fig2c
    
    # 3. Overall metrics comparison
    metrics = ['Precision', 'Recall', 'F1', 'Accuracy']
    llm_values = [llm_avg[m] for m in metrics]
    somef_values = [somef_avg[m] for m in metrics]
    
    fig3, ax3 = plt.subplots(figsize=BAR_FIGURE_SIZE)
    x = np.arange(len(metrics))
    width = 0.35
    
    llm_overall_bars = ax3.bar(x - width/2, llm_values, width, label='LLM')
    somef_overall_bars = ax3.bar(x + width/2, somef_values, width, label='SOMEF')
    
    # Add value labels
    add_value_labels(ax3, llm_overall_bars)
    add_value_labels(ax3, somef_overall_bars)
    
    ax3.set_ylabel('Score')
    ax3.set_title('Overall Performance Comparison')
    ax3.set_xticks(x)
    ax3.set_xticklabels(metrics)
    ax3.legend()
    
    plt.tight_layout()
    figures['overall_comparison'] = fig3
    
    # 4. Performance gap chart
    fig4, ax4 = plt.subplots(figsize=BAR_FIGURE_SIZE)
    
    metrics_df['F1_Gap'] = metrics_df['F1_Diff']
    
    gap_bars = ax4.barh(metrics_df['Element'], metrics_df['F1_Gap'], color='royalblue')
    ax4.set_xlabel('F1 Score Difference (LLM - SOMEF)')
    ax4.set_title('Performance Gap by Element')
    
    # Add a vertical line at x=0
    ax4.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
    
    # Add value labels for horizontal bars
    add_value_labels(ax4, gap_bars, offset=0.01, horizontal=True)
    
    plt.tight_layout()
    figures['performance_gap'] = fig4
    
    # 5. Heatmap of LLM vs SOMEF metrics
    fig5, ax5 = plt.subplots(figsize=BAR_FIGURE_SIZE)
    
    # Create a new DataFrame for the heatmap
    heatmap_data = []
    for element in metrics_df['Element']:
        row = comparison_df[comparison_df['Element'] == element].iloc[0]
        heatmap_data.append({
            'Element': element,
            'LLM Precision': row['LLM_Precision'],
            'LLM Recall': row['LLM_Recall'],
            'LLM F1': row['LLM_F1'],
            'SOMEF Precision': row['SOMEF_Precision'],
            'SOMEF Recall': row['SOMEF_Recall'],
            'SOMEF F1': row['SOMEF_F1']
        })
    
    heatmap_df = pd.DataFrame(heatmap_data)
    heatmap_df = heatmap_df.set_index('Element')
    
    sns.heatmap(heatmap_df, annot=True, cmap='Blues', fmt='.3f', ax=ax5)
    ax5.set_title('Comparison of Metrics by Element')
    
    plt.tight_layout()
    figures['metrics_heatmap'] = fig5
    
    # 6. Raw Counts Plot
    # First for LLM
    fig6a, ax6a = plt.subplots(figsize=BAR_FIGURE_SIZE)
    
    # Reorder counts_df to match the F1 sorted order from metrics_df
    # Use counts_df without sorting
    counts_df_sorted = counts_df.copy()
    
    x = np.arange(len(counts_df_sorted))
    width = 0.15
    
    # Plot the raw counts for LLM
    tp_bars = ax6a.bar(x - width*2, counts_df_sorted['LLM_TP'], width, label='TP', color='green')
    fp_bars = ax6a.bar(x - width, counts_df_sorted['LLM_FP'], width, label='FP', color='red')
    fn_bars = ax6a.bar(x, counts_df_sorted['LLM_FN'], width, label='FN', color='orange')
    tn_bars = ax6a.bar(x + width, counts_df_sorted['LLM_TN'], width, label='TN', color='blue')
    pe_bars = ax6a.bar(x + width*2, counts_df_sorted['LLM_PE'], width, label='PE', color='purple')
    
    # Add value labels for count bars with integer precision
    add_value_labels(ax6a, tp_bars, precision=0)
    add_value_labels(ax6a, fp_bars, precision=0)
    add_value_labels(ax6a, fn_bars, precision=0)
    add_value_labels(ax6a, tn_bars, precision=0)
    add_value_labels(ax6a, pe_bars, precision=0)
    
    ax6a.set_ylabel('Count')
    ax6a.set_title('LLM Raw Counts by Element')
    ax6a.set_xticks(x)
    ax6a.set_xticklabels(counts_df_sorted['Element'], rotation=0)
    ax6a.legend()
    
    plt.tight_layout()
    figures['llm_raw_counts'] = fig6a
    
    # Then for SOMEF
    fig6b, ax6b = plt.subplots(figsize=BAR_FIGURE_SIZE)
    
    # Plot the raw counts for SOMEF
    somef_tp_bars = ax6b.bar(x - width*2, counts_df_sorted['SOMEF_TP'], width, label='TP', color='green')
    somef_fp_bars = ax6b.bar(x - width, counts_df_sorted['SOMEF_FP'], width, label='FP', color='red')
    somef_fn_bars = ax6b.bar(x, counts_df_sorted['SOMEF_FN'], width, label='FN', color='orange')
    somef_tn_bars = ax6b.bar(x + width, counts_df_sorted['SOMEF_TN'], width, label='TN', color='blue')
    somef_pe_bars = ax6b.bar(x + width*2, counts_df_sorted['SOMEF_PE'], width, label='PE', color='purple')
    
    # Add value labels for count bars with integer precision
    add_value_labels(ax6b, somef_tp_bars, precision=0)
    add_value_labels(ax6b, somef_fp_bars, precision=0)
    add_value_labels(ax6b, somef_fn_bars, precision=0)
    add_value_labels(ax6b, somef_tn_bars, precision=0)
    add_value_labels(ax6b, somef_pe_bars, precision=0)
    
    ax6b.set_ylabel('Count')
    ax6b.set_title('SOMEF Raw Counts by Element')
    ax6b.set_xticks(x)
    ax6b.set_xticklabels(counts_df_sorted['Element'], rotation=0)
    ax6b.legend()
    
    plt.tight_layout()
    figures['somef_raw_counts'] = fig6b
    
    # 7. Combined Raw Counts Comparison (stacked bars)
    fig7, (ax7a, ax7b) = plt.subplots(2, 1, figsize=BAR_FIGURE_SIZE)
    
    # Stacked bar for LLM
    fig7a, ax7a = plt.subplots(figsize=BAR_FIGURE_SIZE)
    
    # Calculate cumulative values for adding labels to stacked bars
    llm_tp = counts_df_sorted['LLM_TP']
    llm_fp = counts_df_sorted['LLM_FP']
    llm_fn = counts_df_sorted['LLM_FN']
    llm_pe = counts_df_sorted['LLM_PE']
    
    # Create stacked bars
    bars1 = ax7a.bar(counts_df_sorted['Element'], llm_tp, label='TP', color='green')
    bars2 = ax7a.bar(counts_df_sorted['Element'], llm_fp, bottom=llm_tp, label='FP', color='red')
    bars3 = ax7a.bar(counts_df_sorted['Element'], llm_fn, bottom=llm_tp + llm_fp, label='FN', color='orange')
    bars4 = ax7a.bar(counts_df_sorted['Element'], llm_pe, bottom=llm_tp + llm_fp + llm_fn, label='PE', color='purple')
    
    # Add labels to each segment
    for i, (element, tp, fp, fn, pe) in enumerate(zip(counts_df_sorted['Element'], llm_tp, llm_fp, llm_fn, llm_pe)):
        # Only add labels for non-zero values
        if tp > 0:
            ax7a.text(i, tp/2, str(int(tp)), ha='center', va='center', fontweight='bold', color='white')
        if fp > 0:
            ax7a.text(i, tp + fp/2, str(int(fp)), ha='center', va='center', fontweight='bold', color='white')
        if fn > 0:
            ax7a.text(i, tp + fp + fn/2, str(int(fn)), ha='center', va='center', fontweight='bold', color='white')
        if pe > 0:
            ax7a.text(i, tp + fp + fn + pe/2, str(int(pe)), ha='center', va='center', fontweight='bold', color='white')
    
    ax7a.set_title('LLM Result Counts by Element')
    ax7a.set_ylabel('Count')
    ax7a.legend()
    plt.setp(ax7a.get_xticklabels(), rotation=0)
    
    plt.tight_layout()
    figures['llm_stacked_counts'] = fig7a
    
    # Stacked bar for SOMEF (separate figure)
    fig7b, ax7b = plt.subplots(figsize=BAR_FIGURE_SIZE)
    
    # Calculate cumulative values for SOMEF
    somef_tp = counts_df_sorted['SOMEF_TP']
    somef_fp = counts_df_sorted['SOMEF_FP']
    somef_fn = counts_df_sorted['SOMEF_FN']
    somef_pe = counts_df_sorted['SOMEF_PE']
    
    # Create stacked bars
    somef_bars1 = ax7b.bar(counts_df_sorted['Element'], somef_tp, label='TP', color='green')
    somef_bars2 = ax7b.bar(counts_df_sorted['Element'], somef_fp, bottom=somef_tp, label='FP', color='red')
    somef_bars3 = ax7b.bar(counts_df_sorted['Element'], somef_fn, bottom=somef_tp + somef_fp, label='FN', color='orange')
    somef_bars4 = ax7b.bar(counts_df_sorted['Element'], somef_pe, bottom=somef_tp + somef_fp + somef_fn, label='PE', color='purple')
    
    # Add labels to each segment
    for i, (element, tp, fp, fn, pe) in enumerate(zip(counts_df_sorted['Element'], somef_tp, somef_fp, somef_fn, somef_pe)):
        # Only add labels for non-zero values
        if tp > 0:
            ax7b.text(i, tp/2, str(int(tp)), ha='center', va='center', fontweight='bold', color='white')
        if fp > 0:
            ax7b.text(i, tp + fp/2, str(int(fp)), ha='center', va='center', fontweight='bold', color='white')
        if fn > 0:
            ax7b.text(i, tp + fp + fn/2, str(int(fn)), ha='center', va='center', fontweight='bold', color='white')
        if pe > 0:
            ax7b.text(i, tp + fp + fn + pe/2, str(int(pe)), ha='center', va='center', fontweight='bold', color='white')
    
    ax7b.set_title('SOMEF Result Counts by Element')
    ax7b.set_ylabel('Count')
    ax7b.legend()
    plt.setp(ax7b.get_xticklabels(), rotation=0)
    
    plt.tight_layout()
    figures['somef_stacked_counts'] = fig7b
    
    return figures

# Updated function for repository F1 distribution plot
def add_repository_f1_distribution_plot(results):
    """
    Creates a visualization of repository-level F1 score distributions for LLM and SOMEF.
    
    Args:
        results: List of dictionaries containing comparison results for each repository
        
    Returns:
        Dictionary of matplotlib figure objects
    """
    figures = {}
    
    # Create F1 score distribution intervals
    f1_intervals = [(0.0, 0.2), (0.2, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1.0)]
    interval_labels = ['0.0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']
    
    # Count repositories in each F1 score interval
    llm_counts = [0] * len(f1_intervals)
    somef_counts = [0] * len(f1_intervals)
    
    # Extract F1 scores for each repository
    for repo in results:
        # Get LLM F1 score for the repository
        llm_f1 = float(repo.get('llm', {}).get('f1', 0))
        
        # Get SOMEF F1 score for the repository
        somef_f1 = float(repo.get('somef', {}).get('f1', 0))
        
        # Classify LLM F1 score
        for i, (lower, upper) in enumerate(f1_intervals):
            if lower <= llm_f1 < upper or (i == len(f1_intervals)-1 and llm_f1 == upper):
                llm_counts[i] += 1
                break
        
        # Classify SOMEF F1 score
        for i, (lower, upper) in enumerate(f1_intervals):
            if lower <= somef_f1 < upper or (i == len(f1_intervals)-1 and somef_f1 == upper):
                somef_counts[i] += 1
                break
    
    # Create figure for F1 distribution
    fig, ax = plt.subplots(figsize=BAR_FIGURE_SIZE)
    x = np.arange(len(interval_labels))
    width = 0.35
    
    # Plot the distributions
    llm_bars = ax.bar(x - width/2, llm_counts, width, label='LLM')
    somef_bars = ax.bar(x + width/2, somef_counts, width, label='SOMEF')
    
    # Add count labels with add_value_labels function instead of manual text addition
    add_value_labels(ax, llm_bars, precision=0)
    add_value_labels(ax, somef_bars, precision=0)
    
    ax.set_xlabel('F1 Score Range')
    ax.set_ylabel('Number of Repositories')
    ax.set_title('Distribution of Repository-Level F1 Scores')
    ax.set_xticks(x)
    ax.set_xticklabels(interval_labels)
    ax.legend()
    
    plt.tight_layout()
    figures['repository_f1_distribution'] = fig
    
    return figures

def create_user_friendly_counts_plot(counts_df):
    """
    Creates a more user-friendly visualization of element counts with 
    descriptive labels instead of TP, FP, FN, etc.
    
    Args:
        counts_df: DataFrame with raw counts for each element
        
    Returns:
        Dictionary of matplotlib figure objects
    """
    figures = {}
    
    # Create more user-friendly labels
    friendly_labels = {
        'TP': 'Correctly Extracted',
        'FP': 'Incorrectly Extracted',
        'FN': 'Missed Extraction',
        'TN': 'Correctly Not Extracted',
        'PE': 'Partially Extracted'
    }
    
    # Get elements list
    elements = counts_df['Element'].tolist()
    
    # Create figure for LLM
    fig_llm, ax_llm = plt.subplots(figsize=BAR_FIGURE_SIZE)
    
    x = np.arange(len(elements))
    width = 0.15
    
    # Plot the raw counts for LLM with friendly labels
    tp_bars = ax_llm.bar(x - width*2, counts_df['LLM_TP'], width, label=friendly_labels['TP'], color='green')
    fp_bars = ax_llm.bar(x - width, counts_df['LLM_FP'], width, label=friendly_labels['FP'], color='red')
    fn_bars = ax_llm.bar(x, counts_df['LLM_FN'], width, label=friendly_labels['FN'], color='orange')
    tn_bars = ax_llm.bar(x + width, counts_df['LLM_TN'], width, label=friendly_labels['TN'], color='blue')
    pe_bars = ax_llm.bar(x + width*2, counts_df['LLM_PE'], width, label=friendly_labels['PE'], color='purple')
    
    # Add value labels
    add_value_labels(ax_llm, tp_bars, precision=0)
    add_value_labels(ax_llm, fp_bars, precision=0)
    add_value_labels(ax_llm, fn_bars, precision=0)
    add_value_labels(ax_llm, tn_bars, precision=0)
    add_value_labels(ax_llm, pe_bars, precision=0)
    
    ax_llm.set_ylabel('Count')
    ax_llm.set_title('LLM Element Extraction Results')
    ax_llm.set_xticks(x)
    ax_llm.set_xticklabels(elements, rotation=0)
    ax_llm.legend()
    
    plt.tight_layout()
    figures['llm_friendly_counts'] = fig_llm
    
    # Create figure for SOMEF (non-stacked)
    fig_somef, ax_somef = plt.subplots(figsize=BAR_FIGURE_SIZE)
    
    # Plot the raw counts for SOMEF with friendly labels
    somef_tp_bars = ax_somef.bar(x - width*2, counts_df['SOMEF_TP'], width, label=friendly_labels['TP'], color='green')
    somef_fp_bars = ax_somef.bar(x - width, counts_df['SOMEF_FP'], width, label=friendly_labels['FP'], color='red')
    somef_fn_bars = ax_somef.bar(x, counts_df['SOMEF_FN'], width, label=friendly_labels['FN'], color='orange')
    somef_tn_bars = ax_somef.bar(x + width, counts_df['SOMEF_TN'], width, label=friendly_labels['TN'], color='blue')
    somef_pe_bars = ax_somef.bar(x + width*2, counts_df['SOMEF_PE'], width, label=friendly_labels['PE'], color='purple')
    
    # Add value labels
    add_value_labels(ax_somef, somef_tp_bars, precision=0)
    add_value_labels(ax_somef, somef_fp_bars, precision=0)
    add_value_labels(ax_somef, somef_fn_bars, precision=0)
    add_value_labels(ax_somef, somef_tn_bars, precision=0)
    add_value_labels(ax_somef, somef_pe_bars, precision=0)
    
    ax_somef.set_ylabel('Count')
    ax_somef.set_title('SOMEF Element Extraction Results')
    ax_somef.set_xticks(x)
    ax_somef.set_xticklabels(elements, rotation=0)
    ax_somef.legend()
    
    plt.tight_layout()
    figures['somef_friendly_counts'] = fig_somef
    
    # Create figure for LLM stacked
    fig_llm_stacked = plt.figure(figsize=BAR_FIGURE_SIZE)
    ax_llm_stacked = fig_llm_stacked.add_subplot(111)
    
    # Calculate cumulative values
    llm_tp = counts_df['LLM_TP']
    llm_fp = counts_df['LLM_FP']
    llm_fn = counts_df['LLM_FN']
    llm_pe = counts_df['LLM_PE']
    
    # Create stacked bars
    ax_llm_stacked.bar(elements, llm_tp, label=friendly_labels['TP'], color='green')
    ax_llm_stacked.bar(elements, llm_fp, bottom=llm_tp, label=friendly_labels['FP'], color='red')
    ax_llm_stacked.bar(elements, llm_fn, bottom=llm_tp + llm_fp, label=friendly_labels['FN'], color='orange')
    ax_llm_stacked.bar(elements, llm_pe, bottom=llm_tp + llm_fp + llm_fn, label=friendly_labels['PE'], color='purple')
    
    # Add labels to each segment
    for i, (element, tp, fp, fn, pe) in enumerate(zip(elements, llm_tp, llm_fp, llm_fn, llm_pe)):
        # Only add labels for non-zero values
        if tp > 0:
            ax_llm_stacked.text(i, tp/2, str(int(tp)), ha='center', va='center', fontweight='bold', color='white')
        if fp > 0:
            ax_llm_stacked.text(i, tp + fp/2, str(int(fp)), ha='center', va='center', fontweight='bold', color='white')
        if fn > 0:
            ax_llm_stacked.text(i, tp + fp + fn/2, str(int(fn)), ha='center', va='center', fontweight='bold', color='white')
        if pe > 0:
            ax_llm_stacked.text(i, tp + fp + fn + pe/2, str(int(pe)), ha='center', va='center', fontweight='bold', color='white')
    
    ax_llm_stacked.set_title('LLM Element Extraction Results')
    ax_llm_stacked.set_ylabel('Count')
    ax_llm_stacked.legend()
    plt.setp(ax_llm_stacked.get_xticklabels(), rotation=0)
    plt.tight_layout()
    figures['llm_stacked_friendly'] = fig_llm_stacked
    
    # Stacked bar for SOMEF (separate figure)
    fig_somef_stacked = plt.figure(figsize=BAR_FIGURE_SIZE)
    ax_somef_stacked = fig_somef_stacked.add_subplot(111)
    
    # Calculate cumulative values for SOMEF
    somef_tp = counts_df['SOMEF_TP']
    somef_fp = counts_df['SOMEF_FP']
    somef_fn = counts_df['SOMEF_FN']
    somef_pe = counts_df['SOMEF_PE']
    
    # Create stacked bars
    ax_somef_stacked.bar(elements, somef_tp, label=friendly_labels['TP'], color='green')
    ax_somef_stacked.bar(elements, somef_fp, bottom=somef_tp, label=friendly_labels['FP'], color='red')
    ax_somef_stacked.bar(elements, somef_fn, bottom=somef_tp + somef_fp, label=friendly_labels['FN'], color='orange')
    ax_somef_stacked.bar(elements, somef_pe, bottom=somef_tp + somef_fp + somef_fn, label=friendly_labels['PE'], color='purple')
    
    # Add labels to each segment
    for i, (element, tp, fp, fn, pe) in enumerate(zip(elements, somef_tp, somef_fp, somef_fn, somef_pe)):
        # Only add labels for non-zero values
        if tp > 0:
            ax_somef_stacked.text(i, tp/2, str(int(tp)), ha='center', va='center', fontweight='bold', color='white')
        if fp > 0:
            ax_somef_stacked.text(i, tp + fp/2, str(int(fp)), ha='center', va='center', fontweight='bold', color='white')
        if fn > 0:
            ax_somef_stacked.text(i, tp + fp + fn/2, str(int(fn)), ha='center', va='center', fontweight='bold', color='white')
        if pe > 0:
            ax_somef_stacked.text(i, tp + fp + fn + pe/2, str(int(pe)), ha='center', va='center', fontweight='bold', color='white')
    
    ax_somef_stacked.set_title('SOMEF Element Extraction Results')
    ax_somef_stacked.set_ylabel('Count')
    ax_somef_stacked.legend()
    plt.setp(ax_somef_stacked.get_xticklabels(), rotation=0)
    plt.tight_layout()
    figures['somef_stacked_friendly'] = fig_somef_stacked
    
    return figures

def print_summary(comparison_df, counts_df, llm_avg, somef_avg):
    """
    Prints a summary of the analysis results.
    
    Args:
        comparison_df: DataFrame with element-wise comparison metrics
        counts_df: DataFrame with raw counts for each element
        llm_avg: Dictionary with LLM average metrics
        somef_avg: Dictionary with SOMEF average metrics
    """
    # Element-wise comparison sorted by F1 difference
    print("Element-wise comparison between LLM and SOMEF (sorted by F1 difference):")
    metrics_cols = ['Element', 'LLM_F1', 'SOMEF_F1', 'F1_Diff']
    print(comparison_df[metrics_cols].sort_values('F1_Diff', ascending=False).round(3))
    
    # Overall average performance
    print("\nOverall average performance:")
    print("LLM Average:", {k: round(v, 3) for k, v in llm_avg.items()})
    print("SOMEF Average:", {k: round(v, 3) for k, v in somef_avg.items()})
    
    # Full metrics comparison
    print("\nFull metrics comparison:")
    full_metrics = ['Element', 
                   'LLM_Precision', 'SOMEF_Precision', 'Precision_Diff',
                   'LLM_Recall', 'SOMEF_Recall', 'Recall_Diff',
                   'LLM_F1', 'SOMEF_F1', 'F1_Diff',
                   'LLM_Accuracy', 'SOMEF_Accuracy', 'Accuracy_Diff']
    print(comparison_df[full_metrics].round(3))
    
    # Raw counts by element
    print("\nRaw counts by element:")
    print(counts_df)

def save_visualizations(figures, output_dir="./element_result"):
    """
    Saves all visualization figures to files.
    
    Args:
        figures: Dictionary of matplotlib figure objects
        output_dir: Directory to save the figures
    """
    for name, fig in figures.items():
        filename = f"{output_dir}/{name}.pdf"
        fig.savefig(filename, format='pdf', bbox_inches='tight')
        print(f"Saved {filename}")

def updated_create_visualizations(comparison_df, counts_df, llm_avg, somef_avg):
    """
    Creates all visualizations including the user-friendly counts plots.
    
    Args:
        comparison_df: DataFrame with element-wise comparison metrics
        counts_df: DataFrame with raw counts for each element
        llm_avg: Dictionary with LLM average metrics
        somef_avg: Dictionary with SOMEF average metrics
        
    Returns:
        Dictionary of matplotlib figure objects
    """
    # Get original visualizations
    figures = create_visualizations(comparison_df, counts_df, llm_avg, somef_avg)
    
    # Add user-friendly counts plot
    friendly_counts_figures = create_user_friendly_counts_plot(counts_df)
    figures.update(friendly_counts_figures)
    
    return figures

def create_error_counts_comparison(counts_df):
    figures = {}

    elements = counts_df['Element'].tolist()
    x = np.arange(len(elements))
    width = 0.35

    llm_fp = counts_df['LLM_FP']
    llm_fn = counts_df['LLM_FN']
    somef_fp = counts_df['SOMEF_FP']
    somef_fn = counts_df['SOMEF_FN']

    llm_errors = llm_fp + llm_fn
    somef_errors = somef_fp + somef_fn

    # Calculate percent difference (how much fewer errors LLM made)
    percent_diff = ((somef_errors - llm_errors) / somef_errors * 100).replace([np.inf, -np.inf], np.nan).fillna(0)

    fig, ax = plt.subplots(figsize=BAR_FIGURE_SIZE)

    llm_bars = ax.bar(x - width/2, llm_errors, width, label='LLM Errors (FP + FN)', color='#3366CC')
    somef_bars = ax.bar(x + width/2, somef_errors, width, label='SOMEF Errors (FP + FN)', color='#DC3912')

    # Add count labels only if > 0
    def add_count_labels(bars):
        for bar in bars:
            height = bar.get_height()
            if height > 0:
                x_pos = bar.get_x() + bar.get_width() / 2
                ax.text(x_pos, height + 0.5, f'{int(height)}', ha='center', va='bottom', fontsize=9, fontweight='bold')

    add_count_labels(llm_bars)
    add_count_labels(somef_bars)

    # Add percentage difference between bars — no label, just number
    for i in range(len(x)):
        mid_x = x[i]
        max_height = max(llm_errors[i], somef_errors[i])
        diff = percent_diff[i]
        # Optional: add ↓ arrow if positive
        arrow = ' ↓' if diff > 0 else ''
        ax.text(mid_x, max_height + 4.5, f"{abs(diff):.1f}%{arrow}", ha='center', va='bottom', fontsize=9, color='green' if diff > 0 else 'gray')

    ax.set_ylabel('Error Count')
    ax.set_title('LLM vs SOMEF Total Errors (FP + FN)')
    ax.set_xticks(x)
    ax.set_xticklabels(elements, rotation=0)
    ax.legend()
    # After ax.legend()
    ax.text(0.5, -0.1, '↓ indicates percentage fewer errors compared to SOMEF', 
        ha='center', va='center', transform=ax.transAxes, fontsize=9, color='gray')
    ax.set_ylim(0, max(max(llm_errors), max(somef_errors)) * 1.6)

    figures['error_counts_comparison'] = fig
    return figures

# Modify the existing run_enhanced_analysis function to include the new visualizations
def run_enhanced_analysis(results):
    """
    Run the complete analysis on the results array with enhanced visualizations.
    
    Args:
        results: The array of repository comparison data
        
    Returns:
        Dictionary containing all analysis results
    """
    # Perform element-wise comparison
    comparison_df, counts_df, llm_avg, somef_avg = element_wise_comparison(results)
    
    # Print summary
    print_summary(comparison_df, counts_df, llm_avg, somef_avg)
    
    try:
        # Create visualizations with enhanced features
        figures = updated_create_visualizations(comparison_df, counts_df, llm_avg, somef_avg)
        
        # Add repository-level F1 distribution plot
        repo_f1_dist_figures = add_repository_f1_distribution_plot(results)
        figures.update(repo_f1_dist_figures)
        
        # Add error counts comparison
        error_counts_figures = create_error_counts_comparison(counts_df)
        figures.update(error_counts_figures)
        
        # Save figures
        save_visualizations(figures)
        
        print("All visualizations created successfully")
    except Exception as e:
        print(f"Error creating visualizations: {e}")
        figures = {}
    
    return {
        'comparison_df': comparison_df,
        'counts_df': counts_df,
        'llm_avg': llm_avg,
        'somef_avg': somef_avg,
        'figures': figures
    }


analysis_results = run_enhanced_analysis(results)