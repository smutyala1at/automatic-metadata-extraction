{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-03T13:28:08.683400Z",
     "iopub.status.busy": "2025-04-03T13:28:08.683087Z",
     "iopub.status.idle": "2025-04-03T13:28:08.816544Z",
     "shell.execute_reply": "2025-04-03T13:28:08.815717Z",
     "shell.execute_reply.started": "2025-04-03T13:28:08.683374Z"
    },
    "id": "-VuenKPqoNfS",
    "outputId": "5ab34496-0aaa-4706-dc8d-113b38f274e3",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python -V \n",
    "!nvcc --version # find the CUDA driver build above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-03T13:28:10.747314Z",
     "iopub.status.busy": "2025-04-03T13:28:10.746975Z",
     "iopub.status.idle": "2025-04-03T13:28:30.575796Z",
     "shell.execute_reply": "2025-04-03T13:28:30.574931Z",
     "shell.execute_reply.started": "2025-04-03T13:28:10.747282Z"
    },
    "id": "-BfhqNzQk06m",
    "outputId": "6d8c6e00-5106-4df0-e2ea-18540788848d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
      "Collecting llama-cpp-python==0.2.90\n",
      "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.90-cu122/llama_cpp_python-0.2.90-cp310-cp310-linux_x86_64.whl (443.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 MB\u001b[0m \u001b[31m234.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.90) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.90) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.90)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.90) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.90) (2.1.5)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.90\n"
     ]
    }
   ],
   "source": [
    "# Install key libraries for LLM\n",
    "\n",
    "#Install llama-cpp-python with CUBLAS, compatible to CUDA 12.2 which is the CUDA driver build above\n",
    "!set LLAMA_CUBLAS=1\n",
    "!set CMAKE_ARGS=-DLLAMA_CUBLAS=on\n",
    "!set FORCE_CMAKE=1\n",
    "\n",
    "#Install llama-cpp-python, cuda-enabled package\n",
    "!python -m pip install --no-cache-dir llama-cpp-python==0.2.90 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-03T13:28:56.082965Z",
     "iopub.status.busy": "2025-04-03T13:28:56.082626Z",
     "iopub.status.idle": "2025-04-03T13:28:56.089650Z",
     "shell.execute_reply": "2025-04-03T13:28:56.088767Z",
     "shell.execute_reply.started": "2025-04-03T13:28:56.082941Z"
    },
    "id": "Du8AaWpgpUeu",
    "outputId": "61c4ab54-518e-4eb0-f3b0-4397106124dc",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gpu_requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile gpu_requirements.txt\n",
    "annotated-types==0.7.0\n",
    "anyio==4.4.0\n",
    "certifi==2022.12.7\n",
    "charset-normalizer==2.1.1\n",
    "click==8.1.7\n",
    "colorama==0.4.6\n",
    "diskcache==5.6.3\n",
    "dnspython==2.6.1\n",
    "email_validator==2.1.1\n",
    "exceptiongroup==1.2.1\n",
    "filelock==3.13.1\n",
    "fsspec==2024.6.0\n",
    "h11==0.14.0\n",
    "httpcore==1.0.5\n",
    "httptools==0.6.1\n",
    "httpx==0.27.0\n",
    "huggingface-hub==0.23.3\n",
    "idna==3.4\n",
    "Jinja2==3.1.4\n",
    "llama_cpp_python==0.2.90\n",
    "markdown-it-py==3.0.0\n",
    "MarkupSafe==2.1.5\n",
    "mdurl==0.1.2\n",
    "mpmath==1.3.0\n",
    "networkx==3.2.1\n",
    "numpy==1.26.4\n",
    "orjson==3.10.3\n",
    "packaging==24.0\n",
    "pillow==10.2.0\n",
    "pydantic==2.7.3\n",
    "pydantic_core==2.18.4\n",
    "Pygments==2.18.0\n",
    "python-dotenv==1.0.1\n",
    "python-multipart==0.0.9\n",
    "PyYAML==6.0.1\n",
    "requests==2.28.1\n",
    "rich==13.7.1\n",
    "shellingham==1.5.4\n",
    "sniffio==1.3.1\n",
    "starlette==0.37.2\n",
    "sympy==1.12\n",
    "torch==2.3.0\n",
    "torchaudio==2.3.0\n",
    "torchvision==0.18.0\n",
    "tqdm==4.66.4\n",
    "typer==0.12.3\n",
    "typing_extensions==4.12.1\n",
    "ujson==5.10.0\n",
    "watchfiles==0.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-03T13:28:56.734704Z",
     "iopub.status.busy": "2025-04-03T13:28:56.734401Z",
     "iopub.status.idle": "2025-04-03T13:32:37.564662Z",
     "shell.execute_reply": "2025-04-03T13:32:37.563634Z",
     "shell.execute_reply.started": "2025-04-03T13:28:56.734679Z"
    },
    "id": "hdVSk5iZ1DVB",
    "outputId": "7949862f-4458-4803-e5ce-661d3bcc3cb9",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 1)) (0.7.0)\n",
      "Collecting anyio==4.4.0 (from -r gpu_requirements.txt (line 2))\n",
      "  Downloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting certifi==2022.12.7 (from -r gpu_requirements.txt (line 3))\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting charset-normalizer==2.1.1 (from -r gpu_requirements.txt (line 4))\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 5)) (8.1.7)\n",
      "Requirement already satisfied: colorama==0.4.6 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 6)) (0.4.6)\n",
      "Requirement already satisfied: diskcache==5.6.3 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 7)) (5.6.3)\n",
      "Collecting dnspython==2.6.1 (from -r gpu_requirements.txt (line 8))\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting email_validator==2.1.1 (from -r gpu_requirements.txt (line 9))\n",
      "  Downloading email_validator-2.1.1-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting exceptiongroup==1.2.1 (from -r gpu_requirements.txt (line 10))\n",
      "  Downloading exceptiongroup-1.2.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting filelock==3.13.1 (from -r gpu_requirements.txt (line 11))\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting fsspec==2024.6.0 (from -r gpu_requirements.txt (line 12))\n",
      "  Downloading fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting h11==0.14.0 (from -r gpu_requirements.txt (line 13))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting httpcore==1.0.5 (from -r gpu_requirements.txt (line 14))\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting httptools==0.6.1 (from -r gpu_requirements.txt (line 15))\n",
      "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting httpx==0.27.0 (from -r gpu_requirements.txt (line 16))\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting huggingface-hub==0.23.3 (from -r gpu_requirements.txt (line 17))\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting idna==3.4 (from -r gpu_requirements.txt (line 18))\n",
      "  Downloading idna-3.4-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: Jinja2==3.1.4 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 19)) (3.1.4)\n",
      "Requirement already satisfied: llama_cpp_python==0.2.90 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 20)) (0.2.90)\n",
      "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 21)) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe==2.1.5 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 22)) (2.1.5)\n",
      "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 23)) (0.1.2)\n",
      "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 24)) (1.3.0)\n",
      "Collecting networkx==3.2.1 (from -r gpu_requirements.txt (line 25))\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 26)) (1.26.4)\n",
      "Collecting orjson==3.10.3 (from -r gpu_requirements.txt (line 27))\n",
      "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting packaging==24.0 (from -r gpu_requirements.txt (line 28))\n",
      "  Downloading packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pillow==10.2.0 (from -r gpu_requirements.txt (line 29))\n",
      "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting pydantic==2.7.3 (from -r gpu_requirements.txt (line 30))\n",
      "  Downloading pydantic-2.7.3-py3-none-any.whl.metadata (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic_core==2.18.4 (from -r gpu_requirements.txt (line 31))\n",
      "  Downloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: Pygments==2.18.0 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 32)) (2.18.0)\n",
      "Collecting python-dotenv==1.0.1 (from -r gpu_requirements.txt (line 33))\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting python-multipart==0.0.9 (from -r gpu_requirements.txt (line 34))\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting PyYAML==6.0.1 (from -r gpu_requirements.txt (line 35))\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting requests==2.28.1 (from -r gpu_requirements.txt (line 36))\n",
      "  Downloading requests-2.28.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting rich==13.7.1 (from -r gpu_requirements.txt (line 37))\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: shellingham==1.5.4 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 38)) (1.5.4)\n",
      "Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 39)) (1.3.1)\n",
      "Collecting starlette==0.37.2 (from -r gpu_requirements.txt (line 40))\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting sympy==1.12 (from -r gpu_requirements.txt (line 41))\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting torch==2.3.0 (from -r gpu_requirements.txt (line 42))\n",
      "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchaudio==2.3.0 (from -r gpu_requirements.txt (line 43))\n",
      "  Downloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torchvision==0.18.0 (from -r gpu_requirements.txt (line 44))\n",
      "  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting tqdm==4.66.4 (from -r gpu_requirements.txt (line 45))\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer==0.12.3 (from -r gpu_requirements.txt (line 46))\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting typing_extensions==4.12.1 (from -r gpu_requirements.txt (line 47))\n",
      "  Downloading typing_extensions-4.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: ujson==5.10.0 in /usr/local/lib/python3.10/dist-packages (from -r gpu_requirements.txt (line 48)) (5.10.0)\n",
      "Collecting watchfiles==0.22.0 (from -r gpu_requirements.txt (line 49))\n",
      "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests==2.28.1->-r gpu_requirements.txt (line 36))\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->-r gpu_requirements.txt (line 42))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->-r gpu_requirements.txt (line 42))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->-r gpu_requirements.txt (line 42))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->-r gpu_requirements.txt (line 42))\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->-r gpu_requirements.txt (line 42))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->-r gpu_requirements.txt (line 42))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->-r gpu_requirements.txt (line 42))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->-r gpu_requirements.txt (line 42))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->-r gpu_requirements.txt (line 42))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->-r gpu_requirements.txt (line 42))\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->-r gpu_requirements.txt (line 42))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.0 (from torch==2.3.0->-r gpu_requirements.txt (line 42))\n",
      "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->-r gpu_requirements.txt (line 42))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Downloading anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
      "Downloading exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-24.0-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.7.3-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mmm\n",
      "\u001b[?25hDownloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.3.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.1-py3-none-any.whl (37 kB)\n",
      "Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, typing_extensions, tqdm, sympy, PyYAML, python-multipart, python-dotenv, pillow, packaging, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, idna, httptools, h11, fsspec, filelock, exceptiongroup, dnspython, charset-normalizer, certifi, triton, rich, requests, pydantic_core, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, email_validator, anyio, watchfiles, typer, starlette, pydantic, nvidia-cusolver-cu12, huggingface-hub, httpx, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.5\n",
      "    Uninstalling tqdm-4.66.5:\n",
      "      Successfully uninstalled tqdm-4.66.5\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.4.0\n",
      "    Uninstalling pillow-10.4.0:\n",
      "      Successfully uninstalled pillow-10.4.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.3\n",
      "    Uninstalling networkx-3.3:\n",
      "      Successfully uninstalled networkx-3.3\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.16.1\n",
      "    Uninstalling filelock-3.16.1:\n",
      "      Successfully uninstalled filelock-3.16.1\n",
      "  Attempting uninstall: exceptiongroup\n",
      "    Found existing installation: exceptiongroup 1.2.2\n",
      "    Uninstalling exceptiongroup-1.2.2:\n",
      "      Successfully uninstalled exceptiongroup-1.2.2\n",
      "  Attempting uninstall: dnspython\n",
      "    Found existing installation: dnspython 2.7.0\n",
      "    Uninstalling dnspython-2.7.0:\n",
      "      Successfully uninstalled dnspython-2.7.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.8.30\n",
      "    Uninstalling certifi-2024.8.30:\n",
      "      Successfully uninstalled certifi-2024.8.30\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.8.1\n",
      "    Uninstalling rich-13.8.1:\n",
      "      Successfully uninstalled rich-13.8.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: pydantic_core\n",
      "    Found existing installation: pydantic_core 2.23.4\n",
      "    Uninstalling pydantic_core-2.23.4:\n",
      "      Successfully uninstalled pydantic_core-2.23.4\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.7.1\n",
      "    Uninstalling anyio-3.7.1:\n",
      "      Successfully uninstalled anyio-3.7.1\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.12.5\n",
      "    Uninstalling typer-0.12.5:\n",
      "      Successfully uninstalled typer-0.12.5\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.9.2\n",
      "    Uninstalling pydantic-2.9.2:\n",
      "      Successfully uninstalled pydantic-2.9.2\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.24.7\n",
      "    Uninstalling huggingface-hub-0.24.7:\n",
      "      Successfully uninstalled huggingface-hub-0.24.7\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.1+cu121\n",
      "    Uninstalling torch-2.4.1+cu121:\n",
      "      Successfully uninstalled torch-2.4.1+cu121\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.19.1+cu121\n",
      "    Uninstalling torchvision-0.19.1+cu121:\n",
      "      Successfully uninstalled torchvision-0.19.1+cu121\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.4.1+cu121\n",
      "    Uninstalling torchaudio-2.4.1+cu121:\n",
      "      Successfully uninstalled torchaudio-2.4.1+cu121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 18.1.0 which is incompatible.\n",
      "datasets 3.2.0 requires requests>=2.32.2, but you have requests 2.28.1 which is incompatible.\n",
      "distributed 2024.8.0 requires dask==2024.8.0, but you have dask 2024.12.1 which is incompatible.\n",
      "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.6.0 which is incompatible.\n",
      "google-cloud-bigtable 2.26.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.28.1 which is incompatible.\n",
      "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 18.1.0 which is incompatible.\n",
      "jupyterlab-server 2.27.3 requires requests>=2.31, but you have requests 2.28.1 which is incompatible.\n",
      "kaggle 1.6.17 requires certifi>=2023.7.22, but you have certifi 2022.12.7 which is incompatible.\n",
      "pandas-gbq 0.23.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "pytensor 2.25.4 requires filelock>=3.15, but you have filelock 3.13.1 which is incompatible.\n",
      "yfinance 0.2.43 requires requests>=2.31, but you have requests 2.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.1 anyio-4.4.0 certifi-2022.12.7 charset-normalizer-2.1.1 dnspython-2.6.1 email_validator-2.1.1 exceptiongroup-1.2.1 filelock-3.13.1 fsspec-2024.6.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface-hub-0.23.3 idna-3.4 networkx-3.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.1.105 orjson-3.10.3 packaging-24.0 pillow-10.2.0 pydantic-2.7.3 pydantic_core-2.18.4 python-dotenv-1.0.1 python-multipart-0.0.9 requests-2.28.1 rich-13.7.1 starlette-0.37.2 sympy-1.12 torch-2.3.0 torchaudio-2.3.0 torchvision-0.18.0 tqdm-4.66.4 triton-2.3.0 typer-0.12.3 typing_extensions-4.12.1 urllib3-1.26.20 watchfiles-0.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r gpu_requirements.txt #it's normal to see incompatiblity errors; the most important packages have been installed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:32:37.566173Z",
     "iopub.status.busy": "2025-04-03T13:32:37.565927Z",
     "iopub.status.idle": "2025-04-03T13:32:37.570109Z",
     "shell.execute_reply": "2025-04-03T13:32:37.569213Z",
     "shell.execute_reply.started": "2025-04-03T13:32:37.566150Z"
    },
    "id": "kpXQGhHlij6q",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-03T13:32:37.572444Z",
     "iopub.status.busy": "2025-04-03T13:32:37.572146Z",
     "iopub.status.idle": "2025-04-03T13:33:53.975229Z",
     "shell.execute_reply": "2025-04-03T13:33:53.974500Z",
     "shell.execute_reply.started": "2025-04-03T13:32:37.572412Z"
    },
    "id": "WsYot3Rz1NVf",
    "outputId": "aedeeaff-80b9-450c-f992-b1fd5cbd4f2c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My model path: models/unsloth.F16.gguf\n"
     ]
    }
   ],
   "source": [
    "# Downloaded the trained model from huggingface hub, please replace token with your own token\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name = \"smutyala/llama_f16_gguf_model\"\n",
    "model_file = \"unsloth.F16.gguf\"\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    model_name,\n",
    "    filename=model_file,\n",
    "    local_dir='models/',  # Download the model to the \"models\" folder\n",
    "    token=\"\"  #Replace this token from huggingface with your own token (Setting -> Access Toekns -> New token -> Generate Token)\n",
    ")\n",
    "\n",
    "print(\"My model path:\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:33:53.976689Z",
     "iopub.status.busy": "2025-04-03T13:33:53.976433Z",
     "iopub.status.idle": "2025-04-03T13:33:58.675741Z",
     "shell.execute_reply": "2025-04-03T13:33:58.674722Z",
     "shell.execute_reply.started": "2025-04-03T13:33:53.976665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]                                \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease                                              \n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]                           \n",
      "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [70.9 kB]                 \n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]                             \n",
      "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,381 kB]\n",
      "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,808 kB]                       \n",
      "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]             \n",
      "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]       \n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]                    \n",
      "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,688 kB]                    \n",
      "Get:14 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]         \n",
      "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease    \n",
      "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,972 kB]\n",
      "Get:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,140 kB]    \n",
      "Get:19 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.8 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,773 kB]              \n",
      "Get:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,241 kB]       \n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,540 kB]         \n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,087 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Fetched 30.4 MB in 3s (11.6 MB/s)                             \n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "#!apt install -y cuda-toolkit-12-2\n",
    "!sudo apt-get install -y cuda-toolkit-12-2\n",
    "!python -m pip install --no-cache-dir llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-03T13:35:05.467521Z",
     "iopub.status.busy": "2025-04-03T13:35:05.467218Z",
     "iopub.status.idle": "2025-04-03T13:36:13.372721Z",
     "shell.execute_reply": "2025-04-03T13:36:13.371814Z",
     "shell.execute_reply.started": "2025-04-03T13:35:05.467490Z"
    },
    "id": "fMyy4TNcoKjU",
    "outputId": "a6548bdb-1e29-4d63-fb8d-99abec8d5437",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 31 key-value pairs and 292 tensors from /kaggle/working/models/unsloth.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct-bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 14.96 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct Bnb 4bit\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128004 '<|finetune_right_pad_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 2 CUDA devices:\n",
      "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
      "  Device 1: Tesla T4, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.41 MiB\n",
      "Exception ignored on calling ctypes callback function: <function llama_log_callback at 0x7d1336f4eb90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp/_logger.py\", line 23, in llama_log_callback\n",
      "    @llama_cpp.llama_log_callback\n",
      "KeyboardInterrupt: \n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  1002.00 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  7072.54 MiB\n",
      "llm_load_tensors:      CUDA1 buffer size =  7242.49 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 48000\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  3187.50 MiB\n",
      "llama_kv_cache_init:      CUDA1 KV buffer size =  2812.50 MiB\n",
      "llama_new_context_with_model: KV self size  = 6000.00 MiB, K (f16): 3000.00 MiB, V (f16): 3000.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  3439.01 MiB\n",
      "llama_new_context_with_model:      CUDA1 compute buffer size =  3439.02 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   383.02 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '1', 'llama.attention.value_length': '128', 'llama.attention.key_length': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'tokenizer.ggml.padding_token_id': '128004', 'general.basename': 'Meta-Llama-3.1', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Meta Llama 3.1 8B Instruct Bnb 4bit', 'general.organization': 'Unsloth', 'general.finetune': 'Instruct-bnb-4bit', 'general.type': 'model', 'general.size_label': '8B', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# Llamacpp Configuration\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"/kaggle/working/models/unsloth.F16.gguf\", \n",
    "    n_gpu_layers=-1,          # Try using all layers on GPU again\n",
    "    n_ctx=48000,              # Balanced context size\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:37:08.759294Z",
     "iopub.status.busy": "2025-04-03T13:37:08.759002Z",
     "iopub.status.idle": "2025-04-03T13:37:08.764979Z",
     "shell.execute_reply": "2025-04-03T13:37:08.764221Z",
     "shell.execute_reply.started": "2025-04-03T13:37:08.759269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: First run get_repo_files script from data_extraction/ folder to extract repo files\n",
    "# Then use the extracted data as input here instead of this fsbrain test example\n",
    "\n",
    "content =  \"# fsbrain [![DOI](https://zenodo.org/badge/209085379.svg)](https://zenodo.org/doi/10.5281/zenodo.3559816) An R package for structural neuroimaging. Provides high-level functions to access (read and write) and visualize surface-based brain morphometry data (e.g. cortical thickness) for individual subjects and groups. ![Vis](https://github.com/dfsp-spirit/fsbrain_gallery/blob/master/surface/fsbrain_sulcal_depth_cbar_web.jpg?raw=true \\\"Sulcal depth visualization, created with fsbrain\\\") **Fig.1**: *Visualization of sulcal depth for a subject in FreeSurfer standard space (fsaverage). See the [source code to reproduce this image](https://htmlpreview.github.io/?https://github.com/dfsp-spirit/fsbrain/blob/develop/web/Rmd_web_examples/examples_export.html) in an R notebook.* [About](#about) | [Installation](#installation) | [Documentation](#documentation) | [Unit tests](#unit-tests-and-continuous-integration) | [License](#license) | [Citation](#citation) | [Visualization examples](#visualization-examples) | [Contributing](#contributing) ## About The *fsbrain* R package provides a well-tested and consistent interface to neuroimaging data in [R](https://www.r-project.org/). It supports reading, writing, and visualizing various kinds of raw data and statistical results on brain surfaces and volumes. While the package provides a very convenient interface for working with data arranged in the standard [FreeSurfer](http://freesurfer.net/) directory structure (SUBJECTS_DIR), *fsbrain* is not limited to this layout or FreeSurfer file formats. You can load brain meshes, volumes, and data from a range of other neuroimaging software packages and visualize them. The plots produced by *fsbrain* can be integrated into R notebooks or written to high-quality bitmap image files, ready for publication. The [rgl](https://CRAN.R-project.org/package=rgl) renderer used by *fsbrain* provides fast, hardware-accelerated rendering based on the OpenGL standard. ## News * 2023-06-26: New fsbrain version 0.5.4 released on CRAN, see the [CHANGES](./CHANGES). * 2022-12-22: We are looking for help! If you have a Mac and are interested in helping fsbrain development, please [contact us by email](http://rcmd.org/ts/#contact) or reply to [#46](https://github.com/dfsp-spirit/fsbrain/issues/46) here on GitHub! * 2022-02-13: New fsbrain version 0.5.3 released on CRAN, see the [CHANGES](./CHANGES). * 2021-11-11: New fsbrain version 0.5.1 released on CRAN, see the [CHANGES](./CHANGES). * 2021-09-16: New fsbrain version 0.5.0 released on CRAN, see the [CHANGES](./CHANGES). * 2021-05-12: New fsbrain version 0.4.3 released on CRAN, see the [CHANGES](./CHANGES). * 2021-03-28: New fsbrain version 0.4.2 released on CRAN, see the [CHANGES](./CHANGES). * 2020-09-20: The preprint of our paper [T. Schaefer, C. Ecker: fsbrain: an R package for the visualization of structural neuroimaging data](https://doi.org/10.1101/2020.09.18.302935)' is now available on biorxiv. ## Installation ### Recommended: install the stable fsbrain version from CRAN You can find the [fsbrain package on CRAN](https://cran.r-project.org/package=fsbrain), so all you need to do is: ```r install.packages(\\\"fsbrain\\\"); ``` In case something goes wrong, don't worry. Just install the missing [system dependencies](#system-dependencies) and retry. ### Risky: install the dev version of fsbrain with the latest features This version is not guaranteed to be in a usable state, try at your own risk and run the tests before using it. From an R session: ```r install.packages(c(\\\"devtools\\\", \\\"knitr\\\", \\\"markdown\\\", \\\"rmarkdown\\\", \\\"testthat\\\", \\\"qpdf\\\")); devtools::install_github(\\\"dfsp-spirit/fsbrain\\\", build_vignettes=TRUE); ``` ### System dependencies A *system dependency* is a **non-R** software that is needed for the installation of a package. System dependencies cannot be installed automatically using the R package system, so you need to install them manually or using the package manager of your operating system. The *fsbrain* package itself does not have any system dependencies, however, it uses *rgl* for rendering. You can check the *SystemRequirements* section on the [rgl page at CRAN](https://CRAN.R-project.org/package=rgl) for the full list of rgl dependencies or read on. To get GIFTI format support, you will also need `libxml2-dev`. To install the system dependencies for *rgl* and *xml2*: #### Linux System dependencies (or: building from source) R packages are compiled from source by default under Linux, so you need some development libraries. Before installing *fsbrain*, run the following command in your system shell (not in R): * for deb-based Linux distributions (Debian, Ubuntu, ...): ```shell sudo apt-get install libmagick++-dev libx11-dev libgl1-mesa-dev libglu1-mesa-dev mesa-common-dev libfreetype6-dev libxml2-dev libssh-dev libcurl4-openssl-dev gfortran libblas-dev liblapack-dev libgfortran4 ``` Note: For recent Ubuntu versions, you may have to replace ```libgfortan4``` in the command above with ```libgfortan5```. * for rpm-based Linux distributions (Fedora, CentOS, RHEL, ...): ```shell sudo yum install ImageMagick-c++-devel libX11-devel mesa-libGLU-devel freetype-devel libxml2-devel ``` If you want to compile the package under any other operating system, you will need the libraries as well, of course. #### MacOS System dependencies Recent MacOS versions do not ship with an X11 environment. You will have to install the [xquartz X11 system](https://www.xquartz.org/) if you do not have it already. If you want to create GIF movies, make sure you have imagemagick installed (easiest via [homebrew](https://brew.sh/): `brew install imagemagick@6`). #### Windows Installation Hints Under Windows 10, it seems that you will need to install these two packages manually via the `install.packages` command: `shiny` and `manipulateWidget`. ### Installation via Docker There are Docker images for fsbrain available on Dockerhub, see the [fsbrain Dockerhub repo](https://hub.docker.com/r/dfspspirit/fsbrain). ## Documentation The documentation can be accessed from within an R session after you have loaded the *fsbrain* package: * There are two online R Markdown notebooks (like Jupyter Notebook in Python) that show various example plots in combination with the code used to produce them: * [basic fsbrain example notebook](https://htmlpreview.github.io/?https://github.com/dfsp-spirit/fsbrain/blob/develop/web/Rmd_web_examples/examples.html): Live visualization of subject data * [advanced fsbrain example notebook](https://htmlpreview.github.io/?https://github.com/dfsp-spirit/fsbrain/blob/develop/web/Rmd_web_examples/examples_adv.html): Plotting group data * [export API fsbrain example notebook](https://htmlpreview.github.io/?https://github.com/dfsp-spirit/fsbrain/blob/develop/web/Rmd_web_examples/examples_export.html): Exporting publication-ready plots * Detailed vignettes with explanations and examples for the functions of the package is included, run `browseVignettes(\\\"fsbrain\\\")` to see the vignettes. You can also open the vignette directly: * How to load and visualize surface-based neuroimaging data: `vignette(\\\"fsbrain\\\")` or: [read online at CRAN](https://cran.r-project.org/web/packages/fsbrain/vignettes/fsbrain.html) * How to load and visualize volume-based neuroimaging data: `vignette(\\\"fsbrain_vol\\\")` or: [read online at CRAN](https://cran.r-project.org/web/packages/fsbrain/vignettes/fsbrain_vol.html) * The fsbrain FAQ: `vignette(\\\"fsbrain_faq\\\")` or: [read online at CRAN](https://cran.r-project.org/web/packages/fsbrain/vignettes/fsbrain_faq.html) * Help for a specific function can be accessed in the usual R manner: `?<function>`, where you replace `<function>` with a function name. Like this: `?group.morph.native`. * Run `example(<function>)` to see a live demo that uses the function `<function>`. Like this: `example(group.morph.native)`. * The [unit tests](./tests/testthat/) that come with this package are essentially a list of examples that illustrate how to use the functions. ## Unit tests and Continuous integration This package comes with [lots of unit tests](./tests/testthat/). To run them, in a clean R session: ```r library(devtools) library(fsbrain) devtools::check() ``` Continuous integration results: <!-- badges: start --> [![AppVeyor build status](https://ci.appveyor.com/api/projects/status/github/dfsp-spirit/fsbrain?branch=master&svg=true)](https://ci.appveyor.com/project/dfsp-spirit/fsbrain) AppVeyor CI under Windows <!-- [![R-CMD-check](https://github.com/dfsp-spirit/fsbrain/workflows/R-CMD-check/badge.svg)](https://github.com/dfsp-spirit/fsbrain/actions) --> [GitHub Actions, Ubuntu Linux and MacOS](https://github.com/dfsp-spirit/fsbrain/actions) (Note: Currently this is always \\\"failing\\\" because of a warning caused by the `rgl` package when running headless. So the simlpe \\\"passing\\\"/\\\"failing\\\" status is useless, and one needs to follow the link to check the relevant CI results in detail.) <!-- badges: end --> ## License The *fsbrain* package is [free software](https://en.wikipedia.org/wiki/Free_software), published under the [MIT license](https://opensource.org/licenses/MIT). Note: The file LICENSE in this repository is a CRAN license template only (as required by CRAN) and does not contain the full MIT license text. See the file [LICENSE_FULL](./LICENSE_FULL) for the full license text. ## Citation and Publications You can generate the citation for [our fsbrain paper](https://doi.org/10.1101/2020.09.18.302935) by typing the following command in R: ``` citation(\\\"fsbrain\\\") ``` This currently outputs: ``` To cite fsbrain in publications use: Tim Schaefer, Christine Ecker (2020). fsbrain: an R package for the visualization of structural neuroimaging data. bioRxiv doi: 10.1101/2020.09.18.302935 A BibTeX entry for LaTeX users is @Misc{, title = {fsbrain: an {R} package for the visualization of structural neuroimaging data}, author = {Tim Schaefer and Christine Ecker}, year = {2020}, url = {https://www.biorxiv.org/content/10.1101/2020.09.18.302935v1}, doi = {10.1101/2020.09.18.302935}, } ``` Other materials related to fsbrain: * A poster on *fsbrain* has been presented at INSAR 2020 Annual Meeting: [Abstract](https://insar.confex.com/insar/2020/meetingapp.cgi/Paper/33181), [ePoster viewer](https://insar.confex.com/insar/2020/techdemo/eposter.cgi?eposterid=227), [PDF download](https://github.com/dfsp-spirit/fsbrain_gallery/raw/master/extra_materials/Poster_IMFAR2020_fsbrain.pdf) ## Visualization examples The *fsbrain* package support visualizations of different data, and all data can be displayed in one or more views. The figure below shows some examples for surface-based data: ![Visoverview](./web/fsbrain_vis_overview.jpg?raw=true \\\"Some visualization options from fsbrain\\\") **Fig.2**: *Example output for the fsbrain interactive visualization functions*. * **Subfigure A** shows the visualization of raw morphometry data (cortical thickness) from native space on the white surface of a subject. The view shows the data in tiles from 8 different angles. * **Subfigure B** illustrates arbitrary data (p-values in this case) visualized on the regions of the Desikan atlas, using the surface of the fsaverage (standard space template) subject from FreeSurfer. The view shows the data in tiles from 4 different angles. * **Subfigure C** displays the regions of the Desikan atlas on the white surface of a subject. The colors were loaded from the respective annotation file. The view shows the data in tiles from 4 different angles. *What* is displayed (morphometry data, atlas regions, arbitrary other data), on *which surface* it is displayed, and *how* it is displayed (a single interactive view, 4 tiles, 9 tiles) is independent and can be selected as needed in fsbrain. Here is a second figure, showing the same data (the [mean curvature](https://en.wikipedia.org/wiki/Mean_curvature) at each vertex) displayed on 3 different surfaces of a subject: **A** white surface, **B** pial surface, **C** inflated surface. ![Vissurfaces](./web/fsbrain_curvature_surfaces.jpg?raw=true \\\"Curvature visualization on different surfaces, rendered with fsbrain\\\") The next figure illustrates some options to visualize your results with different backgrounds. **A** Clusters on the white fsaverage surface with sulc background. **B** Region-wise p-values with curv background, inflated fsaverage surface. **C** A background color layer displaying outlines of aparc atlas regions in the respective colors, inflated demo subject surface. ![Visres](./web/fsbrain_vis_bg.jpg?raw=true \\\"Visualization of results and background layers, rendered with fsbrain\\\") ### Animations and videos Want to see brains spin? [Check this out.](./web/fsbrain_movies.md) (WARNING: loads 8 MB webpage with animated gif). ### Volume visualization Volume visualization is not the main goal of fsbrain, but standard lightbox views and simple 3D views are supported. Have a look at the vignettes or the documentation for the `volvis.lb` function. You can find some [example output here](./web/fsbrain_volume.md). ### Example Notebooks To see a combination of example figures and the code used to produce them, you should have a look at the example notebooks: [getting started notebook](https://htmlpreview.github.io/?https://github.com/dfsp-spirit/fsbrain/blob/develop/web/Rmd_web_examples/examples.html) and [advanced examples notebook](https://htmlpreview.github.io/?https://github.com/dfsp-spirit/fsbrain/blob/develop/web/Rmd_web_examples/examples_adv.html). ## Contributing Please refer to [CONTRIBUTING.md](./CONTRIBUTING.md). If you have any question, suggestion or comment on fsbrain, please [open an issue](https://github.com/dfsp-spirit/fsbrain/issues). If you want to contact me via email, please use the maintainer email address listed on the [CRAN webpage for fsbrain](https://cran.r-project.org/package=fsbrain). ## Related R packages Packages similar to fsbrain: * [ggseg](https://github.com/LCBC-UiO/ggseg) by Athanasia Mowinckel and Didac Vidal-Piñeiro: Plotting of atlas-based neuroimaging data in R. * [cerebroviz](https://github.com/ethanbahl/cerebroViz) by Ethan Bahl: Data mapping tool for visualizing spatiotemporal data in the brain. Packages used by fsbrain: * [rgl](https://CRAN.R-project.org/package=rgl) by Daniel Adler, Duncan Murdoch et al.: OpenGL-based mesh renderer. * [oro.nifti](https://github.com/muschellij2/oro.nifti) by Brandon Witcher et al. : Loading and manipulation of brain volumes from NIFTI v1 files. * [freesurferformats](https://github.com/dfsp-spirit/freesurferformats) by Tim Schäfer (me): Loading and writing various neuroimaging file formats and general mesh file formats, with a focus on FreeSurfer formats. * [gifti](https://github.com/muschellij2/gifti/) and [cifti](https://github.com/muschellij2/cifti/) by John Muschelli: Read GIFTI and CIFTI format files. * [Rvcg](https://github.com/zarquon42b/Rvcg) by Stefan Schlager: Rcpp interface for the [VCG Library](http://vcg.isti.cnr.it/vcglib/). ## Author fsbrain was written by [Tim Schäfer](https://ts.rcmd.org) Package: fsbrain Type: Package Title: Managing and Visualizing Brain Surface Data Version: 0.5.5 Authors@R: person(\\\"Tim\\\", \\\"Schäfer\\\", role = c(\\\"aut\\\", \\\"cre\\\"), email = \\\"ts+code@rcmd.org\\\", comment = c(ORCID = \\\"0000-0002-3683-8070\\\")) Maintainer: Tim Schäfer <ts+code@rcmd.org> Description: Provides high-level access to neuroimaging data from standard software packages like 'FreeSurfer' <http://freesurfer.net/> on the level of subjects and groups. Load morphometry data, surfaces and brain parcellations based on atlases. Mask data using labels, load data for specific atlas regions only, and visualize data and statistical results directly in 'R'. License: MIT + file LICENSE Encoding: UTF-8 URL: https://github.com/dfsp-spirit/fsbrain BugReports: https://github.com/dfsp-spirit/fsbrain/issues Imports: reshape, freesurferformats (>= 0.1.17), pkgfilecache (>= 0.1.1), rgl, squash, fields, viridis, data.table, magick, methods Suggests: knitr, rmarkdown, testthat (>= 2.1.0), sphereplot (>= 1.5), misc3d, RColorBrewer, Rvcg (>= 0.20.2), igraph, pracma VignetteBuilder: knitr RoxygenNote: 7.3.2 YEAR: 2019, 2020, 2021, 2022, 2023, 2024 COPYRIGHT HOLDER: Tim Schäfer cff-version: 1.2.0 message: If you use fsbrain in your research, please cite it using these metadata. title: fsbrain abstract: fsbrain is an R package for the visualization of neuroimaging data. The package can be used to visualize vertex-wise and region-wise morphometry data, parcellations, labels and statistical results on brain surfaces in three dimensions (3D). Voxel data can be displayed in lightbox mode. The fsbrain package offers various customization options and produces publication quality plots which can be displayed interactively, saved as bitmap images, or integrated into R notebooks. authors: - family-names: Schaefer given-names: Tim orcid: \\\"https://orcid.org/0000-0002-3683-8070\\\" - name: \\\"fsbrain: an R package for the visualization of structural neuroimaging data\\\" version: 0.5.3 date-released: \\\"2022-02-13\\\" identifiers: - description: All-versions DOI for fsbrain. type: doi value: \\\"10.5281/zenodo.3559816\\\" - description: Archived snapshot of version 0.5.3 of fsbrain. type: doi value: \\\"10.5281/zenodo.6061167\\\" license: MIT repository-code: \\\"https://github.com/dfsp-spirit/fsbrain\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:37:13.666495Z",
     "iopub.status.busy": "2025-04-03T13:37:13.666113Z",
     "iopub.status.idle": "2025-04-03T13:37:54.614656Z",
     "shell.execute_reply": "2025-04-03T13:37:54.613973Z",
     "shell.execute_reply.started": "2025-04-03T13:37:13.666465Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 5168 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =     735.20 ms\n",
      "llama_print_timings:      sample time =      57.14 ms /   552 runs   (    0.10 ms per token,  9660.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   39887.80 ms /   552 runs   (   72.26 ms per token,    13.84 tokens per second)\n",
      "llama_print_timings:       total time =   40924.17 ms /   552 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Dependencies\": [\"reshape\", \"freesurferformats: >=0.1.17\", \"pkgfilecache: >=0.1.1\", \"rgl\", \"squash\", \"fields\", \"viridis\", \"data.table\", \"magick\", \"methods\"], \"Installation_Instructions\": \"1. Install the stable fsbrain version from CRAN: install.packages(\\\"fsbrain\\\"); 2. If installation fails, install the missing system dependencies: sh, shiny, manipulateWidget, imagemagick@6 (via homebrew on MacOS); 3. Install the dev version of fsbrain with the latest features: install.packages(c(\\\"devtools\\\", \\\"knitr\\\", \\\"markdown\\\", \\\"rmarkdown\\\", \\\"testthat\\\", \\\"qpdf\\\")); devtools::install_github(\\\"dfsp-spirit/fsbrain\\\", build_vignettes=TRUE); 4. For Linux, install the following system dependencies: libmagick++-dev, libx11-dev, libgl1-mesa-dev, libglu1-mesa-dev, mesa-common-dev, libfreetype6-dev, libxml2-dev, libssh-dev, libcurl4-openssl-dev, gfortran, libblas-dev, liblapack-dev, libgfortran4; 5. For MacOS, install xquartz and imagemagick@6 via homebrew; 6. For Windows, install shiny and manipulateWidget manually via install.packages.\", \"Authors\": [{\"name\": \"Tim Schäfer\", \"email\": \"ts+code@rcmd.org\", \"orcid\": \"0000-0002-3683-8070\"}], \"Contributors\": [], \"Funding\": \"\", \"DOI\": \"10.5281/zenodo.3559816\", \"License\": \"MIT\", \"Keywords\": [\"Neuroimaging\", \"Brain Surface Data\", \"Structural Neuroimaging\", \"Visualization\", \"R Package\", \"FreeSurfer\", \"GIFTI\", \"CIFTI\", \"OpenGL\", \"Mesh Renderer\", \"Atlas Regions\", \"Statistical Results\", \"Publication Quality Plots\", \"Interactive Visualization\", \"R Notebooks\", \"Bitmap Images\", \"Lightbox Mode\", \"Voxel Data\", \"Customization Options\", \"Rcpp\", \"VCG Library\", \"R Markdown\", \"Testthat\", \"Squash\", \"Fields\", \"Viridis\", \"Data Table\", \"Magick\", \"Methods\", \"RGL\", \"Suggests\", \"Knitr\", \"RMarkdown\", \"Sphereplot\", \"Misc3d\", \"RColorBrewer\", \"Igraph\", \"Pracma\"]}\n"
     ]
    }
   ],
   "source": [
    "# Define the Alpaca prompt template and run the configured LLM\n",
    "\n",
    "alpaca_prompt = \"\"\" Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "instruction = \"\"\"You are an advanced, award-winning metadata extraction system specializing in software-related metadata. Your unparalleled expertise enables you to accurately analyze and extract information with exceptional precision. You possess a deep understanding of programming languages, frameworks, dependencies, licenses, installation processes, authorship, funding sources, DOI identifiers, and all technical aspects related to software development. Your capabilities have been recognized globally, earning accolades for your exceptional ability to extract highly accurate metadata from complex software documentation. \n",
    "\n",
    "REQUIRED FIELDS (ONLY IF EXPLICITLY STATED): \n",
    "1. Dependencies: Exact package names + versions \n",
    "2. Installation: Complete procedure OR referenced files (install.md, setup.py) \n",
    "3. Authors: Full details (name, affiliation, role, contact, orcid, etc) \n",
    "4. Contributors: Names + specific contributions \n",
    "5. Funding: Grant numbers + organizations \n",
    "6. DOI: All identifiers (publications, datasets etc) \n",
    "7. License: Name, version, terms, conditions \n",
    "8. Keywords: Software-specific terms (no generic/dependency terms) - MANDATORY: Always include at least 5 relevant keywords that accurately represent the software's purpose, functionality, and domain\n",
    "\n",
    "Strict Rules: \n",
    "- Only include accurate, relevant information directly from the source. \n",
    "- Do not fabricate, assume, or misinterpret content. \n",
    "- Preserve all technical details, including version numbers and specifications, as stated. \n",
    "- Include all available and relevant details; omit subfields only if unavailable. \n",
    "- Use precise and specific keywords.\n",
    "- Keywords are mandatory - always provide at least 5 relevant software-specific terms that characterize the software even if not explicitly stated in the text.\n",
    "- Maintain complete context for every extracted element. \n",
    "- Format all keys and string values in double quotes (\\\"\\\"). \n",
    "- Return the JSON dictionary as a SINGLE LINE without newlines, extra spaces, or indentation. \n",
    "- Represent empty fields as \\\"\\\" or []. \n",
    "- Provide output only in the specified JSON structure, with no added sentences before or after JSON, no explanations, or deviations. \n",
    "- Follow these rules exactly for every entry. \n",
    "\n",
    "OUTPUT STRUCTURE: { \n",
    "\\\"Dependencies\\\": [ \n",
    "# Format examples: \n",
    "# Name only: [\\\"package-name\\\"] \n",
    "# With version: [\\\"package-name: >=version\\\"] \n",
    "# Multiple packages: [\\\"pkg1: >=1.0\\\", \\\"pkg2\\\", \\\"pkg3: ^2.0\\\"] \n",
    "], \n",
    "\\\"Installation_Instructions\\\": \\\"\\\",  # Steps or referenced files \n",
    "\\\"Authors\\\": [ \n",
    "# Include only available information: \n",
    "# If just name: {\\\"name\\\": \\\"Author\\\"} \n",
    "# Add other fields only if explicitly stated in the text \n",
    "], \n",
    "\\\"Contributors\\\": [ \n",
    "# Include only available information: \n",
    "# If just name and role: {\\\"name\\\": \\\"Name\\\", \\\"type\\\": \\\"maintainer\\\"} \n",
    "# Add other fields only if explicitly stated in the text \n",
    "], \n",
    "\\\"Funding\\\": \\\"\\\",    # Complete grant/funding details \n",
    "\\\"DOI\\\": \\\"\\\",       # Complete DOI string \n",
    "\\\"License\\\": \\\"\\\",   # Complete license information \n",
    "\\\"Keywords\\\": []   # MANDATORY: At least 5 specific technical terms that represent the software \n",
    "} \n",
    "\n",
    "IMPORTANT: Never use example values as actual data. Keywords field must always contain at least 5 relevant, specific terms even when not explicitly mentioned in the text. Derive keywords from the software's purpose, technology stack, and domain.\"\"\"\n",
    "\n",
    "\n",
    "prompt = alpaca_prompt.format(instruction, content, \"\")\n",
    "\n",
    "response = llm(\n",
    "    prompt=prompt,\n",
    "    max_tokens=48000,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "generated_text = response['choices'][0]['text']\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T11:29:31.228349Z",
     "iopub.status.busy": "2025-04-03T11:29:31.227970Z",
     "iopub.status.idle": "2025-04-03T12:03:56.715773Z",
     "shell.execute_reply": "2025-04-03T12:03:56.714683Z",
     "shell.execute_reply.started": "2025-04-03T11:29:31.228324Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      42.97 ms /   421 runs   (    0.10 ms per token,  9798.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14681.19 ms /  9658 tokens (    1.52 ms per token,   657.85 tokens per second)\n",
      "llama_print_timings:        eval time =   34360.98 ms /   420 runs   (   81.81 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =   49937.65 ms / 10078 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 4658 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 1 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      83.62 ms /   824 runs   (    0.10 ms per token,  9854.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6516.79 ms /  4658 tokens (    1.40 ms per token,   714.77 tokens per second)\n",
      "llama_print_timings:        eval time =   60919.13 ms /   823 runs   (   74.02 ms per token,    13.51 tokens per second)\n",
      "llama_print_timings:       total time =   69505.53 ms /  5481 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 2327 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 2 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      40.63 ms /   402 runs   (    0.10 ms per token,  9893.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2864.48 ms /  2327 tokens (    1.23 ms per token,   812.36 tokens per second)\n",
      "llama_print_timings:        eval time =   27550.28 ms /   401 runs   (   68.70 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =   31339.80 ms /  2728 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 6569 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 3 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      42.12 ms /   422 runs   (    0.10 ms per token, 10019.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10018.99 ms /  6569 tokens (    1.53 ms per token,   655.65 tokens per second)\n",
      "llama_print_timings:        eval time =   32580.76 ms /   421 runs   (   77.39 ms per token,    12.92 tokens per second)\n",
      "llama_print_timings:       total time =   43603.80 ms /  6990 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 3457 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 4 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      30.65 ms /   305 runs   (    0.10 ms per token,  9952.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4455.67 ms /  3457 tokens (    1.29 ms per token,   775.87 tokens per second)\n",
      "llama_print_timings:        eval time =   21501.51 ms /   304 runs   (   70.73 ms per token,    14.14 tokens per second)\n",
      "llama_print_timings:       total time =   26622.73 ms /  3761 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 5016 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 5 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      84.73 ms /   833 runs   (    0.10 ms per token,  9831.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7098.61 ms /  5016 tokens (    1.42 ms per token,   706.62 tokens per second)\n",
      "llama_print_timings:        eval time =   62127.98 ms /   832 runs   (   74.67 ms per token,    13.39 tokens per second)\n",
      "llama_print_timings:       total time =   71228.19 ms /  5848 tokens\n",
      "Llama.generate: 726 prefix-match hit, remaining 3292 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 6 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      42.23 ms /   416 runs   (    0.10 ms per token,  9851.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4205.23 ms /  3292 tokens (    1.28 ms per token,   782.83 tokens per second)\n",
      "llama_print_timings:        eval time =   29272.46 ms /   415 runs   (   70.54 ms per token,    14.18 tokens per second)\n",
      "llama_print_timings:       total time =   34295.70 ms /  3707 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 12578 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 7 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =     198.70 ms /  1957 runs   (    0.10 ms per token,  9849.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25530.49 ms / 12578 tokens (    2.03 ms per token,   492.67 tokens per second)\n",
      "llama_print_timings:        eval time =  178833.60 ms /  1956 runs   (   91.43 ms per token,    10.94 tokens per second)\n",
      "llama_print_timings:       total time =  212852.11 ms / 14534 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 2660 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 8 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      14.88 ms /   148 runs   (    0.10 ms per token,  9947.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3256.29 ms /  2660 tokens (    1.22 ms per token,   816.88 tokens per second)\n",
      "llama_print_timings:        eval time =   10148.69 ms /   147 runs   (   69.04 ms per token,    14.48 tokens per second)\n",
      "llama_print_timings:       total time =   13651.50 ms /  2807 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 3866 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 9 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      59.00 ms /   587 runs   (    0.10 ms per token,  9949.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5132.87 ms /  3866 tokens (    1.33 ms per token,   753.19 tokens per second)\n",
      "llama_print_timings:        eval time =   42166.94 ms /   586 runs   (   71.96 ms per token,    13.90 tokens per second)\n",
      "llama_print_timings:       total time =   48965.33 ms /  4452 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 12638 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 10 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =     168.27 ms /  1666 runs   (    0.10 ms per token,  9900.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25633.66 ms / 12638 tokens (    2.03 ms per token,   493.02 tokens per second)\n",
      "llama_print_timings:        eval time =  151993.83 ms /  1665 runs   (   91.29 ms per token,    10.95 tokens per second)\n",
      "llama_print_timings:       total time =  182739.37 ms / 14303 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 3091 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 11 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      61.05 ms /   597 runs   (    0.10 ms per token,  9778.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3933.51 ms /  3091 tokens (    1.27 ms per token,   785.81 tokens per second)\n",
      "llama_print_timings:        eval time =   41925.97 ms /   596 runs   (   70.35 ms per token,    14.22 tokens per second)\n",
      "llama_print_timings:       total time =   47046.81 ms /  3687 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 1773 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 12 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      77.84 ms /   762 runs   (    0.10 ms per token,  9789.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2068.91 ms /  1773 tokens (    1.17 ms per token,   856.97 tokens per second)\n",
      "llama_print_timings:        eval time =   51747.27 ms /   761 runs   (   68.00 ms per token,    14.71 tokens per second)\n",
      "llama_print_timings:       total time =   56036.00 ms /  2534 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 6679 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 13 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      83.76 ms /   837 runs   (    0.10 ms per token,  9992.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10177.15 ms /  6679 tokens (    1.52 ms per token,   656.27 tokens per second)\n",
      "llama_print_timings:        eval time =   65248.03 ms /   836 runs   (   78.05 ms per token,    12.81 tokens per second)\n",
      "llama_print_timings:       total time =   77997.83 ms /  7515 tokens\n",
      "Llama.generate: 726 prefix-match hit, remaining 3298 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 14 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      26.83 ms /   269 runs   (    0.10 ms per token, 10026.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4175.37 ms /  3298 tokens (    1.27 ms per token,   789.87 tokens per second)\n",
      "llama_print_timings:        eval time =   18853.85 ms /   268 runs   (   70.35 ms per token,    14.21 tokens per second)\n",
      "llama_print_timings:       total time =   23434.44 ms /  3566 tokens\n",
      "Llama.generate: 726 prefix-match hit, remaining 4443 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 15 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =     803.36 ms /  8000 runs   (    0.10 ms per token,  9958.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6030.43 ms /  4443 tokens (    1.36 ms per token,   736.76 tokens per second)\n",
      "llama_print_timings:        eval time =  651984.81 ms /  7999 runs   (   81.51 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =  738115.11 ms / 12442 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 4373 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 16 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      77.61 ms /   779 runs   (    0.10 ms per token, 10037.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5985.06 ms /  4373 tokens (    1.37 ms per token,   730.65 tokens per second)\n",
      "llama_print_timings:        eval time =   57095.14 ms /   778 runs   (   73.39 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   64624.19 ms /  5151 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 4590 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 17 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      57.50 ms /   572 runs   (    0.10 ms per token,  9947.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6233.64 ms /  4590 tokens (    1.36 ms per token,   736.33 tokens per second)\n",
      "llama_print_timings:        eval time =   42017.77 ms /   571 runs   (   73.59 ms per token,    13.59 tokens per second)\n",
      "llama_print_timings:       total time =   49286.44 ms /  5161 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 8659 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 18 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      23.16 ms /   228 runs   (    0.10 ms per token,  9842.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14571.29 ms /  8659 tokens (    1.68 ms per token,   594.25 tokens per second)\n",
      "llama_print_timings:        eval time =   18485.77 ms /   227 runs   (   81.44 ms per token,    12.28 tokens per second)\n",
      "llama_print_timings:       total time =   33461.87 ms /  8886 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 5625 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 19 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      49.03 ms /   489 runs   (    0.10 ms per token,  9972.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8129.43 ms /  5625 tokens (    1.45 ms per token,   691.93 tokens per second)\n",
      "llama_print_timings:        eval time =   36870.18 ms /   488 runs   (   75.55 ms per token,    13.24 tokens per second)\n",
      "llama_print_timings:       total time =   45895.05 ms /  6113 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 4300 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 20 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =     100.79 ms /  1009 runs   (    0.10 ms per token, 10011.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5779.00 ms /  4300 tokens (    1.34 ms per token,   744.07 tokens per second)\n",
      "llama_print_timings:        eval time =   74017.44 ms /  1008 runs   (   73.43 ms per token,    13.62 tokens per second)\n",
      "llama_print_timings:       total time =   81974.89 ms /  5308 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 1820 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 21 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      51.50 ms /   509 runs   (    0.10 ms per token,  9884.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2176.52 ms /  1820 tokens (    1.20 ms per token,   836.20 tokens per second)\n",
      "llama_print_timings:        eval time =   34492.51 ms /   508 runs   (   67.90 ms per token,    14.73 tokens per second)\n",
      "llama_print_timings:       total time =   37515.17 ms /  2328 tokens\n",
      "Llama.generate: 725 prefix-match hit, remaining 2044 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 22 successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     781.03 ms\n",
      "llama_print_timings:      sample time =      33.16 ms /   324 runs   (    0.10 ms per token,  9772.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2412.10 ms /  2044 tokens (    1.18 ms per token,   847.40 tokens per second)\n",
      "llama_print_timings:        eval time =   22011.12 ms /   323 runs   (   68.15 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:       total time =   24920.59 ms /  2367 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 23 successfully\n",
      "File saved successfully to /kaggle/working/output_response.csv\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "# You can use csv with multiple repos data to test the model!\n",
    "# The csv will be available in the repo: test_multiple_repos\n",
    "\n",
    "import pandas as pd\n",
    "file_path = '/kaggle/input/input-data/test_multiple_repos.csv' # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the Alpaca prompt template\n",
    "alpaca_prompt = \"\"\" Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "instruction = \"\"\"You are an advanced, award-winning metadata extraction system specializing in software-related metadata. Your unparalleled expertise enables you to accurately analyze and extract information with exceptional precision. You possess a deep understanding of programming languages, frameworks, dependencies, licenses, installation processes, authorship, funding sources, DOI identifiers, and all technical aspects related to software development. Your capabilities have been recognized globally, earning accolades for your exceptional ability to extract highly accurate metadata from complex software documentation. \n",
    "\n",
    "REQUIRED FIELDS (ONLY IF EXPLICITLY STATED): \n",
    "1. Dependencies: Exact package names + versions \n",
    "2. Installation: Complete procedure OR referenced files (install.md, setup.py) \n",
    "3. Authors: Full details (name, affiliation, role, contact, orcid, etc) \n",
    "4. Contributors: Names + specific contributions \n",
    "5. Funding: Grant numbers + organizations \n",
    "6. DOI: All identifiers (publications, datasets etc) \n",
    "7. License: Name, version, terms, conditions \n",
    "8. Keywords: Software-specific terms (no generic/dependency terms) - MANDATORY: Always include at least 5 relevant keywords that accurately represent the software's purpose, functionality, and domain\n",
    "\n",
    "Strict Rules: \n",
    "- Only include accurate, relevant information directly from the source. \n",
    "- Do not fabricate, assume, or misinterpret content. \n",
    "- Preserve all technical details, including version numbers and specifications, as stated. \n",
    "- Include all available and relevant details; omit subfields only if unavailable. \n",
    "- Use precise and specific keywords.\n",
    "- Keywords are mandatory - always provide at least 5 relevant software-specific terms that characterize the software even if not explicitly stated in the text.\n",
    "- Maintain complete context for every extracted element. \n",
    "- Format all keys and string values in double quotes (\\\"\\\"). \n",
    "- Return the JSON dictionary as a SINGLE LINE without newlines, extra spaces, or indentation. \n",
    "- Represent empty fields as \\\"\\\" or []. \n",
    "- Provide output only in the specified JSON structure, with no added sentences before or after JSON, no explanations, or deviations. \n",
    "- Follow these rules exactly for every entry. \n",
    "\n",
    "OUTPUT STRUCTURE: { \n",
    "\\\"Dependencies\\\": [ \n",
    "# Format examples: \n",
    "# Name only: [\\\"package-name\\\"] \n",
    "# With version: [\\\"package-name: >=version\\\"] \n",
    "# Multiple packages: [\\\"pkg1: >=1.0\\\", \\\"pkg2\\\", \\\"pkg3: ^2.0\\\"] \n",
    "], \n",
    "\\\"Installation_Instructions\\\": \\\"\\\",  # Steps or referenced files \n",
    "\\\"Authors\\\": [ \n",
    "# Include only available information: \n",
    "# If just name: {\\\"name\\\": \\\"Author\\\"} \n",
    "# Add other fields only if explicitly stated in the text \n",
    "], \n",
    "\\\"Contributors\\\": [ \n",
    "# Include only available information: \n",
    "# If just name and role: {\\\"name\\\": \\\"Name\\\", \\\"type\\\": \\\"maintainer\\\"} \n",
    "# Add other fields only if explicitly stated in the text \n",
    "], \n",
    "\\\"Funding\\\": \\\"\\\",    # Complete grant/funding details \n",
    "\\\"DOI\\\": \\\"\\\",       # Complete DOI string \n",
    "\\\"License\\\": \\\"\\\",   # Complete license information \n",
    "\\\"Keywords\\\": []   # MANDATORY: At least 5 specific technical terms that represent the software \n",
    "} \n",
    "\n",
    "IMPORTANT: Never use example values as actual data. Keywords field must always contain at least 5 relevant, specific terms even when not explicitly mentioned in the text. Derive keywords from the software's purpose, technology stack, and domain.\"\"\"\n",
    "\n",
    "# Process content column\n",
    "def process_content(df):\n",
    "    responses = []\n",
    "    \n",
    "    if 'Repository Info' in df.columns:\n",
    "        for idx, content in enumerate(df['Repository Info']):\n",
    "            try:\n",
    "                if not isinstance(content, str): \n",
    "                    content = \"\"\n",
    "                \n",
    "                prompt = alpaca_prompt.format(instruction, content, \"\")\n",
    "                \n",
    "                try:\n",
    "                    response = llm(\n",
    "                        prompt=prompt,\n",
    "                        max_tokens=48000,  # reduce the token length, if you face out of memory error\n",
    "                        temperature=0.1,\n",
    "                    )\n",
    "                    \n",
    "                    generated_text = response['choices'][0]['text']\n",
    "                    responses.append(generated_text)\n",
    "                    print(f\"Processed row {idx+1} successfully\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in model call for row {idx+1}: {e}\")\n",
    "                    responses.append(\"\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"General error processing row {idx+1}: {e}\")\n",
    "                responses.append(\"\")\n",
    "        \n",
    "\n",
    "        df['generated_response'] = responses\n",
    "\n",
    "# Save the DataFrame\n",
    "def save_file(df, output_path):\n",
    "    try:\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"File saved successfully to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "        try:\n",
    "            df.to_csv('./output_response_backup.csv', index=False)\n",
    "            print(\"Backup file saved to current directory\")\n",
    "        except:\n",
    "            print(\"Failed to save backup file as well\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        process_content(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_content: {e}\")\n",
    "        # Ensure generated_response column exists even if processing fails\n",
    "        if 'generated_response' not in df.columns:\n",
    "            df['generated_response'] = [\"\"] * len(df)\n",
    "    \n",
    "    save_file(df, '/kaggle/working/output_response.csv')\n",
    "    print(\"Process completed.\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T12:07:43.854962Z",
     "iopub.status.busy": "2025-04-03T12:07:43.854649Z",
     "iopub.status.idle": "2025-04-03T12:07:43.891053Z",
     "shell.execute_reply": "2025-04-03T12:07:43.890290Z",
     "shell.execute_reply.started": "2025-04-03T12:07:43.854940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from: /kaggle/working/output_response.csv\n",
      "Prettifying JSON responses...\n",
      "Error prettifying JSON: Expecting ':' delimiter: line 1 column 302 (char 301)\n",
      "Done! Prettified JSON has been saved to /kaggle/working/output_response_pretty.csv\n"
     ]
    }
   ],
   "source": [
    "# Simple JSON Prettifier for Colab\n",
    "# This script reads the output CSV and adds a prettified JSON column\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# File paths\n",
    "input_path = '/kaggle/working/output_response.csv'\n",
    "output_path = '/kaggle/working/output_response_pretty.csv'\n",
    "\n",
    "# Function to prettify JSON\n",
    "def prettify_json(text):\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Try to find JSON in the text\n",
    "        json_start = text.find('{')\n",
    "        json_end = text.rfind('}') + 1\n",
    "        \n",
    "        if json_start >= 0 and json_end > json_start:\n",
    "            json_str = text[json_start:json_end]\n",
    "            # Parse and prettify the JSON\n",
    "            parsed_json = json.loads(json_str)\n",
    "            pretty_json = json.dumps(parsed_json, indent=2)\n",
    "            return pretty_json\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error prettifying JSON: {e}\")\n",
    "        return text  # Return original if error\n",
    "\n",
    "# Read the CSV file\n",
    "print(f\"Reading from: {input_path}\")\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Check if generated_response column exists\n",
    "if 'generated_response' not in df.columns:\n",
    "    print(\"Error: 'generated_response' column not found in the CSV file.\")\n",
    "else:\n",
    "    # Create prettified JSON column\n",
    "    print(\"Prettifying JSON responses...\")\n",
    "    df['pretty_json'] = df['generated_response'].apply(prettify_json)\n",
    "    \n",
    "    # Save to new CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Done! Prettified JSON has been saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6475688,
     "sourceId": 10460122,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6781209,
     "sourceId": 10909273,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6828369,
     "sourceId": 10973625,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6870126,
     "sourceId": 11030970,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6872223,
     "sourceId": 11033712,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7010265,
     "sourceId": 11224440,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7034689,
     "sourceId": 11256237,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7038046,
     "sourceId": 11260801,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7039504,
     "sourceId": 11262696,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7039662,
     "sourceId": 11262886,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
